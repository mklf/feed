{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.4","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-10-19T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Inconsistent Few-Shot Relation Classification via Cross-Attentional Prototype Networks with Contrastive Learning. (arXiv:2110.08254v1 [cs.LG])","link":"http://arxiv.org/abs/2110.08254","description":"<p>Standard few-shot relation classification (RC) is designed to learn a robust\nclassifier with only few labeled data for each class. However, previous works\nrarely investigate the effects of a different number of classes (i.e., $N$-way)\nand number of labeled data per class (i.e., $K$-shot) during training vs.\ntesting. In this work, we define a new task, \\textit{inconsistent few-shot RC},\nwhere the model needs to handle the inconsistency of $N$ and $K$ between\ntraining and testing. To address this new task, we propose Prototype\nNetwork-based cross-attention contrastive learning (ProtoCACL) to capture the\nrich mutual interactions between the support set and query set. Experimental\nresults demonstrate that our ProtoCACL can outperform the state-of-the-art\nbaseline model under both inconsistent $K$ and inconsistent $N$ settings, owing\nto its more robust and discriminate representations. Moreover, we identify that\nin the inconsistent few-shot learning setting, models can achieve better\nperformance with \\textit{less data} than the standard few-shot setting with\ncarefully-selected $N$ and $K$. In the end of the paper, we provide further\nanalyses and suggestions to systematically guide the selection of $N$ and $K$\nunder different scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongru Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhijing Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Jiarun Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_G/0/1/0/all/0/1\">Gabriel Pui Cheong Fung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_K/0/1/0/all/0/1\">Kam-Fai Wong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Multimodal to Unimodal Attention in Transformers using Knowledge Distillation. (arXiv:2110.08270v1 [cs.LG])","link":"http://arxiv.org/abs/2110.08270","description":"<p>Multimodal Deep Learning has garnered much interest, and transformers have\ntriggered novel approaches, thanks to the cross-attention mechanism. Here we\npropose an approach to deal with two key existing challenges: the high\ncomputational resource demanded and the issue of missing modalities. We\nintroduce for the first time the concept of knowledge distillation in\ntransformers to use only one modality at inference time. We report a full study\nanalyzing multiple student-teacher configurations, levels at which distillation\nis applied, and different methodologies. With the best configuration, we\nimproved the state-of-the-art accuracy by 3%, we reduced the number of\nparameters by 2.5 times and the inference time by 22%. Such\nperformance-computation tradeoff can be exploited in many applications and we\naim at opening a new research area where the deployment of complex models with\nlimited resources is demanded.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_D/0/1/0/all/0/1\">Dhruv Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_T/0/1/0/all/0/1\">Tanay Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrari_L/0/1/0/all/0/1\">Laura M. Ferrari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bremond_F/0/1/0/all/0/1\">Fran&#xe7;ois Bremond</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Boosting coherence of language models. (arXiv:2110.08294v1 [cs.CL])","link":"http://arxiv.org/abs/2110.08294","description":"<p>Naturality of long-term information structure -- coherence -- remains a\nchallenge in language generation. Large language models have insufficiently\nlearned such structure, as their long-form generations differ from natural text\nin measures of coherence. To alleviate this divergence, we propose coherence\nboosting, an inference procedure that increases the effect of distant context\non next-token prediction. We show the benefits of coherence boosting with\npretrained models by distributional analyses of generated ordinary text and\ndialog responses. We also find that coherence boosting with state-of-the-art\nmodels for various zero-shot NLP tasks yields performance gains with no\nadditional training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Malkin_N/0/1/0/all/0/1\">Nikolay Malkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jojic_N/0/1/0/all/0/1\">Nebojsa Jojic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Aspect-Oriented Summarization through Query-Focused Extraction. (arXiv:2110.08296v1 [cs.CL])","link":"http://arxiv.org/abs/2110.08296","description":"<p>A reader interested in a particular topic might be interested in summarizing\ndocuments on that subject with a particular focus, rather than simply seeing\ngeneric summaries produced by most summarization systems. While query-focused\nsummarization has been explored in prior work, this is often approached from\nthe standpoint of document-specific questions or on synthetic data. Real users'\nneeds often fall more closely into aspects, broad topics in a dataset the user\nis interested in rather than specific queries. In this paper, we collect a\ndataset of realistic aspect-oriented test cases, AspectNews, which covers\ndifferent subtopics about articles in news sub-domains. We then investigate how\nquery-focused methods, for which we can construct synthetic data, can handle\nthis aspect-oriented setting: we benchmark extractive query-focused training\nschemes, and propose a contrastive augmentation approach to train the model. We\nevaluate on two aspect-oriented datasets and find this approach yields (a)\nfocused summaries, better than those from a generic summarization system, which\ngo beyond simple keyword matching; (b) a system sensitive to the choice of\nkeywords.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ahuja_O/0/1/0/all/0/1\">Ojas Ahuja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jiacheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Akshay Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horecka_K/0/1/0/all/0/1\">Kevin Horecka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrett_G/0/1/0/all/0/1\">Greg Durrett</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"When Combating Hype, Proceed with Caution. (arXiv:2110.08300v1 [cs.CL])","link":"http://arxiv.org/abs/2110.08300","description":"<p>In an effort to avoid reinforcing widespread hype about the capabilities of\nstate-of-the-art language technology, researchers have developed practices in\nframing and citation that serve to deemphasize the field's successes. Though\nwell-meaning, these practices often yield misleading or even false claims about\nthe limits of our best technology. This is a problem, and it may be more\nserious than it looks: It limits our ability to mitigate short-term harms from\nNLP deployments and it limits our ability to prepare for the potentially\nenormous impacts of more distant future advances. This paper urges researchers\nto be careful about these claims and suggests some research directions and\ncommunication strategies that will make it easier to avoid or rebut them.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bowman_S/0/1/0/all/0/1\">Samuel R. Bowman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Learning the Transformer Kernel. (arXiv:2110.08323v1 [cs.LG])","link":"http://arxiv.org/abs/2110.08323","description":"<p>In this work we introduce KERNELIZED TRANSFORMER, a generic, scalable, data\ndriven framework for learning the kernel function in Transformers. Our\nframework approximates the Transformer kernel as a dot product between spectral\nfeature maps and learns the kernel by learning the spectral distribution. This\nnot only helps in learning a generic kernel end-to-end, but also reduces the\ntime and space complexity of Transformers from quadratic to linear. We show\nthat KERNELIZED TRANSFORMERS achieve performance comparable to existing\nefficient Transformer architectures, both in terms of accuracy as well as\ncomputational efficiency. Our study also demonstrates that the choice of the\nkernel has a substantial impact on performance, and kernel learning variants\nare competitive alternatives to fixed kernel Transformers, both in long as well\nas short sequence tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1\">Sankalan Pal Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Solomou_A/0/1/0/all/0/1\">Adamos Solomou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dubey_A/0/1/0/all/0/1\">Avinava Dubey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Control Prefixes for Text Generation. (arXiv:2110.08329v1 [cs.CL])","link":"http://arxiv.org/abs/2110.08329","description":"<p>Prompt learning methods adapt pre-trained language models to downstream\napplications by using a task-specific prompt together with the input. Most of\nthe current work on prompt learning in text generation relies on a shared\ndataset-level prompt for all examples in the dataset. We extend this approach\nand propose a dynamic method, Control Prefixes, which allows for the inclusion\nof conditional input-dependent information in each prompt. Control Prefixes is\nat the intersection of prompt learning and controlled generation, empowering\nthe model to have finer-grained control during text generation. The method\nincorporates attribute-level learnable representations into different layers of\na pre-trained transformer, allowing for the generated text to be guided in a\nparticular direction. We provide a systematic evaluation of the technique and\napply it to five datasets from the GEM benchmark for natural language\ngeneration (NLG). We present state-of-the-art results on several data-to-text\ndatasets, including WebNLG.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Clive_J/0/1/0/all/0/1\">Jordan Clive</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_K/0/1/0/all/0/1\">Kris Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rei_M/0/1/0/all/0/1\">Marek Rei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Transparent Interactive Semantic Parsing via Step-by-Step Correction. (arXiv:2110.08345v1 [cs.CL])","link":"http://arxiv.org/abs/2110.08345","description":"<p>Existing studies on semantic parsing focus primarily on mapping a\nnatural-language utterance to a corresponding logical form in one turn.\nHowever, because natural language can contain a great deal of ambiguity and\nvariability, this is a difficult challenge. In this work, we investigate an\ninteractive semantic parsing framework that explains the predicted logical form\nstep by step in natural language and enables the user to make corrections\nthrough natural-language feedback for individual steps. We focus on question\nanswering over knowledge bases (KBQA) as an instantiation of our framework,\naiming to increase the transparency of the parsing process and help the user\nappropriately trust the final answer. To do so, we construct INSPIRED, a\ncrowdsourced dialogue dataset derived from the ComplexWebQuestions dataset. Our\nexperiments show that the interactive framework with human feedback has the\npotential to greatly improve overall parse accuracy. Furthermore, we develop a\npipeline for dialogue simulation to evaluate our framework w.r.t. a variety of\nstate-of-the-art KBQA models without involving further crowdsourcing effort.\nThe results demonstrate that our interactive semantic parsing framework\npromises to be effective across such models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mo_L/0/1/0/all/0/1\">Lingbo Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_A/0/1/0/all/0/1\">Ashley Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Huan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+White_M/0/1/0/all/0/1\">Michael White</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Omni-sparsity DNN: Fast Sparsity Optimization for On-Device Streaming E2E ASR via Supernet. (arXiv:2110.08352v1 [cs.SD])","link":"http://arxiv.org/abs/2110.08352","description":"<p>From wearables to powerful smart devices, modern automatic speech recognition\n(ASR) models run on a variety of edge devices with different computational\nbudgets. To navigate the Pareto front of model accuracy vs model size,\nresearchers are trapped in a dilemma of optimizing model accuracy by training\nand fine-tuning models for each individual edge device while keeping the\ntraining GPU-hours tractable. In this paper, we propose Omni-sparsity DNN,\nwhere a single neural network can be pruned to generate optimized model for a\nlarge range of model sizes. We develop training strategies for Omni-sparsity\nDNN that allows it to find models along the Pareto front of word-error-rate\n(WER) vs model size while keeping the training GPU-hours to no more than that\nof training one singular model. We demonstrate the Omni-sparsity DNN with\nstreaming E2E ASR models. Our results show great saving on training time and\nresources with similar or better accuracy on LibriSpeech compared to\nindividually pruned sparse models: 2%-6.6% better WER on Test-other.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haichuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shangguan_Y/0/1/0/all/0/1\">Yuan Shangguan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dilin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Meng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chuang_P/0/1/0/all/0/1\">Pierce Chuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaohui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkatesh_G/0/1/0/all/0/1\">Ganesh Venkatesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalinli_O/0/1/0/all/0/1\">Ozlem Kalinli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandra_V/0/1/0/all/0/1\">Vikas Chandra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning with Noisy Labels by Targeted Relabeling. (arXiv:2110.08355v1 [cs.CL])","link":"http://arxiv.org/abs/2110.08355","description":"<p>Crowdsourcing platforms are often used to collect datasets for training deep\nneural networks, despite higher levels of inaccurate labeling compared to\nexpert labeling. There are two common strategies to manage the impact of this\nnoise, the first involves aggregating redundant annotations, but comes at the\nexpense of labeling substantially fewer examples. Secondly, prior works have\nalso considered using the entire annotation budget to label as many examples as\npossible and subsequently apply denoising algorithms to implicitly clean up the\ndataset. We propose an approach which instead reserves a fraction of\nannotations to explicitly relabel highly probable labeling errors. In\nparticular, we allocate a large portion of the labeling budget to form an\ninitial dataset used to train a model. This model is then used to identify\nspecific examples that appear most likely to be incorrect, which we spend the\nremaining budget to relabel. Experiments across three model variations and four\nnatural language processing tasks show our approach outperforms both label\naggregation and advanced denoising methods designed to handle noisy labels when\nallocated the same annotation budget.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Derek Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bowman_S/0/1/0/all/0/1\">Samuel R. Bowman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Training Dynamics for Text Summarization Models. (arXiv:2110.08370v1 [cs.CL])","link":"http://arxiv.org/abs/2110.08370","description":"<p>Pre-trained language models (e.g. BART) have shown impressive results when\nfine-tuned on large summarization datasets. However, little is understood about\nthis fine-tuning process, including what knowledge is retained from\npre-training models or how content selection and generation strategies are\nlearnt across iterations. In this work, we analyze the training dynamics for\ngeneration models, focusing on news summarization. Across different datasets\n(CNN/DM, XSum, MediaSum) and summary properties, such as abstractiveness and\nhallucination, we study what the model learns at different stages of its\nfine-tuning process. We find that properties such as copy behavior are learnt\nearlier in the training process and these observations are robust across\ndomains. On the other hand, factual errors, such as hallucination of\nunsupported facts, are learnt in the later stages, and this behavior is more\nvaried across domains. Based on these observations, we explore complementary\napproaches for modifying training: first, disregarding high-loss tokens that\nare challenging to learn and second, disregarding low-loss tokens that are\nlearnt very quickly. This simple training modification allows us to configure\nour model to achieve different goals, such as improving factuality or improving\nabstractiveness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Goyal_T/0/1/0/all/0/1\">Tanya Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jiacheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Jessy Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrett_G/0/1/0/all/0/1\">Greg Durrett</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On The Ingredients of an Effective Zero-shot Semantic Parser. (arXiv:2110.08381v1 [cs.CL])","link":"http://arxiv.org/abs/2110.08381","description":"<p>Semantic parsers map natural language utterances into meaning representations\n(e.g., programs). Such models are typically bottlenecked by the paucity of\ntraining data due to the required laborious annotation efforts. Recent studies\nhave performed zero-shot learning by synthesizing training examples of\ncanonical utterances and programs from a grammar, and further paraphrasing\nthese utterances to improve linguistic diversity. However, such synthetic\nexamples cannot fully capture patterns in real data. In this paper we analyze\nzero-shot parsers through the lenses of the language and logical gaps (Herzig\nand Berant, 2019), which quantify the discrepancy of language and programmatic\npatterns between the canonical examples and real-world user-issued ones. We\npropose bridging these gaps using improved grammars, stronger paraphrasers, and\nefficient learning methods using canonical examples that most likely reflect\nreal user intents. Our model achieves strong performance on two semantic\nparsing benchmarks (Scholar, Geo) with zero labeled data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yin_P/0/1/0/all/0/1\">Pengcheng Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wieting_J/0/1/0/all/0/1\">John Wieting</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sil_A/0/1/0/all/0/1\">Avirup Sil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Training Conversational Agents with Generative Conversational Networks. (arXiv:2110.08383v1 [cs.CL])","link":"http://arxiv.org/abs/2110.08383","description":"<p>Rich, open-domain textual data available on the web resulted in great\nadvancements for language processing. However, while that data may be suitable\nfor language processing tasks, they are mostly non-conversational, lacking many\nphenomena that appear in human interactions and this is one of the reasons why\nwe still have many unsolved challenges in conversational AI. In this work, we\nattempt to address this by using Generative Conversational Networks to\nautomatically generate data and train social conversational agents. We evaluate\nour approach on TopicalChat with automatic metrics and human evaluators,\nshowing that with 10% of seed data it performs close to the baseline that uses\n100% of the data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yen-Ting Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papangelis_A/0/1/0/all/0/1\">Alexandros Papangelis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seokhwan Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hakkani_Tur_D/0/1/0/all/0/1\">Dilek Hakkani-Tur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generated Knowledge Prompting for Commonsense Reasoning. (arXiv:2110.08387v1 [cs.CL])","link":"http://arxiv.org/abs/2110.08387","description":"<p>Despite their ability to capture large amount of knowledge during\npretraining, large-scale language models often benefit from incorporating\nexternal knowledge bases, especially on commonsense reasoning tasks. This\nmotivates us to explore how we can best leverage knowledge elicited from\nlanguage models themselves. We propose generating knowledge statements directly\nfrom a language model with a generic prompt format, then selecting the\nknowledge which maximizes prediction probability. Despite its simplicity, this\napproach improves performance of both off-the-shelf and finetuned language\nmodels on four commonsense reasoning tasks, improving the state-of-the-art on\nnumerical commonsense (NumerSense), general commonsense (CommonsenseQA 2.0),\nand scientific commonsense (QASC) benchmarks. Notably, we find that a model's\npredictions can improve when using its own generated knowledge, demonstrating\nthe importance of symbolic knowledge representation in neural reasoning\nprocesses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiacheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Alisa Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Ximing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Welleck_S/0/1/0/all/0/1\">Sean Welleck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+West_P/0/1/0/all/0/1\">Peter West</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bras_R/0/1/0/all/0/1\">Ronan Le Bras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Probing as Quantifying the Inductive Bias of Pre-trained Representations. (arXiv:2110.08388v1 [cs.CL])","link":"http://arxiv.org/abs/2110.08388","description":"<p>Pre-trained contextual representations have led to dramatic performance\nimprovements on a range of downstream tasks. This has motivated researchers to\nquantify and understand the linguistic information encoded in them. In general,\nthis is done by probing, which consists of training a supervised model to\npredict a linguistic property from said representations. Unfortunately, this\ndefinition of probing has been subject to extensive criticism, and can lead to\nparadoxical or counter-intuitive results. In this work, we present a novel\nframework for probing where the goal is to evaluate the inductive bias of\nrepresentations for a particular task, and provide a practical avenue to do\nthis using Bayesian inference. We apply our framework to a series of token-,\narc-, and sentence-level tasks. Our results suggest that our framework solves\nproblems of previous approaches and that fastText can offer a better inductive\nbias than BERT in certain situations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Immer_A/0/1/0/all/0/1\">Alexander Immer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hennigen_L/0/1/0/all/0/1\">Lucas Torroba Hennigen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fortuin_V/0/1/0/all/0/1\">Vincent Fortuin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DS-TOD: Efficient Domain Specialization for Task Oriented Dialog. (arXiv:2110.08395v1 [cs.CL])","link":"http://arxiv.org/abs/2110.08395","description":"<p>Recent work has shown that self-supervised dialog-specific pretraining on\nlarge conversational datasets yields substantial gains over traditional\nlanguage modeling (LM) pretraining in downstream task-oriented dialog (TOD).\nThese approaches, however, exploit general dialogic corpora (e.g., Reddit) and\nthus presumably fail to reliably embed domain-specific knowledge useful for\nconcrete downstream TOD domains. In this work, we investigate the effects of\ndomain specialization of pretrained language models (PLMs) for task-oriented\ndialog. Within our DS-TOD framework, we first automatically extract salient\ndomain-specific terms, and then use them to construct DomainCC and DomainReddit\n-- resources that we leverage for domain-specific pretraining, based on (i)\nmasked language modeling (MLM) and (ii) response selection (RS) objectives,\nrespectively. We further propose a resource-efficient and modular domain\nspecialization by means of domain adapters -- additional parameter-light layers\nin which we encode the domain knowledge. Our experiments with two prominent TOD\ntasks -- dialog state tracking (DST) and response retrieval (RR) --\nencompassing five domains from the MultiWOZ TOD benchmark demonstrate the\neffectiveness of our domain specialization approach. Moreover, we show that the\nlight-weight adapter-based specialization (1) performs comparably to full\nfine-tuning in single-domain setups and (2) is particularly suitable for\nmulti-domain specialization, in which, besides advantageous computational\nfootprint, it can offer better downstream performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hung_C/0/1/0/all/0/1\">Chia-Chien Hung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lauscher_A/0/1/0/all/0/1\">Anne Lauscher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ponzetto_S/0/1/0/all/0/1\">Simone Paolo Ponzetto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glavas_G/0/1/0/all/0/1\">Goran Glava&#x161;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating the Faithfulness of Importance Measures in NLP by Recursively Masking Allegedly Important Tokens and Retraining. (arXiv:2110.08412v1 [cs.CL])","link":"http://arxiv.org/abs/2110.08412","description":"<p>To explain NLP models, many methods inform which inputs tokens are important\nfor a prediction. However, an open question is if these methods accurately\nreflect the model's logic, a property often called faithfulness. In this work,\nwe adapt and improve a recently proposed faithfulness benchmark from computer\nvision called ROAR (RemOve And Retrain), by Hooker et al. (2019).\n</p>\n<p>We improve ROAR by recursively removing dataset redundancies, which otherwise\ninterfere with ROAR. We adapt and apply ROAR, to popular NLP importance\nmeasures, namely attention, gradient, and integrated gradients. Additionally,\nwe use mutual information as an additional baseline. Evaluation is done on a\nsuite of classification tasks often used in the faithfulness of attention\nliterature. Finally, we propose a scalar faithfulness metric, which makes it\neasy to compare results across papers.\n</p>\n<p>We find that, importance measures considered to be unfaithful for computer\nvision tasks perform favorably for NLP tasks, the faithfulness of an importance\nmeasure is task-dependent, and the computational overhead of integrated\ngradient is rarely justified.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Madsen_A/0/1/0/all/0/1\">Andreas Madsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meade_N/0/1/0/all/0/1\">Nicholas Meade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adlakha_V/0/1/0/all/0/1\">Vaibhav Adlakha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1\">Siva Reddy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Invariant Language Modeling. (arXiv:2110.08413v1 [cs.CL])","link":"http://arxiv.org/abs/2110.08413","description":"<p>Modern pretrained language models are critical components of NLP pipelines.\nYet, they suffer from spurious correlations, poor out-of-domain generalization,\nand biases. Inspired by recent progress in causal machine learning, in\nparticular the invariant risk minimization (IRM) paradigm, we propose invariant\nlanguage modeling, a framework for learning invariant representations that\ngeneralize better across multiple environments. In particular, we adapt a\ngame-theoretic implementation of IRM (IRM-games) to language models, where the\ninvariance emerges from a specific training schedule in which all the\nenvironments compete to optimize their own environment-specific loss by\nupdating subsets of the model in a round-robin fashion. In a series of\ncontrolled experiments, we demonstrate the ability of our method to (i) remove\nstructured noise, (ii) ignore specific spurious correlations without affecting\nglobal performance, and (iii) achieve better out-of-domain generalization.\nThese benefits come with a negligible computational overhead compared to\nstandard training, do not require changing the local loss, and can be applied\nto any language model architecture. We believe this framework is promising to\nhelp mitigate spurious correlations and biases in language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peyrard_M/0/1/0/all/0/1\">Maxime Peyrard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghotra_S/0/1/0/all/0/1\">Sarvjeet Singh Ghotra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Josifoski_M/0/1/0/all/0/1\">Martin Josifoski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_V/0/1/0/all/0/1\">Vidhan Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patra_B/0/1/0/all/0/1\">Barun Patra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carignan_D/0/1/0/all/0/1\">Dean Carignan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiciman_E/0/1/0/all/0/1\">Emre Kiciman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+West_R/0/1/0/all/0/1\">Robert West</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual unsupervised sequence segmentation transfers to extremely low-resource languages. (arXiv:2110.08415v1 [cs.CL])","link":"http://arxiv.org/abs/2110.08415","description":"<p>We show that unsupervised sequence-segmentation performance can be\ntransferred to extremely low-resource languages by pre-training a Masked\nSegmental Language Model (Downey et al., 2021) multilingually. Further, we show\nthat this transfer can be achieved by training over a collection of\nlow-resource languages that are typologically similar (but phylogenetically\nunrelated) to the target language. In our experiments, we transfer from a\ncollection of 10 Indigenous American languages (AmericasNLP, Mager et al.,\n2021) to K'iche', a Mayan language. We compare our model to a monolingual\nbaseline, and show that the multilingual pre-trained approach yields much more\nconsistent segmentation quality across target dataset sizes, including a\nzero-shot performance of 20.6 F1, and exceeds the monolingual performance in\n9/10 experimental settings. These results have promising implications for\nlow-resource NLP pipelines involving human-like linguistic units, such as the\nsparse transcription framework proposed by Bird (2020).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Downey_C/0/1/0/all/0/1\">C.M. Downey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drizin_S/0/1/0/all/0/1\">Shannon Drizin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haroutunian_L/0/1/0/all/0/1\">Levon Haroutunian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thukral_S/0/1/0/all/0/1\">Shivin Thukral</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Open Domain Question Answering over Virtual Documents: A Unified Approach for Data and Text. (arXiv:2110.08417v1 [cs.CL])","link":"http://arxiv.org/abs/2110.08417","description":"<p>Due to its potential for a universal interface over both data and text,\ndata-to-text generation is becoming increasingly popular recently. However, few\nprevious work has focused on its application to downstream tasks, e.g. using\nthe converted data for grounding or reasoning. In this work, we aim to bridge\nthis gap and use the data-to-text method as a means for encoding structured\nknowledge for knowledge-intensive applications, i.e. open-domain question\nanswering (QA). Specifically, we propose a verbalizer-retriever-reader\nframework for open-domain QA over data and text where verbalized tables from\nWikipedia and triples from Wikidata are used as augmented knowledge sources. We\nshow that our Unified Data and Text QA, UDT-QA, can effectively benefit from\nthe expanded knowledge index, leading to large gains over text-only baselines.\nNotably, our approach sets the single-model state-of-the-art on Natural\nQuestions. Furthermore, our analyses indicate that verbalized knowledge is\npreferred for answer reasoning for both adapted and hot-swap settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Kaixin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Hao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaodong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nyberg_E/0/1/0/all/0/1\">Eric Nyberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What do Compressed Large Language Models Forget? Robustness Challenges in Model Compression. (arXiv:2110.08419v1 [cs.CL])","link":"http://arxiv.org/abs/2110.08419","description":"<p>Recent works have focused on compressing pre-trained language models (PLMs)\nlike BERT where the major focus has been to improve the compressed model\nperformance for downstream tasks. However, there has been no study in analyzing\nthe impact of compression on the generalizability and robustness of these\nmodels. Towards this end, we study two popular model compression techniques\nincluding knowledge distillation and pruning and show that compressed models\nare significantly less robust than their PLM counterparts on adversarial test\nsets although they obtain similar performance on in-distribution development\nsets for a task. Further analysis indicates that the compressed models overfit\non the easy samples and generalize poorly on the hard ones. We further leverage\nthis observation to develop a regularization strategy for model compression\nbased on sample uncertainty. Experimental results on several natural language\nunderstanding tasks demonstrate our mitigation framework to improve both the\nadversarial generalization as well as in-distribution task performance of the\ncompressed models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_M/0/1/0/all/0/1\">Mengnan Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_S/0/1/0/all/0/1\">Subhabrata Mukherjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yu Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shokouhi_M/0/1/0/all/0/1\">Milad Shokouhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xia Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1\">Ahmed Hassan Awadallah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Information-Theoretic Measures of Dataset Difficulty. (arXiv:2110.08420v1 [cs.CL])","link":"http://arxiv.org/abs/2110.08420","description":"<p>Estimating the difficulty of a dataset typically involves comparing\nstate-of-the-art models to humans; the bigger the performance gap, the harder\nthe dataset is said to be. Not only is this framework informal, but it also\nprovides little understanding of how difficult each instance is, or what\nattributes make it difficult for a given model. To address these problems, we\npropose an information-theoretic perspective, framing dataset difficulty as the\nabsence of $\\textit{usable information}$. Measuring usable information is as\neasy as measuring performance, but has certain theoretical advantages. While\nthe latter only allows us to compare different models w.r.t the same dataset,\nthe former also allows us to compare different datasets w.r.t the same model.\nWe then introduce $\\textit{pointwise}$ $\\mathcal{V}-$$\\textit{information}$\n(PVI) for measuring the difficulty of individual instances, where instances\nwith higher PVI are easier for model $\\mathcal{V}$. By manipulating the input\nbefore measuring usable information, we can understand $\\textit{why}$ a dataset\nis easy or difficult for a given model, which we use to discover annotation\nartefacts in widely-used benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ethayarajh_K/0/1/0/all/0/1\">Kawin Ethayarajh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swayamdipta_S/0/1/0/all/0/1\">Swabha Swayamdipta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EncT5: Fine-tuning T5 Encoder for Non-autoregressive Tasks. (arXiv:2110.08426v1 [cs.CL])","link":"http://arxiv.org/abs/2110.08426","description":"<p>Encoder-decoder transformer architectures have become popular recently with\nthe advent of T5 models. It is also more favorable over architectures like BERT\nfor pre-training on language model task when it comes to large scale models\nwhich could take months to train given it's generality. While being able to\ngeneralize to more tasks, it is not evident if the proposed encoder-decoder\narchitecture is the most efficient for fine-tuning on classification and\nregression tasks given the pre-trained model. In this work, we study\nfine-tuning pre-trained encoder-decoder models such as T5. Particularly, we\npropose \\textbf{EncT5} as a way to efficiently fine-tune pre-trained\nencoder-decoder T5 models for classification and regression tasks by using the\nencoder layers. Our experimental results show that \\textbf{EncT5} with less\nthan half of the parameters of T5 performs similarly to T5 models on GLUE\nbenchmark. We believe our proposed approach can be easily applied to any\npre-trained encoder-decoder model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Frederick Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shakeri_S/0/1/0/all/0/1\">Siamak Shakeri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hongkun Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jing Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Metadata Shaping: Natural Language Annotations for the Tail. (arXiv:2110.08430v1 [cs.CL])","link":"http://arxiv.org/abs/2110.08430","description":"<p>Language models (LMs) have made remarkable progress, but still struggle to\ngeneralize beyond the training data to rare linguistic patterns. Since rare\nentities and facts are prevalent in the queries users submit to popular\napplications such as search and personal assistant systems, improving the\nability of LMs to reliably capture knowledge over rare entities is a pressing\nchallenge studied in significant prior work. Noticing that existing approaches\nprimarily modify the LM architecture or introduce auxiliary objectives to\ninject useful entity knowledge, we ask to what extent we could match the\nquality of these architectures using a base LM architecture, and only changing\nthe data? We propose metadata shaping, a method in which readily available\nmetadata, such as entity descriptions and categorical tags, are appended to\nexamples based on information theoretic metrics. Intuitively, if metadata\ncorresponding to popular entities overlap with metadata for rare entities, the\nLM may be able to better reason about the rare entities using patterns learned\nfrom similar popular entities. On standard entity-rich tasks (TACRED, FewRel,\nOpenEntity), with no changes to the LM whatsoever, metadata shaping exceeds the\nBERT-baseline by up to 5.3 F1 points, and achieves or competes with\nstate-of-the-art results. We further show the improvements are up to 10x larger\non examples containing tail versus popular entities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arora_S/0/1/0/all/0/1\">Simran Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Sen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_E/0/1/0/all/0/1\">Enci Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Re_C/0/1/0/all/0/1\">Christopher Re</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Natural Language Inference Using PHL Triplet Generation. (arXiv:2110.08438v1 [cs.CL])","link":"http://arxiv.org/abs/2110.08438","description":"<p>Transformer-based models have achieved impressive performance on various\nNatural Language Inference (NLI) benchmarks, when trained on respective\ntraining datasets. However, in certain cases, training samples may not be\navailable or collecting them could be time-consuming and resource-intensive. In\nthis work, we address this challenge and present an explorative study on\nunsupervised NLI, a paradigm in which no human-annotated training samples are\navailable. We investigate NLI under three challenging settings: PH, P, and NPH\nthat differ in the extent of unlabeled data available for learning. As a\nsolution, we propose a procedural data generation approach that leverages a set\nof sentence transformations to collect PHL (Premise, Hypothesis, Label)\ntriplets for training NLI models, bypassing the need for human-annotated\ntraining datasets. Comprehensive experiments show that this approach results in\naccuracies of 66.75%, 65.9%, 65.39% in PH, P, NPH settings respectively,\noutperforming all existing baselines. Furthermore, fine-tuning our models with\nas little as ~0.1% of the training dataset (500 samples) leads to 12.2% higher\naccuracy than the model trained from scratch on the same 500 instances.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Varshney_N/0/1/0/all/0/1\">Neeraj Varshney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_P/0/1/0/all/0/1\">Pratyay Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gokhale_T/0/1/0/all/0/1\">Tejas Gokhale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prix-LM: Pretraining for Multilingual Knowledge Base Construction. (arXiv:2110.08443v1 [cs.CL])","link":"http://arxiv.org/abs/2110.08443","description":"<p>Knowledge bases (KBs) contain plenty of structured world and commonsense\nknowledge. As such, they often complement distributional text-based information\nand facilitate various downstream tasks. Since their manual construction is\nresource- and time-intensive, recent efforts have tried leveraging large\npretrained language models (PLMs) to generate additional monolingual knowledge\nfacts for KBs. However, such methods have not been attempted for building and\nenriching multilingual KBs. Besides wider application, such multilingual KBs\ncan provide richer combined knowledge than monolingual (e.g., English) KBs.\nKnowledge expressed in different languages may be complementary and unequally\ndistributed: this implies that the knowledge available in high-resource\nlanguages can be transferred to low-resource ones. To achieve this, it is\ncrucial to represent multilingual knowledge in a shared/unified space. To this\nend, we propose a unified framework, Prix-LM, for multilingual KB construction\nand completion. We leverage two types of knowledge, monolingual triples and\ncross-lingual links, extracted from existing multilingual KBs, and tune a\nmultilingual language encoder XLM-R via a causal language modeling objective.\nPrix-LM integrates useful multilingual and KB-based factual knowledge into a\nsingle model. Experiments on standard entity-related tasks, such as link\nprediction in multiple languages, cross-lingual entity linking and bilingual\nlexicon induction, demonstrate its effectiveness, with gains reported over\nstrong task-specialised baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wenxuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fangyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vulic_I/0/1/0/all/0/1\">Ivan Vuli&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collier_N/0/1/0/all/0/1\">Nigel Collier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Muhao Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Well Do You Know Your Audience? Reader-aware Question Generation. (arXiv:2110.08445v1 [cs.CL])","link":"http://arxiv.org/abs/2110.08445","description":"<p>When writing, a person may need to anticipate questions from their readers,\nbut different types of readers may ask very different types of questions. If\nsomeone is writing for advice about a problem, what question will a domain\nexpert ask, and is this different from how a novice might react? In this paper,\nwe address the task of reader-aware question generation. We collect a new data\nset of questions and posts from social media, augmented with background\ninformation about the post readers. Based on predictive analysis and\ndescriptive differences, we find that different readers, such as experts and\nnovices, consistently ask different types of questions. We next develop several\ntext generation models that incorporate different types of reader background,\nincluding discrete and continuous reader representations based on the readers'\nprior behavior. We demonstrate that reader-aware models can perform on par or\nslightly better than the text-only model in some cases, particularly in cases\nwhere a post attracts very different questions from readers of different\ngroups. Our work has the potential to help writers anticipate the information\nneeds of different readers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stewart_I/0/1/0/all/0/1\">Ian Stewart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1\">Rada Mihalcea</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Good Examples Make A Faster Learner: Simple Demonstration-based Learning for Low-resource NER. (arXiv:2110.08454v1 [cs.CL])","link":"http://arxiv.org/abs/2110.08454","description":"<p>Recent advances in prompt-based learning have shown impressive results on\nfew-shot text classification tasks by using cloze-style language prompts. There\nhave been attempts on prompt-based learning for NER which use manually designed\ntemplates to predict entity types. However, these two-step methods may suffer\nfrom error propagation (from entity span detection), need to prompt for all\npossible text spans which is costly, and neglect the interdependency when\npredicting labels for different spans in a sentence. In this paper, we present\na simple demonstration-based learning method for NER, which augments the prompt\n(learning context) with a few task demonstrations. Such demonstrations help the\nmodel learn the task better under low-resource settings and allow for span\ndetection and classification over all tokens jointly. Here, we explore\nentity-oriented demonstration which selects an appropriate entity example per\neach entity type, and instance-oriented demonstration which retrieves a similar\ninstance example. Through extensive experiments, we find empirically that\nshowing entity example per each entity type, along with its example sentence,\ncan improve the performance both in in-domain and cross-domain settings by 1-3\nF1 score.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dong-Ho Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_M/0/1/0/all/0/1\">Mahak Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadakia_A/0/1/0/all/0/1\">Akshen Kadakia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pujara_J/0/1/0/all/0/1\">Jay Pujara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Enhanced Pretrained Language Models: A Compreshensive Survey. (arXiv:2110.08455v1 [cs.CL])","link":"http://arxiv.org/abs/2110.08455","description":"<p>Pretrained Language Models (PLM) have established a new paradigm through\nlearning informative contextualized representations on large-scale text corpus.\nThis new paradigm has revolutionized the entire field of natural language\nprocessing, and set the new state-of-the-art performance for a wide variety of\nNLP tasks. However, though PLMs could store certain knowledge/facts from\ntraining corpus, their knowledge awareness is still far from satisfactory. To\naddress this issue, integrating knowledge into PLMs have recently become a very\nactive research area and a variety of approaches have been developed. In this\npaper, we provide a comprehensive survey of the literature on this emerging and\nfast-growing field - Knowledge Enhanced Pretrained Language Models (KE-PLMs).\nWe introduce three taxonomies to categorize existing work. Besides, we also\nsurvey the various NLU and NLG applications on which KE-PLM has demonstrated\nsuperior performance over vanilla PLMs. Finally, we discuss challenges that\nface KE-PLMs and also promising directions for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xiaokai Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dejiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatia_P/0/1/0/all/0/1\">Parminder Bhatia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arnold_A/0/1/0/all/0/1\">Andrew Arnold</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Controllable Semantic Parsing via Retrieval Augmentation. (arXiv:2110.08458v1 [cs.CL])","link":"http://arxiv.org/abs/2110.08458","description":"<p>In practical applications of semantic parsing, we often want to rapidly\nchange the behavior of the parser, such as enabling it to handle queries in a\nnew domain, or changing its predictions on certain targeted queries. While we\ncan introduce new training examples exhibiting the target behavior, a mechanism\nfor enacting such behavior changes without expensive model re-training would be\npreferable. To this end, we propose ControllAble Semantic Parser via Exemplar\nRetrieval (CASPER). Given an input query, the parser retrieves related\nexemplars from a retrieval index, augments them to the query, and then applies\na generative seq2seq model to produce an output parse. The exemplars act as a\ncontrol mechanism over the generic generative model: by manipulating the\nretrieval index or how the augmented query is constructed, we can manipulate\nthe behavior of the parser. On the MTOP dataset, in addition to achieving\nstate-of-the-art on the standard setup, we show that CASPER can parse queries\nin a new domain, adapt the prediction toward the specified patterns, or adapt\nto new semantic schemas without having to further re-train the model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pasupat_P/0/1/0/all/0/1\">Panupong Pasupat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guu_K/0/1/0/all/0/1\">Kelvin Guu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Short Study on Compressing Decoder-Based Language Models. (arXiv:2110.08460v1 [cs.CL])","link":"http://arxiv.org/abs/2110.08460","description":"<p>Pre-trained Language Models (PLMs) have been successful for a wide range of\nnatural language processing (NLP) tasks. The state-of-the-art of PLMs, however,\nare extremely large to be used on edge devices. As a result, the topic of model\ncompression has attracted increasing attention in the NLP community. Most of\nthe existing works focus on compressing encoder-based models (tiny-BERT,\ndistilBERT, distilRoBERTa, etc), however, to the best of our knowledge, the\ncompression of decoder-based models (such as GPT-2) has not been investigated\nmuch. Our paper aims to fill this gap. Specifically, we explore two directions:\n1) we employ current state-of-the-art knowledge distillation techniques to\nimprove fine-tuning of DistilGPT-2. 2) we pre-train a compressed GPT-2 model\nusing layer truncation and compare it against the distillation-based method\n(DistilGPT2). The training time of our compressed model is significantly less\nthan DistilGPT-2, but it can achieve better performance when fine-tuned on\ndownstream tasks. We also demonstrate the impact of data cleaning on model\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tianda Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mesbahi_Y/0/1/0/all/0/1\">Yassir El Mesbahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kobyzev_I/0/1/0/all/0/1\">Ivan Kobyzev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rashid_A/0/1/0/all/0/1\">Ahmad Rashid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahmud_A/0/1/0/all/0/1\">Atif Mahmud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anchuri_N/0/1/0/all/0/1\">Nithin Anchuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajimolahoseini_H/0/1/0/all/0/1\">Habib Hajimolahoseini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rezagholizadeh_M/0/1/0/all/0/1\">Mehdi Rezagholizadeh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Knowledge in Multilingual Commonsense Reasoning. (arXiv:2110.08462v1 [cs.CL])","link":"http://arxiv.org/abs/2110.08462","description":"<p>Commonsense reasoning (CSR) requires the model to be equipped with general\nworld knowledge. While CSR is a language-agnostic process, most comprehensive\nknowledge sources are in few popular languages, especially English. Thus, it\nremains unclear how to effectively conduct multilingual commonsense reasoning\n(XCSR) for various languages. In this work, we propose to utilize English\nknowledge sources via a translate-retrieve-translate (TRT) strategy. For\nmultilingual commonsense questions and choices, we collect related knowledge\nvia translation and retrieval from the knowledge sources. The retrieved\nknowledge is then translated into the target language and integrated into a\npre-trained multilingual language model via visible knowledge attention. Then\nwe utilize a diverse of 4 English knowledge sources to provide more\ncomprehensive coverage of knowledge in different formats. Extensive results on\nthe XCSR benchmark demonstrate that TRT with external knowledge can\nsignificantly improve multilingual commonsense reasoning in both zero-shot and\ntranslate-train settings, outperforming 3.3 and 3.6 points over the previous\nstate-of-the-art on XCSR benchmark datasets (X-CSQA and X-CODAH).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yuwei Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuohang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yichong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ruochen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1\">Siqi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1\">Michael Zeng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Seeking Patterns, Not just Memorizing Procedures: Contrastive Learning for Solving Math Word Problems. (arXiv:2110.08464v1 [cs.CL])","link":"http://arxiv.org/abs/2110.08464","description":"<p>Math Word Problem (MWP) solving needs to discover the quantitative\nrelationships over natural language narratives. Recent work shows that existing\nmodels memorize procedures from context and rely on shallow heuristics to solve\nMWPs. In this paper, we look at this issue and argue that the cause is a lack\nof overall understanding of MWP patterns. We first investigate how a neural\nnetwork understands patterns only from semantics, and observe that, if the\nprototype equations are the same, most problems get closer representations and\nthose representations apart from them or close to other prototypes tend to\nproduce wrong solutions. Inspired by it, we propose a contrastive learning\napproach, where the neural network perceives the divergence of patterns. We\ncollect contrastive examples by converting the prototype equation into a tree\nand seeking similar tree structures. The solving model is trained with an\nauxiliary objective on the collected examples, resulting in the representations\nof problems with similar prototypes being pulled closer. We conduct experiments\non the Chinese dataset Math23k and the English dataset MathQA. Our method\ngreatly improves the performance in monolingual and multilingual settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhongli Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenxuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_C/0/1/0/all/0/1\">Chao Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qingyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hongzhi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yunbo Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Safety of Conversational Models: Taxonomy, Dataset, and Benchmark. (arXiv:2110.08466v1 [cs.CL])","link":"http://arxiv.org/abs/2110.08466","description":"<p>Dialogue safety problems severely limit the real-world deployment of neural\nconversational models and attract great research interests recently. We propose\na taxonomy for dialogue safety specifically designed to capture unsafe\nbehaviors that are unique in human-bot dialogue setting, with focuses on\ncontext-sensitive unsafety, which is under-explored in prior works. To spur\nresearch in this direction, we compile DiaSafety, a dataset of 6 unsafe\ncategories with rich context-sensitive unsafe examples. Experiments show that\nexisting utterance-level safety guarding tools fail catastrophically on our\ndataset. As a remedy, we train a context-level dialogue safety classifier to\nprovide a strong baseline for context-sensitive dialogue unsafety detection.\nWith our classifier, we perform safety evaluations on popular conversational\nmodels and show that existing dialogue systems are still stuck in\ncontext-sensitive safety problems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Hao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guangxuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jiawen Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1\">Jiale Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Chujie Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaoyan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Compositional Generalization with Self-Training for Data-to-Text Generation. (arXiv:2110.08467v1 [cs.CL])","link":"http://arxiv.org/abs/2110.08467","description":"<p>Data-to-text generation focuses on generating fluent natural language\nresponses from structured semantic representations. Such representations are\ncompositional, allowing for the combination of atomic meaning schemata in\nvarious ways to express the rich semantics in natural language. Recently,\npretrained language models (LMs) have achieved impressive results on\ndata-to-text tasks, though it remains unclear the extent to which these LMs\ngeneralize to new semantic representations. In this work, we systematically\nstudy the compositional generalization of current state-of-the-art generation\nmodels in data-to-text tasks. By simulating structural shifts in the\ncompositional Weather dataset, we show that T5 models fail to generalize to\nunseen structures. Next, we show that template-based input representations\ngreatly improve the model performance and model scale does not trivially solve\nthe lack of generalization. To further improve the model's performance, we\npropose an approach based on self-training using finetuned BLEURT for\npseudo-response selection. Extensive experiments on the few-shot Weather and\nmulti-domain SGD datasets demonstrate strong gains of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mehta_S/0/1/0/all/0/1\">Sanket Vaibhav Mehta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_J/0/1/0/all/0/1\">Jinfeng Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kale_M/0/1/0/all/0/1\">Mihir Kale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parikh_A/0/1/0/all/0/1\">Ankur Parikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_H/0/1/0/all/0/1\">Hongtao Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strubell_E/0/1/0/all/0/1\">Emma Strubell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Case-based Reasoning for Better Generalization in Text-Adventure Games. (arXiv:2110.08470v1 [cs.CL])","link":"http://arxiv.org/abs/2110.08470","description":"<p>Text-based games (TBG) have emerged as promising environments for driving\nresearch in grounded language understanding and studying problems like\ngeneralization and sample efficiency. Several deep reinforcement learning (RL)\nmethods with varying architectures and learning schemes have been proposed for\nTBGs. However, these methods fail to generalize efficiently, especially under\ndistributional shifts. In a departure from deep RL approaches, in this paper,\nwe propose a general method inspired by case-based reasoning to train agents\nand generalize out of the training distribution. The case-based reasoner\ncollects instances of positive experiences from the agent's interaction with\nthe world in the past and later reuses the collected experiences to act\nefficiently. The method can be applied in conjunction with any existing\non-policy neural agent in the literature for TBGs. Our experiments show that\nthe proposed approach consistently improves existing methods, obtains good\nout-of-distribution generalization, and achieves new state-of-the-art results\non widely used environments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Atzeni_M/0/1/0/all/0/1\">Mattia Atzeni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhuliawala_S/0/1/0/all/0/1\">Shehzaad Dhuliawala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murugesan_K/0/1/0/all/0/1\">Keerthiram Murugesan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Good Prompt Is Worth Millions of Parameters? Low-resource Prompt-based Learning for Vision-Language Models. (arXiv:2110.08484v1 [cs.CV])","link":"http://arxiv.org/abs/2110.08484","description":"<p>Large pretrained vision-language (VL) models can learn a new task with a\nhandful of examples or generalize to a new task without fine-tuning. However,\nthese gigantic VL models are hard to deploy for real-world applications due to\ntheir impractically huge model size and slow inference speed. In this work, we\npropose FewVLM, a few-shot prompt-based learner on vision-language tasks. We\npretrain a sequence-to-sequence Transformer model with both prefix language\nmodeling (PrefixLM) and masked language modeling (MaskedLM), and introduce\nsimple prompts to improve zero-shot and few-shot performance on VQA and image\ncaptioning. Experimental results on five VQA and captioning datasets show that\n\\method\\xspace outperforms Frozen which is 31 times larger than ours by 18.2%\npoint on zero-shot VQAv2 and achieves comparable results to a 246$\\times$\nlarger model, PICa. We observe that (1) prompts significantly affect zero-shot\nperformance but marginally affect few-shot performance, (2) MaskedLM helps\nfew-shot VQA tasks while PrefixLM boosts captioning performance, and (3)\nperformance significantly increases when training set size is small.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_W/0/1/0/all/0/1\">Woojeong Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yu Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yelong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Procedural Knowledge by Sequencing Multimodal Instructional Manuals. (arXiv:2110.08486v1 [cs.CL])","link":"http://arxiv.org/abs/2110.08486","description":"<p>The ability to sequence unordered events is an essential skill to comprehend\nand reason about real world task procedures, which often requires thorough\nunderstanding of temporal common sense and multimodal information, as these\nprocedures are often communicated through a combination of texts and images.\nSuch capability is essential for applications such as sequential task planning\nand multi-source instruction summarization. While humans are capable of\nreasoning about and sequencing unordered multimodal procedural instructions,\nwhether current machine learning models have such essential capability is still\nan open question. In this work, we benchmark models' capability of reasoning\nover and sequencing unordered multimodal instructions by curating datasets from\npopular online instructional manuals and collecting comprehensive human\nannotations. We find models not only perform significantly worse than humans\nbut also seem incapable of efficiently utilizing the multimodal information. To\nimprove machines' performance on multimodal event sequencing, we propose\nsequentiality-aware pretraining techniques that exploit the sequential\nalignment properties of both texts and images, resulting in &gt; 5% significant\nimprovements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Te-Lin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spangher_A/0/1/0/all/0/1\">Alex Spangher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alipoormolabashi_P/0/1/0/all/0/1\">Pegah Alipoormolabashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freedman_M/0/1/0/all/0/1\">Marjorie Freedman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weischedel_R/0/1/0/all/0/1\">Ralph Weischedel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PRIMER: Pyramid-based Masked Sentence Pre-training for Multi-document Summarization. (arXiv:2110.08499v1 [cs.CL])","link":"http://arxiv.org/abs/2110.08499","description":"<p>Recently proposed pre-trained generation models achieve strong performance on\nsingle-document summarization benchmarks. However, most of them are pre-trained\nwith general-purpose objectives and mainly aim to process single document\ninputs. In this paper, we propose PRIMER, a pre-trained model for\nmulti-document representation with focus on summarization that reduces the need\nfor dataset-specific architectures and large amounts of fine-tuning labeled\ndata. Specifically, we adopt the Longformer architecture with proper input\ntransformation and global attention to fit for multi-document inputs, and we\nuse Gap Sentence Generation objective with a new strategy to select salient\nsentences for the whole cluster, called Entity Pyramid, to teach the model to\nselect and aggregate information across a cluster of related documents. With\nextensive experiments on 6 multi-document summarization datasets from 3\ndifferent domains on the zero-shot, few-shot, and full-supervised settings, our\nmodel, PRIMER, outperforms current state-of-the-art models on most of these\nsettings with large margins. Code and pre-trained models are released at\nhttps://github.com/allenai/PRIMER\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_W/0/1/0/all/0/1\">Wen Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beltagy_I/0/1/0/all/0/1\">Iz Beltagy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carenini_G/0/1/0/all/0/1\">Giuseppe Carenini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohan_A/0/1/0/all/0/1\">Arman Cohan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Think Before You Speak: Using Self-talk to Generate Implicit Commonsense Knowledge for Response Generation. (arXiv:2110.08501v1 [cs.CL])","link":"http://arxiv.org/abs/2110.08501","description":"<p>Implicit knowledge, such as common sense, is key to fluid human\nconversations. Current neural response generation (RG) models are trained\nend-to-end, omitting unstated implicit knowledge. In this paper, we present a\nself-talk approach that first generates the implicit commonsense knowledge and\nthen generates response by referencing the externalized knowledge, all using\none generative model. We analyze different choices to collect knowledge-aligned\ndialogues, represent implicit knowledge, and elicit knowledge and responses. We\nintroduce three evaluation aspects: knowledge quality, knowledge-response\nconnection, and response quality and perform extensive human evaluations. Our\nexperimental results show that compared with end-to-end RG models, self-talk\nmodels that externalize the knowledge grounding process by explicitly\ngenerating implicit knowledge also produce responses that are more informative,\nspecific, and follow common sense. We also find via human evaluation that\nself-talk models generate high-quality knowledge around 75% of the time. We\nhope that our findings encourage further work on different approaches to\nmodeling implicit commonsense knowledge and training knowledgeable RG models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gopalakrishnan_K/0/1/0/all/0/1\">Karthik Gopalakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hedayatnia_B/0/1/0/all/0/1\">Behnam Hedayatnia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seokhwan Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pujara_J/0/1/0/all/0/1\">Jay Pujara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hakkani_Tur_D/0/1/0/all/0/1\">Dilek Hakkani-Tur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analyzing Dynamic Adversarial Training Data in the Limit. (arXiv:2110.08514v1 [cs.CL])","link":"http://arxiv.org/abs/2110.08514","description":"<p>To create models that are robust across a wide range of test inputs, training\ndatasets should include diverse examples that span numerous phenomena. Dynamic\nadversarial data collection (DADC), where annotators craft examples that\nchallenge continually improving models, holds promise as an approach for\ngenerating such diverse training sets. Prior work has shown that running DADC\nover 1-3 rounds can help models fix some error types, but it does not\nnecessarily lead to better generalization beyond adversarial test data. We\nargue that running DADC over many rounds maximizes its training-time benefits,\nas the different rounds can together cover many of the task-relevant phenomena.\nWe present the first study of longer-term DADC, where we collect 20 rounds of\nNLI examples for a small set of premise paragraphs, with both adversarial and\nnon-adversarial approaches. Models trained on DADC examples make 26% fewer\nerrors on our expert-curated test set compared to models trained on\nnon-adversarial data. Our analysis shows that DADC yields examples that are\nmore difficult, more lexically and syntactically diverse, and contain fewer\nannotation artifacts compared to non-adversarial examples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wallace_E/0/1/0/all/0/1\">Eric Wallace</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_A/0/1/0/all/0/1\">Adina Williams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1\">Robin Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiela_D/0/1/0/all/0/1\">Douwe Kiela</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Dialogue Response Generation. (arXiv:2110.08515v1 [cs.CL])","link":"http://arxiv.org/abs/2110.08515","description":"<p>Responsing with image has been recognized as an important capability for an\nintelligent conversational agent. Yet existing works only focus on exploring\nthe multimodal dialogue models which depend on retrieval-based methods, but\nneglecting generation methods. To fill in the gaps, we first present a\nmultimodal dialogue generation model, which takes the dialogue history as\ninput, then generates a textual sequence or an image as response. Learning such\na model often requires multimodal dialogues containing both texts and images\nwhich are difficult to obtain. Motivated by the challenge in practice, we\nconsider multimodal dialogue generation under a natural assumption that only\nlimited training examples are available. In such a low-resource setting, we\ndevise a novel conversational agent, Divter, in order to isolate parameters\nthat depend on multimodal dialogues from the entire generation model. By this\nmeans, the major part of the model can be learned from a large number of\ntext-only dialogues and text-image pairs respectively, then the whole\nparameters can be well fitted using the limited training examples. Extensive\nexperiments demonstrate our method achieves state-of-the-art results in both\nautomatic and human evaluation, and can generate informative text and\nhigh-resolution image responses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1\">Qingfeng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yujing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Can Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_K/0/1/0/all/0/1\">Kai Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yaming Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Huang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1\">Fei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jessica Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1\">Xiubo Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MarkupLM: Pre-training of Text and Markup Language for Visually-rich Document Understanding. (arXiv:2110.08518v1 [cs.CL])","link":"http://arxiv.org/abs/2110.08518","description":"<p>Multimodal pre-training with text, layout, and image has made significant\nprogress for Visually-rich Document Understanding (VrDU), especially the\nfixed-layout documents such as scanned document images. While, there are still\na large number of digital documents where the layout information is not fixed\nand needs to be interactively and dynamically rendered for visualization,\nmaking existing layout-based pre-training approaches not easy to apply. In this\npaper, we propose MarkupLM for document understanding tasks with markup\nlanguages as the backbone such as HTML/XML-based documents, where text and\nmarkup information is jointly pre-trained. Experiment results show that the\npre-trained MarkupLM significantly outperforms the existing strong baseline\nmodels on several document understanding tasks. The pre-trained model and code\nwill be publicly available at https://aka.ms/markuplm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junlong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yiheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1\">Lei Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Dataset for Discourse Structure in Peer Review Discussions. (arXiv:2110.08520v1 [cs.CL])","link":"http://arxiv.org/abs/2110.08520","description":"<p>At the foundation of scientific evaluation is the labor-intensive process of\npeer review. This critical task requires participants to consume and interpret\nvast amounts of highly technical text. We show that discourse cues from\nrebuttals can shed light on the quality and interpretation of reviews. Further,\nan understanding of the argumentative strategies employed by the reviewers and\nauthors provides useful signal for area chairs and other decision makers.\n</p>\n<p>This paper presents a new labeled dataset of 20k sentences contained in 506\nreview-rebuttal pairs in English, annotated by experts. While existing datasets\nannotate a subset of review sentences using various schemes, ours synthesizes\nexisting label sets and extends them to include fine-grained annotation of the\nrebuttal sentences, characterizing the authors' stance towards the reviewers'\ncriticisms and their commitment to addressing them. Further, we annotate\n\\textit{every} sentence in both the review and the rebuttal, including a\ndescription of the context for each rebuttal sentence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kennard_N/0/1/0/all/0/1\">Neha Nayak Kennard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OGorman_T/0/1/0/all/0/1\">Tim O&#x27;Gorman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Akshay Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bagchi_C/0/1/0/all/0/1\">Chhandak Bagchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clinton_M/0/1/0/all/0/1\">Matthew Clinton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yelugam_P/0/1/0/all/0/1\">Pranay Kumar Yelugam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_R/0/1/0/all/0/1\">Rajarshi Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zamani_H/0/1/0/all/0/1\">Hamed Zamani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCallum_A/0/1/0/all/0/1\">Andrew McCallum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Power of Prompt Tuning for Low-Resource Semantic Parsing. (arXiv:2110.08525v1 [cs.CL])","link":"http://arxiv.org/abs/2110.08525","description":"<p>Prompt tuning has recently emerged as an effective method for adapting\npre-trained language models to a number of language tasks. In this paper, we\ninvestigate prompt tuning for semantic parsing, the task of mapping natural\nlanguage utterances onto formal meaning representations. For large T5 models we\nfind (i) that prompt tuning significantly outperforms fine-tuning in the low\ndata regime and (ii) that canonicalization -- i.e. naturalizing the meaning\nrepresentations -- barely improves performance. This last result is surprising\nas it suggests that large T5 models can be modulated to generate sequences that\nare far from the pre-training distribution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schucher_N/0/1/0/all/0/1\">Nathan Schucher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1\">Siva Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vries_H/0/1/0/all/0/1\">Harm de Vries</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Empirical Survey of the Effectiveness of Debiasing Techniques for Pre-Trained Language Models. (arXiv:2110.08527v1 [cs.CL])","link":"http://arxiv.org/abs/2110.08527","description":"<p>Recent work has shown that pre-trained language models capture social biases\nfrom the text corpora they are trained on. This has attracted attention to\ndeveloping techniques that mitigate such biases. In this work, we perform a\nempirical survey of five recently proposed debiasing techniques: Counterfactual\nData Augmentation (CDA), Dropout, Iterative Nullspace Projection, Self-Debias,\nand SentenceDebias. We quantify the effectiveness of each technique using three\ndifferent bias benchmarks while also measuring the impact of these techniques\non a model's language modeling ability, as well as its performance on\ndownstream NLU tasks. We experimentally find that: (1) CDA and Self-Debias are\nthe strongest of the debiasing techniques, obtaining improved scores on most of\nthe bias benchmarks (2) Current debiasing techniques do not generalize well\nbeyond gender bias; And (3) improvements on bias benchmarks such as StereoSet\nand CrowS-Pairs by using debiasing strategies are usually accompanied by a\ndecrease in language modeling ability, making it difficult to determine whether\nthe bias mitigation is effective.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meade_N/0/1/0/all/0/1\">Nicholas Meade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poole_Dayan_E/0/1/0/all/0/1\">Elinor Poole-Dayan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1\">Siva Reddy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sharpness-Aware Minimization Improves Language Model Generalization. (arXiv:2110.08529v1 [cs.CL])","link":"http://arxiv.org/abs/2110.08529","description":"<p>The allure of superhuman-level capabilities has led to considerable interest\nin language models like GPT-3 and T5, wherein the research has, by and large,\nrevolved around new model architectures, training tasks, and loss objectives,\nalong with substantial engineering efforts to scale up model capacity and\ndataset size. Comparatively little work has been done to improve the\ngeneralization of these models through better optimization. In this work, we\nshow that Sharpness-Aware Minimization (SAM), a recently proposed optimization\nprocedure that encourages convergence to flatter minima, can substantially\nimprove the generalization of language models without much computational\noverhead. We show that SAM is able to boost performance on SuperGLUE, GLUE, Web\nQuestions, Natural Questions, Trivia QA, and TyDiQA, with particularly large\ngains when training data for these tasks is limited.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bahri_D/0/1/0/all/0/1\">Dara Bahri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mobahi_H/0/1/0/all/0/1\">Hossein Mobahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pro-KD: Progressive Distillation by Following the Footsteps of the Teacher. (arXiv:2110.08532v1 [cs.CL])","link":"http://arxiv.org/abs/2110.08532","description":"<p>With ever growing scale of neural models, knowledge distillation (KD)\nattracts more attention as a prominent tool for neural model compression.\nHowever, there are counter intuitive observations in the literature showing\nsome challenging limitations of KD. A case in point is that the best performing\ncheckpoint of the teacher might not necessarily be the best teacher for\ntraining the student in KD. Therefore, one important question would be how to\nfind the best checkpoint of the teacher for distillation? Searching through the\ncheckpoints of the teacher would be a very tedious and computationally\nexpensive process, which we refer to as the \\textit{checkpoint-search problem}.\nMoreover, another observation is that larger teachers might not necessarily be\nbetter teachers in KD which is referred to as the \\textit{capacity-gap}\nproblem. To address these challenging problems, in this work, we introduce our\nprogressive knowledge distillation (Pro-KD) technique which defines a smoother\ntraining path for the student by following the training footprints of the\nteacher instead of solely relying on distilling from a single mature\nfully-trained teacher. We demonstrate that our technique is quite effective in\nmitigating the capacity-gap problem and the checkpoint search problem. We\nevaluate our technique using a comprehensive set of experiments on different\ntasks such as image classification (CIFAR-10 and CIFAR-100), natural language\nunderstanding tasks of the GLUE benchmark, and question answering (SQuAD 1.1\nand 2.0) using BERT-based models and consistently got superior results over\nstate-of-the-art techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rezagholizadeh_M/0/1/0/all/0/1\">Mehdi Rezagholizadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jafari_A/0/1/0/all/0/1\">Aref Jafari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salad_P/0/1/0/all/0/1\">Puneeth Salad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_P/0/1/0/all/0/1\">Pranav Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasand_A/0/1/0/all/0/1\">Ali Saheb Pasand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghodsi_A/0/1/0/all/0/1\">Ali Ghodsi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lifelong Pretraining: Continually Adapting Language Models to Emerging Corpora. (arXiv:2110.08534v1 [cs.CL])","link":"http://arxiv.org/abs/2110.08534","description":"<p>Pretrained language models (PTLMs) are typically learned over a large, static\ncorpus and further fine-tuned for various downstream tasks. However, when\ndeployed in the real world, a PTLM-based model must deal with data from a new\ndomain that deviates from what the PTLM was initially trained on, or newly\nemerged data that contains out-of-distribution information. In this paper, we\nstudy a lifelong language model pretraining challenge where a PTLM is\ncontinually updated so as to adapt to emerging data. Over a domain-incremental\nresearch paper stream and a chronologically ordered tweet stream, we\nincrementally pretrain a PTLM with different continual learning algorithms, and\nkeep track of the downstream task performance (after fine-tuning) to analyze\nits ability of acquiring new knowledge and preserving learned knowledge. Our\nexperiments show continual learning algorithms improve knowledge preservation,\nwith logit distillation being the most effective approach. We further show that\ncontinual pretraining improves generalization when training and testing data of\ndownstream tasks are drawn from different time steps, but do not improve when\nthey are from the same time steps. We believe our problem formulation, methods,\nand analysis will inspire future studies towards continual pretraining of\nlanguage models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1\">Xisen Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dejiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Henghui Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_W/0/1/0/all/0/1\">Wei Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shang-Wen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xiaokai Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arnold_A/0/1/0/all/0/1\">Andrew Arnold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparse Distillation: Speeding Up Text Classification by Using Bigger Models. (arXiv:2110.08536v1 [cs.CL])","link":"http://arxiv.org/abs/2110.08536","description":"<p>Distilling state-of-the-art transformer models into lightweight student\nmodels is an effective way to reduce computation cost at inference time.\nHowever, the improved inference speed may be still unsatisfactory for certain\ntime-sensitive applications. In this paper, we aim to further push the limit of\ninference speed by exploring a new area in the design space of the student\nmodel. More specifically, we consider distilling a transformer-based text\nclassifier into a billion-parameter, sparsely-activated student model with a\nembedding-averaging architecture. Our experiments show that the student models\nretain 97% of the RoBERTa-Large teacher performance on a collection of six text\nclassification tasks. Meanwhile, the student model achieves up to 600x speed-up\non both GPUs and CPUs, compared to the teacher models. Further investigation\nshows that our pipeline is also effective in privacy-preserving and domain\ngeneralization settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1\">Qinyuan Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khabsa_M/0/1/0/all/0/1\">Madian Khabsa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1\">Mike Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sinong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaech_A/0/1/0/all/0/1\">Aaron Jaech</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Substructure Distribution Projection for Zero-Shot Cross-Lingual Dependency Parsing. (arXiv:2110.08538v1 [cs.CL])","link":"http://arxiv.org/abs/2110.08538","description":"<p>We present substructure distribution projection (SubDP), a technique that\nprojects a distribution over structures in one domain to another, by projecting\nsubstructure distributions separately. Models for the target domains can be\nthen trained, using the projected distributions as soft silver labels. We\nevaluate SubDP on zero-shot cross-lingual dependency parsing, taking dependency\narcs as substructures: we project the predicted dependency arc distributions in\nthe source language(s) to target language(s), and train a target language\nparser to fit the resulting distributions. When an English treebank is the only\nannotation that involves human effort, SubDP achieves better unlabeled\nattachment score than all prior work on the Universal Dependencies v2.2 (Nivre\net al., 2020) test set across eight diverse target languages, as well as the\nbest labeled attachment score on six out of eight languages. In addition, SubDP\nimproves zero-shot cross-lingual dependency parsing with very few (e.g., 50)\nsupervised bitext pairs, across a broader range of target languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Haoyue Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gimpel_K/0/1/0/all/0/1\">Kevin Gimpel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Livescu_K/0/1/0/all/0/1\">Karen Livescu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Solve Complex Tasks by Talking to Agents. (arXiv:2110.08542v1 [cs.CL])","link":"http://arxiv.org/abs/2110.08542","description":"<p>Humans often solve complex problems by interacting (in natural language) with\nexisting agents, such as AI assistants, that can solve simpler sub-tasks. These\nagents themselves can be powerful systems built using extensive resources and\nprivately held data. In contrast, common NLP benchmarks aim for the development\nof self-sufficient models for every task. To address this gap and facilitate\nresearch towards ``green'' AI systems that build upon existing agents, we\npropose a new benchmark called CommaQA that contains three kinds of complex\nreasoning tasks that are designed to be solved by ``talking'' to four agents\nwith different capabilities. We demonstrate that state-of-the-art black-box\nmodels, which are unable to leverage existing agents, struggle on CommaQA\n(exact match score only reaches 40pts) even when given access to the agents'\ninternal knowledge and gold fact supervision. On the other hand, models using\ngold question decomposition supervision can indeed solve CommaQA to a high\naccuracy (over 96\\% exact match) by learning to utilize the agents. Even these\nadditional supervision models, however, do not solve our compositional\ngeneralization test set. Finally the end-goal of learning to solve complex\ntasks by communicating with existing agents \\emph{without relying on any\nadditional supervision} remains unsolved and we hope CommaQA serves as a novel\nbenchmark to enable the development of such systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khot_T/0/1/0/all/0/1\">Tushar Khot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Richardson_K/0/1/0/all/0/1\">Kyle Richardson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khashabi_D/0/1/0/all/0/1\">Daniel Khashabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabharwal_A/0/1/0/all/0/1\">Ashish Sabharwal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tackling Multi-Answer Open-Domain Questions via a Recall-then-Verify Framework. (arXiv:2110.08544v1 [cs.CL])","link":"http://arxiv.org/abs/2110.08544","description":"<p>Open domain questions are likely to be open-ended and ambiguous, leading to\nmultiple valid answers. Existing approaches typically adopt the\nrerank-then-read framework, where a reader reads top-ranking evidence to\npredict answers. According to our empirical analyses, this framework is faced\nwith three problems: to leverage the power of a large reader, the reranker is\nforced to select only a few relevant passages that cover diverse answers, which\nis non-trivial due to unknown effect on the reader's performance; the small\nreading budget also prevents the reader from making use of valuable retrieved\nevidence filtered out by the reranker; besides, as the reader generates\npredictions all at once based on all selected evidence, it may learn\npathological dependencies among answers, i.e., whether to predict an answer may\nalso depend on evidence of the other answers. To avoid these problems, we\npropose to tackle multi-answer open-domain questions with a recall-then-verify\nframework, which separates the reasoning process of each answer so that we can\nmake better use of retrieved evidence while also leveraging the power of large\nmodels under the same memory constraint. Our framework achieves new\nstate-of-the-art results on two multi-answer datasets, and predicts\nsignificantly more gold answers than a rerank-then-read system with an oracle\nreranker.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shao_Z/0/1/0/all/0/1\">Zhihong Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Unified Speaker Adaptation Approach for ASR. (arXiv:2110.08545v1 [eess.AS])","link":"http://arxiv.org/abs/2110.08545","description":"<p>Transformer models have been used in automatic speech recognition (ASR)\nsuccessfully and yields state-of-the-art results. However, its performance is\nstill affected by speaker mismatch between training and test data. Further\nfinetuning a trained model with target speaker data is the most natural\napproach for adaptation, but it takes a lot of compute and may cause\ncatastrophic forgetting to the existing speakers. In this work, we propose a\nunified speaker adaptation approach consisting of feature adaptation and model\nadaptation. For feature adaptation, we employ a speaker-aware persistent memory\nmodel which generalizes better to unseen test speakers by making use of speaker\ni-vectors to form a persistent memory. For model adaptation, we use a novel\ngradual pruning method to adapt to target speakers without changing the model\narchitecture, which to the best of our knowledge, has never been explored in\nASR. Specifically, we gradually prune less contributing parameters on model\nencoder to a certain sparsity level, and use the pruned parameters for\nadaptation, while freezing the unpruned parameters to keep the original model\nperformance. We conduct experiments on the Librispeech dataset. Our proposed\napproach brings relative 2.74-6.52% word error rate (WER) reduction on general\nspeaker adaptation. On target speaker adaptation, our method outperforms the\nbaseline with up to 20.58% relative WER reduction, and surpasses the finetuning\nmethod by up to relative 2.54%. Besides, with extremely low-resource adaptation\ndata (e.g., 1 utterance), our method could improve the WER by relative 6.53%\nwith only a few epochs of training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhao_Y/0/1/0/all/0/1\">Yingzhu Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ni_C/0/1/0/all/0/1\">Chongjia Ni</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Leung_C/0/1/0/all/0/1\">Cheung-Chi Leung</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chng_E/0/1/0/all/0/1\">Eng Siong Chng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ma_B/0/1/0/all/0/1\">Bin Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Making the Most of Multilingual Pretraining for Zero-Shot Neural Machine Translation. (arXiv:2110.08547v1 [cs.CL])","link":"http://arxiv.org/abs/2110.08547","description":"<p>This paper demonstrates that multilingual pretraining, a proper fine-tuning\nmethod and a large-scale parallel dataset from multiple auxiliary languages are\nall critical for zero-shot translation, where the NMT model is tested on source\nlanguages unseen during supervised training. Following this idea, we present\nSixT++, a strong many-to-English NMT model that supports 100 source languages\nbut is trained once with a parallel dataset from only six source languages.\nSixT++ initializes the decoder embedding and the full encoder with XLM-R large,\nand then trains the encoder and decoder layers with a simple two-stage training\nstrategy. SixT++ achieves impressive performance on many-to-English\ntranslation. It significantly outperforms CRISS and m2m-100, two strong\nmultilingual NMT systems, with an average gain of 7.2 and 5.0 BLEU\nrespectively. Additionally, SixT++ offers a set of model parameters that can be\nfurther fine-tuned to develop unsupervised NMT models for low-resource\nlanguages. With back-translation on monolingual data of low-resource language,\nit outperforms all current state-of-the-art unsupervised methods on Nepali and\nSinhal for both translating into and from English.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guanhua Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shuming Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongdong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Jia Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HRKD: Hierarchical Relational Knowledge Distillation for Cross-domain Language Model Compression. (arXiv:2110.08551v1 [cs.CL])","link":"http://arxiv.org/abs/2110.08551","description":"<p>On many natural language processing tasks, large pre-trained language models\n(PLMs) have shown overwhelming performances compared with traditional neural\nnetwork methods. Nevertheless, their huge model size and low inference speed\nhave hindered the deployment on resource-limited devices in practice. In this\npaper, we target to compress PLMs with knowledge distillation, and propose a\nhierarchical relational knowledge distillation (HRKD) method to capture both\nhierarchical and domain relational information. Specifically, to enhance the\nmodel capability and transferability, we leverage the idea of meta-learning and\nset up domain-relational graphs to capture the relational information across\ndifferent domains. And to dynamically select the most representative prototypes\nfor each domain, we propose a hierarchical compare-aggregate mechanism to\ncapture hierarchical relationships. Extensive experiments on public\nmulti-domain datasets demonstrate the superior performance of our HRKD method\nas well as its strong few-shot learning ability. For reproducibility, we\nrelease the code at https://github.com/cheneydon/hrkd.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1\">Chenhe Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yaliang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Ying Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_M/0/1/0/all/0/1\">Minghui Qiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Virtual Augmentation Supported Contrastive Learning of Sentence Representations. (arXiv:2110.08552v1 [cs.CL])","link":"http://arxiv.org/abs/2110.08552","description":"<p>Despite profound successes, contrastive representation learning relies on\ncarefully designed data augmentations using domain specific knowledge. This\nchallenge is magnified in natural language processing where no general rules\nexist for data augmentation due to the discrete nature of natural language. We\ntackle this challenge by presenting a Virtual augmentation Supported\nContrastive Learning of sentence representations (VaSCL). Originating from the\ninterpretation that data augmentation essentially constructs the neighborhoods\nof each training instance, we in turn utilize the neighborhood to generate\neffective data augmentations. Leveraging the large training batch size of\ncontrastive learning, we approximate the neighborhood of an instance via its\nK-nearest in-batch neighbors in the representation space. We then define an\ninstance discrimination task within this neighborhood, and generate the virtual\naugmentation in an adversarial training manner. We access the performance of\nVaSCL on a wide range of downstream tasks, and set a new state-of-the-art for\nunsupervised sentence representation learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dejiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_W/0/1/0/all/0/1\">Wei Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Henghui Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaofei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arnold_A/0/1/0/all/0/1\">Andrew O. Arnold</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PAGnol: An Extra-Large French Generative Model. (arXiv:2110.08554v1 [cs.CL])","link":"http://arxiv.org/abs/2110.08554","description":"<p>Access to large pre-trained models of varied architectures, in many different\nlanguages, is central to the democratization of NLP. We introduce PAGnol, a\ncollection of French GPT models. Using scaling laws, we efficiently train\nPAGnol-XL (1.5B parameters) with the same computational budget as CamemBERT, a\nmodel 13 times smaller. PAGnol-XL is the largest model trained to date for the\nFrench language. We plan to train increasingly large and performing versions of\nPAGnol, exploring the capabilities of French extreme-scale models.\n</p>\n<p>For this first release, we focus on the pre-training and scaling calculations\nunderlining PAGnol. We fit a scaling law for compute for the French language,\nand compare it with its English counterpart. We find the pre-training dataset\nsignificantly conditions the quality of the outputs, with common datasets such\nas OSCAR leading to low-quality offensive text. We evaluate our models on\ndiscriminative and generative tasks in French, comparing to other\nstate-of-the-art French and multilingual models, and reaching the state of the\nart in the abstract summarization task. Our research was conducted on the\npublic GENCI Jean Zay supercomputer, and our models up to the Large are made\npublicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Launay_J/0/1/0/all/0/1\">Julien Launay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tommasone_E/0/1/0/all/0/1\">E.L. Tommasone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pannier_B/0/1/0/all/0/1\">Baptiste Pannier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boniface_F/0/1/0/all/0/1\">Fran&#xe7;ois Boniface</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chatelain_A/0/1/0/all/0/1\">Am&#xe9;lie Chatelain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cappelli_A/0/1/0/all/0/1\">Alessandro Cappelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poli_I/0/1/0/all/0/1\">Iacopo Poli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seddah_D/0/1/0/all/0/1\">Djam&#xe9; Seddah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Robustness of Reading Comprehension Models to Entity Renaming. (arXiv:2110.08555v1 [cs.CL])","link":"http://arxiv.org/abs/2110.08555","description":"<p>We study the robustness of machine reading comprehension (MRC) models to\nentity renaming -- do models make more wrong predictions when answer entities\nhave different names? Such failures would indicate that models are overly\nreliant on entity knowledge to answer questions, and therefore may generalize\npoorly when facts about the world change or questions are asked about novel\nentities. To systematically audit model robustness, we propose a general and\nscalable method to replace person names with names from a variety of sources,\nranging from common English names to names from other languages to arbitrary\nstrings. Across four datasets and three pretrained model architectures, MRC\nmodels consistently perform worse when entities are renamed, with particularly\nlarge accuracy drops on datasets constructed via distant supervision. We also\nfind large differences between models: SpanBERT, which is pretrained with\nspan-level masking, is more robust than RoBERTa, despite having similar\naccuracy on unperturbed test data. Inspired by this, we experiment with\nspan-level and entity-level masking as a continual pretraining objective and\nfind that they can further improve the robustness of MRC models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Jun Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yang Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_S/0/1/0/all/0/1\">Sagnik Mukherjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Bill Yuchen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1\">Robin Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FrugalScore: Learning Cheaper, Lighter and Faster Evaluation Metricsfor Automatic Text Generation. (arXiv:2110.08559v1 [cs.CL])","link":"http://arxiv.org/abs/2110.08559","description":"<p>Fast and reliable evaluation metrics are key to R&amp;D progress. While\ntraditional natural language generation metrics are fast, they are not very\nreliable. Conversely, new metrics based on large pretrained language models are\nmuch more reliable, but require significant computational resources. In this\npaper, we propose FrugalScore, an approach to learn a fixed, low cost version\nof any expensive NLG metric, while retaining most of its original performance.\nExperiments with BERTScore and MoverScore on summarization and translation show\nthat FrugalScore is on par with the original metrics (and sometimes better),\nwhile having several orders of magnitude less parameters and running several\ntimes faster. On average over all learned metrics, tasks, and variants,\nFrugalScore retains 96.8% of the performance, runs 24 times faster, and has 35\ntimes less parameters than the original metrics. We make our trained metrics\npublicly available, to benefit the entire NLP community and in particular\nresearchers and practitioners with limited resources.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Eddine_M/0/1/0/all/0/1\">Moussa Kamal Eddine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_G/0/1/0/all/0/1\">Guokan Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tixier_A/0/1/0/all/0/1\">Antoine J.-P. Tixier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vazirgiannis_M/0/1/0/all/0/1\">Michalis Vazirgiannis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ASR4REAL: An extended benchmark for speech models. (arXiv:2110.08583v1 [eess.AS])","link":"http://arxiv.org/abs/2110.08583","description":"<p>Popular ASR benchmarks such as Librispeech and Switchboard are limited in the\ndiversity of settings and speakers they represent. We introduce a set of\nbenchmarks matching real-life conditions, aimed at spotting possible biases and\nweaknesses in models. We have found out that even though recent models do not\nseem to exhibit a gender bias, they usually show important performance\ndiscrepancies by accent, and even more important ones depending on the\nsocio-economic status of the speakers. Finally, all tested models show a strong\nperformance drop when tested on conversational speech, and in this precise\ncontext even a language model trained on a dataset as big as Common Crawl does\nnot seem to have significant positive effect which reiterates the importance of\ndeveloping conversational language models\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Riviere_M/0/1/0/all/0/1\">Morgane Riviere</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Copet_J/0/1/0/all/0/1\">Jade Copet</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Synnaeve_G/0/1/0/all/0/1\">Gabriel Synnaeve</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"n-stage Latent Dirichlet Allocation: A Novel Approach for LDA. (arXiv:2110.08591v1 [cs.CL])","link":"http://arxiv.org/abs/2110.08591","description":"<p>Nowadays, data analysis has become a problem as the amount of data is\nconstantly increasing. In order to overcome this problem in textual data, many\nmodels and methods are used in natural language processing. The topic modeling\nfield is one of these methods. Topic modeling allows determining the semantic\nstructure of a text document. Latent Dirichlet Allocation (LDA) is the most\ncommon method among topic modeling methods. In this article, the proposed\nn-stage LDA method, which can enable the LDA method to be used more\neffectively, is explained in detail. The positive effect of the method has been\ndemonstrated by the applied English and Turkish studies. Since the method\nfocuses on reducing the word count in the dictionary, it can be used\nlanguage-independently. You can access the open-source code of the method and\nthe example: https://github.com/anil1055/n-stage_LDA\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guven_Z/0/1/0/all/0/1\">Zekeriya Anil Guven</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diri_B/0/1/0/all/0/1\">Banu Diri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cakaloglu_T/0/1/0/all/0/1\">Tolgahan Cakaloglu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Back to Reality: Leveraging Pattern-driven Modeling to Enable Affordable Sentiment Dependency Learning. (arXiv:2110.08604v1 [cs.CL])","link":"http://arxiv.org/abs/2110.08604","description":"<p>Aspect-based Sentiment Classification (ABSC) is a challenging sub-task of\ntraditional sentiment analysis. Due to the difficulty of handling potential\ncorrelations among sentiment polarities of multiple aspects, i.e., sentiment\ndependency, recent popular works tend to exploit syntactic information guiding\nsentiment dependency parsing. However, syntax information (e.g., syntactic\ndependency trees) usually occupies expensive computational resources in terms\nof the operation of the adjacent matrix. Instead, we define the consecutive\naspects with the same sentiment as the sentiment cluster in the case that we\nfind that most sentiment dependency occurs between adjacent aspects. Motivated\nby this finding, we propose the sentiment patterns (SP) to guide the model\ndependency learning. Thereafter, we introduce the local sentiment aggregating\n(LSA) mechanism to focus on learning the sentiment dependency in the sentiment\ncluster. The LSA is more efficient than existing dependency tree-based models\ndue to the absence of additional dependency matrix constructing and modeling.\nFurthermore, we propose differential weighting for aggregation window building\nto measure the importance of sentiment dependency. Experiments on four public\ndatasets show that our models achieve state-of-the-art performance with\nespecially improvement on learning sentiment cluster.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Heng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_B/0/1/0/all/0/1\">Biqing Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mayi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianxing Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Position-Aware Self-Attention based Neural Sequence Labeling. (arXiv:1908.09128v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/1908.09128","description":"<p>Sequence labeling is a fundamental task in natural language processing and\nhas been widely studied. Recently, RNN-based sequence labeling models have\nincreasingly gained attentions. Despite superior performance achieved by\nlearning the long short-term (i.e., successive) dependencies, the way of\nsequentially processing inputs might limit the ability to capture the\nnon-continuous relations over tokens within a sentence. To tackle the problem,\nwe focus on how to effectively model successive and discrete dependencies of\neach token for enhancing the sequence labeling performance. Specifically, we\npropose an innovative attention-based model (called position-aware\nselfattention, i.e., PSA) as well as a well-designed self-attentional context\nfusion layer within a neural network architecture, to explore the positional\ninformation of an input sequence for capturing the latent relations among\ntokens. Extensive experiments on three classical tasks in sequence labeling\ndomain, i.e., partof-speech (POS) tagging, named entity recognition (NER) and\nphrase chunking, demonstrate our proposed model outperforms the\nstate-of-the-arts without any external knowledge, in terms of various metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1\">Wei Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zanbo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_X/0/1/0/all/0/1\">Xianling Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_G/0/1/0/all/0/1\">Guangyou Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Sheng Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modulating Bottom-Up and Top-Down Visual Processing via Language-Conditional Filters. (arXiv:2003.12739v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2003.12739","description":"<p>How to best integrate linguistic and perceptual processing in multi-modal\ntasks that involve language and vision is an important open problem. In this\nwork, we argue that the common practice of using language in a top-down manner,\nto direct visual attention over high-level visual features, may not be optimal.\nWe hypothesize that the use of language to also condition the bottom-up\nprocessing from pixels to high-level features can provide benefits to the\noverall performance. To support our claim, we propose a model for\nlanguage-vision problems involving dense prediction, and perform experiments on\ntwo different multi-modal tasks: image segmentation from referring expressions\nand language-guided image colorization. We compare results where either one or\nboth of the top-down and bottom-up visual branches are conditioned on language.\nOur experiments reveal that using language to control the filters for bottom-up\nvisual processing in addition to top-down attention leads to better results on\nboth tasks and achieves state-of-the-art performance. Our analysis of different\nword types in input expressions suggest that the bottom-up conditioning is\nespecially helpful in the presence of low level visual concepts like color.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kesen_I/0/1/0/all/0/1\">&#x130;lker Kesen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Can_O/0/1/0/all/0/1\">Ozan Arkan Can</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erdem_E/0/1/0/all/0/1\">Erkut Erdem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erdem_A/0/1/0/all/0/1\">Aykut Erdem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuret_D/0/1/0/all/0/1\">Deniz Yuret</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simulated Chats for Building Dialog Systems: Learning to Generate Conversations from Instructions. (arXiv:2010.10216v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.10216","description":"<p>Popular dialog datasets such as MultiWOZ are created by providing crowd\nworkers an instruction, expressed in natural language, that describes the task\nto be accomplished. Crowd workers play the role of a user and an agent to\ngenerate dialogs to accomplish tasks involving booking restaurant tables,\ncalling a taxi etc. In this paper, we present a data creation strategy that\nuses the pre-trained language model, GPT2, to simulate the interaction between\ncrowd workers by creating a user bot and an agent bot. We train the simulators\nusing a smaller percentage of actual crowd-generated conversations and their\ncorresponding instructions. We demonstrate that by using the simulated data, we\nachieve significant improvements in low-resource settings on two publicly\navailable datasets - the MultiWOZ dataset and the Persona chat dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohapatra_B/0/1/0/all/0/1\">Biswesh Mohapatra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandey_G/0/1/0/all/0/1\">Gaurav Pandey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Contractor_D/0/1/0/all/0/1\">Danish Contractor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_S/0/1/0/all/0/1\">Sachindra Joshi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LSTM Based Sentiment Analysis for Cryptocurrency Prediction. (arXiv:2103.14804v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.14804","description":"<p>Recent studies in big data analytics and natural language processing develop\nautomatic techniques in analyzing sentiment in the social media information. In\naddition, the growing user base of social media and the high volume of posts\nalso provide valuable sentiment information to predict the price fluctuation of\nthe cryptocurrency. This research is directed to predicting the volatile price\nmovement of cryptocurrency by analyzing the sentiment in social media and\nfinding the correlation between them. While previous work has been developed to\nanalyze sentiment in English social media posts, we propose a method to\nidentify the sentiment of the Chinese social media posts from the most popular\nChinese social media platform Sina-Weibo. We develop the pipeline to capture\nWeibo posts, describe the creation of the crypto-specific sentiment dictionary,\nand propose a long short-term memory (LSTM) based recurrent neural network\nalong with the historical cryptocurrency price movement to predict the price\ntrend for future time frames. The conducted experiments demonstrate the\nproposed approach outperforms the state of the art auto regressive based model\nby 18.5% in precision and 15.4% in recall.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenbin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xuejiao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mingli Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Surbiryala_J/0/1/0/all/0/1\">Jayachander Surbiryala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iosifidis_V/0/1/0/all/0/1\">Vasileios Iosifidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Ji Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What Will it Take to Fix Benchmarking in Natural Language Understanding?. (arXiv:2104.02145v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.02145","description":"<p>Evaluation for many natural language understanding (NLU) tasks is broken:\nUnreliable and biased systems score so highly on standard benchmarks that there\nis little room for researchers who develop better systems to demonstrate their\nimprovements. The recent trend to abandon IID benchmarks in favor of\nadversarially-constructed, out-of-distribution test sets ensures that current\nmodels will perform poorly, but ultimately only obscures the abilities that we\nwant our benchmarks to measure. In this position paper, we lay out four\ncriteria that we argue NLU benchmarks should meet. We argue most current\nbenchmarks fail at these criteria, and that adversarial data collection does\nnot meaningfully address the causes of these failures. Instead, restoring a\nhealthy evaluation ecosystem will require significant progress in the design of\nbenchmark datasets, the reliability with which they are annotated, their size,\nand the ways they handle social bias.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bowman_S/0/1/0/all/0/1\">Samuel R. Bowman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dahl_G/0/1/0/all/0/1\">George E. Dahl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Family of Origin and Family of Choice: Massively Parallel Lexiconized Iterative Pretraining for Severely Low Resource Machine Translation. (arXiv:2104.05848v7 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.05848","description":"<p>We translate a closed text that is known in advance into a severely low\nresource language by leveraging massive source parallelism. In other words,\ngiven a text in 124 source languages, we translate it into a severely low\nresource language using only ~1,000 lines of low resource data without any\nexternal help. Firstly, we propose a systematic method to rank and choose\nsource languages that are close to the low resource language. We call the\nlinguistic definition of language family Family of Origin (FAMO), and we call\nthe empirical definition of higher-ranked languages using our metrics Family of\nChoice (FAMC). Secondly, we build an Iteratively Pretrained Multilingual\nOrder-preserving Lexiconized Transformer (IPML) to train on ~1,000 lines\n(~3.5%) of low resource data. To translate named entities correctly, we build a\nmassive lexicon table for 2,939 Bible named entities in 124 source languages,\nand include many that occur once and covers more than 66 severely low resource\nlanguages. Moreover, we also build a novel method of combining translations\nfrom different source languages into one. Using English as a hypothetical low\nresource language, we get a +23.9 BLEU increase over a multilingual baseline,\nand a +10.3 BLEU increase over our asymmetric baseline in the Bible dataset. We\nget a 42.8 BLEU score for Portuguese-English translation on the medical EMEA\ndataset. We also have good results for a real severely low resource Mayan\nlanguage, Eastern Pokomchi.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waibel_A/0/1/0/all/0/1\">Alex Waibel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ExplaGraphs: An Explanation Graph Generation Task for Structured Commonsense Reasoning. (arXiv:2104.07644v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07644","description":"<p>Recent commonsense-reasoning tasks are typically discriminative in nature,\nwhere a model answers a multiple-choice question for a certain context.\nDiscriminative tasks are limiting because they fail to adequately evaluate the\nmodel's ability to reason and explain predictions with underlying commonsense\nknowledge. They also allow such models to use reasoning shortcuts and not be\n\"right for the right reasons\". In this work, we present ExplaGraphs, a new\ngenerative and structured commonsense-reasoning task (and an associated\ndataset) of explanation graph generation for stance prediction. Specifically,\ngiven a belief and an argument, a model has to predict if the argument supports\nor counters the belief and also generate a commonsense-augmented graph that\nserves as non-trivial, complete, and unambiguous explanation for the predicted\nstance. We collect explanation graphs through a novel Create-Verify-And-Refine\ngraph collection framework that improves the graph quality (up to 90%) via\nmultiple rounds of verification and refinement. A significant 79% of our graphs\ncontain external commonsense nodes with diverse structures and reasoning\ndepths. Next, we propose a multi-level evaluation framework, consisting of\nautomatic metrics and human evaluation, that check for the structural and\nsemantic correctness of the generated graphs and their degree of match with\nground-truth graphs. Finally, we present several structured,\ncommonsense-augmented, and text generation models as strong starting points for\nthis explanation graph generation task, and observe that there is a large gap\nwith human performance, thereby encouraging future work for this new\nchallenging task. ExplaGraphs will be publicly available at\nhttps://explagraphs.github.io.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saha_S/0/1/0/all/0/1\">Swarnadeep Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yadav_P/0/1/0/all/0/1\">Prateek Yadav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bauer_L/0/1/0/all/0/1\">Lisa Bauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Task Generalization via Natural Language Crowdsourcing Instructions. (arXiv:2104.08773v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08773","description":"<p>Humans (e.g., crowdworkers) have a remarkable ability in solving different\ntasks, by simply reading textual instructions that define them and looking at a\nfew examples. NLP models built with the conventional paradigm, however, often\nstruggle with generalization across tasks (e.g., a question-answering system\ncannot solve classification tasks). A long-standing challenge in AI is to build\na model that learns a new task by understanding the human-readable instructions\nthat define it. To study this, we introduce NATURAL INSTRUCTIONS, a dataset of\n61 distinct tasks, their human-authored instructions and 193k task instances.\nThe instructions are obtained from crowdsourcing instructions used to create\nexisting NLP datasets and mapped to a unified schema. We adopt generative\npre-trained language models to encode task-specific instructions along with\ninput and generate task output. Our results indicate that models benefit from\ninstructions when evaluated in terms of generalization to unseen tasks. These\nmodels, however, are far behind supervised task-specific models, indicating\nsignificant room for more progress in this direction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Swaroop Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khashabi_D/0/1/0/all/0/1\">Daniel Khashabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Misinfo Reaction Frames: Reasoning about Readers' Reactions to News Headlines. (arXiv:2104.08790v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08790","description":"<p>Even to a simple and short news headline, readers react in a multitude of\nways: cognitively (e.g., inferring the writer's intent), emotionally (e.g.,\nfeeling distrust), and behaviorally (e.g., sharing the news with their\nfriends). Such reactions are instantaneous and yet complex, as they rely on\nfactors that go beyond interpreting the factual content of the news headline.\nInstead, understanding reactions requires pragmatic understanding of the news\nheadline, including broader background knowledge about contentious news topics\nas well as commonsense reasoning about people's intents and emotional\nreactions. We propose Misinfo Reaction Frames, a pragmatic formalism for\nmodeling how readers might react to a news headline cognitively, emotionally,\nand behaviorally. We also introduce a Misinfo Reaction Frames corpus, a dataset\nof over 200k news headline/annotated dimension pairs with crowdsourced\nreactions focusing on global crises: the Covid-19 pandemic, climate change, and\ncancer. Empirical results confirm that it is indeed possible to learn the\nprominent patterns of readers' reactions to news headlines. We also find a\npotentially positive use case of our model; When we present our model generated\ninferences to people, we find that the machine inferences can increase readers'\ntrust in real news while decreasing their trust in misinformation. Our work\ndemonstrates the feasibility and the importance of pragmatic inferences of news\nto help enhance AI-guided misinformation detection and mitigation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gabriel_S/0/1/0/all/0/1\">Saadia Gabriel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hallinan_S/0/1/0/all/0/1\">Skyler Hallinan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sap_M/0/1/0/all/0/1\">Maarten Sap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_P/0/1/0/all/0/1\">Pemi Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roesner_F/0/1/0/all/0/1\">Franziska Roesner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1\">Eunsol Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SUPERB: Speech processing Universal PERformance Benchmark. (arXiv:2105.01051v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.01051","description":"<p>Self-supervised learning (SSL) has proven vital for advancing research in\nnatural language processing (NLP) and computer vision (CV). The paradigm\npretrains a shared model on large volumes of unlabeled data and achieves\nstate-of-the-art (SOTA) for various tasks with minimal adaptation. However, the\nspeech processing community lacks a similar setup to systematically explore the\nparadigm. To bridge this gap, we introduce Speech processing Universal\nPERformance Benchmark (SUPERB). SUPERB is a leaderboard to benchmark the\nperformance of a shared model across a wide range of speech processing tasks\nwith minimal architecture changes and labeled data. Among multiple usages of\nthe shared model, we especially focus on extracting the representation learned\nfrom SSL due to its preferable re-usability. We present a simple framework to\nsolve SUPERB tasks by learning task-specialized lightweight prediction heads on\ntop of the frozen shared model. Our results demonstrate that the framework is\npromising as SSL representations show competitive generalizability and\naccessibility across SUPERB tasks. We release SUPERB as a challenge with a\nleaderboard and a benchmark toolkit to fuel the research in representation\nlearning and general speech processing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shu-wen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chi_P/0/1/0/all/0/1\">Po-Han Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chuang_Y/0/1/0/all/0/1\">Yung-Sung Chuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_C/0/1/0/all/0/1\">Cheng-I Jeff Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakhotia_K/0/1/0/all/0/1\">Kushal Lakhotia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yist Y. Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Andy T. Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jiatong Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xuankai Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1\">Guan-Ting Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Tzu-Hsien Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tseng_W/0/1/0/all/0/1\">Wei-Cheng Tseng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Ko-tik Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Da-Rong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zili Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_S/0/1/0/all/0/1\">Shuyan Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shang-Wen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1\">Abdelrahman Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic-WER: A Unified Metric for the Evaluation of ASR Transcript for End Usability. (arXiv:2106.02016v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.02016","description":"<p>Recent advances in supervised, semi-supervised and self-supervised deep\nlearning algorithms have shown significant improvement in the performance of\nautomatic speech recognition(ASR) systems. The state-of-the-art systems have\nachieved a word error rate (WER) less than 5%. However, in the past,\nresearchers have argued the non-suitability of the WER metric for the\nevaluation of ASR systems for downstream tasks such as spoken language\nunderstanding (SLU) and information retrieval. The reason is that the WER works\nat the surface level and does not include any syntactic and semantic\nknowledge.The current work proposes Semantic-WER (SWER), a metric to evaluate\nthe ASR transcripts for downstream applications in general. The SWER can be\neasily customized for any down-stream task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1\">Somnath Roy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Machine Translation into Low-resource Language Varieties. (arXiv:2106.06797v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.06797","description":"<p>State-of-the-art machine translation (MT) systems are typically trained to\ngenerate the \"standard\" target language; however, many languages have multiple\nvarieties (regional varieties, dialects, sociolects, non-native varieties) that\nare different from the standard language. Such varieties are often\nlow-resource, and hence do not benefit from contemporary NLP solutions, MT\nincluded. We propose a general framework to rapidly adapt MT systems to\ngenerate language varieties that are close to, but different from, the standard\ntarget language, using no parallel (source--variety) data. This also includes\nadaptation of MT systems to low-resource typologically-related target\nlanguages. We experiment with adapting an English--Russian MT system to\ngenerate Ukrainian and Belarusian, an English--Norwegian Bokm{\\aa}l system to\ngenerate Nynorsk, and an English--Arabic system to generate four Arabic\ndialects, obtaining significant improvements over competitive baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Sachin Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anastasopoulos_A/0/1/0/all/0/1\">Antonios Anastasopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wintner_S/0/1/0/all/0/1\">Shuly Wintner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1\">Yulia Tsvetkov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LoRA: Low-Rank Adaptation of Large Language Models. (arXiv:2106.09685v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.09685","description":"<p>An important paradigm of natural language processing consists of large-scale\npre-training on general domain data and adaptation to particular tasks or\ndomains. As we pre-train larger models, full fine-tuning, which retrains all\nmodel parameters, becomes less feasible. Using GPT-3 175B as an example --\ndeploying independent instances of fine-tuned models, each with 175B\nparameters, is prohibitively expensive. We propose Low-Rank Adaptation, or\nLoRA, which freezes the pre-trained model weights and injects trainable rank\ndecomposition matrices into each layer of the Transformer architecture, greatly\nreducing the number of trainable parameters for downstream tasks. Compared to\nGPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable\nparameters by 10,000 times and the GPU memory requirement by 3 times. LoRA\nperforms on-par or better than fine-tuning in model quality on RoBERTa,\nDeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher\ntraining throughput, and, unlike adapters, no additional inference latency. We\nalso provide an empirical investigation into rank-deficiency in language model\nadaptation, which sheds light on the efficacy of LoRA. We release a package\nthat facilitates the integration of LoRA with PyTorch models and provide our\nimplementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at\nhttps://github.com/microsoft/LoRA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_E/0/1/0/all/0/1\">Edward J. Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yelong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wallis_P/0/1/0/all/0/1\">Phillip Wallis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allen_Zhu_Z/0/1/0/all/0/1\">Zeyuan Allen-Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuanzhi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shean Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Investigation of the (In)effectiveness of Counterfactually Augmented Data. (arXiv:2107.00753v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.00753","description":"<p>While pretrained language models achieve excellent performance on natural\nlanguage understanding benchmarks, they tend to rely on spurious correlations\nand generalize poorly to out-of-distribution (OOD) data. Recent work has\nexplored using counterfactually-augmented data (CAD) -- data generated by\nminimally perturbing examples to flip the ground-truth label -- to identify\nrobust features that are invariant under distribution shift. However, empirical\nresults using CAD for OOD generalization have been mixed. To explain this\ndiscrepancy, we draw insights from a linear Gaussian model and demonstrate the\npitfalls of CAD. Specifically, we show that (a) while CAD is effective at\nidentifying robust features, it may prevent the model from learning unperturbed\nrobust features; and (b) CAD may exacerbate existing spurious correlations in\nthe data. On two crowdsourced CAD datasets, our results show that the lack of\nperturbation diversity limits their effectiveness on OOD generalization,\ncalling for innovative crowdsourcing procedures to elicit diverse perturbation\nof examples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Joshi_N/0/1/0/all/0/1\">Nitish Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">He He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improved Language Identification Through Cross-Lingual Self-Supervised Learning. (arXiv:2107.04082v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.04082","description":"<p>Language identification greatly impacts the success of downstream tasks such\nas automatic speech recognition. Recently, self-supervised speech\nrepresentations learned by wav2vec 2.0 have been shown to be very effective for\na range of speech tasks. We extend previous self-supervised work on language\nidentification by experimenting with pre-trained models which were learned on\nreal-world unconstrained speech in multiple languages and not just on English.\nWe show that models pre-trained on many languages perform better and enable\nlanguage identification systems that require very little labeled data to\nperform well. Results on a 26 languages setup show that with only 10 minutes of\nlabeled data per language, a cross-lingually pre-trained model can achieve over\n89.2% accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tjandra_A/0/1/0/all/0/1\">Andros Tjandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choudhury_D/0/1/0/all/0/1\">Diptanu Gon Choudhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Frank Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_K/0/1/0/all/0/1\">Kritika Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Conneau_A/0/1/0/all/0/1\">Alexis Conneau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baevski_A/0/1/0/all/0/1\">Alexei Baevski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sela_A/0/1/0/all/0/1\">Assaf Sela</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saraf_Y/0/1/0/all/0/1\">Yatharth Saraf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Auli_M/0/1/0/all/0/1\">Michael Auli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MuSiQue: Multi-hop Questions via Single-hop Question Composition. (arXiv:2108.00573v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.00573","description":"<p>Can we create a question answering (QA) dataset that, by construction,\nrequires proper multi-hop reasoning? This goal has been surprisingly elusive.\nWe introduce a bottom-up approach that systematically selects composable pairs\nof single-hop questions that are connected, i.e., where one reasoning step\nrequires information from the other. This bottom-up approach allows greater\ncontrol over the properties of the resulting $k$-hop questions. We add\nstringent filters and other mechanisms targeting connected reasoning, including\nminimizing many forms of train-test leakage, improved distractor contexts, and\ncontrasting unanswerable questions at the sub-question level. We use this\nprocess to construct MuSiQue-Ans, a new multihop QA dataset with 25K 2-4 hop\nquestions, built using seed questions from 5 existing single-hop datasets. Our\nexperiments demonstrate that MuSiQue-Ans is challenging for state-of-the-art QA\nmodels significantly harder than existing datasets (3x human-machine gap in a\ncomparable setting), and substantially less cheatable (e.g., a single-hop model\nis worse by 30 F1 pts). We also build a more challenging dataset, MuSiQue-Full,\nconsisting of answerable and unanswerable contrast question pairs, where model\nperformance drops further by 14 F1 pts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Trivedi_H/0/1/0/all/0/1\">Harsh Trivedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_N/0/1/0/all/0/1\">Niranjan Balasubramanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khot_T/0/1/0/all/0/1\">Tushar Khot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabharwal_A/0/1/0/all/0/1\">Ashish Sabharwal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Monolingual versus Multilingual BERTology for Vietnamese Extractive Multi-Document Summarization. (arXiv:2108.13741v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.13741","description":"<p>Recent researches have demonstrated that BERT shows potential in a wide range\nof natural language processing tasks. It is adopted as an encoder for many\nstate-of-the-art automatic summarizing systems, which achieve excellent\nperformance. However, so far, there is not much work done for Vietnamese. In\nthis paper, we showcase how BERT can be implemented for extractive text\nsummarization in Vietnamese on multi-document. We introduce a novel comparison\nbetween different multilingual and monolingual BERT models. The experiment\nresults indicate that monolingual models produce promising results compared to\nother multilingual models and previous text summarizing models for Vietnamese.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+To_H/0/1/0/all/0/1\">Huy Quoc To</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Kiet Van Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1\">Ngan Luu-Thuy Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1\">Anh Gia-Tuan Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sequence-to-Sequence Learning with Latent Neural Grammars. (arXiv:2109.01135v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.01135","description":"<p>Sequence-to-sequence learning with neural networks has become the de facto\nstandard for sequence prediction tasks. This approach typically models the\nlocal distribution over the next word with a powerful neural network that can\ncondition on arbitrary context. While flexible and performant, these models\noften require large datasets for training and can fail spectacularly on\nbenchmarks designed to test for compositional generalization. This work\nexplores an alternative, hierarchical approach to sequence-to-sequence learning\nwith quasi-synchronous grammars, where each node in the target tree is\ntransduced by a node in the source tree. Both the source and target trees are\ntreated as latent and induced during training. We develop a neural\nparameterization of the grammar which enables parameter sharing over the\ncombinatorial space of derivation rules without the need for manual feature\nengineering. We apply this latent neural grammar to various domains -- a\ndiagnostic language navigation task designed to test for compositional\ngeneralization (SCAN), style transfer, and small-scale machine translation --\nand find that it performs respectably compared to standard baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yoon Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multitask Balanced and Recalibrated Network for Medical Code Prediction. (arXiv:2109.02418v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.02418","description":"<p>Human coders assign standardized medical codes to clinical documents\ngenerated during patients' hospitalization, which is error-prone and\nlabor-intensive. Automated medical coding approaches have been developed using\nmachine learning methods such as deep neural networks. Nevertheless, automated\nmedical coding is still challenging because of the imbalanced class problem,\ncomplex code association, and noise in lengthy documents. To solve these\nissues, we propose a novel neural network called Multitask Balanced and\nRecalibrated Neural Network. Significantly, the multitask learning scheme\nshares the relationship knowledge between different code branches to capture\nthe code association. A recalibrated aggregation module is developed by\ncascading convolutional blocks to extract high-level semantic features that\nmitigate the impact of noise in documents. Also, the cascaded structure of the\nrecalibrated module can benefit the learning from lengthy notes. To solve the\nclass imbalanced problem, we deploy the focal loss to redistribute the\nattention of low and high-frequency medical codes. Experimental results show\nthat our proposed model outperforms competitive baselines on a real-world\nclinical dataset MIMIC-III.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Wei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shaoxiong Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cambria_E/0/1/0/all/0/1\">Erik Cambria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marttinen_P/0/1/0/all/0/1\">Pekka Marttinen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Balancing Methods for Multi-label Text Classification with Long-Tailed Class Distribution. (arXiv:2109.04712v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.04712","description":"<p>Multi-label text classification is a challenging task because it requires\ncapturing label dependencies. It becomes even more challenging when class\ndistribution is long-tailed. Resampling and re-weighting are common approaches\nused for addressing the class imbalance problem, however, they are not\neffective when there is label dependency besides class imbalance because they\nresult in oversampling of common labels. Here, we introduce the application of\nbalancing loss functions for multi-label text classification. We perform\nexperiments on a general domain dataset with 90 labels (Reuters-21578) and a\ndomain-specific dataset from PubMed with 18211 labels. We find that a\ndistribution-balanced loss function, which inherently addresses both the class\nimbalance and label linkage problems, outperforms commonly used loss functions.\nDistribution balancing methods have been successfully used in the image\nrecognition field. Here, we show their effectiveness in natural language\nprocessing. Source code is available at\nhttps://github.com/Roche/BalancedLossNLP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giledereli_B/0/1/0/all/0/1\">Buse Giledereli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koksal_A/0/1/0/all/0/1\">Abdullatif K&#xf6;ksal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozgur_A/0/1/0/all/0/1\">Arzucan &#xd6;zg&#xfc;r</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozkirimli_E/0/1/0/all/0/1\">Elif Ozkirimli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Contrastive Learning via Novel Data Augmentation and Curriculum Learning. (arXiv:2109.05941v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.05941","description":"<p>We introduce EfficientCL, a memory-efficient continual pretraining method\nthat applies contrastive learning with novel data augmentation and curriculum\nlearning. For data augmentation, we stack two types of operation sequentially:\ncutoff and PCA jittering. While pretraining steps proceed, we apply curriculum\nlearning by incrementing the augmentation degree for each difficulty step.\nAfter data augmentation is finished, contrastive learning is applied on\nprojected embeddings of original and augmented examples. When finetuned on GLUE\nbenchmark, our model outperforms baseline models, especially for sentence-level\ntasks. Additionally, this improvement is capable with only 70% of computational\nmemory compared to the baseline model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_S/0/1/0/all/0/1\">Seonghyeon Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jiseon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_A/0/1/0/all/0/1\">Alice Oh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Jointly Modeling Aspect and Polarity for Aspect-based Sentiment Analysis in Persian Reviews. (arXiv:2109.07680v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.07680","description":"<p>Identification of user's opinions from natural language text has become an\nexciting field of research due to its growing applications in the real world.\nThe research field is known as sentiment analysis and classification, where\naspect category detection (ACD) and aspect category polarity (ACP) are two\nimportant sub-tasks of aspect-based sentiment analysis. The goal in ACD is to\nspecify which aspect of the entity comes up in opinion while ACP aims to\nspecify the polarity of each aspect category from the ACD task. The previous\nworks mostly propose separate solutions for these two sub-tasks. This paper\nfocuses on the ACD and ACP sub-tasks to solve both problems simultaneously. The\nproposed method carries out multi-label classification where four different\ndeep models were employed and comparatively evaluated to examine their\nperformance. A dataset of Persian reviews was collected from CinemaTicket\nwebsite including 2200 samples from 14 categories. The developed models were\nevaluated using the collected dataset in terms of example-based and label-based\nmetrics. The results indicate the high applicability and preference of the CNN\nand GRU models in comparison to LSTM and Bi-LSTM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vazan_M/0/1/0/all/0/1\">Milad Vazan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Razmara_J/0/1/0/all/0/1\">Jafar Razmara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reframing Instructional Prompts to GPTk's Language. (arXiv:2109.07830v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.07830","description":"<p>How can model designers turn task instructions into effective prompts for\nlanguage models? Backed by extensive empirical analysis on GPT3, we observe\nimportant features for successful instructional prompts, and propose several\nreframing techniques for model designers to create such prompts. For example, a\ncomplex task can be decomposed into multiple simpler tasks. We experiment over\n12 NLP tasks across 6 diverse categories (question generation, classification,\netc.). Our results show that reframing improves few-shot and zero-shot learning\nperformance by 14% and 17% respectively while reducing sample complexity over\nother recent few-shot baselines. The performance gains are particularly\nimportant on large language models, such as GPT3 where tuning models or prompts\non large datasets is not feasible. Furthermore, we observe that such gains are\nnot limited to GPT3; the reframed tasks remain superior over raw instructions\nacross different model architectures, underscoring the cross-model generality\nof these guidelines. We hope these empirical-driven techniques will pave way\nfor more effective ways to prompt LMs in the future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Swaroop Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khashabi_D/0/1/0/all/0/1\">Daniel Khashabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FastCorrect 2: Fast Error Correction on Multiple Candidates for Automatic Speech Recognition. (arXiv:2109.14420v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.14420","description":"<p>Error correction is widely used in automatic speech recognition (ASR) to\npost-process the generated sentence, and can further reduce the word error rate\n(WER). Although multiple candidates are generated by an ASR system through beam\nsearch, current error correction approaches can only correct one sentence at a\ntime, failing to leverage the voting effect from multiple candidates to better\ndetect and correct error tokens. In this work, we propose FastCorrect 2, an\nerror correction model that takes multiple ASR candidates as input for better\ncorrection accuracy. FastCorrect 2 adopts non-autoregressive generation for\nfast inference, which consists of an encoder that processes multiple source\nsentences and a decoder that generates the target sentence in parallel from the\nadjusted source sentence, where the adjustment is based on the predicted\nduration of each source token. However, there are some issues when handling\nmultiple source sentences. First, it is non-trivial to leverage the voting\neffect from multiple source sentences since they usually vary in length. Thus,\nwe propose a novel alignment algorithm to maximize the degree of token\nalignment among multiple sentences in terms of token and pronunciation\nsimilarity. Second, the decoder can only take one adjusted source sentence as\ninput, while there are multiple source sentences. Thus, we develop a candidate\npredictor to detect the most suitable candidate for the decoder. Experiments on\nour inhouse dataset and AISHELL-1 show that FastCorrect 2 can further reduce\nthe WER over the previous correction model with single candidate by 3.2% and\n2.6%, demonstrating the effectiveness of leveraging multiple candidates in ASR\nerror correction. FastCorrect 2 achieves better performance than the cascaded\nre-scoring and correction pipeline and can serve as a unified post-processing\nmodule for ASR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Leng_Y/0/1/0/all/0/1\">Yichong Leng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xu Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Linchen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenjie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Linquan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang-Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_E/0/1/0/all/0/1\">Edward Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Factorized Neural Transducer for Efficient Language Model Adaptation. (arXiv:2110.01500v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.01500","description":"<p>In recent years, end-to-end (E2E) based automatic speech recognition (ASR)\nsystems have achieved great success due to their simplicity and promising\nperformance. Neural Transducer based models are increasingly popular in\nstreaming E2E based ASR systems and have been reported to outperform the\ntraditional hybrid system in some scenarios. However, the joint optimization of\nacoustic model, lexicon and language model in neural Transducer also brings\nabout challenges to utilize pure text for language model adaptation. This\ndrawback might prevent their potential applications in practice. In order to\naddress this issue, in this paper, we propose a novel model, factorized neural\nTransducer, by factorizing the blank and vocabulary prediction, and adopting a\nstandalone language model for the vocabulary prediction. It is expected that\nthis factorization can transfer the improvement of the standalone language\nmodel to the Transducer for speech recognition, which allows various language\nmodel adaptation techniques to be applied. We demonstrate that the proposed\nfactorized neural Transducer yields 15% to 20% WER improvements when\nout-of-domain text data is used for language model adaptation, at the cost of a\nminor degradation in WER on a general test set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1\">Zhong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parthasarathy_S/0/1/0/all/0/1\">Sarangarajan Parthasarathy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinyu Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WenetSpeech: A 10000+ Hours Multi-domain Mandarin Corpus for Speech Recognition. (arXiv:2110.03370v3 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2110.03370","description":"<p>In this paper, we present WenetSpeech, a multi-domain Mandarin corpus\nconsisting of 10000+ hours high-quality labeled speech, 2400+ hours weakly\nlabeled speech, and about 10000 hours unlabeled speech, with 22400+ hours in\ntotal. We collect the data from YouTube and Podcast, which covers a variety of\nspeaking styles, scenarios, domains, topics, and noisy conditions. An optical\ncharacter recognition (OCR) based method is introduced to generate the\naudio/text segmentation candidates for the YouTube data on its corresponding\nvideo captions, while a high-quality ASR transcription system is used to\ngenerate audio/text pair candidates for the Podcast data. Then we propose a\nnovel end-to-end label error detection approach to further validate and filter\nthe candidates. We also provide three manually labelled high-quality test sets\nalong with WenetSpeech for evaluation -- Dev for cross-validation purpose in\ntraining, Test_Net, collected from Internet for matched test, and\nTest\\_Meeting, recorded from real meetings for more challenging mismatched\ntest. Baseline systems trained with WenetSpeech are provided for three popular\nspeech recognition toolkits, namely Kaldi, ESPnet, and WeNet, and recognition\nresults on the three test sets are also provided as benchmarks. To the best of\nour knowledge, WenetSpeech is the current largest open-sourced Mandarin speech\ncorpus with transcriptions, which benefits research on production-level speech\nrecognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Binbin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_H/0/1/0/all/0/1\">Hang Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_P/0/1/0/all/0/1\">Pengcheng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Q/0/1/0/all/0/1\">Qijie Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1\">Lei Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bu_H/0/1/0/all/0/1\">Hui Bu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaoyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_C/0/1/0/all/0/1\">Chenchen Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Di Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1\">Zhendong Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Relation-aware Video Reading Comprehension for Temporal Language Grounding. (arXiv:2110.05717v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.05717","description":"<p>Temporal language grounding in videos aims to localize the temporal span\nrelevant to the given query sentence. Previous methods treat it either as a\nboundary regression task or a span extraction task. This paper will formulate\ntemporal language grounding into video reading comprehension and propose a\nRelation-aware Network (RaNet) to address it. This framework aims to select a\nvideo moment choice from the predefined answer set with the aid of\ncoarse-and-fine choice-query interaction and choice-choice relation\nconstruction. A choice-query interactor is proposed to match the visual and\ntextual information simultaneously in sentence-moment and token-moment levels,\nleading to a coarse-and-fine cross-modal interaction. Moreover, a novel\nmulti-choice relation constructor is introduced by leveraging graph convolution\nto capture the dependencies among video moment choices for the best choice\nselection. Extensive experiments on ActivityNet-Captions, TACoS, and\nCharades-STA demonstrate the effectiveness of our solution. Codes will be\nreleased soon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jialin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mengmeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bernard Ghanem</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting IPA-based Cross-lingual Text-to-speech. (arXiv:2110.07187v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.07187","description":"<p>International Phonetic Alphabet (IPA) has been widely used in cross-lingual\ntext-to-speech (TTS) to achieve cross-lingual voice cloning (CL VC). However,\nIPA itself has been understudied in cross-lingual TTS. In this paper, we report\nsome empirical findings of building a cross-lingual TTS model using IPA as\ninputs. Experiments show that the way to process the IPA and suprasegmental\nsequence has a negligible impact on the CL VC performance. Furthermore, we find\nthat using a dataset including one speaker per language to build an IPA-based\nTTS system would fail CL VC since the language-unique IPA and tone/stress\nsymbols could leak the speaker information. In addition, we experiment with\ndifferent combinations of speakers in the training dataset to further\ninvestigate the effect of the number of speakers on the CL VC performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haitong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_H/0/1/0/all/0/1\">Haoyue Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xinyuan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yue Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks. (arXiv:2110.07602v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.07602","description":"<p>Prompt tuning, which only tunes continuous prompts with a frozen language\nmodel, substantially reduces per-task storage and memory usage at training.\nHowever, in the context of NLU, prior work reveals that prompt tuning does not\nperform well for normal-sized pre-trained models. We also find that existing\nmethods of prompt tuning cannot handle hard sequence tagging tasks, indicating\na lack of universality. We present a novel empirical finding that properly\noptimized prompt tuning can be universally effective across a wide range of\nmodel scales and NLU tasks. It matches the performance of fine-tuning while\nhaving only 0.1\\%-3\\% tuned parameters. Our method P-Tuning v2 is not a new\nmethod, but a version of prefix-tuning \\cite{li2021prefix} optimized and\nadapted for NLU. Given the universality and simplicity of P-Tuning v2, we\nbelieve it can serve as an alternative to fine-tuning and a strong baseline for\nfuture research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_K/0/1/0/all/0/1\">Kaixuan Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yicheng Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Z/0/1/0/all/0/1\">Zhengxiao Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhilin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jie Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Core Challenges in Embodied Vision-Language Planning. (arXiv:2106.13948v2 [cs.LG] CROSS LISTED)","link":"http://arxiv.org/abs/2106.13948","description":"<p>Recent advances in the areas of multimodal machine learning and artificial\nintelligence (AI) have led to the development of challenging tasks at the\nintersection of Computer Vision, Natural Language Processing, and Embodied AI.\nWhereas many approaches and previous survey pursuits have characterised one or\ntwo of these dimensions, there has not been a holistic analysis at the center\nof all three. Moreover, even when combinations of these topics are considered,\nmore focus is placed on describing, e.g., current architectural methods, as\nopposed to also illustrating high-level challenges and opportunities for the\nfield. In this survey paper, we discuss Embodied Vision-Language Planning\n(EVLP) tasks, a family of prominent embodied navigation and manipulation\nproblems that jointly use computer vision and natural language. We propose a\ntaxonomy to unify these tasks and provide an in-depth analysis and comparison\nof the new and current algorithmic approaches, metrics, simulated environments,\nas well as the datasets used for EVLP tasks. Finally, we present the core\nchallenges that we believe new EVLP works should seek to address, and we\nadvocate for task construction that enables model generalizability and furthers\nreal-world deployment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Francis_J/0/1/0/all/0/1\">Jonathan Francis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitamura_N/0/1/0/all/0/1\">Nariaki Kitamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labelle_F/0/1/0/all/0/1\">Felix Labelle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiaopeng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navarro_I/0/1/0/all/0/1\">Ingrid Navarro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1\">Jean Oh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-10-18T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"FlexMatch: Boosting Semi-Supervised Learning with Curriculum Pseudo Labeling. (arXiv:2110.08263v1 [cs.LG])","link":"http://arxiv.org/abs/2110.08263","description":"<p>The recently proposed FixMatch achieved state-of-the-art results on most\nsemi-supervised learning (SSL) benchmarks. However, like other modern SSL\nalgorithms, FixMatch uses a pre-defined constant threshold for all classes to\nselect unlabeled data that contribute to the training, thus failing to consider\ndifferent learning status and learning difficulties of different classes. To\naddress this issue, we propose Curriculum Pseudo Labeling (CPL), a curriculum\nlearning approach to leverage unlabeled data according to the model's learning\nstatus. The core of CPL is to flexibly adjust thresholds for different classes\nat each time step to let pass informative unlabeled data and their pseudo\nlabels. CPL does not introduce additional parameters or computations (forward\nor backward propagation). We apply CPL to FixMatch and call our improved\nalgorithm FlexMatch. FlexMatch achieves state-of-the-art performance on a\nvariety of SSL benchmarks, with especially strong performances when the labeled\ndata are extremely limited or when the task is challenging. For example,\nFlexMatch outperforms FixMatch by 14.32% and 24.55% on CIFAR-100 and STL-10\ndatasets respectively, when there are only 4 labels per class. CPL also\nsignificantly boosts the convergence speed, e.g., FlexMatch can use only 1/5\ntraining time of FixMatch to achieve even better performance. Furthermore, we\nshow that CPL can be easily adapted to other SSL algorithms and remarkably\nimprove their performances. We open source our code at\nhttps://github.com/TorchSSL/TorchSSL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bowen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yidong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_W/0/1/0/all/0/1\">Wenxin Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jindong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okumura_M/0/1/0/all/0/1\">Manabu Okumura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shinozaki_T/0/1/0/all/0/1\">Takahiro Shinozaki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Training Deep Neural Networks with Joint Quantization and Pruning of Weights and Activations. (arXiv:2110.08271v1 [cs.LG])","link":"http://arxiv.org/abs/2110.08271","description":"<p>Quantization and pruning are core techniques used to reduce the inference\ncosts of deep neural networks. State-of-the-art quantization techniques are\ncurrently applied to both the weights and activations; however, pruning is most\noften applied to only the weights of the network. In this work, we jointly\napply novel uniform quantization and unstructured pruning methods to both the\nweights and activations of deep neural networks during training. Using our\nmethods, we empirically evaluate the currently accepted prune-then-quantize\nparadigm across a wide range of computer vision tasks and observe a\nnon-commutative nature when applied to both the weights and activations of deep\nneural networks. Informed by these observations, we articulate the\nnon-commutativity hypothesis: for a given deep neural network being trained for\na specific task, there exists an exact training schedule in which quantization\nand pruning can be introduced to optimize network performance. We identify that\nthis optimal ordering not only exists, but also varies across discriminative\nand generative tasks. Using the optimal training schedule within our training\nframework, we demonstrate increased performance per memory footprint over\nexisting solutions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colbert_I/0/1/0/all/0/1\">Ian Colbert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kreutz_Delgado_K/0/1/0/all/0/1\">Ken Kreutz-Delgado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1\">Srinjoy Das</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D Human Pose Estimation for Free-form Activity Using WiFi Signals. (arXiv:2110.08314v1 [cs.CV])","link":"http://arxiv.org/abs/2110.08314","description":"<p>WiFi human sensing has become increasingly attractive in enabling emerging\nhuman-computer interaction applications. The corresponding technique has\ngradually evolved from the classification of multiple activity types to more\nfine-grained tracking of 3D human poses. However, existing WiFi-based 3D human\npose tracking is limited to a set of predefined activities. In this work, we\npresent Winect, a 3D human pose tracking system for free-form activity using\ncommodity WiFi devices. Our system tracks free-form activity by estimating a 3D\nskeleton pose that consists of a set of joints of the human body. In\nparticular, we combine signal separation and joint movement modeling to achieve\nfree-form activity tracking. Our system first identifies the moving limbs by\nleveraging the two-dimensional angle of arrival of the signals reflected off\nthe human body and separates the entangled signals for each limb. Then, it\ntracks each limb and constructs a 3D skeleton of the body by modeling the\ninherent relationship between the movements of the limb and the corresponding\njoints. Our evaluation results show that Winect is environment-independent and\nachieves centimeter-level accuracy for free-form activity tracking under\nvarious challenging environments including the none-line-of-sight (NLoS)\nscenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Yili Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jie Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Solving Image PDEs with a Shallow Network. (arXiv:2110.08327v1 [cs.CV])","link":"http://arxiv.org/abs/2110.08327","description":"<p>Partial differential equations (PDEs) are typically used as models of\nphysical processes but are also of great interest in PDE-based image\nprocessing. However, when it comes to their use in imaging, conventional\nnumerical methods for solving PDEs tend to require very fine grid resolution\nfor stability, and as a result have impractically high computational cost. This\nwork applies BLADE (Best Linear Adaptive Enhancement), a shallow learnable\nfiltering framework, to PDE solving, and shows that the resulting approach is\nefficient and accurate, operating more reliably at coarse grid resolutions than\nclassical methods. As such, the model can be flexibly used for a wide variety\nof problems in imaging.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Getreuer_P/0/1/0/all/0/1\">Pascal Tom Getreuer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milanfar_P/0/1/0/all/0/1\">Peyman Milanfar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xiyang Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Trigger Hunting with a Topological Prior for Trojan Detection. (arXiv:2110.08335v1 [cs.CV])","link":"http://arxiv.org/abs/2110.08335","description":"<p>Despite their success and popularity, deep neural networks (DNNs) are\nvulnerable when facing backdoor attacks. This impedes their wider adoption,\nespecially in mission critical applications. This paper tackles the problem of\nTrojan detection, namely, identifying Trojaned models -- models trained with\npoisoned data. One popular approach is reverse engineering, i.e., recovering\nthe triggers on a clean image by manipulating the model's prediction. One major\nchallenge of reverse engineering approach is the enormous search space of\ntriggers. To this end, we propose innovative priors such as diversity and\ntopological simplicity to not only increase the chances of finding the\nappropriate triggers but also improve the quality of the found triggers.\nMoreover, by encouraging a diverse set of trigger candidates, our method can\nperform effectively in cases with unknown target labels. We demonstrate that\nthese priors can significantly improve the quality of the recovered triggers,\nresulting in substantially improved Trojan detection accuracy as validated on\nboth synthetic and publicly available TrojAI benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaoling Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xiao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cogswell_M/0/1/0/all/0/1\">Michael Cogswell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yi Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1\">Susmit Jha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chao Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Counting Objects by Diffused Index: geometry-free and training-free approach. (arXiv:2110.08365v1 [cs.CV])","link":"http://arxiv.org/abs/2110.08365","description":"<p>Counting objects is a fundamental but challenging problem. In this paper, we\npropose diffusion-based, geometry-free, and learning-free methodologies to\ncount the number of objects in images. The main idea is to represent each\nobject by a unique index value regardless of its intensity or size, and to\nsimply count the number of index values. First, we place different vectors,\nrefer to as seed vectors, uniformly throughout the mask image. The mask image\nhas boundary information of the objects to be counted. Secondly, the seeds are\ndiffused using an edge-weighted harmonic variational optimization model within\neach object. We propose an efficient algorithm based on an operator splitting\napproach and alternating direction minimization method, and theoretical\nanalysis of this algorithm is given. An optimal solution of the model is\nobtained when the distributed seeds are completely diffused such that there is\na unique intensity within each object, which we refer to as an index. For\ncomputational efficiency, we stop the diffusion process before a full\nconvergence, and propose to cluster these diffused index values. We refer to\nthis approach as Counting Objects by Diffused Index (CODI). We explore scalar\nand multi-dimensional seed vectors. For Scalar seeds, we use Gaussian fitting\nin histogram to count, while for vector seeds, we exploit a high-dimensional\nclustering method for the final step of counting via clustering. The proposed\nmethod is flexible even if the boundary of the object is not clear nor fully\nenclosed. We present counting results in various applications such as\nbiological cells, agriculture, concert crowd, and transportation. Some\ncomparisons with existing methods are presented.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_M/0/1/0/all/0/1\">Mengyi Tang</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Yashtini_M/0/1/0/all/0/1\">Maryam Yashtini</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Kang_S/0/1/0/all/0/1\">Sung Ha Kang</a> (1) ((1) Georgia Institute of Technology, (2) Georgetown University )"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Starkit: RoboCup Humanoid KidSize 2021 Worldwide Champion Team Paper. (arXiv:2110.08377v1 [cs.RO])","link":"http://arxiv.org/abs/2110.08377","description":"<p>This article is devoted to the features that were under development between\nRoboCup 2019 Sydney and RoboCup 2021 Worldwide. These features include\nvision-related matters, such as detection and localization, mechanical and\nalgorithmic novelties. Since the competition was held virtually, the\nsimulation-specific features are also considered in the article. We give an\noverview of the approaches that were tried out along with the analysis of their\npreconditions, perspectives and the evaluation of their performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Davydenko_E/0/1/0/all/0/1\">Egor Davydenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khokhlov_I/0/1/0/all/0/1\">Ivan Khokhlov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Litvinenko_V/0/1/0/all/0/1\">Vladimir Litvinenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryakin_I/0/1/0/all/0/1\">Ilya Ryakin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osokin_I/0/1/0/all/0/1\">Ilya Osokin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Babaev_A/0/1/0/all/0/1\">Azer Babaev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comparing Human and Machine Bias in Face Recognition. (arXiv:2110.08396v1 [cs.CV])","link":"http://arxiv.org/abs/2110.08396","description":"<p>Much recent research has uncovered and discussed serious concerns of bias in\nfacial analysis technologies, finding performance disparities between groups of\npeople based on perceived gender, skin type, lighting condition, etc. These\naudits are immensely important and successful at measuring algorithmic bias but\nhave two major challenges: the audits (1) use facial recognition datasets which\nlack quality metadata, like LFW and CelebA, and (2) do not compare their\nobserved algorithmic bias to the biases of their human alternatives. In this\npaper, we release improvements to the LFW and CelebA datasets which will enable\nfuture researchers to obtain measurements of algorithmic bias that are not\ntainted by major flaws in the dataset (e.g. identical images appearing in both\nthe gallery and test set). We also use these new data to develop a series of\nchallenging facial identification and verification questions that we\nadministered to various algorithms and a large, balanced sample of human\nreviewers. We find that both computer models and human survey participants\nperform significantly better at the verification task, generally obtain lower\naccuracy rates on dark-skinned or female subjects for both tasks, and obtain\nhigher accuracy rates when their demographics match that of the question.\nComputer models are observed to achieve a higher level of accuracy than the\nsurvey participants on both tasks and exhibit bias to similar degrees as the\nhuman survey participants.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dooley_S/0/1/0/all/0/1\">Samuel Dooley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Downing_R/0/1/0/all/0/1\">Ryan Downing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_G/0/1/0/all/0/1\">George Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shankar_N/0/1/0/all/0/1\">Nathan Shankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thymes_B/0/1/0/all/0/1\">Bradon Thymes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thorkelsdottir_G/0/1/0/all/0/1\">Gudrun Thorkelsdottir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurtz_Miott_T/0/1/0/all/0/1\">Tiye Kurtz-Miott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mattson_R/0/1/0/all/0/1\">Rachel Mattson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Obiwumi_O/0/1/0/all/0/1\">Olufemi Obiwumi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cherepanova_V/0/1/0/all/0/1\">Valeriia Cherepanova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldblum_M/0/1/0/all/0/1\">Micah Goldblum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dickerson_J/0/1/0/all/0/1\">John P Dickerson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1\">Tom Goldstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mind the Gap: Domain Gap Control for Single Shot Domain Adaptation for Generative Adversarial Networks. (arXiv:2110.08398v1 [cs.CV])","link":"http://arxiv.org/abs/2110.08398","description":"<p>We present a new method for one shot domain adaptation. The input to our\nmethod is trained GAN that can produce images in domain A and a single\nreference image I_B from domain B. The proposed algorithm can translate any\noutput of the trained GAN from domain A to domain B. There are two main\nadvantages of our method compared to the current state of the art: First, our\nsolution achieves higher visual quality, e.g. by noticeably reducing\noverfitting. Second, our solution allows for more degrees of freedom to control\nthe domain gap, i.e. what aspects of image I_B are used to define the domain B.\nTechnically, we realize the new method by building on a pre-trained StyleGAN\ngenerator as GAN and a pre-trained CLIP model for representing the domain gap.\nWe propose several new regularizers for controlling the domain gap to optimize\nthe weights of the pre-trained StyleGAN generator to output images in domain B\ninstead of domain A. The regularizers prevent the optimization from taking on\ntoo many attributes of the single reference image. Our results show significant\nvisual improvements over the state of the art as well as multiple applications\nthat highlight improved control.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_P/0/1/0/all/0/1\">Peihao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdal_R/0/1/0/all/0/1\">Rameen Abdal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Femiani_J/0/1/0/all/0/1\">John Femiani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wonka_P/0/1/0/all/0/1\">Peter Wonka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bridging the gap between paired and unpaired medical image translation. (arXiv:2110.08407v1 [eess.IV])","link":"http://arxiv.org/abs/2110.08407","description":"<p>Medical image translation has the potential to reduce the imaging workload,\nby removing the need to capture some sequences, and to reduce the annotation\nburden for developing machine learning methods. GANs have been used\nsuccessfully to translate images from one domain to another, such as MR to CT.\nAt present, paired data (registered MR and CT images) or extra supervision\n(e.g. segmentation masks) is needed to learn good translation models.\nRegistering multiple modalities or annotating structures within each of them is\na tedious and laborious task. Thus, there is a need to develop improved\ntranslation methods for unpaired data. Here, we introduce modified pix2pix\nmodels for tasks CT$\\rightarrow$MR and MR$\\rightarrow$CT, trained with unpaired\nCT and MR data, and MRCAT pairs generated from the MR scans. The proposed\nmodifications utilize the paired MR and MRCAT images to ensure good alignment\nbetween input and translated images, and unpaired CT images ensure the\nMR$\\rightarrow$CT model produces realistic-looking CT and CT$\\rightarrow$MR\nmodel works well with real CT as input. The proposed pix2pix variants\noutperform baseline pix2pix, pix2pixHD and CycleGAN in terms of FID and KID,\nand generate more realistic looking CT and MR translations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Paavilainen_P/0/1/0/all/0/1\">Pauliina Paavilainen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Akram_S/0/1/0/all/0/1\">Saad Ullah Akram</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kannala_J/0/1/0/all/0/1\">Juho Kannala</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dataset Knowledge Transfer for Class-Incremental Learning without Memory. (arXiv:2110.08421v1 [cs.CV])","link":"http://arxiv.org/abs/2110.08421","description":"<p>Incremental learning enables artificial agents to learn from sequential data.\nWhile important progress was made by exploiting deep neural networks,\nincremental learning remains very challenging. This is particularly the case\nwhen no memory of past data is allowed and catastrophic forgetting has a strong\nnegative effect. We tackle class-incremental learning without memory by\nadapting prediction bias correction, a method which makes predictions of past\nand new classes more comparable. It was proposed when a memory is allowed and\ncannot be directly used without memory, since samples of past classes are\nrequired. We introduce a two-step learning process which allows the transfer of\nbias correction parameters between reference and target datasets. Bias\ncorrection is first optimized offline on reference datasets which have an\nassociated validation memory. The obtained correction parameters are then\ntransferred to target datasets, for which no memory is available. The second\ncontribution is to introduce a finer modeling of bias correction by learning\nits parameters per incremental state instead of the usual past vs. new class\nmodeling. The proposed dataset knowledge transfer is applicable to any\nincremental method which works without memory. We test its effectiveness by\napplying it to four existing methods. Evaluation with four target datasets and\ndifferent configurations shows consistent improvement, with practically no\ncomputational and memory overhead.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Slim_H/0/1/0/all/0/1\">Habib Slim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belouadah_E/0/1/0/all/0/1\">Eden Belouadah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Popescu_A/0/1/0/all/0/1\">Adrian Popescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Onchis_D/0/1/0/all/0/1\">Darian Onchis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep learning-based detection of intravenous contrast in computed tomography scans. (arXiv:2110.08424v1 [eess.IV])","link":"http://arxiv.org/abs/2110.08424","description":"<p>Purpose: Identifying intravenous (IV) contrast use within CT scans is a key\ncomponent of data curation for model development and testing. Currently, IV\ncontrast is poorly documented in imaging metadata and necessitates manual\ncorrection and annotation by clinician experts, presenting a major barrier to\nimaging analyses and algorithm deployment. We sought to develop and validate a\nconvolutional neural network (CNN)-based deep learning (DL) platform to\nidentify IV contrast within CT scans. Methods: For model development and\nevaluation, we used independent datasets of CT scans of head, neck (HN) and\nlung cancer patients, totaling 133,480 axial 2D scan slices from 1,979 CT scans\nmanually annotated for contrast presence by clinical experts. Five different DL\nmodels were adopted and trained in HN training datasets for slice-level\ncontrast detection. Model performances were evaluated on a hold-out set and on\nan independent validation set from another institution. DL models was then\nfine-tuned on chest CT data and externally validated on a separate chest CT\ndataset. Results: Initial DICOM metadata tags for IV contrast were missing or\nerroneous in 1,496 scans (75.6%). The EfficientNetB4-based model showed the\nbest overall detection performance. For HN scans, AUC was 0.996 in the internal\nvalidation set (n = 216) and 1.0 in the external validation set (n = 595). The\nfine-tuned model on chest CTs yielded an AUC: 1.0 for the internal validation\nset (n = 53), and AUC: 0.980 for the external validation set (n = 402).\nConclusion: The DL model could accurately detect IV contrast in both HN and\nchest CT scans with near-perfect performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ye_Z/0/1/0/all/0/1\">Zezhong Ye</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qian_J/0/1/0/all/0/1\">Jack M. Qian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hosny_A/0/1/0/all/0/1\">Ahmed Hosny</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zeleznik_R/0/1/0/all/0/1\">Roman Zeleznik</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Plana_D/0/1/0/all/0/1\">Deborah Plana</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Likitlersuang_J/0/1/0/all/0/1\">Jirapat Likitlersuang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhongyi Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mak_R/0/1/0/all/0/1\">Raymond H. Mak</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Aerts_H/0/1/0/all/0/1\">Hugo J. W. L. Aerts</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kann_B/0/1/0/all/0/1\">Benjamin H. Kann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COVID-19 Detection in Chest X-ray Images Using Swin-Transformer and Transformer in Transformer. (arXiv:2110.08427v1 [eess.IV])","link":"http://arxiv.org/abs/2110.08427","description":"<p>The Coronavirus Disease 2019 (COVID-19) has spread globally and caused\nserious damages. Chest X-ray images are widely used for COVID-19 diagnosis and\nArtificial Intelligence method can assist to increase the efficiency and\naccuracy. In the Challenge of Chest XR COVID-19 detection in Ethics and\nExplainability for Responsible Data Science (EE-RDS) conference 2021, we\nproposed a method which combined Swin Transformer and Transformer in\nTransformer to classify chest X-ray images as three classes: COVID-19,\nPneumonia and Normal (healthy) and achieved 0.9475 accuracy on test set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Jiang_J/0/1/0/all/0/1\">Juntao Jiang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_S/0/1/0/all/0/1\">Shuyi Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TorchEsegeta: Framework for Interpretability and Explainability of Image-based Deep Learning Models. (arXiv:2110.08429v1 [cs.CV])","link":"http://arxiv.org/abs/2110.08429","description":"<p>Clinicians are often very sceptical about applying automatic image processing\napproaches, especially deep learning based methods, in practice. One main\nreason for this is the black-box nature of these approaches and the inherent\nproblem of missing insights of the automatically derived decisions. In order to\nincrease trust in these methods, this paper presents approaches that help to\ninterpret and explain the results of deep learning algorithms by depicting the\nanatomical areas which influence the decision of the algorithm most. Moreover,\nthis research presents a unified framework, TorchEsegeta, for applying various\ninterpretability and explainability techniques for deep learning models and\ngenerate visual interpretations and explanations for clinicians to corroborate\ntheir clinical findings. In addition, this will aid in gaining confidence in\nsuch methods. The framework builds on existing interpretability and\nexplainability techniques that are currently focusing on classification models,\nextending them to segmentation tasks. In addition, these methods have been\nadapted to 3D models for volumetric analysis. The proposed framework provides\nmethods to quantitatively compare visual explanations using infidelity and\nsensitivity metrics. This framework can be used by data scientists to perform\npost-hoc interpretations and explanations of their models, develop more\nexplainable tools and present the findings to clinicians to increase their\nfaith in such models. The proposed framework was evaluated based on a use case\nscenario of vessel segmentation models trained on Time-of-fight (TOF) Magnetic\nResonance Angiogram (MRA) images of the human brain. Quantitative and\nqualitative results of a comparative study of different models and\ninterpretability methods are presented. Furthermore, this paper provides an\nextensive overview of several existing interpretability and explainability\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chatterjee_S/0/1/0/all/0/1\">Soumick Chatterjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1\">Arnab Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mandal_C/0/1/0/all/0/1\">Chirag Mandal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukhopadhyay_B/0/1/0/all/0/1\">Budhaditya Mukhopadhyay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vipinraj_M/0/1/0/all/0/1\">Manish Vipinraj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shukla_A/0/1/0/all/0/1\">Aniruddh Shukla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_R/0/1/0/all/0/1\">Rajatha Nagaraja Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarasaen_C/0/1/0/all/0/1\">Chompunuch Sarasaen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Speck_O/0/1/0/all/0/1\">Oliver Speck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nurnberger_A/0/1/0/all/0/1\">Andreas N&#xfc;rnberger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Annotated Training for Controllable Image Captioning. (arXiv:2110.08446v1 [cs.AI])","link":"http://arxiv.org/abs/2110.08446","description":"<p>The Controllable Image Captioning (CIC) task aims to generate captions\nconditioned on designated control signals. In this paper, we improve CIC from\ntwo aspects: 1) Existing reinforcement training methods are not applicable to\nstructure-related CIC models due to the fact that the accuracy-based reward\nfocuses mainly on contents rather than semantic structures. The lack of\nreinforcement training prevents the model from generating more accurate and\ncontrollable sentences. To solve the problem above, we propose a novel\nreinforcement training method for structure-related CIC models: Self-Annotated\nTraining (SAT), where a recursive sampling mechanism (RSM) is designed to force\nthe input control signal to match the actual output sentence. Extensive\nexperiments conducted on MSCOCO show that our SAT method improves C-Transformer\n(XE) on CIDEr-D score from 118.6 to 130.1 in the length-control task and from\n132.2 to 142.7 in the tense-control task, while maintaining more than 99$\\%$\nmatching accuracy with the control signal. 2) We introduce a new control\nsignal: sentence quality. Equipped with it, CIC models are able to generate\ncaptions of different quality levels as needed. Experiments show that without\nadditional information of ground truth captions, models controlled by the\nhighest level of sentence quality perform much better in accuracy than baseline\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhangzi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianlei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_H/0/1/0/all/0/1\">Hong Qu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Joint 3D Human Shape Recovery from A Single Imag with Bilayer-Graph. (arXiv:2110.08472v1 [cs.CV])","link":"http://arxiv.org/abs/2110.08472","description":"<p>The ability to estimate the 3D human shape and pose from images can be useful\nin many contexts. Recent approaches have explored using graph convolutional\nnetworks and achieved promising results. The fact that the 3D shape is\nrepresented by a mesh, an undirected graph, makes graph convolutional networks\na natural fit for this problem. However, graph convolutional networks have\nlimited representation power. Information from nodes in the graph is passed to\nconnected neighbors, and propagation of information requires successive graph\nconvolutions. To overcome this limitation, we propose a dual-scale graph\napproach. We use a coarse graph, derived from a dense graph, to estimate the\nhuman's 3D pose, and the dense graph to estimate the 3D shape. Information in\ncoarse graphs can be propagated over longer distances compared to dense graphs.\nIn addition, information about pose can guide to recover local shape detail and\nvice versa. We recognize that the connection between coarse and dense is itself\na graph, and introduce graph fusion blocks to exchange information between\ngraphs with different scales. We train our model end-to-end and show that we\ncan achieve state-of-the-art results for several evaluation datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baar_J/0/1/0/all/0/1\">Jeroen van Baar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Siheng Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Good Prompt Is Worth Millions of Parameters? Low-resource Prompt-based Learning for Vision-Language Models. (arXiv:2110.08484v1 [cs.CV])","link":"http://arxiv.org/abs/2110.08484","description":"<p>Large pretrained vision-language (VL) models can learn a new task with a\nhandful of examples or generalize to a new task without fine-tuning. However,\nthese gigantic VL models are hard to deploy for real-world applications due to\ntheir impractically huge model size and slow inference speed. In this work, we\npropose FewVLM, a few-shot prompt-based learner on vision-language tasks. We\npretrain a sequence-to-sequence Transformer model with both prefix language\nmodeling (PrefixLM) and masked language modeling (MaskedLM), and introduce\nsimple prompts to improve zero-shot and few-shot performance on VQA and image\ncaptioning. Experimental results on five VQA and captioning datasets show that\n\\method\\xspace outperforms Frozen which is 31 times larger than ours by 18.2%\npoint on zero-shot VQAv2 and achieves comparable results to a 246$\\times$\nlarger model, PICa. We observe that (1) prompts significantly affect zero-shot\nperformance but marginally affect few-shot performance, (2) MaskedLM helps\nfew-shot VQA tasks while PrefixLM boosts captioning performance, and (3)\nperformance significantly increases when training set size is small.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_W/0/1/0/all/0/1\">Woojeong Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yu Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yelong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Procedural Knowledge by Sequencing Multimodal Instructional Manuals. (arXiv:2110.08486v1 [cs.CL])","link":"http://arxiv.org/abs/2110.08486","description":"<p>The ability to sequence unordered events is an essential skill to comprehend\nand reason about real world task procedures, which often requires thorough\nunderstanding of temporal common sense and multimodal information, as these\nprocedures are often communicated through a combination of texts and images.\nSuch capability is essential for applications such as sequential task planning\nand multi-source instruction summarization. While humans are capable of\nreasoning about and sequencing unordered multimodal procedural instructions,\nwhether current machine learning models have such essential capability is still\nan open question. In this work, we benchmark models' capability of reasoning\nover and sequencing unordered multimodal instructions by curating datasets from\npopular online instructional manuals and collecting comprehensive human\nannotations. We find models not only perform significantly worse than humans\nbut also seem incapable of efficiently utilizing the multimodal information. To\nimprove machines' performance on multimodal event sequencing, we propose\nsequentiality-aware pretraining techniques that exploit the sequential\nalignment properties of both texts and images, resulting in &gt; 5% significant\nimprovements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Te-Lin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spangher_A/0/1/0/all/0/1\">Alex Spangher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alipoormolabashi_P/0/1/0/all/0/1\">Pegah Alipoormolabashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freedman_M/0/1/0/all/0/1\">Marjorie Freedman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weischedel_R/0/1/0/all/0/1\">Ralph Weischedel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Grayscale Based Algorithm for Remote Sensing with Deep Learning. (arXiv:2110.08493v1 [cs.CV])","link":"http://arxiv.org/abs/2110.08493","description":"<p>Remote sensing is the image acquisition of a target without having physical\ncontact with it. Nowadays remote sensing data is widely preferred due to its\nreduced image acquisition period. The remote sensing of ground targets is more\nchallenging because of the various factors that affect the propagation of light\nthrough different mediums from a satellite acquisition. Several Convolutional\nNeural Network-based algorithms are being implemented in the field of remote\nsensing. Supervised learning is a machine learning technique where the data is\nlabelled according to their classes prior to the training. In order to detect\nand classify the targets more accurately, YOLOv3, an algorithm based on\nbounding and anchor boxes is adopted. In order to handle the various effects of\nlight travelling through the atmosphere, Grayscale based YOLOv3 configuration\nis introduced. For better prediction and for solving the Rayleigh scattering\neffect, RGB based grayscale algorithms are proposed. The acquired images are\nanalysed and trained with the grayscale based YOLO3 algorithm for target\ndetection. The results show that the grayscale-based method can sense the\ntarget more accurately and effectively than the traditional YOLOv3 approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+CS_S/0/1/0/all/0/1\">Sai Ganesh CS</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Y_A/0/1/0/all/0/1\">Aouthithiye Barathwaj SR Y</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azhagumurugan_R/0/1/0/all/0/1\">R. Azhagumurugan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+S_R/0/1/0/all/0/1\">R. Swethaa S</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hybrid Mutimodal Fusion for Dimensional Emotion Recognition. (arXiv:2110.08495v1 [cs.CV])","link":"http://arxiv.org/abs/2110.08495","description":"<p>In this paper, we extensively present our solutions for the MuSe-Stress\nsub-challenge and the MuSe-Physio sub-challenge of Multimodal Sentiment\nChallenge (MuSe) 2021. The goal of MuSe-Stress sub-challenge is to predict the\nlevel of emotional arousal and valence in a time-continuous manner from\naudio-visual recordings and the goal of MuSe-Physio sub-challenge is to predict\nthe level of psycho-physiological arousal from a) human annotations fused with\nb) galvanic skin response (also known as Electrodermal Activity (EDA)) signals\nfrom the stressed people. The Ulm-TSST dataset which is a novel subset of the\naudio-visual textual Ulm-Trier Social Stress dataset that features German\nspeakers in a Trier Social Stress Test (TSST) induced stress situation is used\nin both sub-challenges. For the MuSe-Stress sub-challenge, we highlight our\nsolutions in three aspects: 1) the audio-visual features and the bio-signal\nfeatures are used for emotional state recognition. 2) the Long Short-Term\nMemory (LSTM) with the self-attention mechanism is utilized to capture complex\ntemporal dependencies within the feature sequences. 3) the late fusion strategy\nis adopted to further boost the model's recognition performance by exploiting\ncomplementary information scattered across multimodal sequences. Our proposed\nmodel achieves CCC of 0.6159 and 0.4609 for valence and arousal respectively on\nthe test set, which both rank in the top 3. For the MuSe-Physio sub-challenge,\nwe first extract the audio-visual features and the bio-signal features from\nmultiple modalities. Then, the LSTM module with the self-attention mechanism,\nand the Gated Convolutional Neural Networks (GCNN) as well as the LSTM network\nare utilized for modeling the complex temporal dependencies in the sequence.\nFinally, the late fusion strategy is used. Our proposed method also achieves\nCCC of 0.5412 on the test set, which ranks in the top 3.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Ziyu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_F/0/1/0/all/0/1\">Fuyan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1\">Bin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shutao Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BAPGAN: GAN-based Bone Age Progression of Femur and Phalange X-ray Images. (arXiv:2110.08509v1 [eess.IV])","link":"http://arxiv.org/abs/2110.08509","description":"<p>Convolutional Neural Networks play a key role in bone age assessment for\ninvestigating endocrinology, genetic, and growth disorders under various\nmodalities and body regions. However, no researcher has tackled bone age\nprogression/regression despite its valuable potential applications:\nbone-related disease diagnosis, clinical knowledge acquisition, and museum\neducation. Therefore, we propose Bone Age Progression Generative Adversarial\nNetwork (BAPGAN) to progress/regress both femur/phalange X-ray images while\npreserving identity and realism. We exhaustively confirm the BAPGAN's clinical\npotential via Frechet Inception Distance, Visual Turing Test by two expert\northopedists, and t-Distributed Stochastic Neighbor Embedding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Nakazawa_S/0/1/0/all/0/1\">Shinji Nakazawa</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Han_C/0/1/0/all/0/1\">Changhee Han</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hasei_J/0/1/0/all/0/1\">Joe Hasei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nakahara_R/0/1/0/all/0/1\">Ryuichi Nakahara</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ozaki_T/0/1/0/all/0/1\">Toshifumi Ozaki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Dialogue Response Generation. (arXiv:2110.08515v1 [cs.CL])","link":"http://arxiv.org/abs/2110.08515","description":"<p>Responsing with image has been recognized as an important capability for an\nintelligent conversational agent. Yet existing works only focus on exploring\nthe multimodal dialogue models which depend on retrieval-based methods, but\nneglecting generation methods. To fill in the gaps, we first present a\nmultimodal dialogue generation model, which takes the dialogue history as\ninput, then generates a textual sequence or an image as response. Learning such\na model often requires multimodal dialogues containing both texts and images\nwhich are difficult to obtain. Motivated by the challenge in practice, we\nconsider multimodal dialogue generation under a natural assumption that only\nlimited training examples are available. In such a low-resource setting, we\ndevise a novel conversational agent, Divter, in order to isolate parameters\nthat depend on multimodal dialogues from the entire generation model. By this\nmeans, the major part of the model can be learned from a large number of\ntext-only dialogues and text-image pairs respectively, then the whole\nparameters can be well fitted using the limited training examples. Extensive\nexperiments demonstrate our method achieves state-of-the-art results in both\nautomatic and human evaluation, and can generate informative text and\nhigh-resolution image responses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1\">Qingfeng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yujing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Can Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_K/0/1/0/all/0/1\">Kai Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yaming Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Huang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1\">Fei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jessica Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1\">Xiubo Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Locally Adaptive Structure and Texture Similarity for Image Quality Assessment. (arXiv:2110.08521v1 [eess.IV])","link":"http://arxiv.org/abs/2110.08521","description":"<p>The latest advances in full-reference image quality assessment (IQA) involve\nunifying structure and texture similarity based on deep representations. The\nresulting Deep Image Structure and Texture Similarity (DISTS) metric, however,\nmakes rather global quality measurements, ignoring the fact that natural\nphotographic images are locally structured and textured across space and scale.\nIn this paper, we describe a locally adaptive structure and texture similarity\nindex for full-reference IQA, which we term A-DISTS. Specifically, we rely on a\nsingle statistical feature, namely the dispersion index, to localize texture\nregions at different scales. The estimated probability (of one patch being\ntexture) is in turn used to adaptively pool local structure and texture\nmeasurements. The resulting A-DISTS is adapted to local image content, and is\nfree of expensive human perceptual scores for supervised training. We\ndemonstrate the advantages of A-DISTS in terms of correlation with human data\non ten IQA databases and optimization of single image super-resolution methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ding_K/0/1/0/all/0/1\">Keyan Ding</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yi Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zou_X/0/1/0/all/0/1\">Xueyi Zou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1\">Shiqi Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ma_K/0/1/0/all/0/1\">Kede Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-View Stereo Network with attention thin volume. (arXiv:2110.08556v1 [cs.CV])","link":"http://arxiv.org/abs/2110.08556","description":"<p>We propose an efficient multi-view stereo (MVS) network for infering depth\nvalue from multiple RGB images. Recent studies have shown that mapping the\ngeometric relationship in real space to neural network is an essential topic of\nthe MVS problem. Specifically, these methods focus on how to express the\ncorrespondence between different views by constructing a nice cost volume. In\nthis paper, we propose a more complete cost volume construction approach based\non absorbing previous experience. First of all, we introduce the self-attention\nmechanism to fully aggregate the dominant information from input images and\naccurately model the long-range dependency, so as to selectively aggregate\nreference features. Secondly, we introduce the group-wise correlation to\nfeature aggregation, which greatly reduces the memory and calculation burden.\nMeanwhile, this method enhances the information interaction between different\nfeature channels. With this approach, a more lightweight and efficient cost\nvolume is constructed. Finally we follow the coarse to fine strategy and refine\nthe depth sampling range scale by scale with the help of uncertainty\nestimation. We further combine the previous steps to get the attention thin\nvolume. Quantitative and qualitative experiments are presented to demonstrate\nthe performance of our model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wan_Z/0/1/0/all/0/1\">Zihang Wan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Network Pruning Through Constrained Reinforcement Learning. (arXiv:2110.08558v1 [cs.CV])","link":"http://arxiv.org/abs/2110.08558","description":"<p>Network pruning reduces the size of neural networks by removing (pruning)\nneurons such that the performance drop is minimal. Traditional pruning\napproaches focus on designing metrics to quantify the usefulness of a neuron\nwhich is often quite tedious and sub-optimal. More recent approaches have\ninstead focused on training auxiliary networks to automatically learn how\nuseful each neuron is however, they often do not take computational limitations\ninto account. In this work, we propose a general methodology for pruning neural\nnetworks. Our proposed methodology can prune neural networks to respect\npre-defined computational budgets on arbitrary, possibly non-differentiable,\nfunctions. Furthermore, we only assume the ability to be able to evaluate these\nfunctions for different inputs, and hence they do not need to be fully\nspecified beforehand. We achieve this by proposing a novel pruning strategy via\nconstrained reinforcement learning algorithms. We prove the effectiveness of\nour approach via comparison with state-of-the-art methods on standard image\nclassification datasets. Specifically, we reduce 83-92.90 of total parameters\non various variants of VGG while achieving comparable or better performance\nthan that of original networks. We also achieved 75.09 reduction in parameters\non ResNet18 without incurring any loss in accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Malik_S/0/1/0/all/0/1\">Shehryar Malik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haider_M/0/1/0/all/0/1\">Muhammad Umair Haider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iqbal_O/0/1/0/all/0/1\">Omer Iqbal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taj_M/0/1/0/all/0/1\">Murtaza Taj</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BNAS v2: Learning Architectures for Binary Networks with Empirical Improvements. (arXiv:2110.08562v1 [cs.CV])","link":"http://arxiv.org/abs/2110.08562","description":"<p>Backbone architectures of most binary networks are well-known floating point\n(FP) architectures such as the ResNet family. Questioning that the\narchitectures designed for FP networks might not be the best for binary\nnetworks, we propose to search architectures for binary networks (BNAS) by\ndefining a new search space for binary architectures and a novel search\nobjective. Specifically, based on the cell based search method, we define the\nnew search space of binary layer types, design a new cell template, and\nrediscover the utility of and propose to use the Zeroise layer instead of using\nit as a placeholder. The novel search objective diversifies early search to\nlearn better performing binary architectures. We show that our method searches\narchitectures with stable training curves despite the quantization error\ninherent in binary networks. Quantitative analyses demonstrate that our\nsearched architectures outperform the architectures used in state-of-the-art\nbinary networks and outperform or perform on par with state-of-the-art binary\nnetworks that employ various techniques other than architectural changes. In\naddition, we further propose improvements to the training scheme of our\nsearched architectures. With the new training scheme for our searched\narchitectures, we achieve the state-of-the-art performance by binary networks\nby outperforming all previous methods by non-trivial margins.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Dahyun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_K/0/1/0/all/0/1\">Kunal Pratap Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jonghyun Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ASFormer: Transformer for Action Segmentation. (arXiv:2110.08568v1 [cs.CV])","link":"http://arxiv.org/abs/2110.08568","description":"<p>Algorithms for the action segmentation task typically use temporal models to\npredict what action is occurring at each frame for a minute-long daily\nactivity. Recent studies have shown the potential of Transformer in modeling\nthe relations among elements in sequential data. However, there are several\nmajor concerns when directly applying the Transformer to the action\nsegmentation task, such as the lack of inductive biases with small training\nsets, the deficit in processing long input sequence, and the limitation of the\ndecoder architecture to utilize temporal relations among multiple action\nsegments to refine the initial predictions. To address these concerns, we\ndesign an efficient Transformer-based model for action segmentation task, named\nASFormer, with three distinctive characteristics: (i) We explicitly bring in\nthe local connectivity inductive priors because of the high locality of\nfeatures. It constrains the hypothesis space within a reliable scope, and is\nbeneficial for the action segmentation task to learn a proper target function\nwith small training sets. (ii) We apply a pre-defined hierarchical\nrepresentation pattern that efficiently handles long input sequences. (iii) We\ncarefully design the decoder to refine the initial predictions from the\nencoder. Extensive experiments on three public datasets demonstrate that\neffectiveness of our methods. Code is available at\n\\url{https://github.com/ChinaYi/ASFormer}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yi_F/0/1/0/all/0/1\">Fangqiu Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_H/0/1/0/all/0/1\">Hongyu Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1\">Tingting Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Image Debanding. (arXiv:2110.08569v1 [eess.IV])","link":"http://arxiv.org/abs/2110.08569","description":"<p>Banding or false contour is an annoying visual artifact whose impact is even\nmore pronounced in ultra high definition, high dynamic range, and wide colour\ngamut visual content, which is becoming increasingly popular. Since users\nassociate a heightened expectation of quality with such content and banding\nleads to deteriorated visual quality-of-experience, the area of banding removal\nor debanding has taken paramount importance. Existing debanding approaches are\nmostly knowledge-driven. Despite the widespread success of deep learning in\nother areas of image processing and computer vision, data-driven debanding\napproaches remain surprisingly missing. In this work, we make one of the first\nattempts to develop a deep learning based banding artifact removal method for\nimages and name it deep debanding network (deepDeband). For its training, we\nconstruct a large-scale dataset of 51,490 pairs of corresponding pristine and\nbanded image patches. Performance evaluation shows that deepDeband is\nsuccessful at greatly reducing banding artifacts in images, outperforming\nexisting methods both quantitatively and visually.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhou_R/0/1/0/all/0/1\">Raymond Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Athar_S/0/1/0/all/0/1\">Shahrukh Athar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1\">Zhongling Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1\">Zhou Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explore before Moving: A Feasible Path Estimation and Memory Recalling Framework for Embodied Navigation. (arXiv:2110.08571v1 [cs.CV])","link":"http://arxiv.org/abs/2110.08571","description":"<p>An embodied task such as embodied question answering (EmbodiedQA), requires\nan agent to explore the environment and collect clues to answer a given\nquestion that related with specific objects in the scene. The solution of such\ntask usually includes two stages, a navigator and a visual Q&amp;A module. In this\npaper, we focus on the navigation and solve the problem of existing navigation\nalgorithms lacking experience and common sense, which essentially results in a\nfailure finding target when robot is spawn in unknown environments.\n</p>\n<p>Inspired by the human ability to think twice before moving and conceive\nseveral feasible paths to seek a goal in unfamiliar scenes, we present a route\nplanning method named Path Estimation and Memory Recalling (PEMR) framework.\nPEMR includes a \"looking ahead\" process, \\textit{i.e.} a visual feature\nextractor module that estimates feasible paths for gathering 3D navigational\ninformation, which is mimicking the human sense of direction. PEMR contains\nanother process ``looking behind'' process that is a memory recall mechanism\naims at fully leveraging past experience collected by the feature extractor.\nLast but not the least, to encourage the navigator to learn more accurate prior\nexpert experience, we improve the original benchmark dataset and provide a\nfamily of evaluation metrics for diagnosing both navigation and question\nanswering modules. We show strong experimental results of PEMR on the\nEmbodiedQA navigation task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Shirui Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guanbin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Liang Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual-aware Attention Dual-stream Decoder for Video Captioning. (arXiv:2110.08578v1 [cs.CV])","link":"http://arxiv.org/abs/2110.08578","description":"<p>Video captioning is a challenging task that captures different visual parts\nand describes them in sentences, for it requires visual and linguistic\ncoherence. The attention mechanism in the current video captioning method\nlearns to assign weight to each frame, promoting the decoder dynamically. This\nmay not explicitly model the correlation and the temporal coherence of the\nvisual features extracted in the sequence frames.To generate semantically\ncoherent sentences, we propose a new Visual-aware Attention (VA) model, which\nconcatenates dynamic changes of temporal sequence frames with the words at the\nprevious moment, as the input of attention mechanism to extract sequence\nfeatures.In addition, the prevalent approaches widely use the teacher-forcing\n(TF) learning during training, where the next token is generated conditioned on\nthe previous ground-truth tokens. The semantic information in the previously\ngenerated tokens is lost. Therefore, we design a self-forcing (SF) stream that\ntakes the semantic information in the probability distribution of the previous\ntoken as input to enhance the current token.The Dual-stream Decoder (DD)\narchitecture unifies the TF and SF streams, generating sentences to promote the\nannotated captioning for both streams.Meanwhile, with the Dual-stream Decoder\nutilized, the exposure bias problem is alleviated, caused by the discrepancy\nbetween the training and testing in the TF learning.The effectiveness of the\nproposed Visual-aware Attention Dual-stream Decoder (VADD) is demonstrated\nthrough the result of experimental studies on Microsoft video description\n(MSVD) corpus and MSR-Video to text (MSR-VTT) datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zhixin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_X/0/1/0/all/0/1\">Xian Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shuqin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_L/0/1/0/all/0/1\">Luo Zhong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Intelligent Video Editing: Incorporating Modern Talking Face Generation Algorithms in a Video Editor. (arXiv:2110.08580v1 [cs.CV])","link":"http://arxiv.org/abs/2110.08580","description":"<p>This paper proposes a video editor based on OpenShot with several\nstate-of-the-art facial video editing algorithms as added functionalities. Our\neditor provides an easy-to-use interface to apply modern lip-syncing algorithms\ninteractively. Apart from lip-syncing, the editor also uses audio and facial\nre-enactment to generate expressive talking faces. The manual control improves\nthe overall experience of video editing without missing out on the benefits of\nmodern synthetic video generation algorithms. This control enables us to\nlip-sync complex dubbed movie scenes, interviews, television shows, and other\nvisual content. Furthermore, our editor provides features that automatically\ntranslate lectures from spoken content, lip-sync of the professor, and\nbackground content like slides. While doing so, we also tackle the critical\naspect of synchronizing background content with the translated speech. We\nqualitatively evaluate the usefulness of the proposed editor by conducting\nhuman evaluations. Our evaluations show a clear improvement in the efficiency\nof using human editors and an improved video generation quality. We attach demo\nvideos with the supplementary material clearly explaining the tool and also\nshowcasing multiple results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Anchit Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1\">Faizan Farooq Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukhopadhyay_R/0/1/0/all/0/1\">Rudrabha Mukhopadhyay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Namboodiri_V/0/1/0/all/0/1\">Vinay P. Namboodiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jawahar_C/0/1/0/all/0/1\">C. V. Jawahar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pseudo-label refinement using superpixels for semi-supervised brain tumour segmentation. (arXiv:2110.08589v1 [cs.CV])","link":"http://arxiv.org/abs/2110.08589","description":"<p>Training neural networks using limited annotations is an important problem in\nthe medical domain. Deep Neural Networks (DNNs) typically require large,\nannotated datasets to achieve acceptable performance which, in the medical\ndomain, are especially difficult to obtain as they require significant time\nfrom expert radiologists. Semi-supervised learning aims to overcome this\nproblem by learning segmentations with very little annotated data, whilst\nexploiting large amounts of unlabelled data. However, the best-known technique,\nwhich utilises inferred pseudo-labels, is vulnerable to inaccurate\npseudo-labels degrading the performance. We propose a framework based on\nsuperpixels - meaningful clusters of adjacent pixels - to improve the accuracy\nof the pseudo labels and address this issue. Our framework combines superpixels\nwith semi-supervised learning, refining the pseudo-labels during training using\nthe features and edges of the superpixel maps. This method is evaluated on a\nmultimodal magnetic resonance imaging (MRI) dataset for the task of brain\ntumour region segmentation. Our method demonstrates improved performance over\nthe standard semi-supervised pseudo-labelling baseline when there is a reduced\nannotator burden and only 5 annotated patients are available. We report\nDSC=0.824 and DSC=0.707 for the test set whole tumour and tumour core regions\nrespectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thompson_B/0/1/0/all/0/1\">Bethany H. Thompson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caterina_G/0/1/0/all/0/1\">Gaetano Di Caterina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Voisey_J/0/1/0/all/0/1\">Jeremy P. Voisey</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated Remote Sensing Forest Inventory Using Satelite Imagery. (arXiv:2110.08590v1 [cs.CV])","link":"http://arxiv.org/abs/2110.08590","description":"<p>For many countries like Russia, Canada, or the USA, a robust and detailed\ntree species inventory is essential to manage their forests sustainably. Since\none can not apply unmanned aerial vehicle (UAV) imagery-based approaches to\nlarge-scale forest inventory applications, the utilization of machine learning\nalgorithms on satellite imagery is a rising topic of research. Although\nsatellite imagery quality is relatively low, additional spectral channels\nprovide a sufficient amount of information for tree crown classification tasks.\nAssuming that tree crowns are detected already, we use embeddings of tree\ncrowns generated by Autoencoders as a data set to train classical Machine\nLearning algorithms. We compare our Autoencoder (AE) based approach to\ntraditional convolutional neural networks (CNN) end-to-end classifiers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shtanchaev_A/0/1/0/all/0/1\">Abduragim Shtanchaev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bille_A/0/1/0/all/0/1\">Artur Bille</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sutyrina_O/0/1/0/all/0/1\">Olga Sutyrina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elelimy_S/0/1/0/all/0/1\">Sara Elelimy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A MIMO Radar-based Few-Shot Learning Approach for Human-ID. (arXiv:2110.08595v1 [eess.SP])","link":"http://arxiv.org/abs/2110.08595","description":"<p>Radar for deep learning-based human identification has become a research area\nof increasing interest. It has been shown that micro-Doppler (\\(\\upmu\\)-D) can\nreflect the walking behavior through capturing the periodic limbs'\nmicro-motions. One of the main aspects is maximizing the number of included\nclasses while considering the real-time and training dataset size constraints.\nIn this paper, a multiple-input-multiple-output (MIMO) radar is used to\nformulate micro-motion spectrograms of the elevation angular velocity\n(\\(\\upmu\\)-\\(\\omega\\)). The effectiveness of concatenating this\nnewly-formulated spectrogram with the commonly used \\(\\upmu\\)-D is\ninvestigated. To accommodate for non-constrained real walking motion, an\nadaptive cycle segmentation framework is utilized and a metric learning network\nis trained on half gait cycles (\\(\\approx\\) 0.5 s). Studies on the effects of\nvarious numbers of classes (5--20), different dataset sizes, and varying\nobservation time windows 1--2 s are conducted. A non-constrained walking\ndataset of 22 subjects is collected with different aspect angles with respect\nto the radar. The proposed few-shot learning (FSL) approach achieves a\nclassification error of 11.3 % with only 2 min of training data per subject.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Weller_P/0/1/0/all/0/1\">Pascal Weller</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Aziz_F/0/1/0/all/0/1\">Fady Aziz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Abdulatif_S/0/1/0/all/0/1\">Sherif Abdulatif</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schneider_U/0/1/0/all/0/1\">Urs Schneider</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huber_M/0/1/0/all/0/1\">Marco F. Huber</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mapping illegal waste dumping sites with neural-network classification of satellite imagery. (arXiv:2110.08599v1 [cs.LG])","link":"http://arxiv.org/abs/2110.08599","description":"<p>Public health and habitat quality are crucial goals of urban planning. In\nrecent years, the severe social and environmental impact of illegal waste\ndumping sites has made them one of the most serious problems faced by cities in\nthe Global South, in a context of scarce information available for decision\nmaking. To help identify the location of dumping sites and track their\nevolution over time we adopt a data-driven model from the machine learning\ndomain, analyzing satellite images. This allows us to take advantage of the\nincreasing availability of geo-spatial open-data, high-resolution satellite\nimagery, and open source tools to train machine learning algorithms with a\nsmall set of known waste dumping sites in Buenos Aires, and then predict the\nlocation of other sites over vast areas at high speed and low cost. This case\nstudy shows the results of a collaboration between Dymaxion Labs and\nFundaci\\'on Bunge y Born to harness this technique in order to create a\ncomprehensive map of potential locations of illegal waste dumping sites in the\nregion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Devesa/0/1/0/all/0/1\">Devesa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roberta_M/0/1/0/all/0/1\">Maria Roberta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brust_V/0/1/0/all/0/1\">Vazquez Brust</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Antonio_H/0/1/0/all/0/1\">H. Antonio</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MAAD: A Model and Dataset for \"Attended Awareness\" in Driving. (arXiv:2110.08610v1 [cs.HC])","link":"http://arxiv.org/abs/2110.08610","description":"<p>We propose a computational model to estimate a person's attended awareness of\ntheir environment. We define attended awareness to be those parts of a\npotentially dynamic scene which a person has attended to in recent history and\nwhich they are still likely to be physically aware of. Our model takes as input\nscene information in the form of a video and noisy gaze estimates, and outputs\nvisual saliency, a refined gaze estimate, and an estimate of the person's\nattended awareness. In order to test our model, we capture a new dataset with a\nhigh-precision gaze tracker including 24.5 hours of gaze sequences from 23\nsubjects attending to videos of driving scenes. The dataset also contains\nthird-party annotations of the subjects' attended awareness based on\nobservations of their scan path. Our results show that our model is able to\nreasonably estimate attended awareness in a controlled setting, and in the\nfuture could potentially be extended to real egocentric driving data to help\nenable more effective ahead-of-time warnings in safety systems and thereby\naugment driver performance. We also demonstrate our model's effectiveness on\nthe tasks of saliency, gaze calibration, and denoising, using both our dataset\nand an existing saliency dataset. We make our model and dataset available at\nhttps://github.com/ToyotaResearchInstitute/att-aware/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gopinath_D/0/1/0/all/0/1\">Deepak Gopinath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosman_G/0/1/0/all/0/1\">Guy Rosman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stent_S/0/1/0/all/0/1\">Simon Stent</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Terahata_K/0/1/0/all/0/1\">Katsuya Terahata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fletcher_L/0/1/0/all/0/1\">Luke Fletcher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Argall_B/0/1/0/all/0/1\">Brenna Argall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leonard_J/0/1/0/all/0/1\">John Leonard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SAGAN: Adversarial Spatial-asymmetric Attention for Noisy Nona-Bayer Reconstruction. (arXiv:2110.08619v1 [eess.IV])","link":"http://arxiv.org/abs/2110.08619","description":"<p>Nona-Bayer colour filter array (CFA) pattern is considered one of the most\nviable alternatives to traditional Bayer patterns. Despite the substantial\nadvantages, such non-Bayer CFA patterns are susceptible to produce visual\nartefacts while reconstructing RGB images from noisy sensor data. This study\naddresses the challenges of learning RGB image reconstruction from noisy\nNona-Bayer CFA comprehensively. We propose a novel spatial-asymmetric attention\nmodule to jointly learn bi-direction transformation and large-kernel global\nattention to reduce the visual artefacts. We combine our proposed module with\nadversarial learning to produce plausible images from Nona-Bayer CFA. The\nfeasibility of the proposed method has been verified and compared with the\nstate-of-the-art image reconstruction method. The experiments reveal that the\nproposed method can reconstruct RGB images from noisy Nona-Bayer CFA without\nproducing any visually disturbing artefacts. Also, it can outperform the\nstate-of-the-art image reconstruction method in both qualitative and\nquantitative comparison. Code available:\nhttps://github.com/sharif-apu/SAGAN_BMVC21.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Sharif_S/0/1/0/all/0/1\">S M A Sharif</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Naqvi_R/0/1/0/all/0/1\">Rizwan Ali Naqvi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Biswas_M/0/1/0/all/0/1\">Mithun Biswas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DPC: Unsupervised Deep Point Correspondence via Cross and Self Construction. (arXiv:2110.08636v1 [cs.CV])","link":"http://arxiv.org/abs/2110.08636","description":"<p>We present a new method for real-time non-rigid dense correspondence between\npoint clouds based on structured shape construction. Our method, termed Deep\nPoint Correspondence (DPC), requires a fraction of the training data compared\nto previous techniques and presents better generalization capabilities. Until\nnow, two main approaches have been suggested for the dense correspondence\nproblem. The first is a spectral-based approach that obtains great results on\nsynthetic datasets but requires mesh connectivity of the shapes and long\ninference processing time while being unstable in real-world scenarios. The\nsecond is a spatial approach that uses an encoder-decoder framework to regress\nan ordered point cloud for the matching alignment from an irregular input.\nUnfortunately, the decoder brings considerable disadvantages, as it requires a\nlarge amount of training data and struggles to generalize well in cross-dataset\nevaluations. DPC's novelty lies in its lack of a decoder component. Instead, we\nuse latent similarity and the input coordinates themselves to construct the\npoint cloud and determine correspondence, replacing the coordinate regression\ndone by the decoder. Extensive experiments show that our construction scheme\nleads to a performance boost in comparison to recent state-of-the-art\ncorrespondence methods. Our code is publicly available at\nhttps://github.com/dvirginz/DPC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lang_I/0/1/0/all/0/1\">Itai Lang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ginzburg_D/0/1/0/all/0/1\">Dvir Ginzburg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avidan_S/0/1/0/all/0/1\">Shai Avidan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raviv_D/0/1/0/all/0/1\">Dan Raviv</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Face Verification with Challenging Imposters and Diversified Demographics. (arXiv:2110.08667v1 [cs.CV])","link":"http://arxiv.org/abs/2110.08667","description":"<p>Face verification aims to distinguish between genuine and imposter pairs of\nfaces, which include the same or different identities, respectively. The\nperformance reported in recent years gives the impression that the task is\npractically solved. Here, we revisit the problem and argue that existing\nevaluation datasets were built using two oversimplifying design choices. First,\nthe usual identity selection to form imposter pairs is not challenging enough\nbecause, in practice, verification is needed to detect challenging imposters.\nSecond, the underlying demographics of existing datasets are often insufficient\nto account for the wide diversity of facial characteristics of people from\nacross the world. To mitigate these limitations, we introduce the $FaVCI2D$\ndataset. Imposter pairs are challenging because they include visually similar\nfaces selected from a large pool of demographically diversified identities. The\ndataset also includes metadata related to gender, country and age to facilitate\nfine-grained analysis of results. $FaVCI2D$ is generated from freely\ndistributable resources. Experiments with state-of-the-art deep models that\nprovide nearly 100\\% performance on existing datasets show a significant\nperformance drop for $FaVCI2D$, confirming our starting hypothesis. Equally\nimportant, we analyze legal and ethical challenges which appeared in recent\nyears and hindered the development of face analysis research. We introduce a\nseries of design choices which address these challenges and make the dataset\nconstitution and usage more sustainable and fairer. $FaVCI2D$ is available\nat~\\url{https://github.com/AIMultimediaLab/FaVCI2D-Face-Verification-with-Challenging-Imposters-and-Diversified-Demographics}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Popescu_A/0/1/0/all/0/1\">Adrian Popescu</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Stefan_L/0/1/0/all/0/1\">Liviu-Daniel &#x15e;tefan</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Deshayes_Chossart_J/0/1/0/all/0/1\">J&#xe9;r&#xf4;me Deshayes-Chossart</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Ionescu_B/0/1/0/all/0/1\">Bogdan Ionescu</a> (2) ((1) Universit&#xe9; Paris-Saclay, CEA, List, Palaiseau, France, (2) University Politehnica of Bucharest, Romania)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Resource Efficient 3D Convolutional Neural Networks. (arXiv:1904.02422v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1904.02422","description":"<p>Recently, convolutional neural networks with 3D kernels (3D CNNs) have been\nvery popular in computer vision community as a result of their superior ability\nof extracting spatio-temporal features within video frames compared to 2D CNNs.\nAlthough there has been great advances recently to build resource efficient 2D\nCNN architectures considering memory and power budget, there is hardly any\nsimilar resource efficient architectures for 3D CNNs. In this paper, we have\nconverted various well-known resource efficient 2D CNNs to 3D CNNs and\nevaluated their performance on three major benchmarks in terms of\nclassification accuracy for different complexity levels. We have experimented\non (1) Kinetics-600 dataset to inspect their capacity to learn, (2) Jester\ndataset to inspect their ability to capture motion patterns, and (3) UCF-101 to\ninspect the applicability of transfer learning. We have evaluated the run-time\nperformance of each model on a single Titan XP GPU and a Jetson TX2 embedded\nsystem. The results of this study show that these models can be utilized for\ndifferent types of real-world applications since they provide real-time\nperformance with considerable accuracies and memory usage. Our analysis on\ndifferent complexity levels shows that the resource efficient 3D CNNs should\nnot be designed too shallow or narrow in order to save complexity. The codes\nand pretrained models used in this work are publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kopuklu_O/0/1/0/all/0/1\">Okan K&#xf6;p&#xfc;kl&#xfc;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kose_N/0/1/0/all/0/1\">Neslihan Kose</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunduz_A/0/1/0/all/0/1\">Ahmet Gunduz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rigoll_G/0/1/0/all/0/1\">Gerhard Rigoll</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"You Only Watch Once: A Unified CNN Architecture for Real-Time Spatiotemporal Action Localization. (arXiv:1911.06644v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1911.06644","description":"<p>Spatiotemporal action localization requires the incorporation of two sources\nof information into the designed architecture: (1) temporal information from\nthe previous frames and (2) spatial information from the key frame. Current\nstate-of-the-art approaches usually extract these information with separate\nnetworks and use an extra mechanism for fusion to get detections. In this work,\nwe present YOWO, a unified CNN architecture for real-time spatiotemporal action\nlocalization in video streams. YOWO is a single-stage architecture with two\nbranches to extract temporal and spatial information concurrently and predict\nbounding boxes and action probabilities directly from video clips in one\nevaluation. Since the whole architecture is unified, it can be optimized\nend-to-end. The YOWO architecture is fast providing 34 frames-per-second on\n16-frames input clips and 62 frames-per-second on 8-frames input clips, which\nis currently the fastest state-of-the-art architecture on spatiotemporal action\nlocalization task. Remarkably, YOWO outperforms the previous state-of-the art\nresults on J-HMDB-21 and UCF101-24 with an impressive improvement of ~3% and\n~12%, respectively. Moreover, YOWO is the first and only single-stage\narchitecture that provides competitive results on AVA dataset. We make our code\nand pretrained models publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kopuklu_O/0/1/0/all/0/1\">Okan K&#xf6;p&#xfc;kl&#xfc;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xiangyu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rigoll_G/0/1/0/all/0/1\">Gerhard Rigoll</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Nonparametric Continuous Sensor Registration. (arXiv:2001.04286v4 [math.OC] UPDATED)","link":"http://arxiv.org/abs/2001.04286","description":"<p>This paper develops a new mathematical framework that enables nonparametric\njoint semantic and geometric representation of continuous functions using data.\nThe joint embedding is modeled by representing the processes in a reproducing\nkernel Hilbert space. The functions can be defined on arbitrary smooth\nmanifolds where the action of a Lie group aligns them. The continuous functions\nallow the registration to be independent of a specific signal resolution. The\nframework is fully analytical with a closed-form derivation of the Riemannian\ngradient and Hessian. We study a more specialized but widely used case where\nthe Lie group acts on functions isometrically. We solve the problem by\nmaximizing the inner product between two functions defined over data, while the\ncontinuous action of the rigid body motion Lie group is captured through the\nintegration of the flow in the corresponding Lie algebra. Low-dimensional cases\nare derived with numerical examples to show the generality of the proposed\nframework. The high-dimensional derivation for the special Euclidean group\nacting on the Euclidean space showcases the point cloud registration and\nbird's-eye view map registration abilities. An implementation of this framework\nfor RGB-D cameras outperforms the state-of-the-art robust visual odometry and\nperforms well in texture and structure-scarce environments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/math/1/au:+Clark_W/0/1/0/all/0/1\">William Clark</a>, <a href=\"http://arxiv.org/find/math/1/au:+Ghaffari_M/0/1/0/all/0/1\">Maani Ghaffari</a>, <a href=\"http://arxiv.org/find/math/1/au:+Bloch_A/0/1/0/all/0/1\">Anthony Bloch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the benefits of defining vicinal distributions in latent space. (arXiv:2003.06566v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2003.06566","description":"<p>The vicinal risk minimization (VRM) principle is an empirical risk\nminimization (ERM) variant that replaces Dirac masses with vicinal functions.\nThere is strong numerical and theoretical evidence showing that VRM outperforms\nERM in terms of generalization if appropriate vicinal functions are chosen.\nMixup Training (MT), a popular choice of vicinal distribution, improves the\ngeneralization performance of models by introducing globally linear behavior in\nbetween training examples. Apart from generalization, recent works have shown\nthat mixup trained models are relatively robust to input\nperturbations/corruptions and at the same time are calibrated better than their\nnon-mixup counterparts. In this work, we investigate the benefits of defining\nthese vicinal distributions like mixup in latent space of generative models\nrather than in input space itself. We propose a new approach - \\textit{VarMixup\n(Variational Mixup)} - to better sample mixup images by using the latent\nmanifold underlying the data. Our empirical studies on CIFAR-10, CIFAR-100, and\nTiny-ImageNet demonstrate that models trained by performing mixup in the latent\nmanifold learned by VAEs are inherently more robust to various input\ncorruptions/perturbations, are significantly better calibrated, and exhibit\nmore local-linear loss landscapes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mangla_P/0/1/0/all/0/1\">Puneet Mangla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_V/0/1/0/all/0/1\">Vedant Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Havaldar_S/0/1/0/all/0/1\">Shreyas Jayant Havaldar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_V/0/1/0/all/0/1\">Vineeth N Balasubramanian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modulating Bottom-Up and Top-Down Visual Processing via Language-Conditional Filters. (arXiv:2003.12739v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2003.12739","description":"<p>How to best integrate linguistic and perceptual processing in multi-modal\ntasks that involve language and vision is an important open problem. In this\nwork, we argue that the common practice of using language in a top-down manner,\nto direct visual attention over high-level visual features, may not be optimal.\nWe hypothesize that the use of language to also condition the bottom-up\nprocessing from pixels to high-level features can provide benefits to the\noverall performance. To support our claim, we propose a model for\nlanguage-vision problems involving dense prediction, and perform experiments on\ntwo different multi-modal tasks: image segmentation from referring expressions\nand language-guided image colorization. We compare results where either one or\nboth of the top-down and bottom-up visual branches are conditioned on language.\nOur experiments reveal that using language to control the filters for bottom-up\nvisual processing in addition to top-down attention leads to better results on\nboth tasks and achieves state-of-the-art performance. Our analysis of different\nword types in input expressions suggest that the bottom-up conditioning is\nespecially helpful in the presence of low level visual concepts like color.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kesen_I/0/1/0/all/0/1\">&#x130;lker Kesen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Can_O/0/1/0/all/0/1\">Ozan Arkan Can</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erdem_E/0/1/0/all/0/1\">Erkut Erdem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erdem_A/0/1/0/all/0/1\">Aykut Erdem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuret_D/0/1/0/all/0/1\">Deniz Yuret</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Speak2Label: Using Domain Knowledge for Creating a Large Scale Driver Gaze Zone Estimation Dataset. (arXiv:2004.05973v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2004.05973","description":"<p>Labelling of human behavior analysis data is a complex and time consuming\ntask. In this paper, a fully automatic technique for labelling an image based\ngaze behavior dataset for driver gaze zone estimation is proposed. Domain\nknowledge is added to the data recording paradigm and later labels are\ngenerated in an automatic manner using Speech To Text conversion (STT). In\norder to remove the noise in the STT process due to different illumination and\nethnicity of subjects in our data, the speech frequency and energy are\nanalysed. The resultant Driver Gaze in the Wild (DGW) dataset contains 586\nrecordings, captured during different times of the day including evenings. The\nlarge scale dataset contains 338 subjects with an age range of 18-63 years. As\nthe data is recorded in different lighting conditions, an illumination robust\nlayer is proposed in the Convolutional Neural Network (CNN). The extensive\nexperiments show the variance in the dataset resembling real-world conditions\nand the effectiveness of the proposed CNN pipeline. The proposed network is\nalso fine-tuned for the eye gaze prediction task, which shows the\ndiscriminativeness of the representation learnt by our network on the proposed\nDGW dataset. Project Page:\nhttps://sites.google.com/view/drivergazeprediction/home\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Shreya Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhall_A/0/1/0/all/0/1\">Abhinav Dhall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_G/0/1/0/all/0/1\">Garima Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1\">Sarthak Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1\">Nicu Sebe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Noisy Differentiable Architecture Search. (arXiv:2005.03566v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2005.03566","description":"<p>Simplicity is the ultimate sophistication. Differentiable Architecture Search\n(DARTS) has now become one of the mainstream paradigms of neural architecture\nsearch. However, it largely suffers from the well-known performance collapse\nissue due to the aggregation of skip connections. It is thought to have overly\nbenefited from the residual structure which accelerates the information flow.\nTo weaken this impact, we propose to inject unbiased random noise to impede the\nflow. We name this novel approach NoisyDARTS. In effect, a network optimizer\nshould perceive this difficulty at each training step and refrain from\novershooting, especially on skip connections. In the long run, since we add no\nbias to the gradient in terms of expectation, it is still likely to converge to\nthe right solution area. We also prove that the injected noise plays a role in\nsmoothing the loss landscape, which makes the optimization easier. Our method\nfeatures extreme simplicity and acts as a new strong baseline. We perform\nextensive experiments across various search spaces, datasets, and tasks, where\nwe robustly achieve state-of-the-art results. Our code is available at\nhttps://github.com/xiaomi-automl/NoisyDARTS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1\">Xiangxiang Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bo Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Generative Model for Texture Synthesis based on Optimal Transport between Feature Distributions. (arXiv:2007.03408v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2007.03408","description":"<p>We propose GOTEX, a general framework for texture synthesis by optimization\nthat constrains the statistical distribution of local features. While our model\nencompasses several existing texture models, we focus on the case where the\ncomparison between feature distributions relies on optimal transport distances.\nWe show that the semi-dual formulation of optimal transport allows to control\nthe distribution of various possible features, even if these features live in a\nhigh-dimensional space. We then study the resulting minimax optimization\nproblem, which corresponds to a Wasserstein generative model, for which the\ninner concave maximization problem can be solved with standard stochastic\ngradient methods. The alternate optimization algorithm is shown to be versatile\nin terms of applications, features and architecture; in particular it allows to\nproduce high-quality synthesized textures with different sets of features. We\nanalyze the results obtained by constraining the distribution of patches or the\ndistribution of responses to a pre-learned VGG neural network. We show that the\npatch representation can retrieve the desired textural aspect in a more precise\nmanner. We also provide a detailed comparison with state-of-the-art texture\nsynthesis methods. The GOTEX model based on patch features is also adapted to\ntexture inpainting and texture interpolation. Finally, we show how to use our\nframework to learn a feed-forward neural network that can synthesize on-the-fly\nnew textures of arbitrary size in a very fast manner. Experimental results and\ncomparisons with the mainstream methods from the literature illustrate the\nrelevance of the generative models learned with GOTEX.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Houdard_A/0/1/0/all/0/1\">Antoine Houdard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leclaire_A/0/1/0/all/0/1\">Arthur Leclaire</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papadakis_N/0/1/0/all/0/1\">Nicolas Papadakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rabin_J/0/1/0/all/0/1\">Julien Rabin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dissected 3D CNNs: Temporal Skip Connections for Efficient Online Video Processing. (arXiv:2009.14639v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2009.14639","description":"<p>Convolutional Neural Networks with 3D kernels (3D-CNNs) currently achieve\nstate-of-the-art results in video recognition tasks due to their supremacy in\nextracting spatiotemporal features within video frames. There have been many\nsuccessful 3D-CNN architectures surpassing the state-of-the-art results\nsuccessively. However, nearly all of them are designed to operate offline\ncreating several serious handicaps during online operation. Firstly,\nconventional 3D-CNNs are not dynamic since their output features represent the\ncomplete input clip instead of the most recent frame in the clip. Secondly,\nthey are not temporal resolution-preserving due to their inherent temporal\ndownsampling. Lastly, 3D-CNNs are constrained to be used with fixed temporal\ninput size limiting their flexibility. In order to address these drawbacks, we\npropose dissected 3D-CNNs, where the intermediate volumes of the network are\ndissected and propagated over depth (time) dimension for future calculations,\nsubstantially reducing the number of computations at online operation. For\naction classification, the dissected version of ResNet models performs 77-90%\nfewer computations at online operation while achieving ~5% better\nclassification accuracy on the Kinetics-600 dataset than conventional 3D-ResNet\nmodels. Moreover, the advantages of dissected 3D-CNNs are demonstrated by\ndeploying our approach onto several vision tasks, which consistently improved\nthe performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kopuklu_O/0/1/0/all/0/1\">Okan K&#xf6;p&#xfc;kl&#xfc;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hormann_S/0/1/0/all/0/1\">Stefan H&#xf6;rmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herzog_F/0/1/0/all/0/1\">Fabian Herzog</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cevikalp_H/0/1/0/all/0/1\">Hakan Cevikalp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rigoll_G/0/1/0/all/0/1\">Gerhard Rigoll</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Back to the Future: Cycle Encoding Prediction for Self-supervised Contrastive Video Representation Learning. (arXiv:2010.07217v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2010.07217","description":"<p>In this paper we show that learning video feature spaces in which temporal\ncycles are maximally predictable benefits action classification. In particular,\nwe propose a novel learning approach termed Cycle Encoding Prediction (CEP)\nthat is able to effectively represent high-level spatio-temporal structure of\nunlabelled video content. CEP builds a latent space wherein the concept of\nclosed forward-backward as well as backward-forward temporal loops is\napproximately preserved. As a self-supervision signal, CEP leverages the\nbi-directional temporal coherence of the video stream and applies loss\nfunctions that encourage both temporal cycle closure as well as contrastive\nfeature separation. Architecturally, the underpinning network structure\nutilises a single feature encoder for all video snippets, adding two predictive\nmodules that learn temporal forward and backward transitions. We apply our\nframework for pretext training of networks for action recognition tasks. We\nreport significantly improved results for the standard datasets UCF101 and\nHMDB51. Detailed ablation studies support the effectiveness of the proposed\ncomponents. We publish source code for the CEP components in full with this\npaper.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xinyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirmehdi_M/0/1/0/all/0/1\">Majid Mirmehdi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burghardt_T/0/1/0/all/0/1\">Tilo Burghardt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Center-wise Local Image Mixture For Contrastive Representation Learning. (arXiv:2011.02697v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.02697","description":"<p>Contrastive learning based on instance discrimination trains model to\ndiscriminate different transformations of the anchor sample from other samples,\nwhich does not consider the semantic similarity among samples. This paper\nproposes a new kind of contrastive learning method, named CLIM, which uses\npositives from other samples in the dataset. This is achieved by searching\nlocal similar samples of the anchor, and selecting samples that are closer to\nthe corresponding cluster center, which we denote as center-wise local image\nselection. The selected samples are instantiated via an data mixture strategy,\nwhich performs as a smoothing regularization. As a result, CLIM encourages both\nlocal similarity and global aggregation in a robust way, which we find is\nbeneficial for feature representation. Besides, we introduce\n\\emph{multi-resolution} augmentation, which enables the representation to be\nscale invariant. We reach 75.5% top-1 accuracy with linear evaluation over\nResNet-50, and 59.3% top-1 accuracy when fine-tuned with only 1% labels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaopeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1\">Hongkai Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Monitoring and Diagnosability of Perception Systems. (arXiv:2011.07010v5 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2011.07010","description":"<p>Perception is a critical component of high-integrity applications of robotics\nand autonomous systems, such as self-driving vehicles. In these applications,\nfailure of perception systems may put human life at risk, and a broad adoption\nof these technologies requires the development of methodologies to guarantee\nand monitor safe operation. Despite the paramount importance of perception\nsystems, currently there is no formal approach for system-level monitoring. In\nthis work, we propose a mathematical model for runtime monitoring and fault\ndetection and identification in perception systems. Towards this goal, we draw\nconnections with the literature on diagnosability in multiprocessor systems,\nand generalize it to account for modules with heterogeneous outputs that\ninteract over time. The resulting temporal diagnostic graphs (i) provide a\nframework to reason over the consistency of perception outputs -- across\nmodules and over time -- thus enabling fault detection, (ii) allow us to\nestablish formal guarantees on the maximum number of faults that can be\nuniquely identified in a given perception system, and (iii) enable the design\nof efficient algorithms for fault identification. We demonstrate our monitoring\nsystem, dubbed PerSyS, in realistic simulations using the LGSVL self-driving\nsimulator and the Apollo Auto autonomy software stack, and show that PerSyS is\nable to detect failures in challenging scenarios (including scenarios that have\ncaused self-driving car accidents in recent years), and is able to correctly\nidentify faults while entailing a minimal computation overhead (&lt; 5 ms on a\nsingle-core CPU).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Antonante_P/0/1/0/all/0/1\">Pasquale Antonante</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spivak_D/0/1/0/all/0/1\">David I. Spivak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carlone_L/0/1/0/all/0/1\">Luca Carlone</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End Object Detection with Adaptive Clustering Transformer. (arXiv:2011.09315v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.09315","description":"<p>End-to-end Object Detection with Transformer (DETR)proposes to perform object\ndetection with Transformer and achieve comparable performance with two-stage\nobject detection like Faster-RCNN. However, DETR needs huge computational\nresources for training and inference due to the high-resolution spatial input.\nIn this paper, a novel variant of transformer named Adaptive Clustering\nTransformer(ACT) has been proposed to reduce the computation cost for\nhigh-resolution input. ACT cluster the query features adaptively using Locality\nSensitive Hashing (LSH) and ap-proximate the query-key interaction using the\nprototype-key interaction. ACT can reduce the quadratic O(N2) complexity inside\nself-attention into O(NK) where K is the number of prototypes in each layer.\nACT can be a drop-in module replacing the original self-attention module\nwithout any training. ACT achieves a good balance between accuracy and\ncomputation cost (FLOPs). The code is available as supplementary for the ease\nof experiment replication and verification. Code is released at\n\\url{https://github.com/gaopengcuhk/SMCA-DETR/}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1\">Minghang Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Peng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Renrui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kunchang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaogang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Hao Dong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"4D Human Body Capture from Egocentric Video via 3D Scene Grounding. (arXiv:2011.13341v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.13341","description":"<p>We introduce a novel task of reconstructing a time series of second-person 3D\nhuman body meshes from monocular egocentric videos. The unique viewpoint and\nrapid embodied camera motion of egocentric videos raise additional technical\nbarriers for human body capture. To address those challenges, we propose a\nsimple yet effective optimization-based approach that leverages 2D observations\nof the entire video sequence and human-scene interaction constraint to estimate\nsecond-person human poses, shapes, and global motion that are grounded on the\n3D environment captured from the egocentric view. We conduct detailed ablation\nstudies to validate our design choice. Moreover, we compare our method with the\nprevious state-of-the-art method on human motion capture from monocular video,\nand show that our method estimates more accurate human-body poses and shapes\nunder the challenging egocentric setting. In addition, we demonstrate that our\napproach produces more realistic human-scene interaction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Miao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Dexin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1\">Zhaopeng Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rehg_J/0/1/0/all/0/1\">James M. Rehg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Siyu Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Curiosity-driven 3D Object Detection Without Labels. (arXiv:2012.01230v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.01230","description":"<p>In this paper we set out to solve the task of 6-DOF 3D object detection from\n2D images, where the only supervision is a geometric representation of the\nobjects we aim to find. In doing so, we remove the need for 6-DOF labels (i.e.,\nposition, orientation etc.), allowing our network to be trained on unlabeled\nimages in a self-supervised manner. We achieve this through a neural network\nwhich learns an explicit scene parameterization which is subsequently passed\ninto a differentiable renderer. We analyze why analysis-by-synthesis-like\nlosses for supervision of 3D scene structure using differentiable rendering is\nnot practical, as it almost always gets stuck in local minima of visual\nambiguities. This can be overcome by a novel form of training, where an\nadditional network is employed to steer the optimization itself to explore the\nentire parameter space i.e., to be curious, and hence, to resolve those\nambiguities and find workable minima.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Griffiths_D/0/1/0/all/0/1\">David Griffiths</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boehm_J/0/1/0/all/0/1\">Jan Boehm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ritschel_T/0/1/0/all/0/1\">Tobias Ritschel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prototype-based Incremental Few-Shot Semantic Segmentation. (arXiv:2012.01415v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.01415","description":"<p>Semantic segmentation models have two fundamental weaknesses: i) they require\nlarge training sets with costly pixel-level annotations, and ii) they have a\nstatic output space, constrained to the classes of the training set. Toward\naddressing both problems, we introduce a new task, Incremental Few-Shot\nSegmentation (iFSS). The goal of iFSS is to extend a pretrained segmentation\nmodel with new classes from few annotated images and without access to old\ntraining data. To overcome the limitations of existing models iniFSS, we\npropose Prototype-based Incremental Few-Shot Segmentation (PIFS) that couples\nprototype learning and knowledge distillation. PIFS exploits prototypes to\ninitialize the classifiers of new classes, fine-tuning the network to refine\nits features representation. We design a prototype-based distillation loss on\nthe scores of both old and new class prototypes to avoid overfitting and\nforgetting, and batch-renormalization to cope with non-i.i.d.few-shot data. We\ncreate an extensive benchmark for iFSS showing that PIFS outperforms several\nfew-shot and incremental learning methods in all scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cermelli_F/0/1/0/all/0/1\">Fabio Cermelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mancini_M/0/1/0/all/0/1\">Massimiliano Mancini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xian_Y/0/1/0/all/0/1\">Yongqin Xian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akata_Z/0/1/0/all/0/1\">Zeynep Akata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caputo_B/0/1/0/all/0/1\">Barbara Caputo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Object SLAM-Based Active Mapping and Robotic Grasping. (arXiv:2012.01788v3 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2012.01788","description":"<p>This paper presents the first active object mapping framework for complex\nrobotic manipulation and autonomous perception tasks. The framework is built on\nan object SLAM system integrated with a simultaneous multi-object pose\nestimation process that is optimized for robotic grasping. Aiming to reduce the\nobservation uncertainty on target objects and increase their pose estimation\naccuracy, we also design an object-driven exploration strategy to guide the\nobject mapping process, enabling autonomous mapping and high-level perception.\nCombining the mapping module and the exploration strategy, an accurate object\nmap that is compatible with robotic grasping can be generated. Additionally,\nquantitative evaluations also indicate that the proposed framework has a very\nhigh mapping accuracy. Experiments with manipulation (including object grasping\nand placement) and augmented reality significantly demonstrate the\neffectiveness and advantages of our proposed framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yanmin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yunzhou Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_D/0/1/0/all/0/1\">Delong Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coleman_S/0/1/0/all/0/1\">Sonya Coleman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Wenkai Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xinggang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1\">Zhiqiang Deng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Wisdom of Committees: An Overlooked Approach To Faster and More Accurate Models. (arXiv:2012.01988v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.01988","description":"<p>Committee-based models (ensembles or cascades) construct models by combining\nexisting pre-trained ones. While ensembles and cascades are well-known\ntechniques that were proposed before deep learning, they are not considered a\ncore building block of deep model architectures and are rarely compared to in\nrecent literature on developing efficient models. In this work, we go back to\nbasics and conduct a comprehensive analysis of the efficiency of\ncommittee-based models. We find that even the most simplistic method for\nbuilding committees from existing, independently trained networks can match or\nexceed the accuracy of state-of-the-art models while being drastically more\nefficient. These simple committee-based models also outperform sophisticated\nneural architecture search methods (e.g., BigNAS). These findings hold true for\nseveral tasks, including image classification, video classification, and\nsemantic segmentation, and various architecture families, such as ViT,\nEfficientNet, ResNet, MobileNetV2, and X3D. For example, an EfficientNet\ncascade can achieve a 5.4x speedup over B7 and a ViT-based cascade can achieve\na 2.3x speedup over ViT-L-384 while being equally accurate.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaofang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kondratyuk_D/0/1/0/all/0/1\">Dan Kondratyuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Christiansen_E/0/1/0/all/0/1\">Eric Christiansen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitani_K/0/1/0/all/0/1\">Kris M. Kitani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alon_Y/0/1/0/all/0/1\">Yair Alon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eban_E/0/1/0/all/0/1\">Elad Eban</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DIPPAS: A Deep Image Prior PRNU Anonymization Scheme. (arXiv:2012.03581v2 [cs.MM] UPDATED)","link":"http://arxiv.org/abs/2012.03581","description":"<p>Source device identification is an important topic in image forensics since\nit allows to trace back the origin of an image. Its forensics counter-part is\nsource device anonymization, that is, to mask any trace on the image that can\nbe useful for identifying the source device. A typical trace exploited for\nsource device identification is the Photo Response Non-Uniformity (PRNU), a\nnoise pattern left by the device on the acquired images. In this paper, we\ndevise a methodology for suppressing such a trace from natural images without\nsignificant impact on image quality. Specifically, we turn PRNU anonymization\ninto an optimization problem in a Deep Image Prior (DIP) framework. In a\nnutshell, a Convolutional Neural Network (CNN) acts as generator and returns an\nimage that is anonymized with respect to the source PRNU, still maintaining\nhigh visual quality. With respect to widely-adopted deep learning paradigms,\nour proposed CNN is not trained on a set of input-target pairs of images.\nInstead, it is optimized to reconstruct the PRNU-free image from the original\nimage under analysis itself. This makes the approach particularly suitable in\nscenarios where large heterogeneous databases are analyzed and prevents any\nproblem due to lack of generalization. Through numerical examples on publicly\navailable datasets, we prove our methodology to be effective compared to\nstate-of-the-art techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Picetti_F/0/1/0/all/0/1\">Francesco Picetti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mandelli_S/0/1/0/all/0/1\">Sara Mandelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bestagini_P/0/1/0/all/0/1\">Paolo Bestagini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipari_V/0/1/0/all/0/1\">Vincenzo Lipari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tubaro_S/0/1/0/all/0/1\">Stefano Tubaro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Asynchronous Kalman Filter for Hybrid Event Cameras. (arXiv:2012.05590v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.05590","description":"<p>Event cameras are ideally suited to capture HDR visual information without\nblur but perform poorly on static or slowly changing scenes. Conversely,\nconventional image sensors measure absolute intensity of slowly changing scenes\neffectively but do poorly on high dynamic range or quickly changing scenes. In\nthis paper, we present an event-based video reconstruction pipeline for High\nDynamic Range (HDR) scenarios. The proposed algorithm includes a frame\naugmentation pre-processing step that deblurs and temporally interpolates frame\ndata using events. The augmented frame and event data are then fused using a\nnovel asynchronous Kalman filter under a unifying uncertainty model for both\nsensors. Our experimental results are evaluated on both publicly available\ndatasets with challenging lighting conditions and fast motions and our new\ndataset with HDR reference. The proposed algorithm outperforms state-of-the-art\nmethods in both absolute intensity error (48% reduction) and image similarity\nindexes (average 11% improvement).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Ziwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_Y/0/1/0/all/0/1\">Yonhon Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scheerlinck_C/0/1/0/all/0/1\">Cedric Scheerlinck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahony_R/0/1/0/all/0/1\">Robert Mahony</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Geometric Adversarial Attacks and Defenses on 3D Point Clouds. (arXiv:2012.05657v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.05657","description":"<p>Deep neural networks are prone to adversarial examples that maliciously alter\nthe network's outcome. Due to the increasing popularity of 3D sensors in\nsafety-critical systems and the vast deployment of deep learning models for 3D\npoint sets, there is a growing interest in adversarial attacks and defenses for\nsuch models. So far, the research has focused on the semantic level, namely,\ndeep point cloud classifiers. However, point clouds are also widely used in a\ngeometric-related form that includes encoding and reconstructing the geometry.\nIn this work, we are the first to consider the problem of adversarial examples\nat a geometric level. In this setting, the question is how to craft a small\nchange to a clean source point cloud that leads, after passing through an\nautoencoder model, to the reconstruction of a different target shape. Our\nattack is in sharp contrast to existing semantic attacks on 3D point clouds.\nWhile such works aim to modify the predicted label by a classifier, we alter\nthe entire reconstructed geometry. Additionally, we demonstrate the robustness\nof our attack in the case of defense, where we show that remnant\ncharacteristics of the target shape are still present at the output after\napplying the defense to the adversarial input. Our code is publicly available\nat https://github.com/itailang/geometric_adv.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lang_I/0/1/0/all/0/1\">Itai Lang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kotlicki_U/0/1/0/all/0/1\">Uriel Kotlicki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avidan_S/0/1/0/all/0/1\">Shai Avidan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improved StyleGAN Embedding: Where are the Good Latents?. (arXiv:2012.09036v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.09036","description":"<p>StyleGAN is able to produce photorealistic images that are almost\nindistinguishable from real photos. The reverse problem of finding an embedding\nfor a given image poses a challenge. Embeddings that reconstruct an image well\nare not always robust to editing operations. In this paper, we address the\nproblem of finding an embedding that both reconstructs images and also supports\nimage editing tasks. First, we introduce a new normalized space to analyze the\ndiversity and the quality of the reconstructed latent codes. This space can\nhelp answer the question of where good latent codes are located in latent\nspace. Second, we propose an improved embedding algorithm using a novel\nregularization method based on our analysis. Finally, we analyze the quality of\ndifferent embedding algorithms. We compare our results with the current\nstate-of-the-art methods and achieve a better trade-off between reconstruction\nquality and editing quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_P/0/1/0/all/0/1\">Peihao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdal_R/0/1/0/all/0/1\">Rameen Abdal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yipeng Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Femiani_J/0/1/0/all/0/1\">John Femiani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wonka_P/0/1/0/all/0/1\">Peter Wonka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Power-SLIC: Fast Superpixel Segmentations by Diagrams. (arXiv:2012.11772v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.11772","description":"<p>Superpixel algorithms grouping pixels with similar color and other low-level\nproperties are increasingly used for pre-processing in image segmentation. In\nrecent years, a focus has been placed on developing geometric superpixel\nmethods that facilitate the extraction and analysis of geometric image\nfeatures. Diagram-based superpixel methods are important among the geometric\nmethods as they generate compact and sparsely representable superpixels.\nIntroducing generalized balanced power diagrams to the field of superpixels, we\npropose a diagram method called Power-SLIC. Power-SLIC is the first geometric\nsuperpixel method to generate piecewise quadratic boundaries. Its speed,\ncompetitive with fast state-of-the-art methods, is unprecedented for diagram\napproaches. Extensive computational experiments show that Power-SLIC\noutperforms existing diagram approaches in boundary recall, under segmentation\nerror, achievable segmentation accuracy, and compression quality. Moreover,\nPower-SLIC is robust to Gaussian noise.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fiedler_M/0/1/0/all/0/1\">Maximilian Fiedler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alpers_A/0/1/0/all/0/1\">Andreas Alpers</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FaceX-Zoo: A PyTorch Toolbox for Face Recognition. (arXiv:2101.04407v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.04407","description":"<p>Deep learning based face recognition has achieved significant progress in\nrecent years. Yet, the practical model production and further research of deep\nface recognition are in great need of corresponding public support. For\nexample, the production of face representation network desires a modular\ntraining scheme to consider the proper choice from various candidates of\nstate-of-the-art backbone and training supervision subject to the real-world\nface recognition demand; for performance analysis and comparison, the standard\nand automatic evaluation with a bunch of models on multiple benchmarks will be\na desired tool as well; besides, a public groundwork is welcomed for deploying\nthe face recognition in the shape of holistic pipeline. Furthermore, there are\nsome newly-emerged challenges, such as the masked face recognition caused by\nthe recent world-wide COVID-19 pandemic, which draws increasing attention in\npractical applications. A feasible and elegant solution is to build an\neasy-to-use unified framework to meet the above demands. To this end, we\nintroduce a novel open-source framework, named FaceX-Zoo, which is oriented to\nthe research-development community of face recognition. Resorting to the highly\nmodular and scalable design, FaceX-Zoo provides a training module with various\nsupervisory heads and backbones towards state-of-the-art face recognition, as\nwell as a standardized evaluation module which enables to evaluate the models\nin most of the popular benchmarks just by editing a simple configuration. Also,\na simple yet fully functional face SDK is provided for the validation and\nprimary application of the trained models. Rather than including as many as\npossible of the prior techniques, we enable FaceX-Zoo to easily upgrade and\nextend along with the development of face related domains. The source code and\nmodels are available at https://github.com/JDAI-CV/FaceX-Zoo.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yinglu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yibo Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Hailin Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_T/0/1/0/all/0/1\">Tao Mei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Makeup Face Verification by Exploring Part-Based Representations. (arXiv:2101.07338v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.07338","description":"<p>Recently, we have seen an increase in the global facial recognition market\nsize. Despite significant advances in face recognition technology with the\nadoption of convolutional neural networks, there are still open challenges,\nsuch as when there is makeup in the face. To address this challenge, we propose\nand evaluate the adoption of facial parts to fuse with current holistic\nrepresentations. We propose two strategies of facial parts: one with four\nregions (left periocular, right periocular, nose and mouth) and another with\nthree facial thirds (upper, middle and lower). Experimental results obtained in\nfour public makeup face datasets and in a challenging cross-dataset protocol\nshow that the fusion of deep features extracted of facial parts with holistic\nrepresentation increases the accuracy of face verification systems and\ndecreases the error rates, even without any retraining of the CNN models. Our\nproposed pipeline achieved competitive results for the four datasets (EMFD,\nFAM, M501 and YMU).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Angeloni_M/0/1/0/all/0/1\">Marcus de Assis Angeloni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pedrini_H/0/1/0/all/0/1\">Helio Pedrini</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diminishing Domain Bias by Leveraging Domain Labels in Object Detection on UAVs. (arXiv:2101.12677v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.12677","description":"<p>Object detection from Unmanned Aerial Vehicles (UAVs) is of great importance\nin many aerial vision-based applications. Despite the great success of generic\nobject detection methods, a significant performance drop is observed when\napplied to images captured by UAVs. This is due to large variations in imaging\nconditions, such as varying altitudes, dynamically changing viewing angles, and\ndifferent capture times. These variations lead to domain imbalances and, thus,\ntrained models suffering from domain bias. We demonstrate that domain knowledge\nis a valuable source of information and thus propose domain-aware object\ndetectors by using freely accessible sensor data. By splitting the model into\ncross-domain and domain-specific parts, substantial performance improvements\nare achieved on multiple data sets across various models and metrics without\nchanging the architecture. In particular, we achieve a new state-of-the-art\nperformance on UAVDT for embedded real-time detectors. Furthermore, we create a\nnew airborne image data set by annotating 13,713 objects in 2,900 images\nfeaturing precise altitude and viewing angle annotations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kiefer_B/0/1/0/all/0/1\">Benjamin Kiefer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Messmer_M/0/1/0/all/0/1\">Martin Messmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zell_A/0/1/0/all/0/1\">Andreas Zell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Global to Local Double Embedding Method for Multi-person Pose Estimation. (arXiv:2102.07318v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.07318","description":"<p>Multi-person pose estimation is a fundamental and challenging problem to many\ncomputer vision tasks. Most existing methods can be broadly categorized into\ntwo classes: top-down and bottom-up methods. Both of the two types of methods\ninvolve two stages, namely, person detection and joints detection.\nConventionally, the two stages are implemented separately without considering\ntheir interactions between them, and this may inevitably cause some issue\nintrinsically. In this paper, we present a novel method to simplify the\npipeline by implementing person detection and joints detection simultaneously.\nWe propose a Double Embedding (DE) method to complete the multi-person pose\nestimation task in a global-to-local way. DE consists of Global Embedding (GE)\nand Local Embedding (LE). GE encodes different person instances and processes\ninformation covering the whole image and LE encodes the local limbs\ninformation. GE functions for the person detection in top-down strategy while\nLE connects the rest joints sequentially which functions for joint grouping and\ninformation processing in A bottom-up strategy. Based on LE, we design the\nMutual Refine Machine (MRM) to reduce the prediction difficulty in complex\nscenarios. MRM can effectively realize the information communicating between\nkeypoints and further improve the accuracy. We achieve the competitive results\non benchmarks MSCOCO, MPII and CrowdPose, demonstrating the effectiveness and\ngeneralization ability of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yiming Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiaxin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yiheng Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yan Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_H/0/1/0/all/0/1\">Hua-Liang Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HVAQ: A High-Resolution Vision-Based Air Quality Dataset. (arXiv:2102.09332v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.09332","description":"<p>Air pollutants, such as particulate matter, negatively impact human health.\nMost existing pollution monitoring techniques use stationary sensors, which are\ntypically sparsely deployed. However, real-world pollution distributions vary\nrapidly with position and the visual effects of air pollution can be used to\nestimate concentration, potentially at high spatial resolution. Accurate\npollution monitoring requires either densely deployed conventional point\nsensors, at-a-distance vision-based pollution monitoring, or a combination of\nboth.\n</p>\n<p>The main contribution of this paper is that to the best of our knowledge, it\nis the first publicly available, high temporal and spatial resolution air\nquality dataset containing simultaneous point sensor measurements and\ncorresponding images. The dataset enables, for the first time, high spatial\nresolution evaluation of image-based air pollution estimation algorithms. It\ncontains PM2.5, PM10, temperature, and humidity data. We evaluate several\nstate-of-art vision-based PM concentration estimation algorithms on our dataset\nand quantify the increase in accuracy resulting from higher point sensor\ndensity and the use of images. It is our intent and belief that this dataset\ncan enable advances by other research teams working on air quality estimation.\nOur dataset is available at\nhttps://github.com/implicitDeclaration/HVAQ-dataset/tree/master.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zuohui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tony Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhuangzhi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_Y/0/1/0/all/0/1\">Yun Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xuan_Q/0/1/0/all/0/1\">Qi Xuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dick_R/0/1/0/all/0/1\">Robert P. Dick</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fixing Data Augmentation to Improve Adversarial Robustness. (arXiv:2103.01946v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.01946","description":"<p>Adversarial training suffers from robust overfitting, a phenomenon where the\nrobust test accuracy starts to decrease during training. In this paper, we\nfocus on both heuristics-driven and data-driven augmentations as a means to\nreduce robust overfitting. First, we demonstrate that, contrary to previous\nfindings, when combined with model weight averaging, data augmentation can\nsignificantly boost robust accuracy. Second, we explore how state-of-the-art\ngenerative models can be leveraged to artificially increase the size of the\ntraining set and further improve adversarial robustness. Finally, we evaluate\nour approach on CIFAR-10 against $\\ell_\\infty$ and $\\ell_2$ norm-bounded\nperturbations of size $\\epsilon = 8/255$ and $\\epsilon = 128/255$,\nrespectively. We show large absolute improvements of +7.06% and +5.88% in\nrobust accuracy compared to previous state-of-the-art methods. In particular,\nagainst $\\ell_\\infty$ norm-bounded perturbations of size $\\epsilon = 8/255$,\nour model reaches 64.20% robust accuracy without using any external data,\nbeating most prior works that use external data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rebuffi_S/0/1/0/all/0/1\">Sylvestre-Alvise Rebuffi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gowal_S/0/1/0/all/0/1\">Sven Gowal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calian_D/0/1/0/all/0/1\">Dan A. Calian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stimberg_F/0/1/0/all/0/1\">Florian Stimberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiles_O/0/1/0/all/0/1\">Olivia Wiles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mann_T/0/1/0/all/0/1\">Timothy Mann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Local Patch AutoAugment with Multi-Agent Collaboration. (arXiv:2103.11099v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.11099","description":"<p>Data augmentation (DA) plays a critical role in improving the generalization\nof deep learning models. Recent works on automatically searching for DA\npolicies from data have achieved great success. However, existing automated DA\nmethods generally perform the search at the image level, which limits the\nexploration of diversity in local regions. In this paper, we propose a more\nfine-grained automated DA approach, dubbed Patch AutoAugment, to divide an\nimage into a grid of patches and search for the joint optimal augmentation\npolicies for the patches. We formulate it as a multi-agent reinforcement\nlearning (MARL) problem, where each agent learns an augmentation policy for\neach patch based on its content together with the semantics of the whole image.\nThe agents cooperate with each other to achieve the optimal augmentation effect\nof the entire image by sharing a team reward. We show the effectiveness of our\nmethod on multiple benchmark datasets of image classification and fine-grained\nimage recognition (e.g., CIFAR-10, CIFAR-100, ImageNet, CUB-200-2011, Stanford\nCars and FGVC-Aircraft). Extensive experiments demonstrate that our method\noutperforms the state-of-the-art DA methods while requiring fewer computational\nresources.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Shiqi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_R/0/1/0/all/0/1\">Ruoyu Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1\">Xin Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhibo Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dual Mesh Convolutional Networks for Human Shape Correspondence. (arXiv:2103.12459v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.12459","description":"<p>Convolutional networks have been extremely successful for regular data\nstructures such as 2D images and 3D voxel grids. The transposition to meshes\nis, however, not straight-forward due to their irregular structure. We explore\nhow the dual, face-based representation of triangular meshes can be leveraged\nas a data structure for graph convolutional networks. In the dual mesh, each\nnode (face) has a fixed number of neighbors, which makes the networks less\nsusceptible to overfitting on the mesh topology, and also al-lows the use of\ninput features that are naturally defined over faces, such as surface normals\nand face areas. We evaluate the dual approach on the shape correspondence task\non theFaust human shape dataset and variants of it with differ-ent mesh\ntopologies. Our experiments show that results of graph convolutional networks\nimprove when defined over the dual rather than primal mesh. Moreover, our\nmodels that explicitly leverage the neighborhood regularity of dual meshes\nallow improving results further while being more robust to changes in the mesh\ntopology.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Verma_N/0/1/0/all/0/1\">Nitika Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boukhayma_A/0/1/0/all/0/1\">Adnane Boukhayma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verbeek_J/0/1/0/all/0/1\">Jakob Verbeek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boyer_E/0/1/0/all/0/1\">Edmond Boyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical Proxy-based Loss for Deep Metric Learning. (arXiv:2103.13538v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.13538","description":"<p>Proxy-based metric learning losses are superior to pair-based losses due to\ntheir fast convergence and low training complexity. However, existing\nproxy-based losses focus on learning class-discriminative features while\noverlooking the commonalities shared across classes which are potentially\nuseful in describing and matching samples. Moreover, they ignore the implicit\nhierarchy of categories in real-world datasets, where similar subordinate\nclasses can be grouped together. In this paper, we present a framework that\nleverages this implicit hierarchy by imposing a hierarchical structure on the\nproxies and can be used with any existing proxy-based loss. This allows our\nmodel to capture both class-discriminative features and class-shared\ncharacteristics without breaking the implicit data hierarchy. We evaluate our\nmethod on five established image retrieval datasets such as In-Shop and SOP.\nResults demonstrate that our hierarchical proxy-based loss framework improves\nthe performance of existing proxy-based losses, especially on large datasets\nwhich exhibit strong hierarchical structure.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhibo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bastan_M/0/1/0/all/0/1\">Muhammet Bastan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xinliang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gray_D/0/1/0/all/0/1\">Doug Gray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samaras_D/0/1/0/all/0/1\">Dimitris Samaras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Training a Task-Specific Image Reconstruction Loss. (arXiv:2103.14616v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2103.14616","description":"<p>The choice of a loss function is an important factor when training neural\nnetworks for image restoration problems, such as single image super resolution.\nThe loss function should encourage natural and perceptually pleasing results. A\npopular choice for a loss is a pre-trained network, such as VGG, which is used\nas a feature extractor for computing the difference between restored and\nreference images. However, such an approach has multiple drawbacks: it is\ncomputationally expensive, requires regularization and hyper-parameter tuning,\nand involves a large network trained on an unrelated task. Furthermore, it has\nbeen observed that there is no single loss function that works best across all\napplications and across different datasets. In this work, we instead propose to\ntrain a set of loss functions that are application specific in nature. Our loss\nfunction comprises a series of discriminators that are trained to detect and\npenalize the presence of application-specific artifacts. We show that a single\nnatural image and corresponding distortions are sufficient to train our feature\nextractor that outperforms state-of-the-art loss functions in applications like\nsingle image super resolution, denoising, and JPEG artifact removal. Finally,\nwe conclude that an effective loss function does not have to be a good\npredictor of perceived image quality, but instead needs to be specialized in\nidentifying the distortions for a given restoration method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Mustafa_A/0/1/0/all/0/1\">Aamir Mustafa</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mikhailiuk_A/0/1/0/all/0/1\">Aliaksei Mikhailiuk</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Iliescu_D/0/1/0/all/0/1\">Dan Andrei Iliescu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Babbar_V/0/1/0/all/0/1\">Varun Babbar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mantiuk_R/0/1/0/all/0/1\">Rafal K. Mantiuk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image Composition Assessment with Saliency-augmented Multi-pattern Pooling. (arXiv:2104.03133v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.03133","description":"<p>Image composition assessment is crucial in aesthetic assessment, which aims\nto assess the overall composition quality of a given image. However, to the\nbest of our knowledge, there is neither dataset nor method specifically\nproposed for this task. In this paper, we contribute the first composition\nassessment dataset CADB with composition scores for each image provided by\nmultiple professional raters. Besides, we propose a composition assessment\nnetwork SAMP-Net with a novel Saliency-Augmented Multi-pattern Pooling (SAMP)\nmodule, which analyses visual layout from the perspectives of multiple\ncomposition patterns. We also leverage composition-relevant attributes to\nfurther boost the performance, and extend Earth Mover's Distance (EMD) loss to\nweighted EMD loss to eliminate the content bias. The experimental results show\nthat our SAMP-Net can perform more favorably than previous aesthetic assessment\napproaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_L/0/1/0/all/0/1\">Li Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liqing Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Occlusion Guided Self-supervised Scene Flow Estimation on 3D Point Clouds. (arXiv:2104.04724v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.04724","description":"<p>Understanding the flow in 3D space of sparsely sampled points between two\nconsecutive time frames is the core stone of modern geometric-driven systems\nsuch as VR/AR, Robotics, and Autonomous driving. The lack of real,\nnon-simulated, labeled data for this task emphasizes the importance of self- or\nun-supervised deep architectures. This work presents a new self-supervised\ntraining method and an architecture for the 3D scene flow estimation under\nocclusions. Here we show that smart multi-layer fusion between flow prediction\nand occlusion detection outperforms traditional architectures by a large margin\nfor occluded and non-occluded scenarios. We report state-of-the-art results on\nFlyingthings3D and KITTI datasets for both the supervised and self-supervised\ntraining.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_B/0/1/0/all/0/1\">Bojun Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raviv_D/0/1/0/all/0/1\">Dan Raviv</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge-Augmented Contrastive Learning for Abnormality Classification and Localization in Chest X-rays with Radiomics using a Feedback Loop. (arXiv:2104.04968v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.04968","description":"<p>Building a highly accurate predictive model for classification and\nlocalization of abnormalities in chest X-rays usually requires a large number\nof manually annotated labels and pixel regions (bounding boxes) of\nabnormalities. However, it is expensive to acquire such annotations, especially\nthe bounding boxes. Recently, contrastive learning has shown strong promise in\nleveraging unlabeled natural images to produce highly generalizable and\ndiscriminative features. However, extending its power to the medical image\ndomain is under-explored and highly non-trivial, since medical images are much\nless amendable to data augmentations. In contrast, their prior knowledge, as\nwell as radiomic features, is often crucial. To bridge this gap, we propose an\nend-to-end semi-supervised knowledge-augmented contrastive learning framework,\nthat simultaneously performs disease classification and localization tasks. The\nkey knob of our framework is a unique positive sampling approach tailored for\nthe medical images, by seamlessly integrating radiomic features as a knowledge\naugmentation. Specifically, we first apply an image encoder to classify the\nchest X-rays and to generate the image features. We next leverage Grad-CAM to\nhighlight the crucial (abnormal) regions for chest X-rays (even when\nunannotated), from which we extract radiomic features. The radiomic features\nare then passed through another dedicated encoder to act as the positive sample\nfor the image features generated from the same chest X-ray. In this way, our\nframework constitutes a feedback loop for image and radiomic modality features\nto mutually reinforce each other. Their contrasting yields knowledge-augmented\nrepresentations that are both robust and interpretable. Extensive experiments\non the NIH Chest X-ray dataset demonstrate that our approach outperforms\nexisting baselines in both classification and localization tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1\">Yan Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chongyan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tewfik_A/0/1/0/all/0/1\">Ahmed Tewfik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glicksberg_B/0/1/0/all/0/1\">Benjamin Glicksberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Ying Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yifan Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Task Generalization via Natural Language Crowdsourcing Instructions. (arXiv:2104.08773v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08773","description":"<p>Humans (e.g., crowdworkers) have a remarkable ability in solving different\ntasks, by simply reading textual instructions that define them and looking at a\nfew examples. NLP models built with the conventional paradigm, however, often\nstruggle with generalization across tasks (e.g., a question-answering system\ncannot solve classification tasks). A long-standing challenge in AI is to build\na model that learns a new task by understanding the human-readable instructions\nthat define it. To study this, we introduce NATURAL INSTRUCTIONS, a dataset of\n61 distinct tasks, their human-authored instructions and 193k task instances.\nThe instructions are obtained from crowdsourcing instructions used to create\nexisting NLP datasets and mapped to a unified schema. We adopt generative\npre-trained language models to encode task-specific instructions along with\ninput and generate task output. Our results indicate that models benefit from\ninstructions when evaluated in terms of generalization to unseen tasks. These\nmodels, however, are far behind supervised task-specific models, indicating\nsignificant room for more progress in this direction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Swaroop Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khashabi_D/0/1/0/all/0/1\">Daniel Khashabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving state-of-the-art in Detecting Student Engagement with Resnet and TCN Hybrid Network. (arXiv:2104.10122v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.10122","description":"<p>Automatic detection of students' engagement in online learning settings is a\nkey element to improve the quality of learning and to deliver personalized\nlearning materials to them. Varying levels of engagement exhibited by students\nin an online classroom is an affective behavior that takes place over space and\ntime. Therefore, we formulate detecting levels of students' engagement from\nvideos as a spatio-temporal classification problem. In this paper, we present a\nnovel end-to-end Residual Network (ResNet) and Temporal Convolutional Network\n(TCN) hybrid neural network architecture for students' engagement level\ndetection in videos. The 2D ResNet extracts spatial features from consecutive\nvideo frames, and the TCN analyzes the temporal changes in video frames to\ndetect the level of engagement. The spatial and temporal arms of the hybrid\nnetwork are jointly trained on raw video frames of a large publicly available\nstudents' engagement detection dataset, DAiSEE. We compared our method with\nseveral competing students' engagement detection methods on this dataset. The\nResNet+TCN architecture outperforms all other studied methods, improves the\nstate-of-the-art engagement level detection accuracy, and sets a new baseline\nfor future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abedi_A/0/1/0/all/0/1\">Ali Abedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1\">Shehroz S. Khan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FIERY: Future Instance Prediction in Bird's-Eye View from Surround Monocular Cameras. (arXiv:2104.10490v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.10490","description":"<p>Driving requires interacting with road agents and predicting their future\nbehaviour in order to navigate safely. We present FIERY: a probabilistic future\nprediction model in bird's-eye view from monocular cameras. Our model predicts\nfuture instance segmentation and motion of dynamic agents that can be\ntransformed into non-parametric future trajectories. Our approach combines the\nperception, sensor fusion and prediction components of a traditional autonomous\ndriving stack by estimating bird's-eye-view prediction directly from surround\nRGB monocular camera inputs. FIERY learns to model the inherent stochastic\nnature of the future solely from camera driving data in an end-to-end manner,\nwithout relying on HD maps, and predicts multimodal future trajectories. We\nshow that our model outperforms previous prediction baselines on the NuScenes\nand Lyft datasets. The code and trained models are available at\nhttps://github.com/wayveai/fiery.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_A/0/1/0/all/0/1\">Anthony Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murez_Z/0/1/0/all/0/1\">Zak Murez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohan_N/0/1/0/all/0/1\">Nikhil Mohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dudas_S/0/1/0/all/0/1\">Sof&#xed;a Dudas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hawke_J/0/1/0/all/0/1\">Jeffrey Hawke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Badrinarayanan_V/0/1/0/all/0/1\">Vijay Badrinarayanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cipolla_R/0/1/0/all/0/1\">Roberto Cipolla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kendall_A/0/1/0/all/0/1\">Alex Kendall</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VidTr: Video Transformer Without Convolutions. (arXiv:2104.11746v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.11746","description":"<p>We introduce Video Transformer (VidTr) with separable-attention for video\nclassification. Comparing with commonly used 3D networks, VidTr is able to\naggregate spatio-temporal information via stacked attentions and provide better\nperformance with higher efficiency. We first introduce the vanilla video\ntransformer and show that transformer module is able to perform spatio-temporal\nmodeling from raw pixels, but with heavy memory usage. We then present VidTr\nwhich reduces the memory cost by 3.3$\\times$ while keeping the same\nperformance. To further optimize the model, we propose the standard deviation\nbased topK pooling for attention ($pool_{topK\\_std}$), which reduces the\ncomputation by dropping non-informative features along temporal dimension.\nVidTr achieves state-of-the-art performance on five commonly used datasets with\nlower computational requirement, showing both the efficiency and effectiveness\nof our design. Finally, error analysis and visualization show that VidTr is\nespecially good at predicting actions that require long-term temporal\nreasoning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yanyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xinyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chunhui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shuai_B/0/1/0/all/0/1\">Bing Shuai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brattoli_B/0/1/0/all/0/1\">Biagio Brattoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marsic_I/0/1/0/all/0/1\">Ivan Marsic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tighe_J/0/1/0/all/0/1\">Joseph Tighe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Collaborative Regression of Expressive Bodies using Moderation. (arXiv:2105.05301v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.05301","description":"<p>Recovering expressive humans from images is essential for understanding human\nbehavior. Methods that estimate 3D bodies, faces, or hands have progressed\nsignificantly, yet separately. Face methods recover accurate 3D shape and\ngeometric details, but need a tight crop and struggle with extreme views and\nlow resolution. Whole-body methods are robust to a wide range of poses and\nresolutions, but provide only a rough 3D face shape without details like\nwrinkles. To get the best of both worlds, we introduce PIXIE, which produces\nanimatable, whole-body 3D avatars with realistic facial detail, from a single\nimage. For this, PIXIE uses two key observations. First, existing work combines\nindependent estimates from body, face, and hand experts, by trusting them\nequally. PIXIE introduces a novel moderator that merges the features of the\nexperts, weighted by their confidence. All part experts can contribute to the\nwhole, using SMPL-X's shared shape space across all body parts. Second, human\nshape is highly correlated with gender, but existing work ignores this. We\nlabel training images as male, female, or non-binary, and train PIXIE to infer\n\"gendered\" 3D body shapes with a novel shape loss. In addition to 3D body pose\nand shape parameters, PIXIE estimates expression, illumination, albedo and 3D\nfacial surface displacements. Quantitative and qualitative evaluation shows\nthat PIXIE estimates more accurate whole-body shape and detailed face shape\nthan the state of the art. Models and code are available at\nhttps://pixie.is.tue.mpg.de.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yao Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choutas_V/0/1/0/all/0/1\">Vasileios Choutas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bolkart_T/0/1/0/all/0/1\">Timo Bolkart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tzionas_D/0/1/0/all/0/1\">Dimitrios Tzionas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Black_M/0/1/0/all/0/1\">Michael J. Black</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SBEVNet: End-to-End Deep Stereo Layout Estimation. (arXiv:2105.11705v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.11705","description":"<p>Accurate layout estimation is crucial for planning and navigation in robotics\napplications, such as self-driving. In this paper, we introduce the Stereo\nBird's Eye ViewNetwork (SBEVNet), a novel supervised end-to-end framework for\nestimation of bird's eye view layout from a pair of stereo images. Although our\nnetwork reuses some of the building blocks from the state-of-the-art deep\nlearning networks for disparity estimation, we show that explicit depth\nestimation is neither sufficient nor necessary. Instead, the learning of a good\ninternal bird's eye view feature representation is effective for layout\nestimation. Specifically, we first generate a disparity feature volume using\nthe features of the stereo images and then project it to the bird's eye view\ncoordinates. This gives us coarse-grained information about the scene\nstructure. We also apply inverse perspective mapping (IPM) to map the input\nimages and their features to the bird's eye view. This gives us fine-grained\ntexture information. Concatenating IPM features with the projected feature\nvolume creates a rich bird's eye view representation which is useful for\nspatial reasoning. We use this representation to estimate the BEV semantic map.\nAdditionally, we show that using the IPM features as a supervisory signal for\nstereo features can give an improvement in performance. We demonstrate our\napproach on two datasets:the KITTI dataset and a synthetically generated\ndataset from the CARLA simulator. For both of these datasets, we establish\nstate-of-the-art performance compared to baseline techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_D/0/1/0/all/0/1\">Divam Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_W/0/1/0/all/0/1\">Wei Pu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tabor_T/0/1/0/all/0/1\">Trenton Tabor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schneider_J/0/1/0/all/0/1\">Jeff Schneider</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhanced Isotropy Maximization Loss: Seamless and High-Performance Out-of-Distribution Detection Simply Replacing the SoftMax Loss. (arXiv:2105.14399v7 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2105.14399","description":"<p>Current out-of-distribution detection approaches usually present special\nrequirements (e.g., collecting outlier data and hyperparameter validation) and\nproduce side effects (e.g., classification accuracy drop and slow/inefficient\ninferences). Recently, entropic out-of-distribution detection has been proposed\nas a seamless approach (i.e., a solution that avoids all of the previously\nmentioned drawbacks). The entropic out-of-distribution detection solution uses\nthe IsoMax loss for training and the entropic score for out-of-distribution\ndetection. The IsoMax loss works as a SoftMax loss drop-in replacement because\nswapping the SoftMax loss with the IsoMax loss requires no changes in the\nmodel's architecture or training procedures/hyperparameters. In this paper, we\nperform what we call an isometrization of the distances used in the IsoMax\nloss. Additionally, we propose replacing the entropic score with the minimum\ndistance score. Experiments showed that these simple modifications increase\nout-of-distribution detection performance while keeping the solution seamless.\nBesides being competitive with or outperforming all major current approaches,\nthe proposed solution avoids all their current limitations in addition to being\nmuch easier to use because only a simple loss replacement for training the\nneural network is required.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Macedo_D/0/1/0/all/0/1\">David Mac&#xea;do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ludermir_T/0/1/0/all/0/1\">Teresa Ludermir</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analogous to Evolutionary Algorithm: Designing a Unified Sequence Model. (arXiv:2105.15089v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.15089","description":"<p>Inspired by biological evolution, we explain the rationality of Vision\nTransformer by analogy with the proven practical Evolutionary Algorithm (EA)\nand derive that both of them have consistent mathematical representation.\nAnalogous to the dynamic local population in EA, we improve the existing\ntransformer structure and propose a more efficient EAT model, and design\ntask-related heads to deal with different tasks more flexibly. Moreover, we\nintroduce the spatial-filling curve into the current vision transformer to\nsequence image data into a uniform sequential format. Thus we can design a\nunified EAT framework to address multi-modal tasks, separating the network\narchitecture from the data format adaptation. Our approach achieves\nstate-of-the-art results on the ImageNet classification task compared with\nrecent vision transformer works while having smaller parameters and greater\nthroughput. We further conduct multi-modal tasks to demonstrate the superiority\nof the unified EAT, e.g., Text-Based Image Retrieval, and our approach improves\nthe rank-1 by +3.7 points over the baseline on the CSS dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiangning Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wenzhou Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yabiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1\">Ying Tai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengjie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Feiyue Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Container: Context Aggregation Network. (arXiv:2106.01401v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.01401","description":"<p>Convolutional neural networks (CNNs) are ubiquitous in computer vision, with\na myriad of effective and efficient variations. Recently, Transformers --\noriginally introduced in natural language processing -- have been increasingly\nadopted in computer vision. While early adopters continue to employ CNN\nbackbones, the latest networks are end-to-end CNN-free Transformer solutions. A\nrecent surprising finding shows that a simple MLP based solution without any\ntraditional convolutional or Transformer components can produce effective\nvisual representations. While CNNs, Transformers and MLP-Mixers may be\nconsidered as completely disparate architectures, we provide a unified view\nshowing that they are in fact special cases of a more general method to\naggregate spatial context in a neural network stack. We present the \\model\n(CONText AggregatIon NEtwoRk), a general-purpose building block for multi-head\ncontext aggregation that can exploit long-range interactions \\emph{a la}\nTransformers while still exploiting the inductive bias of the local convolution\noperation leading to faster convergence speeds, often seen in CNNs. In contrast\nto Transformer-based methods that do not scale well to downstream tasks that\nrely on larger input image resolutions, our efficient network, named\n\\modellight, can be employed in object detection and instance segmentation\nnetworks such as DETR, RetinaNet and Mask-RCNN to obtain an impressive\ndetection mAP of 38.9, 43.8, 45.1 and mask mAP of 41.3, providing large\nimprovements of 6.6, 7.3, 6.9 and 6.6 pts respectively, compared to a ResNet-50\nbackbone with a comparable compute and parameter size. Our method also achieves\npromising results on self-supervised learning compared to DeiT on the DINO\nframework. Code is released at \\url{https://github.com/allenai/container}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Peng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiasen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mottaghi_R/0/1/0/all/0/1\">Roozbeh Mottaghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kembhavi_A/0/1/0/all/0/1\">Aniruddha Kembhavi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Barbershop: GAN-based Image Compositing using Segmentation Masks. (arXiv:2106.01505v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.01505","description":"<p>Seamlessly blending features from multiple images is extremely challenging\nbecause of complex relationships in lighting, geometry, and partial occlusion\nwhich cause coupling between different parts of the image. Even though recent\nwork on GANs enables synthesis of realistic hair or faces, it remains difficult\nto combine them into a single, coherent, and plausible image rather than a\ndisjointed set of image patches. We present a novel solution to image blending,\nparticularly for the problem of hairstyle transfer, based on GAN-inversion. We\npropose a novel latent space for image blending which is better at preserving\ndetail and encoding spatial information, and propose a new GAN-embedding\nalgorithm which is able to slightly modify images to conform to a common\nsegmentation mask. Our novel representation enables the transfer of the visual\nproperties from multiple reference images including specific details such as\nmoles and wrinkles, and because we do image blending in a latent-space we are\nable to synthesize images that are coherent. Our approach avoids blending\nartifacts present in other approaches and finds a globally consistent image.\nOur results demonstrate a significant improvement over the current state of the\nart in a user study, with users preferring our blending solution over 95\npercent of the time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_P/0/1/0/all/0/1\">Peihao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdal_R/0/1/0/all/0/1\">Rameen Abdal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Femiani_J/0/1/0/all/0/1\">John Femiani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wonka_P/0/1/0/all/0/1\">Peter Wonka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Image Local Autoregressive Transformer. (arXiv:2106.02514v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.02514","description":"<p>Recently, AutoRegressive (AR) models for the whole image generation empowered\nby transformers have achieved comparable or even better performance to\nGenerative Adversarial Networks (GANs). Unfortunately, directly applying such\nAR models to edit/change local image regions, may suffer from the problems of\nmissing global information, slow inference speed, and information leakage of\nlocal guidance. To address these limitations, we propose a novel model -- image\nLocal Autoregressive Transformer (iLAT), to better facilitate the locally\nguided image synthesis. Our iLAT learns the novel local discrete\nrepresentations, by the newly proposed local autoregressive (LA) transformer of\nthe attention mask and convolution mechanism. Thus iLAT can efficiently\nsynthesize the local image regions by key guidance information. Our iLAT is\nevaluated on various locally guided image syntheses, such as pose-guided person\nimage synthesis and face editing. Both the quantitative and qualitative results\nshow the efficacy of our model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_C/0/1/0/all/0/1\">Chenjie Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1\">Yuxin Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengrong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chengming Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_X/0/1/0/all/0/1\">XiangYang Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yanwei Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Resolution Network. (arXiv:2106.02898v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.02898","description":"<p>Deep convolutional neural networks (CNNs) are often of sophisticated design\nwith numerous learnable parameters for the accuracy reason. To alleviate the\nexpensive costs of deploying them on mobile devices, recent works have made\nhuge efforts for excavating redundancy in pre-defined architectures.\nNevertheless, the redundancy on the input resolution of modern CNNs has not\nbeen fully investigated, i.e., the resolution of input image is fixed. In this\npaper, we observe that the smallest resolution for accurately predicting the\ngiven image is different using the same neural network. To this end, we propose\na novel dynamic-resolution network (DRNet) in which the input resolution is\ndetermined dynamically based on each input sample. Wherein, a resolution\npredictor with negligible computational costs is explored and optimized jointly\nwith the desired network. Specifically, the predictor learns the smallest\nresolution that can retain and even exceed the original recognition accuracy\nfor each image. During the inference, each input image will be resized to its\npredicted resolution for minimizing the overall computation burden. We then\nconduct extensive experiments on several benchmark networks and datasets. The\nresults show that our DRNet can be embedded in any off-the-shelf network\narchitecture to obtain a considerable reduction in computational complexity.\nFor instance, DR-ResNet-50 achieves similar performance with an about 34%\ncomputation reduction, while gains 1.4% accuracy increase with 10% computation\nreduction compared to the original ResNet-50 on ImageNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1\">Mingjian Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1\">Kai Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_E/0/1/0/all/0/1\">Enhua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qiulin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_Y/0/1/0/all/0/1\">Ying Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_Z/0/1/0/all/0/1\">Zhenzhong Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunhe Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the relation between statistical learning and perceptual distances. (arXiv:2106.04427v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.04427","description":"<p>It has been demonstrated many times that the behavior of the human visual\nsystem is connected to the statistics of natural images. Since machine learning\nrelies on the statistics of training data as well, the above connection has\ninteresting implications when using perceptual distances (which mimic the\nbehavior of the human visual system) as a loss function. In this paper, we aim\nto unravel the non-trivial relationships between the probability distribution\nof the data, perceptual distances, and unsupervised machine learning. To this\nend, we show that perceptual sensitivity is correlated with the probability of\nan image in its close neighborhood. We also explore the relation between\ndistances induced by autoencoders and the probability distribution of the\ntraining data, as well as how these induced distances are correlated with human\nperception. Finally, we find perceptual distances do not always lead to\nnoticeable gains in performance over Euclidean distance in common image\nprocessing tasks, except when data is scarce and the perceptual distance\nprovides regularization. We propose this may be due to a \\emph{double-counting}\neffect of the image statistics, once in the perceptual distance and once in the\ntraining procedure.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hepburn_A/0/1/0/all/0/1\">Alexander Hepburn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laparra_V/0/1/0/all/0/1\">Valero Laparra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santos_Rodriguez_R/0/1/0/all/0/1\">Raul Santos-Rodriguez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balle_J/0/1/0/all/0/1\">Johannes Ball&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malo_J/0/1/0/all/0/1\">Jes&#xfa;s Malo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparse Training via Boosting Pruning Plasticity with Neuroregeneration. (arXiv:2106.10404v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.10404","description":"<p>Works on lottery ticket hypothesis (LTH) and single-shot network pruning\n(SNIP) have raised a lot of attention currently on post-training pruning\n(iterative magnitude pruning), and before-training pruning (pruning at\ninitialization). The former method suffers from an extremely large computation\ncost and the latter usually struggles with insufficient performance. In\ncomparison, during-training pruning, a class of pruning methods that\nsimultaneously enjoys the training/inference efficiency and the comparable\nperformance, temporarily, has been less explored. To better understand\nduring-training pruning, we quantitatively study the effect of pruning\nthroughout training from the perspective of pruning plasticity (the ability of\nthe pruned networks to recover the original performance). Pruning plasticity\ncan help explain several other empirical observations about neural network\npruning in literature. We further find that pruning plasticity can be\nsubstantially improved by injecting a brain-inspired mechanism called\nneuroregeneration, i.e., to regenerate the same number of connections as\npruned. We design a novel gradual magnitude pruning (GMP) method, named gradual\npruning with zero-cost neuroregeneration (\\textbf{GraNet}), that advances state\nof the art. Perhaps most impressively, its sparse-to-sparse version for the\nfirst time boosts the sparse-to-sparse training performance over various\ndense-to-sparse methods with ResNet-50 on ImageNet without extending the\ntraining time. We release all codes in\nhttps://github.com/Shiweiliuiiiiiii/GraNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shiwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianlong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaohan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atashgahi_Z/0/1/0/all/0/1\">Zahra Atashgahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_L/0/1/0/all/0/1\">Lu Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kou_H/0/1/0/all/0/1\">Huanyu Kou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Li Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pechenizkiy_M/0/1/0/all/0/1\">Mykola Pechenizkiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mocanu_D/0/1/0/all/0/1\">Decebal Constantin Mocanu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Alias-Free Generative Adversarial Networks. (arXiv:2106.12423v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.12423","description":"<p>We observe that despite their hierarchical convolutional nature, the\nsynthesis process of typical generative adversarial networks depends on\nabsolute pixel coordinates in an unhealthy manner. This manifests itself as,\ne.g., detail appearing to be glued to image coordinates instead of the surfaces\nof depicted objects. We trace the root cause to careless signal processing that\ncauses aliasing in the generator network. Interpreting all signals in the\nnetwork as continuous, we derive generally applicable, small architectural\nchanges that guarantee that unwanted information cannot leak into the\nhierarchical synthesis process. The resulting networks match the FID of\nStyleGAN2 but differ dramatically in their internal representations, and they\nare fully equivariant to translation and rotation even at subpixel scales. Our\nresults pave the way for generative models better suited for video and\nanimation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karras_T/0/1/0/all/0/1\">Tero Karras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aittala_M/0/1/0/all/0/1\">Miika Aittala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laine_S/0/1/0/all/0/1\">Samuli Laine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harkonen_E/0/1/0/all/0/1\">Erik H&#xe4;rk&#xf6;nen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hellsten_J/0/1/0/all/0/1\">Janne Hellsten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lehtinen_J/0/1/0/all/0/1\">Jaakko Lehtinen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aila_T/0/1/0/all/0/1\">Timo Aila</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hear Me Out: Fusional Approaches for Audio Augmented Temporal Action Localization. (arXiv:2106.14118v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.14118","description":"<p>State of the art architectures for untrimmed video Temporal Action\nLocalization (TAL) have only considered RGB and Flow modalities, leaving the\ninformation-rich audio modality totally unexploited. Audio fusion has been\nexplored for the related but arguably easier problem of trimmed (clip-level)\naction recognition. However, TAL poses a unique set of challenges. In this\npaper, we propose simple but effective fusion-based approaches for TAL. To the\nbest of our knowledge, our work is the first to jointly consider audio and\nvideo modalities for supervised TAL. We experimentally show that our schemes\nconsistently improve performance for state of the art video-only TAL\napproaches. Specifically, they help achieve new state of the art performance on\nlarge-scale benchmark datasets - ActivityNet-1.3 (54.34 mAP@0.5) and THUMOS14\n(57.18 mAP@0.5). Our experiments include ablations involving multiple fusion\nschemes, modality combinations and TAL architectures. Our code, models and\nassociated data are available at https://github.com/skelemoa/tal-hmo.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bagchi_A/0/1/0/all/0/1\">Anurag Bagchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahmood_J/0/1/0/all/0/1\">Jazib Mahmood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandes_D/0/1/0/all/0/1\">Dolton Fernandes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarvadevabhatla_R/0/1/0/all/0/1\">Ravi Kiran Sarvadevabhatla</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"JPGNet: Joint Predictive Filtering and Generative Network for Image Inpainting. (arXiv:2107.04281v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.04281","description":"<p>Image inpainting aims to restore the missing regions of corrupted images and\nmake the recovery result identical to the originally complete image, which is\ndifferent from the common generative task emphasizing the naturalness or\nrealism of generated images. Nevertheless, existing works usually regard it as\na pure generation problem and employ cutting-edge deep generative techniques to\naddress it. The generative networks can fill the main missing parts with\nrealistic contents but usually distort the local structures or introduce\nobvious artifacts. In this paper, for the first time, we formulate image\ninpainting as a mix of two problems, predictive filtering and deep generation.\nPredictive filtering is good at preserving local structures and removing\nartifacts but falls short to complete the large missing regions. The deep\ngenerative network can fill the numerous missing pixels based on the\nunderstanding of the whole scene but hardly restores the details identical to\nthe original ones. To make use of their respective advantages, we propose the\njoint predictive filtering and generative network (JPGNet) that contains three\nbranches: predictive filtering &amp; uncertainty network (PFUNet), deep generative\nnetwork, and uncertainty-aware fusion network (UAFNet). The PFUNet can\nadaptively predict pixel-wise kernels for filtering-based inpainting according\nto the input image and output an uncertainty map. This map indicates the pixels\nshould be processed by filtering or generative networks, which is further fed\nto the UAFNet for a smart combination between filtering and generative results.\nNote that, our method as a novel inpainting framework can benefit any existing\ngeneration-based methods. We validate our method on three public datasets,\nDunhuang, Places2, and CelebA, and demonstrate that our method can enhance\nthree state-of-the-art generative methods significantly with slightly extra\ntime costs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1\">Qing Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoguang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Juefei_Xu_F/0/1/0/all/0/1\">Felix Juefei-Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hongkai Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+wang_S/0/1/0/all/0/1\">Song wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AdvFilter: Predictive Perturbation-aware Filtering against Adversarial Attack via Multi-domain Learning. (arXiv:2107.06501v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.06501","description":"<p>High-level representation-guided pixel denoising and adversarial training are\nindependent solutions to enhance the robustness of CNNs against adversarial\nattacks by pre-processing input data and re-training models, respectively. Most\nrecently, adversarial training techniques have been widely studied and improved\nwhile the pixel denoising-based method is getting less attractive. However, it\nis still questionable whether there exists a more advanced pixel\ndenoising-based method and whether the combination of the two solutions\nbenefits each other. To this end, we first comprehensively investigate two\nkinds of pixel denoising methods for adversarial robustness enhancement (i.e.,\nexisting additive-based and unexplored filtering-based methods) under the loss\nfunctions of image-level and semantic-level, respectively, showing that\npixel-wise filtering can obtain much higher image quality (e.g., higher PSNR)\nas well as higher robustness (e.g., higher accuracy on adversarial examples)\nthan existing pixel-wise additive-based method. However, we also observe that\nthe robustness results of the filtering-based method rely on the perturbation\namplitude of adversarial examples used for training. To address this problem,\nwe propose predictive perturbation-aware &amp; pixel-wise filtering}, where\ndual-perturbation filtering and an uncertainty-aware fusion module are designed\nand employed to automatically perceive the perturbation amplitude during the\ntraining and testing process. The method is termed as AdvFilter. Moreover, we\ncombine adversarial pixel denoising methods with three adversarial\ntraining-based methods, hinting that considering data and models jointly is\nable to achieve more robust CNNs. The experiments conduct on NeurIPS-2017DEV,\nSVHN and CIFAR10 datasets and show advantages over enhancing CNNs' robustness,\nhigh generalization to different models and noise levels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yihao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1\">Qing Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Juefei_Xu_F/0/1/0/all/0/1\">Felix Juefei-Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_W/0/1/0/all/0/1\">Weikai Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_G/0/1/0/all/0/1\">Geguang Pu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diff-Net: Image Feature Difference based High-Definition Map Change Detection for Autonomous Driving. (arXiv:2107.07030v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.07030","description":"<p>Up-to-date High-Definition (HD) maps are essential for self-driving cars. To\nachieve constantly updated HD maps, we present a deep neural network (DNN),\nDiff-Net, to detect changes in them. Compared to traditional methods based on\nobject detectors, the essential design in our work is a parallel feature\ndifference calculation structure that infers map changes by comparing features\nextracted from the camera and rasterized images. To generate these rasterized\nimages, we project map elements onto images in the camera view, yielding\nmeaningful map representations that can be consumed by a DNN accordingly. As we\nformulate the change detection task as an object detection problem, we leverage\nthe anchor-based structure that predicts bounding boxes with different change\nstatus categories. To the best of our knowledge, the proposed method is the\nfirst end-to-end network that tackles the high-definition map change detection\ntask, yielding a single stage solution. Furthermore, rather than relying on\nsingle frame input, we introduce a spatio-temporal fusion module that fuses\nfeatures from history frames into the current, thus improving the overall\nperformance. Finally, we comprehensively validate our method's effectiveness\nusing freshly collected datasets. Results demonstrate that our Diff-Net\nachieves better performance than the baseline methods and is ready to be\nintegrated into a map production pipeline maintaining an up-to-date HD map.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Lei He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Shengjie Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaoqing Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1\">Ning Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Shiyu Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Adversarially Blur Visual Object Tracking. (arXiv:2107.12085v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.12085","description":"<p>Motion blur caused by the moving of the object or camera during the exposure\ncan be a key challenge for visual object tracking, affecting tracking accuracy\nsignificantly. In this work, we explore the robustness of visual object\ntrackers against motion blur from a new angle, i.e., adversarial blur attack\n(ABA). Our main objective is to online transfer input frames to their natural\nmotion-blurred counterparts while misleading the state-of-the-art trackers\nduring the tracking process. To this end, we first design the motion blur\nsynthesizing method for visual tracking based on the generation principle of\nmotion blur, considering the motion information and the light accumulation\nprocess. With this synthetic method, we propose optimization-based ABA (OP-ABA)\nby iteratively optimizing an adversarial objective function against the\ntracking w.r.t. the motion and light accumulation parameters. The OP-ABA is\nable to produce natural adversarial examples but the iteration can cause heavy\ntime cost, making it unsuitable for attacking real-time trackers. To alleviate\nthis issue, we further propose one-step ABA (OS-ABA) where we design and train\na joint adversarial motion and accumulation predictive network (JAMANet) with\nthe guidance of OP-ABA, which is able to efficiently estimate the adversarial\nmotion and accumulation parameters in a one-step way. The experiments on four\npopular datasets (e.g., OTB100, VOT2018, UAV123, and LaSOT) demonstrate that\nour methods are able to cause significant accuracy drops on four\nstate-of-the-art trackers with high transferability. Please find the source\ncode at\n\\href{https://github.com/tsingqguo/ABA}{https://github.com/tsingqguo/ABA}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1\">Qing Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Ziyi Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Juefei_Xu_F/0/1/0/all/0/1\">Felix Juefei-Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xiaofei Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jianjun Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rain Removal and Illumination Enhancement Done in One Go. (arXiv:2108.03873v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2108.03873","description":"<p>Rain removal plays an important role in the restoration of degraded images.\nRecently, data-driven methods have achieved remarkable success. However, these\napproaches neglect that the appearance of rain is often accompanied by low\nlight conditions, which will further degrade the image quality. Therefore, it\nis very indispensable to jointly remove the rain and enhance the light for\nreal-world rain image restoration. In this paper, we aim to address this\nproblem from two aspects. First, we proposed a novel entangled network, namely\nEMNet, which can remove the rain and enhance illumination in one go.\nSpecifically, two encoder-decoder networks interact complementary information\nthrough entanglement structure, and parallel rain removal and illumination\nenhancement. Considering that the encoder-decoder structure is unreliable in\npreserving spatial details, we employ a detail recovery network to restore the\ndesired fine texture. Second, we present a new synthetic dataset, namely\nDarkRain, to boost the development of rain image restoration algorithms in\npractical scenarios. DarkRain not only contains different degrees of rain, but\nalso considers different lighting conditions, and more realistically simulates\nthe rainfall in the real world. EMNet is extensively evaluated on the proposed\nbenchmark and achieves state-of-the-art results. In addition, after a simple\ntransformation, our method outshines existing methods in both rain removal and\nlow-light image enhancement. The source code and dataset will be made publicly\navailable later.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wan_Y/0/1/0/all/0/1\">Yecong Wan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheng_Y/0/1/0/all/0/1\">Yuanshuo Cheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shao_M/0/1/0/all/0/1\">Mingwen Shao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Iterative Distillation for Better Uncertainty Estimates in Multitask Emotion Recognition. (arXiv:2108.04228v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.04228","description":"<p>When recognizing emotions, subtle nuances in displays of emotion generate\nambiguity or uncertainty in emotion perception. Emotion uncertainty has been\npreviously interpreted as inter-rater disagreement among multiple annotators.\nIn this paper, we consider a more common and challenging scenario: modeling\nemotion uncertainty when only single emotion labels are available. From a\nBayesian perspective, we propose to use deep ensembles to capture uncertainty\nfor multiple emotion descriptors, i.e., action units, discrete expression\nlabels and continuous descriptors. We further apply iterative\nself-distillation. Iterative distillation over multiple generations\nsignificantly improves performance in both emotion recognition and uncertainty\nestimation. Our method generates single student models that provide accurate\nestimates of uncertainty for in-domain samples and a student ensemble that can\ndetect out-of-domain samples. Our experiments on emotion recognition and\nuncertainty estimation using the Aff-wild2 dataset demonstrate that our\nalgorithm gives more reliable uncertainty estimates than both Temperature\nScaling and Monte Carol Dropout.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_D/0/1/0/all/0/1\">Didan Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Liang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1\">Bertram E. Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Classification of Abnormal Hand Movement for Aiding in Autism Detection: Machine Learning Study. (arXiv:2108.07917v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.07917","description":"<p>A formal autism diagnosis is an inefficient and lengthy process. Families\noften have to wait years before receiving a diagnosis for their child; some may\nnot receive one at all due to this delay. One approach to this problem is to\nuse digital technologies to detect the presence of behaviors related to autism,\nwhich in aggregate may lead to remote and automated diagnostics. One of the\nstrongest indicators of autism is stimming, which is a set of repetitive,\nself-stimulatory behaviors such as hand flapping, headbanging, and spinning.\nUsing computer vision to detect hand flapping is especially difficult due to\nthe sparsity of public training data in this space and excessive shakiness and\nmotion in such data. Our work demonstrates a novel method that overcomes these\nissues: we use hand landmark detection over time as a feature representation\nwhich is then fed into a Long Short-Term Memory (LSTM) model. We achieve a\nvalidation accuracy and F1 Score of about 72% on detecting whether videos from\nthe Self-Stimulatory Behaviour Dataset (SSBD) contain hand flapping or not. Our\nbest model also predicts accurately on external videos we recorded of ourselves\noutside of the dataset it was trained on. This model uses less than 26,000\nparameters, providing promise for fast deployment into ubiquitous and wearable\ndigital settings for a remote autism diagnosis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lakkapragada_A/0/1/0/all/0/1\">Anish Lakkapragada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kline_A/0/1/0/all/0/1\">Aaron Kline</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mutlu_O/0/1/0/all/0/1\">Onur Cezmi Mutlu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paskov_K/0/1/0/all/0/1\">Kelley Paskov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chrisman_B/0/1/0/all/0/1\">Brianna Chrisman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stockham_N/0/1/0/all/0/1\">Nate Stockham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Washington_P/0/1/0/all/0/1\">Peter Washington</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wall_D/0/1/0/all/0/1\">Dennis Wall</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LLVIP: A Visible-infrared Paired Dataset for Low-light Vision. (arXiv:2108.10831v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.10831","description":"<p>It is very challenging for various visual tasks such as image fusion,\npedestrian detection and image-to-image translation in low light conditions due\nto the loss of effective target areas. In this case, infrared and visible\nimages can be used together to provide both rich detail information and\neffective target areas. In this paper, we present LLVIP, a visible-infrared\npaired dataset for low-light vision. This dataset contains 30976 images, or\n15488 pairs, most of which were taken at very dark scenes, and all of the\nimages are strictly aligned in time and space. Pedestrians in the dataset are\nlabeled. We compare the dataset with other visible-infrared datasets and\nevaluate the performance of some popular visual algorithms including image\nfusion, pedestrian detection and image-to-image translation on the dataset. The\nexperimental results demonstrate the complementary effect of fusion on image\ninformation, and find the deficiency of existing algorithms of the three visual\ntasks in very low-light conditions. We believe the LLVIP dataset will\ncontribute to the community of computer vision by promoting image fusion,\npedestrian detection and image-to-image translation in very low-light\napplications. The dataset is being released in\nhttps://bupt-ai-cz.github.io/LLVIP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1\">Xinyu Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chuang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Minzhen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_W/0/1/0/all/0/1\">Wenqi Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wenli Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A realistic approach to generate masked faces applied on two novel masked face recognition data sets. (arXiv:2109.01745v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.01745","description":"<p>The COVID-19 pandemic raises the problem of adapting face recognition systems\nto the new reality, where people may wear surgical masks to cover their noses\nand mouths. Traditional data sets (e.g., CelebA, CASIA-WebFace) used for\ntraining these systems were released before the pandemic, so they now seem\nunsuited due to the lack of examples of people wearing masks. We propose a\nmethod for enhancing data sets containing faces without masks by creating\nsynthetic masks and overlaying them on faces in the original images. Our method\nrelies on SparkAR Studio, a developer program made by Facebook that is used to\ncreate Instagram face filters. In our approach, we use 9 masks of different\ncolors, shapes and fabrics. We employ our method to generate a number of\n445,446 (90%) samples of masks for the CASIA-WebFace data set and 196,254\n(96.8%) masks for the CelebA data set, releasing the mask images at\nhttps://github.com/securifai/masked_faces. We show that our method produces\nsignificantly more realistic training examples of masks overlaid on faces by\nasking volunteers to qualitatively compare it to other methods or data sets\ndesigned for the same task. We also demonstrate the usefulness of our method by\nevaluating state-of-the-art face recognition systems (FaceNet, VGG-face,\nArcFace) trained on our enhanced data sets and showing that they outperform\nequivalent systems trained on original data sets (containing faces without\nmasks) or competing data sets (containing masks generated by related methods),\nwhen the test benchmarks contain masked faces.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mare_T/0/1/0/all/0/1\">Tudor Mare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duta_G/0/1/0/all/0/1\">Georgian Duta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Georgescu_M/0/1/0/all/0/1\">Mariana-Iuliana Georgescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sandru_A/0/1/0/all/0/1\">Adrian Sandru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alexe_B/0/1/0/all/0/1\">Bogdan Alexe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Popescu_M/0/1/0/all/0/1\">Marius Popescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ionescu_R/0/1/0/all/0/1\">Radu Tudor Ionescu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fishr: Invariant Gradient Variances for Out-of-distribution Generalization. (arXiv:2109.02934v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2109.02934","description":"<p>Learning robust models that generalize well under changes in the data\ndistribution is critical for real-world applications. To this end, there has\nbeen a growing surge of interest to learn simultaneously from multiple training\ndomains -- while enforcing different types of invariance across those domains.\nYet, all existing approaches fail to show systematic benefits under controlled\nevaluation protocols. In this paper, we introduce a new regularization -- named\nFishr -- that enforces domain invariance in the space of the gradients of the\nloss: specifically, the domain-level variances of gradients are matched across\ntraining domains. Our approach is based on the close relations between the\ngradient covariance, the Fisher Information and the Hessian of the loss: in\nparticular, we show that Fishr eventually aligns the domain-level loss\nlandscapes locally around the final weights. Extensive experiments demonstrate\nthe effectiveness of Fishr for out-of-distribution generalization. Notably,\nFishr improves the state of the art on the DomainBed benchmark and performs\nconsistently better than Empirical Risk Minimization. The code is released at\nhttps://github.com/alexrame/fishr.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rame_A/0/1/0/all/0/1\">Alexandre Rame</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dancette_C/0/1/0/all/0/1\">Corentin Dancette</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cord_M/0/1/0/all/0/1\">Matthieu Cord</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RobustART: Benchmarking Robustness on Architecture Design and Training Techniques. (arXiv:2109.05211v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.05211","description":"<p>Deep neural networks (DNNs) are vulnerable to adversarial noises, which\nmotivates the benchmark of model robustness. Existing benchmarks mainly focus\non evaluating the defenses, but there are no comprehensive studies of how\narchitecture design and general training techniques affect robustness.\nComprehensively benchmarking their relationships will be highly beneficial for\nbetter understanding and developing robust DNNs. Thus, we propose RobustART,\nthe first comprehensive Robustness investigation benchmark on ImageNet\n(including open-source toolkit, pre-trained model zoo, datasets, and analyses)\nregarding ARchitecture design (44 human-designed off-the-shelf architectures\nand 1200+ networks from neural architecture search) and Training techniques\n(10+ general techniques, e.g., data augmentation) towards diverse noises\n(adversarial, natural, and system noises). Extensive experiments revealed and\nsubstantiated several insights for the first time, for example: (1) adversarial\ntraining largely improves the clean accuracy and all types of robustness for\nTransformers and MLP-Mixers; (2) with comparable sizes, CNNs &gt; Transformers &gt;\nMLP-Mixers on robustness against natural and system noises; Transformers &gt;\nMLP-Mixers &gt; CNNs on adversarial robustness; (3) for some light-weight\narchitectures (e.g., EfficientNet, MobileNetV2, and MobileNetV3), increasing\nmodel sizes or using extra training data cannot improve robustness. Our\nbenchmark <a href=\"http://robust.art/\">this http URL</a> : (1) presents an open-source platform for\nconducting comprehensive evaluation on diverse robustness types; (2) provides a\nvariety of pre-trained models with different training techniques to facilitate\nrobustness evaluation; (3) proposes a new view to better understand the\nmechanism towards designing robust DNN architectures, backed up by the\nanalysis. We will continuously contribute to building this ecosystem for the\ncommunity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Shiyu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_R/0/1/0/all/0/1\">Ruihao Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Aishan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiakai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xinyun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fengwei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xianglong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Dawn Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1\">Alan Yuille</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip H.S. Torr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Online Unsupervised Learning of Visual Representations and Categories. (arXiv:2109.05675v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.05675","description":"<p>Real world learning scenarios involve a nonstationary distribution of classes\nwith sequential dependencies among the samples, in contrast to the standard\nmachine learning formulation of drawing samples independently from a fixed,\ntypically uniform distribution. Furthermore, real world interactions demand\nlearning on-the-fly from few or no class labels. In this work, we propose an\nunsupervised model that simultaneously performs online visual representation\nlearning and few-shot learning of new categories without relying on any class\nlabels. Our model is a prototype-based memory network with a control component\nthat determines when to form a new class prototype. We formulate it as an\nonline Gaussian mixture model, where components are created online with only a\nsingle new example, and assignments do not have to be balanced, which permits\nan approximation to natural imbalanced distributions from uncurated raw data.\nLearning includes a contrastive loss that encourages different views of the\nsame image to be assigned to the same prototype. The result is a mechanism that\nforms categorical representations of objects in nonstationary environments.\nExperiments show that our method can learn from an online stream of visual\ninput data and is significantly better at category recognition compared to\nstate-of-the-art self-supervised learning methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ren_M/0/1/0/all/0/1\">Mengye Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scott_T/0/1/0/all/0/1\">Tyler R. Scott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iuzzolino_M/0/1/0/all/0/1\">Michael L. Iuzzolino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mozer_M/0/1/0/all/0/1\">Michael C. Mozer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zemel_R/0/1/0/all/0/1\">Richard Zemel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BabelCalib: A Universal Approach to Calibrating Central Cameras. (arXiv:2109.09704v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.09704","description":"<p>Existing calibration methods occasionally fail for large field-of-view\ncameras due to the non-linearity of the underlying problem and the lack of good\ninitial values for all parameters of the used camera model. This might occur\nbecause a simpler projection model is assumed in an initial step, or a poor\ninitial guess for the internal parameters is pre-defined. A lot of the\ndifficulties of general camera calibration lie in the use of a forward\nprojection model. We side-step these challenges by first proposing a solver to\ncalibrate the parameters in terms of a back-projection model and then regress\nthe parameters for a target forward model. These steps are incorporated in a\nrobust estimation framework to cope with outlying detections. Extensive\nexperiments demonstrate that our approach is very reliable and returns the most\naccurate calibration parameters as measured on the downstream task of absolute\npose estimation on test sets. The code is released at\nhttps://github.com/ylochman/babelcalib.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lochman_Y/0/1/0/all/0/1\">Yaroslava Lochman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liepieshov_K/0/1/0/all/0/1\">Kostiantyn Liepieshov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jianhui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perdoch_M/0/1/0/all/0/1\">Michal Perdoch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zach_C/0/1/0/all/0/1\">Christopher Zach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pritts_J/0/1/0/all/0/1\">James Pritts</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated segmentation and extraction of posterior eye segment using OCT scans. (arXiv:2109.10000v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2109.10000","description":"<p>This paper proposes an automated method for the segmentation and extraction\nof the posterior segment of the human eye, including the vitreous, retina,\nchoroid, and sclera compartments, using multi-vendor optical coherence\ntomography (OCT) scans. The proposed method works in two phases. First extracts\nthe retinal pigment epithelium (RPE) layer by applying the adaptive\nthresholding technique to identify the retina-choroid junction. Then, it\nexploits the structure tensor guided approach to extract the inner limiting\nmembrane (ILM) and the choroidal stroma (CS) layers, locating the\nvitreous-retina and choroid-sclera junctions in the candidate OCT scan.\nFurthermore, these three junction boundaries are utilized to conduct posterior\neye compartmentalization effectively for both healthy and disease eye OCT\nscans. The proposed framework is evaluated over 1000 OCT scans, where it\nobtained the mean intersection over union (IoU) and mean Dice similarity\ncoefficient (DSC) scores of 0.874 and 0.930, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hassan_B/0/1/0/all/0/1\">Bilal Hassan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hassan_T/0/1/0/all/0/1\">Taimur Hassan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ahmed_R/0/1/0/all/0/1\">Ramsha Ahmed</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qin_S/0/1/0/all/0/1\">Shiyin Qin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Werghi_N/0/1/0/all/0/1\">Naoufel Werghi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ProTo: Program-Guided Transformer for Program-Guided Tasks. (arXiv:2110.00804v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.00804","description":"<p>Programs, consisting of semantic and structural information, play an\nimportant role in the communication between humans and agents. Towards learning\ngeneral program executors to unify perception, reasoning, and decision making,\nwe formulate program-guided tasks which require learning to execute a given\nprogram on the observed task specification. Furthermore, we propose the\nProgram-guided Transformer (ProTo), which integrates both semantic and\nstructural guidance of a program by leveraging cross-attention and masked\nself-attention to pass messages between the specification and routines in the\nprogram. ProTo executes a program in a learned latent space and enjoys stronger\nrepresentation ability than previous neural-symbolic approaches. We demonstrate\nthat ProTo significantly outperforms the previous state-of-the-art methods on\nGQA visual reasoning and 2D Minecraft policy learning datasets. Additionally,\nProTo demonstrates better generalization to unseen, complex, and human-written\nprograms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zelin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samel_K/0/1/0/all/0/1\">Karan Samel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Binghong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1\">Le Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic-Guided Zero-Shot Learning for Low-Light Image/Video Enhancement. (arXiv:2110.00970v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.00970","description":"<p>Low-light images challenge both human perceptions and computer vision\nalgorithms. It is crucial to make algorithms robust to enlighten low-light\nimages for computational photography and computer vision applications such as\nreal-time detection and segmentation. This paper proposes a semantic-guided\nzero-shot low-light enhancement network which is trained in the absence of\npaired images, unpaired datasets, and segmentation annotation. Firstly, we\ndesign an enhancement factor extraction network using depthwise separable\nconvolution for an efficient estimate of the pixel-wise light deficiency of a\nlow-light image. Secondly, we propose a recurrent image enhancement network to\nprogressively enhance the low-light image with affordable model size. Finally,\nwe introduce an unsupervised semantic segmentation network for preserving the\nsemantic information during intensive enhancement. Extensive experiments on\nbenchmark datasets and a low-light video demonstrate that our model outperforms\nthe previous state-of-the-art qualitatively and quantitatively. We further\ndiscuss the benefits of the proposed method for low-light detection and\nsegmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Shen Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_G/0/1/0/all/0/1\">Gaurav Gupta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Incremental Class Learning using Variational Autoencoders with Similarity Learning. (arXiv:2110.01303v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.01303","description":"<p>Catastrophic forgetting in neural networks during incremental learning\nremains a challenging problem. Previous research investigated catastrophic\nforgetting in fully connected networks, with some earlier work exploring\nactivation functions and learning algorithms. Applications of neural networks\nhave been extended to include similarity learning. It is of significant\ninterest to understand how similarity learning loss functions would be affected\nby catastrophic forgetting. Our research investigates catastrophic forgetting\nfor four well-known similarity-based loss functions during incremental class\nlearning. The loss functions are angular, contrastive, centre, and triplet\nloss. Our results show that the rate of catastrophic forgetting is different\nacross loss functions on multiple datasets. The angular loss was least\naffected, followed by contrastive, triplet loss, and centre loss with good\nmining techniques. We implemented three existing incremental learning\ntechniques, iCaRL, EWC, and EBLL. We further proposed our novel technique using\nVAEs to generate representation as exemplars that are passed through\nintermediate layers of the network. Our method outperformed the three existing\ntechniques. We have shown that we do not require stored images as exemplars for\nincremental learning with similarity learning. The generated representations\ncan help preserve regions of the embedding space used by prior knowledge so\nthat new knowledge will not ``overwrite'' prior knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huo_J/0/1/0/all/0/1\">Jiahao Huo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zyl_T/0/1/0/all/0/1\">Terence L. van Zyl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Causal Representation for Face Transfer across Large Appearance Gap. (arXiv:2110.01571v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.01571","description":"<p>Identity transfer often faces the challenge of generalizing to new situations\nwhere large pose and expression or background gaps exist between source and\ntarget face images. To improve generalization in such situations, biases take a\nkey role~\\cite{mitchell_1980_bias}. This paper proposes an Errors-in-Variables\nAdapter (EVA) model to induce learning of proper generalizations by explicitly\nemploying biases to identity estimation based on prior knowledge about the\ntarget situation. To better match the source face with the target situation in\nterms of pose, expression, and background factors, we model the bias as a\ncausal effect of the target situation on source identity and estimate this\neffect through a controlled intervention trial. To achieve smoother transfer\nfor the target face across the identity gap, we eliminate the target face\nspecificity through multiple kernel regressions. The kernels are used to\nconstrain the regressions to operate only on identity information in the\ninternal representations of the target image, while leaving other perceptual\ninformation invariant. Combining these post-regression representations with the\nbiased estimation for identity, EVA shows impressive performance even in the\npresence of large gaps, providing empirical evidence supporting the utility of\nthe inductive biases in identity estimation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_G/0/1/0/all/0/1\">Gege Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Huaibo Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1\">Chaoyou Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1\">Ran He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weak Novel Categories without Tears: A Survey on Weak-Shot Learning. (arXiv:2110.02651v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.02651","description":"<p>Deep learning is a data-hungry approach, which requires massive training\ndata. However, it is time-consuming and labor-intensive to collect abundant\nfully-annotated training data for all categories. Assuming the existence of\nbase categories with adequate fully-annotated training samples, different\nparadigms requiring fewer training samples or weaker annotations for novel\ncategories have attracted growing research interest. Among them, zero-shot\n(resp., few-shot) learning explores using zero (resp., a few) training samples\nfor novel categories, which lowers the quantity requirement for novel\ncategories. Instead, weak-shot learning lowers the quality requirement for\nnovel categories. Specifically, sufficient training samples are collected for\nnovel categories but they only have weak annotations. In different tasks, weak\nannotations are presented in different forms (e.g., noisy labels for image\nclassification, image labels for object detection, bounding boxes for\nsegmentation), similar to the definitions in weakly supervised learning.\nTherefore, weak-shot learning can also be treated as weakly supervised learning\nwith auxiliary fully supervised categories. In this paper, we discuss the\nexisting weak-shot learning methodologies in different tasks and summarize the\ncodes at https://github.com/bcmi/Awesome-Weak-Shot-Learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Niu_L/0/1/0/all/0/1\">Li Niu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepBBS: Deep Best Buddies for Point Cloud Registration. (arXiv:2110.03016v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.03016","description":"<p>Recently, several deep learning approaches have been proposed for point cloud\nregistration. These methods train a network to generate a representation that\nhelps finding matching points in two 3D point clouds. Finding good matches\nallows them to calculate the transformation between the point clouds\naccurately. Two challenges of these techniques are dealing with occlusions and\ngeneralizing to objects of classes unseen during training. This work proposes\nDeepBBS, a novel method for learning a representation that takes into account\nthe best buddy distance between points during training. Best Buddies (i.e.,\nmutual nearest neighbors) are pairs of points nearest to each other. The Best\nBuddies criterion is a strong indication for correct matches that, in turn,\nleads to accurate registration. Our experiments show improved performance\ncompared to previous methods. In particular, our learned representation leads\nto an accurate registration for partial shapes and in unseen categories.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hezroni_I/0/1/0/all/0/1\">Itan Hezroni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drory_A/0/1/0/all/0/1\">Amnon Drory</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giryes_R/0/1/0/all/0/1\">Raja Giryes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avidan_S/0/1/0/all/0/1\">Shai Avidan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Propagating State Uncertainty Through Trajectory Forecasting. (arXiv:2110.03267v3 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2110.03267","description":"<p>Uncertainty pervades through the modern robotic autonomy stack, with nearly\nevery component (e.g., sensors, detection, classification, tracking, behavior\nprediction) producing continuous or discrete probabilistic distributions.\nTrajectory forecasting, in particular, is surrounded by uncertainty as its\ninputs are produced by (noisy) upstream perception and its outputs are\npredictions that are often probabilistic for use in downstream planning.\nHowever, most trajectory forecasting methods do not account for upstream\nuncertainty, instead taking only the most-likely values. As a result,\nperceptual uncertainties are not propagated through forecasting and predictions\nare frequently overconfident. To address this, we present a novel method for\nincorporating perceptual state uncertainty in trajectory forecasting, a key\ncomponent of which is a new statistical distance-based loss function which\nencourages predicting uncertainties that better match upstream perception. We\nevaluate our approach both in illustrative simulations and on large-scale,\nreal-world data, demonstrating its efficacy in propagating perceptual state\nuncertainty through prediction and producing more calibrated predictions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ivanovic_B/0/1/0/all/0/1\">Boris Ivanovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yifeng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_S/0/1/0/all/0/1\">Shubham Shrivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakravarty_P/0/1/0/all/0/1\">Punarjay Chakravarty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavone_M/0/1/0/all/0/1\">Marco Pavone</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Meta-Learning with Task-Adaptive Loss Function for Few-Shot Learning. (arXiv:2110.03909v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.03909","description":"<p>In few-shot learning scenarios, the challenge is to generalize and perform\nwell on new unseen examples when only very few labeled examples are available\nfor each task. Model-agnostic meta-learning (MAML) has gained the popularity as\none of the representative few-shot learning methods for its flexibility and\napplicability to diverse problems. However, MAML and its variants often resort\nto a simple loss function without any auxiliary loss function or regularization\nterms that can help achieve better generalization. The problem lies in that\neach application and task may require different auxiliary loss function,\nespecially when tasks are diverse and distinct. Instead of attempting to\nhand-design an auxiliary loss function for each application and task, we\nintroduce a new meta-learning framework with a loss function that adapts to\neach task. Our proposed framework, named Meta-Learning with Task-Adaptive Loss\nFunction (MeTAL), demonstrates the effectiveness and the flexibility across\nvarious domains, such as few-shot classification and few-shot regression.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baik_S/0/1/0/all/0/1\">Sungyong Baik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Janghoon Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Heewon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_D/0/1/0/all/0/1\">Dohee Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_J/0/1/0/all/0/1\">Jaesik Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kyoung Mu Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised Point Cloud Prediction Using 3D Spatio-temporal Convolutional Networks. (arXiv:2110.04076v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.04076","description":"<p>Exploiting past 3D LiDAR scans to predict future point clouds is a promising\nmethod for autonomous mobile systems to realize foresighted state estimation,\ncollision avoidance, and planning. In this paper, we address the problem of\npredicting future 3D LiDAR point clouds given a sequence of past LiDAR scans.\nEstimating the future scene on the sensor level does not require any preceding\nsteps as in localization or tracking systems and can be trained\nself-supervised. We propose an end-to-end approach that exploits a 2D range\nimage representation of each 3D LiDAR scan and concatenates a sequence of range\nimages to obtain a 3D tensor. Based on such tensors, we develop an\nencoder-decoder architecture using 3D convolutions to jointly aggregate spatial\nand temporal information of the scene and to predict the future 3D point\nclouds. We evaluate our method on multiple datasets and the experimental\nresults suggest that our method outperforms existing point cloud prediction\narchitectures and generalizes well to new, unseen environments without\nadditional fine-tuning. Our method operates online and is faster than the\ncommon LiDAR frame rate of 10 Hz.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mersch_B/0/1/0/all/0/1\">Benedikt Mersch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xieyuanli Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Behley_J/0/1/0/all/0/1\">Jens Behley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stachniss_C/0/1/0/all/0/1\">Cyrill Stachniss</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Toward a Human-Level Video Understanding Intelligence. (arXiv:2110.04203v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2110.04203","description":"<p>We aim to develop an AI agent that can watch video clips and have a\nconversation with human about the video story. Developing video understanding\nintelligence is a significantly challenging task, and evaluation methods for\nadequately measuring and analyzing the progress of AI agent are lacking as\nwell. In this paper, we propose the Video Turing Test to provide effective and\npractical assessments of video understanding intelligence as well as\nhuman-likeness evaluation of AI agents. We define a general format and\nprocedure of the Video Turing Test and present a case study to confirm the\neffectiveness and usefulness of the proposed test.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Heo_Y/0/1/0/all/0/1\">Yu-Jung Heo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Minsu Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1\">Seongho Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_W/0/1/0/all/0/1\">Woo Suk Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_M/0/1/0/all/0/1\">Minjung Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_M/0/1/0/all/0/1\">Minjoon Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryu_J/0/1/0/all/0/1\">Jeh-Kwang Ryu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Byoung-Tak Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep learning-based person re-identification methods: A survey and outlook of recent works. (arXiv:2110.04764v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.04764","description":"<p>In recent years, with the increasing demand for public safety and the rapid\ndevelopment of intelligent surveillance networks, person re-identification\n(Re-ID) has become one of the hot research topics in the field of computer\nvision. The main research goal of person Re-ID is to retrieve persons with the\nsame identity from different cameras. However, traditional person Re-ID methods\nrequire manual marking of person targets, which consumes a lot of labor cost.\nWith the widespread application of deep neural networks in the field of\ncomputer vision, a large number of deep learning-based person Re-ID methods\nhave emerged. Therefore, this paper is to facilitate researchers to better\nunderstand the latest research results and the future trends in the field.\nFirstly, we summarize the main study of several recently published person\nre-identification surveys and try to fill the gaps between them. Secondly, We\npropose a multi-dimensional taxonomy to categorize the most current deep\nlearning-based person Re-ID methods according to different characteristics,\nincluding methods for deep metric learning, local feature learning, generate\nadversarial networks, sequence feature learning and graph convolutional\nnetworks. Furthermore, we subdivide the above five categories according to\ntheir technique types, discussing and comparing the experimental performance of\npart subcategories. Finally, we conclude this paper and discuss future research\ndirections for person Re-ID.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ming_Z/0/1/0/all/0/1\">Zhangqiang Ming</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1\">Min Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xiaoyong Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiangkun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jiamin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1\">Junlong Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yong Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Training of 3D Seismic Image Fault Segmentation Network under Sparse Labels by Weakening Anomaly Annotation. (arXiv:2110.05319v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.05319","description":"<p>Seismic data fault detection has recently been regarded as a 3D image\nsegmentation task. The nature of fault structures in seismic image makes it\ndifficult to manually label faults. Manual labeling often has many false\nnegative labels (abnormal annotations), which will seriously harm the training\nprocess. In this work, we find that region-based loss significantly outperforms\ndistribution-based loss when dealing with false negative labels, therefore we\nproposed Mask Dice loss (MD loss), which is the first reported region-based\nloss function for training 3D image segmentation networks using sparse 2D slice\nlabels. In addition, fault is an edge feature, and the current network widely\nused for fault segmentation downsamples the features multiple times, which is\nnot conducive to edge representation and thus requires many parameters and\ncomputational effort to preserve the features. We proposed Fault-Net, which\nuses a high-resolution and shallow structure to propagate multi-scale features\nin parallel, fully preserving edge features. Meanwhile, in order to efficiently\nfuse multi-scale features, we decouple the convolution process into feature\nselection and channel fusion, and proposed a lightweight feature fusion block,\nMulti-Scale Compression Fusion (MCF). Because the Fault-Net always keeps the\nedge features during propagation, only few parameters and computation are\nrequired. Experimental results show that MD loss can clearly weaken the effect\nof false negative labels. The Fault-Net parameter is only 0.42MB, support up to\n528^3 (1.5x10^8, Float32) size cuboid inference on 16GB video ram, its\ninference speed on CPU and GPU is significantly faster than other networks. It\nworks well on most of the open data seismic images, and the result of our\nmethod is the state-of-the-art in the FORCE fault identification competition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dou_Y/0/1/0/all/0/1\">Yimin Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kewen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jianbing Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Timing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1\">Shaoquan Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zongchao Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NAS-Bench-360: Benchmarking Diverse Tasks for Neural Architecture Search. (arXiv:2110.05668v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.05668","description":"<p>Most existing neural architecture search (NAS) benchmarks and algorithms\nprioritize performance on well-studied tasks, e.g., image classification on\nCIFAR and ImageNet. This makes the applicability of NAS approaches in more\ndiverse areas inadequately understood. In this paper, we present NAS-Bench-360,\na benchmark suite for evaluating state-of-the-art NAS methods for convolutional\nneural networks (CNNs). To construct it, we curate a collection of ten tasks\nspanning a diverse array of application domains, dataset sizes, problem\ndimensionalities, and learning objectives. By carefully selecting tasks that\ncan both interoperate with modern CNN-based search methods but that are also\nfar-afield from their original development domain, we can use NAS-Bench-360 to\ninvestigate the following central question: do existing state-of-the-art NAS\nmethods perform well on diverse tasks? Our experiments show that a modern NAS\nprocedure designed for image classification can indeed find good architectures\nfor tasks with other dimensionalities and learning objectives; however, the\nsame method struggles against more task-specific methods and performs\ncatastrophically poorly on classification in non-vision domains. The case for\nNAS robustness becomes even more dire in a resource-constrained setting, where\na recent NAS method provides little-to-no benefit over much simpler baselines.\nThese results demonstrate the need for a benchmark such as NAS-Bench-360 to\nhelp develop NAS approaches that work well on a variety of tasks, a crucial\ncomponent of a truly robust and automated pipeline. We conclude with a\ndemonstration of the kind of future research our suite of tasks will enable.\nAll data and code is made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tu_R/0/1/0/all/0/1\">Renbo Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khodak_M/0/1/0/all/0/1\">Mikhail Khodak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roberts_N/0/1/0/all/0/1\">Nicholas Roberts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talwalkar_A/0/1/0/all/0/1\">Ameet Talwalkar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Relation-aware Video Reading Comprehension for Temporal Language Grounding. (arXiv:2110.05717v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.05717","description":"<p>Temporal language grounding in videos aims to localize the temporal span\nrelevant to the given query sentence. Previous methods treat it either as a\nboundary regression task or a span extraction task. This paper will formulate\ntemporal language grounding into video reading comprehension and propose a\nRelation-aware Network (RaNet) to address it. This framework aims to select a\nvideo moment choice from the predefined answer set with the aid of\ncoarse-and-fine choice-query interaction and choice-choice relation\nconstruction. A choice-query interactor is proposed to match the visual and\ntextual information simultaneously in sentence-moment and token-moment levels,\nleading to a coarse-and-fine cross-modal interaction. Moreover, a novel\nmulti-choice relation constructor is introduced by leveraging graph convolution\nto capture the dependencies among video moment choices for the best choice\nselection. Extensive experiments on ActivityNet-Captions, TACoS, and\nCharades-STA demonstrate the effectiveness of our solution. Codes will be\nreleased soon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jialin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mengmeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bernard Ghanem</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Open Source User Activity Traces with Applications to User Mobility Characterization and Modeling. (arXiv:2110.06382v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.06382","description":"<p>The current state-of-the-art in user mobility research has extensively relied\non open-source mobility traces captured from pedestrian and vehicular activity\nthrough a variety of communication technologies as users engage in a wide-range\nof applications, including connected healthcare, localization, social media,\ne-commerce, etc. Most of these traces are feature-rich and diverse, not only in\nthe information they provide, but also in how they can be used and leveraged.\nThis diversity poses two main challenges for researchers and practitioners who\nwish to make use of available mobility datasets. First, it is quite difficult\nto get a bird's eye view of the available traces without spending considerable\ntime looking them up. Second, once they have found the traces, they still need\nto figure out whether the traces are adequate to their needs.\n</p>\n<p>The purpose of this survey is three-fold. It proposes a taxonomy to classify\nopen-source mobility traces including their mobility mode, data source and\ncollection technology. It then uses the proposed taxonomy to classify existing\nopen-source mobility traces and finally, highlights three case studies using\npopular publicly available datasets to showcase how our taxonomy can tease out\nfeature sets in traces to help determine their applicability to specific\nuse-cases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+King_S/0/1/0/all/0/1\">Sinjoni Mukhopadhyay King</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nawab_F/0/1/0/all/0/1\">Faisal Nawab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Obraczka_K/0/1/0/all/0/1\">Katia Obraczka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"THOMAS: Trajectory Heatmap Output with learned Multi-Agent Sampling. (arXiv:2110.06607v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.06607","description":"<p>In this paper, we propose THOMAS, a joint multi-agent trajectory prediction\nframework allowing for efficient and consistent prediction of multi-agent\nmulti-modal trajectories. We present a unified model architecture for fast and\nsimultaneous agent future heatmap estimation leveraging hierarchical and sparse\nimage generation. We demonstrate that heatmap output enables a higher level of\ncontrol on the predicted trajectories compared to vanilla multi-modal\ntrajectory regression, allowing to incorporate additional constraints for\ntighter sampling or collision-free predictions in a deterministic way. However,\nwe also highlight that generating scene-consistent predictions goes beyond the\nmere generation of collision-free trajectories. We therefore propose a\nlearnable trajectory recombination model that takes as input a set of predicted\ntrajectories for each agent and outputs its consistent reordered recombination.\nWe report our results on the Interaction multi-agent prediction challenge and\nrank $1^{st}$ on the online test leaderboard.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gilles_T/0/1/0/all/0/1\">Thomas Gilles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabatini_S/0/1/0/all/0/1\">Stefano Sabatini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsishkou_D/0/1/0/all/0/1\">Dzmitry Tsishkou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stanciulescu_B/0/1/0/all/0/1\">Bogdan Stanciulescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moutarde_F/0/1/0/all/0/1\">Fabien Moutarde</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ADOP: Approximate Differentiable One-Pixel Point Rendering. (arXiv:2110.06635v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.06635","description":"<p>We present a novel point-based, differentiable neural rendering pipeline for\nscene refinement and novel view synthesis. The input are an initial estimate of\nthe point cloud and the camera parameters. The output are synthesized images\nfrom arbitrary camera poses. The point cloud rendering is performed by a\ndifferentiable renderer using multi-resolution one-pixel point rasterization.\nSpatial gradients of the discrete rasterization are approximated by the novel\nconcept of ghost geometry. After rendering, the neural image pyramid is passed\nthrough a deep neural network for shading calculations and hole-filling. A\ndifferentiable, physically-based tonemapper then converts the intermediate\noutput to the target image. Since all stages of the pipeline are\ndifferentiable, we optimize all of the scene's parameters i.e. camera model,\ncamera pose, point position, point color, environment map, rendering network\nweights, vignetting, camera response function, per image exposure, and per\nimage white balance. We show that our system is able to synthesize sharper and\nmore consistent novel views than existing approaches because the initial\nreconstruction is refined during training. The efficient one-pixel point\nrasterization allows us to use arbitrary camera models and display scenes with\nwell over 100M points in real time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ruckert_D/0/1/0/all/0/1\">Darius R&#xfc;ckert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Franke_L/0/1/0/all/0/1\">Linus Franke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stamminger_M/0/1/0/all/0/1\">Marc Stamminger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Users' Mental Model with Attention-directed Counterfactual Edits. (arXiv:2110.06863v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.06863","description":"<p>In the domain of Visual Question Answering (VQA), studies have shown\nimprovement in users' mental model of the VQA system when they are exposed to\nexamples of how these systems answer certain Image-Question (IQ) pairs. In this\nwork, we show that showing controlled counterfactual image-question examples\nare more effective at improving the mental model of users as compared to simply\nshowing random examples. We compare a generative approach and a retrieval-based\napproach to show counterfactual examples. We use recent advances in generative\nadversarial networks (GANs) to generate counterfactual images by deleting and\ninpainting certain regions of interest in the image. We then expose users to\nchanges in the VQA system's answer on those altered images. To select the\nregion of interest for inpainting, we experiment with using both\nhuman-annotated attention maps and a fully automatic method that uses the VQA\nsystem's attention values. Finally, we test the user's mental model by asking\nthem to predict the model's performance on a test counterfactual image. We note\nan overall improvement in users' accuracy to predict answer change when shown\ncounterfactual explanations. While realistic retrieved counterfactuals\nobviously are the most effective at improving the mental model, we show that a\ngenerative approach can also be equally effective.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alipour_K/0/1/0/all/0/1\">Kamran Alipour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ray_A/0/1/0/all/0/1\">Arijit Ray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xiao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cogswell_M/0/1/0/all/0/1\">Michael Cogswell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schulze_J/0/1/0/all/0/1\">Jurgen P. Schulze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yi Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burachas_G/0/1/0/all/0/1\">Giedrius T. Burachas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking the Representational Continuity: Towards Unsupervised Continual Learning. (arXiv:2110.06976v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.06976","description":"<p>Continual learning (CL) aims to learn a sequence of tasks without forgetting\nthe previously acquired knowledge. However, recent advances in continual\nlearning are restricted to supervised continual learning (SCL) scenarios.\nConsequently, they are not scalable to real-world applications where the data\ndistribution is often biased and unannotated. In this work, we focus on\nunsupervised continual learning (UCL), where we learn the feature\nrepresentations on an unlabelled sequence of tasks and show that reliance on\nannotated data is not necessary for continual learning. We conduct a systematic\nstudy analyzing the learned feature representations and show that unsupervised\nvisual representations are surprisingly more robust to catastrophic forgetting,\nconsistently achieve better performance, and generalize better to\nout-of-distribution tasks than SCL. Furthermore, we find that UCL achieves a\nsmoother loss landscape through qualitative analysis of the learned\nrepresentations and learns meaningful feature representations. Additionally, we\npropose Lifelong Unsupervised Mixup (LUMP), a simple yet effective technique\nthat leverages the interpolation between the current task and previous tasks'\ninstances to alleviate catastrophic forgetting for unsupervised\nrepresentations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Madaan_D/0/1/0/all/0/1\">Divyam Madaan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1\">Jaehong Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuanchun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yunxin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Sung Ju Hwang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Proposal Extension with LSTM Network for Weakly Supervised Object Detection. (arXiv:2110.07511v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.07511","description":"<p>Weakly supervised object detection (WSOD) has attracted more and more\nattention since it only uses image-level labels and can save huge annotation\ncosts. Most of the WSOD methods use Multiple Instance Learning (MIL) as their\nbasic framework, which regard it as an instance classification problem.\nHowever, these methods based on MIL tends to converge only on the most\ndiscriminate regions of different instances, rather than their corresponding\ncomplete regions, that is, insufficient integrity. Inspired by the habit of\nobserving things by the human, we propose a new method by comparing the initial\nproposals and the extension ones to optimize those initial proposals.\nSpecifically, we propose one new strategy for WSOD by involving contrastive\nproposal extension (CPE), which consists of multiple directional contrastive\nproposal extensions (D-CPE), and each D-CPE contains encoders based on LSTM\nnetwork and corresponding decoders. Firstly, the boundary of initial proposals\nin MIL is extended to different positions according to well-designed sequential\norder. Then, CPE compares the extended proposal and the initial proposal by\nextracting the feature semantics of them using the encoders, and calculates the\nintegrity of the initial proposal to optimize the score of the initial\nproposal. These contrastive contextual semantics will guide the basic WSOD to\nsuppress bad proposals and improve the scores of good ones. In addition, a\nsimple two-stream network is designed as the decoder to constrain the temporal\ncoding of LSTM and improve the performance of WSOD further. Experiments on\nPASCAL VOC 2007, VOC 2012 and MS-COCO datasets show that our method has\nachieved the state-of-the-art results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lv_P/0/1/0/all/0/1\">Pei Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Suqi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_T/0/1/0/all/0/1\">Tianran Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Haohan Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1\">Lisha Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1\">Haoyi Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mingliang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Changsheng Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NeRS: Neural Reflectance Surfaces for Sparse-view 3D Reconstruction in the Wild. (arXiv:2110.07604v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.07604","description":"<p>Recent history has seen a tremendous growth of work exploring implicit\nrepresentations of geometry and radiance, popularized through Neural Radiance\nFields (NeRF). Such works are fundamentally based on a (implicit) volumetric\nrepresentation of occupancy, allowing them to model diverse scene structure\nincluding translucent objects and atmospheric obscurants. But because the vast\nmajority of real-world scenes are composed of well-defined surfaces, we\nintroduce a surface analog of such implicit models called Neural Reflectance\nSurfaces (NeRS). NeRS learns a neural shape representation of a closed surface\nthat is diffeomorphic to a sphere, guaranteeing water-tight reconstructions.\nEven more importantly, surface parameterizations allow NeRS to learn (neural)\nbidirectional surface reflectance functions (BRDFs) that factorize\nview-dependent appearance into environmental illumination, diffuse color\n(albedo), and specular \"shininess.\" Finally, rather than illustrating our\nresults on synthetic scenes or controlled in-the-lab capture, we assemble a\nnovel dataset of multi-view images from online marketplaces for selling goods.\nSuch \"in-the-wild\" multi-view image sets pose a number of challenges, including\na small number of views with unknown/rough camera estimates. We demonstrate\nthat surface-based neural reconstructions enable learning from such data,\noutperforming volumetric neural rendering-based reconstructions. We hope that\nNeRS serves as a first step toward building scalable, high-quality libraries of\nreal-world shape, materials, and illumination. The project page with code and\nvideo visualizations can be found at https://jasonyzhang.com/ners.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jason Y. Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1\">Gengshan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tulsiani_S/0/1/0/all/0/1\">Shubham Tulsiani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanan_D/0/1/0/all/0/1\">Deva Ramanan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EMDS-7: Environmental Microorganism Image Dataset Seventh Version for Multiple Object Detection Evaluation. (arXiv:2110.07723v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.07723","description":"<p>The Environmental Microorganism Image Dataset Seventh Version (EMDS-7) is a\nmicroscopic image data set, including the original Environmental Microorganism\nimages (EMs) and the corresponding object labeling files in \".XML\" format file.\nThe EMDS-7 data set consists of 41 types of EMs, which has a total of 2365\nimages and 13216 labeled objects. The EMDS-7 database mainly focuses on the\nobject detection. In order to prove the effectiveness of EMDS-7, we select the\nmost commonly used deep learning methods (Faster-RCNN, YOLOv3, YOLOv4, SSD and\nRetinaNet) and evaluation indices for testing and evaluation. EMDS-7 is freely\npublished for non-commercial purpose at: https://github.com/yanghechen/EMDS-7\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hechen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_B/0/1/0/all/0/1\">Bencheng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiawei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_P/0/1/0/all/0/1\">Pingli Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1\">Peng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1\">Ao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1\">Tao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Hongzan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teng_Y/0/1/0/all/0/1\">Yueyang Teng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_S/0/1/0/all/0/1\">Shouliang Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1\">Tao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grzegorzek_M/0/1/0/all/0/1\">Marcin Grzegorzek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PTQ-SL: Exploring the Sub-layerwise Post-training Quantization. (arXiv:2110.07809v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.07809","description":"<p>Network quantization is a powerful technique to compress convolutional neural\nnetworks. The quantization granularity determines how to share the scaling\nfactors in weights, which affects the performance of network quantization. Most\nexisting approaches share the scaling factors layerwisely or channelwisely for\nquantization of convolutional layers. Channelwise quantization and layerwise\nquantization have been widely used in various applications. However, other\nquantization granularities are rarely explored. In this paper, we will explore\nthe sub-layerwise granularity that shares the scaling factor across multiple\ninput and output channels. We propose an efficient post-training quantization\nmethod in sub-layerwise granularity (PTQ-SL). Then we systematically experiment\non various granularities and observe that the prediction accuracy of the\nquantized neural network has a strong correlation with the granularity.\nMoreover, we find that adjusting the position of the channels can improve the\nperformance of sub-layerwise quantization. Therefore, we propose a method to\nreorder the channels for sub-layerwise quantization. The experiments\ndemonstrate that the sub-layerwise quantization with appropriate channel\nreordering can outperform the channelwise quantization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zhihang Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yiqi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_C/0/1/0/all/0/1\">Chenhao Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chenguang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qiankun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1\">Guangyu Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MaGNET: Uniform Sampling from Deep Generative Network Manifolds Without Retraining. (arXiv:2110.08009v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.08009","description":"<p>Deep Generative Networks (DGNs) are extensively employed in Generative\nAdversarial Networks (GANs), Variational Autoencoders (VAEs), and their\nvariants to approximate the data manifold, and data distribution on that\nmanifold. However, training samples are often obtained based on preferences,\ncosts, or convenience producing artifacts in the empirical data distribution\ne.g., the large fraction of smiling faces in the CelebA dataset or the large\nfraction of dark-haired individuals in FFHQ. These inconsistencies will be\nreproduced when sampling from the trained DGN, which has far-reaching potential\nimplications for fairness, data augmentation, anomaly detection, domain\nadaptation, and beyond. In response, we develop a differential geometry based\nsampler -- coined MaGNET -- that, given any trained DGN, produces samples that\nare uniformly distributed on the learned manifold. We prove theoretically and\nempirically that our technique produces a uniform distribution on the manifold\nregardless of the training set distribution. We perform a range of experiments\non various datasets and DGNs. One of them considers the state-of-the-art\nStyleGAN2 trained on FFHQ dataset, where uniform sampling via MaGNET increases\ndistribution precision and recall by 4.1% &amp; 3.0% and decreases gender bias by\n41.2%, without requiring labels or retraining.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Humayun_A/0/1/0/all/0/1\">Ahmed Imtiaz Humayun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balestriero_R/0/1/0/all/0/1\">Randall Balestriero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baraniuk_R/0/1/0/all/0/1\">Richard Baraniuk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FlexConv: Continuous Kernel Convolutions with Differentiable Kernel Sizes. (arXiv:2110.08059v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.08059","description":"<p>When designing Convolutional Neural Networks (CNNs), one must select the size\nof the convolutional kernels before training. Recent works show CNNs benefit\nfrom different kernel sizes at different layers, but exploring all possible\ncombinations is unfeasible in practice. A more efficient approach is to learn\nthe kernel size during training. However, existing works that learn the kernel\nsize have a limited bandwidth. These approaches scale kernels by dilation, and\nthus the detail they can describe is limited. In this work, we propose\nFlexConv, a novel convolutional operation with which high bandwidth\nconvolutional kernels of learnable kernel size can be learned at a fixed\nparameter cost. FlexNets model long-term dependencies without the use of\npooling, achieve state-of-the-art performance on several sequential datasets,\noutperform recent works with learned kernel sizes, and are competitive with\nmuch deeper ResNets on image benchmark datasets. Additionally, FlexNets can be\ndeployed at higher resolutions than those seen during training. To avoid\naliasing, we propose a novel kernel parameterization with which the frequency\nof the kernels can be analytically controlled. Our novel kernel\nparameterization shows higher descriptive power and faster convergence speed\nthan existing parameterizations. This leads to important improvements in\nclassification accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Romero_D/0/1/0/all/0/1\">David W. Romero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruintjes_R/0/1/0/all/0/1\">Robert-Jan Bruintjes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tomczak_J/0/1/0/all/0/1\">Jakub M. Tomczak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bekkers_E/0/1/0/all/0/1\">Erik J. Bekkers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoogendoorn_M/0/1/0/all/0/1\">Mark Hoogendoorn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gemert_J/0/1/0/all/0/1\">Jan C. van Gemert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MedAug: Contrastive learning leveraging patient metadata improves representations for chest X-ray interpretation. (arXiv:2102.10663v2 [eess.IV] CROSS LISTED)","link":"http://arxiv.org/abs/2102.10663","description":"<p>Self-supervised contrastive learning between pairs of multiple views of the\nsame image has been shown to successfully leverage unlabeled data to produce\nmeaningful visual representations for both natural and medical images. However,\nthere has been limited work on determining how to select pairs for medical\nimages, where availability of patient metadata can be leveraged to improve\nrepresentations. In this work, we develop a method to select positive pairs\ncoming from views of possibly different images through the use of patient\nmetadata. We compare strategies for selecting positive pairs for chest X-ray\ninterpretation including requiring them to be from the same patient, imaging\nstudy or laterality. We evaluate downstream task performance by fine-tuning the\nlinear layer on 1% of the labeled dataset for pleural effusion classification.\nOur best performing positive pair selection strategy, which involves using\nimages from the same patient from the same study across all lateralities,\nachieves a performance increase of 14.4% in mean AUC from the ImageNet\npretrained baseline. Our controlled experiments show that the keys to improving\ndownstream performance on disease classification are (1) using patient metadata\nto appropriately create positive pairs from different images with the same\nunderlying pathologies, and (2) maximizing the number of different images used\nin query pairing. In addition, we explore leveraging patient metadata to select\nhard negative pairs for contrastive learning, but do not find improvement over\nbaselines that do not use metadata. Our method is broadly applicable to medical\nimage interpretation and allows flexibility for incorporating medical insights\nin choosing pairs for contrastive learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Vu_Y/0/1/0/all/0/1\">Yen Nhi Truong Vu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_R/0/1/0/all/0/1\">Richard Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Balachandar_N/0/1/0/all/0/1\">Niranjan Balachandar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_C/0/1/0/all/0/1\">Can Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ng_A/0/1/0/all/0/1\">Andrew Y. Ng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rajpurkar_P/0/1/0/all/0/1\">Pranav Rajpurkar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Core Challenges in Embodied Vision-Language Planning. (arXiv:2106.13948v2 [cs.LG] CROSS LISTED)","link":"http://arxiv.org/abs/2106.13948","description":"<p>Recent advances in the areas of multimodal machine learning and artificial\nintelligence (AI) have led to the development of challenging tasks at the\nintersection of Computer Vision, Natural Language Processing, and Embodied AI.\nWhereas many approaches and previous survey pursuits have characterised one or\ntwo of these dimensions, there has not been a holistic analysis at the center\nof all three. Moreover, even when combinations of these topics are considered,\nmore focus is placed on describing, e.g., current architectural methods, as\nopposed to also illustrating high-level challenges and opportunities for the\nfield. In this survey paper, we discuss Embodied Vision-Language Planning\n(EVLP) tasks, a family of prominent embodied navigation and manipulation\nproblems that jointly use computer vision and natural language. We propose a\ntaxonomy to unify these tasks and provide an in-depth analysis and comparison\nof the new and current algorithmic approaches, metrics, simulated environments,\nas well as the datasets used for EVLP tasks. Finally, we present the core\nchallenges that we believe new EVLP works should seek to address, and we\nadvocate for task construction that enables model generalizability and furthers\nreal-world deployment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Francis_J/0/1/0/all/0/1\">Jonathan Francis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitamura_N/0/1/0/all/0/1\">Nariaki Kitamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labelle_F/0/1/0/all/0/1\">Felix Labelle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiaopeng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navarro_I/0/1/0/all/0/1\">Ingrid Navarro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1\">Jean Oh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-10-18T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/"}}]}]}