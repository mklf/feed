{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-01-24T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Regional Negative Bias in Word Embeddings Predicts Racial Animus--but only via Name Frequency. (arXiv:2201.08451v1 [cs.CL])","link":"http://arxiv.org/abs/2201.08451","description":"<p>The word embedding association test (WEAT) is an important method for\nmeasuring linguistic biases against social groups such as ethnic minorities in\nlarge text corpora. It does so by comparing the semantic relatedness of words\nprototypical of the groups (e.g., names unique to those groups) and attribute\nwords (e.g., 'pleasant' and 'unpleasant' words). We show that anti-black WEAT\nestimates from geo-tagged social media data at the level of metropolitan\nstatistical areas strongly correlate with several measures of racial\nanimus--even when controlling for sociodemographic covariates. However, we also\nshow that every one of these correlations is explained by a third variable: the\nfrequency of Black names in the underlying corpora relative to White names.\nThis occurs because word embeddings tend to group positive (negative) words and\nfrequent (rare) words together in the estimated semantic space. As the\nfrequency of Black names on social media is strongly correlated with Black\nAmericans' prevalence in the population, this results in spurious anti-Black\nWEAT estimates wherever few Black Americans live. This suggests that research\nusing the WEAT to measure bias should consider term frequency, and also\ndemonstrates the potential consequences of using black-box models like word\nembeddings to study human cognition and behavior.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Loon_A/0/1/0/all/0/1\">Austin van Loon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giorgi_S/0/1/0/all/0/1\">Salvatore Giorgi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Willer_R/0/1/0/all/0/1\">Robb Willer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eichstaedt_J/0/1/0/all/0/1\">Johannes Eichstaedt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SciBERTSUM: Extractive Summarization for Scientific Documents. (arXiv:2201.08495v1 [cs.CL])","link":"http://arxiv.org/abs/2201.08495","description":"<p>The summarization literature focuses on the summarization of news articles.\nThe news articles in the CNN-DailyMail are relatively short documents with\nabout 30 sentences per document on average. We introduce SciBERTSUM, our\nsummarization framework designed for the summarization of long documents like\nscientific papers with more than 500 sentences. SciBERTSUM extends BERTSUM to\nlong documents by 1) adding a section embedding layer to include section\ninformation in the sentence vector and 2) applying a sparse attention mechanism\nwhere each sentences will attend locally to nearby sentences and only a small\nnumber of sentences attend globally to all other sentences. We used slides\ngenerated by the authors of scientific papers as reference summaries since they\ncontain the technical details from the paper. The results show the superiority\nof our model in terms of ROUGE scores.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sefid_A/0/1/0/all/0/1\">Athar Sefid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giles_C/0/1/0/all/0/1\">C Lee Giles</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Two-Step Hybrid Policy for Graph-Based Interpretable Reinforcement Learning. (arXiv:2201.08520v1 [cs.LG])","link":"http://arxiv.org/abs/2201.08520","description":"<p>We present a two-step hybrid reinforcement learning (RL) policy that is\ndesigned to generate interpretable and robust hierarchical policies on the RL\nproblem with graph-based input. Unlike prior deep reinforcement learning\npolicies parameterized by an end-to-end black-box graph neural network, our\napproach disentangles the decision-making process into two steps. The first\nstep is a simplified classification problem that maps the graph input to an\naction group where all actions share a similar semantic meaning. The second\nstep implements a sophisticated rule-miner that conducts explicit one-hop\nreasoning over the graph and identifies decisive edges in the graph input\nwithout the necessity of heavy domain knowledge. This two-step hybrid policy\npresents human-friendly interpretations and achieves better performance in\nterms of generalization and robustness. Extensive experimental studies on four\nlevels of complex text-based games have demonstrated the superiority of the\nproposed method compared to the state-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mu_T/0/1/0/all/0/1\">Tongzhou Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_K/0/1/0/all/0/1\">Kaixiang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_F/0/1/0/all/0/1\">Feiyang Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thattai_G/0/1/0/all/0/1\">Govind Thattai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Black-box Prompt Learning for Pre-trained Language Models. (arXiv:2201.08531v1 [cs.CL])","link":"http://arxiv.org/abs/2201.08531","description":"<p>Domain-specific fine-tuning strategies for large pre-trained models received\nvast attention in recent years. In previously studied settings, the model\narchitectures and parameters are tunable or at least visible, which we refer to\nas white-box settings. This work considers a new scenario, where we do not have\naccess to a pre-trained model, except for its outputs given inputs, and we call\nthis problem black-box fine-tuning. To illustrate our approach, we first\nintroduce the black-box setting formally on text classification, where the\npre-trained model is not only frozen but also invisible. We then propose our\nsolution black-box prompt, a new technique in the prompt-learning family, which\ncan leverage the knowledge learned by pre-trained models from the pre-training\ncorpus. Our experiments demonstrate that the proposed method achieved the\nstate-of-the-art performance on eight datasets. Further analyses on different\nhuman-designed objectives, prompt lengths, and intuitive explanations\ndemonstrate the robustness and flexibility of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Diao_S/0/1/0/all/0/1\">Shizhe Diao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuechun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhichao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tong Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Model Compression Improve NLP Fairness. (arXiv:2201.08542v1 [cs.CL])","link":"http://arxiv.org/abs/2201.08542","description":"<p>Model compression techniques are receiving increasing attention; however, the\neffect of compression on model fairness is still under explored. This is the\nfirst paper to examine the effect of distillation and pruning on the toxicity\nand bias of generative language models. We test Knowledge Distillation and\nPruning methods on the GPT2 model and found a consistent pattern of toxicity\nand bias reduction after model distillation; this result can be potentially\ninterpreted by existing line of research which describes model compression as a\nregularization technique; our work not only serves as a reference for safe\ndeployment of compressed models, but also extends the discussion of\n\"compression as regularization\" into the setting of neural LMs, and hints at\nthe possibility of using compression to develop fairer models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guangxuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1\">Qingyuan Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Identifying Adversarial Attacks on Text Classifiers. (arXiv:2201.08555v1 [cs.CL])","link":"http://arxiv.org/abs/2201.08555","description":"<p>The landscape of adversarial attacks against text classifiers continues to\ngrow, with new attacks developed every year and many of them available in\nstandard toolkits, such as TextAttack and OpenAttack. In response, there is a\ngrowing body of work on robust learning, which reduces vulnerability to these\nattacks, though sometimes at a high cost in compute time or accuracy. In this\npaper, we take an alternate approach -- we attempt to understand the attacker\nby analyzing adversarial text to determine which methods were used to create\nit. Our first contribution is an extensive dataset for attack detection and\nlabeling: 1.5~million attack instances, generated by twelve adversarial attacks\ntargeting three classifiers trained on six source datasets for sentiment\nanalysis and abuse detection in English. As our second contribution, we use\nthis dataset to develop and benchmark a number of classifiers for attack\nidentification -- determining if a given text has been adversarially\nmanipulated and by which attack. As a third contribution, we demonstrate the\neffectiveness of three classes of features for these tasks: text properties,\ncapturing content and presentation of text; language model properties,\ndetermining which tokens are more or less probable throughout the input; and\ntarget model properties, representing how the text classifier is influenced by\nthe attack, including internal node activations. Overall, this represents a\nfirst step towards forensics for adversarial attacks against text classifiers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zhouhang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brophy_J/0/1/0/all/0/1\">Jonathan Brophy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noack_A/0/1/0/all/0/1\">Adam Noack</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_W/0/1/0/all/0/1\">Wencong You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asthana_K/0/1/0/all/0/1\">Kalyani Asthana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perkins_C/0/1/0/all/0/1\">Carter Perkins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reis_S/0/1/0/all/0/1\">Sabrina Reis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sameer Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lowd_D/0/1/0/all/0/1\">Daniel Lowd</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Taxonomy Enrichment with Text and Graph Vector Representations. (arXiv:2201.08598v1 [cs.CL])","link":"http://arxiv.org/abs/2201.08598","description":"<p>Knowledge graphs such as DBpedia, Freebase or Wikidata always contain a\ntaxonomic backbone that allows the arrangement and structuring of various\nconcepts in accordance with the hypo-hypernym (\"class-subclass\") relationship.\nWith the rapid growth of lexical resources for specific domains, the problem of\nautomatic extension of the existing knowledge bases with new words is becoming\nmore and more widespread. In this paper, we address the problem of taxonomy\nenrichment which aims at adding new words to the existing taxonomy.\n</p>\n<p>We present a new method that allows achieving high results on this task with\nlittle effort. It uses the resources which exist for the majority of languages,\nmaking the method universal. We extend our method by incorporating deep\nrepresentations of graph structures like node2vec, Poincar\\'e embeddings, GCN\netc. that have recently demonstrated promising results on various NLP tasks.\nFurthermore, combining these representations with word embeddings allows us to\nbeat the state of the art.\n</p>\n<p>We conduct a comprehensive study of the existing approaches to taxonomy\nenrichment based on word and graph vector representations and their fusion\napproaches. We also explore the ways of using deep learning architectures to\nextend the taxonomic backbones of knowledge graphs. We create a number of\ndatasets for taxonomy extension for English and Russian. We achieve\nstate-of-the-art results across different datasets and provide an in-depth\nerror analysis of mistakes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nikishina_I/0/1/0/all/0/1\">Irina Nikishina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tikhomirov_M/0/1/0/all/0/1\">Mikhail Tikhomirov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Logacheva_V/0/1/0/all/0/1\">Varvara Logacheva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nazarov_Y/0/1/0/all/0/1\">Yuriy Nazarov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panchenko_A/0/1/0/all/0/1\">Alexander Panchenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loukachevitch_N/0/1/0/all/0/1\">Natalia Loukachevitch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text Style Transfer for Bias Mitigation using Masked Language Modeling. (arXiv:2201.08643v1 [cs.CL])","link":"http://arxiv.org/abs/2201.08643","description":"<p>It is well known that textual data on the internet and other digital\nplatforms contain significant levels of bias and stereotypes. Although many\nsuch texts contain stereotypes and biases that inherently exist in natural\nlanguage for reasons that are not necessarily malicious, there are crucial\nreasons to mitigate these biases. For one, these texts are being used as\ntraining corpus to train language models for salient applications like\ncv-screening, search engines, and chatbots; such applications are turning out\nto produce discriminatory results. Also, several research findings have\nconcluded that biased texts have significant effects on the target demographic\ngroups. For instance, masculine-worded job advertisements tend to be less\nappealing to female applicants.\n</p>\n<p>In this paper, we present a text style transfer model that can be used to\nautomatically debias textual data. Our style transfer model improves on the\nlimitations of many existing style transfer techniques such as loss of content\ninformation. Our model solves such issues by combining latent content encoding\nwith explicit keyword replacement. We will show that this technique produces\nbetter content preservation whilst maintaining good style transfer accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tokpo_E/0/1/0/all/0/1\">Ewoenam Kwaku Tokpo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calders_T/0/1/0/all/0/1\">Toon Calders</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Context-Tuning: Learning Contextualized Prompts for Natural Language Generation. (arXiv:2201.08670v1 [cs.CL])","link":"http://arxiv.org/abs/2201.08670","description":"<p>Recently, pretrained language models (PLMs) have made exceptional success in\nlanguage generation. To leverage the rich knowledge encoded by PLMs, a simple\nyet powerful mechanism is to use prompts, in the form of either discrete tokens\nor continuous embeddings. In existing studies, manual prompts are\ntime-consuming and require domain expertise, while continuous prompts are\ntypically independent of the inputs. To address this issue, we propose a novel\ncontinuous prompting approach, called Context-Tuning, to fine-tuning PLMs for\nnatural language generation. Firstly, the prompts are derived based on the\ninput text, so that they can elicit useful knowledge from PLMs for generation.\nWe refer to such prompts as contextualized prompts. Secondly, to further\nenhance the relevance of the generated text to the inputs, we utilize\ncontinuous inverse prompting to refine the process of natural language\ngeneration by modeling an inverse generation process from output to input.\nMoreover, we propose a lightweight contexttuning, fine-tuning only 0.4% of\nparameters while retaining well performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_T/0/1/0/all/0/1\">Tianyi Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gender Bias in Text: Labeled Datasets and Lexicons. (arXiv:2201.08675v1 [cs.CL])","link":"http://arxiv.org/abs/2201.08675","description":"<p>Language has a profound impact on our thoughts, perceptions, and conceptions\nof gender roles. Gender-inclusive language is, therefore, a key tool to promote\nsocial inclusion and contribute to achieving gender equality. Consequently,\ndetecting and mitigating gender bias in texts is instrumental in halting its\npropagation and societal implications. However, there is a lack of gender bias\ndatasets and lexicons for automating the detection of gender bias using\nsupervised and unsupervised machine learning (ML) and natural language\nprocessing (NLP) techniques. Therefore, the main contribution of this work is\nto publicly provide labeled datasets and exhaustive lexicons by collecting,\nannotating, and augmenting relevant sentences to facilitate the detection of\ngender bias in English text. Towards this end, we present an updated version of\nour previously proposed taxonomy by re-formalizing its structure, adding a new\nbias type, and mapping each bias subtype to an appropriate detection\nmethodology. The released datasets and lexicons span multiple bias subtypes\nincluding: Generic He, Generic She, Explicit Marking of Sex, and Gendered\nNeologisms. We leveraged the use of word embedding models to further augment\nthe collected lexicons. The underlying motivation of our work is to enable the\ntechnical community to combat gender bias in text and halt its propagation\nusing ML and NLP techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Doughman_J/0/1/0/all/0/1\">Jad Doughman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khreich_W/0/1/0/all/0/1\">Wael Khreich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comparative Study on Language Models for Task-Oriented Dialogue Systems. (arXiv:2201.08687v1 [cs.CL])","link":"http://arxiv.org/abs/2201.08687","description":"<p>The recent development of language models has shown promising results by\nachieving state-of-the-art performance on various natural language tasks by\nfine-tuning pretrained models. In task-oriented dialogue (ToD) systems,\nlanguage models can be used for end-to-end training without relying on dialogue\nstate tracking to track the dialogue history but allowing the language models\nto generate responses according to the context given as input. This paper\nconducts a comparative study to show the effectiveness and strength of using\nrecent pretrained models for fine-tuning, such as BART and T5, on endto-end ToD\nsystems. The experimental results show substantial performance improvements\nafter language model fine-tuning. The models produce more fluent responses\nafter adding knowledge to the context that guides the model to avoid\nhallucination and generate accurate entities in the generated responses.\nFurthermore, we found that BART and T5 outperform GPT-based models in BLEU and\nF1 scores and achieve state-of-the-art performance in a ToD system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Andreas_V/0/1/0/all/0/1\">Vinsen Marselino Andreas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winata_G/0/1/0/all/0/1\">Genta Indra Winata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Purwarianti_A/0/1/0/all/0/1\">Ayu Purwarianti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dual Contrastive Learning: Text Classification via Label-Aware Data Augmentation. (arXiv:2201.08702v1 [cs.CL])","link":"http://arxiv.org/abs/2201.08702","description":"<p>Contrastive learning has achieved remarkable success in representation\nlearning via self-supervision in unsupervised settings. However, effectively\nadapting contrastive learning to supervised learning tasks remains as a\nchallenge in practice. In this work, we introduce a dual contrastive learning\n(DualCL) framework that simultaneously learns the features of input samples and\nthe parameters of classifiers in the same space. Specifically, DualCL regards\nthe parameters of the classifiers as augmented samples associating to different\nlabels and then exploits the contrastive learning between the input samples and\nthe augmented samples. Empirical studies on five benchmark text classification\ndatasets and their low-resource version demonstrate the improvement in\nclassification accuracy and confirm the capability of learning discriminative\nrepresentations of DualCL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qianben Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Richong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yaowei Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1\">Yongyi Mao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Personality Type Based on Myers-Briggs Type Indicator with Text Posting Style by using Traditional and Deep Learning. (arXiv:2201.08717v1 [cs.CL])","link":"http://arxiv.org/abs/2201.08717","description":"<p>The term personality may be expressed in terms of the individual differences\nin characteristics pattern of thinking, feeling, and behavior. This work\npresents several machine learning techniques including Naive Bayes, Support\nVector Machines, and Recurrent Neural Networks to predict people personality\nfrom text based on Myers-Briggs Type Indicator (MBTI). Furthermore, this\nproject applies CRISP-DM, which stands for Cross-Industry Standard Process for\nData Mining, to guide the learning process. Since, CRISP-DM is kind of\niterative development, we have adopted it with agile methodology, which is a\nrapid iterative software development method, in order to reduce the development\ncycle to be minimal.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ontoum_S/0/1/0/all/0/1\">Sakdipat Ontoum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_J/0/1/0/all/0/1\">Jonathan H. Chan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Building Economic Models of Conversational Search. (arXiv:2201.08742v1 [cs.IR])","link":"http://arxiv.org/abs/2201.08742","description":"<p>Various conceptual and descriptive models of conversational search have been\nproposed in the literature -- while useful, they do not provide insights into\nhow interaction between the agent and user would change in response to the\ncosts and benefits of the different interactions. In this paper, we develop two\neconomic models of conversational search based on patterns previously observed\nduring conversational search sessions, which we refer to as: Feedback First\nwhere the agent asks clarifying questions then presents results, and Feedback\nAfter where the agent presents results, and then asks follow up questions. Our\nmodels show that the amount of feedback given/requested depends on its\nefficiency at improving the initial or subsequent query and the relative cost\nof providing said feedback. This theoretical framework for conversational\nsearch provides a number of insights that can be used to guide and inform the\ndevelopment of conversational search agents. However, empirical work is needed\nto estimate the parameters in order to make predictions specific to a given\nconversational search setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Azzopardi_L/0/1/0/all/0/1\">Leif Azzopardi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aliannejadi_M/0/1/0/all/0/1\">Mohammad Aliannejadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanoulas_E/0/1/0/all/0/1\">Evangelos Kanoulas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conversational Information Seeking. (arXiv:2201.08808v1 [cs.IR])","link":"http://arxiv.org/abs/2201.08808","description":"<p>Conversational information seeking (CIS) is concerned with a sequence of\ninteractions between one or more users and an information system. Interactions\nin CIS are primarily based on natural language dialogue, while they may include\nother types of interactions, such as click, touch, and body gestures. This\nmonograph provides a thorough overview of CIS definitions, applications,\ninteractions, interfaces, design, implementation, and evaluation. This\nmonograph views CIS applications as including conversational search,\nconversational question answering, and conversational recommendation. Our aim\nis to provide an overview of past research related to CIS, introduce the\ncurrent state-of-the-art in CIS, highlight the challenges still being faced in\nthe community. and suggest future directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zamani_H/0/1/0/all/0/1\">Hamed Zamani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trippas_J/0/1/0/all/0/1\">Johanne R. Trippas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dalton_J/0/1/0/all/0/1\">Jeff Dalton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radlinski_F/0/1/0/all/0/1\">Filip Radlinski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GAP-Gen: Guided Automatic Python Code Generation. (arXiv:2201.08810v1 [cs.PL])","link":"http://arxiv.org/abs/2201.08810","description":"<p>Automatic code generation from natural language descriptions can be highly\nbeneficial during the process of software development. In this work, we propose\nGAP-Gen, an automatic code generation method guided by Python syntactic\nconstraints and semantic constraints. We first introduce Python syntactic\nconstraints in the form of Syntax-Flow, which is a simplified version of\nAbstract Syntax Tree (AST) reducing the size and high complexity of Abstract\nSyntax Tree but maintaining the crucial syn-tactic information of Python code.\nIn addition to Syntax-Flow, we introduce Variable-Flow which abstracts variable\nand function names consistently throughout the code. In our work, rather than\npre-training, we focus on modifying the fine-tuning process which reduces\ncomputational requirements but retains high generation performance on automatic\nPython code generation task. GAP-Gen fine-tunes the transformer-based language\nmodels T5 and CodeT5 using the Code-to-Docstring datasets CodeSearchNet,\nCodeSearchNet AdvTest, and Code-Docstring-Corpus from EdinburghNLP. Our\nexperiments show that GAP-Gen achieves better results on automatic Python code\ngeneration task than previous works\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Junchen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yurun Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Junlin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harris_I/0/1/0/all/0/1\">Ian G. Harris</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Biochemical Space Language in Relation to Multiset Rewriting Systems. (arXiv:2201.08817v1 [cs.LO])","link":"http://arxiv.org/abs/2201.08817","description":"<p>This technical report relates Biochemical Space Language (BCSL) to Multiset\nrewriting systems (MRS). For a BCSL model, the semantics are defined in terms\nof transition systems, while for an MRS, they are defined in terms of a set of\nruns. In this report, we relate BCSL to MRS by first showing how the transition\nsystem is related to a set of runs and consequently showing how for every BCSL\nmodel, an MRS can be constructed such that both represent the same set of runs.\nThe motivation of this step is to establish BCSL in the context of a more\ngeneral rewriting system and benefit from properties shown for them. Finally,\nwe show that regulations defined for MRS can be consequently used in the BCSL\nmodel.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Trojak_M/0/1/0/all/0/1\">Matej Troj&#xe1;k</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Safranek_D/0/1/0/all/0/1\">David &#x160;afr&#xe1;nek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brim_L/0/1/0/all/0/1\">Lubo&#x161; Brim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pairwise Representation Learning for Event Coreference. (arXiv:2010.12808v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.12808","description":"<p>Natural Language Processing tasks such as resolving the coreference of events\nrequire understanding the relations between two text snippets. These tasks are\ntypically formulated as (binary) classification problems over independently\ninduced representations of the text snippets. In this work, we develop a\nPairwise Representation Learning (PairwiseRL) scheme for the event mention\npairs, in which we jointly encode a pair of text snippets so that the\nrepresentation of each mention in the pair is induced in the context of the\nother one. Furthermore, our representation supports a finer, structured\nrepresentation of the text snippet to facilitate encoding events and their\narguments. We show that PairwiseRL, despite its simplicity, outperforms the\nprior state-of-the-art event coreference systems on both cross-document and\nwithin-document event coreference benchmarks. We also conduct in-depth analysis\nin terms of the improvement and the limitation of pairwise representation so as\nto provide insights for future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xiaodong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_W/0/1/0/all/0/1\">Wenpeng Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1\">Dan Roth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Graph Reasoning with Relational Digraph. (arXiv:2108.06040v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2108.06040","description":"<p>Reasoning on the knowledge graph (KG) aims to infer new facts from existing\nones. Methods based on the relational path have shown strong, interpretable,\nand transferable reasoning ability. However, paths are naturally limited in\ncapturing local evidence in graphs. In this paper, we introduce a novel\nrelational structure, i.e., relational directed graph (r-digraph), which is\ncomposed of overlapped relational paths, to capture the KG's local evidence.\nSince the r- digraphs are more complex than paths, how to efficiently construct\nand effectively learn from them are challenging. Directly encoding the\nr-digraphs cannot scale well and capturing query-dependent information is hard\nin r-digraphs. We propose a variant of graph neural network, i.e., RED-GNN, to\naddress the above challenges. Specifically, RED-GNN makes use of dynamic\nprogramming to recursively encodes multiple r-digraphs with shared edges, and\nutilizes a query-dependent attention mechanism to select the strongly\ncorrelated edges. We demonstrate that RED-GNN is not only efficient but also\ncan achieve significant performance gains in both inductive and transductive\nreasoning tasks over existing methods. Besides, the learned attention weights\nin RED-GNN can exhibit interpretable evidence for KG reasoning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongqi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Q/0/1/0/all/0/1\">Quanming Yao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Czech News Dataset for Semantic Textual Similarity. (arXiv:2108.08708v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.08708","description":"<p>This paper describes a novel dataset consisting of sentences with semantic\nsimilarity annotations. The data originate from the journalistic domain in the\nCzech language. We describe the process of collecting and annotating the data\nin detail. The dataset contains 138,556 human annotations divided into train\nand test sets. In total, 485 journalism students participated in the creation\nprocess. To increase the reliability of the test set, we compute the annotation\nas an average of 9 individual annotations. We evaluate the quality of the\ndataset by measuring inter and intra annotation annotators' agreements. Beside\nagreement numbers, we provide detailed statistics of the collected dataset. We\nconclude our paper with a baseline experiment of building a system for\npredicting the semantic similarity of sentences. Due to the massive number of\ntraining annotations (116 956), the model can perform significantly better than\nan average annotator (0,92 versus 0,86 of Person's correlation coefficients).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sido_J/0/1/0/all/0/1\">Jakub Sido</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sejak_M/0/1/0/all/0/1\">Michal Sej&#xe1;k</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prazak_O/0/1/0/all/0/1\">Ond&#x159;ej Pra&#x17e;&#xe1;k</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konopik_M/0/1/0/all/0/1\">Miloslav Konop&#xed;k</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moravec_V/0/1/0/all/0/1\">V&#xe1;clav Moravec</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Capturing Structural Locality in Non-parametric Language Models. (arXiv:2110.02870v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.02870","description":"<p>Structural locality is a ubiquitous feature of real-world datasets, wherein\ndata points are organized into local hierarchies. Some examples include topical\nclusters in text or project hierarchies in source code repositories. In this\npaper, we explore utilizing this structural locality within non-parametric\nlanguage models, which generate sequences that reference retrieved examples\nfrom an external source. We propose a simple yet effective approach for adding\nlocality information into such models by adding learned parameters that improve\nthe likelihood of retrieving examples from local neighborhoods. Experiments on\ntwo different domains, Java source code and Wikipedia text, demonstrate that\nlocality features improve model efficacy over models without access to these\nfeatures, with interesting differences. We also perform an analysis of how and\nwhere locality features contribute to improved performance and why the\ntraditionally used contextual similarity metrics alone are not enough to grasp\nthe locality structure.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1\">Frank F. Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Junxian He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hellendoorn_V/0/1/0/all/0/1\">Vincent J. Hellendoorn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GNN-LM: Language Modeling based on Global Contexts via GNN. (arXiv:2110.08743v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.08743","description":"<p>Inspired by the notion that {\\it to copy is easier than to memorize}, in this\nwork, we introduce GNN-LM, which extends the vanilla neural language model (LM)\nby allowing to reference similar contexts in the entire training corpus. We\nbuild a directed heterogeneous graph between an input context and its\nsemantically related neighbors selected from the training corpus, where nodes\nare tokens in the input context and retrieved neighbor contexts, and edges\nrepresent connections between nodes. Graph neural networks (GNNs) are\nconstructed upon the graph to aggregate information from similar contexts to\ndecode the token. This learning paradigm provides direct access to the\nreference contexts and helps improve a model's generalization ability. We\nconduct comprehensive experiments to validate the effectiveness of the GNN-LM:\nGNN-LM achieves a new state-of-the-art perplexity of 14.8 on WikiText-103 (a\n4.5 point improvement over its counterpart of the vanilla LM model) and shows\nsubstantial improvement on One Billion Word and Enwiki8 datasets against strong\nbaselines. In-depth ablation studies are performed to understand the mechanics\nof GNN-LM. The code can be found at \\url{https://github.com/ShannonAI/GNN-LM}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1\">Yuxian Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zong_S/0/1/0/all/0/1\">Shi Zong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoya Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiaofei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiwei Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"English-to-Chinese Transliteration with Phonetic Back-transliteration. (arXiv:2112.10321v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.10321","description":"<p>Transliteration is a task of translating named entities from a language to\nanother, based on phonetic similarity. The task has embraced deep learning\napproaches in recent years, yet, most ignore the phonetic features of the\ninvolved languages. In this work, we incorporate phonetic information into\nneural networks in two ways: we synthesize extra data using forward and\nback-translation but in a phonetic manner; and we pre-train models on a\nphonetic task before learning transliteration. Our experiments include three\nlanguage pairs and six directions, namely English to and from Chinese, Hebrew\nand Thai. Results indicate that our proposed approach brings benefits to the\nmodel and achieves better or similar performance when compared to state of the\nart.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1\">Shi Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1\">Zhuofei Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Songpeng Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scaling Language Models: Methods, Analysis & Insights from Training Gopher. (arXiv:2112.11446v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.11446","description":"<p>Language modelling provides a step towards intelligent communication systems\nby harnessing large repositories of written human knowledge to better predict\nand understand the world. In this paper, we present an analysis of\nTransformer-based language model performance across a wide range of model\nscales -- from models with tens of millions of parameters up to a 280 billion\nparameter model called Gopher. These models are evaluated on 152 diverse tasks,\nachieving state-of-the-art performance across the majority. Gains from scale\nare largest in areas such as reading comprehension, fact-checking, and the\nidentification of toxic language, but logical and mathematical reasoning see\nless benefit. We provide a holistic analysis of the training dataset and\nmodel's behaviour, covering the intersection of model scale with bias and\ntoxicity. Finally we discuss the application of language models to AI safety\nand the mitigation of downstream harms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rae_J/0/1/0/all/0/1\">Jack W. Rae</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borgeaud_S/0/1/0/all/0/1\">Sebastian Borgeaud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_T/0/1/0/all/0/1\">Trevor Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Millican_K/0/1/0/all/0/1\">Katie Millican</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoffmann_J/0/1/0/all/0/1\">Jordan Hoffmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_F/0/1/0/all/0/1\">Francis Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aslanides_J/0/1/0/all/0/1\">John Aslanides</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henderson_S/0/1/0/all/0/1\">Sarah Henderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ring_R/0/1/0/all/0/1\">Roman Ring</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Young_S/0/1/0/all/0/1\">Susannah Young</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rutherford_E/0/1/0/all/0/1\">Eliza Rutherford</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hennigan_T/0/1/0/all/0/1\">Tom Hennigan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menick_J/0/1/0/all/0/1\">Jacob Menick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cassirer_A/0/1/0/all/0/1\">Albin Cassirer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Powell_R/0/1/0/all/0/1\">Richard Powell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Driessche_G/0/1/0/all/0/1\">George van den Driessche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hendricks_L/0/1/0/all/0/1\">Lisa Anne Hendricks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rauh_M/0/1/0/all/0/1\">Maribeth Rauh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1\">Po-Sen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glaese_A/0/1/0/all/0/1\">Amelia Glaese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Welbl_J/0/1/0/all/0/1\">Johannes Welbl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dathathri_S/0/1/0/all/0/1\">Sumanth Dathathri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Saffron Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uesato_J/0/1/0/all/0/1\">Jonathan Uesato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mellor_J/0/1/0/all/0/1\">John Mellor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Higgins_I/0/1/0/all/0/1\">Irina Higgins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Creswell_A/0/1/0/all/0/1\">Antonia Creswell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McAleese_N/0/1/0/all/0/1\">Nat McAleese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_A/0/1/0/all/0/1\">Amy Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elsen_E/0/1/0/all/0/1\">Erich Elsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jayakumar_S/0/1/0/all/0/1\">Siddhant Jayakumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buchatskaya_E/0/1/0/all/0/1\">Elena Buchatskaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Budden_D/0/1/0/all/0/1\">David Budden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sutherland_E/0/1/0/all/0/1\">Esme Sutherland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simonyan_K/0/1/0/all/0/1\">Karen Simonyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paganini_M/0/1/0/all/0/1\">Michela Paganini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sifre_L/0/1/0/all/0/1\">Laurent Sifre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martens_L/0/1/0/all/0/1\">Lena Martens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Lorraine Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuncoro_A/0/1/0/all/0/1\">Adhiguna Kuncoro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nematzadeh_A/0/1/0/all/0/1\">Aida Nematzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gribovskaya_E/0/1/0/all/0/1\">Elena Gribovskaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Donato_D/0/1/0/all/0/1\">Domenic Donato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lazaridou_A/0/1/0/all/0/1\">Angeliki Lazaridou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mensch_A/0/1/0/all/0/1\">Arthur Mensch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lespiau_J/0/1/0/all/0/1\">Jean-Baptiste Lespiau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsimpoukelli_M/0/1/0/all/0/1\">Maria Tsimpoukelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grigorev_N/0/1/0/all/0/1\">Nikolai Grigorev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fritz_D/0/1/0/all/0/1\">Doug Fritz</a>, et al. (31 additional authors not shown)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning grammar with a divide-and-concur neural network. (arXiv:2201.07341v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.07341","description":"<p>We implement a divide-and-concur iterative projection approach to\ncontext-free grammar inference. Unlike most state-of-the-art models of natural\nlanguage processing, our method requires a relatively small number of discrete\nparameters, making the inferred grammar directly interpretable -- one can read\noff from a solution how to construct grammatically valid sentences. Another\nadvantage of our approach is the ability to infer meaningful grammatical rules\nfrom just a few sentences, compared to the hundreds of gigabytes of training\ndata many other models employ. We demonstrate several ways of applying our\napproach: classifying words and inferring a grammar from scratch, taking an\nexisting grammar and refining its categories and rules, and taking an existing\ngrammar and expanding its lexicon as it encounters new words in new data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deyo_S/0/1/0/all/0/1\">Sean Deyo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elser_V/0/1/0/all/0/1\">Veit Elser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VISA: An Ambiguous Subtitles Dataset for Visual Scene-Aware Machine Translation. (arXiv:2201.08054v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.08054","description":"<p>Existing multimodal machine translation (MMT) datasets consist of images and\nvideo captions or general subtitles, which rarely contain linguistic ambiguity,\nmaking visual information not so effective to generate appropriate\ntranslations. We introduce VISA, a new dataset that consists of 40k\nJapanese-English parallel sentence pairs and corresponding video clips with the\nfollowing key features: (1) the parallel sentences are subtitles from movies\nand TV episodes; (2) the source subtitles are ambiguous, which means they have\nmultiple possible translations with different meanings; (3) we divide the\ndataset into Polysemy and Omission according to the cause of ambiguity. We show\nthat VISA is challenging for the latest MMT system, and we hope that the\ndataset can facilitate MMT research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yihang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shimizu_S/0/1/0/all/0/1\">Shuichiro Shimizu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_W/0/1/0/all/0/1\">Weiqi Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_C/0/1/0/all/0/1\">Chenhui Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurohashi_S/0/1/0/all/0/1\">Sadao Kurohashi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-01-23T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","syn":"http://purl.org/rss/1.0/modules/syndication/","dc":"http://purl.org/dc/elements/1.1/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","admin":"http://webns.net/mvcb/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Improving Specificity in Mammography Using Cross-correlation between Wavelet and Fourier Transform. (arXiv:2201.08385v1 [eess.IV])","link":"http://arxiv.org/abs/2201.08385","description":"<p>Breast cancer is in the most common malignant tumor in women. It accounted\nfor 30% of new malignant tumor cases. Although the incidence of breast cancer\nremains high around the world, the mortality rate has been continuously\nreduced. This is mainly due to recent developments in molecular biology\ntechnology and improved level of comprehensive diagnosis and standard\ntreatment. Early detection by mammography is an integral part of that. The most\ncommon breast abnormalities that may indicate breast cancer are masses and\ncalcifications. Previous detection approaches usually obtain relatively high\nsensitivity but unsatisfactory specificity. We will investigate an approach\nthat applies the discrete wavelet transform and Fourier transform to parse the\nimages and extracts statistical features that characterize an image's content,\nsuch as the mean intensity and the skewness of the intensity. A naive Bayesian\nclassifier uses these features to classify the images. We expect to achieve an\noptimal high specificity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1\">Liuhua Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Steerable Pyramid Transform Enables Robust Left Ventricle Quantification. (arXiv:2201.08388v1 [eess.IV])","link":"http://arxiv.org/abs/2201.08388","description":"<p>Although multifarious variants of convolutional neural networks (CNNs) have\nproved successful in cardiac index quantification, they seem vulnerable to mild\ninput perturbations, e.g., spatial transformations, image distortions, and\nadversarial attacks. Such brittleness erodes our trust in CNN-based automated\ndiagnosis of various cardiovascular diseases. In this work, we describe a\nsimple and effective method to learn robust CNNs for left ventricle (LV)\nquantification, including cavity and myocardium areas, directional dimensions,\nand regional wall thicknesses. The key to the success of our approach is the\nuse of the biologically-inspired steerable pyramid transform (SPT) as fixed\nfront-end processing, which brings three computational advantages to LV\nquantification. First, the basis functions of SPT match the anatomical\nstructure of the LV as well as the geometric characteristics of the estimated\nindices. Second, SPT enables sharing a CNN across different orientations as a\nform of parameter regularization, and explicitly captures the scale variations\nof the LV in a natural way. Third, the residual highpass subband can be\nconveniently discarded to further encourage robust feature learning. A concise\nand effective metric, named Robustness Ratio, is proposed to evaluate the\nrobustness under various input perturbations. Extensive experiments on 145\ncardiac sequences show that our SPT-augmented method performs favorably against\nstate-of-the-art algorithms in terms of prediction accuracy, but is\nsignificantly more robust under input perturbations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhu_X/0/1/0/all/0/1\">Xiangyang Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ma_K/0/1/0/all/0/1\">Kede Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xue_W/0/1/0/all/0/1\">Wufeng Xue</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SoftDropConnect (SDC) -- Effective and Efficient Quantification of the Network Uncertainty in Deep MR Image Analysis. (arXiv:2201.08418v1 [eess.IV])","link":"http://arxiv.org/abs/2201.08418","description":"<p>Recently, deep learning has achieved remarkable successes in medical image\nanalysis. Although deep neural networks generate clinically important\npredictions, they have inherent uncertainty. Such uncertainty is a major\nbarrier to report these predictions with confidence. In this paper, we propose\na novel yet simple Bayesian inference approach called SoftDropConnect (SDC) to\nquantify the network uncertainty in medical imaging tasks with gliomas\nsegmentation and metastases classification as initial examples. Our key idea is\nthat during training and testing SDC modulates network parameters continuously\nso as to allow affected information processing channels still in operation,\ninstead of disabling them as Dropout or DropConnet does. When compared with\nthree popular Bayesian inference methods including Bayes By Backprop, Dropout,\nand DropConnect, our SDC method (SDC-W after optimization) outperforms the\nthree competing methods with a substantial margin. Quantitatively, our proposed\nmethod generates results withsubstantially improved prediction accuracy (by\n10.0%, 5.4% and 3.7% respectively for segmentation in terms of dice score; by\n11.7%, 3.9%, 8.7% on classification in terms of test accuracy) and greatly\nreduced uncertainty in terms of mutual information (by 64%, 33% and 70% on\nsegmentation; 98%, 88%, and 88% on classification). Our approach promises to\ndeliver better diagnostic performance and make medical AI imaging more\nexplainable and trustworthy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lyu_Q/0/1/0/all/0/1\">Qing Lyu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Whitlow_C/0/1/0/all/0/1\">Christopher T. Whitlow</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_G/0/1/0/all/0/1\">Ge Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FaceOcc: A Diverse, High-quality Face Occlusion Dataset for Human Face Extraction. (arXiv:2201.08425v1 [cs.CV])","link":"http://arxiv.org/abs/2201.08425","description":"<p>Occlusions often occur in face images in the wild, troubling face-related\ntasks such as landmark detection, 3D reconstruction, and face recognition. It\nis beneficial to extract face regions from unconstrained face images\naccurately. However, current face segmentation datasets suffer from small data\nvolumes, few occlusion types, low resolution, and imprecise annotation,\nlimiting the performance of data-driven-based algorithms. This paper proposes a\nnovel face occlusion dataset with manually labeled face occlusions from the\nCelebA-HQ and the internet. The occlusion types cover sunglasses, spectacles,\nhands, masks, scarfs, microphones, etc. To the best of our knowledge, it is by\nfar the largest and most comprehensive face occlusion dataset. Combining it\nwith the attribute mask in CelebAMask-HQ, we trained a straightforward face\nsegmentation model but obtained SOTA performance, convincingly demonstrating\nthe effectiveness of the proposed dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1\">Xiangnan Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liming Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Visual Analytics Approach to Building Logistic Regression Models and its Application to Health Records. (arXiv:2201.08429v1 [cs.LG])","link":"http://arxiv.org/abs/2201.08429","description":"<p>Multidimensional data analysis has become increasingly important in many\nfields, mainly due to current vast data availability and the increasing demand\nto extract knowledge from it. In most applications, the role of the final user\nis crucial to build proper machine learning models and to explain the patterns\nfound in data. In this paper, we present an open unified approach for\ngenerating, evaluating, and applying regression models in high-dimensional data\nsets within a user-guided process. The approach is based on exposing a broad\ncorrelation panorama for attributes, by which the user can select relevant\nattributes to build and evaluate prediction models for one or more contexts. We\nname the approach UCReg (User-Centered Regression). We demonstrate\neffectiveness and efficiency of UCReg through the application of our framework\nto the analysis of Covid-19 and other synthetic and real health records data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Artur_E/0/1/0/all/0/1\">Erasmo Artur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minghim_R/0/1/0/all/0/1\">Rosane Minghim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Empirical Investigation of Model-to-Model Distribution Shifts in Trained Convolutional Filters. (arXiv:2201.08465v1 [cs.CV])","link":"http://arxiv.org/abs/2201.08465","description":"<p>We present first empirical results from our ongoing investigation of\ndistribution shifts in image data used for various computer vision tasks.\nInstead of analyzing the original training and test data, we propose to study\nshifts in the learned weights of trained models. In this work, we focus on the\nproperties of the distributions of dominantly used 3x3 convolution filter\nkernels. We collected and publicly provide a data set with over half a billion\nfilters from hundreds of trained CNNs, using a wide range of data sets,\narchitectures, and vision tasks. Our analysis shows interesting distribution\nshifts (or the lack thereof) between trained filters along different axes of\nmeta-parameters, like data type, task, architecture, or layer depth. We argue,\nthat the observed properties are a valuable source for further investigation\ninto a better understanding of the impact of shifts in the input data to the\ngeneralization abilities of CNN models and novel methods for more robust\ntransfer-learning in this domain. Data available at:\nhttps://github.com/paulgavrikov/CNN-Filter-DB/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gavrikov_P/0/1/0/all/0/1\">Paul Gavrikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keuper_J/0/1/0/all/0/1\">Janis Keuper</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vertical Federated Edge Learning with Distributed Integrated Sensing and Communication. (arXiv:2201.08512v1 [eess.SP])","link":"http://arxiv.org/abs/2201.08512","description":"<p>This letter studies a vertical federated edge learning (FEEL) system for\ncollaborative objects/human motion recognition by exploiting the distributed\nintegrated sensing and communication (ISAC). In this system, distributed edge\ndevices first send wireless signals to sense targeted objects/human, and then\nexchange intermediate computed vectors (instead of raw sensing data) for\ncollaborative recognition while preserving data privacy. To boost the spectrum\nand hardware utilization efficiency for FEEL, we exploit ISAC for both target\nsensing and data exchange, by employing dedicated frequency-modulated\ncontinuous-wave (FMCW) signals at each edge device. Under this setup, we\npropose a vertical FEEL framework for realizing the recognition based on the\ncollected multi-view wireless sensing data. In this framework, each edge device\nowns an individual local L-model to transform its sensing data into an\nintermediate vector with relatively low dimensions, which is then transmitted\nto a coordinating edge device for final output via a common downstream S-model.\nBy considering a human motion recognition task, experimental results show that\nour vertical FEEL based approach achieves recognition accuracy up to 98\\% with\nan improvement up to 8\\% compared to the benchmarks, including on-device\ntraining and horizontal FEEL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Liu_P/0/1/0/all/0/1\">Peixi Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_G/0/1/0/all/0/1\">Guangxu Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jiang_W/0/1/0/all/0/1\">Wei Jiang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Luo_W/0/1/0/all/0/1\">Wu Luo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_J/0/1/0/all/0/1\">Jie Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cui_S/0/1/0/all/0/1\">Shuguang Cui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What Can Machine Vision Do for Lymphatic Histopathology Image Analysis: A Comprehensive Review. (arXiv:2201.08550v1 [cs.CV])","link":"http://arxiv.org/abs/2201.08550","description":"<p>In the past ten years, the computing power of machine vision (MV) has been\ncontinuously improved, and image analysis algorithms have developed rapidly. At\nthe same time, histopathological slices can be stored as digital images.\nTherefore, MV algorithms can provide doctors with diagnostic references. In\nparticular, the continuous improvement of deep learning algorithms has further\nimproved the accuracy of MV in disease detection and diagnosis. This paper\nreviews the applications of image processing technology based on MV in lymphoma\nhistopathological images in recent years, including segmentation,\nclassification and detection. Finally, the current methods are analyzed, some\nmore potential methods are proposed, and further prospects are made.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoqi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haoyuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahaman_M/0/1/0/all/0/1\">Md Mamunur Rahaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xintong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoyan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Hongzan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grzegorzek_M/0/1/0/all/0/1\">Marcin Grzegorzek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Classroom Slide Narration System. (arXiv:2201.08574v1 [cs.CV])","link":"http://arxiv.org/abs/2201.08574","description":"<p>Slide presentations are an effective and efficient tool used by the teaching\ncommunity for classroom communication. However, this teaching model can be\nchallenging for blind and visually impaired (VI) students. The VI student\nrequired personal human assistance for understand the presented slide. This\nshortcoming motivates us to design a Classroom Slide Narration System (CSNS)\nthat generates audio descriptions corresponding to the slide content. This\nproblem poses as an image-to-markup language generation task. The initial step\nis to extract logical regions such as title, text, equation, figure, and table\nfrom the slide image. In the classroom slide images, the logical regions are\ndistributed based on the location of the image. To utilize the location of the\nlogical regions for slide image segmentation, we propose the architecture,\nClassroom Slide Segmentation Network (CSSN). The unique attributes of this\narchitecture differs from most other semantic segmentation networks. Publicly\navailable benchmark datasets such as WiSe and SPaSe are used to validate the\nperformance of our segmentation architecture. We obtained 9.54 segmentation\naccuracy improvement in WiSe dataset. We extract content (information) from the\nslide using four well-established modules such as optical character recognition\n(OCR), figure classification, equation description, and table structure\nrecognizer. With this information, we build a Classroom Slide Narration System\n(CSNS) to help VI students understand the slide content. The users have given\nbetter feedback on the quality output of the proposed CSNS in comparison to\nexisting systems like Facebooks Automatic Alt-Text (AAT) and Tesseract.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+V%2E_J/0/1/0/all/0/1\">Jobin K.V.</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mondal_A/0/1/0/all/0/1\">Ajoy Mondal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jawahar_C/0/1/0/all/0/1\">C. V. Jawahar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SegTransVAE: Hybrid CNN -- Transformer with Regularization for medical image segmentation. (arXiv:2201.08582v1 [eess.IV])","link":"http://arxiv.org/abs/2201.08582","description":"<p>Current research on deep learning for medical image segmentation exposes\ntheir limitations in learning either global semantic information or local\ncontextual information. To tackle these issues, a novel network named\nSegTransVAE is proposed in this paper. SegTransVAE is built upon\nencoder-decoder architecture, exploiting transformer with the variational\nautoencoder (VAE) branch to the network to reconstruct the input images jointly\nwith segmentation. To the best of our knowledge, this is the first method\ncombining the success of CNN, transformer, and VAE. Evaluation on various\nrecently introduced datasets shows that SegTransVAE outperforms previous\nmethods in Dice Score and $95\\%$-Haudorff Distance while having comparable\ninference time to a simple CNN-based architecture network. The source code is\navailable at: https://github.com/itruonghai/SegTransVAE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Pham_Q/0/1/0/all/0/1\">Quan-Dung Pham</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_Truong_H/0/1/0/all/0/1\">Hai Nguyen-Truong</a> (1, 2 and 3), <a href=\"http://arxiv.org/find/eess/1/au:+Phuong_N/0/1/0/all/0/1\">Nam Nguyen Phuong</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_K/0/1/0/all/0/1\">Khoa N. A. Nguyen</a> (1, 2 and 3) ((1) VinBrain JSC., Vietnam, (2) University of Science, Ho Chi Minh City, Vietnam, (3) Vietnam National University, Ho Chi Minh City, Vietnam)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pseudo-Labeled Auto-Curriculum Learning for Semi-Supervised Keypoint Localization. (arXiv:2201.08613v1 [cs.CV])","link":"http://arxiv.org/abs/2201.08613","description":"<p>Localizing keypoints of an object is a basic visual problem. However,\nsupervised learning of a keypoint localization network often requires a large\namount of data, which is expensive and time-consuming to obtain. To remedy\nthis, there is an ever-growing interest in semi-supervised learning (SSL),\nwhich leverages a small set of labeled data along with a large set of unlabeled\ndata. Among these SSL approaches, pseudo-labeling (PL) is one of the most\npopular. PL approaches apply pseudo-labels to unlabeled data, and then train\nthe model with a combination of the labeled and pseudo-labeled data\niteratively. The key to the success of PL is the selection of high-quality\npseudo-labeled samples. Previous works mostly select training samples by\nmanually setting a single confidence threshold. We propose to automatically\nselect reliable pseudo-labeled samples with a series of dynamic thresholds,\nwhich constitutes a learning curriculum. Extensive experiments on six keypoint\nlocalization benchmark datasets demonstrate that the proposed approach\nsignificantly outperforms the previous state-of-the-art SSL approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Can Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_S/0/1/0/all/0/1\">Sheng Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_Y/0/1/0/all/0/1\">Yingda Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wentao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1\">Chen Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1\">Wanli Ouyang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dangerous Cloaking: Natural Trigger based Backdoor Attacks on Object Detectors in the Physical World. (arXiv:2201.08619v1 [cs.CV])","link":"http://arxiv.org/abs/2201.08619","description":"<p>Deep learning models have been shown to be vulnerable to recent backdoor\nattacks. A backdoored model behaves normally for inputs containing no\nattacker-secretly-chosen trigger and maliciously for inputs with the trigger.\nTo date, backdoor attacks and countermeasures mainly focus on image\nclassification tasks. And most of them are implemented in the digital world\nwith digital triggers. Besides the classification tasks, object detection\nsystems are also considered as one of the basic foundations of computer vision\ntasks. However, there is no investigation and understanding of the backdoor\nvulnerability of the object detector, even in the digital world with digital\ntriggers. For the first time, this work demonstrates that existing object\ndetectors are inherently susceptible to physical backdoor attacks. We use a\nnatural T-shirt bought from a market as a trigger to enable the cloaking\neffect--the person bounding-box disappears in front of the object detector. We\nshow that such a backdoor can be implanted from two exploitable attack\nscenarios into the object detector, which is outsourced or fine-tuned through a\npretrained model. We have extensively evaluated three popular object detection\nalgorithms: anchor-based Yolo-V3, Yolo-V4, and anchor-free CenterNet. Building\nupon 19 videos shot in real-world scenes, we confirm that the backdoor attack\nis robust against various factors: movement, distance, angle, non-rigid\ndeformation, and lighting. Specifically, the attack success rate (ASR) in most\nvideos is 100% or close to it, while the clean data accuracy of the backdoored\nmodel is the same as its clean counterpart. The latter implies that it is\ninfeasible to detect the backdoor behavior merely through a validation set. The\naveraged ASR still remains sufficiently high to be 78% in the transfer learning\nattack scenarios evaluated on CenterNet. See the demo video on\nhttps://youtu.be/Q3HOF4OobbY.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1\">Hua Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yinshan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yansong Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abuadbba_A/0/1/0/all/0/1\">Alsharif Abuadbba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_A/0/1/0/all/0/1\">Anmin Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyoungshick Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Sarawi_S/0/1/0/all/0/1\">Said F. Al-Sarawi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Surya_N/0/1/0/all/0/1\">Nepal Surya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbott_D/0/1/0/all/0/1\">Derek Abbott</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VIPriors 2: Visual Inductive Priors for Data-Efficient Deep Learning Challenges. (arXiv:2201.08625v1 [cs.CV])","link":"http://arxiv.org/abs/2201.08625","description":"<p>The second edition of the \"VIPriors: Visual Inductive Priors for\nData-Efficient Deep Learning\" challenges featured five data-impaired\nchallenges, where models are trained from scratch on a reduced number of\ntraining samples for various key computer vision tasks. To encourage new and\ncreative ideas on incorporating relevant inductive biases to improve the data\nefficiency of deep learning models, we prohibited the use of pre-trained\ncheckpoints and other transfer learning techniques. The provided baselines are\noutperformed by a large margin in all five challenges, mainly thanks to\nextensive data augmentation policies, model ensembling, and data efficient\nnetwork architectures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lengyel_A/0/1/0/all/0/1\">Attila Lengyel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruintjes_R/0/1/0/all/0/1\">Robert-Jan Bruintjes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rios_M/0/1/0/all/0/1\">Marcos Baptista Rios</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kayhan_O/0/1/0/all/0/1\">Osman Semih Kayhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zambrano_D/0/1/0/all/0/1\">Davide Zambrano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tomen_N/0/1/0/all/0/1\">Nergis Tomen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gemert_J/0/1/0/all/0/1\">Jan van Gemert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-view Monocular Depth and Uncertainty Prediction with Deep SfM in Dynamic Environments. (arXiv:2201.08633v1 [cs.CV])","link":"http://arxiv.org/abs/2201.08633","description":"<p>3D reconstruction of depth and motion from monocular video in dynamic\nenvironments is a highly ill-posed problem due to scale ambiguities when\nprojecting to the 2D image domain. In this work, we investigate the performance\nof the current State-of-the-Art (SotA) deep multi-view systems in such\nenvironments. We find that current supervised methods work surprisingly well\ndespite not modelling individual object motions, but make systematic errors due\nto a lack of dense ground truth data. To detect such errors during usage, we\nextend the cost volume based Deep Video to Depth (DeepV2D) framework\n\\cite{teed2018deepv2d} with a learned uncertainty. Our Deep Video to certain\nDepth (DeepV2cD) model allows i) to perform en par or better with current SotA\nand ii) achieve a better uncertainty measure than the naive Shannon entropy.\nOur experiments show that a simple filter strategy based on the uncertainty can\nsignificantly reduce systematic errors. This results in cleaner reconstructions\nboth on static and dynamic parts of the scene.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Homeyer_C/0/1/0/all/0/1\">Christian Homeyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lange_O/0/1/0/all/0/1\">Oliver Lange</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schnorr_C/0/1/0/all/0/1\">Christoph Schn&#xf6;rr</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conceptor Learning for Class Activation Mapping. (arXiv:2201.08636v1 [cs.CV])","link":"http://arxiv.org/abs/2201.08636","description":"<p>Class Activation Mapping (CAM) has been widely adopted to generate saliency\nmaps which provides visual explanations for deep neural networks (DNNs). The\nsaliency maps are conventionally generated by fusing the channels of the target\nfeature map using a weighted average scheme. It is a weak model for the\ninter-channel relation, in the sense that it only models the relation among\nchannels in a contrastive way (i.e., channels that play key roles in the\nprediction are given higher weights for them to stand out in the fusion). The\ncollaborative relation, which makes the channels work together to provide cross\nreference, has been ignored. Furthermore, the model has neglected the\nintra-channel relation thoroughly.In this paper, we address this problem by\nintroducing Conceptor learning into CAM generation. Conceptor leaning has been\noriginally proposed to model the patterns of state changes in recurrent neural\nnetworks (RNNs). By relaxing the dependency of Conceptor learning to RNNs, we\nmake Conceptor-CAM not only generalizable to more DNN architectures but also\nable to learn both the inter- and intra-channel relations for better saliency\nmap generation. Moreover, we have enabled the use of Boolean operations to\ncombine the positive and pseudo-negative evidences, which has made the CAM\ninference more robust and comprehensive. The effectiveness of Conceptor-CAM has\nbeen validated with both formal verifications and experiments on the dataset of\nthe largest scale in literature. The experimental results show that\nConceptor-CAM is compatible with and can bring significant improvement to all\nwell recognized CAM-based methods, and has outperformed the state-of-the-art\nmethods by 43.14%~72.79% (88.39%~168.15%) on ILSVRC2012 in Average Increase\n(Drop), 15.42%~42.55% (47.09%~372.09%) on VOC, and 17.43%~31.32%\n(47.54%~206.45%) on COCO, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qian_G/0/1/0/all/0/1\">Guangwu Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhen-Qun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xu-Lu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaowei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xiao-Yong Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Pseudo Label Quality for Semi-SupervisedDomain-Generalized Medical Image Segmentation. (arXiv:2201.08657v1 [cs.CV])","link":"http://arxiv.org/abs/2201.08657","description":"<p>Generalizing the medical image segmentation algorithms tounseen domains is an\nimportant research topic for computer-aided diagnosis and surgery. Most\nexisting methods requirea fully labeled dataset in each source domain. Although\n(Liuet al. 2021b) developed a semi-supervised domain general-ized method, it\nstill requires the domain labels. This paperpresents a novel confidence-aware\ncross pseudo supervisionalgorithm for semi-supervised domain generalized\nmedicalimage segmentation. The main goal is to enhance the pseudolabel quality\nfor unlabeled images from unknown distribu-tions. To achieve it, we perform the\nFourier transformationto learn low-level statistic information across domains\nandaugment the images to incorporate cross-domain information.With these\naugmentations as perturbations, we feed the inputto a confidence-aware cross\npseudo supervision network tomeasure the variance of pseudo labels and\nregularize the net-work to learn with more confident pseudo labels. Our\nmethodsets new records on public datasets,i.e., M&amp;Ms and SCGM.Notably, without\nusing domain labels, our method surpassesthe prior art that even uses domain\nlabels by 11.67% on Diceon M&amp;Ms dataset with 2% labeled data. Code will be\navail-able after the conference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1\">Huifeng Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaowei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaomeng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast Differentiable Matrix Square Root. (arXiv:2201.08663v1 [cs.CV])","link":"http://arxiv.org/abs/2201.08663","description":"<p>Computing the matrix square root or its inverse in a differentiable manner is\nimportant in a variety of computer vision tasks. Previous methods either adopt\nthe Singular Value Decomposition (SVD) to explicitly factorize the matrix or\nuse the Newton-Schulz iteration (NS iteration) to derive the approximate\nsolution. However, both methods are not computationally efficient enough in\neither the forward pass or in the backward pass. In this paper, we propose two\nmore efficient variants to compute the differentiable matrix square root. For\nthe forward propagation, one method is to use Matrix Taylor Polynomial (MTP),\nand the other method is to use Matrix Pad\\'e Approximants (MPA). The backward\ngradient is computed by iteratively solving the continuous-time Lyapunov\nequation using the matrix sign function. Both methods yield considerable\nspeed-up compared with the SVD or the Newton-Schulz iteration. Experimental\nresults on the de-correlated batch normalization and second-order vision\ntransformer demonstrate that our methods can also achieve competitive and even\nslightly better performances. The code is available at\n\\href{https://github.com/KingJamesSong/FastDifferentiableMatSqrt}{https://github.com/KingJamesSong/FastDifferentiableMatSqrt}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yue Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1\">Nicu Sebe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Deep Convolutional Candlestick Learner. (arXiv:2201.08669v1 [cs.CV])","link":"http://arxiv.org/abs/2201.08669","description":"<p>Candlestick pattern is one of the most fundamental and valuable graphical\ntools in financial trading that supports traders observing the current market\nconditions to make the proper decision. This task has a long history and, most\nof the time, human experts. Recently, efforts have been made to automatically\nclassify these patterns with the deep learning models. The GAF-CNN model is a\nwell-suited way to imitate how human traders capture the candlestick pattern by\nintegrating spatial features visually. However, with the great potential of the\nGAF encoding, this classification task can be extended to a more complicated\nobject detection level. This work presents an innovative integration of modern\nobject detection techniques and GAF time-series encoding on candlestick pattern\ntasks. We make crucial modifications to the representative yet straightforward\nYOLO version 1 model based on our time-series encoding method and the property\nof such data type. Powered by the deep neural networks and the unique\narchitectural design, the proposed model performs pretty well in candlestick\nclassification and location recognition. The results show tremendous potential\nin applying modern object detection techniques on time-series tasks in a\nreal-time manner.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jun-Hao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsai_Y/0/1/0/all/0/1\">Yun-Cheng Tsai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Fusion Strategies for Accurate RGBT Visual Object Tracking. (arXiv:2201.08673v1 [cs.CV])","link":"http://arxiv.org/abs/2201.08673","description":"<p>We address the problem of multi-modal object tracking in video and explore\nvarious options of fusing the complementary information conveyed by the visible\n(RGB) and thermal infrared (TIR) modalities including pixel-level,\nfeature-level and decision-level fusion. Specifically, different from the\nexisting methods, paradigm of image fusion task is heeded for fusion at pixel\nlevel. Feature-level fusion is fulfilled by attention mechanism with channels\nexcited optionally. Besides, at decision level, a novel fusion strategy is put\nforward since an effortless averaging configuration has shown the superiority.\nThe effectiveness of the proposed decision-level fusion strategy owes to a\nnumber of innovative contributions, including a dynamic weighting of the RGB\nand TIR contributions and a linear template update operation. A variant of\nwhich produced the winning tracker at the Visual Object Tracking Challenge 2020\n(VOT-RGBT2020). The concurrent exploration of innovative pixel- and\nfeature-level fusion strategies highlights the advantages of the proposed\ndecision-level fusion method. Extensive experimental results on three\nchallenging datasets, \\textit{i.e.}, GTOT, VOT-RGBT2019, and VOT-RGBT2020,\ndemonstrate the effectiveness and robustness of the proposed method, compared\nto the state-of-the-art approaches. Code will be shared at\n\\textcolor{blue}{\\emph{https://github.com/Zhangyong-Tang/DFAT}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1\">Zhangyong Tang</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Tianyang Xu</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hui Li</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiao-Jun Wu</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xuefeng Zhu</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Kittler_J/0/1/0/all/0/1\">Josef Kittler</a> (2) ((1) Jiangnan University, Wuxi, China, (2) University of Surrey, UK)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distance-Ratio-Based Formulation for Metric Learning. (arXiv:2201.08676v1 [cs.LG])","link":"http://arxiv.org/abs/2201.08676","description":"<p>In metric learning, the goal is to learn an embedding so that data points\nwith the same class are close to each other and data points with different\nclasses are far apart. We propose a distance-ratio-based (DR) formulation for\nmetric learning. Like softmax-based formulation for metric learning, it models\n$p(y=c|x')$, which is a probability that a query point $x'$ belongs to a class\n$c$. The DR formulation has two useful properties. First, the corresponding\nloss is not affected by scale changes of an embedding. Second, it outputs the\noptimal (maximum or minimum) classification confidence scores on representing\npoints for classes. To demonstrate the effectiveness of our formulation, we\nconduct few-shot classification experiments using softmax-based and DR\nformulations on CUB and mini-ImageNet datasets. The results show that DR\nformulation generally enables faster and more stable metric learning than the\nsoftmax-based formulation. As a result, using DR formulation achieves improved\nor comparable generalization performances.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyeongji Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parviainen_P/0/1/0/all/0/1\">Pekka Parviainen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malde_K/0/1/0/all/0/1\">Ketil Malde</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comprehensive Study of Vision Transformers on Dense Prediction Tasks. (arXiv:2201.08683v1 [cs.CV])","link":"http://arxiv.org/abs/2201.08683","description":"<p>Convolutional Neural Networks (CNNs), architectures consisting of\nconvolutional layers, have been the standard choice in vision tasks. Recent\nstudies have shown that Vision Transformers (VTs), architectures based on\nself-attention modules, achieve comparable performance in challenging tasks\nsuch as object detection and semantic segmentation. However, the image\nprocessing mechanism of VTs is different from that of conventional CNNs. This\nposes several questions about their generalizability, robustness, reliability,\nand texture bias when used to extract features for complex tasks. To address\nthese questions, we study and compare VT and CNN architectures as feature\nextractors in object detection and semantic segmentation. Our extensive\nempirical results show that the features generated by VTs are more robust to\ndistribution shifts, natural corruptions, and adversarial attacks in both\ntasks, whereas CNNs perform better at higher image resolutions in object\ndetection. Furthermore, our results demonstrate that VTs in dense prediction\ntasks produce more reliable and less texture-biased predictions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jeeveswaran_K/0/1/0/all/0/1\">Kishaan Jeeveswaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kathiresan_S/0/1/0/all/0/1\">Senthilkumar Kathiresan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varma_A/0/1/0/all/0/1\">Arnav Varma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Magdy_O/0/1/0/all/0/1\">Omar Magdy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zonooz_B/0/1/0/all/0/1\">Bahram Zonooz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arani_E/0/1/0/all/0/1\">Elahe Arani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SparseAlign: A Super-Resolution Algorithm for Automatic Marker Localization and Deformation Estimation in Cryo-Electron Tomography. (arXiv:2201.08706v1 [eess.IV])","link":"http://arxiv.org/abs/2201.08706","description":"<p>Tilt-series alignment is crucial to obtaining high-resolution reconstructions\nin cryo-electron tomography. Beam-induced local deformation of the sample is\nhard to estimate from the low-contrast sample alone, and often requires\nfiducial gold bead markers. The state-of-the-art approach for deformation\nestimation uses (semi-)manually labelled marker locations in projection data to\nfit the parameters of a polynomial deformation model. Manually-labelled marker\nlocations are difficult to obtain when data are noisy or markers overlap in\nprojection data. We propose an alternative mathematical approach for\nsimultaneous marker localization and deformation estimation by extending a\ngrid-free super-resolution algorithm first proposed in the context of\nsingle-molecule localization microscopy. Our approach does not require labelled\nmarker locations; instead, we use an image-based loss where we compare the\nforward projection of markers with the observed data. We equip this marker\nlocalization scheme with an additional deformation estimation component and\nsolve for a reduced number of deformation parameters. Using extensive numerical\nstudies on marker-only samples, we show that our approach automatically finds\nmarkers and reliably estimates sample deformation without labelled marker data.\nWe further demonstrate the applicability of our approach for a broad range of\nmodel mismatch scenarios, including experimental electron tomography data of\ngold markers on ice.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ganguly_P/0/1/0/all/0/1\">Poulami Somanya Ganguly</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lucka_F/0/1/0/all/0/1\">Felix Lucka</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kohr_H/0/1/0/all/0/1\">Holger Kohr</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Franken_E/0/1/0/all/0/1\">Erik Franken</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hupkes_H/0/1/0/all/0/1\">Hermen Jan Hupkes</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Batenburg_K/0/1/0/all/0/1\">K Joost Batenburg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Across-Dataset Brain Tissue Segmentation Using Transformer. (arXiv:2201.08741v1 [eess.IV])","link":"http://arxiv.org/abs/2201.08741","description":"<p>Brain tissue segmentation has demonstrated great utility in quantifying MRI\ndata through Voxel-Based Morphometry and highlighting subtle structural changes\nassociated with various conditions within the brain. However, manual\nsegmentation is highly labor-intensive, and automated approaches have struggled\ndue to properties inherent to MRI acquisition, leaving a great need for an\neffective segmentation tool. Despite the recent success of deep convolutional\nneural networks (CNNs) for brain tissue segmentation, many such solutions do\nnot generalize well to new datasets, which is critical for a reliable solution.\nTransformers have demonstrated success in natural image segmentation and have\nrecently been applied to 3D medical image segmentation tasks due to their\nability to capture long-distance relationships in the input where the local\nreceptive fields of CNNs struggle. This study introduces a novel\nCNN-Transformer hybrid architecture designed for brain tissue segmentation. We\nvalidate our model's performance across four multi-site T1w MRI datasets,\ncovering different vendors, field strengths, scan parameters, time points, and\nneuropsychiatric conditions. In all situations, our model achieved the greatest\ngenerality and reliability. Out method is inherently robust and can serve as a\nvaluable tool for brain-related T1w MRI studies. The code for the TABS network\nis available at: https://github.com/raovish6/TABS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Rao_V/0/1/0/all/0/1\">Vishwanatha M. Rao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wan_Z/0/1/0/all/0/1\">Zihan Wan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ma_D/0/1/0/all/0/1\">David J. Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_P/0/1/0/all/0/1\">Pin-Yu Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tian_Y/0/1/0/all/0/1\">Ye Tian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Laine_A/0/1/0/all/0/1\">Andrew F. Laine</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guo_J/0/1/0/all/0/1\">Jia Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ERS: a novel comprehensive endoscopy image dataset for machine learning, compliant with the MST 3.0 specification. (arXiv:2201.08746v1 [cs.CV])","link":"http://arxiv.org/abs/2201.08746","description":"<p>The article presents a new multi-label comprehensive image dataset from\nflexible endoscopy, colonoscopy and capsule endoscopy, named ERS. The\ncollection has been labeled according to the full medical specification of\n'Minimum Standard Terminology 3.0' (MST 3.0), describing all possible findings\nin the gastrointestinal tract (104 possible labels), extended with an\nadditional 19 labels useful in common machine learning applications.\n</p>\n<p>The dataset contains around 6000 precisely and 115,000 approximately labeled\nframes from endoscopy videos, 3600 precise and 22,600 approximate segmentation\nmasks, and 1.23 million unlabeled frames from flexible and capsule endoscopy\nvideos. The labeled data cover almost entirely the MST 3.0 standard. The data\ncame from 1520 videos of 1135 patients.\n</p>\n<p>Additionally, this paper proposes and describes four exemplary experiments in\ngastrointestinal image classification task performed using the created dataset.\nThe obtained results indicate the high usefulness and flexibility of the\ndataset in training and testing machine learning algorithms in the field of\nendoscopic data analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cychnerski_J/0/1/0/all/0/1\">Jan Cychnerski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dziubich_T/0/1/0/all/0/1\">Tomasz Dziubich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brzeski_A/0/1/0/all/0/1\">Adam Brzeski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Object Detection in Aerial Images: What Improves the Accuracy?. (arXiv:2201.08763v1 [cs.CV])","link":"http://arxiv.org/abs/2201.08763","description":"<p>Object detection is a challenging and popular computer vision problem. The\nproblem is even more challenging in aerial images due to significant variation\nin scale and viewpoint in a diverse set of object categories. Recently, deep\nlearning-based object detection approaches have been actively explored for the\nproblem of object detection in aerial images. In this work, we investigate the\nimpact of Faster R-CNN for aerial object detection and explore numerous\nstrategies to improve its performance for aerial images. We conduct extensive\nexperiments on the challenging iSAID dataset. The resulting adapted Faster\nR-CNN obtains a significant mAP gain of 4.96% over its vanilla baseline\ncounterpart on the iSAID validation set, demonstrating the impact of different\nstrategies investigated in this work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Malik_H/0/1/0/all/0/1\">Hashmat Shadab Malik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sobirov_I/0/1/0/all/0/1\">Ikboljon Sobirov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1\">Abdelrahman Mohamed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive and Selective Hidden Embeddings for Medical Image Segmentation. (arXiv:2201.08779v1 [cs.CV])","link":"http://arxiv.org/abs/2201.08779","description":"<p>Medical image segmentation has been widely recognized as a pivot procedure\nfor clinical diagnosis, analysis, and treatment planning. However, the\nlaborious and expensive annotation process lags down the speed of further\nadvances. Contrastive learning-based weight pre-training provides an\nalternative by leveraging unlabeled data to learn a good representation. In\nthis paper, we investigate how contrastive learning benefits the general\nsupervised medical segmentation tasks. To this end, patch-dragsaw contrastive\nregularization (PDCR) is proposed to perform patch-level tugging and repulsing\nwith the extent controlled by a continuous affinity score. And a new structure\ndubbed uncertainty-aware feature selection block (UAFS) is designed to perform\nthe feature selection process, which can handle the learning target shift\ncaused by minority features with high uncertainty. By plugging the proposed 2\nmodules into the existing segmentation architecture, we achieve\nstate-of-the-art results across 8 public datasets from 6 domains. Newly\ndesigned modules further decrease the amount of training data to a quarter\nwhile achieving comparable, if not better, performances. From this perspective,\nwe take the opposite direction of the original self/un-supervised contrastive\nlearning by further excavating information contained within the label.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhuowei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zihao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhiqiang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_Q/0/1/0/all/0/1\">Qing Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_R/0/1/0/all/0/1\">Ruiqin Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shaoting Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metaxas_D/0/1/0/all/0/1\">Dimitris Metaxas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1\">Tingting Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AiTLAS: Artificial Intelligence Toolbox for Earth Observation. (arXiv:2201.08789v1 [cs.CV])","link":"http://arxiv.org/abs/2201.08789","description":"<p>The AiTLAS toolbox (Artificial Intelligence Toolbox for Earth Observation)\nincludes state-of-the-art machine learning methods for exploratory and\npredictive analysis of satellite imagery as well as repository of AI-ready\nEarth Observation (EO) datasets. It can be easily applied for a variety of\nEarth Observation tasks, such as land use and cover classification, crop type\nprediction, localization of specific objects (semantic segmentation), etc. The\nmain goal of AiTLAS is to facilitate better usability and adoption of novel AI\nmethods (and models) by EO experts, while offering easy access and standardized\nformat of EO datasets to AI experts which further allows benchmarking of\nvarious existing and novel AI methods tailored for EO data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dimitrovski_I/0/1/0/all/0/1\">Ivica Dimitrovski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitanovski_I/0/1/0/all/0/1\">Ivan Kitanovski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panov_P/0/1/0/all/0/1\">Pan&#x10d;e Panov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simidjievski_N/0/1/0/all/0/1\">Nikola Simidjievski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kocev_D/0/1/0/all/0/1\">Dragi Kocev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Realtime 3D Object Detection for Headsets. (arXiv:2201.08812v1 [cs.CV])","link":"http://arxiv.org/abs/2201.08812","description":"<p>Mobile headsets should be capable of understanding 3D physical environments\nto offer a truly immersive experience for augmented/mixed reality (AR/MR).\nHowever, their small form-factor and limited computation resources make it\nextremely challenging to execute in real-time 3D vision algorithms, which are\nknown to be more compute-intensive than their 2D counterparts. In this paper,\nwe propose DeepMix, a mobility-aware, lightweight, and hybrid3D object\ndetection framework for improving the user experience of AR/MR on mobile\nheadsets. Motivated by our analysis and evaluation of state-of-the-art 3D\nobject detection models, DeepMix intelligently combines edge-assisted 2D object\ndetection and novel, on-device 3D bounding box estimations that leverage depth\ndata captured by headsets. This leads to low end-to-end latency and\nsignificantly boosts detection accuracy in mobile scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guan_Y/0/1/0/all/0/1\">Yongjie Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_X/0/1/0/all/0/1\">Xueyu Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_N/0/1/0/all/0/1\">Nan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1\">Bo Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_T/0/1/0/all/0/1\">Tao Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Active Predictive Coding Networks: A Neural Solution to the Problem of Learning Reference Frames and Part-Whole Hierarchies. (arXiv:2201.08813v1 [cs.CV])","link":"http://arxiv.org/abs/2201.08813","description":"<p>We introduce Active Predictive Coding Networks (APCNs), a new class of neural\nnetworks that solve a major problem posed by Hinton and others in the fields of\nartificial intelligence and brain modeling: how can neural networks learn\nintrinsic reference frames for objects and parse visual scenes into part-whole\nhierarchies by dynamically allocating nodes in a parse tree? APCNs address this\nproblem by using a novel combination of ideas: (1) hypernetworks are used for\ndynamically generating recurrent neural networks that predict parts and their\nlocations within intrinsic reference frames conditioned on higher object-level\nembedding vectors, and (2) reinforcement learning is used in conjunction with\nbackpropagation for end-to-end learning of model parameters. The APCN\narchitecture lends itself naturally to multi-level hierarchical learning and is\nclosely related to predictive coding models of cortical function. Using the\nMNIST, Fashion-MNIST and Omniglot datasets, we demonstrate that APCNs can (a)\nlearn to parse images into part-whole hierarchies, (b) learn compositional\nrepresentations, and (c) transfer their knowledge to unseen classes of objects.\nWith their ability to dynamically generate parse trees with part locations for\nobjects, APCNs offer a new framework for explainable AI that leverages advances\nin deep learning while retaining interpretability and compositionality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gklezakos_D/0/1/0/all/0/1\">Dimitrios C. Gklezakos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_R/0/1/0/all/0/1\">Rajesh P. N. Rao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning from One and Only One Shot. (arXiv:2201.08815v1 [cs.CV])","link":"http://arxiv.org/abs/2201.08815","description":"<p>Humans can generalize from only a few examples and from little pre-training\non similar tasks. Yet, machine learning (ML) typically requires large data to\nlearn or pre-learn to transfer. Inspired by nativism, we directly model basic\nhuman-innate priors in abstract visual tasks e.g., character/doodle\nrecognition. This yields a white-box model that learns general-appearance\nsimilarity -- how any two images look in general -- by mimicking how humans\nnaturally \"distort\" an object at first sight. Using simply the nearest-neighbor\nclassifier on this similarity space, we achieve human-level character\nrecognition using only 1--10 examples per class and nothing else (no\npre-training). This differs from few-shot learning (FSL) using significant\npre-training. On standard benchmarks MNIST/EMNIST and the Omniglot challenge,\nwe outperform both neural-network-based and classical ML in the \"tiny-data\"\nregime, including FSL pre-trained on large data. Our model enables unsupervised\nlearning too: by learning the non-Euclidean, general-appearance similarity\nspace in a k-means style, we can generate human-intuitive archetypes as cluster\n``centroids''.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Haizi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mineyev_I/0/1/0/all/0/1\">Igor Mineyev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varshney_L/0/1/0/all/0/1\">Lav R. Varshney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Evans_J/0/1/0/all/0/1\">James A. Evans</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Skyline variations allow estimating distance to trees on landscape photos using semantic segmentation. (arXiv:2201.08816v1 [cs.CV])","link":"http://arxiv.org/abs/2201.08816","description":"<p>Approximate distance estimation can be used to determine fundamental\nlandscape properties including complexity and openness. We show that variations\nin the skyline of landscape photos can be used to estimate distances to trees\non the horizon. A methodology based on the variations of the skyline has been\ndeveloped and used to investigate potential relationships with the distance to\nskyline objects. The skyline signal, defined by the skyline height expressed in\npixels, was extracted for several Land Use/Cover Area frame Survey (LUCAS)\nlandscape photos. Photos were semantically segmented with DeepLabV3+ trained\nwith the Common Objects in Context (COCO) dataset. This provided pixel-level\nclassification of the objects forming the skyline. A Conditional Random Fields\n(CRF) algorithm was also applied to increase the details of the skyline signal.\nThree metrics, able to capture the skyline signal variations, were then\nconsidered for the analysis. These metrics shows a functional relationship with\ndistance for the class of trees, whose contours have a fractal nature. In\nparticular, regression analysis was performed against 475 ortho-photo based\ndistance measurements, and, in the best case, a R2 score equal to 0.47 was\nachieved. This is an encouraging result which shows the potential of skyline\nvariation metrics for inferring distance related information.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Martinez_Sanchez_L/0/1/0/all/0/1\">Laura Martinez-Sanchez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borio_D/0/1/0/all/0/1\">Daniele Borio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+dAndrimont_R/0/1/0/all/0/1\">Rapha&#xeb;l d&#x27;Andrimont</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Velde_M/0/1/0/all/0/1\">Marijn van der Velde</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reliable Detection of Doppelg\\\"angers based on Deep Face Representations. (arXiv:2201.08831v1 [cs.CV])","link":"http://arxiv.org/abs/2201.08831","description":"<p>Doppelg\\\"angers (or lookalikes) usually yield an increased probability of\nfalse matches in a facial recognition system, as opposed to random face image\npairs selected for non-mated comparison trials. In this work, we assess the\nimpact of doppelg\\\"angers on the HDA Doppelg\\\"anger and Disguised Faces in The\nWild databases using a state-of-the-art face recognition system. It is found\nthat doppelg\\\"anger image pairs yield very high similarity scores resulting in\na significant increase of false match rates. Further, we propose a\ndoppelg\\\"anger detection method which distinguishes doppelg\\\"angers from mated\ncomparison trials by analysing differences in deep representations obtained\nfrom face image pairs. The proposed detection system employs a machine\nlearning-based classifier, which is trained with generated doppelg\\\"anger image\npairs utilising face morphing techniques. Experimental evaluations conducted on\nthe HDA Doppelg\\\"anger and Look-Alike Face databases reveal a detection equal\nerror rate of approximately 2.7% for the task of separating mated\nauthentication attempts from doppelg\\\"angers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rathgeb_C/0/1/0/all/0/1\">Christian Rathgeb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fischer_D/0/1/0/all/0/1\">Daniel Fischer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drozdowski_P/0/1/0/all/0/1\">Pawel Drozdowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Busch_C/0/1/0/all/0/1\">Christoph Busch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Point-NeRF: Point-based Neural Radiance Fields. (arXiv:2201.08845v1 [cs.CV])","link":"http://arxiv.org/abs/2201.08845","description":"<p>Volumetric neural rendering methods like NeRF generate high-quality view\nsynthesis results but are optimized per-scene leading to prohibitive\nreconstruction time. On the other hand, deep multi-view stereo methods can\nquickly reconstruct scene geometry via direct network inference. Point-NeRF\ncombines the advantages of these two approaches by using neural 3D point\nclouds, with associated neural features, to model a radiance field. Point-NeRF\ncan be rendered efficiently by aggregating neural point features near scene\nsurfaces, in a ray marching-based rendering pipeline. Moreover, Point-NeRF can\nbe initialized via direct inference of a pre-trained deep network to produce a\nneural point cloud; this point cloud can be finetuned to surpass the visual\nquality of NeRF with 30X faster training time. Point-NeRF can be combined with\nother 3D reconstruction methods and handles the errors and outliers in such\nmethods via a novel pruning and growing mechanism.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qiangeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zexiang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Philip_J/0/1/0/all/0/1\">Julien Philip</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_S/0/1/0/all/0/1\">Sai Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_Z/0/1/0/all/0/1\">Zhixin Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sunkavalli_K/0/1/0/all/0/1\">Kalyan Sunkavalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neumann_U/0/1/0/all/0/1\">Ulrich Neumann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Globally Optimal 2D-to-3D Deformable Shape Matching. (arXiv:1601.06070v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1601.06070","description":"<p>We propose the first algorithm for non-rigid 2D-to-3D shape matching, where\nthe input is a 2D shape represented as a planar curve and a 3D shape\nrepresented as a surface; the output is a continuous curve on the surface. We\ncast the problem as finding the shortest circular path on the product\n3-manifold of the surface and the curve. We prove that the optimal matching can\nbe computed in polynomial time with a (worst-case) complexity of\n$O(mn^2\\log(n))$, where $m$ and $n$ denote the number of vertices on the\ntemplate curve and the 3D shape respectively. We also demonstrate that in\npractice the runtime is essentially linear in $m\\!\\cdot\\! n$ making it an\nefficient method for shape analysis and shape retrieval. Quantitative\nevaluation confirms that the method provides excellent results for sketch-based\ndeformable 3D shape retrieval.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lahner_Z/0/1/0/all/0/1\">Zorah L&#xe4;hner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodola_E/0/1/0/all/0/1\">Emanuele Rodol&#xe0;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_F/0/1/0/all/0/1\">Frank R. Schmidt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bronstein_M/0/1/0/all/0/1\">Michael M. Bronstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cremers_D/0/1/0/all/0/1\">Daniel Cremers</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analysis and algorithms for $\\ell_p$-based semi-supervised learning on graphs. (arXiv:1901.05031v3 [math.NA] UPDATED)","link":"http://arxiv.org/abs/1901.05031","description":"<p>This paper addresses theory and applications of $\\ell_p$-based Laplacian\nregularization in semi-supervised learning. The graph $p$-Laplacian for $p&gt;2$\nhas been proposed recently as a replacement for the standard ($p=2$) graph\nLaplacian in semi-supervised learning problems with very few labels, where\nLaplacian learning is degenerate.\n</p>\n<p>In the first part of the paper we prove new discrete to continuum convergence\nresults for $p$-Laplace problems on $k$-nearest neighbor ($k$-NN) graphs, which\nare more commonly used in practice than random geometric graphs. Our analysis\nshows that, on $k$-NN graphs, the $p$-Laplacian retains information about the\ndata distribution as $p\\to \\infty$ and Lipschitz learning ($p=\\infty$) is\nsensitive to the data distribution. This situation can be contrasted with\nrandom geometric graphs, where the $p$-Laplacian forgets the data distribution\nas $p\\to \\infty$. We also present a general framework for proving discrete to\ncontinuum convergence results in graph-based learning that only requires\npointwise consistency and monotonicity.\n</p>\n<p>In the second part of the paper, we develop fast algorithms for solving the\nvariational and game-theoretic $p$-Laplace equations on weighted graphs for\n$p&gt;2$. We present several efficient and scalable algorithms for both\nformulations, and present numerical results on synthetic data indicating their\nconvergence properties. Finally, we conduct extensive numerical experiments on\nthe MNIST, FashionMNIST and EMNIST datasets that illustrate the effectiveness\nof the $p$-Laplacian formulation for semi-supervised learning with few labels.\nIn particular, we find that Lipschitz learning ($p=\\infty$) performs well with\nvery few labels on $k$-NN graphs, which experimentally validates our\ntheoretical findings that Lipschitz learning retains information about the data\ndistribution (the unlabeled data) on $k$-NN graphs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/math/1/au:+Rios_M/0/1/0/all/0/1\">Mauricio Flores Rios</a>, <a href=\"http://arxiv.org/find/math/1/au:+Calder_J/0/1/0/all/0/1\">Jeff Calder</a>, <a href=\"http://arxiv.org/find/math/1/au:+Lerman_G/0/1/0/all/0/1\">Gilad Lerman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Pixel to Patch: Synthesize Context-aware Features for Zero-shot Semantic Segmentation. (arXiv:2009.12232v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2009.12232","description":"<p>Zero-shot learning has been actively studied for image classification task to\nrelieve the burden of annotating image labels. Interestingly, semantic\nsegmentation task requires more labor-intensive pixel-wise annotation, but\nzero-shot semantic segmentation has only attracted limited research interest.\nThus, we focus on zero-shot semantic segmentation, which aims to segment unseen\nobjects with only category-level semantic representations provided for unseen\ncategories. In this paper, we propose a novel Context-aware feature Generation\nNetwork (CaGNet), which can synthesize context-aware pixel-wise visual features\nfor unseen categories based on category-level semantic representations and\npixel-wise contextual information. The synthesized features are used to\nfinetune the classifier to enable segmenting unseen objects. Furthermore, we\nextend pixel-wise feature generation and finetuning to patch-wise feature\ngeneration and finetuning, which additionally considers inter-pixel\nrelationship. Experimental results on Pascal-VOC, Pascal-Context, and\nCOCO-stuff show that our method significantly outperforms the existing\nzero-shot semantic segmentation methods. Code is available at\nhttps://github.com/bcmi/CaGNetv2-Zero-Shot-Semantic-Segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_Z/0/1/0/all/0/1\">Zhangxuan Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Siyuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_L/0/1/0/all/0/1\">Li Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zihan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liqing Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Learning with Stronger Augmentations. (arXiv:2104.07713v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.07713","description":"<p>Representation learning has significantly been developed with the advance of\ncontrastive learning methods. Most of those methods have benefited from various\ndata augmentations that are carefully designated to maintain their identities\nso that the images transformed from the same instance can still be retrieved.\nHowever, those carefully designed transformations limited us to further explore\nthe novel patterns exposed by other transformations. Meanwhile, as found in our\nexperiments, the strong augmentations distorted the images' structures,\nresulting in difficult retrieval. Thus, we propose a general framework called\nContrastive Learning with Stronger Augmentations~(CLSA) to complement current\ncontrastive learning approaches. Here, the distribution divergence between the\nweakly and strongly augmented images over the representation bank is adopted to\nsupervise the retrieval of strongly augmented queries from a pool of instances.\nExperiments on the ImageNet dataset and downstream datasets showed the\ninformation from the strongly augmented images can significantly boost the\nperformance. For example, CLSA achieves top-1 accuracy of 76.2% on ImageNet\nwith a standard ResNet-50 architecture with a single-layer classifier\nfine-tuned, which is almost the same level as 76.5% of supervised results. The\ncode and pre-trained models are available in\nhttps://github.com/maple-research-lab/CLSA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_G/0/1/0/all/0/1\">Guo-Jun Qi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Aliased Resizing and Surprising Subtleties in GAN Evaluation. (arXiv:2104.11222v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.11222","description":"<p>Metrics for evaluating generative models aim to measure the discrepancy\nbetween real and generated images. The often-used Frechet Inception Distance\n(FID) metric, for example, extracts \"high-level\" features using a deep network\nfrom the two sets. However, we find that the differences in \"low-level\"\npreprocessing, specifically image resizing and compression, can induce large\nvariations and have unforeseen consequences. For instance, when resizing an\nimage, e.g., with a bilinear or bicubic kernel, signal processing principles\nmandate adjusting prefilter width depending on the downsampling factor, to\nantialias to the appropriate bandwidth. However, commonly-used implementations\nuse a fixed-width prefilter, resulting in aliasing artifacts. Such aliasing\nleads to corruptions in the feature extraction downstream. Next, lossy\ncompression, such as JPEG, is commonly used to reduce the file size of an\nimage. Although designed to minimally degrade the perceptual quality of an\nimage, the operation also produces variations downstream. Furthermore, we show\nthat if compression is used on real training images, FID can actually improve\nif the generated images are also subsequently compressed. This paper shows that\nchoices in low-level image processing have been an underappreciated aspect of\ngenerative modeling. We identify and characterize variations in generative\nmodeling development pipelines, provide recommendations based on signal\nprocessing principles, and release a reference implementation to facilitate\nfuture comparisons.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Parmar_G/0/1/0/all/0/1\">Gaurav Parmar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Richard Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jun-Yan Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CapillaryNet: An Automated System to Quantify Skin Capillary Density and Red Blood Cell Velocity from Handheld Vital Microscopy. (arXiv:2104.11574v4 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2104.11574","description":"<p>Capillaries are the smallest vessels in the body responsible for delivering\noxygen and nutrients to surrounding cells. Various life-threatening diseases\nare known to alter the density of healthy capillaries and the flow velocity of\nerythrocytes within the capillaries. In previous studies, capillary density and\nflow velocity were manually assessed by trained specialists. However, manual\nanalysis of a standard 20-second microvascular video requires 20 minutes on\naverage and necessitates extensive training. Thus, manual analysis has been\nreported to hinder the application of microvascular microscopy in a clinical\nenvironment. To address this problem, this paper presents a fully automated\nstate-of-the-art system to quantify skin nutritive capillary density and red\nblood cell velocity captured by handheld-based microscopy videos. The proposed\nmethod combines the speed of traditional computer vision algorithms with the\naccuracy of convolutional neural networks to enable clinical capillary\nanalysis. The results show that the proposed system fully automates capillary\ndetection with an accuracy exceeding that of trained analysts and measures\nseveral novel microvascular parameters that had eluded quantification thus far,\nnamely, capillary hematocrit and intracapillary flow velocity heterogeneity.\nThe proposed end-to-end system, named CapillaryNet, can detect capillaries at\n$\\sim$0.9 seconds per frame with $\\sim$93\\% accuracy. The system is currently\nbeing used as a clinical research product in a larger e-health application to\nanalyse capillary data captured from patients suffering from COVID-19,\npancreatitis, and acute heart diseases. CapillaryNet narrows the gap between\nthe analysis of microcirculation images in a clinical environment and\nstate-of-the-art systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Helmy_M/0/1/0/all/0/1\">Maged Helmy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dykyy_A/0/1/0/all/0/1\">Anastasiya Dykyy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Truong_T/0/1/0/all/0/1\">Tuyen Trung Truong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ferreira_P/0/1/0/all/0/1\">Paulo Ferreira</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jul_E/0/1/0/all/0/1\">Eric Jul</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End Jet Classification of Boosted Top Quarks with the CMS Open Data. (arXiv:2104.14659v3 [physics.data-an] UPDATED)","link":"http://arxiv.org/abs/2104.14659","description":"<p>We describe a novel application of the end-to-end deep learning technique to\nthe task of discriminating top quark-initiated jets from those originating from\nthe hadronization of a light quark or a gluon. The end-to-end deep learning\ntechnique combines deep learning algorithms and low-level detector\nrepresentation of the high-energy collision event. In this study, we use\nlow-level detector information from the simulated CMS Open Data samples to\nconstruct the top jet classifiers. To optimize classifier performance we\nprogressively add low-level information from the CMS tracking detector,\nincluding pixel detector reconstructed hits and impact parameters, and\ndemonstrate the value of additional tracking information even when no new\nspatial structures are added. Relying only on calorimeter energy deposits and\nreconstructed pixel detector hits, the end-to-end classifier achieves an AUC\nscore of 0.975$\\pm$0.002 for the task of classifying boosted top quark jets.\nAfter adding derived track quantities, the classifier AUC score increases to\n0.9824$\\pm$0.0013, serving as the first performance benchmark for these CMS\nOpen Data samples. We additionally provide a timing performance comparison of\ndifferent processor unit architectures for training the network.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Andrews_M/0/1/0/all/0/1\">Michael Andrews</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Burkle_B/0/1/0/all/0/1\">Bjorn Burkle</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Chen_Y/0/1/0/all/0/1\">Yi-fan Chen</a>, <a href=\"http://arxiv.org/find/physics/1/au:+DiCroce_D/0/1/0/all/0/1\">Davide DiCroce</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Gleyzer_S/0/1/0/all/0/1\">Sergei Gleyzer</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Heintz_U/0/1/0/all/0/1\">Ulrich Heintz</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Narain_M/0/1/0/all/0/1\">Meenakshi Narain</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Paulini_M/0/1/0/all/0/1\">Manfred Paulini</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Pervan_N/0/1/0/all/0/1\">Nikolas Pervan</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Shafi_Y/0/1/0/all/0/1\">Yusef Shafi</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Sun_W/0/1/0/all/0/1\">Wei Sun</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Usai_E/0/1/0/all/0/1\">Emanuele Usai</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Yang_K/0/1/0/all/0/1\">Kun Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"You Never Cluster Alone. (arXiv:2106.01908v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.01908","description":"<p>Recent advances in self-supervised learning with instance-level contrastive\nobjectives facilitate unsupervised clustering. However, a standalone datum is\nnot perceiving the context of the holistic cluster, and may undergo sub-optimal\nassignment. In this paper, we extend the mainstream contrastive learning\nparadigm to a cluster-level scheme, where all the data subjected to the same\ncluster contribute to a unified representation that encodes the context of each\ndata group. Contrastive learning with this representation then rewards the\nassignment of each datum. To implement this vision, we propose twin-contrast\nclustering (TCC). We define a set of categorical variables as clustering\nassignment confidence, which links the instance-level learning track with the\ncluster-level one. On one hand, with the corresponding assignment variables\nbeing the weight, a weighted aggregation along the data points implements the\nset representation of a cluster. We further propose heuristic cluster\naugmentation equivalents to enable cluster-level contrastive learning. On the\nother hand, we derive the evidence lower-bound of the instance-level\ncontrastive objective with the assignments. By reparametrizing the assignment\nvariables, TCC is trained end-to-end, requiring no alternating steps. Extensive\nexperiments show that TCC outperforms the state-of-the-art on challenging\nbenchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yuming Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Ziyi Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Menghan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1\">Jie Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip H.S. Torr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ProtoRes: Proto-Residual Architecture for Deep Modeling of Human Pose. (arXiv:2106.01981v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.01981","description":"<p>Our work focuses on the development of a learnable neural representation of\nhuman pose for advanced AI assisted animation tooling. Specifically, we tackle\nthe problem of constructing a full static human pose based on sparse and\nvariable user inputs (e.g. locations and/or orientations of a subset of body\njoints). To solve this problem, we propose a novel neural architecture that\ncombines residual connections with prototype encoding of a partially specified\npose to create a new complete pose from the learned latent space. We show that\nour architecture outperforms a baseline based on Transformer, both in terms of\naccuracy and computational efficiency. Additionally, we develop a user\ninterface to integrate our neural model in Unity, a real-time 3D development\nplatform. Furthermore, we introduce two new datasets representing the static\nhuman pose modeling problem, based on high-quality human motion capture data,\nwhich will be released publicly along with model code.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Oreshkin_B/0/1/0/all/0/1\">Boris N. Oreshkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bocquelet_F/0/1/0/all/0/1\">Florent Bocquelet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harvey_F/0/1/0/all/0/1\">F&#xe9;lix G. Harvey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raitt_B/0/1/0/all/0/1\">Bay Raitt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laflamme_D/0/1/0/all/0/1\">Dominic Laflamme</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continual learning under domain transfer with sparse synaptic bursting. (arXiv:2108.12056v5 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2108.12056","description":"<p>Existing machines are functionally specific tools that were made for easy\nprediction and control. Tomorrow's machines may be closer to biological systems\nin their mutability, resilience, and autonomy. But first they must be capable\nof learning, and retaining, new information without repeated exposure to it.\nPast efforts to engineer such systems have sought to build or regulate\nartificial neural networks using task-specific modules with constrained\ncircumstances of application. This has not yet enabled continual learning over\nlong sequences of previously unseen data without corrupting existing knowledge:\na problem known as catastrophic forgetting. In this paper, we introduce a\nsystem that can learn sequentially over previously unseen datasets (ImageNet,\nCIFAR-100) with little forgetting over time. This is accomplished by regulating\nthe activity of weights in a convolutional neural network on the basis of\ninputs using top-down modulation generated by a second feed-forward neural\nnetwork. We find that our method learns continually under domain transfer with\nsparse bursts of activity in weights that are recycled across tasks, rather\nthan by maintaining task-specific modules. Sparse synaptic bursting is found to\nbalance enhanced and diminished activity in a way that facilitates adaptation\nto new inputs without corrupting previously acquired functions. This behavior\nemerges during a prior meta-learning phase in which regulated synapses are\nselectively disinhibited, or grown, from an initial state of uniform\nsuppression.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Beaulieu_S/0/1/0/all/0/1\">Shawn L. Beaulieu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clune_J/0/1/0/all/0/1\">Jeff Clune</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheney_N/0/1/0/all/0/1\">Nick Cheney</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Review of Computer Vision Technologies for Fish Tracking. (arXiv:2110.02551v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.02551","description":"<p>Fish tracking based on computer vision is a complex and challenging task in\nfishery production and ecological studies. Most of the applications of fish\ntracking use classic filtering algorithms, which lack in accuracy and\nefficiency. To solve this issue, deep learning methods utilized deep neural\nnetworks to extract the features, which achieve a good performance in the fish\ntracking. Some one-stage detection algorithms have gradually been adopted in\nthis area for the real-time applications. The transfer learning to fish target\nis the current development direction. At present, fish tracking technology is\nnot enough to cover actual application requirements. According to the\nliterature data collected by us, there has not been any extensive review about\nvision-based fish tracking in the community. In this paper, we introduced the\ndevelopment and application prospects of fish tracking technology in last ten\nyears. Firstly, we introduced the open source datasets of fish, and summarized\nthe preprocessing technologies of underwater images. Secondly, we analyzed the\ndetection and tracking algorithms for fish, and sorted out some transferable\nfrontier tracking model. Thirdly, we listed the actual applications, metrics\nand bottlenecks of the fish tracking such as occlusion and multi-scale.\nFinally, we give the discussion for fish tracking datasets, solutions of the\nbottlenecks, and improvements. We expect that our work can help the fish\ntracking models to achieve higher accuracy and robustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenbo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Weiran Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_M/0/1/0/all/0/1\">Meng Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"THOMAS: Trajectory Heatmap Output with learned Multi-Agent Sampling. (arXiv:2110.06607v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.06607","description":"<p>In this paper, we propose THOMAS, a joint multi-agent trajectory prediction\nframework allowing for an efficient and consistent prediction of multi-agent\nmulti-modal trajectories. We present a unified model architecture for\nsimultaneous agent future heatmap estimation, in which we leverage hierarchical\nand sparse image generation for fast and memory-efficient inference. We propose\na learnable trajectory recombination model that takes as input a set of\npredicted trajectories for each agent and outputs its consistent reordered\nrecombination. This recombination module is able to realign the initially\nindependent modalities so that they do no collide and are coherent with each\nother. We report our results on the Interaction multi-agent prediction\nchallenge and rank $1^{st}$ on the online test leaderboard.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gilles_T/0/1/0/all/0/1\">Thomas Gilles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabatini_S/0/1/0/all/0/1\">Stefano Sabatini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsishkou_D/0/1/0/all/0/1\">Dzmitry Tsishkou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stanciulescu_B/0/1/0/all/0/1\">Bogdan Stanciulescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moutarde_F/0/1/0/all/0/1\">Fabien Moutarde</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MaGNET: Uniform Sampling from Deep Generative Network Manifolds Without Retraining. (arXiv:2110.08009v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.08009","description":"<p>Deep Generative Networks (DGNs) are extensively employed in Generative\nAdversarial Networks (GANs), Variational Autoencoders (VAEs), and their\nvariants to approximate the data manifold and distribution. However, training\nsamples are often distributed in a non-uniform fashion on the manifold, due to\ncosts or convenience of collection. For example, the CelebA dataset contains a\nlarge fraction of smiling faces. These inconsistencies will be reproduced when\nsampling from the trained DGN, which is not always preferred, e.g., for\nfairness or data augmentation. In response, we develop MaGNET, a novel and\ntheoretically motivated latent space sampler for any pre-trained DGN, that\nproduces samples uniformly distributed on the learned manifold. We perform a\nrange of experiments on various datasets and DGNs, e.g., for the\nstate-of-the-art StyleGAN2 trained on FFHQ dataset, uniform sampling via MaGNET\nincreases distribution precision and recall by 4.1\\% \\&amp; 3.0\\% and decreases\ngender bias by 41.2\\%, without requiring labels or retraining. As uniform\ndistribution does not imply uniform semantic distribution, we also explore\nseparately how semantic attributes of generated samples vary under MaGNET\nsampling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Humayun_A/0/1/0/all/0/1\">Ahmed Imtiaz Humayun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balestriero_R/0/1/0/all/0/1\">Randall Balestriero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baraniuk_R/0/1/0/all/0/1\">Richard Baraniuk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PP-ShiTu: A Practical Lightweight Image Recognition System. (arXiv:2111.00775v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.00775","description":"<p>In recent years, image recognition applications have developed rapidly. A\nlarge number of studies and techniques have emerged in different fields, such\nas face recognition, pedestrian and vehicle re-identification, landmark\nretrieval, and product recognition. In this paper, we propose a practical\nlightweight image recognition system, named PP-ShiTu, consisting of the\nfollowing 3 modules, mainbody detection, feature extraction and vector search.\nWe introduce popular strategies including metric learning, deep hash, knowledge\ndistillation and model quantization to improve accuracy and inference speed.\nWith strategies above, PP-ShiTu works well in different scenarios with a set of\nmodels trained on a mixed dataset. Experiments on different datasets and\nbenchmarks show that the system is widely effective in different domains of\nimage recognition. All the above mentioned models are open-sourced and the code\nis available in the GitHub repository PaddleClas on PaddlePaddle.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_S/0/1/0/all/0/1\">Shengyu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_R/0/1/0/all/0/1\">Ruoyu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_C/0/1/0/all/0/1\">Cheng Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_B/0/1/0/all/0/1\">Bin Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_S/0/1/0/all/0/1\">Shuilong Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_T/0/1/0/all/0/1\">Tingquan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yuning Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Ying Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_X/0/1/0/all/0/1\">Xueying Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qiwen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaoguang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dianhai Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yanjun Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BEVT: BERT Pretraining of Video Transformers. (arXiv:2112.01529v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.01529","description":"<p>This paper studies the BERT pretraining of video transformers. It is a\nstraightforward but worth-studying extension given the recent success from BERT\npretraining of image transformers. We introduce BEVT which decouples video\nrepresentation learning into spatial representation learning and temporal\ndynamics learning. In particular, BEVT first performs masked image modeling on\nimage data, and then conducts masked image modeling jointly with masked video\nmodeling on video data. This design is motivated by two observations: 1)\ntransformers learned on image datasets provide decent spatial priors that can\nease the learning of video transformers, which are often times\ncomputationally-intensive if trained from scratch; 2) discriminative clues,\ni.e., spatial and temporal information, needed to make correct predictions vary\namong different videos due to large intra-class and inter-class variations. We\nconduct extensive experiments on three challenging video benchmarks where BEVT\nachieves very promising results. On Kinetics 400, for which recognition mostly\nrelies on discriminative spatial representations, BEVT achieves comparable\nresults to strong supervised baselines. On Something-Something-V2 and Diving\n48, which contain videos relying on temporal dynamics, BEVT outperforms by\nclear margins all alternative baselines and achieves state-of-the-art\nperformance with a 71.4% and 87.2% Top-1 accuracy respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dongdong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zuxuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yinpeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1\">Xiyang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Mengchen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yu-Gang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Luowei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Body-Aware 3D Shape Generative Models. (arXiv:2112.07022v3 [cs.GR] UPDATED)","link":"http://arxiv.org/abs/2112.07022","description":"<p>The shape of many objects in the built environment is dictated by their\nrelationships to the human body: how will a person interact with this object?\nExisting data-driven generative models of 3D shapes produce plausible objects\nbut do not reason about the relationship of those objects to the human body. In\nthis paper, we learn body-aware generative models of 3D shapes. Specifically,\nwe train generative models of chairs, an ubiquitous shape category, which can\nbe conditioned on a given body shape or sitting pose. The\nbody-shape-conditioned models produce chairs which will be comfortable for a\nperson with the given body shape; the pose-conditioned models produce chairs\nwhich accommodate the given sitting pose. To train these models, we define a\n\"sitting pose matching\" metric and a novel \"sitting comfort\" metric.\nCalculating these metrics requires an expensive optimization to sit the body\ninto the chair, which is too slow to be used as a loss function for training a\ngenerative model. Thus, we train neural networks to efficiently approximate\nthese metrics. We use our approach to train three body-aware generative shape\nmodels: a structured part-based generator, a point cloud generator, and an\nimplicit surface generator. In all cases, our approach produces models which\nadapt their output chair shapes to input human body specifications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Blinn_B/0/1/0/all/0/1\">Bryce Blinn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_A/0/1/0/all/0/1\">Alexander Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jones_R/0/1/0/all/0/1\">R. Kenny Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savva_M/0/1/0/all/0/1\">Manolis Savva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sridhar_S/0/1/0/all/0/1\">Srinath Sridhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ritchie_D/0/1/0/all/0/1\">Daniel Ritchie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Approach to Addressing Zero-Shot Learning Problem. (arXiv:2201.01391v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.01391","description":"<p>In recent years, self-supervised learning has had significant success in\napplications involving computer vision and natural language processing. The\ntype of pretext task is important to this boost in performance. One common\npretext task is the measure of similarity and dissimilarity between pairs of\nimages. In this scenario, the two images that make up the negative pair are\nvisibly different to humans. However, in entomology, species are nearly\nindistinguishable and thus hard to differentiate. In this study, we explored\nthe performance of a Siamese neural network using contrastive loss by learning\nto push apart embeddings of bumblebee species pair that are dissimilar, and\npull together similar embeddings. Our experimental results show a 61% F1-score\non zero-shot instances, a performance showing 11% improvement on samples of\nclasses that share intersections with the training set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Okerinde_A/0/1/0/all/0/1\">Ademola Okerinde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoggatt_S/0/1/0/all/0/1\">Sam Hoggatt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakkireddy_D/0/1/0/all/0/1\">Divya Vani Lakkireddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brubaker_N/0/1/0/all/0/1\">Nolan Brubaker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">William Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shamir_L/0/1/0/all/0/1\">Lior Shamir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spiesman_B/0/1/0/all/0/1\">Brian Spiesman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning with Less Labels in Digital Pathology via Scribble Supervision from Natural Images. (arXiv:2201.02627v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2201.02627","description":"<p>A critical challenge of training deep learning models in the Digital\nPathology (DP) domain is the high annotation cost by medical experts. One way\nto tackle this issue is via transfer learning from the natural image domain\n(NI), where the annotation cost is considerably cheaper. Cross-domain transfer\nlearning from NI to DP is shown to be successful via class labels. One\npotential weakness of relying on class labels is the lack of spatial\ninformation, which can be obtained from spatial labels such as full pixel-wise\nsegmentation labels and scribble labels. We demonstrate that scribble labels\nfrom NI domain can boost the performance of DP models on two cancer\nclassification datasets (Patch Camelyon Breast Cancer and Colorectal Cancer\ndataset). Furthermore, we show that models trained with scribble labels yield\nthe same performance boost as full pixel-wise segmentation labels despite being\nsignificantly easier and faster to collect.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Teh_E/0/1/0/all/0/1\">Eu Wern Teh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Taylor_G/0/1/0/all/0/1\">Graham W. Taylor</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"In Defense of the Unitary Scalarization for Deep Multi-Task Learning. (arXiv:2201.04122v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2201.04122","description":"<p>Recent multi-task learning research argues against unitary scalarization,\nwhere training simply minimizes the sum of the task losses. Several ad-hoc\nmulti-task optimization algorithms have instead been proposed, inspired by\nvarious hypotheses about what makes multi-task settings difficult. The majority\nof these optimizers require per-task gradients, and introduce significant\nmemory, runtime, and implementation overhead. We present a theoretical analysis\nsuggesting that many specialized multi-task optimizers can be interpreted as\nforms of regularization. Moreover, we show that, when coupled with standard\nregularization and stabilization techniques from single-task learning, unitary\nscalarization matches or improves upon the performance of complex multi-task\noptimizers in both supervised and reinforcement learning settings. We believe\nour results call for a critical reevaluation of recent research in the area.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kurin_V/0/1/0/all/0/1\">Vitaly Kurin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palma_A/0/1/0/all/0/1\">Alessandro De Palma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kostrikov_I/0/1/0/all/0/1\">Ilya Kostrikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Whiteson_S/0/1/0/all/0/1\">Shimon Whiteson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_M/0/1/0/all/0/1\">M. Pawan Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformers in Action: Weakly Supervised Action Segmentation. (arXiv:2201.05675v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.05675","description":"<p>The video action segmentation task is regularly explored under weaker forms\nof supervision, such as transcript supervision, where a list of actions is\neasier to obtain than dense frame-wise labels. In this formulation, the task\npresents various challenges for sequence modeling approaches due to the\nemphasis on action transition points, long sequence lengths, and frame\ncontextualization, making the task well-posed for transformers. Given\ndevelopments enabling transformers to scale linearly, we demonstrate through\nour architecture how they can be applied to improve action alignment accuracy\nover the equivalent RNN-based models with the attention mechanism focusing\naround salient action transition regions. Additionally, given the recent focus\non inference-time transcript selection, we propose a supplemental transcript\nembedding approach to select transcripts more quickly at inference-time.\nFurthermore, we subsequently demonstrate how this approach can also improve the\noverall segmentation performance. Finally, we evaluate our proposed methods\nacross the benchmark datasets to better understand the applicability of\ntransformers and the importance of transcript selection on this video-driven\nweakly-supervised task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ridley_J/0/1/0/all/0/1\">John Ridley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coskun_H/0/1/0/all/0/1\">Huseyin Coskun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_D/0/1/0/all/0/1\">David Joseph Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1\">Nassir Navab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tombari_F/0/1/0/all/0/1\">Federico Tombari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weighting and Pruning based Ensemble Deep Random Vector Functional Link Network for Tabular Data Classification. (arXiv:2201.05809v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2201.05809","description":"<p>In this paper, we first introduce batch normalization to the edRVFL network.\nThis re-normalization method can help the network avoid divergence of the\nhidden features. Then we propose novel variants of Ensemble Deep Random Vector\nFunctional Link (edRVFL). Weighted edRVFL (WedRVFL) uses weighting methods to\ngive training samples different weights in different layers according to how\nthe samples were classified confidently in the previous layer thereby\nincreasing the ensemble's diversity and accuracy. Furthermore, a pruning-based\nedRVFL (PedRVFL) has also been proposed. We prune some inferior neurons based\non their importance for classification before generating the next hidden layer.\nThrough this method, we ensure that the randomly generated inferior features\nwill not propagate to deeper layers. Subsequently, the combination of weighting\nand pruning, called Weighting and Pruning based Ensemble Deep Random Vector\nFunctional Link Network (WPedRVFL), is proposed. We compare their performances\nwith other state-of-the-art deep feedforward neural networks (FNNs) on 24\ntabular UCI classification datasets. The experimental results illustrate the\nsuperior performance of our proposed methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_Q/0/1/0/all/0/1\">Qiushi Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suganthan_P/0/1/0/all/0/1\">Ponnuthurai Nagaratnam Suganthan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katuwal_R/0/1/0/all/0/1\">Rakesh Katuwal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Two-Stage is Enough: A Concise Deep Unfolding Reconstruction Network for Flexible Video Compressive Sensing. (arXiv:2201.05810v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2201.05810","description":"<p>We consider the reconstruction problem of video compressive sensing (VCS)\nunder the deep unfolding/rolling structure. Yet, we aim to build a flexible and\nconcise model using minimum stages. Different from existing deep unfolding\nnetworks used for inverse problems, where more stages are used for higher\nperformance but without flexibility to different masks and scales, hereby we\nshow that a 2-stage deep unfolding network can lead to the state-of-the-art\n(SOTA) results (with a 1.7dB gain in PSNR over the single stage model, RevSCI)\nin VCS. The proposed method possesses the properties of adaptation to new masks\nand ready to scale to large data without any additional training thanks to the\nadvantages of deep unfolding. Furthermore, we extend the proposed model for\ncolor VCS to perform joint reconstruction and demosaicing. Experimental results\ndemonstrate that our 2-stage model has also achieved SOTA on color VCS\nreconstruction, leading to a &gt;2.3dB gain in PSNR over the previous SOTA\nalgorithm based on plug-and-play framework, meanwhile speeds up the\nreconstruction by &gt;17 times. In addition, we have found that our network is\nalso flexible to the mask modulation and scale size for color VCS\nreconstruction so that a single trained network can be applied to different\nhardware systems. The code and models will be released to the public.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zheng_S/0/1/0/all/0/1\">Siming Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_X/0/1/0/all/0/1\">Xiaoyu Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yuan_X/0/1/0/all/0/1\">Xin Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"YOLO -- You only look 10647 times. (arXiv:2201.06159v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.06159","description":"<p>With this work we are explaining the \"You Only Look Once\" (YOLO) single-stage\nobject detection approach as a parallel classification of 10647 fixed region\nproposals. We support this view by showing that each of YOLOs output pixel is\nattentive to a specific sub-region of previous layers, comparable to a local\nregion proposal. This understanding reduces the conceptual gap between\nYOLO-like single-stage object detection models, RCNN-like two-stage region\nproposal based models, and ResNet-like image classification models. In\naddition, we created interactive exploration tools for a better visual\nunderstanding of the YOLO information processing streams:\nhttps://limchr.github.io/yolo_visualization\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Limberg_C/0/1/0/all/0/1\">Christian Limberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melnik_A/0/1/0/all/0/1\">Andrew Melnik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harter_A/0/1/0/all/0/1\">Augustin Harter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ritter_H/0/1/0/all/0/1\">Helge Ritter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised Video Representation Learning with Cascade Positive Retrieval. (arXiv:2201.07989v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.07989","description":"<p>Self-supervised video representation learning has been shown to effectively\nimprove downstream tasks such as video retrieval and action recognition. In\nthis paper, we present the Cascade Positive Retrieval (CPR) that successively\nmines positive examples w.r.t. the query for contrastive learning in a cascade\nof stages. Specifically, CPR exploits multiple views of a query example in\ndifferent modalities, where an alternative view may help find another positive\nexample dissimilar in the query view. We explore the effects of possible CPR\nconfigurations in ablations including the number of mining stages, the top\nsimilar example selection ratio in each stage, and progressive training with an\nincremental number of the final Top-k selection. The overall mining quality is\nmeasured to reflect the recall across training set classes. CPR reaches a\nmedian class mining recall of 83.3%, outperforming previous work by 5.5%.\nImplementation-wise, CPR is complementary to pretext tasks and can be easily\napplied to previous work. In the evaluation of pretraining on UCF101, CPR\nconsistently improves existing work and even achieves state-of-the-art R@1 of\n56.7% and 24.4% in video retrieval as well as 83.8% and 54.8% in action\nrecognition on UCF101 and HMDB51. For transfer from large video dataset\nKinetics400 to UCF101 and HDMB, CPR benefits existing work, showing competitive\nTop-1 accuracies of 85.1% and 57.4% despite pretraining at a lower resolution\nand frame sampling rate. The code will be released soon for reproducing the\nresults. The code is available at https://github.com/necla-ml/CPR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Cheng-En Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_F/0/1/0/all/0/1\">Farley Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yu Hen Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadav_A/0/1/0/all/0/1\">Asim Kadav</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TerViT: An Efficient Ternary Vision Transformer. (arXiv:2201.08050v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.08050","description":"<p>Vision transformers (ViTs) have demonstrated great potential in various\nvisual tasks, but suffer from expensive computational and memory cost problems\nwhen deployed on resource-constrained devices. In this paper, we introduce a\nternary vision transformer (TerViT) to ternarize the weights in ViTs, which are\nchallenged by the large loss surface gap between real-valued and ternary\nparameters. To address the issue, we introduce a progressive training scheme by\nfirst training 8-bit transformers and then TerViT, and achieve a better\noptimization than conventional methods. Furthermore, we introduce channel-wise\nternarization, by partitioning each matrix to different channels, each of which\nis with an unique distribution and ternarization interval. We apply our methods\nto popular DeiT and Swin backbones, and extensive results show that we can\nachieve competitive performance. For example, TerViT can quantize Swin-S to\n13.1MB model size while achieving above 79% Top-1 accuracy on ImageNet dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Sheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanjing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1\">Teli Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_B/0/1/0/all/0/1\">Bohan Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Baochang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Peng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_J/0/1/0/all/0/1\">Jinhu Lv</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DIVA-DAF: A Deep Learning Framework for Historical Document Image Analysis. (arXiv:2201.08295v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.08295","description":"<p>In this paper, we introduce a new deep learning framework called DIVA-DAF. We\nhave developed this framework to support our research on historical document\nimage analysis tasks and to develop techniques to reduce the need for\nmanually-labeled ground truth. We want to apply self-supervised learning\ntechniques and use different kinds of training data. Our new framework aids us\nin performing rapid prototyping and reproducible experiments. We present a\nfirst semantic segmentation experiment on DIVA-HisDB using our framework,\nachieving state-of-the-art results. The DIVA-DAF framework is open-source, and\nwe encourage other research groups to use it for their experiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vogtlin_L/0/1/0/all/0/1\">Lars V&#xf6;gtlin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maergner_P/0/1/0/all/0/1\">Paul Maergner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ingold_R/0/1/0/all/0/1\">Rolf Ingold</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stitch it in Time: GAN-Based Facial Editing of Real Videos. (arXiv:2201.08361v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.08361","description":"<p>The ability of Generative Adversarial Networks to encode rich semantics\nwithin their latent space has been widely adopted for facial image editing.\nHowever, replicating their success with videos has proven challenging. Sets of\nhigh-quality facial videos are lacking, and working with videos introduces a\nfundamental barrier to overcome - temporal coherency. We propose that this\nbarrier is largely artificial. The source video is already temporally coherent,\nand deviations from this state arise in part due to careless treatment of\nindividual components in the editing pipeline. We leverage the natural\nalignment of StyleGAN and the tendency of neural networks to learn low\nfrequency functions, and demonstrate that they provide a strongly consistent\nprior. We draw on these insights and propose a framework for semantic editing\nof faces in videos, demonstrating significant improvements over the current\nstate-of-the-art. Our method produces meaningful face manipulations, maintains\na higher degree of temporal consistency, and can be applied to challenging,\nhigh quality, talking head videos which current methods struggle with.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tzaban_R/0/1/0/all/0/1\">Rotem Tzaban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mokady_R/0/1/0/all/0/1\">Ron Mokady</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gal_R/0/1/0/all/0/1\">Rinon Gal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bermano_A/0/1/0/all/0/1\">Amit H. Bermano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_Or_D/0/1/0/all/0/1\">Daniel Cohen-Or</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-01-23T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}}]}]}