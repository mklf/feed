{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-03-01T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Learning English with Peppa Pig. (arXiv:2202.12917v1 [cs.CL])","link":"http://arxiv.org/abs/2202.12917","description":"<p>Attempts to computationally simulate the acquisition of spoken language via\ngrounding in perception have a long tradition but have gained momentum in the\npast few years. Current neural approaches exploit associations between the\nspoken and visual modality and learn to represent speech and visual data in a\njoint vector space. A major unresolved issue from the point of ecological\nvalidity is the training data, typically consisting of images or videos paired\nwith spoken descriptions of what is depicted. Such a setup guarantees an\nunrealistically strong correlation between speech and the visual world. In the\nreal world the coupling between the linguistic and the visual is loose, and\noften contains confounds in the form of correlations with non-semantic aspects\nof the speech signal. The current study is a first step towards simulating a\nnaturalistic grounding scenario by using a dataset based on the children's\ncartoon Peppa Pig. We train a simple bi-modal architecture on the portion of\nthe data consisting of naturalistic dialog between characters, and evaluate on\nsegments containing descriptive narrations. Despite the weak and confounded\nsignal in this training data our model succeeds at learning aspects of the\nvisual semantics of spoken language.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nikolaus_M/0/1/0/all/0/1\">Mitja Nikolaus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alishahi_A/0/1/0/all/0/1\">Afra Alishahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chrupala_G/0/1/0/all/0/1\">Grzegorz Chrupa&#x142;a</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ASSIST: Towards Label Noise-Robust Dialogue State Tracking. (arXiv:2202.13024v1 [cs.CL])","link":"http://arxiv.org/abs/2202.13024","description":"<p>The MultiWOZ 2.0 dataset has greatly boosted the research on dialogue state\ntracking (DST). However, substantial noise has been discovered in its state\nannotations. Such noise brings about huge challenges for training DST models\nrobustly. Although several refined versions, including MultiWOZ 2.1-2.4, have\nbeen published recently, there are still lots of noisy labels, especially in\nthe training set. Besides, it is costly to rectify all the problematic\nannotations. In this paper, instead of improving the annotation quality\nfurther, we propose a general framework, named ASSIST (lAbel noiSe-robuSt\ndIalogue State Tracking), to train DST models robustly from noisy labels.\nASSIST first generates pseudo labels for each sample in the training set by\nusing an auxiliary model trained on a small clean dataset, then puts the\ngenerated pseudo labels and vanilla noisy labels together to train the primary\nmodel. We show the validity of ASSIST theoretically. Experimental results also\ndemonstrate that ASSIST improves the joint goal accuracy of DST by up to\n$28.16\\%$ on the initial version MultiWOZ 2.0 and $8.41\\%$ on the latest\nversion MultiWOZ 2.4, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_F/0/1/0/all/0/1\">Fanghua Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yue Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yilmaz_E/0/1/0/all/0/1\">Emine Yilmaz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AugESC: Large-scale Data Augmentation for Emotional Support Conversation with Pre-trained Language Models. (arXiv:2202.13047v1 [cs.CL])","link":"http://arxiv.org/abs/2202.13047","description":"<p>Crowd-sourcing is commonly adopted for dialog data collection. However, it is\nhighly costly and time-consuming, and the collected data is limited in scale\nand topic coverage. In this paper, aiming to generate emotional support\nconversations, we propose exploiting large-scale pre-trained language models\nfor data augmentation, and provide key findings in our pilot exploration. Our\nadopted approach leverages the 6B-parameter GPT-J model and utilizes publicly\navailable dialog posts to trigger conversations on various topics. Then we\nconstruct AugESC, a machine-augmented dataset for emotional support\nconversation. It is two orders of magnitude larger than the original ESConv\ndataset in scale, covers more diverse topics, and is shown to be of high\nquality by human evaluation. Lastly, we demonstrate with interactive evaluation\nthat AugESC can further enhance dialog models tuned on ESConv to handle various\nconversation topics and to provide significantly more effective emotional\nsupport.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Chujie Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabour_S/0/1/0/all/0/1\">Sahand Sabour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Jiaxin Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated Identification of Toxic Code Reviews: How Far Can We Go?. (arXiv:2202.13056v1 [cs.SE])","link":"http://arxiv.org/abs/2202.13056","description":"<p>Toxic conversations during software development interactions may have serious\nrepercussions on a Free and Open Source Software (FOSS) development project.\nFor example, victims of toxic conversations may become afraid to express\nthemselves, therefore get demotivated, and may eventually leave the project.\nAutomated filtering of toxic conversations may help a FOSS community to\nmaintain healthy interactions among its members. However, off-the-shelf\ntoxicity detectors perform poorly on Software Engineering (SE) dataset, such as\none curated from code review comments. To encounter this challenge, we present\nToxiCR, a supervised learning-based toxicity identification tool for code\nreview interactions. ToxiCR includes a choice to select one of the ten\nsupervised learning algorithms, an option to select text vectorization\ntechniques, five mandatory and three optional SE domain specific processing\nsteps, and a large scale labeled dataset of 19,571 code review comments. With\nour rigorous evaluation of the models with various combinations of\npreprocessing steps and vectorization techniques, we have identified the best\ncombination for our dataset that boosts 95.8% accuracy and 88.9% F1 score.\nToxiCR significantly outperforms existing toxicity detectors on our dataset. We\nhave released our dataset, pretrained models, evaluation results, and source\ncode publicly available at: https://github.com/WSU-SEAL/ToxiCR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sarker_J/0/1/0/all/0/1\">Jaydeb Sarker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turzo_A/0/1/0/all/0/1\">Asif Kamal Turzo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_M/0/1/0/all/0/1\">Ming Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bosu_A/0/1/0/all/0/1\">Amiangshu Bosu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bi-directional Joint Neural Networks for Intent Classification and Slot Filling. (arXiv:2202.13079v1 [cs.CL])","link":"http://arxiv.org/abs/2202.13079","description":"<p>Intent classification and slot filling are two critical tasks for natural\nlanguage understanding. Traditionally the two tasks proceeded independently.\nHowever, more recently joint models for intent classification and slot filling\nhave achieved state-of-the-art performance, and have proved that there exists a\nstrong relationship between the two tasks. In this paper, we propose a\nbi-directional joint model for intent classification and slot filling, which\nincludes a multi-stage hierarchical process via BERT and bi-directional joint\nnatural language understanding mechanisms, including intent2slot and\nslot2intent, to obtain mutual performance enhancement between intent\nclassification and slot filling. The evaluations show that our model achieves\nstate-of-the-art results on intent classification accuracy, slot filling F1,\nand significantly improves sentence-level semantic frame accuracy when applied\nto publicly available benchmark datasets, ATIS (88.6%) and SNIPS (92.8%).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Soyeon Caren Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_S/0/1/0/all/0/1\">Siqu Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Huichun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weld_H/0/1/0/all/0/1\">Henry Weld</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poon_J/0/1/0/all/0/1\">Josiah Poon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Level Contrastive Learning for Cross-Lingual Alignment. (arXiv:2202.13083v1 [cs.CL])","link":"http://arxiv.org/abs/2202.13083","description":"<p>Cross-language pre-trained models such as multilingual BERT (mBERT) have\nachieved significant performance in various cross-lingual downstream NLP tasks.\nThis paper proposes a multi-level contrastive learning (ML-CTL) framework to\nfurther improve the cross-lingual ability of pre-trained models. The proposed\nmethod uses translated parallel data to encourage the model to generate similar\nsemantic embeddings for different languages. However, unlike the sentence-level\nalignment used in most previous studies, in this paper, we explicitly integrate\nthe word-level information of each pair of parallel sentences into contrastive\nlearning. Moreover, cross-zero noise contrastive estimation (CZ-NCE) loss is\nproposed to alleviate the impact of the floating-point error in the training\nprocess with a small batch size. The proposed method significantly improves the\ncross-lingual transfer ability of our basic model (mBERT) and outperforms on\nmultiple zero-shot cross-lingual downstream tasks compared to the same-size\nmodels in the Xtreme benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Beiduo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1\">Wu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_B/0/1/0/all/0/1\">Bin Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Quan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongchao Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring the Impact of Negative Samples of Contrastive Learning: A Case Study of Sentence Embeddin. (arXiv:2202.13093v1 [cs.CL])","link":"http://arxiv.org/abs/2202.13093","description":"<p>Contrastive learning is emerging as a powerful technique for extracting\nknowledge from unlabeled data. This technique requires a balanced mixture of\ntwo ingredients: positive (similar) and negative (dissimilar) samples. This is\ntypically achieved by maintaining a queue of negative samples during training.\nPrior works in the area typically uses a fixed-length negative sample queue,\nbut how the negative sample size affects the model performance remains unclear.\nThe opaque impact of the number of negative samples on performance when\nemploying contrastive learning aroused our in-depth exploration. This paper\npresents a momentum contrastive learning model with negative sample queue for\nsentence embedding, namely MoCoSE. We add the prediction layer to the online\nbranch to make the model asymmetric and together with EMA update mechanism of\nthe target branch to prevent model from collapsing. We define a maximum\ntraceable distance metric, through which we learn to what extent the text\ncontrastive learning benefits from the historical information of negative\nsamples. Our experiments find that the best results are obtained when the\nmaximum traceable distance is at a certain range, demonstrating that there is\nan optimal range of historical information for a negative sample queue. We\nevaluate the proposed unsupervised MoCoSE on the semantic text similarity (STS)\ntask and obtain an average Spearman's correlation of $77.27\\%$. Source code is\navailable at https://github.com/xbdxwyh/mocose\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_R/0/1/0/all/0/1\">Rui Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yihao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yuxin Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Ling Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Jie Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Jie Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zheng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Supervision: Enabling Generalization over Output Spaces. (arXiv:2202.13100v1 [cs.LG])","link":"http://arxiv.org/abs/2202.13100","description":"<p>In this paper, we propose Semantic Supervision (SemSup) - a unified paradigm\nfor training classifiers that generalize over output spaces. In contrast to\nstandard classification, which treats classes as discrete symbols, SemSup\nrepresents them as dense vector features obtained from descriptions of classes\n(e.g., \"The cat is a small carnivorous mammal\"). This allows the output space\nto be unbounded (in the space of descriptions) and enables models to generalize\nboth over unseen inputs and unseen outputs (e.g. \"The aardvark is a nocturnal\nburrowing mammal with long ears\"). Specifically, SemSup enables four types of\ngeneralization, to -- (1) unseen class descriptions, (2) unseen classes, (3)\nunseen super-classes, and (4) unseen tasks. Through experiments on four\nclassification datasets across two variants (multi-class and multi-label), two\ninput modalities (text and images), and two output description modalities (text\nand JSON), we show that our SemSup models significantly outperform standard\nsupervised models and existing models that leverage word embeddings over class\nnames. For instance, our model outperforms baselines by 40% and 20% precision\npoints on unseen descriptions and classes, respectively, on a news\ncategorization dataset (RCV1). SemSup can serve as a pathway for scaling neural\nmodels to large unbounded output spaces and enabling better generalization and\nmodel reuse for unseen tasks and domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hanjie_A/0/1/0/all/0/1\">Austin W. Hanjie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deshpande_A/0/1/0/all/0/1\">Ameet Deshpande</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narasimhan_K/0/1/0/all/0/1\">Karthik Narasimhan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"QuoteR: A Benchmark of Quote Recommendation for Writing. (arXiv:2202.13145v1 [cs.CL])","link":"http://arxiv.org/abs/2202.13145","description":"<p>It is very common to use quotations (quotes) to make our writings more\nelegant or convincing. To help people find appropriate quotes more efficiently,\nthe task of quote recommendation is presented, aiming to recommend quotes that\nfit the current context of writing. There have been various quote\nrecommendation approaches, but they are evaluated on different unpublished\ndatasets. To facilitate the research on this task, we build a large and fully\nopen quote recommendation dataset called QuoteR, which comprises three parts\nincluding English, standard Chinese and classical Chinese. Any part of it is\nlarger than previous unpublished counterparts. We conduct an extensive\nevaluation of existing quote recommendation methods on QuoteR. Furthermore, we\npropose a new quote recommendation model that significantly outperforms\nprevious methods on all three parts of QuoteR. All the code and data of this\npaper are available at https://github.com/thunlp/QuoteR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qi_F/0/1/0/all/0/1\">Fanchao Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yanhui Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1\">Jing Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zhili Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COMPASS: a Creative Support System that Alerts Novelists to the Unnoticed Missing Contents. (arXiv:2202.13151v1 [cs.CL])","link":"http://arxiv.org/abs/2202.13151","description":"<p>When humans write, they may unintentionally omit some information.\nComplementing the omitted information using a computer is helpful in providing\nwriting support. Recently, in the field of story understanding and generation,\nstory completion (SC) was proposed to generate the missing parts of an\nincomplete story. Although its applicability is limited because it requires\nthat the user have prior knowledge of the missing part of a story, missing\nposition prediction (MPP) can be used to compensate for this problem. MPP aims\nto predict the position of the missing part, but the prerequisite knowledge\nthat \"one sentence is missing\" is still required. In this study, we propose\nVariable Number MPP (VN-MPP), a new MPP task that removes this restriction;\nthat is, the task to predict multiple missing sentences or to judge whether\nthere are no missing sentences in the first place. We also propose two methods\nfor this new MPP task. Furthermore, based on the novel task and methods, we\ndeveloped a creative writing support system, COMPASS. The results of a user\nexperiment involving professional creators who write texts in Japanese confirm\nthe efficacy and utility of the developed system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mori_Y/0/1/0/all/0/1\">Yusuke Mori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamane_H/0/1/0/all/0/1\">Hiroaki Yamane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shimizu_R/0/1/0/all/0/1\">Ryohei Shimizu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukuta_Y/0/1/0/all/0/1\">Yusuke Mukuta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harada_T/0/1/0/all/0/1\">Tatsuya Harada</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Integrating Text Inputs For Training and Adapting RNN Transducer ASR Models. (arXiv:2202.13155v1 [cs.CL])","link":"http://arxiv.org/abs/2202.13155","description":"<p>Compared to hybrid automatic speech recognition (ASR) systems that use a\nmodular architecture in which each component can be independently adapted to a\nnew domain, recent end-to-end (E2E) ASR system are harder to customize due to\ntheir all-neural monolithic construction. In this paper, we propose a novel\ntext representation and training framework for E2E ASR models. With this\napproach, we show that a trained RNN Transducer (RNN-T) model's internal LM\ncomponent can be effectively adapted with text-only data. An RNN-T model\ntrained using both speech and text inputs improves over a baseline model\ntrained on just speech with close to 13% word error rate (WER) reduction on the\nSwitchboard and CallHome test sets of the NIST Hub5 2000 evaluation. The\nusefulness of the proposed approach is further demonstrated by customizing this\ngeneral purpose RNN-T model to three separate datasets. We observe 20-45%\nrelative word error rate (WER) reduction in these settings with this novel LM\nstyle customization technique using only unpaired text data from the new\ndomains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thomas_S/0/1/0/all/0/1\">Samuel Thomas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kingsbury_B/0/1/0/all/0/1\">Brian Kingsbury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saon_G/0/1/0/all/0/1\">George Saon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuo_H/0/1/0/all/0/1\">Hong-Kwang J. Kuo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Systematic Evaluation of Large Language Models of Code. (arXiv:2202.13169v1 [cs.PL])","link":"http://arxiv.org/abs/2202.13169","description":"<p>Large language models (LMs) of code have recently shown tremendous promise in\ncompleting code and synthesizing code from natural language descriptions.\nHowever, the current state-of-the-art code LMs (e.g., Codex (Chen et al.,\n2021)) are not publicly available, leaving many questions about their model and\ndata design decisions. We aim to fill in some of these blanks through a\nsystematic evaluation of the largest existing models: Codex, GPT-J, GPT-Neo,\nGPT-NeoX-20B, and CodeParrot, across various programming languages. Although\nCodex itself is not open-source, we find that existing open-source models do\nachieve close results in some programming languages, although targeted mainly\nfor natural language modeling. We further identify an important missing piece\nin the form of a large open-source model trained exclusively on a multi-lingual\ncorpus of code. We release a new model, PolyCoder, with 2.7B parameters based\non the GPT-2 architecture, which was trained on 249GB of code across 12\nprogramming languages on a single machine. In the C programming language,\nPolyCoder outperforms all models including Codex. Our trained models are\nopen-source and publicly available at https://github.com/VHellendoorn/Code-LMs,\nwhich enables future research and application in this area.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1\">Frank F. Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alon_U/0/1/0/all/0/1\">Uri Alon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hellendoorn_V/0/1/0/all/0/1\">Vincent J. Hellendoorn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BioADAPT-MRC: Adversarial Learning-based Domain Adaptation Improves Biomedical Machine Reading Comprehension Task. (arXiv:2202.13174v1 [cs.CL])","link":"http://arxiv.org/abs/2202.13174","description":"<p>Motivation: Biomedical machine reading comprehension (biomedical-MRC) aims to\ncomprehend complex biomedical narratives and assist healthcare professionals in\nretrieving information from them. The high performance of modern neural\nnetwork-based MRC systems depends on high-quality, large-scale, human-annotated\ntraining datasets. In the biomedical domain, a crucial challenge in creating\nsuch datasets is the requirement for domain knowledge, inducing the scarcity of\nlabeled data and the need for transfer learning from the labeled\ngeneral-purpose (source) domain to the biomedical (target) domain. However,\nthere is a discrepancy in marginal distributions between the general-purpose\nand biomedical domains due to the variances in topics. Therefore,\ndirect-transferring of learned representations from a model trained on a\ngeneral-purpose domain to the biomedical domain can hurt the model's\nperformance.\n</p>\n<p>Results: We present an adversarial learning-based domain adaptation framework\nfor the biomedical machine reading comprehension task (BioADAPT-MRC), a neural\nnetwork-based method to address the discrepancies in the marginal distributions\nbetween the general and biomedical domain datasets. BioADAPT-MRC relaxes the\nneed for generating pseudo labels for training a well-performing biomedical-MRC\nmodel. We extensively evaluate the performance of BioADAPT-MRC by comparing it\nwith the best existing methods on three widely used benchmark biomedical-MRC\ndatasets -- BioASQ-7b, BioASQ-8b, and BioASQ-9b. Our results suggest that\nwithout using any synthetic or human-annotated data from the biomedical domain,\nBioADAPT-MRC can achieve state-of-the-art performance on these datasets.\n</p>\n<p>Availability: BioADAPT-MRC is freely available as an open-source project\nat\\\\https://github.com/mmahbub/BioADAPT-MRC\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mahbub_M/0/1/0/all/0/1\">Maria Mahbub</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_S/0/1/0/all/0/1\">Sudarshan Srinivasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Begoli_E/0/1/0/all/0/1\">Edmon Begoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peterson_G/0/1/0/all/0/1\">Gregory D Peterson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Generative Model for Relation Extraction and Classification. (arXiv:2202.13229v1 [cs.CL])","link":"http://arxiv.org/abs/2202.13229","description":"<p>Relation extraction (RE) is an important information extraction task which\nprovides essential information to many NLP applications such as knowledge base\npopulation and question answering. In this paper, we present a novel generative\nmodel for relation extraction and classification (which we call GREC), where RE\nis modeled as a sequence-to-sequence generation task. We explore various\nencoding representations for the source and target sequences, and design\neffective schemes that enable GREC to achieve state-of-the-art performance on\nthree benchmark RE datasets. In addition, we introduce negative sampling and\ndecoding scaling techniques which provide a flexible tool to tune the precision\nand recall performance of the model. Our approach can be extended to extract\nall relation triples from a sentence in one pass. Although the one-pass\napproach incurs certain performance loss, it is much more computationally\nefficient.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ni_J/0/1/0/all/0/1\">Jian Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rossiello_G/0/1/0/all/0/1\">Gaetano Rossiello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gliozzo_A/0/1/0/all/0/1\">Alfio Gliozzo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Florian_R/0/1/0/all/0/1\">Radu Florian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Controllable Natural Language Generation with Contrastive Prefixes. (arXiv:2202.13257v1 [cs.CL])","link":"http://arxiv.org/abs/2202.13257","description":"<p>To guide the generation of large pretrained language models (LM), previous\nwork has focused on directly fine-tuning the language model or utilizing an\nattribute discriminator. In this work, we propose a novel lightweight framework\nfor controllable GPT2 generation, which utilizes a set of small\nattribute-specific vectors, called prefixes, to steer natural language\ngeneration. Different from prefix-tuning, where each prefix is trained\nindependently, we take the relationship among prefixes into consideration and\ntrain multiple prefixes simultaneously. We propose a novel supervised method\nand also an unsupervised method to train the prefixes for single-aspect control\nwhile the combination of these two methods can achieve multi-aspect control.\nExperimental results on both single-aspect and multi-aspect control show that\nour methods can guide generation towards the desired attributes while keeping\nhigh linguistic quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qian_J/0/1/0/all/0/1\">Jing Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1\">Li Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yelong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OCR Improves Machine Translation for Low-Resource Languages. (arXiv:2202.13274v1 [cs.CL])","link":"http://arxiv.org/abs/2202.13274","description":"<p>We aim to investigate the performance of current OCR systems on low resource\nlanguages and low resource scripts. We introduce and make publicly available a\nnovel benchmark, \\textsc{OCR4MT}, consisting of real and synthetic data,\nenriched with noise, for 60 low-resource languages in low resource scripts. We\nevaluate state-of-the-art OCR systems on our benchmark and analyse most common\nerrors. We show that OCR monolingual data is a valuable resource that can\nincrease performance of Machine Translation models, when used in\nbacktranslation. We then perform an ablation study to investigate how OCR\nerrors impact Machine Translation performance and determine what is the minimum\nlevel of OCR quality needed for the monolingual data to be useful for Machine\nTranslation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ignat_O/0/1/0/all/0/1\">Oana Ignat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maillard_J/0/1/0/all/0/1\">Jean Maillard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhary_V/0/1/0/all/0/1\">Vishrav Chaudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guzman_F/0/1/0/all/0/1\">Francisco Guzm&#xe1;n</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning the Beauty in Songs: Neural Singing Voice Beautifier. (arXiv:2202.13277v1 [eess.AS])","link":"http://arxiv.org/abs/2202.13277","description":"<p>We are interested in a novel task, singing voice beautifying (SVB). Given the\nsinging voice of an amateur singer, SVB aims to improve the intonation and\nvocal tone of the voice, while keeping the content and vocal timbre. Current\nautomatic pitch correction techniques are immature, and most of them are\nrestricted to intonation but ignore the overall aesthetic quality. Hence, we\nintroduce Neural Singing Voice Beautifier (NSVB), the first generative model to\nsolve the SVB task, which adopts a conditional variational autoencoder as the\nbackbone and learns the latent representations of vocal tone. In NSVB, we\npropose a novel time-warping approach for pitch correction: Shape-Aware Dynamic\nTime Warping (SADTW), which ameliorates the robustness of existing time-warping\napproaches, to synchronize the amateur recording with the template pitch curve.\nFurthermore, we propose a latent-mapping algorithm in the latent space to\nconvert the amateur vocal tone to the professional one. To achieve this, we\nalso propose a new dataset containing parallel singing recordings of both\namateur and professional versions. Extensive experiments on both Chinese and\nEnglish songs demonstrate the effectiveness of our methods in terms of both\nobjective and subjective metrics. Audio samples are available\nat~\\url{https://neuralsvb.github.io}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jinglin Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_C/0/1/0/all/0/1\">Chengxi Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ren_Y/0/1/0/all/0/1\">Yi Ren</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhiying Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhou Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Subgraph Retrieval Enhanced Model for Multi-hop Knowledge Base Question Answering. (arXiv:2202.13296v1 [cs.CL])","link":"http://arxiv.org/abs/2202.13296","description":"<p>Recent works on knowledge base question answering (KBQA) retrieve subgraphs\nfor easier reasoning. A desired subgraph is crucial as a small one may exclude\nthe answer but a large one might introduce more noises. However, the existing\nretrieval is either heuristic or interwoven with the reasoning, causing\nreasoning on the partial subgraphs, which increases the reasoning bias when the\nintermediate supervision is missing. This paper proposes a trainable subgraph\nretriever (SR) decoupled from the subsequent reasoning process, which enables a\nplug-and-play framework to enhance any subgraph-oriented KBQA model. Extensive\nexperiments demonstrate SR achieves significantly better retrieval and QA\nperformance than existing retrieval methods. Via weakly supervised pre-training\nas well as the end-to-end fine-tuning, SRl achieves new state-of-the-art\nperformance when combined with NSM, a subgraph-oriented reasoner, for\nembedding-based KBQA methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaokang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jifan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jian Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jie Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Cuiping Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hong Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HiCLRE: A Hierarchical Contrastive Learning Framework for Distantly Supervised Relation Extraction. (arXiv:2202.13352v1 [cs.CL])","link":"http://arxiv.org/abs/2202.13352","description":"<p>Distant supervision assumes that any sentence containing the same entity\npairs reflects identical relationships. Previous works of distantly supervised\nrelation extraction (DSRE) task generally focus on sentence-level or bag-level\nde-noising techniques independently, neglecting the explicit interaction with\ncross levels. In this paper, we propose a hierarchical contrastive learning\nFramework for Distantly Supervised relation extraction (HiCLRE) to reduce noisy\nsentences, which integrate the global structural information and local\nfine-grained interaction. Specifically, we propose a three-level hierarchical\nlearning framework to interact with cross levels, generating the de-noising\ncontext-aware representations via adapting the existing multi-head\nself-attention, named Multi-Granularity Recontextualization. Meanwhile, pseudo\npositive samples are also provided in the specific level for contrastive\nlearning via a dynamic gradient-based data augmentation strategy, named Dynamic\nGradient Adversarial Perturbation. Experiments demonstrate that HiCLRE\nsignificantly outperforms strong baselines in various mainstream DSRE datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dongyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Taolin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_N/0/1/0/all/0/1\">Nan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaofeng He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Variational Autoencoder with Disentanglement Priors for Low-Resource Task-Specific Natural Language Generation. (arXiv:2202.13363v1 [cs.CL])","link":"http://arxiv.org/abs/2202.13363","description":"<p>In this paper, we propose a variational autoencoder with disentanglement\npriors, VAE-DPRIOR, for conditional natural language generation with none or a\nhandful of task-specific labeled examples. In order to improve compositional\ngeneralization, our model performs disentangled representation learning by\nintroducing a prior for the latent content space and another prior for the\nlatent label space. We show both empirically and theoretically that the\nconditional priors can already disentangle representations even without\nspecific regularizations as in the prior work. We can also sample diverse\ncontent representations from the content space without accessing data of the\nseen tasks, and fuse them with the representations of novel tasks for\ngenerating diverse texts in the low-resource settings. Our extensive\nexperiments demonstrate the superior performance of our model over competitive\nbaselines in terms of i) data augmentation in continuous zero/few-shot\nlearning, and ii) text style transfer in both zero/few-shot settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhuang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1\">Lizhen Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qiongkai Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tongtong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_T/0/1/0/all/0/1\">Tianyang Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haffari_G/0/1/0/all/0/1\">Gholamreza Haffari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Simple but Effective Pluggable Entity Lookup Table for Pre-trained Language Models. (arXiv:2202.13392v1 [cs.CL])","link":"http://arxiv.org/abs/2202.13392","description":"<p>Pre-trained language models (PLMs) cannot well recall rich factual knowledge\nof entities exhibited in large-scale corpora, especially those rare entities.\nIn this paper, we propose to build a simple but effective Pluggable Entity\nLookup Table (PELT) on demand by aggregating the entity's output\nrepresentations of multiple occurrences in the corpora. PELT can be compatibly\nplugged as inputs to infuse supplemental entity knowledge into PLMs. Compared\nto previous knowledge-enhanced PLMs, PELT only requires 0.2%-5% pre-computation\nwith capability of acquiring knowledge from out-of-domain corpora for domain\nadaptation scenario. The experiments on knowledge-related tasks demonstrate\nthat our method, PELT, can flexibly and effectively transfer entity knowledge\nfrom related corpora into PLMs with different architectures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_D/0/1/0/all/0/1\">Deming Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Multimodal German Dataset for Automatic Lip Reading Systems and Transfer Learning. (arXiv:2202.13403v1 [cs.CV])","link":"http://arxiv.org/abs/2202.13403","description":"<p>Large datasets as required for deep learning of lip reading do not exist in\nmany languages. In this paper we present the dataset GLips (German Lips)\nconsisting of 250,000 publicly available videos of the faces of speakers of the\nHessian Parliament, which was processed for word-level lip reading using an\nautomatic pipeline. The format is similar to that of the English language LRW\n(Lip Reading in the Wild) dataset, with each video encoding one word of\ninterest in a context of 1.16 seconds duration, which yields compatibility for\nstudying transfer learning between both datasets. By training a deep neural\nnetwork, we investigate whether lip reading has language-independent features,\nso that datasets of different languages can be used to improve lip reading\nmodels. We demonstrate learning from scratch and show that transfer learning\nfrom LRW to GLips and vice versa improves learning speed and performance, in\nparticular for the validation set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schwiebert_G/0/1/0/all/0/1\">Gerald Schwiebert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weber_C/0/1/0/all/0/1\">Cornelius Weber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1\">Leyuan Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siqueira_H/0/1/0/all/0/1\">Henrique Siqueira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wermter_S/0/1/0/all/0/1\">Stefan Wermter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Candidate Retrieval with Entity Profile Generation for Wikidata Entity Linking. (arXiv:2202.13404v1 [cs.CL])","link":"http://arxiv.org/abs/2202.13404","description":"<p>Entity linking (EL) is the task of linking entity mentions in a document to\nreferent entities in a knowledge base (KB). Many previous studies focus on\nWikipedia-derived KBs. There is little work on EL over Wikidata, even though it\nis the most extensive crowdsourced KB. The scale of Wikidata can open up many\nnew real-world applications, but its massive number of entities also makes EL\nchallenging. To effectively narrow down the search space, we propose a novel\ncandidate retrieval paradigm based on entity profiling. Wikidata entities and\ntheir textual fields are first indexed into a text search engine (e.g.,\nElasticsearch). During inference, given a mention and its context, we use a\nsequence-to-sequence (seq2seq) model to generate the profile of the target\nentity, which consists of its title and description. We use the profile to\nquery the indexed search engine to retrieve candidate entities. Our approach\ncomplements the traditional approach of using a Wikipedia anchor-text\ndictionary, enabling us to further design a highly effective hybrid method for\ncandidate retrieval. Combined with a simple cross-attention reranker, our\ncomplete EL framework achieves state-of-the-art results on three Wikidata-based\ndatasets and strong performance on TACKBP-2010.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lai_T/0/1/0/all/0/1\">Tuan Manh Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_C/0/1/0/all/0/1\">ChengXiang Zhai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Legal Argument Mining with Domain Pre-training and Neural Networks. (arXiv:2202.13457v1 [cs.CL])","link":"http://arxiv.org/abs/2202.13457","description":"<p>The contextual word embedding model, BERT, has proved its ability on\ndownstream tasks with limited quantities of annotated data. BERT and its\nvariants help to reduce the burden of complex annotation work in many\ninterdisciplinary research areas, for example, legal argument mining in digital\nhumanities. Argument mining aims to develop text analysis tools that can\nautomatically retrieve arguments and identify relationships between\nargumentation clauses. Since argumentation is one of the key aspects of case\nlaw, argument mining tools for legal texts are applicable to both academic and\nnon-academic legal research. Domain-specific BERT variants (pre-trained with\ncorpora from a particular background) have also achieved strong performance in\nmany tasks. To our knowledge, previous machine learning studies of argument\nmining on judicial case law still heavily rely on statistical models. In this\npaper, we provide a broad study of both classic and contextual embedding models\nand their performance on practical case law from the European Court of Human\nRights (ECHR). During our study, we also explore a number of neural networks\nwhen being combined with different embeddings. Our experiments provide a\ncomprehensive overview of a variety of approaches to the legal argument mining\ntask. We conclude that domain pre-trained transformer models have great\npotential in this area, although traditional embeddings can also achieve strong\nperformance when combined with additional neural network layers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Gechuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nulty_P/0/1/0/all/0/1\">Paul Nulty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lillis_D/0/1/0/all/0/1\">David Lillis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UCTopic: Unsupervised Contrastive Learning for Phrase Representations and Topic Mining. (arXiv:2202.13469v1 [cs.CL])","link":"http://arxiv.org/abs/2202.13469","description":"<p>High-quality phrase representations are essential to finding topics and\nrelated terms in documents (a.k.a. topic mining). Existing phrase\nrepresentation learning methods either simply combine unigram representations\nin a context-free manner or rely on extensive annotations to learn\ncontext-aware knowledge. In this paper, we propose UCTopic, a novel\nunsupervised contrastive learning framework for context-aware phrase\nrepresentations and topic mining. UCTopic is pretrained in a large scale to\ndistinguish if the contexts of two phrase mentions have the same semantics. The\nkey to pretraining is positive pair construction from our phrase-oriented\nassumptions. However, we find traditional in-batch negatives cause performance\ndecay when finetuning on a dataset with small topic numbers. Hence, we propose\ncluster-assisted contrastive learning(CCL) which largely reduces noisy\nnegatives by selecting negatives from clusters and further improves phrase\nrepresentations for topics accordingly. UCTopic outperforms the\nstate-of-the-art phrase representation model by 38.2% NMI in average on four\nentity cluster-ing tasks. Comprehensive evaluation on topic mining shows that\nUCTopic can extract coherent and diverse topical phrases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiacheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_J/0/1/0/all/0/1\">Jingbo Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1\">Julian McAuley</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Does Order Matter? An Empirical Study on Generating Multiple Keyphrases as a Sequence. (arXiv:1909.03590v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/1909.03590","description":"<p>Recently, concatenating multiple keyphrases as a target sequence has been\nproposed as a new learning paradigm for keyphrase generation. Existing studies\nconcatenate target keyphrases in different orders but no study has examined the\neffects of ordering on models' behavior. In this paper, we propose several\norderings for concatenation and inspect the important factors for training a\nsuccessful keyphrase generation model. By running comprehensive comparisons, we\nobserve one preferable ordering and summarize a number of empirical findings\nand challenges, which can shed light on future research on this line of work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meng_R/0/1/0/all/0/1\">Rui Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xingdi Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brusilovsky_P/0/1/0/all/0/1\">Peter Brusilovsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trischler_A/0/1/0/all/0/1\">Adam Trischler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1\">Daqing He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Empirical Study on Text-Independent Speaker Verification based on the GE2E Method. (arXiv:2011.04896v4 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2011.04896","description":"<p>While many researchers in the speaker recognition area have started to\nreplace the former classical state-of-the-art methods with deep learning\ntechniques, some of the traditional i-vector-based methods are still\nstate-of-the-art in the context of text-independent speaker verification.\nGoogle's Generalized End-to-End Loss for Speaker Verification (GE2E), a deep\nlearning-based technique using long short-term memory units, has recently\ngained a lot of attention due to its speed in convergence and generalization.\nIn this study, we aim at further studying the GE2E method and comparing\ndifferent scenarios in order to investigate all of its aspects. Various\nexperiments including the effects of a random sampling of test and enrollment\nutterances, test utterance duration, and the number of enrollment utterances\nare discussed in this article. Furthermore, we compare the GE2E method with the\nbaseline state-of-the-art i-vector-based methods for text-independent speaker\nverification and show that it outperforms them by resulting in lower error\nrates while being end-to-end and requiring less training time for convergence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Arasteh_S/0/1/0/all/0/1\">Soroosh Tayebi Arasteh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FFCI: A Framework for Interpretable Automatic Evaluation of Summarization. (arXiv:2011.13662v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2011.13662","description":"<p>In this paper, we propose FFCI, a framework for fine-grained summarization\nevaluation that comprises four elements: faithfulness (degree of factual\nconsistency with the source), focus (precision of summary content relative to\nthe reference), coverage (recall of summary content relative to the reference),\nand inter-sentential coherence (document fluency between adjacent sentences).\nWe construct a novel dataset for focus, coverage, and inter-sentential\ncoherence, and develop automatic methods for evaluating each of the four\ndimensions of FFCI based on cross-comparison of evaluation metrics and\nmodel-based evaluation methods, including question answering (QA) approaches,\nsemantic textual similarity (STS), next-sentence prediction (NSP), and scores\nderived from 19 pre-trained language models. We then apply the developed\nmetrics in evaluating a broad range of summarization models across two\ndatasets, with some surprising findings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Koto_F/0/1/0/all/0/1\">Fajri Koto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baldwin_T/0/1/0/all/0/1\">Timothy Baldwin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lau_J/0/1/0/all/0/1\">Jey Han Lau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attention-based Clinical Note Summarization. (arXiv:2104.08942v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08942","description":"<p>In recent years, the trend of deploying digital systems in numerous\nindustries has hiked. The health sector has observed an extensive adoption of\ndigital systems and services that generate significant medical records.\nElectronic health records contain valuable information for prospective and\nretrospective analysis that is often not entirely exploited because of the\ncomplicated dense information storage. The crude purpose of condensing health\nrecords is to select the information that holds most characteristics of the\noriginal documents based on a reported disease. These summaries may boost\ndiagnosis and save a doctor's time during a saturated workload situation like\nthe COVID-19 pandemic. In this paper, we are applying a multi-head\nattention-based mechanism to perform extractive summarization of meaningful\nphrases on clinical notes. Our method finds major sentences for a summary by\ncorrelating tokens, segments, and positional embeddings of sentences in a\nclinical note. The model outputs attention scores that are statistically\ntransformed to extract critical phrases for visualization on the heat-mapping\ntool and for human use.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kanwal_N/0/1/0/all/0/1\">Neel Kanwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rizzo_G/0/1/0/all/0/1\">Giuseppe Rizzo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"When FastText Pays Attention: Efficient Estimation of Word Representations using Constrained Positional Weighting. (arXiv:2104.09691v6 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.09691","description":"<p>In 2018, Mikolov et al. introduced the positional language model, which has\ncharacteristics of attention-based neural machine translation models and which\nachieved state-of-the-art performance on the intrinsic word analogy task.\nHowever, the positional model is not practically fast and it has never been\nevaluated on qualitative criteria or extrinsic tasks. We propose a constrained\npositional model, which adapts the sparse attention mechanism from neural\nmachine translation to improve the speed of the positional model. We evaluate\nthe positional and constrained positional models on three novel qualitative\ncriteria and on language modeling. We show that the positional and constrained\npositional models contain interpretable information about the grammatical\nproperties of words and outperform other shallow models on language modeling.\nWe also show that our constrained model outperforms the positional model on\nlanguage modeling and trains twice as fast.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Novotny_V/0/1/0/all/0/1\">V&#xed;t Novotn&#xfd;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stefanik_M/0/1/0/all/0/1\">Michal &#x160;tef&#xe1;nik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayetiran_E/0/1/0/all/0/1\">Eniafe Festus Ayetiran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sojka_P/0/1/0/all/0/1\">Petr Sojka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reh%5Cr%7Bu%7Drek_R/0/1/0/all/0/1\">Radim &#x158;eh&#x16f;&#x159;ek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NAAQA: A Neural Architecture for Acoustic Question Answering. (arXiv:2106.06147v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.06147","description":"<p>The goal of the Acoustic Question Answering (AQA) task is to answer a\nfree-form text question about the content of an acoustic scene. It was inspired\nby the Visual Question Answering (VQA) task. In this paper, based on the\npreviously introduced CLEAR dataset, we propose a new benchmark for AQA, namely\nCLEAR2, that emphasizes the specific challenges of acoustic inputs. These\ninclude handling of variable duration scenes, and scenes built with elementary\nsounds that differ between training and test set. We also introduce NAAQA, a\nneural architecture that leverages specific properties of acoustic inputs. The\nuse of 1D convolutions in time and frequency to process 2D spectro-temporal\nrepresentations of acoustic content shows promising results and enables\nreductions in model complexity. We show that time coordinate maps augment\ntemporal localization capabilities which enhance performance of the network by\n~17 percentage points. On the other hand, frequency coordinate maps have little\ninfluence on this task. NAAQA achieves 79.5% of accuracy on the AQA task with\n~4 times fewer parameters than the previously explored VQA model. We evaluate\nthe perfomance of NAAQA on an independent data set reconstructed from DAQA. We\nalso test the addition of a MALiMo module in our model on both CLEAR2 and DAQA.\nWe provide a detailed analysis of the results for the different question types.\nWe release the code to produce CLEAR2 as well as NAAQA to foster research in\nthis newly emerging machine learning task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abdelnour_J/0/1/0/all/0/1\">Jerome Abdelnour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rouat_J/0/1/0/all/0/1\">Jean Rouat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salvi_G/0/1/0/all/0/1\">Giampiero Salvi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Packed Levitated Marker for Entity and Relation Extraction. (arXiv:2109.06067v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.06067","description":"<p>Recent entity and relation extraction works focus on investigating how to\nobtain a better span representation from the pre-trained encoder. However, a\nmajor limitation of existing works is that they ignore the interrelation\nbetween spans (pairs). In this work, we propose a novel span representation\napproach, named Packed Levitated Markers (PL-Marker), to consider the\ninterrelation between the spans (pairs) by strategically packing the markers in\nthe encoder. In particular, we propose a neighborhood-oriented packing\nstrategy, which considers the neighbor spans integrally to better model the\nentity boundary information. Furthermore, for those more complicated span pair\nclassification tasks, we design a subject-oriented packing strategy, which\npacks each subject and all its objects to model the interrelation between the\nsame-subject span pairs. The experimental results show that, with the enhanced\nmarker feature, our model advances baselines on six NER benchmarks, and obtains\na 4.1%-4.3% strict relation F1 improvement with higher speed over previous\nstate-of-the-art models on ACE04 and ACE05.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_D/0/1/0/all/0/1\">Deming Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sequential Reptile: Inter-Task Gradient Alignment for Multilingual Learning. (arXiv:2110.02600v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.02600","description":"<p>Multilingual models jointly pretrained on multiple languages have achieved\nremarkable performance on various multilingual downstream tasks. Moreover,\nmodels finetuned on a single monolingual downstream task have shown to\ngeneralize to unseen languages. In this paper, we first show that it is crucial\nfor those tasks to align gradients between them in order to maximize knowledge\ntransfer while minimizing negative transfer. Despite its importance, the\nexisting methods for gradient alignment either have a completely different\npurpose, ignore inter-task alignment, or aim to solve continual learning\nproblems in rather inefficient ways. As a result of the misaligned gradients\nbetween tasks, the model suffers from severe negative transfer in the form of\ncatastrophic forgetting of the knowledge acquired from the pretraining. To\novercome the limitations, we propose a simple yet effective method that can\nefficiently align gradients between tasks. Specifically, we perform each\ninner-optimization by sequentially sampling batches from all the tasks,\nfollowed by a Reptile outer update. Thanks to the gradients aligned between\ntasks by our method, the model becomes less vulnerable to negative transfer and\ncatastrophic forgetting. We extensively validate our method on various\nmulti-task learning and zero-shot cross-lingual transfer tasks, where our\nmethod largely outperforms all the relevant baselines we consider.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seanie Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hae Beom Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Juho Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Sung Ju Hwang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Linguistic Cues of Deception in a Multilingual April Fools' Day Context. (arXiv:2111.03913v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.03913","description":"<p>In this work we consider the collection of deceptive April Fools' Day(AFD)\nnews articles as a useful addition in existing datasets for deception detection\ntasks. Such collections have an established ground truth and are relatively\neasy to construct across languages. As a result, we introduce a corpus that\nincludes diachronic AFD and normal articles from Greek newspapers and news\nwebsites. On top of that, we build a rich linguistic feature set, and analyze\nand compare its deception cues with the only AFD collection currently\navailable, which is in English. Following a current research thread, we also\ndiscuss the individualism/collectivism dimension in deception with respect to\nthese two datasets. Lastly, we build classifiers by testing various monolingual\nand crosslingual settings. The results showcase that AFD datasets can be\nhelpful in deception detection studies, and are in alignment with the\nobservations of other deception detection works.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Papantoniou_K/0/1/0/all/0/1\">Katerina Papantoniou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papadakos_P/0/1/0/all/0/1\">Panagiotis Papadakos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flouris_G/0/1/0/all/0/1\">Giorgos Flouris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plexousakis_D/0/1/0/all/0/1\">Dimitris Plexousakis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards More Robust Natural Language Understanding. (arXiv:2112.02992v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.02992","description":"<p>Natural Language Understanding (NLU) is a branch of Natural Language\nProcessing (NLP) that uses intelligent computer software to understand texts\nthat encode human knowledge. Recent years have witnessed notable progress\nacross various NLU tasks with deep learning techniques, especially with\npretrained language models. Besides proposing more advanced model\narchitectures, constructing more reliable and trustworthy datasets also plays a\nhuge role in improving NLU systems, without which it would be impossible to\ntrain a decent NLU model. It's worth noting that the human ability of\nunderstanding natural language is flexible and robust. On the contrary, most of\nexisting NLU systems fail to achieve desirable performance on out-of-domain\ndata or struggle on handling challenging items (e.g., inherently ambiguous\nitems, adversarial items) in the real world. Therefore, in order to have NLU\nmodels understand human language more effectively, it is expected to prioritize\nthe study on robust natural language understanding. In this thesis, we deem\nthat NLU systems are consisting of two components: NLU models and NLU datasets.\nAs such, we argue that, to achieve robust NLU, the model architecture/training\nand the dataset are equally important. Specifically, we will focus on three NLU\ntasks to illustrate the robustness problem in different NLU tasks and our\ncontributions (i.e., novel models and new datasets) to help achieve more robust\nnatural language understanding. Moving forward, the ultimate goal for robust\nnatural language understanding is to build NLU models which can behave humanly.\nThat is, it's expected that robust NLU systems are capable to transfer the\nknowledge from training corpus to unseen documents more reliably and survive\nwhen encountering challenging items even if the system doesn't know a priori of\nusers' inputs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinliang Frederick Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Pretrained Language Models Based Text Generation. (arXiv:2201.05273v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.05273","description":"<p>Text Generation aims to produce plausible and readable text in human language\nfrom input data. The resurgence of deep learning has greatly advanced this\nfield by neural generation models, especially the paradigm of pretrained\nlanguage models (PLMs). Text generation based on PLMs is viewed as a promising\narea in both academics and industry. In this survey, we begin with introducing\nthree key aspects of applying PLMs to text generation: 1) how to encode the\ninput as representations preserving input semantics which can be fused into\nPLMs; 2) how to design an effective and performant PLM served as the generation\nmodel; and 3) how to effectively optimize PLMs given the reference text and\nensure the generated text satisfying special text properties. Then, we figure\nout some major challenges and solutions corresponding to the three key views.\nNext, we present a summary of various useful resources and typical text\ngeneration applications to work with PLMs. Finally, we highlight some of the\nfuture research directions which will further improve these PLMs for text\ngeneration. We strongly believe that this comprehensive survey paper will serve\nas a valuable resource to learn the core concepts as well as stay up to date on\nthe latest developments in PLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_T/0/1/0/all/0/1\">Tianyi Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_J/0/1/0/all/0/1\">Jian-Yun Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Streaming Multi-Talker ASR with Token-Level Serialized Output Training. (arXiv:2202.00842v3 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2202.00842","description":"<p>This paper proposes a token-level serialized output training (t-SOT), a novel\nframework for streaming multi-talker automatic speech recognition (ASR). Unlike\nexisting streaming multi-talker ASR models using multiple output layers, the\nt-SOT model has only a single output layer that generates recognition tokens\n(e.g., words, subwords) of multiple speakers in chronological order based on\ntheir emission times. A special token that indicates the change of \"virtual\"\noutput channels is introduced to keep track of the overlapping utterances.\nCompared to the prior streaming multi-talker ASR models, the t-SOT model has\nthe advantages of less inference cost and a simpler model architecture.\nMoreover, in our experiments with LibriSpeechMix and LibriCSS datasets, the\nt-SOT-based transformer transducer model achieves the state-of-the-art word\nerror rates by a significant margin to the prior results. For non-overlapping\nspeech, the t-SOT model is on par with a single-talker ASR model in terms of\nboth accuracy and computational cost, opening the door for deploying one model\nfor both single- and multi-talker scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kanda_N/0/1/0/all/0/1\">Naoyuki Kanda</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_J/0/1/0/all/0/1\">Jian Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_Y/0/1/0/all/0/1\">Yu Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiao_X/0/1/0/all/0/1\">Xiong Xiao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meng_Z/0/1/0/all/0/1\">Zhong Meng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1\">Xiaofei Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gaur_Y/0/1/0/all/0/1\">Yashesh Gaur</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Z/0/1/0/all/0/1\">Zhuo Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Jinyu Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yoshioka_T/0/1/0/all/0/1\">Takuya Yoshioka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BLUE at Memotion 2.0 2022: You have my Image, my Text and my Transformer. (arXiv:2202.07543v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.07543","description":"<p>Memes are prevalent on the internet and continue to grow and evolve alongside\nour culture. An automatic understanding of memes propagating on the internet\ncan shed light on the general sentiment and cultural attitudes of people. In\nthis work, we present team BLUE's solution for the second edition of the\nMEMOTION competition. We showcase two approaches for meme classification (i.e.\nsentiment, humour, offensive, sarcasm and motivation levels) using a text-only\nmethod using BERT, and a Multi-Modal-Multi-Task transformer network that\noperates on both the meme image and its caption to output the final scores. In\nboth approaches, we leverage state-of-the-art pretrained models for text (BERT,\nSentence Transformer) and image processing (EfficientNetV4, CLIP). Through our\nefforts, we obtain first place in task A, second place in task B and third\nplace in task C. In addition, our team obtained the highest average score for\nall three tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bucur_A/0/1/0/all/0/1\">Ana-Maria Bucur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cosma_A/0/1/0/all/0/1\">Adrian Cosma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iordache_I/0/1/0/all/0/1\">Ioan-Bogdan Iordache</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reward Modeling for Mitigating Toxicity in Transformer-based Language Models. (arXiv:2202.09662v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.09662","description":"<p>Transformer-based language models are able to generate fluent text and be\nefficiently adapted across various natural language generation tasks. However,\nlanguage models that are pretrained on large unlabeled web text corpora have\nbeen shown to suffer from degenerating toxic content and social bias behaviors,\nconsequently hindering their safe deployment. Various detoxification methods\nwere proposed to mitigate the language model's toxicity; however, these methods\nstruggled to detoxify language models when conditioned on prompts that contain\nspecific social identities related to gender, race, or religion. In this study,\nwe propose Reinforce-Detoxify; A reinforcement learning-based method for\nmitigating toxicity in language models. We address the challenge of safety in\nlanguage models and propose a new reward model that is able to detect toxic\ncontent and mitigate unintended bias towards social identities in toxicity\nprediction. The experiments demonstrate that the Reinforce-Detoxify method for\nlanguage model detoxification outperforms existing detoxification approaches in\nautomatic evaluation metrics, indicating the ability of our approach in\nlanguage model detoxification and less prone to unintended bias toward social\nidentities in generated content.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Faal_F/0/1/0/all/0/1\">Farshid Faal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmitt_K/0/1/0/all/0/1\">Ketra Schmitt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jia Yuan Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatically Generating Counterfactuals for Relation Exaction. (arXiv:2202.10668v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.10668","description":"<p>The goal of relation extraction (RE) is to extract the semantic relations\nbetween/among entities in the text. As a fundamental task in natural language\nprocessing, it is crucial to ensure the robustness of RE models. Despite the\nhigh accuracy current deep neural models have achieved in RE tasks, they are\neasily affected by spurious correlations. One solution to this problem is to\ntrain the model with counterfactually augmented data (CAD) such that it can\nlearn the causation rather than the confounding. However, no attempt has been\nmade on generating counterfactuals for RE tasks. In this paper, we formulate\nthe problem of automatically generating CAD for RE tasks from an entity-centric\nviewpoint, and develop a novel approach to derive contextual counterfactuals\nfor entities. Specifically, we exploit two elementary topological properties,\ni.e., the centrality and the shortest path, in syntactic and semantic\ndependency graphs, to first identify and then intervene on the contextual\ncausal features for entities. We conduct a comprehensive evaluation on four RE\ndatasets by combining our proposed approach with a variety of backbone RE\nmodels. The results demonstrate that our approach not only improves the\nperformance of the backbones, but also makes them more robust in the\nout-of-domain test.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_T/0/1/0/all/0/1\">Tieyun Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Ting Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NU HLT at CMCL 2022 Shared Task: Multilingual and Crosslingual Prediction of Human Reading Behavior in Universal Language Space. (arXiv:2202.10855v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.10855","description":"<p>In this paper, we present a unified model that works for both multilingual\nand crosslingual prediction of reading times of words in various languages. The\nsecret behind the success of this model is in the preprocessing step where all\nwords are transformed to their universal language representation via the\nInternational Phonetic Alphabet (IPA). To the best of our knowledge, this is\nthe first study to favorable exploit this phonological property of language for\nthe two tasks. Various feature types were extracted covering basic frequencies,\nn-grams, information theoretic, and psycholinguistically-motivated predictors\nfor model training. A finetuned Random Forest model obtained best performance\nfor both tasks with 3.8031 and 3.9065 MAE scores for mean first fixation\nduration (FFDAvg) and mean total reading time (TRTAvg) respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Imperial_J/0/1/0/all/0/1\">Joseph Marvin Imperial</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Attention for Incomplete Utterance Rewriting. (arXiv:2202.12160v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.12160","description":"<p>Incomplete utterance rewriting (IUR) has recently become an essential task in\nNLP, aiming to complement the incomplete utterance with sufficient context\ninformation for comprehension. In this paper, we propose a novel method by\ndirectly extracting the coreference and omission relationship from the\nself-attention weight matrix of the transformer instead of word embeddings and\nedit the original text accordingly to generate the complete utterance.\nBenefiting from the rich information in the self-attention weight matrix, our\nmethod achieved competitive results on public IUR datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhitao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianzong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_N/0/1/0/all/0/1\">Ning Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jing Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"JParaCrawl v3.0: A Large-scale English-Japanese Parallel Corpus. (arXiv:2202.12607v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.12607","description":"<p>Most current machine translation models are mainly trained with parallel\ncorpora, and their translation accuracy largely depends on the quality and\nquantity of the corpora. Although there are billions of parallel sentences for\na few language pairs, effectively dealing with most language pairs is difficult\ndue to a lack of publicly available parallel corpora. This paper creates a\nlarge parallel corpus for English-Japanese, a language pair for which only\nlimited resources are available, compared to such resource-rich languages as\nEnglish-German. It introduces a new web-based English-Japanese parallel corpus\nnamed JParaCrawl v3.0. Our new corpus contains more than 21 million unique\nparallel sentence pairs, which is more than twice as many as the previous\nJParaCrawl v2.0 corpus. Through experiments, we empirically show how our new\ncorpus boosts the accuracy of machine translation models on various domains.\nThe JParaCrawl v3.0 corpus will eventually be publicly available online for\nresearch purposes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Morishita_M/0/1/0/all/0/1\">Makoto Morishita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chousa_K/0/1/0/all/0/1\">Katsuki Chousa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suzuki_J/0/1/0/all/0/1\">Jun Suzuki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagata_M/0/1/0/all/0/1\">Masaaki Nagata</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Robust Cybersecurity Topic Classification Tool. (arXiv:2109.02473v2 [cs.IR] CROSS LISTED)","link":"http://arxiv.org/abs/2109.02473","description":"<p>In this research, we use user defined labels from three internet text sources\n(Reddit, Stackexchange, Arxiv) to train 21 different machine learning models\nfor the topic classification task of detecting cybersecurity discussions in\nnatural text. We analyze the false positive and false negative rates of each of\nthe 21 model's in a cross validation experiment. Then we present a\nCybersecurity Topic Classification (CTC) tool, which takes the majority vote of\nthe 21 trained machine learning models as the decision mechanism for detecting\ncybersecurity related text. We also show that the majority vote mechanism of\nthe CTC tool provides lower false negative and false positive rates on average\nthan any of the 21 individual models. We show that the CTC tool is scalable to\nthe hundreds of thousands of documents with a wall clock time on the order of\nhours.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pelofske_E/0/1/0/all/0/1\">Elijah Pelofske</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liebrock_L/0/1/0/all/0/1\">Lorie M. Liebrock</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Urias_V/0/1/0/all/0/1\">Vincent Urias</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Federated Distillation of Natural Language Understanding with Confident Sinkhorns. (arXiv:2110.02432v1 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2110.02432","description":"<p>Enhancing the user experience is an essential task for application service\nproviders. For instance, two users living wide apart may have different tastes\nof food. A food recommender mobile application installed on an edge device\nmight want to learn from user feedback (reviews) to satisfy the client's needs\npertaining to distinct domains. Retrieving user data comes at the cost of\nprivacy while asking for model parameters trained on a user device becomes\nspace inefficient at a large scale. In this work, we propose an approach to\nlearn a central (global) model from the federation of (local) models which are\ntrained on user-devices, without disclosing the local data or model parameters\nto the server. We propose a federation mechanism for the problems with natural\nsimilarity metric between the labels which commonly appear in natural language\nunderstanding (NLU) tasks. To learn the global model, the objective is to\nminimize the optimal transport cost of the global model's predictions from the\nconfident sum of soft-targets assigned by local models. The confidence (a model\nweighting scheme) score of a model is defined as the L2 distance of a model's\nprediction from its probability bias. The method improves the global model's\nperformance over the baseline designed on three NLU tasks with intrinsic label\nspace semantics, i.e., fine-grained sentiment analysis, emotion recognition in\nconversation, and natural language inference. We make our codes public at\nhttps://github.com/declare-lab/sinkhorn-loss.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhardwaj_R/0/1/0/all/0/1\">Rishabh Bhardwaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vaidya_T/0/1/0/all/0/1\">Tushar Vaidya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poria_S/0/1/0/all/0/1\">Soujanya Poria</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-02-28T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/","syn":"http://purl.org/rss/1.0/modules/syndication/","dc":"http://purl.org/dc/elements/1.1/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Refining Self-Supervised Learning in Imaging: Beyond Linear Metric. (arXiv:2202.12921v1 [cs.CV])","link":"http://arxiv.org/abs/2202.12921","description":"<p>We introduce in this paper a new statistical perspective, exploiting the\nJaccard similarity metric, as a measure-based metric to effectively invoke\nnon-linear features in the loss of self-supervised contrastive learning.\nSpecifically, our proposed metric may be interpreted as a dependence measure\nbetween two adapted projections learned from the so-called latent\nrepresentations. This is in contrast to the cosine similarity measure in the\nconventional contrastive learning model, which accounts for correlation\ninformation. To the best of our knowledge, this effectively non-linearly fused\ninformation embedded in the Jaccard similarity, is novel to self-supervision\nlearning with promising results. The proposed approach is compared to two\nstate-of-the-art self-supervised contrastive learning methods on three image\ndatasets. We not only demonstrate its amenable applicability in current ML\nproblems, but also its improved performance and training efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1\">Bo Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krim_H/0/1/0/all/0/1\">Hamid Krim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tianfu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cansever_D/0/1/0/all/0/1\">Derya Cansever</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OptGAN: Optimizing and Interpreting the Latent Space of the Conditional Text-to-Image GANs. (arXiv:2202.12929v1 [cs.CV])","link":"http://arxiv.org/abs/2202.12929","description":"<p>Text-to-image generation intends to automatically produce a photo-realistic\nimage, conditioned on a textual description. It can be potentially employed in\nthe field of art creation, data augmentation, photo-editing, etc. Although many\nefforts have been dedicated to this task, it remains particularly challenging\nto generate believable, natural scenes. To facilitate the real-world\napplications of text-to-image synthesis, we focus on studying the following\nthree issues: 1) How to ensure that generated samples are believable, realistic\nor natural? 2) How to exploit the latent space of the generator to edit a\nsynthesized image? 3) How to improve the explainability of a text-to-image\ngeneration framework? In this work, we constructed two novel data sets (i.e.,\nthe Good &amp; Bad bird and face data sets) consisting of successful as well as\nunsuccessful generated samples, according to strict criteria. To effectively\nand efficiently acquire high-quality images by increasing the probability of\ngenerating Good latent codes, we use a dedicated Good/Bad classifier for\ngenerated images. It is based on a pre-trained front end and fine-tuned on the\nbasis of the proposed Good &amp; Bad data set. After that, we present a novel\nalgorithm which identifies semantically-understandable directions in the latent\nspace of a conditional text-to-image GAN architecture by performing independent\ncomponent analysis on the pre-trained weight values of the generator.\nFurthermore, we develop a background-flattening loss (BFL), to improve the\nbackground appearance in the edited image. Subsequently, we introduce linear\ninterpolation analysis between pairs of keywords. This is extended into a\nsimilar triangular `linguistic' interpolation in order to take a deep look into\nwhat a text-to-image synthesis model has learned within the linguistic\nembeddings. Our data set is available at\nhttps://zenodo.org/record/6283798#.YhkN_ujMI2w.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhenxing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schomaker_L/0/1/0/all/0/1\">Lambert Schomaker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-View Fusion Transformer for Sensor-Based Human Activity Recognition. (arXiv:2202.12949v1 [eess.SP])","link":"http://arxiv.org/abs/2202.12949","description":"<p>As a fundamental problem in ubiquitous computing and machine learning,\nsensor-based human activity recognition (HAR) has drawn extensive attention and\nmade great progress in recent years. HAR aims to recognize human activities\nbased on the availability of rich time-series data collected from multi-modal\nsensors such as accelerometers and gyroscopes. However, recent deep learning\nmethods are focusing on one view of the data, i.e., the temporal view, while\nshallow methods tend to utilize the hand-craft features for recognition, e.g.,\nthe statistics view. In this paper, to extract a better feature for advancing\nthe performance, we propose a novel method, namely multi-view fusion\ntransformer (MVFT) along with a novel attention mechanism. First, MVFT encodes\nthree views of information, i.e., the temporal, frequent, and statistical views\nto generate multi-view features. Second, the novel attention mechanism uncovers\ninner- and cross-view clues to catalyze mutual interactions between three views\nfor detailed relation modeling. Moreover, extensive experiments on two datasets\nillustrate the superiority of our methods over several state-of-the-art\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yimu Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_K/0/1/0/all/0/1\">Kun Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xue_H/0/1/0/all/0/1\">Hui Xue</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Model Attribution of Face-swap Deepfake Videos. (arXiv:2202.12951v1 [cs.CV])","link":"http://arxiv.org/abs/2202.12951","description":"<p>AI-created face-swap videos, commonly known as Deepfakes, have attracted wide\nattention as powerful impersonation attacks. Existing research on Deepfakes\nmostly focuses on binary detection to distinguish between real and fake videos.\nHowever, it is also important to determine the specific generation model for a\nfake video, which can help attribute it to the source for forensic\ninvestigation. In this paper, we fill this gap by studying the model\nattribution problem of Deepfake videos. We first introduce a new dataset with\nDeepFakes from Different Models (DFDM) based on several Autoencoder models.\nSpecifically, five generation models with variations in encoder, decoder,\nintermediate layer, input resolution, and compression ratio have been used to\ngenerate a total of 6,450 Deepfake videos based on the same input. Then we take\nDeepfakes model attribution as a multiclass classification task and propose a\nspatial and temporal attention based method to explore the differences among\nDeepfakes in the new dataset. Experimental evaluation shows that most existing\nDeepfakes detection methods failed in Deepfakes model attribution, while the\nproposed method achieved over 70% accuracy on the high-quality DFDM dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jia_S/0/1/0/all/0/1\">Shan Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1\">Siwei Lyu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image reconstruction algorithms in radio interferometry: from handcrafted to learned denoisers. (arXiv:2202.12959v1 [eess.IV])","link":"http://arxiv.org/abs/2202.12959","description":"<p>We introduce a new class of iterative image reconstruction algorithms for\nradio interferometry, at the interface of convex optimization and deep\nlearning, inspired by plug-and-play methods. The approach consists in learning\na prior image model by training a deep neural network (DNN) as a denoiser, and\nsubstituting it for the handcrafted proximal regularization operator of an\noptimization algorithm. The proposed AIRI (\"AI for Regularization in\nRadio-Interferometric Imaging\") framework, for imaging complex intensity\nstructure with diffuse and faint emission, inherits the robustness and\ninterpretability of optimization, and the learning power and speed of networks.\nOur approach relies on three steps. Firstly, we design a low dynamic range\ndatabase for supervised training from optical intensity images. Secondly, we\ntrain a DNN denoiser with basic architecture ensuring positivity of the output\nimage, at a noise level inferred from the signal-to-noise ratio of the data. We\nuse either $\\ell_2$ or $\\ell_1$ training losses, enhanced with a\nnonexpansiveness term ensuring algorithm convergence, and including on-the-fly\ndatabase dynamic range enhancement via exponentiation. Thirdly, we plug the\nlearned denoiser into the forward-backward optimization algorithm, resulting in\na simple iterative structure alternating a denoising step with a\ngradient-descent data-fidelity step. The resulting AIRI-$\\ell_2$ and\nAIRI-$\\ell_1$ were validated against CLEAN and optimization algorithms of the\nSARA family, propelled by the \"average sparsity\" proximal regularization\noperator. Simulation results show that these first AIRI incarnations are\ncompetitive in imaging quality with SARA and its unconstrained\nforward-backward-based version uSARA, while providing significant acceleration.\nCLEAN remains faster but offers lower reconstruction quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Terris_M/0/1/0/all/0/1\">Matthieu Terris</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dabbech_A/0/1/0/all/0/1\">Arwa Dabbech</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tang_C/0/1/0/all/0/1\">Chao Tang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wiaux_Y/0/1/0/all/0/1\">Yves Wiaux</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FSGANv2: Improved Subject Agnostic Face Swapping and Reenactment. (arXiv:2202.12972v1 [cs.CV])","link":"http://arxiv.org/abs/2202.12972","description":"<p>We present Face Swapping GAN (FSGAN) for face swapping and reenactment.\nUnlike previous work, we offer a subject agnostic swapping scheme that can be\napplied to pairs of faces without requiring training on those faces. We derive\na novel iterative deep learning--based approach for face reenactment which\nadjusts significant pose and expression variations that can be applied to a\nsingle image or a video sequence. For video sequences, we introduce a\ncontinuous interpolation of the face views based on reenactment, Delaunay\nTriangulation, and barycentric coordinates. Occluded face regions are handled\nby a face completion network. Finally, we use a face blending network for\nseamless blending of the two faces while preserving the target skin color and\nlighting conditions. This network uses a novel Poisson blending loss combining\nPoisson optimization with a perceptual loss. We compare our approach to\nexisting state-of-the-art systems and show our results to be both qualitatively\nand quantitatively superior. This work describes extensions of the FSGAN\nmethod, proposed in an earlier conference version of our work, as well as\nadditional experiments and results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nirkin_Y/0/1/0/all/0/1\">Yuval Nirkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keller_Y/0/1/0/all/0/1\">Yosi Keller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassner_T/0/1/0/all/0/1\">Tal Hassner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OCR-IDL: OCR Annotations for Industry Document Library Dataset. (arXiv:2202.12985v1 [cs.CV])","link":"http://arxiv.org/abs/2202.12985","description":"<p>Pretraining has proven successful in Document Intelligence tasks where deluge\nof documents are used to pretrain the models only later to be finetuned on\ndownstream tasks. One of the problems of the pretraining approaches is the\ninconsistent usage of pretraining data with different OCR engines leading to\nincomparable results between models. In other words, it is not obvious whether\nthe performance gain is coming from diverse usage of amount of data and\ndistinct OCR engines or from the proposed models. To remedy the problem, we\nmake public the OCR annotations for IDL documents using commercial OCR engine\ngiven their superior performance over open source OCR models. The contributed\ndataset (OCR-IDL) has an estimated monetary value over 20K US$. It is our hope\nthat OCR-IDL can be a starting point for future works on Document Intelligence.\nAll of our data and its collection process with the annotations can be found in\nhttps://github.com/furkanbiten/idl_data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Biten_A/0/1/0/all/0/1\">Ali Furkan Biten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tito_R/0/1/0/all/0/1\">Rub&#xe8;n Tito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomez_L/0/1/0/all/0/1\">Lluis Gomez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valveny_E/0/1/0/all/0/1\">Ernest Valveny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karatzas_D/0/1/0/all/0/1\">Dimosthenis Karatzas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extracting Effective Subnetworks with Gumebel-Softmax. (arXiv:2202.12986v1 [cs.CV])","link":"http://arxiv.org/abs/2202.12986","description":"<p>Large and performant neural networks are often overparameterized and can be\ndrastically reduced in size and complexity thanks to pruning. Pruning is a\ngroup of methods, which seeks to remove redundant or unnecessary weights or\ngroups of weights in a network. These techniques allow the creation of\nlightweight networks, which are particularly critical in embedded or mobile\napplications. In this paper, we devise an alternative pruning method that\nallows extracting effective subnetworks from larger untrained ones. Our method\nis stochastic and extracts subnetworks by exploring different topologies which\nare sampled using Gumbel Softmax. The latter is also used to train probability\ndistributions which measure the relevance of weights in the sampled topologies.\nThe resulting subnetworks are further enhanced using a highly efficient\nrescaling mechanism that reduces training time and improves performances.\nExtensive experiments conducted on CIFAR10 show the outperformance of our\nsubnetwork extraction method against the related work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dupont_R/0/1/0/all/0/1\">Robin Dupont</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alaoui_M/0/1/0/all/0/1\">Mohammed Amine Alaoui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahbi_H/0/1/0/all/0/1\">Hichem Sahbi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lebois_A/0/1/0/all/0/1\">Alice Lebois</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Brief Survey on Adaptive Video Streaming Quality Assessment. (arXiv:2202.12987v1 [cs.MM])","link":"http://arxiv.org/abs/2202.12987","description":"<p>Quality of experience (QoE) assessment for adaptive video streaming plays a\nsignificant role in advanced network management systems. It is especially\nchallenging in case of dynamic adaptive streaming schemes over HTTP (DASH)\nwhich has increasingly complex characteristics including additional playback\nissues. In this paper, we provide a brief overview of adaptive video streaming\nquality assessment. Upon our review of related works, we analyze and compare\ndifferent variations of objective QoE assessment models with or without using\nmachine learning techniques for adaptive video streaming. Through the\nperformance analysis, we observe that hybrid models perform better than both\nquality-of-service (QoS) driven QoE approaches and signal fidelity measurement.\nMoreover, the machine learning-based model slightly outperforms the model\nwithout using machine learning for the same setting. In addition, we find that\nexisting video streaming QoE assessment models still have limited performance,\nwhich makes it difficult to be applied in practical communication systems.\nTherefore, based on the success of deep learned feature representations for\ntraditional video quality prediction, we also apply the off-the-shelf deep\nconvolutional neural network (DCNN) to evaluate the perceptual quality of\nstreaming videos, where the spatio-temporal properties of streaming videos are\ntaken into consideration. Experiments demonstrate its superiority, which sheds\nlight on the future development of specifically designed deep learning\nframeworks for adaptive video streaming quality assessment. We believe this\nsurvey can serve as a guideline for QoE assessment of adaptive video streaming.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_X/0/1/0/all/0/1\">Xiongkuo Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Q/0/1/0/all/0/1\">Qiuping Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly Supervised Instance Segmentation using Motion Information via Optical Flow. (arXiv:2202.13006v1 [cs.CV])","link":"http://arxiv.org/abs/2202.13006","description":"<p>Weakly supervised instance segmentation has gained popularity because it\nreduces high annotation cost of pixel-level masks required for model training.\nRecent approaches for weakly supervised instance segmentation detect and\nsegment objects using appearance information obtained from a static image.\nHowever, it poses the challenge of identifying objects with a\nnon-discriminatory appearance. In this study, we address this problem by using\nmotion information from image sequences. We propose a two-stream encoder that\nleverages appearance and motion features extracted from images and optical\nflows. Additionally, we propose a novel pairwise loss that considers both\nappearance and motion information to supervise segmentation. We conducted\nextensive evaluations on the YouTube-VIS 2019 benchmark dataset. Our results\ndemonstrate that the proposed method improves the Average Precision of the\nstate-of-the-art method by 3.1.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ikeda_J/0/1/0/all/0/1\">Jun Ikeda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mori_J/0/1/0/all/0/1\">Junichiro Mori</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-view Gradient Consistency for SVBRDF Estimation of Complex Scenes under Natural Illumination. (arXiv:2202.13017v1 [cs.CV])","link":"http://arxiv.org/abs/2202.13017","description":"<p>This paper presents a process for estimating the spatially varying surface\nreflectance of complex scenes observed under natural illumination. In contrast\nto previous methods, our process is not limited to scenes viewed under\ncontrolled lighting conditions but can handle complex indoor and outdoor scenes\nviewed under arbitrary illumination conditions. An end-to-end process uses a\nmodel of the scene's geometry and several images capturing the scene's surfaces\nfrom arbitrary viewpoints and under various natural illumination conditions. We\ndevelop a differentiable path tracer that leverages least-square conformal\nmapping for handling multiple disjoint objects appearing in the scene. We\nfollow a two-step optimization process and introduce a multi-view gradient\nconsistency loss which results in up to 30-50% improvement in the image\nreconstruction loss and can further achieve better disentanglement of the\ndiffuse and specular BRDFs compared to other state-of-the-art. We demonstrate\nthe process in real-world indoor and outdoor scenes from images in the wild and\nshow that we can produce realistic renders consistent with actual images using\nthe estimated reflectance properties. Experiments show that our technique\nproduces realistic results for arbitrary outdoor scenes with complex geometry.\nThe source code is publicly available at:\nhttps://gitlab.com/alen.joy/multi-view-gradient-consistency-for-svbrdf-estimation-of-complex-scenes-under-natural-illumination\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Joy_A/0/1/0/all/0/1\">Alen Joy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poullis_C/0/1/0/all/0/1\">Charalambos Poullis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HCIL: Hierarchical Class Incremental Learning for Longline Fishing Visual Monitoring. (arXiv:2202.13018v1 [cs.CV])","link":"http://arxiv.org/abs/2202.13018","description":"<p>The goal of electronic monitoring of longline fishing is to visually monitor\nthe fish catching activities on fishing vessels based on cameras, either for\nregulatory compliance or catch counting. The previous hierarchical\nclassification method demonstrates efficient fish species identification of\ncatches from longline fishing, where fishes are under severe deformation and\nself-occlusion during the catching process. Although the hierarchical\nclassification mitigates the laborious efforts of human reviews by providing\nconfidence scores in different hierarchical levels, its performance drops\ndramatically under the class incremental learning (CIL) scenario. A CIL system\nshould be able to learn about more and more classes over time from a stream of\ndata, i.e., only the training data for a small number of classes have to be\npresent at the beginning and new classes can be added progressively. In this\nwork, we introduce a Hierarchical Class Incremental Learning (HCIL) model,\nwhich significantly improves the state-of-the-art hierarchical classification\nmethods under the CIL scenario.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mei_J/0/1/0/all/0/1\">Jie Mei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romain_S/0/1/0/all/0/1\">Suzanne Romain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rose_C/0/1/0/all/0/1\">Craig Rose</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Magrane_K/0/1/0/all/0/1\">Kelsey Magrane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1\">Jenq-Neng Hwang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Building a visual semantics aware object hierarchy. (arXiv:2202.13021v1 [cs.CV])","link":"http://arxiv.org/abs/2202.13021","description":"<p>The semantic gap is defined as the difference between the linguistic\nrepresentations of the same concept, which usually leads to misunderstanding\nbetween individuals with different knowledge backgrounds. Since linguistically\nannotated images are extensively used for training machine learning models,\nsemantic gap problem (SGP) also results in inevitable bias on image annotations\nand further leads to poor performance on current computer vision tasks. To\naddress this problem, we propose a novel unsupervised method to build visual\nsemantics aware object hierarchy, aiming to get a classification model by\nlearning from pure-visual information and to dissipate the bias of linguistic\nrepresentations caused by SGP. Our intuition in this paper comes from\nreal-world knowledge representation where concepts are hierarchically\norganized, and each concept can be described by a set of features rather than a\nlinguistic annotation, namely visual semantic. The evaluation consists of two\nparts, firstly we apply the constructed hierarchy on the object recognition\ntask and then we compare our visual hierarchy and existing lexical hierarchies\nto show the validity of our method. The preliminary results reveal the\nefficiency and potential of our proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Diao_X/0/1/0/all/0/1\">Xiaolei Diao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalized Label Shift Correction via Minimum Uncertainty Principle: Theory and Algorithm. (arXiv:2202.13043v1 [cs.LG])","link":"http://arxiv.org/abs/2202.13043","description":"<p>As a fundamental problem in machine learning, dataset shift induces a\nparadigm to learn and transfer knowledge under changing environment. Previous\nmethods assume the changes are induced by covariate, which is less practical\nfor complex real-world data. We consider the Generalized Label Shift (GLS),\nwhich provides an interpretable insight into the learning and transfer of\ndesirable knowledge. Current GLS methods: 1) are not well-connected with the\nstatistical learning theory; 2) usually assume the shifting conditional\ndistributions will be matched with an implicit transformation, but its explicit\nmodeling is unexplored. In this paper, we propose a conditional adaptation\nframework to deal with these challenges. From the perspective of learning\ntheory, we prove that the generalization error of conditional adaptation is\nlower than previous covariate adaptation. Following the theoretical results, we\npropose the minimum uncertainty principle to learn conditional invariant\ntransformation via discrepancy optimization. Specifically, we propose the\n\\textit{conditional metric operator} on Hilbert space to characterize the\ndistinctness of conditional distributions. For finite observations, we prove\nthat the empirical estimation is always well-defined and will converge to\nunderlying truth as sample size increases. The results of extensive experiments\ndemonstrate that the proposed model achieves competitive performance under\ndifferent GLS scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">You-Wei Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_C/0/1/0/all/0/1\">Chuan-Xian Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Optical flow-based branch segmentation for complex orchard environments. (arXiv:2202.13050v1 [cs.CV])","link":"http://arxiv.org/abs/2202.13050","description":"<p>Machine vision is a critical subsystem for enabling robots to be able to\nperform a variety of tasks in orchard environments. However, orchards are\nhighly visually complex environments, and computer vision algorithms operating\nin them must be able to contend with variable lighting conditions and\nbackground noise. Past work on enabling deep learning algorithms to operate in\nthese environments has typically required large amounts of hand-labeled data to\ntrain a deep neural network or physically controlling the conditions under\nwhich the environment is perceived. In this paper, we train a neural network\nsystem in simulation only using simulated RGB data and optical flow. This\nresulting neural network is able to perform foreground segmentation of branches\nin a busy orchard environment without additional real-world training or using\nany special setup or equipment beyond a standard camera. Our results show that\nour system is highly accurate and, when compared to a network using manually\nlabeled RGBD data, achieves significantly more consistent and robust\nperformance across environments that differ from the training set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+You_A/0/1/0/all/0/1\">Alexander You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grimm_C/0/1/0/all/0/1\">Cindy Grimm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davidson_J/0/1/0/all/0/1\">Joseph R. Davidson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Depth from Focal Stack with Defocus Model for Camera-Setting Invariance. (arXiv:2202.13055v1 [cs.CV])","link":"http://arxiv.org/abs/2202.13055","description":"<p>We propose a learning-based depth from focus/defocus (DFF), which takes a\nfocal stack as input for estimating scene depth. Defocus blur is a useful cue\nfor depth estimation. However, the size of the blur depends on not only scene\ndepth but also camera settings such as focus distance, focal length, and\nf-number. Current learning-based methods without any defocus models cannot\nestimate a correct depth map if camera settings are different at training and\ntest times. Our method takes a plane sweep volume as input for the constraint\nbetween scene depth, defocus images, and camera settings, and this intermediate\nrepresentation enables depth estimation with different camera settings at\ntraining and test times. This camera-setting invariance can enhance the\napplicability of learning-based DFF methods. The experimental results also\nindicate that our method is robust against a synthetic-to-real domain gap, and\nexhibits state-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fujimura_Y/0/1/0/all/0/1\">Yuki Fujimura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iiyama_M/0/1/0/all/0/1\">Masaaki Iiyama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Funatomi_T/0/1/0/all/0/1\">Takuya Funatomi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukaigawa_Y/0/1/0/all/0/1\">Yasuhiro Mukaigawa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An End-to-End Transformer Model for Crowd Localization. (arXiv:2202.13065v1 [cs.CV])","link":"http://arxiv.org/abs/2202.13065","description":"<p>Crowd localization, predicting head positions, is a more practical and\nhigh-level task than simply counting. Existing methods employ pseudo-bounding\nboxes or pre-designed localization maps, relying on complex post-processing to\nobtain the head positions. In this paper, we propose an elegant, end-to-end\nCrowd Localization TRansformer named CLTR that solves the task in the\nregression-based paradigm. The proposed method views the crowd localization as\na direct set prediction problem, taking extracted features and trainable\nembeddings as input of the transformer-decoder. To achieve good matching\nresults, we introduce a KMO-based Hungarian, which innovatively revisits the\nlabel assignment from a context view instead of an independent instance view.\nExtensive experiments conducted on five datasets in various data settings show\nthe effectiveness of our method. In particular, the proposed method achieves\nthe best localization performance on the NWPU-Crowd, UCF-QNRF, and ShanghaiTech\nPart A datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1\">Dingkang Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xiang Bai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Robust Document Image Watermarking Scheme using Deep Neural Network. (arXiv:2202.13067v1 [cs.CV])","link":"http://arxiv.org/abs/2202.13067","description":"<p>Watermarking is an important copyright protection technology which generally\nembeds the identity information into the carrier imperceptibly. Then the\nidentity can be extracted to prove the copyright from the watermarked carrier\neven after suffering various attacks. Most of the existing watermarking\ntechnologies take the nature images as carriers. Different from the natural\nimages, document images are not so rich in color and texture, and thus have\nless redundant information to carry watermarks. This paper proposes an\nend-to-end document image watermarking scheme using the deep neural network.\nSpecifically, an encoder and a decoder are designed to embed and extract the\nwatermark. A noise layer is added to simulate the various attacks that could be\nencountered in reality, such as the Cropout, Dropout, Gaussian blur, Gaussian\nnoise, Resize, and JPEG Compression. A text-sensitive loss function is designed\nto limit the embedding modification on characters. An embedding strength\nadjustment strategy is proposed to improve the quality of watermarked image\nwith little loss of extraction accuracy. Experimental results show that the\nproposed document image watermarking technology outperforms three\nstate-of-the-arts in terms of the robustness and image quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1\">Sulong Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_Z/0/1/0/all/0/1\">Zhihua Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_J/0/1/0/all/0/1\">Jianwei Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xingming Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weng_J/0/1/0/all/0/1\">Jian Weng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncertainty-Aware Deep Multi-View Photometric Stereo. (arXiv:2202.13071v1 [cs.CV])","link":"http://arxiv.org/abs/2202.13071","description":"<p>This paper presents a simple and effective solution to the problem of\nmulti-view photometric stereo (MVPS). It is well-known that photometric stereo\n(PS) is excellent at recovering high-frequency surface details, whereas\nmulti-view stereo (MVS) can help remove the low-frequency distortion due to PS\nand retain the global geometry of the shape. This paper proposes an approach\nthat can effectively utilize such complementary strengths of PS and MVS. Our\nkey idea is to suitably combine them while taking into account the per-pixel\nuncertainty of their estimates. To this end, we estimate per-pixel surface\nnormals and depth using an uncertainty-aware deep-PS network and deep-MVS\nnetwork, respectively. Uncertainty modeling helps select reliable surface\nnormal and depth estimates at each pixel which then act as a true\nrepresentative of the dense surface geometry. At each pixel, our approach\neither selects or discards deep-PS and deep-MVS network prediction depending on\nthe prediction uncertainty measure. For dense, detailed, and precise inference\nof the object's surface profile, we propose to learn the implicit neural shape\nrepresentation via a multilayer perceptron (MLP). Our approach encourages the\nMLP to converge to a natural zero-level set surface using the confident\nprediction from deep-PS and deep-MVS networks, providing superior dense surface\nreconstruction. Extensive experiments on the DiLiGenT-MV benchmark dataset show\nthat our method outperforms most of the existing approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kaya_B/0/1/0/all/0/1\">Berk Kaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Suryansh Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_C/0/1/0/all/0/1\">Carlos Oliveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrari_V/0/1/0/all/0/1\">Vittorio Ferrari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Contrastive Self-Supervised Learning. (arXiv:2202.13072v1 [cs.CV])","link":"http://arxiv.org/abs/2202.13072","description":"<p>Recently, learning from vast unlabeled data, especially self-supervised\nlearning, has been emerging and attracted widespread attention. Self-supervised\nlearning followed by the supervised fine-tuning on a few labeled examples can\nsignificantly improve label efficiency and outperform standard supervised\ntraining using fully annotated data. In this work, we present a novel\nself-supervised deep learning paradigm based on online hard negative pair\nmining. Specifically, we design a student-teacher network to generate\nmulti-view of the data for self-supervised learning and integrate hard negative\npair mining into the training. Then we derive a new triplet-like loss\nconsidering both positive sample pairs and mined hard negative sample pairs.\nExtensive experiments demonstrate the effectiveness of the proposed method and\nits components on ILSVRC-2012.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wentao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_H/0/1/0/all/0/1\">Hang Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_T/0/1/0/all/0/1\">Tingxun Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_C/0/1/0/all/0/1\">Chao Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Ji Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Global Instance Tracking: Locating Target More Like Humans. (arXiv:2202.13073v1 [cs.CV])","link":"http://arxiv.org/abs/2202.13073","description":"<p>Target tracking, the essential ability of the human visual system, has been\nsimulated by computer vision tasks. However, existing trackers perform well in\naustere experimental environments but fail in challenges like occlusion and\nfast motion. The massive gap indicates that researches only measure tracking\nperformance rather than intelligence. How to scientifically judge the\nintelligence level of trackers? Distinct from decision-making problems, lacking\nthree requirements (a challenging task, a fair environment, and a scientific\nevaluation procedure) makes it strenuous to answer the question. In this\narticle, we first propose the global instance tracking (GIT) task, which is\nsupposed to search an arbitrary user-specified instance in a video without any\nassumptions about camera or motion consistency, to model the human visual\ntracking ability. Whereafter, we construct a high-quality and large-scale\nbenchmark VideoCube to create a challenging environment. Finally, we design a\nscientific evaluation procedure using human capabilities as the baseline to\njudge tracking intelligence. Additionally, we provide an online platform with\ntoolkit and an updated leaderboard. Although the experimental results indicate\na definite gap between trackers and humans, we expect to take a step forward to\ngenerate authentic human-like trackers. The database, toolkit, evaluation\nserver, and baseline results are available at <a href=\"http://videocube.aitestunion.com.\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shiyu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Lianghua Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kaiqi Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Utility and Feasibility of a Center Surround Event Camera. (arXiv:2202.13076v1 [cs.CV])","link":"http://arxiv.org/abs/2202.13076","description":"<p>Standard dynamic vision sensor (DVS) event cameras output a stream of\nspatially-independent log-intensity brightness change events so they cannot\nsuppress spatial redundancy. Nearly all biological retinas use an antagonistic\ncenter-surround organization. This paper proposes a practical method of\nimplementing a compact, energy-efficient Center Surround DVS (CSDVS) with a\nsurround smoothing network that uses compact polysilicon resistors for lateral\nresistance. The paper includes behavioral simulation results for the CSDVS (see\nsites.google.com/view/csdvs/home). The CSDVS would significantly reduce events\ncaused by low spatial frequencies, but amplify the informative high frequency\nspatiotemporal events.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Delbruck_T/0/1/0/all/0/1\">Tobi Delbruck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chenghan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Graca_R/0/1/0/all/0/1\">Rui Graca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mcreynolds_B/0/1/0/all/0/1\">Brian Mcreynolds</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SWIS: Self-Supervised Representation Learning For Writer Independent Offline Signature Verification. (arXiv:2202.13078v1 [cs.CV])","link":"http://arxiv.org/abs/2202.13078","description":"<p>Writer independent offline signature verification is one of the most\nchallenging tasks in pattern recognition as there is often a scarcity of\ntraining data. To handle such data scarcity problem, in this paper, we propose\na novel self-supervised learning (SSL) framework for writer independent offline\nsignature verification. To our knowledge, this is the first attempt to utilize\nself-supervised setting for the signature verification task. The objective of\nself-supervised representation learning from the signature images is achieved\nby minimizing the cross-covariance between two random variables belonging to\ndifferent feature directions and ensuring a positive cross-covariance between\nthe random variables denoting the same feature direction. This ensures that the\nfeatures are decorrelated linearly and the redundant information is discarded.\nThrough experimental results on different data sets, we obtained encouraging\nresults.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Manna_S/0/1/0/all/0/1\">Siladittya Manna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chattopadhyay_S/0/1/0/all/0/1\">Soumitri Chattopadhyay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_S/0/1/0/all/0/1\">Saumik Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pal_U/0/1/0/all/0/1\">Umapada Pal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improved Hard Example Mining Approach for Single Shot Object Detectors. (arXiv:2202.13080v1 [cs.CV])","link":"http://arxiv.org/abs/2202.13080","description":"<p>Hard example mining methods generally improve the performance of the object\ndetectors, which suffer from imbalanced training sets. In this work, two\nexisting hard example mining approaches (LRM and focal loss, FL) are adapted\nand combined in a state-of-the-art real-time object detector, YOLOv5. The\neffectiveness of the proposed approach for improving the performance on hard\nexamples is extensively evaluated. The proposed method increases mAP by 3%\ncompared to using the original loss function and around 1-2% compared to using\nthe hard-mining methods (LRM or FL) individually on 2021 Anti-UAV Challenge\nDataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Koksal_A/0/1/0/all/0/1\">Aybora Koksal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuzcuoglu_O/0/1/0/all/0/1\">Onder Tuzcuoglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ince_K/0/1/0/all/0/1\">Kutalmis Gokalp Ince</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ataseven_Y/0/1/0/all/0/1\">Yoldas Ataseven</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alatan_A/0/1/0/all/0/1\">A. Aydin Alatan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Improved Deep Learning Approach For Product Recognition on Racks in Retail Stores. (arXiv:2202.13081v1 [cs.CV])","link":"http://arxiv.org/abs/2202.13081","description":"<p>Automated product recognition in retail stores is an important real-world\napplication in the domain of Computer Vision and Pattern Recognition. In this\npaper, we consider the problem of automatically identifying the classes of the\nproducts placed on racks in retail stores from an image of the rack and\ninformation about the query/product images. We improve upon the existing\napproaches in terms of effectiveness and memory requirement by developing a\ntwo-stage object detection and recognition pipeline comprising of a\nFaster-RCNN-based object localizer that detects the object regions in the rack\nimage and a ResNet-18-based image encoder that classifies the detected regions\ninto the appropriate classes. Each of the models is fine-tuned using\nappropriate data sets for better prediction and data augmentation is performed\non each query image to prepare an extensive gallery set for fine-tuning the\nResNet-18-based product recognition model. This encoder is trained using a\ntriplet loss function following the strategy of online-hard-negative-mining for\nimproved prediction. The proposed models are lightweight and can be connected\nin an end-to-end manner during deployment for automatically identifying each\nproduct object placed in a rack image. Extensive experiments using Grozi-32k\nand GP-180 data sets verify the effectiveness of the proposed model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sinha_A/0/1/0/all/0/1\">Ankit Sinha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_S/0/1/0/all/0/1\">Soham Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chattopadhyay_P/0/1/0/all/0/1\">Pratik Chattopadhyay</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Speech Recognition for Multiple Languages in the Wild. (arXiv:2202.13084v1 [cs.CV])","link":"http://arxiv.org/abs/2202.13084","description":"<p>Visual speech recognition (VSR) aims to recognise the content of speech based\non the lip movements without relying on the audio stream. Advances in deep\nlearning and the availability of large audio-visual datasets have led to the\ndevelopment of much more accurate and robust VSR models than ever before.\nHowever, these advances are usually due to larger training sets rather than the\nmodel design. In this work, we demonstrate that designing better models is\nequally important to using larger training sets. We propose the addition of\nprediction-based auxiliary tasks to a VSR model and highlight the importance of\nhyper-parameter optimisation and appropriate data augmentations. We show that\nsuch model works for different languages and outperforms all previous methods\ntrained on publicly available datasets by a large margin. It even outperforms\nmodels that were trained on non-publicly available datasets containing up to to\n21 times more data. We show furthermore that using additional training data,\neven in other languages or with automatically generated transcriptions, results\nin further improvement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_P/0/1/0/all/0/1\">Pingchuan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petridis_S/0/1/0/all/0/1\">Stavros Petridis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pantic_M/0/1/0/all/0/1\">Maja Pantic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RIConv++: Effective Rotation Invariant Convolutions for 3D Point Clouds Deep Learning. (arXiv:2202.13094v1 [cs.CV])","link":"http://arxiv.org/abs/2202.13094","description":"<p>3D point clouds deep learning is a promising field of research that allows a\nneural network to learn features of point clouds directly, making it a robust\ntool for solving 3D scene understanding tasks. While recent works show that\npoint cloud convolutions can be invariant to translation and point permutation,\ninvestigations of the rotation invariance property for point cloud convolution\nhas been so far scarce. Some existing methods perform point cloud convolutions\nwith rotation-invariant features, existing methods generally do not perform as\nwell as translation-invariant only counterpart. In this work, we argue that a\nkey reason is that compared to point coordinates, rotation-invariant features\nconsumed by point cloud convolution are not as distinctive. To address this\nproblem, we propose a simple yet effective convolution operator that enhances\nfeature distinction by designing powerful rotation invariant features from the\nlocal regions. We consider the relationship between the point of interest and\nits neighbors as well as the internal relationship of the neighbors to largely\nimprove the feature descriptiveness. Our network architecture can capture both\nlocal and global context by simply tuning the neighborhood size in each\nconvolution layer. We conduct several experiments on synthetic and real-world\npoint cloud classifications, part segmentation, and shape retrieval to evaluate\nour method, which achieves the state-of-the-art accuracy under challenging\nrotations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhiyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_B/0/1/0/all/0/1\">Binh-Son Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeung_S/0/1/0/all/0/1\">Sai-Kit Yeung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continuous Human Action Recognition for Human-Machine Interaction: A Review. (arXiv:2202.13096v1 [cs.CV])","link":"http://arxiv.org/abs/2202.13096","description":"<p>With advances in data-driven machine learning research, a wide variety of\nprediction models have been proposed to capture spatio-temporal features for\nthe analysis of video streams. Recognising actions and detecting action\ntransitions within an input video are challenging but necessary tasks for\napplications that require real-time human-machine interaction. By reviewing a\nlarge body of recent related work in the literature, we thoroughly analyse,\nexplain and compare action segmentation methods and provide details on the\nfeature extraction and learning strategies that are used on most\nstate-of-the-art methods. We cover the impact of the performance of object\ndetection and tracking techniques on human action segmentation methodologies.\nWe investigate the application of such models to real-world scenarios and\ndiscuss several limitations and key research directions towards improving\ninterpretability, generalisation, optimisation and deployment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gammulle_H/0/1/0/all/0/1\">Harshala Gammulle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmedt_Aristizabal_D/0/1/0/all/0/1\">David Ahmedt-Aristizabal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denman_S/0/1/0/all/0/1\">Simon Denman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tychsen_Smith_L/0/1/0/all/0/1\">Lachlan Tychsen-Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petersson_L/0/1/0/all/0/1\">Lars Petersson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fookes_C/0/1/0/all/0/1\">Clinton Fookes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Symmetric Convolutional Filters: A Novel Way to Constrain Parameters in CNN. (arXiv:2202.13099v1 [cs.CV])","link":"http://arxiv.org/abs/2202.13099","description":"<p>We propose a novel technique to constrain parameters in CNN based on\nsymmetric filters. We investigate the impact on SOTA networks when varying the\ncombinations of symmetricity. We demonstrate that our models offer effective\ngeneralisation and a structured elimination of redundancy in parameters. We\nconclude by comparing our method with other pruning techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_H/0/1/0/all/0/1\">Harish Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+T%2E_S/0/1/0/all/0/1\">Sumana T.</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nandy_S/0/1/0/all/0/1\">S.K. Nandy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Supervision: Enabling Generalization over Output Spaces. (arXiv:2202.13100v1 [cs.LG])","link":"http://arxiv.org/abs/2202.13100","description":"<p>In this paper, we propose Semantic Supervision (SemSup) - a unified paradigm\nfor training classifiers that generalize over output spaces. In contrast to\nstandard classification, which treats classes as discrete symbols, SemSup\nrepresents them as dense vector features obtained from descriptions of classes\n(e.g., \"The cat is a small carnivorous mammal\"). This allows the output space\nto be unbounded (in the space of descriptions) and enables models to generalize\nboth over unseen inputs and unseen outputs (e.g. \"The aardvark is a nocturnal\nburrowing mammal with long ears\"). Specifically, SemSup enables four types of\ngeneralization, to -- (1) unseen class descriptions, (2) unseen classes, (3)\nunseen super-classes, and (4) unseen tasks. Through experiments on four\nclassification datasets across two variants (multi-class and multi-label), two\ninput modalities (text and images), and two output description modalities (text\nand JSON), we show that our SemSup models significantly outperform standard\nsupervised models and existing models that leverage word embeddings over class\nnames. For instance, our model outperforms baselines by 40% and 20% precision\npoints on unseen descriptions and classes, respectively, on a news\ncategorization dataset (RCV1). SemSup can serve as a pathway for scaling neural\nmodels to large unbounded output spaces and enabling better generalization and\nmodel reuse for unseen tasks and domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hanjie_A/0/1/0/all/0/1\">Austin W. Hanjie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deshpande_A/0/1/0/all/0/1\">Ameet Deshpande</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narasimhan_K/0/1/0/all/0/1\">Karthik Narasimhan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analysis of Visual Reasoning on One-Stage Object Detection. (arXiv:2202.13115v1 [cs.CV])","link":"http://arxiv.org/abs/2202.13115","description":"<p>Current state-of-the-art one-stage object detectors are limited by treating\neach image region separately without considering possible relations of the\nobjects. This causes dependency solely on high-quality convolutional feature\nrepresentations for detecting objects successfully. However, this may not be\npossible sometimes due to some challenging conditions. In this paper, the usage\nof reasoning features on one-stage object detection is analyzed. We attempted\ndifferent architectures that reason the relations of the image regions by using\nself-attention. YOLOv3-Reasoner2 model spatially and semantically enhances\nfeatures in the reasoning layer and fuses them with the original convolutional\nfeatures to improve performance. The YOLOv3-Reasoner2 model achieves around\n2.5% absolute improvement with respect to baseline YOLOv3 on COCO in terms of\nmAP while still running in real-time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aksoy_T/0/1/0/all/0/1\">Tolga Aksoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Halici_U/0/1/0/all/0/1\">Ugur Halici</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Unsupervised Cross-Modal Hashing Method Robust to Noisy Training Image-Text Correspondences in Remote Sensing. (arXiv:2202.13117v1 [cs.CV])","link":"http://arxiv.org/abs/2202.13117","description":"<p>The development of accurate and scalable cross-modal image-text retrieval\nmethods, where queries from one modality (e.g., text) can be matched to archive\nentries from another (e.g., remote sensing image) has attracted great attention\nin remote sensing (RS). Most of the existing methods assume that a reliable\nmulti-modal training set with accurately matched text-image pairs is existing.\nHowever, this assumption may not always hold since the multi-modal training\nsets may include noisy pairs (i.e., textual descriptions/captions associated to\ntraining images can be noisy), distorting the learning process of the retrieval\nmethods. To address this problem, we propose a novel unsupervised cross-modal\nhashing method robust to the noisy image-text correspondences (CHNR). CHNR\nconsists of three modules: 1) feature extraction module, which extracts feature\nrepresentations of image-text pairs; 2) noise detection module, which detects\npotential noisy correspondences; and 3) hashing module that generates\ncross-modal binary hash codes. The proposed CHNR includes two training phases:\ni) meta-learning phase that uses a small portion of clean (i.e., reliable) data\nto train the noise detection module in an adversarial fashion; and ii) the main\ntraining phase for which the trained noise detection module is used to identify\nnoisy correspondences while the hashing module is trained on the noisy\nmulti-modal training set. Experimental results show that the proposed CHNR\noutperforms state-of-the-art methods. Our code is publicly available at\nhttps://git.tu-berlin.de/rsim/chnr\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mikriukov_G/0/1/0/all/0/1\">Georgii Mikriukov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravanbakhsh_M/0/1/0/all/0/1\">Mahdyar Ravanbakhsh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demir_B/0/1/0/all/0/1\">Beg&#xfc;m Demir</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Accurate Human Body Reconstruction for Volumetric Video. (arXiv:2202.13118v1 [cs.CV])","link":"http://arxiv.org/abs/2202.13118","description":"<p>In this work, we enhance a professional end-to-end volumetric video\nproduction pipeline to achieve high-fidelity human body reconstruction using\nonly passive cameras. While current volumetric video approaches estimate depth\nmaps using traditional stereo matching techniques, we introduce and optimize\ndeep learning-based multi-view stereo networks for depth map estimation in the\ncontext of professional volumetric video reconstruction. Furthermore, we\npropose a novel depth map post-processing approach including filtering and\nfusion, by taking into account photometric confidence, cross-view geometric\nconsistency, foreground masks as well as camera viewing frustums. We show that\nour method can generate high levels of geometric detail for reconstructed human\nbodies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Decai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Worchel_M/0/1/0/all/0/1\">Markus Worchel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feldmann_I/0/1/0/all/0/1\">Ingo Feldmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schreer_O/0/1/0/all/0/1\">Oliver Schreer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eisert_P/0/1/0/all/0/1\">Peter Eisert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Person Re-identification: A Retrospective on Domain Specific Open Challenges and Future Trends. (arXiv:2202.13121v1 [cs.CV])","link":"http://arxiv.org/abs/2202.13121","description":"<p>Person re-identification (Re-ID) is one of the primary components of an\nautomated visual surveillance system. It aims to automatically identify/search\npersons in a multi-camera network having non-overlapping field-of-views. Owing\nto its potential in various applications and research significance, a plethora\nof deep learning based re-Id approaches have been proposed in the recent years.\nHowever, there exist several vision related challenges, e.g., occlusion, pose\nscale \\&amp; viewpoint variance, background clutter, person misalignment and\ncross-domain generalization across camera modalities, which makes the problem\nof re-Id still far from being solved. Majority of the proposed approaches\ndirectly or indirectly aim to solve one or multiple of these existing\nchallenges. In this context, a comprehensive review of current re-ID approaches\nin solving theses challenges is needed to analyze and focus on particular\naspects for further advancements. At present, such a focused review does not\nexist and henceforth in this paper, we have presented a systematic\nchallenge-specific literature survey of 230+ papers between the years of\n2015-21. For the first time a survey of this type have been presented where the\nperson re-Id approaches are reviewed in such solution-oriented perspective.\nMoreover, we have presented several diversified prominent developing trends in\nthe respective research domain which will provide a visionary perspective\nregarding ongoing person re-Id research and eventually help to develop\npractical real world solutions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zahra_A/0/1/0/all/0/1\">Asmat Zahra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perwaiz_N/0/1/0/all/0/1\">Nazia Perwaiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahzad_M/0/1/0/all/0/1\">Muhammad Shahzad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fraz_M/0/1/0/all/0/1\">Muhammad Moazam Fraz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Content-Variant Reference Image Quality Assessment via Knowledge Distillation. (arXiv:2202.13123v1 [cs.CV])","link":"http://arxiv.org/abs/2202.13123","description":"<p>Generally, humans are more skilled at perceiving differences between\nhigh-quality (HQ) and low-quality (LQ) images than directly judging the quality\nof a single LQ image. This situation also applies to image quality assessment\n(IQA). Although recent no-reference (NR-IQA) methods have made great progress\nto predict image quality free from the reference image, they still have the\npotential to achieve better performance since HQ image information is not fully\nexploited. In contrast, full-reference (FR-IQA) methods tend to provide more\nreliable quality evaluation, but its practicability is affected by the\nrequirement for pixel-level aligned reference images. To address this, we\nfirstly propose the content-variant reference method via knowledge distillation\n(CVRKD-IQA). Specifically, we use non-aligned reference (NAR) images to\nintroduce various prior distributions of high-quality images. The comparisons\nof distribution differences between HQ and LQ images can help our model better\nassess the image quality. Further, the knowledge distillation transfers more\nHQ-LQ distribution difference information from the FR-teacher to the\nNAR-student and stabilizing CVRKD-IQA performance. Moreover, to fully mine the\nlocal-global combined information, while achieving faster inference speed, our\nmodel directly processes multiple image patches from the input with the\nMLP-mixer. Cross-dataset experiments verify that our model can outperform all\nNAR/NR-IQA SOTAs, even reach comparable performance with FR-IQA methods on some\noccasions. Since the content-variant and non-aligned reference HQ images are\neasy to obtain, our model can support more IQA applications with its relative\nrobustness to content variations. Our code and more detailed elaborations of\nsupplements are available: https://github.com/guanghaoyin/CVRKD-IQA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yin_G/0/1/0/all/0/1\">Guanghao Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zehuan Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1\">Chuchu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_W/0/1/0/all/0/1\">Wei Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1\">Shouqian Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Changhu Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-image Super-resolution via Quality Map Associated Temporal Attention Network. (arXiv:2202.13124v1 [eess.IV])","link":"http://arxiv.org/abs/2202.13124","description":"<p>With the rising interest in deep learning-based methods in remote sensing,\nneural networks have made remarkable advancements in multi-image fusion and\nsuper-resolution. To fully exploit the advantages of multi-image\nsuper-resolution, temporal attention is crucial as it allows a model to focus\non reliable features rather than noises. Despite the presence of quality maps\n(QMs) that indicate noises in images, most of the methods tested in the PROBA-V\ndataset have not been used QMs for temporal attention. We present a quality map\nassociated temporal attention network (QA-Net), a novel method that\nincorporates QMs into both feature representation and fusion processes for the\nfirst time. Low-resolution features are temporally attended by QM features in\nrepeated multi-head attention modules. The proposed method achieved\nstate-of-the-art results in the PROBA-V dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lee_M/0/1/0/all/0/1\">Minji Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Koo_I/0/1/0/all/0/1\">Inyong Koo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ko_K/0/1/0/all/0/1\">Kangwook Ko</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_C/0/1/0/all/0/1\">Changick Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Nonlinear Discrete Optimisation of Reversible Steganographic Coding. (arXiv:2202.13133v1 [cs.CV])","link":"http://arxiv.org/abs/2202.13133","description":"<p>Authentication mechanisms are at the forefront of defending the world from\nvarious types of cybercrime. Steganography can serve as an authentication\nsolution by embedding a digital signature into a carrier object to ensure the\nintegrity of the object and simultaneously lighten the burden of metadata\nmanagement. However, steganographic distortion, albeit generally imperceptible\nto human sensory systems, might be inadmissible in fidelity-sensitive\nsituations. This has led to the concept of reversible steganography. A\nfundamental element of reversible steganography is predictive analytics, for\nwhich powerful neural network models have been effectively deployed. As another\ncore aspect, contemporary reversible steganographic coding is based primarily\non heuristics and therefore worth further study. While attempts have been made\nto realise automatic coding with neural networks, perfect reversibility is\nstill unreachable via such an unexplainable intelligent machinery. Instead of\nrelying on deep learning, we aim to derive an optimal coding by means of\nmathematical optimisation. In this study, we formulate reversible\nsteganographic coding as a nonlinear discrete optimisation problem with a\nlogarithmic capacity constraint and a quadratic distortion objective.\nLinearisation techniques are developed to enable mixed-integer linear\nprogramming. Experimental results validate the near-optimality of the proposed\noptimisation algorithm benchmarked against a brute-force method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1\">Ching-Chun Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RONELDv2: A faster, improved lane tracking method. (arXiv:2202.13137v1 [cs.CV])","link":"http://arxiv.org/abs/2202.13137","description":"<p>Lane detection is an integral part of control systems in autonomous vehicles\nand lane departure warning systems as lanes are a key component of the\noperating environment for road vehicles. In a previous paper, a robust neural\nnetwork output enhancement for active lane detection (RONELD) method augmenting\ndeep learning lane detection models to improve active, or ego, lane accuracy\nperformance was presented. This paper extends the work by further investigating\nthe lane tracking methods used to increase robustness of the method to lane\nchanges and different lane dimensions (e.g. lane marking thickness) and\nproposes an improved, lighter weight lane detection method, RONELDv2. It\nimproves on the previous RONELD method by detecting the lane point variance,\nmerging lanes to find a more accurate set of lane parameters, and using an\nexponential moving average method to calculate more robust lane weights.\nExperiments using the proposed improvements show a consistent increase in lane\ndetection accuracy results across different datasets and deep learning models,\nas well as a decrease in computational complexity observed via an up to\ntwo-fold decrease in runtime, which enhances its suitability for real-time use\non autonomous vehicles and lane departure warning systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chng_Z/0/1/0/all/0/1\">Zhe Ming Chng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lew_J/0/1/0/all/0/1\">Joseph Mun Hung Lew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jimmy Addison Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Blind Image Super Resolution with Semantic-Aware Quantized Texture Prior. (arXiv:2202.13142v1 [cs.CV])","link":"http://arxiv.org/abs/2202.13142","description":"<p>A key challenge of blind image super resolution is to recover realistic\ntextures for low-resolution images with unknown degradations. Most recent works\ncompletely rely on the generative ability of GANs, which are difficult to\ntrain. Other methods resort to high-resolution image references that are\nusually not available. In this work, we propose a novel framework, denoted as\nQuanTexSR, to restore realistic textures with the Quantized Texture Priors\nencoded in Vector Quantized GAN. The QuanTexSR generates textures by aligning\nthe textureless content features to the quantized feature vectors, i.e., a\npretrained feature codebook. Specifically, QuanTexSR formulates the texture\ngeneration as a feature matching problem between textureless features and a\npretrained feature codebook. The final textures are then generated by the\nquantized features from the codebook. Since features in the codebook have shown\nthe ability to generate natural textures in the pretrain stage, QuanTexSR can\ngenerate rich and realistic textures with the pretrained codebook as texture\npriors. Moreover, we propose a semantic regularization technique that\nregularizes the pre-training of the codebook using clusters of features\nextracted from the pretrained VGG19 network. This further improves texture\ngeneration with semantic context. Experiments demonstrate that the proposed\nQuanTexSR can generate competitive or better textures than previous approaches.\nCode will be made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chaofeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xinyu Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yipeng Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xiaoguang Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Tao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Shihui Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DGSS : Domain Generalized Semantic Segmentation using Iterative Style Mining and Latent Representation Alignment. (arXiv:2202.13144v1 [cs.CV])","link":"http://arxiv.org/abs/2202.13144","description":"<p>Semantic segmentation algorithms require access to well-annotated datasets\ncaptured under diverse illumination conditions to ensure consistent\nperformance. However, poor visibility conditions at varying illumination\nconditions result in laborious and error-prone labeling. Alternatively, using\nsynthetic samples to train segmentation algorithms has gained interest with the\ndrawback of domain gap that results in sub-optimal performance. While current\nstate-of-the-art (SoTA) have proposed different mechanisms to bridge the domain\ngap, they still perform poorly in low illumination conditions with an average\nperformance drop of - 10.7 mIOU. In this paper, we focus upon single source\ndomain generalization to overcome the domain gap and propose a two-step\nframework wherein we first identify an adversarial style that maximizes the\ndomain gap between stylized and source images. Subsequently, these stylized\nimages are used to categorically align features such that features belonging to\nthe same class are clustered together in latent space, irrespective of domain\ngap. Furthermore, to increase intra-class variance while training, we propose a\nstyle mixing mechanism wherein the same objects from different styles are mixed\nto construct a new training image. This framework allows us to achieve a domain\ngeneralized semantic segmentation algorithm with consistent performance without\nprior information of the target domain while relying on a single source. Based\non extensive experiments, we match SoTA performance on SYNTHIA $\\to$\nCityscapes, GTAV $\\to$ Cityscapes while setting new SoTA on GTAV $\\to$ Dark\nZurich and GTAV $\\to$ Night Driving benchmarks without retraining.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shyam_P/0/1/0/all/0/1\">Pranjay Shyam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bangunharcana_A/0/1/0/all/0/1\">Antyanta Bangunharcana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_K/0/1/0/all/0/1\">Kuk-Jin Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kyung-Soo Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pix2NeRF: Unsupervised Conditional $\\pi$-GAN for Single Image to Neural Radiance Fields Translation. (arXiv:2202.13162v1 [cs.CV])","link":"http://arxiv.org/abs/2202.13162","description":"<p>We propose a pipeline to generate Neural Radiance Fields~(NeRF) of an object\nor a scene of a specific class, conditioned on a single input image. This is a\nchallenging task, as training NeRF requires multiple views of the same scene,\ncoupled with corresponding poses, which are hard to obtain. Our method is based\non $\\pi$-GAN, a generative model for unconditional 3D-aware image synthesis,\nwhich maps random latent codes to radiance fields of a class of objects. We\njointly optimize (1) the $\\pi$-GAN objective to utilize its high-fidelity\n3D-aware generation and (2) a carefully designed reconstruction objective. The\nlatter includes an encoder coupled with $\\pi$-GAN generator to form an\nauto-encoder. Unlike previous few-shot NeRF approaches, our pipeline is\nunsupervised, capable of being trained with independent images without 3D,\nmulti-view, or pose supervision. Applications of our pipeline include 3d avatar\ngeneration, object-centric novel view synthesis with a single input image, and\n3d-aware super-resolution, to name a few.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_S/0/1/0/all/0/1\">Shengqu Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Obukhov_A/0/1/0/all/0/1\">Anton Obukhov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1\">Dengxin Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Edge Augmentation for Large-Scale Sketch Recognition without Sketches. (arXiv:2202.13164v1 [cs.CV])","link":"http://arxiv.org/abs/2202.13164","description":"<p>This work addresses scaling up the sketch classification task into a large\nnumber of categories. Collecting sketches for training is a slow and tedious\nprocess that has so far precluded any attempts to large-scale sketch\nrecognition. We overcome the lack of training sketch data by exploiting labeled\ncollections of natural images that are easier to obtain. To bridge the domain\ngap we present a novel augmentation technique that is tailored to the task of\nlearning sketch recognition from a training set of natural images.\nRandomization is introduced in the parameters of edge detection and edge\nselection. Natural images are translated to a pseudo-novel domain called\n\"randomized Binary Thin Edges\" (rBTE), which is used as a training domain\ninstead of natural images. The ability to scale up is demonstrated by training\nCNN-based sketch recognition of more than 2.5 times larger number of categories\nthan used previously. For this purpose, a dataset of natural images from 874\ncategories is constructed by combining a number of popular computer vision\ndatasets. The categories are selected to be suitable for sketch recognition. To\nestimate the performance, a subset of 393 categories with sketches is also\ncollected.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Efthymiadis_N/0/1/0/all/0/1\">Nikos Efthymiadis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tolias_G/0/1/0/all/0/1\">Giorgos Tolias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chum_O/0/1/0/all/0/1\">Ondrej Chum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Domain Adaptive Salient Object Detection Through Uncertainty-Aware Pseudo-Label Learning. (arXiv:2202.13170v1 [cs.CV])","link":"http://arxiv.org/abs/2202.13170","description":"<p>Recent advances in deep learning significantly boost the performance of\nsalient object detection (SOD) at the expense of labeling larger-scale\nper-pixel annotations. To relieve the burden of labor-intensive labeling, deep\nunsupervised SOD methods have been proposed to exploit noisy labels generated\nby handcrafted saliency methods. However, it is still difficult to learn\naccurate saliency details from rough noisy labels. In this paper, we propose to\nlearn saliency from synthetic but clean labels, which naturally has higher\npixel-labeling quality without the effort of manual annotations. Specifically,\nwe first construct a novel synthetic SOD dataset by a simple copy-paste\nstrategy. Considering the large appearance differences between the synthetic\nand real-world scenarios, directly training with synthetic data will lead to\nperformance degradation on real-world scenarios. To mitigate this problem, we\npropose a novel unsupervised domain adaptive SOD method to adapt between these\ntwo domains by uncertainty-aware self-training. Experimental results show that\nour proposed method outperforms the existing state-of-the-art deep unsupervised\nSOD methods on several benchmark datasets, and is even comparable to\nfully-supervised ones.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_P/0/1/0/all/0/1\">Pengxiang Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Ziyi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Mengmeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_K/0/1/0/all/0/1\">Kun Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Liang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guanbin Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Relational Surrogate Loss Learning. (arXiv:2202.13197v1 [cs.LG])","link":"http://arxiv.org/abs/2202.13197","description":"<p>Evaluation metrics in machine learning are often hardly taken as loss\nfunctions, as they could be non-differentiable and non-decomposable, e.g.,\naverage precision and F1 score. This paper aims to address this problem by\nrevisiting the surrogate loss learning, where a deep neural network is employed\nto approximate the evaluation metrics. Instead of pursuing an exact recovery of\nthe evaluation metric through a deep neural network, we are reminded of the\npurpose of the existence of these evaluation metrics, which is to distinguish\nwhether one model is better or worse than another. In this paper, we show that\ndirectly maintaining the relation of models between surrogate losses and\nmetrics suffices, and propose a rank correlation-based optimization method to\nmaximize this relation and learn surrogate losses. Compared to previous works,\nour method is much easier to optimize and enjoys significant efficiency and\nperformance gains. Extensive experiments show that our method achieves\nimprovements on various tasks including image classification and neural machine\ntranslation, and even outperforms state-of-the-art methods on human pose\nestimation and machine reading comprehension tasks. Code is available at:\nhttps://github.com/hunto/ReLoss.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Tao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zekang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Hua Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1\">Yong Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shusheng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yang Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_S/0/1/0/all/0/1\">Shan You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chang Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dropout can Simulate Exponential Number of Models for Sample Selection Techniques. (arXiv:2202.13203v1 [cs.LG])","link":"http://arxiv.org/abs/2202.13203","description":"<p>Following Coteaching, generally in the literature, two models are used in\nsample selection based approaches for training with noisy labels. Meanwhile, it\nis also well known that Dropout when present in a network trains an ensemble of\nsub-networks. We show how to leverage this property of Dropout to train an\nexponential number of shared models, by training a single model with Dropout.\nWe show how we can modify existing two model-based sample selection\nmethodologies to use an exponential number of shared models. Not only is it\nmore convenient to use a single model with Dropout, but this approach also\ncombines the natural benefits of Dropout with that of training an exponential\nnumber of models, leading to improved results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lakshya/0/1/0/all/0/1\">Lakshya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How much depth information can radar infer and contribute. (arXiv:2202.13220v1 [cs.CV])","link":"http://arxiv.org/abs/2202.13220","description":"<p>Since the release of radar data in large scale autonomous driving dataset,\nmany works have been proposed fusing radar data as an additional guidance\nsignal into monocular depth estimation models. Although positive performances\nare reported, it is still hard to tell how much depth information radar can\ninfer and contribute in depth estimation models. In this paper, we conduct two\nexperiments to investigate the intrinsic depth capability of radar data using\nstate-of-the-art depth estimation models. Our experiments demonstrate that the\nestimated depth from only sparse radar input can detect the shape of\nsurroundings to a certain extent. Furthermore, the monocular depth estimation\nmodel supervised by preprocessed radar only during training can achieve 70%\nperformance in delta_1 score compared to the baseline model trained with sparse\nlidar.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lo_C/0/1/0/all/0/1\">Chen-Chou Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vandewalle_P/0/1/0/all/0/1\">Patrick Vandewalle</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Orientation-Discriminative Feature Representation for Decentralized Pedestrian Tracking. (arXiv:2202.13237v1 [cs.CV])","link":"http://arxiv.org/abs/2202.13237","description":"<p>This paper focuses on the problem of decentralized pedestrian tracking using\na sensor network. Traditional works on pedestrian tracking usually use a\ncentralized framework, which becomes less practical for robotic applications\ndue to limited communication bandwidth. Our paper proposes a\ncommunication-efficient, orientation-discriminative feature representation to\ncharacterize pedestrian appearance information, that can be shared among\nsensors. Building upon that representation, our work develops a cross-sensor\ntrack association approach to achieve decentralized tracking. Extensive\nevaluations are conducted on publicly available datasets and results show that\nour proposed approach leads to improved performance in multi-sensor tracking.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shree_V/0/1/0/all/0/1\">Vikram Shree</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diaz_Ruiz_C/0/1/0/all/0/1\">Carlos Diaz-Ruiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hariharan_B/0/1/0/all/0/1\">Bharath Hariharan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Campbell_M/0/1/0/all/0/1\">Mark Campbell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On-chip QNN: Towards Efficient On-Chip Training of Quantum Neural Networks. (arXiv:2202.13239v1 [quant-ph])","link":"http://arxiv.org/abs/2202.13239","description":"<p>Quantum Neural Network (QNN) is drawing increasing research interest thanks\nto its potential to achieve quantum advantage on near-term Noisy Intermediate\nScale Quantum (NISQ) hardware. In order to achieve scalable QNN learning, the\ntraining process needs to be offloaded to real quantum machines instead of\nusing exponential-cost classical simulators. One common approach to obtain QNN\ngradients is parameter shift whose cost scales linearly with the number of\nqubits. We present On-chip QNN, the first experimental demonstration of\npractical on-chip QNN training with parameter shift. Nevertheless, we find that\ndue to the significant quantum errors (noises) on real machines, gradients\nobtained from naive parameter shift have low fidelity and thus degrade the\ntraining accuracy. To this end, we further propose probabilistic gradient\npruning to firstly identify gradients with potentially large errors and then\nremove them. Specifically, small gradients have larger relative errors than\nlarge ones, thus having a higher probability to be pruned. We perform extensive\nexperiments on 5 classification tasks with 5 real quantum machines. The results\ndemonstrate that our on-chip training achieves over 90% and 60% accuracy for\n2-class and 4-class image classification tasks. The probabilistic gradient\npruning brings up to 7% QNN accuracy improvements over no pruning. Overall, we\nsuccessfully obtain similar on-chip training accuracy compared with noise-free\nsimulation but have much better training scalability. The code for parameter\nshift on-chip training is available in the TorchQuantum library.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/quant-ph/1/au:+Wang_H/0/1/0/all/0/1\">Hanrui Wang</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Li_Z/0/1/0/all/0/1\">Zirui Li</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Gu_J/0/1/0/all/0/1\">Jiaqi Gu</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Ding_Y/0/1/0/all/0/1\">Yongshan Ding</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Pan_D/0/1/0/all/0/1\">David Z. Pan</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Han_S/0/1/0/all/0/1\">Song Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Supervising Remote Sensing Change Detection Models with 3D Surface Semantics. (arXiv:2202.13251v1 [cs.CV])","link":"http://arxiv.org/abs/2202.13251","description":"<p>Remote sensing change detection, identifying changes between scenes of the\nsame location, is an active area of research with a broad range of\napplications. Recent advances in multimodal self-supervised pretraining have\nresulted in state-of-the-art methods which surpass vision models trained solely\non optical imagery. In the remote sensing field, there is a wealth of\noverlapping 2D and 3D modalities which can be exploited to supervise\nrepresentation learning in vision models. In this paper we propose Contrastive\nSurface-Image Pretraining (CSIP) for joint learning using optical RGB and above\nground level (AGL) map pairs. We then evaluate these pretrained models on\nseveral building segmentation and change detection datasets to show that our\nmethod does, in fact, extract features relevant to downstream applications\nwhere natural and artificial surface information is relevant.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Corley_I/0/1/0/all/0/1\">Isaac Corley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Najafirad_P/0/1/0/all/0/1\">Peyman Najafirad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Next-Best-View Prediction for Active Stereo Cameras and Highly Reflective Objects. (arXiv:2202.13263v1 [cs.CV])","link":"http://arxiv.org/abs/2202.13263","description":"<p>Depth acquisition with the active stereo camera is a challenging task for\nhighly reflective objects. When setup permits, multi-view fusion can provide\nincreased levels of depth completion. However, due to the slow acquisition\nspeed of high-end active stereo cameras, collecting a large number of\nviewpoints for a single scene is generally not practical. In this work, we\npropose a next-best-view framework to strategically select camera viewpoints\nfor completing depth data on reflective objects. In particular, we explicitly\nmodel the specular reflection of reflective surfaces based on the Phong\nreflection model and a photometric response function. Given the object CAD\nmodel and grayscale image, we employ an RGB-based pose estimator to obtain\ncurrent pose predictions from the existing data, which is used to form\npredicted surface normal and depth hypotheses, and allows us to then assess the\ninformation gain from a subsequent frame for any candidate viewpoint. Using\nthis formulation, we implement an active perception pipeline which is evaluated\non a challenging real-world dataset. The evaluation results demonstrate that\nour active depth acquisition method outperforms two strong baselines for both\ndepth completion and object pose estimation performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waslander_S/0/1/0/all/0/1\">Steven L. Waslander</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Texture Characterization of Histopathologic Images Using Ecological Diversity Measures and Discrete Wavelet Transform. (arXiv:2202.13270v1 [cs.CV])","link":"http://arxiv.org/abs/2202.13270","description":"<p>Breast cancer is a health problem that affects mainly the female population.\nAn early detection increases the chances of effective treatment, improving the\nprognosis of the disease. In this regard, computational tools have been\nproposed to assist the specialist in interpreting the breast digital image\nexam, providing features for detecting and diagnosing tumors and cancerous\ncells. Nonetheless, detecting tumors with a high sensitivity rate and reducing\nthe false positives rate is still challenging. Texture descriptors have been\nquite popular in medical image analysis, particularly in histopathologic images\n(HI), due to the variability of both the texture found in such images and the\ntissue appearance due to irregularity in the staining process. Such variability\nmay exist depending on differences in staining protocol such as fixation,\ninconsistency in the staining condition, and reagents, either between\nlaboratories or in the same laboratory. Textural feature extraction for\nquantifying HI information in a discriminant way is challenging given the\ndistribution of intrinsic properties of such images forms a non-deterministic\ncomplex system. This paper proposes a method for characterizing texture across\nHIs with a considerable success rate. By employing ecological diversity\nmeasures and discrete wavelet transform, it is possible to quantify the\nintrinsic properties of such images with promising accuracy on two HI datasets\ncompared with state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ataky_S/0/1/0/all/0/1\">Steve Tsham Mpinda Ataky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koerich_A/0/1/0/all/0/1\">Alessandro Lameiras Koerich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Dual Neighborhood Hypergraph Neural Network for Change Detection in VHR Remote Sensing Images. (arXiv:2202.13275v1 [cs.CV])","link":"http://arxiv.org/abs/2202.13275","description":"<p>The very high spatial resolution (VHR) remote sensing images have been an\nextremely valuable source for monitoring changes occurred on the earth surface.\nHowever, precisely detecting relevant changes in VHR images still remains a\nchallenge, due to the complexity of the relationships among ground objects. To\naddress this limitation, a dual neighborhood hypergraph neural network is\nproposed in this article, which combines the multiscale superpixel segmentation\nand hypergraph convolution to model and exploit the complex relationships.\nFirst, the bi-temporal image pairs are segmented under two scales and fed to a\npre-trained U-net to obtain node features by treating each object under the\nfine scale as a node. The dual neighborhood is then defined using the\nfather-child and adjacent relationships of the segmented objects to construct\nthe hypergraph, which permits models to represent the higher-order structured\ninformation far more complex than just pairwise relationships. The hypergraph\nconvolutions are conducted on the constructed hypergraph to propagate the label\ninformation from a small amount of labeled nodes to the other unlabeled ones by\nthe node-edge-node transform. Moreover, to alleviate the problem of imbalanced\nsample, the focal loss function is adopted to train the hypergraph neural\nnetwork. The experimental results on optical, SAR and heterogeneous optical/SAR\ndata sets demonstrate that the proposed method comprises better effectiveness\nand robustness compared to many state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Junzheng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_R/0/1/0/all/0/1\">Ruigang Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_W/0/1/0/all/0/1\">Weiping Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_K/0/1/0/all/0/1\">Kenan Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Biao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yuli Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Computer Vision-assisted Approach to Automated Real-Time Road Infrastructure Management. (arXiv:2202.13285v1 [cs.CV])","link":"http://arxiv.org/abs/2202.13285","description":"<p>Accurate automated detection of road pavement distresses is critical for the\ntimely identification and repair of potentially accident-inducing road hazards\nsuch as potholes and other surface-level asphalt cracks. Deployment of such a\nsystem would be further advantageous in low-resource environments where lack of\ngovernment funding for infrastructure maintenance typically entails heightened\nrisks of potentially fatal vehicular road accidents as a result of inadequate\nand infrequent manual inspection of road systems for road hazards. To remedy\nthis, a recent research initiative organized by the Institute of Electrical and\nElectronics Engineers (\"IEEE\") as part of their 2020 Global Road Damage\nDetection (\"GRDC\") Challenge published in May 2020 a novel 21,041 annotated\nimage dataset of various road distresses calling upon academic and other\nresearchers to submit innovative deep learning-based solutions to these road\nhazard detection problems. Making use of this dataset, we propose a supervised\nobject detection approach leveraging You Only Look Once (\"YOLO\") and the Faster\nR-CNN frameworks to detect and classify road distresses in real-time via a\nvehicle dashboard-mounted smartphone camera, producing 0.68 F1-score\nexperimental results ranking in the top 5 of 121 teams that entered this\nchallenge as of December 2021.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Heitzmann_P/0/1/0/all/0/1\">Philippe Heitzmann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DXM-TransFuse U-net: Dual Cross-Modal Transformer Fusion U-net for Automated Nerve Identification. (arXiv:2202.13304v1 [eess.IV])","link":"http://arxiv.org/abs/2202.13304","description":"<p>Accurate nerve identification is critical during surgical procedures for\npreventing any damages to nerve tissues. Nerve injuries can lead to long-term\ndetrimental effects for patients as well as financial overburdens. In this\nstudy, we develop a deep-learning network framework using the U-Net\narchitecture with a Transformer block based fusion module at the bottleneck to\nidentify nerve tissues from a multi-modal optical imaging system. By leveraging\nand extracting the feature maps of each modality independently and using each\nmodalities information for cross-modal interactions, we aim to provide a\nsolution that would further increase the effectiveness of the imaging systems\nfor enabling the noninvasive intraoperative nerve identification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Xie_B/0/1/0/all/0/1\">Baijun Xie</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Milam_G/0/1/0/all/0/1\">Gary Milam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ning_B/0/1/0/all/0/1\">Bo Ning</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cha_J/0/1/0/all/0/1\">Jaepyeong Cha</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Park_C/0/1/0/all/0/1\">Chung Hyuk Park</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attention-based Cross-Layer Domain Alignment for Unsupervised Domain Adaptation. (arXiv:2202.13310v1 [cs.CV])","link":"http://arxiv.org/abs/2202.13310","description":"<p>Unsupervised domain adaptation (UDA) aims to learn transferable knowledge\nfrom a labeled source domain and adapts a trained model to an unlabeled target\ndomain. To bridge the gap between source and target domains, one prevailing\nstrategy is to minimize the distribution discrepancy by aligning their semantic\nfeatures extracted by deep models. The existing alignment-based methods mainly\nfocus on reducing domain divergence in the same model layer. However, the same\nlevel of semantic information could distribute across model layers due to the\ndomain shifts. To further boost model adaptation performance, we propose a\nnovel method called Attention-based Cross-layer Domain Alignment (ACDA), which\ncaptures the semantic relationship between the source and target domains across\nmodel layers and calibrates each level of semantic information automatically\nthrough a dynamic attention mechanism. An elaborate attention mechanism is\ndesigned to reweight each cross-layer pair based on their semantic similarity\nfor precise domain alignment, effectively matching each level of semantic\ninformation during model adaptation. Extensive experiments on multiple\nbenchmark datasets consistently show that the proposed method ACDA yields\nstate-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Junkun Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yen-wei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_R/0/1/0/all/0/1\">Ruofeng Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Lanfen Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Efficient End-to-End 3D Model Reconstructionbased on Neural Architecture Search. (arXiv:2202.13313v1 [cs.CV])","link":"http://arxiv.org/abs/2202.13313","description":"<p>Using neural networks to represent 3D objects has become popular. However,\nmany previous works employ neural networks with fixed architecture and size to\nrepresent different 3D objects, which lead to excessive network parameters for\nsimple objects and limited reconstruction accuracy for complex objects. For\neach 3D model, it is desirable to have an end-to-end neural network with as few\nparameters as possible to achieve high-fidelity reconstruction. In this paper,\nwe propose an efficient model reconstruction method utilizing neural\narchitecture search (NAS) and binary classification. Taking the number of\nlayers, the number of nodes in each layer, and the activation function of each\nlayer as the search space, a specific network architecture can be obtained\nbased on reinforcement learning technology. Furthermore, to get rid of the\ntraditional surface reconstruction algorithms (e.g., marching cube) used after\nnetwork inference, we complete the end-to-end network by classifying binary\nvoxels. Compared to other signed distance field (SDF) prediction or binary\nclassification networks, our method achieves significantly higher\nreconstruction accuracy using fewer network parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yongdong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuanzhan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1\">Xulong Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Siyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_S/0/1/0/all/0/1\">Shen Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1\">Ting Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuqi Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Topology-Preserving Segmentation Network: A Deep Learning Segmentation Framework for Connected Component. (arXiv:2202.13331v1 [eess.IV])","link":"http://arxiv.org/abs/2202.13331","description":"<p>Medical image segmentation, which aims to automatically extract anatomical or\npathological structures, plays a key role in computer-aided diagnosis and\ndisease analysis. Despite the problem has been widely studied, existing methods\nare prone to topological errors. In medical imaging, the topology of the\nstructure, such as the kidney or lung, is usually known. Preserving the\ntopology of the structure in the segmentation process is of utmost importance\nfor accurate image analysis. In this work, a novel learning-based segmentation\nmodel is proposed. A {\\it topology-preserving segmentation network (TPSN)} is\ntrained to give an accurate segmentation result of an input image that\npreserves the prescribed topology. TPSN is a deformation-based model that\nyields a deformation map through a UNet, which takes the medical image and a\ntemplate mask as inputs. The main idea is to deform a template mask describing\nthe prescribed topology by a diffeomorphism to segment the object in the image.\nThe topology of the shape in the template mask is well preserved under the\ndiffeomorphic map. The diffeomorphic property of the map is controlled by\nintroducing a regularization term related to the Jacobian in the loss function.\nAs such, a topology-preserving segmentation result can be guaranteed.\nFurthermore, a multi-scale TPSN is developed in this paper that incorporates\nmulti-level information of images to produce more precise segmentation results.\nTo evaluate our method, we applied the 2D TPSN on Ham10000 and 3D TPSN on\nKiTS21. Experimental results illustrate our method outperforms the baseline\nUNet segmentation model with/without connected-component analysis (CCA) by both\nthe dice score and IoU score. Besides, results show that our method can produce\nreliable results even in challenging cases, where pixel-wise segmentation\nmodels by UNet and CCA fail to obtain accurate results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1\">Han Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lui_L/0/1/0/all/0/1\">Lok Ming Lui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data Overlap: A Prerequisite For Disentanglement. (arXiv:2202.13341v1 [cs.LG])","link":"http://arxiv.org/abs/2202.13341","description":"<p>Learning disentangled representations with variational autoencoders (VAEs) is\noften attributed to the regularisation component of the loss. In this work, we\nhighlight the interaction between data and the reconstruction term of the loss\nas the main contributor to disentanglement in VAEs. We note that standardised\nbenchmark datasets are constructed in a way that is conducive to learning what\nappear to be disentangled representations. We design an intuitive adversarial\ndataset that exploits this mechanism to break existing state-of-the-art\ndisentanglement frameworks. Finally, we provide solutions in the form of a\nmodified reconstruction loss suggesting that VAEs are accidental distance\nlearners.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Michlo_N/0/1/0/all/0/1\">Nathan Michlo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+James_S/0/1/0/all/0/1\">Steven James</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klein_R/0/1/0/all/0/1\">Richard Klein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Robust Multimodal Remote Sensing Image Registration Method and System Using Steerable Filters with First- and Second-order Gradients. (arXiv:2202.13347v1 [cs.CV])","link":"http://arxiv.org/abs/2202.13347","description":"<p>Co-registration of multimodal remote sensing images is still an ongoing\nchallenge because of nonlinear radiometric differences (NRD) and significant\ngeometric distortions (e.g., scale and rotation changes) between these images.\nIn this paper, a robust matching method based on the Steerable filters is\nproposed consisting of two critical steps. First, to address severe NRD, a\nnovel structural descriptor named the Steerable Filters of first- and\nsecond-Order Channels (SFOC) is constructed, which combines the first- and\nsecond-order gradient information by using the steerable filters with a\nmulti-scale strategy to depict more discriminative structure features of\nimages. Then, a fast similarity measure is established called Fast Normalized\nCross-Correlation (Fast-NCCSFOC), which employs the Fast Fourier Transform\ntechnique and the integral image to improve the matching efficiency.\nFurthermore, to achieve reliable registration performance, a coarse-to-fine\nmultimodal registration system is designed consisting of two pivotal modules.\nThe local coarse registration is first conducted by involving both detection of\ninterest points (IPs) and local geometric correction, which effectively\nutilizes the prior georeferencing information of RS images to address global\ngeometric distortions. In the fine registration stage, the proposed SFOC is\nused to resist significant NRD, and to detect control points between multimodal\nimages by a template matching scheme. The performance of the proposed matching\nmethod has been evaluated with many different kinds of multimodal RS images.\nThe results show its superior matching performance compared with the\nstate-of-the-art methods. Moreover, the designed registration system also\noutperforms the popular commercial software in both registration accuracy and\ncomputational efficiency. Our system is available at\nhttps://github.com/yeyuanxin110.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1\">Yuanxin Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1\">Bai Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_T/0/1/0/all/0/1\">Tengfeng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qizhi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guo Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Self-Supervised LiDAR Odometry via Representative Structure Discovery and 3D Inherent Error Modeling. (arXiv:2202.13353v1 [cs.CV])","link":"http://arxiv.org/abs/2202.13353","description":"<p>The correct ego-motion estimation basically relies on the understanding of\ncorrespondences between adjacent LiDAR scans. However, given the complex\nscenarios and the low-resolution LiDAR, finding reliable structures for\nidentifying correspondences can be challenging. In this paper, we delve into\nstructure reliability for accurate self-supervised ego-motion estimation and\naim to alleviate the influence of unreliable structures in training, inference\nand mapping phases. We improve the self-supervised LiDAR odometry substantially\nfrom three aspects: 1) A two-stage odometry estimation network is developed,\nwhere we obtain the ego-motion by estimating a set of sub-region\ntransformations and averaging them with a motion voting mechanism, to encourage\nthe network focusing on representative structures. 2) The inherent alignment\nerrors, which cannot be eliminated via ego-motion optimization, are\ndown-weighted in losses based on the 3D point covariance estimations. 3) The\ndiscovered representative structures and learned point covariances are\nincorporated in the mapping module to improve the robustness of map\nconstruction. Our two-frame odometry outperforms the previous state of the arts\nby 16%/12% in terms of translational/rotational errors on the KITTI dataset and\nperforms consistently well on the Apollo-Southbay datasets. We can even rival\nthe fully supervised counterparts with our mapping module and more unlabeled\ntraining data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Junyi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jianping Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guofeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaogang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly Supervised Learning for cell recognition in immunohistochemical cytoplasm staining images. (arXiv:2202.13372v1 [eess.IV])","link":"http://arxiv.org/abs/2202.13372","description":"<p>Cell classification and counting in immunohistochemical cytoplasm staining\nimages play a pivotal role in cancer diagnosis. Weakly supervised learning is a\npotential method to deal with labor-intensive labeling. However, the inconstant\ncell morphology and subtle differences between classes also bring challenges.\nTo this end, we present a novel cell recognition framework based on multi-task\nlearning, which utilizes two additional auxiliary tasks to guide robust\nrepresentation learning of the main task. To deal with misclassification, the\ntissue prior learning branch is introduced to capture the spatial\nrepresentation of tumor cells without additional tissue annotation. Moreover,\ndynamic masks and consistency learning are adopted to learn the invariance of\ncell scale and shape. We have evaluated our framework on immunohistochemical\ncytoplasm staining images, and the results demonstrate that our method\noutperforms recent cell recognition approaches. Besides, we have also done some\nablation studies to show significant improvements after adding the auxiliary\nbranches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_S/0/1/0/all/0/1\">Shichuan Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_C/0/1/0/all/0/1\">Chenglu Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1\">Honglin Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cai_J/0/1/0/all/0/1\">Jiatong Cai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_L/0/1/0/all/0/1\">Lin Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Meta-RangeSeg: LiDAR Sequence Semantic Segmentation Using Multiple Feature Aggregation. (arXiv:2202.13377v1 [cs.CV])","link":"http://arxiv.org/abs/2202.13377","description":"<p>LiDAR sensor is essential to the perception system in autonomous vehicles and\nintelligent robots. To fulfill the real-time requirements in real-world\napplications, it is necessary to efficiently segment the LiDAR scans. Most of\nprevious approaches directly project 3D point cloud onto the 2D spherical range\nimage so that they can make use of the efficient 2D convolutional operations\nfor image segmentation. Although having achieved the encouraging results, the\nneighborhood information is not well-preserved in the spherical projection.\nMoreover, the temporal information is not taken into consideration in the\nsingle scan segmentation task. To tackle these problems, we propose a novel\napproach to semantic segmentation for LiDAR sequences named Meta-RangeSeg,\nwhere a novel range residual image representation is introduced to capture the\nspatial-temporal information. Specifically, Meta-Kernel is employed to extract\nthe meta features, which reduces the inconsistency between the 2D range image\ncoordinates input and Cartesian coordinates output. An efficient U-Net backbone\nis used to obtain the multi-scale features. Furthermore, Feature Aggregation\nModule (FAM) aggregates the meta features and multi-scale features, which tends\nto strengthen the role of range channel. We have conducted extensive\nexperiments for performance evaluation on SemanticKITTI, which is the de-facto\ndataset for LiDAR semantic segmentation. The promising results show that our\nproposed Meta-RangeSeg method is more efficient and effective than the existing\napproaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Song Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jianke Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruixiang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PanoFlow: Learning Optical Flow for Panoramic Images. (arXiv:2202.13388v1 [cs.CV])","link":"http://arxiv.org/abs/2202.13388","description":"<p>Optical flow estimation is a basic task in self-driving and robotics systems,\nwhich enables to temporally interpret the traffic scene. Autonomous vehicles\nclearly benefit from the ultra-wide Field of View (FoV) offered by 360-degree\npanoramic sensors. However, due to the unique imaging process of panoramic\nimages, models designed for pinhole images do not directly generalize\nsatisfactorily to 360-degree panoramic images. In this paper, we put forward a\nnovel network framework--PanoFlow, to learn optical flow for panoramic images.\nTo overcome the distortions introduced by equirectangular projection in\npanoramic transformation, we design a Flow Distortion Augmentation (FDA)\nmethod. We further propose a Cyclic Flow Estimation (CFE) method by leveraging\nthe cyclicity of spherical images to infer 360-degree optical flow and\nconverting large displacement to relatively small displacement. PanoFlow is\napplicable to any existing flow estimation method and benefit from the progress\nof narrow-FoV flow estimation. In addition, we create and release a synthetic\npanoramic dataset Flow360 based on CARLA to facilitate training and\nquantitative analysis. PanoFlow achieves state-of-the-art performance. Our\nproposed approach reduces the End-Point-Error (EPE) on the established Flow360\ndataset by 26%. On the public OmniFlowNet dataset, PanoFlow achieves an EPE of\n3.34 pixels, a 53.1% error reduction from the best published result (7.12\npixels). We also validate our method via an outdoor collection vehicle,\nindicating strong potential and robustness for real-world navigation\napplications. Code and dataset are publicly available at\nhttps://github.com/MasterHow/PanoFlow.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Hao Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yifan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kailun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1\">Yaozu Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1\">Xiaoting Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1\">Zhe Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_S/0/1/0/all/0/1\">Shi Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kaiwei Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformer-based Knowledge Distillation for Efficient Semantic Segmentation of Road-driving Scenes. (arXiv:2202.13393v1 [cs.CV])","link":"http://arxiv.org/abs/2202.13393","description":"<p>For scene understanding in robotics and automated driving, there is a growing\ninterest in solving semantic segmentation tasks with transformer-based methods.\nHowever, effective transformers are always too cumbersome and computationally\nexpensive to solve semantic segmentation in real time, which is desired for\nrobotic systems. Moreover, due to the lack of inductive biases compared to\nConvolutional Neural Networks (CNNs), pre-training on a large dataset is\nessential but it takes a long time. Knowledge Distillation (KD) speeds up\ninference and maintains accuracy while transferring knowledge from a\npre-trained cumbersome teacher model to a compact student model. Most\ntraditional KD methods for CNNs focus on response-based knowledge and\nfeature-based knowledge. In contrast, we present a novel KD framework according\nto the nature of transformers, i.e., training compact transformers by\ntransferring the knowledge from feature maps and patch embeddings of large\ntransformers. To this purpose, two modules are proposed: (1) the Selective\nKernel Fusion (SKF) module, which helps to construct an efficient\nrelation-based KD framework, Selective Kernel Review (SKR); (2) the Patch\nEmbedding Alignment (PEA) module, which performs the dimensional transformation\nof patch embeddings. The combined KD framework is called SKR+PEA. Through\ncomprehensive experiments on Cityscapes and ACDC datasets, it indicates that\nour proposed approach outperforms recent state-of-the-art KD frameworks and\nrivals the time-consuming pre-training method. Code will be made publicly\navailable at https://github.com/RuipingL/SKR_PEA.git\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Ruiping Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kailun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Huayao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiaming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_K/0/1/0/all/0/1\">Kunyu Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stiefelhagen_R/0/1/0/all/0/1\">Rainer Stiefelhagen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Concept Graph Neural Networks for Surgical Video Understanding. (arXiv:2202.13402v1 [cs.CV])","link":"http://arxiv.org/abs/2202.13402","description":"<p>We constantly integrate our knowledge and understanding of the world to\nenhance our interpretation of what we see.\n</p>\n<p>This ability is crucial in application domains which entail reasoning about\nmultiple entities and concepts, such as AI-augmented surgery. In this paper, we\npropose a novel way of integrating conceptual knowledge into temporal analysis\ntasks via temporal concept graph networks. In the proposed networks, a global\nknowledge graph is incorporated into the temporal analysis of surgical\ninstances, learning the meaning of concepts and relations as they apply to the\ndata. We demonstrate our results in surgical video data for tasks such as\nverification of critical view of safety, as well as estimation of Parkland\ngrading scale. The results show that our method improves the recognition and\ndetection of complex benchmarks as well as enables other analytic applications\nof interest.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ban_Y/0/1/0/all/0/1\">Yutong Ban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eckhoff_J/0/1/0/all/0/1\">Jennifer A. Eckhoff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ward_T/0/1/0/all/0/1\">Thomas M. Ward</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashimoto_D/0/1/0/all/0/1\">Daniel A. Hashimoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meireles_O/0/1/0/all/0/1\">Ozanan R. Meireles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rus_D/0/1/0/all/0/1\">Daniela Rus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosman_G/0/1/0/all/0/1\">Guy Rosman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Multimodal German Dataset for Automatic Lip Reading Systems and Transfer Learning. (arXiv:2202.13403v1 [cs.CV])","link":"http://arxiv.org/abs/2202.13403","description":"<p>Large datasets as required for deep learning of lip reading do not exist in\nmany languages. In this paper we present the dataset GLips (German Lips)\nconsisting of 250,000 publicly available videos of the faces of speakers of the\nHessian Parliament, which was processed for word-level lip reading using an\nautomatic pipeline. The format is similar to that of the English language LRW\n(Lip Reading in the Wild) dataset, with each video encoding one word of\ninterest in a context of 1.16 seconds duration, which yields compatibility for\nstudying transfer learning between both datasets. By training a deep neural\nnetwork, we investigate whether lip reading has language-independent features,\nso that datasets of different languages can be used to improve lip reading\nmodels. We demonstrate learning from scratch and show that transfer learning\nfrom LRW to GLips and vice versa improves learning speed and performance, in\nparticular for the validation set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schwiebert_G/0/1/0/all/0/1\">Gerald Schwiebert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weber_C/0/1/0/all/0/1\">Cornelius Weber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1\">Leyuan Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siqueira_H/0/1/0/all/0/1\">Henrique Siqueira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wermter_S/0/1/0/all/0/1\">Stefan Wermter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Meta-path Analysis on Spatio-Temporal Graphs for Pedestrian Trajectory Prediction. (arXiv:2202.13427v1 [cs.LG])","link":"http://arxiv.org/abs/2202.13427","description":"<p>Spatio-temporal graphs (ST-graphs) have been used to model time series tasks\nsuch as traffic forecasting, human motion modeling, and action recognition. The\nhigh-level structure and corresponding features from ST-graphs have led to\nimproved performance over traditional architectures. However, current methods\ntend to be limited by simple features, despite the rich information provided by\nthe full graph structure, which leads to inefficiencies and suboptimal\nperformance in downstream tasks. We propose the use of features derived from\nmeta-paths, walks across different types of edges, in ST-graphs to improve the\nperformance of Structural Recurrent Neural Network. In this paper, we present\nthe Meta-path Enhanced Structural Recurrent Neural Network (MESRNN), a generic\nframework that can be applied to any spatio-temporal task in a simple and\nscalable manner. We employ MESRNN for pedestrian trajectory prediction,\nutilizing these meta-path based features to capture the relationships between\nthe trajectories of pedestrians at different points in time and space. We\ncompare our MESRNN against state-of-the-art ST-graph methods on standard\ndatasets to show the performance boost provided by meta-path information. The\nproposed model consistently outperforms the baselines in trajectory prediction\nover long time horizons by over 32\\%, and produces more socially compliant\ntrajectories in dense crowds. For more information please refer to the project\nwebsite at https://sites.google.com/illinois.edu/mesrnn/home.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hasan_A/0/1/0/all/0/1\">Aamir Hasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sriram_P/0/1/0/all/0/1\">Pranav Sriram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Driggs_Campbell_K/0/1/0/all/0/1\">Katherine Driggs-Campbell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Unified Wasserstein Distributional Robustness Framework for Adversarial Training. (arXiv:2202.13437v1 [cs.LG])","link":"http://arxiv.org/abs/2202.13437","description":"<p>It is well-known that deep neural networks (DNNs) are susceptible to\nadversarial attacks, exposing a severe fragility of deep learning systems. As\nthe result, adversarial training (AT) method, by incorporating adversarial\nexamples during training, represents a natural and effective approach to\nstrengthen the robustness of a DNN-based classifier. However, most AT-based\nmethods, notably PGD-AT and TRADES, typically seek a pointwise adversary that\ngenerates the worst-case adversarial example by independently perturbing each\ndata sample, as a way to \"probe\" the vulnerability of the classifier. Arguably,\nthere are unexplored benefits in considering such adversarial effects from an\nentire distribution. To this end, this paper presents a unified framework that\nconnects Wasserstein distributional robustness with current state-of-the-art AT\nmethods. We introduce a new Wasserstein cost function and a new series of risk\nfunctions, with which we show that standard AT methods are special cases of\ntheir counterparts in our framework. This connection leads to an intuitive\nrelaxation and generalization of existing AT methods and facilitates the\ndevelopment of a new family of distributional robustness AT-based algorithms.\nExtensive experiments show that our distributional robustness AT algorithms\nrobustify further their standard AT counterparts in various settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1\">Tuan Anh Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1\">Trung Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_Q/0/1/0/all/0/1\">Quan Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">He Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phung_D/0/1/0/all/0/1\">Dinh Phung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Application of DatasetGAN in medical imaging: preliminary studies. (arXiv:2202.13463v1 [cs.CV])","link":"http://arxiv.org/abs/2202.13463","description":"<p>Generative adversarial networks (GANs) have been widely investigated for many\npotential applications in medical imaging. DatasetGAN is a recently proposed\nframework based on modern GANs that can synthesize high-quality segmented\nimages while requiring only a small set of annotated training images. The\nsynthesized annotated images could be potentially employed for many medical\nimaging applications, where images with segmentation information are required.\nHowever, to the best of our knowledge, there are no published studies focusing\non its applications to medical imaging. In this work, preliminary studies were\nconducted to investigate the utility of DatasetGAN in medical imaging. Three\nimprovements were proposed to the original DatasetGAN framework, considering\nthe unique characteristics of medical images. The synthesized segmented images\nby DatasetGAN were visually evaluated. The trained DatasetGAN was further\nanalyzed by evaluating the performance of a pre-defined image segmentation\ntechnique, which was trained by the use of the synthesized datasets. The\neffectiveness, concerns, and potential usage of DatasetGAN were discussed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1\">Zong Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kelkar_V/0/1/0/all/0/1\">Varun Kelkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anastasio_M/0/1/0/all/0/1\">Mark A. Anastasio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hua Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Synergistic Network Learning and Label Correction for Noise-robust Image Classification. (arXiv:2202.13472v1 [cs.CV])","link":"http://arxiv.org/abs/2202.13472","description":"<p>Large training datasets almost always contain examples with inaccurate or\nincorrect labels. Deep Neural Networks (DNNs) tend to overfit training label\nnoise, resulting in poorer model performance in practice. To address this\nproblem, we propose a robust label correction framework combining the ideas of\nsmall loss selection and noise correction, which learns network parameters and\nreassigns ground truth labels iteratively. Taking the expertise of DNNs to\nlearn meaningful patterns before fitting noise, our framework first trains two\nnetworks over the current dataset with small loss selection. Based on the\nclassification loss and agreement loss of two networks, we can measure the\nconfidence of training data. More and more confident samples are selected for\nlabel correction during the learning process. We demonstrate our method on both\nsynthetic and real-world datasets with different noise types and rates,\nincluding CIFAR-10, CIFAR-100 and Clothing1M, where our method outperforms the\nbaseline approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gong_C/0/1/0/all/0/1\">Chen Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bin_K/0/1/0/all/0/1\">Kong Bin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seibel_E/0/1/0/all/0/1\">Eric J. Seibel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1\">Youbing Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Q/0/1/0/all/0/1\">Qi Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Spectral Bias of Polynomial Neural Networks. (arXiv:2202.13473v1 [cs.LG])","link":"http://arxiv.org/abs/2202.13473","description":"<p>Polynomial neural networks (PNNs) have been recently shown to be particularly\neffective at image generation and face recognition, where high-frequency\ninformation is critical. Previous studies have revealed that neural networks\ndemonstrate a $\\textit{spectral bias}$ towards low-frequency functions, which\nyields faster learning of low-frequency components during training. Inspired by\nsuch studies, we conduct a spectral analysis of the Neural Tangent Kernel (NTK)\nof PNNs. We find that the $\\Pi$-Net family, i.e., a recently proposed\nparametrization of PNNs, speeds up the learning of the higher frequencies. We\nverify the theoretical bias through extensive experiments. We expect our\nanalysis to provide novel insights into designing architectures and learning\nframeworks by incorporating multiplicative interactions via polynomials.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choraria_M/0/1/0/all/0/1\">Moulik Choraria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dadi_L/0/1/0/all/0/1\">Leello Tadesse Dadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chrysos_G/0/1/0/all/0/1\">Grigorios Chrysos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mairal_J/0/1/0/all/0/1\">Julien Mairal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cevher_V/0/1/0/all/0/1\">Volkan Cevher</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interpretable Concept-based Prototypical Networks for Few-Shot Learning. (arXiv:2202.13474v1 [cs.LG])","link":"http://arxiv.org/abs/2202.13474","description":"<p>Few-shot learning aims at recognizing new instances from classes with limited\nsamples. This challenging task is usually alleviated by performing\nmeta-learning on similar tasks. However, the resulting models are black-boxes.\nThere has been growing concerns about deploying black-box machine learning\nmodels and FSL is not an exception in this regard. In this paper, we propose a\nmethod for FSL based on a set of human-interpretable concepts. It constructs a\nset of metric spaces associated with the concepts and classifies samples of\nnovel classes by aggregating concept-specific decisions. The proposed method\ndoes not require concept annotations for query samples. This interpretable\nmethod achieved results on a par with six previously state-of-the-art black-box\nFSL methods on the CUB fine-grained bird classification dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zarei_M/0/1/0/all/0/1\">Mohammad Reza Zarei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Komeili_M/0/1/0/all/0/1\">Majid Komeili</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Point Label Aware Superpixels for Multi-species Segmentation of Underwater Imagery. (arXiv:2202.13487v1 [cs.CV])","link":"http://arxiv.org/abs/2202.13487","description":"<p>Monitoring coral reefs using underwater vehicles increases the range of\nmarine surveys and availability of historical ecological data by collecting\nsignificant quantities of images. Analysis of this imagery can be automated\nusing a model trained to perform semantic segmentation, however it is too\ncostly and time-consuming to densely label images for training supervised\nmodels. In this letter, we leverage photo-quadrat imagery labeled by ecologists\nwith sparse point labels. We propose a point label aware method for propagating\nlabels within superpixel regions to obtain augmented ground truth for training\na semantic segmentation model. Our point label aware superpixel method utilizes\nthe sparse point labels, and clusters pixels using learned features to\naccurately generate single-species segments in cluttered, complex coral images.\nOur method outperforms prior methods on the UCSD Mosaics dataset by 3.62% for\npixel accuracy and 8.35% for mean IoU for the label propagation task.\nFurthermore, our approach reduces computation time reported by previous\napproaches by 76%. We train a DeepLabv3+ architecture and outperform\nstate-of-the-art for semantic segmentation by 2.91% for pixel accuracy and\n9.65% for mean IoU on the UCSD Mosaics dataset and by 4.19% for pixel accuracy\nand 14.32% mean IoU for the Eilat dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Raine_S/0/1/0/all/0/1\">Scarlett Raine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marchant_R/0/1/0/all/0/1\">Ross Marchant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kusy_B/0/1/0/all/0/1\">Brano Kusy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maire_F/0/1/0/all/0/1\">Frederic Maire</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fischer_T/0/1/0/all/0/1\">Tobias Fischer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Globally Optimal Boresight Alignment of UAV-LiDAR Systems. (arXiv:2202.13501v1 [cs.CV])","link":"http://arxiv.org/abs/2202.13501","description":"<p>In airborne light detection and ranging (LiDAR) systems, misalignments\nbetween the LiDAR-scanner and the inertial navigation system (INS) mounted on\nan unmanned aerial vehicle (UAV)'s frame can lead to inaccurate 3D point\nclouds. Determining the orientation offset, or boresight error is key to many\nLiDAR-based applications. In this work, we introduce a mixed-integer\nquadratically constrained quadratic program (MIQCQP) that can globally solve\nthis misalignment problem. We also propose a nested spatial branch and bound\n(nsBB) algorithm that improves computational performance. The nsBB relies on\nnovel preprocessing steps that progressively reduce the problem size. In\naddition, an adaptive grid search (aGS) allowing us to obtain quick heuristic\nsolutions is presented. Our algorithms are open-source, multi-threaded and\nmulti-machine compatible.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gopinath_S/0/1/0/all/0/1\">Smitha Gopinath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hijazi_H/0/1/0/all/0/1\">Hassan L. Hijazi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collins_A/0/1/0/all/0/1\">Adam Collins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lemons_J/0/1/0/all/0/1\">Julian Dann Nathan Lemons</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schultz_Fellenz_E/0/1/0/all/0/1\">Emily Schultz-Fellenz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bent_R/0/1/0/all/0/1\">Russell Bent</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hijazi_A/0/1/0/all/0/1\">Amira Hijazi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riemersma_G/0/1/0/all/0/1\">Gert Riemersma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ESW Edge-Weights : Ensemble Stochastic Watershed Edge-Weights for Hyperspectral Image Classification. (arXiv:2202.13502v1 [cs.CV])","link":"http://arxiv.org/abs/2202.13502","description":"<p>Hyperspectral image (HSI) classification is a topic of active research. One\nof the main challenges of HSI classification is the lack of reliable labelled\nsamples. Various semi-supervised and unsupervised classification methods are\nproposed to handle the low number of labelled samples. Chief among them are\ngraph convolution networks (GCN) and their variants. These approaches exploit\nthe graph structure for semi-supervised and unsupervised classification. While\nseveral of these methods implicitly construct edge-weights, to our knowledge,\nnot much work has been done to estimate the edge-weights explicitly. In this\narticle, we estimate the edge-weights explicitly and use them for the\ndownstream classification tasks - both semi-supervised and unsupervised. The\nproposed edge-weights are based on two key insights - (a) Ensembles reduce the\nvariance and (b) Classes in HSI datasets and feature similarity have only\none-sided implications. That is, while same classes would have similar\nfeatures, similar features do not necessarily imply the same classes.\nExploiting these, we estimate the edge-weights using an aggregate of ensembles\nof watersheds over subsamples of features. These edge weights are evaluated for\nboth semi-supervised and unsupervised classification tasks. The evaluation for\nsemi-supervised tasks uses Random-Walk based approach. For the unsupervised\ncase, we use a simple filter using a graph convolution network (GCN). In both\nthese cases, the proposed edge weights outperform the traditional approaches to\ncompute edge-weights - Euclidean distances and cosine similarities.\nFascinatingly, with the proposed edge-weights, the simplest GCN obtained\nresults comparable to the recent state-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_R/0/1/0/all/0/1\">Rohan Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aziz_A/0/1/0/all/0/1\">Aman Aziz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnan_A/0/1/0/all/0/1\">Aditya Suraj Krishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Challa_A/0/1/0/all/0/1\">Aditya Challa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danda_S/0/1/0/all/0/1\">Sravan Danda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cyber Mobility Mirror: Deep Learning-based Real-time 3D Object Perception and Reconstruction Using Roadside LiDAR. (arXiv:2202.13505v1 [cs.CV])","link":"http://arxiv.org/abs/2202.13505","description":"<p>Enabling Cooperative Driving Automation (CDA) requires high-fidelity and\nreal-time perception information, which is available from onboard sensors or\nvehicle-to-everything (V2X) communications. Nevertheless, the accessibility of\nthis information may suffer from the range and occlusion of perception or\nlimited penetration rates in connectivity. In this paper, we introduce the\nprototype of Cyber Mobility Mirror (CMM), a next-generation real-time traffic\nsurveillance system for 3D object detection, classification, tracking, and\nreconstruction, to provide CAVs with wide-range high-fidelity perception\ninformation in a mixed traffic environment. The CMM system consists of six main\ncomponents: 1) the data pre-processor to retrieve and pre-process raw data from\nthe roadside LiDAR; 2) the 3D object detector to generate 3D bounding boxes\nbased on point cloud data; 3) the multi-objects tracker to endow unique IDs to\ndetected objects and estimate their dynamic states; 4) the global locator to\nmap positioning information from the LiDAR coordinate to geographic coordinate\nusing coordinate transformation; 5) the cloud-based communicator to transmit\nperception information from roadside sensors to equipped vehicles; and 6) the\nonboard advisor to reconstruct and display the real-time traffic conditions via\nGraphical User Interface (GUI). In this study, a field-operational prototype\nsystem is deployed at a real-world intersection, University Avenue and Iowa\nAvenue in Riverside, California to assess the feasibility and performance of\nour CMM system. Results from field tests demonstrate that our CMM prototype\nsystem can provide satisfactory perception performance with 96.99% precision\nand 83.62% recall. High-fidelity real-time traffic conditions (at the object\nlevel) can be displayed on the GUI of the equipped vehicle with a frequency of\n3-4 Hz.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_Z/0/1/0/all/0/1\">Zhengwei Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nayak_S/0/1/0/all/0/1\">Saswat Priyadarshi Nayak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xuanpeng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1\">Guoyuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barth_M/0/1/0/all/0/1\">Matthew J. Barth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1\">Xuewei Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yongkang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oguchi_K/0/1/0/all/0/1\">Kentaro Oguchi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GRAPHITE: Generating Automatic Physical Examples for Machine-Learning Attacks on Computer Vision Systems. (arXiv:2002.07088v6 [cs.CR] UPDATED)","link":"http://arxiv.org/abs/2002.07088","description":"<p>This paper investigates an adversary's ease of attack in generating\nadversarial examples for real-world scenarios. We address three key\nrequirements for practical attacks for the real-world: 1) automatically\nconstraining the size and shape of the attack so it can be applied with\nstickers, 2) transform-robustness, i.e., robustness of a attack to\nenvironmental physical variations such as viewpoint and lighting changes, and\n3) supporting attacks in not only white-box, but also black-box hard-label\nscenarios, so that the adversary can attack proprietary models. In this work,\nwe propose GRAPHITE, an efficient and general framework for generating attacks\nthat satisfy the above three key requirements. GRAPHITE takes advantage of\ntransform-robustness, a metric based on expectation over transforms (EoT), to\nautomatically generate small masks and optimize with gradient-free\noptimization. GRAPHITE is also flexible as it can easily trade-off\ntransform-robustness, perturbation size, and query count in black-box settings.\nOn a GTSRB model in a hard-label black-box setting, we are able to find attacks\non all possible 1,806 victim-target class pairs with averages of 77.8%\ntransform-robustness, perturbation size of 16.63% of the victim images, and\n126K queries per pair. For digital-only attacks where achieving\ntransform-robustness is not a requirement, GRAPHITE is able to find successful\nsmall-patch attacks with an average of only 566 queries for 92.2% of\nvictim-target pairs. GRAPHITE is also able to find successful attacks using\nperturbations that modify small areas of the input image against PatchGuard, a\nrecently proposed defense against patch-based attacks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_R/0/1/0/all/0/1\">Ryan Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mangaokar_N/0/1/0/all/0/1\">Neal Mangaokar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiefeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandes_E/0/1/0/all/0/1\">Earlence Fernandes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1\">Somesh Jha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prakash_A/0/1/0/all/0/1\">Atul Prakash</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Character Labeling in Movie Videos: Data Resources and Self-supervised Feature Adaptation. (arXiv:2008.11289v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2008.11289","description":"<p>Robust face clustering is a vital step in enabling computational\nunderstanding of visual character portrayal in media. Face clustering for\nlong-form content is challenging because of variations in appearance and lack\nof supporting large-scale labeled data. Our work in this paper focuses on two\nkey aspects of this problem: the lack of domain-specific training or benchmark\ndatasets, and adapting face embeddings learned on web images to long-form\ncontent, specifically movies. First, we present a dataset of over 169,000 face\ntracks curated from 240 Hollywood movies with weak labels on whether a pair of\nface tracks belong to the same or a different character. We propose an offline\nalgorithm based on nearest-neighbor search in the embedding space to mine\nhard-examples from these tracks. We then investigate triplet-loss and multiview\ncorrelation-based methods for adapting face embeddings to hard-examples. Our\nexperimental results highlight the usefulness of weakly labeled data for\ndomain-specific feature adaptation. Overall, we find that multiview\ncorrelation-based adaptation yields more discriminative and robust face\nembeddings. Its performance on downstream face verification and clustering\ntasks is comparable to that of the state-of-the-art results in this domain. We\nalso present the SAIL-Movie Character Benchmark corpus developed to augment\nexisting benchmarks. It consists of racially diverse actors and provides\nface-quality labels for subsequent error analysis. We hope that the large-scale\ndatasets developed in this work can further advance automatic character\nlabeling in videos. All resources are available freely at\nhttps://sail.usc.edu/~ccmi/multiface.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Somandepalli_K/0/1/0/all/0/1\">Krishna Somandepalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hebbar_R/0/1/0/all/0/1\">Rajat Hebbar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayanan_S/0/1/0/all/0/1\">Shrikanth Narayanan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ray-based classification framework for high-dimensional data. (arXiv:2010.00500v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2010.00500","description":"<p>While classification of arbitrary structures in high dimensions may require\ncomplete quantitative information, for simple geometrical structures,\nlow-dimensional qualitative information about the boundaries defining the\nstructures can suffice. Rather than using dense, multi-dimensional data, we\npropose a deep neural network (DNN) classification framework that utilizes a\nminimal collection of one-dimensional representations, called \\emph{rays}, to\nconstruct the \"fingerprint\" of the structure(s) based on substantially reduced\ninformation. We empirically study this framework using a synthetic dataset of\ndouble and triple quantum dot devices and apply it to the classification\nproblem of identifying the device state. We show that the performance of the\nray-based classifier is already on par with traditional 2D images for low\ndimensional systems, while significantly cutting down the data acquisition\ncost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zwolak_J/0/1/0/all/0/1\">Justyna P. Zwolak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalantre_S/0/1/0/all/0/1\">Sandesh S. Kalantre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McJunkin_T/0/1/0/all/0/1\">Thomas McJunkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weber_B/0/1/0/all/0/1\">Brian J. Weber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taylor_J/0/1/0/all/0/1\">Jacob M. Taylor</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Classification of Single-View Object Point Clouds. (arXiv:2012.10042v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.10042","description":"<p>Object point cloud classification has drawn great research attention since\nthe release of benchmarking datasets, such as the ModelNet and the ShapeNet.\nThese benchmarks assume point clouds covering complete surfaces of object\ninstances, for which plenty of high-performing methods have been developed.\nHowever, their settings deviate from those often met in practice, where, due to\n(self-)occlusion, a point cloud covering partial surface of an object is\ncaptured from an arbitrary view. We show in this paper that performance of\nexisting point cloud classifiers drops drastically under the considered\nsingle-view, partial setting; the phenomenon is consistent with the observation\nthat semantic category of a partial object surface is less ambiguous only when\nits distribution on the whole surface is clearly specified. To this end, we\nargue for a single-view, partial setting where supervised learning of object\npose estimation should be accompanied with classification. Technically, we\npropose a baseline method of Pose-Accompanied Point cloud classification\nNetwork (PAPNet); built upon SE(3)-equivariant convolutions, the PAPNet learns\nintermediate pose transformations for equivariant features defined on vector\nfields, which makes the subsequent classification easier (ideally) in the\ncategory-level, canonical pose. By adapting existing ModelNet40 and ScanNet\ndatasets to the single-view, partial setting, experiment results can verify the\nnecessity of object pose estimation and superiority of our PAPNet to existing\nclassifiers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zelin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Ke Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kangjun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1\">Changxing Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaowei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_K/0/1/0/all/0/1\">Kui Jia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Memory-Augmented Reinforcement Learning for Image-Goal Navigation. (arXiv:2101.05181v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.05181","description":"<p>In this work, we present a memory-augmented approach for image-goal\nnavigation. Earlier attempts, including RL-based and SLAM-based approaches have\neither shown poor generalization performance, or are heavily-reliant on\npose/depth sensors. Our method is based on an attention-based end-to-end model\nthat leverages an episodic memory to learn to navigate. First, we train a\nstate-embedding network in a self-supervised fashion, and then use it to embed\npreviously-visited states into the agent's memory. Our navigation policy takes\nadvantage of this information through an attention mechanism. We validate our\napproach with extensive evaluations, and show that our model establishes a new\nstate of the art on the challenging Gibson dataset. Furthermore, we achieve\nthis impressive performance from RGB input alone, without access to additional\ninformation such as position or depth, in stark contrast to related work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mezghani_L/0/1/0/all/0/1\">Lina Mezghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sukhbaatar_S/0/1/0/all/0/1\">Sainbayar Sukhbaatar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lavril_T/0/1/0/all/0/1\">Thibaut Lavril</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maksymets_O/0/1/0/all/0/1\">Oleksandr Maksymets</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batra_D/0/1/0/all/0/1\">Dhruv Batra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bojanowski_P/0/1/0/all/0/1\">Piotr Bojanowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alahari_K/0/1/0/all/0/1\">Karteek Alahari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A DCNN-based Arbitrarily-Oriented Object Detector for Quality Control and Inspection Application. (arXiv:2101.07383v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.07383","description":"<p>Following the success of machine vision systems for on-line automated quality\ncontrol and inspection processes, an object recognition solution is presented\nin this work for two different specific applications, i.e., the detection of\nquality control items in surgery toolboxes prepared for sterilizing in a\nhospital, as well as the detection of defects in vessel hulls to prevent\npotential structural failures. The solution has two stages. First, a feature\npyramid architecture based on Single Shot MultiBox Detector (SSD) is used to\nimprove the detection performance, and a statistical analysis based on ground\ntruth is employed to select parameters of a range of default boxes. Second, a\nlightweight neural network is exploited to achieve oriented detection results\nusing a regression method. The first stage of the proposed method is capable of\ndetecting the small targets considered in the two scenarios. In the second\nstage, despite the simplicity, it is efficient to detect elongated targets\nwhile maintaining high running efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_K/0/1/0/all/0/1\">Kai Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ortiz_A/0/1/0/all/0/1\">Alberto Ortiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bonnin_Pascual_F/0/1/0/all/0/1\">Francisco Bonnin-Pascual</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hyperspherical embedding for novel class classification. (arXiv:2102.03243v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.03243","description":"<p>Deep learning models have become increasingly useful in many different\nindustries. On the domain of image classification, convolutional neural\nnetworks proved the ability to learn robust features for the closed set\nproblem, as shown in many different datasets, such as MNIST FASHIONMNIST,\nCIFAR10, CIFAR100, and IMAGENET. These approaches use deep neural networks with\ndense layers with softmax activation functions in order to learn features that\ncan separate classes in a latent space. However, this traditional approach is\nnot useful for identifying classes unseen on the training set, known as the\nopen set problem. A similar problem occurs in scenarios involving learning on\nsmall data. To tackle both problems, few-shot learning has been proposed. In\nparticular, metric learning learns features that obey constraints of a metric\ndistance in the latent space in order to perform classification. However, while\nthis approach proves to be useful for the open set problem, current\nimplementation requires pair-wise training, where both positive and negative\nexamples of similar images are presented during the training phase, which\nlimits the applicability of these approaches in large data or large class\nscenarios given the combinatorial nature of the possible inputs.In this paper,\nwe present a constraint-based approach applied to the representations in the\nlatent space under the normalized softmax loss, proposed by[18]. We\nexperimentally validate the proposed approach for the classification of unseen\nclasses on different datasets using both metric learning and the normalized\nsoftmax loss, on disjoint and joint scenarios. Our results show that not only\nour proposed strategy can be efficiently trained on larger set of classes, as\nit does not require pairwise learning, but also present better classification\nresults than the metric learning strategies surpassing its accuracy by a\nsignificant margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pereira_R/0/1/0/all/0/1\">Rafael S. Pereira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joly_A/0/1/0/all/0/1\">Alexis Joly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valduriez_P/0/1/0/all/0/1\">Patrick Valduriez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Porto_F/0/1/0/all/0/1\">Fabio Porto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Theoretical bounds on data requirements for the ray-based classification. (arXiv:2103.09577v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2103.09577","description":"<p>The problem of classifying high-dimensional shapes in real-world data grows\nin complexity as the dimension of the space increases. For the case of\nidentifying convex shapes of different geometries, a new classification\nframework has recently been proposed in which the intersections of a set of\none-dimensional representations, called rays, with the boundaries of the shape\nare used to identify the specific geometry. This ray-based classification (RBC)\nhas been empirically verified using a synthetic dataset of two- and\nthree-dimensional shapes (Zwolak et al. in Proceedings of Third Workshop on\nMachine Learning and the Physical Sciences (NeurIPS 2020), Vancouver, Canada\n[December 11, 2020], <a href=\"/abs/2010.00500\">arXiv:2010.00500</a>, 2020) and, more recently, has also been\nvalidated experimentally (Zwolak et al., PRX Quantum 2:020335, 2021). Here, we\nestablish a bound on the number of rays necessary for shape classification,\ndefined by key angular metrics, for arbitrary convex shapes. For two\ndimensions, we derive a lower bound on the number of rays in terms of the\nshape's length, diameter, and exterior angles. For convex polytopes in\n$\\mathbb{R}^N$, we generalize this result to a similar bound given as a\nfunction of the dihedral angle and the geometrical parameters of polygonal\nfaces. This result enables a different approach for estimating high-dimensional\nshapes using substantially fewer data elements than volumetric or surface-based\napproaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Weber_B/0/1/0/all/0/1\">Brian J. Weber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalantre_S/0/1/0/all/0/1\">Sandesh S. Kalantre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McJunkin_T/0/1/0/all/0/1\">Thomas McJunkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taylor_J/0/1/0/all/0/1\">Jacob M. Taylor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zwolak_J/0/1/0/all/0/1\">Justyna P. Zwolak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conditional Frechet Inception Distance. (arXiv:2103.11521v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2103.11521","description":"<p>We consider distance functions between conditional distributions. We focus on\nthe Wasserstein metric and its Gaussian case known as the Frechet Inception\nDistance (FID). We develop conditional versions of these metrics, analyze their\nrelations and provide a closed form solution to the conditional FID (CFID)\nmetric. We numerically compare the metrics in the context of performance\nevaluation of modern conditional generative models. Our results show the\nadvantages of CFID compared to the classical FID and mean squared error (MSE)\nmeasures. In contrast to FID, CFID is useful in identifying failures where\nrealistic outputs which are not related to their inputs are generated. On the\nother hand, compared to MSE, CFID is useful in identifying failures where a\nsingle realistic output is generated even though there is a diverse set of\nequally probable outputs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Soloveitchik_M/0/1/0/all/0/1\">Michael Soloveitchik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diskin_T/0/1/0/all/0/1\">Tzvi Diskin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morin_E/0/1/0/all/0/1\">Efrat Morin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiesel_A/0/1/0/all/0/1\">Ami Wiesel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepBF: Malicious URL detection using Learned Bloom Filter and Evolutionary Deep Learning. (arXiv:2103.12544v2 [cs.CR] UPDATED)","link":"http://arxiv.org/abs/2103.12544","description":"<p>Malicious URL detection is an emerging research area due to continuous\nmodernization of various systems, for instance, Edge Computing. In this\narticle, we present a novel malicious URL detection technique, called deepBF\n(deep learning and Bloom Filter). deepBF is presented in two-fold. Firstly, we\npropose a learned Bloom Filter using 2-dimensional Bloom Filter. We\nexperimentally decide the best non-cryptography string hash function. Then, we\nderive a modified non-cryptography string hash function from the selected hash\nfunction for deepBF by introducing biases in the hashing method and compared\namong the string hash functions. The modified string hash function is compared\nto other variants of diverse non-cryptography string hash functions. It is also\ncompared with various filters, particularly, counting Bloom Filter, Kirsch\n\\textit{et al.}, and Cuckoo Filter using various use cases. The use cases\nunearth weakness and strength of the filters. Secondly, we propose a malicious\nURL detection mechanism using deepBF. We apply the evolutionary convolutional\nneural network to identify the malicious URLs. The evolutionary convolutional\nneural network is trained and tested with malicious URL datasets. The output is\ntested in deepBF for accuracy. We have achieved many conclusions from our\nexperimental evaluation and results and are able to reach various conclusive\ndecisions which are presented in the article.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Patgiri_R/0/1/0/all/0/1\">Ripon Patgiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biswas_A/0/1/0/all/0/1\">Anupam Biswas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nayak_S/0/1/0/all/0/1\">Sabuzima Nayak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A State-of-the-art Survey of Artificial Neural Networks for Whole-slide Image Analysis:from Popular Convolutional Neural Networks to Potential Visual Transformers. (arXiv:2104.06243v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2104.06243","description":"<p>To increase the objectivity and accuracy of pathologists' work, artificial\nneural network(ANN) methods have been generally needed in the segmentation,\nclassification, and detection of histopathological WSI. In this paper, WSI\nanalysis methods based on ANN are reviewed. Firstly, the development status of\nWSI and ANN methods is introduced. Secondly, we summarize the common ANN\nmethods. Next, we discuss publicly available WSI datasets and evaluation\nmetrics. These ANN architectures for WSI processing are divided into classical\nneural networks and deep neural networks(DNNs) and then analyzed. Finally, the\napplication prospect of the analytical method in this field is discussed. The\nimportant potential method is Visual Transformers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1\">Xintong Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_W/0/1/0/all/0/1\">Weiming Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_C/0/1/0/all/0/1\">Chen Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jiang_T/0/1/0/all/0/1\">Tao Jiang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_H/0/1/0/all/0/1\">Hongzan Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1\">Xiaoyan Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_X/0/1/0/all/0/1\">Xinyu Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Grzegorzek_M/0/1/0/all/0/1\">Marcin Grzegorzek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An End-to-End Computer Vision Methodology for Quantitative Metallography. (arXiv:2104.11159v2 [cond-mat.mtrl-sci] UPDATED)","link":"http://arxiv.org/abs/2104.11159","description":"<p>Metallography is crucial for a proper assessment of material's properties. It\ninvolves mainly the investigation of spatial distribution of grains and the\noccurrence and characteristics of inclusions or precipitates. This work\npresents an holistic artificial intelligence model for Anomaly Detection that\nautomatically quantifies the degree of anomaly of impurities in alloys. We\nsuggest the following examination process: (1) Deep semantic segmentation is\nperformed on the inclusions (based on a suitable metallographic database of\nalloys and corresponding tags of inclusions), producing inclusions masks that\nare saved into a separated database. (2) Deep image inpainting is performed to\nfill the removed inclusions parts, resulting in 'clean' metallographic images,\nwhich contain the background of grains. (3) Grains' boundaries are marked using\ndeep semantic segmentation (based on another metallographic database of\nalloys), producing boundaries that are ready for further inspection on the\ndistribution of grains' size. (4) Deep anomaly detection and pattern\nrecognition is performed on the inclusions masks to determine spatial, shape\nand area anomaly detection of the inclusions. Finally, the system recommends to\nan expert on areas of interests for further examination. The performance of the\nmodel is presented and analyzed based on few representative cases. Although the\nmodels presented here were developed for metallography analysis, most of them\ncan be generalized to a wider set of problems in which anomaly detection of\ngeometrical objects is desired. All models as well as the data-sets that were\ncreated for this work, are publicly available at\nhttps://github.com/Scientific-Computing-Lab-NRCN/MLography.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cond-mat/1/au:+Rusanovsky_M/0/1/0/all/0/1\">Matan Rusanovsky</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Beeri_O/0/1/0/all/0/1\">Ofer Beeri</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Ifergane_S/0/1/0/all/0/1\">Sigalit Ifergane</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Oren_G/0/1/0/all/0/1\">Gal Oren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Domain and Disentangled Face Manipulation with 3D Guidance. (arXiv:2104.11228v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.11228","description":"<p>Face image manipulation via three-dimensional guidance has been widely\napplied in various interactive scenarios due to its semantically-meaningful\nunderstanding and user-friendly controllability. However, existing\n3D-morphable-model-based manipulation methods are not directly applicable to\nout-of-domain faces, such as non-photorealistic paintings, cartoon portraits,\nor even animals, mainly due to the formidable difficulties in building the\nmodel for each specific face domain. To overcome this challenge, we propose, as\nfar as we know, the first method to manipulate faces in arbitrary domains using\nhuman 3DMM. This is achieved through two major steps: 1) disentangled mapping\nfrom 3DMM parameters to the latent space embedding of a pre-trained StyleGAN2\nthat guarantees disentangled and precise controls for each semantic attribute;\nand 2) cross-domain adaptation that bridges domain discrepancies and makes\nhuman 3DMM applicable to out-of-domain faces by enforcing a consistent latent\nspace embedding. Experiments and comparisons demonstrate the superiority of our\nhigh-quality semantic manipulation method on a variety of face domains with all\nmajor 3D facial attributes controllable-pose, expression, shape, albedo, and\nillumination. Moreover, we develop an intuitive editing interface to support\nuser-friendly control and instant feedback. Our project page is\nhttps://cassiepython.github.io/cddfm3d/index.html\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Can Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_M/0/1/0/all/0/1\">Menglei Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_M/0/1/0/all/0/1\">Mingming He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dongdong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_J/0/1/0/all/0/1\">Jing Liao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Blurs Behave Like Ensembles: Spatial Smoothings to Improve Accuracy, Uncertainty, and Robustness. (arXiv:2105.12639v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2105.12639","description":"<p>Neural network ensembles, such as Bayesian neural networks (BNNs), have shown\nsuccess in the areas of uncertainty estimation and robustness. However, a\ncrucial challenge prohibits their use in practice. BNNs require a large number\nof predictions to produce reliable results, leading to a significant increase\nin computational cost. To alleviate this issue, we propose spatial smoothing, a\nmethod that spatially ensembles neighboring feature map points of convolutional\nneural networks. By simply adding a few blur layers to the models, we\nempirically show that spatial smoothing improves accuracy, uncertainty\nestimation, and robustness of BNNs across a whole range of ensemble sizes. In\nparticular, BNNs incorporating spatial smoothing achieve high predictive\nperformance merely with a handful of ensembles. Moreover, this method also can\nbe applied to canonical deterministic neural networks to improve the\nperformances. A number of evidences suggest that the improvements can be\nattributed to the stabilized feature maps and the smoothing of the loss\nlandscape. In addition, we provide a fundamental explanation for prior works -\nnamely, global average pooling, pre-activation, and ReLU6 - by addressing them\nas special cases of spatial smoothing. These not only enhance accuracy, but\nalso improve uncertainty estimation and robustness by making the loss landscape\nsmoother in the same manner as spatial smoothing. The code is available at\nhttps://github.com/xxxnell/spatial-smoothing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_N/0/1/0/all/0/1\">Namuk Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Songkuk Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Hamilton-Jacobi PDEs and image denoising models with certain non-additive noise. (arXiv:2105.13997v2 [math.OC] UPDATED)","link":"http://arxiv.org/abs/2105.13997","description":"<p>We consider image denoising problems formulated as variational problems. It\nis known that Hamilton-Jacobi PDEs govern the solution of such optimization\nproblems when the noise model is additive. In this work, we address certain\nnon-additive noise models and show that they are also related to\nHamilton-Jacobi PDEs. These findings allow us to establish new connections\nbetween additive and non-additive noise imaging models. Specifically, we study\nhow the solutions to these optimization problems depend on the parameters and\nthe observed images. We show that the optimal values are ruled by some\nHamilton-Jacobi PDEs, while the optimizers are characterized by the spatial\ngradient of the solution to the Hamilton-Jacobi PDEs. Moreover, we use these\nrelations to investigate the asymptotic behavior of the variational model as\nthe parameter goes to infinity, that is, when the influence of the noise\nvanishes. With these connections, some non-convex models for non-additive noise\ncan be solved by applying convex optimization algorithms to the equivalent\nconvex models for additive noise. Several numerical results are provided for\ndenoising problems with Poisson noise or multiplicative noise.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/math/1/au:+Darbon_J/0/1/0/all/0/1\">J&#xe9;r&#xf4;me Darbon</a>, <a href=\"http://arxiv.org/find/math/1/au:+Meng_T/0/1/0/all/0/1\">Tingwei Meng</a>, <a href=\"http://arxiv.org/find/math/1/au:+Resmerita_E/0/1/0/all/0/1\">Elena Resmerita</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Incremental False Negative Detection for Contrastive Learning. (arXiv:2106.03719v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.03719","description":"<p>Self-supervised learning has recently shown great potential in vision tasks\nthrough contrastive learning, which aims to discriminate each image, or\ninstance, in the dataset. However, such instance-level learning ignores the\nsemantic relationship among instances and sometimes undesirably repels the\nanchor from the semantically similar samples, termed as \"false negatives\". In\nthis work, we show that the unfavorable effect from false negatives is more\nsignificant for the large-scale datasets with more semantic concepts. To\naddress the issue, we propose a novel self-supervised contrastive learning\nframework that incrementally detects and explicitly removes the false negative\nsamples. Specifically, following the training process, our method dynamically\ndetects increasing high-quality false negatives considering that the encoder\ngradually improves and the embedding space becomes more semantically\nstructural. Next, we discuss two strategies to explicitly remove the detected\nfalse negatives during contrastive learning. Extensive experiments show that\nour framework outperforms other self-supervised contrastive learning methods on\nmultiple benchmarks in a limited resource setup.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tsai-Shien Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hung_W/0/1/0/all/0/1\">Wei-Chih Hung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tseng_H/0/1/0/all/0/1\">Hung-Yu Tseng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chien_S/0/1/0/all/0/1\">Shao-Yi Chien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Ming-Hsuan Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"It Takes Two to Tango: Mixup for Deep Metric Learning. (arXiv:2106.04990v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.04990","description":"<p>Metric learning involves learning a discriminative representation such that\nembeddings of similar classes are encouraged to be close, while embeddings of\ndissimilar classes are pushed far apart. State-of-the-art methods focus mostly\non sophisticated loss functions or mining strategies. On the one hand, metric\nlearning losses consider two or more examples at a time. On the other hand,\nmodern data augmentation methods for classification consider two or more\nexamples at a time. The combination of the two ideas is under-studied.\n</p>\n<p>In this work, we aim to bridge this gap and improve representations using\nmixup, which is a powerful data augmentation approach interpolating two or more\nexamples and corresponding target labels at a time. This task is challenging\nbecause unlike classification, the loss functions used in metric learning are\nnot additive over examples, so the idea of interpolating target labels is not\nstraightforward. To the best of our knowledge, we are the first to investigate\nmixing both examples and target labels for deep metric learning. We develop a\ngeneralized formulation that encompasses existing metric learning loss\nfunctions and modify it to accommodate for mixup, introducing Metric Mix, or\nMetrix. We also introduce a new metric - utilization, to demonstrate that by\nmixing examples during training, we are exploring areas of the embedding space\nbeyond the training classes, thereby improving representations. To validate the\neffect of improved representations, we show that mixing inputs, intermediate\nrepresentations or embeddings along with target labels significantly\noutperforms state-of-the-art metric learning methods on four benchmark deep\nmetric learning datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Venkataramanan_S/0/1/0/all/0/1\">Shashanka Venkataramanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Psomas_B/0/1/0/all/0/1\">Bill Psomas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kijak_E/0/1/0/all/0/1\">Ewa Kijak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amsaleg_L/0/1/0/all/0/1\">Laurent Amsaleg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karantzalos_K/0/1/0/all/0/1\">Konstantinos Karantzalos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avrithis_Y/0/1/0/all/0/1\">Yannis Avrithis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning stochastic object models from medical imaging measurements by use of advanced ambient generative adversarial networks. (arXiv:2106.14324v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2106.14324","description":"<p>Purpose: To objectively assess new medical imaging technologies via\ncomputer-simulations, it is important to account for the variability in the\nensemble of objects to be imaged. This source of variability can be described\nby stochastic object models (SOMs). It is generally desirable to establish SOMs\nfrom experimental imaging measurements acquired by use of a well-characterized\nimaging system, but this task has remained challenging. Approach: A generative\nadversarial network (GAN)-based method that employs AmbientGANs with modern\nprogressive or multiresolution training approaches is proposed. AmbientGANs\nestablished using the proposed training procedure are systematically validated\nin a controlled way using computer-simulated magnetic resonance imaging (MRI)\ndata corresponding to a stylized imaging system. Emulated single-coil\nexperimental MRI data are also employed to demonstrate the methods under less\nstylized conditions. Results: The proposed AmbientGAN method can generate clean\nimages when the imaging measurements are contaminated by measurement noise.\nWhen the imaging measurement data are incomplete, the proposed AmbientGAN can\nreliably learn the distribution of the measurement components of the objects.\nConclusions: Both visual examinations and quantitative analyses, including\ntask-specific validations using the Hotelling observer, demonstrated that the\nproposed AmbientGAN method holds promise to establish realistic SOMs from\nimaging measurements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhou_W/0/1/0/all/0/1\">Weimin Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bhadra_S/0/1/0/all/0/1\">Sayantan Bhadra</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Brooks_F/0/1/0/all/0/1\">Frank J. Brooks</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1\">Hua Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Anastasio_M/0/1/0/all/0/1\">Mark A. Anastasio</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video Crowd Localization with Multi-focus Gaussian Neighborhood Attention and a Large-Scale Benchmark. (arXiv:2107.08645v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.08645","description":"<p>Video crowd localization is a crucial yet challenging task, which aims to\nestimate exact locations of human heads in the given crowded videos. To model\nspatial-temporal dependencies of human mobility, we propose a multi-focus\nGaussian neighborhood attention (GNA), which can effectively exploit long-range\ncorrespondences while maintaining the spatial topological structure of the\ninput videos. In particular, our GNA can also capture the scale variation of\nhuman heads well using the equipped multi-focus mechanism. Based on the\nmulti-focus GNA, we develop a unified neural network called GNANet to\naccurately locate head centers in video clips by fully aggregating\nspatial-temporal information via a scene modeling module and a context\ncross-attention module. Moreover, to facilitate future researches in this\nfield, we introduce a large-scale crowd video benchmark named SenseCrowd, which\nconsists of 60K+ frames captured in various surveillance scenarios and 2M+ head\nannotations. Finally, we conduct extensive experiments on three datasets\nincluding our SenseCrowd, and the experiment results show that the proposed\nmethod is capable to achieve state-of-the-art performance for both video crowd\nlocalization and counting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haopeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lingbo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kunlin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shinan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Junyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1\">Bin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1\">Jun Hou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DCL: Differential Contrastive Learning for Geometry-Aware Depth Synthesis. (arXiv:2107.13087v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.13087","description":"<p>We describe a method for unpaired realistic depth synthesis that learns\ndiverse variations from the real-world depth scans and ensures geometric\nconsistency between the synthetic and synthesized depth. The synthesized\nrealistic depth can then be used to train task-specific networks facilitating\nlabel transfer from the synthetic domain. Unlike existing image synthesis\npipelines, where geometries are mostly ignored, we treat geometries carried by\nthe depth scans based on their own existence. We propose differential\ncontrastive learning that explicitly enforces the underlying geometric\nproperties to be invariant regarding the real variations been learned. The\nresulting depth synthesis method is task-agnostic, and we demonstrate the\neffectiveness of the proposed synthesis method by extensive evaluations on\nreal-world geometric reasoning tasks. The networks trained with the depth\nsynthesized by our method consistently achieve better performance across a wide\nrange of tasks than state of the art, and can even surpass the networks\nsupervised with full real-world annotations when slightly fine-tuned, showing\ngood transferability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yuefan Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yanchao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Youyi Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">C. Karen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1\">Leonidas Guibas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards to Robust and Generalized Medical Image Segmentation Framework. (arXiv:2108.03823v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.03823","description":"<p>Deep learning-based computer-aided diagnosis is gradually deployed to review\nand analyze medical images. However, this paradigm is restricted in real-world\nclinical applications due to the poor robustness and generalization. The issue\nis more sinister with a lack of training data. In this paper, we address the\nchallenge from the transfer learning point of view. Different from the common\nsetting that transferring knowledge from the natural image domain to the\nmedical image domain, we find the knowledge from the same domain further boosts\nthe model robustness and generalization. Therefore, we propose a novel\ntwo-stage framework for robust generalized medical image segmentation. Firstly,\nan unsupervised tile-wise autoencoder pretraining architecture is proposed to\nlearn local and global knowledge. Secondly, the downstream segmentation model\ncoupled with an auxiliary reconstruction network is designed. The\nreconstruction branch encourages the model to capture more general semantic\nfeatures. Experiments of lung segmentation on multi chest X-ray datasets are\nconducted. Comprehensive results demonstrate the superior robustness of the\nproposed framework to corruption and high generalization performance on unseen\ndatasets, especially under the scenario of the limited training data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yurong Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ARCH++: Animation-Ready Clothed Human Reconstruction Revisited. (arXiv:2108.07845v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.07845","description":"<p>We present ARCH++, an image-based method to reconstruct 3D avatars with\narbitrary clothing styles. Our reconstructed avatars are animation-ready and\nhighly realistic, in both the visible regions from input views and the unseen\nregions. While prior work shows great promise of reconstructing animatable\nclothed humans with various topologies, we observe that there exist fundamental\nlimitations resulting in sub-optimal reconstruction quality. In this paper, we\nrevisit the major steps of image-based avatar reconstruction and address the\nlimitations with ARCH++. First, we introduce an end-to-end point based geometry\nencoder to better describe the semantics of the underlying 3D human body, in\nreplacement of previous hand-crafted features. Second, in order to address the\noccupancy ambiguity caused by topological changes of clothed humans in the\ncanonical pose, we propose a co-supervising framework with cross-space\nconsistency to jointly estimate the occupancy in both the posed and canonical\nspaces. Last, we use image-to-image translation networks to further refine\ndetailed geometry and texture on the reconstructed surface, which improves the\nfidelity and consistency across arbitrary viewpoints. In the experiments, we\ndemonstrate improvements over the state of the art on both public benchmarks\nand user studies in reconstruction quality and realism.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1\">Tong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yuanlu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saito_S/0/1/0/all/0/1\">Shunsuke Saito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1\">Stefano Soatto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tung_T/0/1/0/all/0/1\">Tony Tung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"S4-Crowd: Semi-Supervised Learning with Self-Supervised Regularisation for Crowd Counting. (arXiv:2108.13969v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.13969","description":"<p>Crowd counting has drawn more attention because of its wide application in\nsmart cities. Recent works achieved promising performance but relied on the\nsupervised paradigm with expensive crowd annotations. To alleviate annotation\ncost, in this work we proposed a semi-supervised learning framework S4-Crowd,\nwhich can leverage both unlabeled/labeled data for robust crowd modelling. In\nthe unsupervised pathway, two self-supervised losses were proposed to simulate\nthe crowd variations such as scale, illumination, etc., based on which and the\nsupervised information pseudo labels were generated and gradually refined. We\nalso proposed a crowd-driven recurrent unit Gated-Crowd-Recurrent-Unit (GCRU),\nwhich can preserve discriminant crowd information by extracting second-order\nstatistics, yielding pseudo labels with improved quality. A joint loss\nincluding both unsupervised/supervised information was proposed, and a dynamic\nweighting strategy was employed to balance the importance of the unsupervised\nloss and supervised loss at different training stages. We conducted extensive\nexperiments on four popular crowd counting datasets in semi-supervised\nsettings. Experimental results suggested the effectiveness of each proposed\ncomponent in our S4-Crowd framework. Our method also outperformed other\nstate-of-the-art semi-supervised learning approaches on these crowd datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Duan_H/0/1/0/all/0/1\">Haoran Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_Y/0/1/0/all/0/1\">Yu Guan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Binaural SoundNet: Predicting Semantics, Depth and Motion with Binaural Sounds. (arXiv:2109.02763v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2109.02763","description":"<p>Humans can robustly recognize and localize objects by using visual and/or\nauditory cues. While machines are able to do the same with visual data already,\nless work has been done with sounds. This work develops an approach for scene\nunderstanding purely based on binaural sounds. The considered tasks include\npredicting the semantic masks of sound-making objects, the motion of\nsound-making objects, and the depth map of the scene. To this aim, we propose a\nnovel sensor setup and record a new audio-visual dataset of street scenes with\neight professional binaural microphones and a 360-degree camera. The\nco-existence of visual and audio cues is leveraged for supervision transfer. In\nparticular, we employ a cross-modal distillation framework that consists of\nmultiple vision teacher methods and a sound student method -- the student\nmethod is trained to generate the same results as the teacher methods do. This\nway, the auditory system can be trained without using human annotations. To\nfurther boost the performance, we propose another novel auxiliary task, coined\nSpatial Sound Super-Resolution, to increase the directional resolution of\nsounds. We then formulate the four tasks into one end-to-end trainable\nmulti-tasking network aiming to boost the overall performance. Experimental\nresults show that 1) our method achieves good results for all four tasks, 2)\nthe four tasks are mutually beneficial -- training them together achieves the\nbest performance, 3) the number and orientation of microphones are both\nimportant, and 4) features learned from the standard spectrogram and features\nobtained by the classic signal processing pipeline are complementary for\nauditory perception tasks. The data and code are released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1\">Dengxin Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasudevan_A/0/1/0/all/0/1\">Arun Balajee Vasudevan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matas_J/0/1/0/all/0/1\">Jiri Matas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AirLoop: Lifelong Loop Closure Detection. (arXiv:2109.08975v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2109.08975","description":"<p>Loop closure detection is an important building block that ensures the\naccuracy and robustness of simultaneous localization and mapping (SLAM)\nsystems. Due to their generalization ability, CNN-based approaches have\nreceived increasing attention. Although they normally benefit from training on\ndatasets that are diverse and reflective of the environments, new environments\noften emerge after the model is deployed. It is therefore desirable to\nincorporate the data newly collected during operation for incremental learning.\nNevertheless, simply finetuning the model on new data is infeasible since it\nmay cause the model's performance on previously learned data to degrade over\ntime, which is also known as the problem of catastrophic forgetting. In this\npaper, we present AirLoop, a method that leverages techniques from lifelong\nlearning to minimize forgetting when training loop closure detection models\nincrementally. We experimentally demonstrate the effectiveness of AirLoop on\nTartanAir, Nordland, and RobotCar datasets. To the best of our knowledge,\nAirLoop is one of the first works to achieve lifelong learning of deep loop\nclosure detectors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_D/0/1/0/all/0/1\">Dasong Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scherer_S/0/1/0/all/0/1\">Sebastian Scherer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CaTGrasp: Learning Category-Level Task-Relevant Grasping in Clutter from Simulation. (arXiv:2109.09163v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2109.09163","description":"<p>Task-relevant grasping is critical for industrial assembly, where downstream\nmanipulation tasks constrain the set of valid grasps. Learning how to perform\nthis task, however, is challenging, since task-relevant grasp labels are hard\nto define and annotate. There is also yet no consensus on proper\nrepresentations for modeling or off-the-shelf tools for performing\ntask-relevant grasps. This work proposes a framework to learn task-relevant\ngrasping for industrial objects without the need of time-consuming real-world\ndata collection or manual annotation. To achieve this, the entire framework is\ntrained solely in simulation, including supervised training with synthetic\nlabel generation and self-supervised, hand-object interaction. In the context\nof this framework, this paper proposes a novel, object-centric canonical\nrepresentation at the category level, which allows establishing dense\ncorrespondence across object instances and transferring task-relevant grasps to\nnovel instances. Extensive experiments on task-relevant grasping of\ndensely-cluttered industrial objects are conducted in both simulation and\nreal-world setups, demonstrating the effectiveness of the proposed framework.\nCode and data are available at https://sites.google.com/view/catgrasp.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wen_B/0/1/0/all/0/1\">Bowen Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_W/0/1/0/all/0/1\">Wenzhao Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bekris_K/0/1/0/all/0/1\">Kostas Bekris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schaal_S/0/1/0/all/0/1\">Stefan Schaal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unseen Object Amodal Instance Segmentation via Hierarchical Occlusion Modeling. (arXiv:2109.11103v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2109.11103","description":"<p>Instance-aware segmentation of unseen objects is essential for a robotic\nsystem in an unstructured environment. Although previous works achieved\nencouraging results, they were limited to segmenting the only visible regions\nof unseen objects. For robotic manipulation in a cluttered scene, amodal\nperception is required to handle the occluded objects behind others. This paper\naddresses Unseen Object Amodal Instance Segmentation (UOAIS) to detect 1)\nvisible masks, 2) amodal masks, and 3) occlusions on unseen object instances.\nFor this, we propose a Hierarchical Occlusion Modeling (HOM) scheme designed to\nreason about the occlusion by assigning a hierarchy to a feature fusion and\nprediction order. We evaluated our method on three benchmarks (tabletop,\nindoors, and bin environments) and achieved state-of-the-art (SOTA)\nperformance. Robot demos for picking up occluded objects, codes, and datasets\nare available at https://sites.google.com/view/uoais\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Back_S/0/1/0/all/0/1\">Seunghyeok Back</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Joosoon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Taewon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noh_S/0/1/0/all/0/1\">Sangjun Noh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_R/0/1/0/all/0/1\">Raeyoung Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bak_S/0/1/0/all/0/1\">Seongho Bak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kyoobin Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Domain Adaptation for LiDAR Panoptic Segmentation. (arXiv:2109.15286v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.15286","description":"<p>Scene understanding is a pivotal task for autonomous vehicles to safely\nnavigate in the environment. Recent advances in deep learning enable accurate\nsemantic reconstruction of the surroundings from LiDAR data. However, these\nmodels encounter a large domain gap while deploying them on vehicles equipped\nwith different LiDAR setups which drastically decreases their performance.\nFine-tuning the model for every new setup is infeasible due to the expensive\nand cumbersome process of recording and manually labeling new data.\nUnsupervised Domain Adaptation (UDA) techniques are thus essential to fill this\ndomain gap and retain the performance of models on new sensor setups without\nthe need for additional data labeling. In this paper, we propose AdaptLPS, a\nnovel UDA approach for LiDAR panoptic segmentation that leverages task-specific\nknowledge and accounts for variation in the number of scan lines, mounting\nposition, intensity distribution, and environmental conditions. We tackle the\nUDA task by employing two complementary domain adaptation strategies,\ndata-based and model-based. While data-based adaptations reduce the domain gap\nby processing the raw LiDAR scans to resemble the scans in the target domain,\nmodel-based techniques guide the network in extracting features that are\nrepresentative for both domains. Extensive evaluations on three pairs of\nreal-world autonomous driving datasets demonstrate that AdaptLPS outperforms\nexisting UDA approaches by up to 6.41 pp in terms of the PQ score.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Besic_B/0/1/0/all/0/1\">Borna Be&#x161;i&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gosala_N/0/1/0/all/0/1\">Nikhil Gosala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cattaneo_D/0/1/0/all/0/1\">Daniele Cattaneo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valada_A/0/1/0/all/0/1\">Abhinav Valada</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"See Yourself in Others: Attending Multiple Tasks for Own Failure Detection. (arXiv:2110.02549v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.02549","description":"<p>Autonomous robots deal with unexpected scenarios in real environments. Given\ninput images, various visual perception tasks can be performed, e.g., semantic\nsegmentation, depth estimation and normal estimation. These different tasks\nprovide rich information for the whole robotic perception system. All tasks\nhave their own characteristics while sharing some latent correlations. However,\nsome of the task predictions may suffer from the unreliability dealing with\ncomplex scenes and anomalies. We propose an attention-based failure detection\napproach by exploiting the correlations among multiple tasks. The proposed\nframework infers task failures by evaluating the individual prediction, across\nmultiple visual perception tasks for different regions in an image. The\nformulation of the evaluations is based on an attention network supervised by\nmulti-task uncertainty estimation and their corresponding prediction errors.\nOur proposed framework generates more accurate estimations of the prediction\nerror for the different task's predictions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1\">Boyang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_J/0/1/0/all/0/1\">Jiaxu Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blum_H/0/1/0/all/0/1\">Hermann Blum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siegwart_R/0/1/0/all/0/1\">Roland Siegwart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cadena_C/0/1/0/all/0/1\">Cesar Cadena</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reducing the Covariate Shift by Mirror Samples in Cross Domain Alignment. (arXiv:2110.06448v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.06448","description":"<p>Eliminating the covariate shift cross domains is one of the common methods to\ndeal with the issue of domain shift in visual unsupervised domain adaptation.\nHowever, current alignment methods, especially the prototype based or\nsample-level based methods neglect the structural properties of the underlying\ndistribution and even break the condition of covariate shift. To relieve the\nlimitations and conflicts, we introduce a novel concept named (virtual) mirror,\nwhich represents the equivalent sample in another domain. The equivalent sample\npairs, named mirror pairs reflect the natural correspondence of the empirical\ndistributions. Then a mirror loss, which aligns the mirror pairs cross domains,\nis constructed to enhance the alignment of the domains. The proposed method\ndoes not distort the internal structure of the underlying distribution. We also\nprovide theoretical proof that the mirror samples and mirror loss have better\nasymptotic properties in reducing the domain shift. By applying the virtual\nmirror and mirror loss to the generic unsupervised domain adaptation model, we\nachieved consistent superior performance on several mainstream benchmarks. Code\nis available at https://github.com/CTI-VISION/Mirror-Sample\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Minquan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_L/0/1/0/all/0/1\">Longjun Cai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A comprehensive review of Binary Neural Network. (arXiv:2110.06804v3 [cs.NE] UPDATED)","link":"http://arxiv.org/abs/2110.06804","description":"<p>Deep learning (DL) has recently changed the development of intelligent\nsystems and is widely adopted in many real-life applications. Despite their\nvarious benefits and potentials, there is a high demand for DL processing in\ndifferent computationally limited and energy-constrained devices. It is natural\nto study game-changing technologies such as Binary Neural Networks (BNN) to\nincrease deep learning capabilities. Recently remarkable progress has been made\nin BNN since they can be implemented and embedded on tiny restricted devices\nand save a significant amount of storage, computation cost, and energy\nconsumption. However, nearly all BNN acts trade with extra memory, computation\ncost, and higher performance. This article provides a complete overview of\nrecent developments in BNN. This article focuses exclusively on 1-bit\nactivations and weights 1-bit convolution networks, contrary to previous\nsurveys in which low-bit works are mixed in. It conducted a complete\ninvestigation of BNN's development -from their predecessors to the latest BNN\nalgorithms/techniques, presenting a broad design pipeline and discussing each\nmodule's variants. Along the way, it examines BNN (a) purpose: their early\nsuccesses and challenges; (b) BNN optimization: selected representative works\nthat contain essential optimization techniques; (c) deployment: open-source\nframeworks for BNN modeling and development; (d) terminal: efficient computing\narchitectures and devices for BNN and (e) applications: diverse applications\nwith BNN. Moreover, this paper discusses potential directions and future\nresearch opportunities in each section.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1\">Chunyu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agaian_S/0/1/0/all/0/1\">Sos S. Agaian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ADMM-DAD net: a deep unfolding network for analysis compressed sensing. (arXiv:2110.06986v4 [cs.IT] UPDATED)","link":"http://arxiv.org/abs/2110.06986","description":"<p>In this paper, we propose a new deep unfolding neural network based on the\nADMM algorithm for analysis Compressed Sensing. The proposed network jointly\nlearns a redundant analysis operator for sparsification and reconstructs the\nsignal of interest. We compare our proposed network with a state-of-the-art\nunfolded ISTA decoder, that also learns an orthogonal sparsifier. Moreover, we\nconsider not only image, but also speech datasets as test examples.\nComputational experiments demonstrate that our proposed network outperforms the\nstate-of-the-art deep unfolding network, consistently for both real-world image\nand speech datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kouni_V/0/1/0/all/0/1\">Vasiliki Kouni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paraskevopoulos_G/0/1/0/all/0/1\">Georgios Paraskevopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rauhut_H/0/1/0/all/0/1\">Holger Rauhut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alexandropoulos_G/0/1/0/all/0/1\">George C. Alexandropoulos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gray Matter Segmentation in Ultra High Resolution 7 Tesla ex vivo T2w MRI of Human Brain Hemispheres. (arXiv:2110.07711v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2110.07711","description":"<p>Ex vivo MRI of the brain provides remarkable advantages over in vivo MRI for\nvisualizing and characterizing detailed neuroanatomy. However, automated\ncortical segmentation methods in ex vivo MRI are not well developed, primarily\ndue to limited availability of labeled datasets, and heterogeneity in scanner\nhardware and acquisition protocols. In this work, we present a high resolution\n7 Tesla dataset of 32 ex vivo human brain specimens. We benchmark the cortical\nmantle segmentation performance of nine neural network architectures, trained\nand evaluated using manually-segmented 3D patches sampled from specific\ncortical regions, and show excellent generalizing capabilities across whole\nbrain hemispheres in different specimens, and also on unseen images acquired at\ndifferent magnetic field strength and imaging sequences. Finally, we provide\ncortical thickness measurements across key regions in 3D ex vivo human brain\nimages. Our code and processed datasets are publicly available at\nhttps://github.com/Pulkit-Khandelwal/picsl-ex-vivo-segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Khandelwal_P/0/1/0/all/0/1\">Pulkit Khandelwal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sadaghiani_S/0/1/0/all/0/1\">Shokufeh Sadaghiani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ravikumar_S/0/1/0/all/0/1\">Sadhana Ravikumar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lim_S/0/1/0/all/0/1\">Sydney Lim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arezoumandan_S/0/1/0/all/0/1\">Sanaz Arezoumandan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Peterson_C/0/1/0/all/0/1\">Claire Peterson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chung_E/0/1/0/all/0/1\">Eunice Chung</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bedard_M/0/1/0/all/0/1\">Madigan Bedard</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Capp_N/0/1/0/all/0/1\">Noah Capp</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ittyerah_R/0/1/0/all/0/1\">Ranjit Ittyerah</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Migdal_E/0/1/0/all/0/1\">Elyse Migdal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Choi_G/0/1/0/all/0/1\">Grace Choi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kopp_E/0/1/0/all/0/1\">Emily Kopp</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Loja_B/0/1/0/all/0/1\">Bridget Loja</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hasan_E/0/1/0/all/0/1\">Eusha Hasan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Jiacheng Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Prabhakaran_K/0/1/0/all/0/1\">Karthik Prabhakaran</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mizsei_G/0/1/0/all/0/1\">Gabor Mizsei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gabrielyan_M/0/1/0/all/0/1\">Marianna Gabrielyan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schuck_T/0/1/0/all/0/1\">Theresa Schuck</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Robinson_J/0/1/0/all/0/1\">John Robinson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ohm_D/0/1/0/all/0/1\">Daniel Ohm</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_E/0/1/0/all/0/1\">Edward Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Trojanowski_J/0/1/0/all/0/1\">John Q. Trojanowski</a>, <a href=\"http://arxiv.org/find/eess/1/au:+McMillan_C/0/1/0/all/0/1\">Corey McMillan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Grossman_M/0/1/0/all/0/1\">Murray Grossman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Irwin_D/0/1/0/all/0/1\">David Irwin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tisdall_M/0/1/0/all/0/1\">M. Dylan Tisdall</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Das_S/0/1/0/all/0/1\">Sandhitsu R. Das</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wisse_L/0/1/0/all/0/1\">Laura E.M. Wisse</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wolk_D/0/1/0/all/0/1\">David A. Wolk</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yushkevich_P/0/1/0/all/0/1\">Paul A. Yushkevich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Shape and Reflectance Reconstruction in Uncontrolled Environments by Differentiable Rendering. (arXiv:2110.12975v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.12975","description":"<p>Simultaneous reconstruction of geometry and reflectance properties in\nuncontrolled environments remains a challenging problem. In this paper, we\npropose an efficient method to reconstruct the scene's 3D geometry and\nreflectance from multi-view photography using conventional hand-held cameras.\nOur method automatically builds a virtual scene in a differentiable rendering\nsystem that roughly matches the real world's scene parameters, optimized by\nminimizing photometric objectives alternatingly and stochastically. With the\noptimal scene parameters evaluated, photo-realistic novel views for various\nviewing angles and distances can then be generated by our approach. We present\nthe results of captured scenes with complex geometry and various reflection\ntypes. Our method also shows superior performance compared to state-of-the-art\nalternatives in novel view synthesis visually and quantitatively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Rui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zang_G/0/1/0/all/0/1\">Guangmin Zang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_M/0/1/0/all/0/1\">Miao Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heidrich_W/0/1/0/all/0/1\">Wolfgang Heidrich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Model Fusion of Heterogeneous Neural Networks via Cross-Layer Alignment. (arXiv:2110.15538v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.15538","description":"<p>Layer-wise model fusion via optimal transport, named OTFusion, applies soft\nneuron association for unifying different pre-trained networks to save\ncomputational resources. While enjoying its success, OTFusion requires the\ninput networks to have the same number of layers. To address this issue, we\npropose a novel model fusion framework, named CLAFusion, to fuse neural\nnetworks with a different number of layers, which we refer to as heterogeneous\nneural networks, via cross-layer alignment. The cross-layer alignment problem,\nwhich is an unbalanced assignment problem, can be solved efficiently using\ndynamic programming. Based on the cross-layer alignment, our framework balances\nthe number of layers of neural networks before applying layer-wise model\nfusion. Our experiments indicate that CLAFusion, with an extra finetuning\nprocess, improves the accuracy of residual networks on CIFAR10 and CIFAR100\ndatasets. Furthermore, we explore its practical usage for model compression and\nknowledge distillation when applying to the teacher-student setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Dang Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Khai Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_N/0/1/0/all/0/1\">Nhat Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phung_D/0/1/0/all/0/1\">Dinh Phung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_H/0/1/0/all/0/1\">Hung Bui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Loop closure detection using local 3D deep descriptors. (arXiv:2111.00440v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.00440","description":"<p>We present a simple yet effective method to address loop closure detection in\nsimultaneous localisation and mapping using local 3D deep descriptors (L3Ds).\nL3Ds are emerging compact representations of patches extracted from point\nclouds that are learnt from data using a deep learning algorithm. We propose a\nnovel overlap measure for loop detection by computing the metric error between\npoints that correspond to mutually-nearest-neighbour descriptors after\nregistering the loop candidate point cloud by its estimated relative pose. This\nnovel approach enables us to accurately detect loops and estimate six\ndegrees-of-freedom poses in the case of small overlaps. We compare our\nL3D-based loop closure approach with recent approaches on LiDAR data and\nachieve state-of-the-art loop closure detection accuracy. Additionally, we\nembed our loop closure approach in RESLAM, a recent edge-based SLAM system, and\nperform the evaluation on real-world RGBD-TUM and synthetic ICL datasets. Our\napproach enables RESLAM to achieve a better localisation accuracy compared to\nits original loop closure strategy. Our project page is available at\ngithub.com/yiming107/l3d_loop_closure.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Youjie Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yiming Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poiesi_F/0/1/0/all/0/1\">Fabio Poiesi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Q/0/1/0/all/0/1\">Qi Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1\">Yi Wan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Disentangle Scenes for Person Re-identification. (arXiv:2111.05476v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.05476","description":"<p>There are many challenging problems in the person re-identification (ReID)\ntask, such as the occlusion and scale variation. Existing works usually tried\nto solve them by employing a one-branch network. This one-branch network needs\nto be robust to various challenging problems, which makes this network\noverburdened. This paper proposes to divide-and-conquer the ReID task. For this\npurpose, we employ several self-supervision operations to simulate different\nchallenging problems and handle each challenging problem using different\nnetworks. Concretely, we use the random erasing operation and propose a novel\nrandom scaling operation to generate new images with controllable\ncharacteristics. A general multi-branch network, including one master branch\nand two servant branches, is introduced to handle different scenes. These\nbranches learn collaboratively and achieve different perceptive abilities. In\nthis way, the complex scenes in the ReID task are effectively disentangled, and\nthe burden of each branch is relieved. The results from extensive experiments\ndemonstrate that the proposed method achieves state-of-the-art performances on\nthree ReID benchmarks and two occluded ReID benchmarks. Ablation study also\nshows that the proposed scheme and operations significantly improve the\nperformance in various scenes. The code is available at\nhttps://git.openi.org.cn/zangxh/LDS.git.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zang_X/0/1/0/all/0/1\">Xianghao Zang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Ge Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1\">Wei Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_X/0/1/0/all/0/1\">Xiujun Shu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sci-Net: a Scale Invariant Model for Buildings Segmentation from Aerial Images. (arXiv:2111.06812v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.06812","description":"<p>Buildings' segmentation is a fundamental task in the field of earth\nobservation and aerial imagery analysis. Most existing deep learning-based\nmethods in the literature can be applied to fixed or narrow-ranged spatial\nresolution imagery. In practical scenarios, users deal with a broad spectrum of\nimage resolutions. Thus, a given aerial image often needs to be re-sampled to\nmatch the spatial resolution of the dataset used to train the deep learning\nmodel, which results in a degradation in segmentation performance. To overcome\nthis, we propose a Scale-invariant Neural Network (Sci-Net) that can segment\nbuildings present in aerial images at different spatial resolutions.\nSpecifically, our approach leverages UNet hierarchical representations and\ndilated convolutions to extract fine-grained multi-scale representations. Our\nmethod significantly outperforms other state of the art models on the Open\nCities AI dataset with a steady improvements margin across different\nresolutions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nasrallah_H/0/1/0/all/0/1\">Hasan Nasrallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shukor_M/0/1/0/all/0/1\">Mustafa Shukor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghandour_A/0/1/0/all/0/1\">Ali J. Ghandour</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Finding Optimal Tangent Points for Reducing Distortions of Hard-label Attacks. (arXiv:2111.07492v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.07492","description":"<p>One major problem in black-box adversarial attacks is the high query\ncomplexity in the hard-label attack setting, where only the top-1 predicted\nlabel is available. In this paper, we propose a novel geometric-based approach\ncalled Tangent Attack (TA), which identifies an optimal tangent point of a\nvirtual hemisphere located on the decision boundary to reduce the distortion of\nthe attack. Assuming the decision boundary is locally flat, we theoretically\nprove that the minimum $\\ell_2$ distortion can be obtained by reaching the\ndecision boundary along the tangent line passing through such tangent point in\neach iteration. To improve the robustness of our method, we further propose a\ngeneralized method which replaces the hemisphere with a semi-ellipsoid to adapt\nto curved decision boundaries. Our approach is free of pre-training. Extensive\nexperiments conducted on the ImageNet and CIFAR-10 datasets demonstrate that\nour approach can consume only a small number of queries to achieve the\nlow-magnitude distortion. The implementation source code is released online at\nhttps://github.com/machanic/TangentAttack.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Chen Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xiangyu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Li Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yong_J/0/1/0/all/0/1\">Jun-Hai Yong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yisen Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lidar with Velocity: Motion Distortion Correction of Point Clouds from Oscillating Scanning Lidars. (arXiv:2111.09497v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2111.09497","description":"<p>Lidar point cloud distortion from moving object is an important problem in\nautonomous driving, and recently becomes even more demanding with the emerging\nof newer lidars, which feature back-and-forth scanning patterns. Accurately\nestimating moving object velocity would not only provide a tracking capability\nbut also correct the point cloud distortion with more accurate description of\nthe moving object. Since lidar measures the time-of-flight distance but with a\nsparse angular resolution, the measurement is precise in the radial measurement\nbut lacks angularly. Camera on the other hand provides a dense angular\nresolution. In this paper, Gaussian-based lidar and camera fusion is proposed\nto estimate the full velocity and correct the lidar distortion. A probabilistic\nKalman-filter framework is provided to track the moving objects, estimate their\nvelocities and simultaneously correct the point clouds distortions. The\nframework is evaluated on real road data and the fusion method outperforms the\ntraditional ICP-based and point-cloud only method. The complete working\nframework is open-sourced\n(https://github.com/ISEE-Technology/lidar-with-velocity) to accelerate the\nadoption of the emerging lidars.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1\">Zheng Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1\">Baifu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_X/0/1/0/all/0/1\">Xiaoping Hong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lebanon Solar Rooftop Potential Assessment using Buildings Segmentation from Aerial Images. (arXiv:2111.11397v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.11397","description":"<p>Estimating the solar rooftop potential of buildings' rooftops at a large\nscale is a fundamental step for every country to utilize its solar power\nefficiently. However, such estimation becomes time-consuming and costly if done\nthrough on-site measurements. This paper uses deep learning-based multi-class\ninstance segmentation to extract buildings' footprints from satellite images.\nHence, we introduce Lebanon's first complete and comprehensive buildings'\nfootprints map. Furthermore, we propose a photovoltaic panels placement\nalgorithm to estimate the solar potential of every rooftop, which results in\nLebanon's first buildings' solar rooftop potential map too. Finally, we report\ntotal and average solar rooftop potential per district and localize regions\ncorresponding to the highest solar rooftop potential yield. Conducted analysis\nreveal solar rooftop potential urban patterns and provide policymakers and key\nstakeholders with tangible insights.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nasrallah_H/0/1/0/all/0/1\">Hasan Nasrallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samhat_A/0/1/0/all/0/1\">Abed Ellatif Samhat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yilei Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaoxiang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faour_G/0/1/0/all/0/1\">Ghaleb Faour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghandour_A/0/1/0/all/0/1\">Ali J. Ghandour</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploiting full Resolution Feature Context for Liver Tumor and Vessel Segmentation via Integrate Framework: Application to Liver Tumor and Vessel 3D Reconstruction under embedded microprocessor. (arXiv:2111.13299v4 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2111.13299","description":"<p>Liver cancer is one of the most common malignant diseases in the world.\nSegmentation and labeling of liver tumors and blood vessels in CT images can\nprovide convenience for doctors in liver tumor diagnosis and surgical\nintervention. In the past decades, many state-of-the-art medical image\nsegmentation algorithms appeared during this period. With the development of\nembedded devices, embedded deployment for medical segmentation and automatic\nreconstruction brings prospects for future automated surgical tasks. Yet, most\nof the existing segmentation methods mostly care about the spatial feature\ncontext and have a perception defect in the semantic relevance of medical\nimages, which significantly affects the segmentation accuracy of liver tumors\nand blood vessels. Deploying large and complex models into embedded devices\nrequires a reasonable trade-off between model accuracy, reasoning speed and\nmodel capacity. Given these problems, we introduce a multi-scale feature fusion\nnetwork called TransFusionNet based on Transformer. This network achieved very\ncompetitive performance for liver vessel and liver tumor segmentation tasks,\nmeanwhile it can improve the recognition of morphologic margins of liver tumors\nby exploiting the global information of CT images. Experiments show that in\nvessel segmentation task TransFusionNet achieved mean Dice coefficients of\n0.899 and in liver tumor segmentation task TransFusionNet achieved mean Dice\ncoefficients of 0.961. Compared with the state-of-the-art framework, our model\nachieves the best segmentation result. In addition, we deployed the model into\nan embedded micro-structure and constructed an integrated model for liver tumor\nvascular segmentation and reconstruction. This proprietary structure will be\nthe exclusive component of the future medical field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Meng_X/0/1/0/all/0/1\">Xiangyu Meng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1\">Xudong Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_G/0/1/0/all/0/1\">Gan Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Ying Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shi_X/0/1/0/all/0/1\">Xin Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dai_H/0/1/0/all/0/1\">Huanhuan Dai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1\">Zixuan Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1\">Xun Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Attentional Guided Image Filtering. (arXiv:2112.06401v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.06401","description":"<p>Guided filter is a fundamental tool in computer vision and computer graphics\nwhich aims to transfer structure information from guidance image to target\nimage. Most existing methods construct filter kernels from the guidance itself\nwithout considering the mutual dependency between the guidance and the target.\nHowever, since there typically exist significantly different edges in the two\nimages, simply transferring all structural information of the guidance to the\ntarget would result in various artifacts. To cope with this problem, we propose\nan effective framework named deep attentional guided image filtering, the\nfiltering process of which can fully integrate the complementary information\ncontained in both images. Specifically, we propose an attentional kernel\nlearning module to generate dual sets of filter kernels from the guidance and\nthe target, respectively, and then adaptively combine them by modeling the\npixel-wise dependency between the two images. Meanwhile, we propose a\nmulti-scale guided image filtering module to progressively generate the\nfiltering result with the constructed kernels in a coarse-to-fine manner.\nCorrespondingly, a multi-scale fusion strategy is introduced to reuse the\nintermediate results in the coarse-to-fine process. Extensive experiments show\nthat the proposed framework compares favorably with the state-of-the-art\nmethods in a wide range of guided image filtering applications, such as guided\nsuper-resolution, cross-modality restoration, texture removal, and semantic\nsegmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1\">Zhiwei Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xianming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Junjun Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Debin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1\">Xiangyang Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic-Based Few-Shot Learning by Interactive Psychometric Testing. (arXiv:2112.09201v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.09201","description":"<p>Few-shot classification tasks aim to classify images in query sets based on\nonly a few labeled examples in support sets. Most studies usually assume that\neach image in a task has a single and unique class association. Under these\nassumptions, these algorithms may not be able to identify the proper class\nassignment when there is no exact matching between support and query classes.\nFor example, given a few images of lions, bikes, and apples to classify a\ntiger. However, in a more general setting, we could consider the higher-level\nconcept, the large carnivores, to match the tiger to the lion for semantic\nclassification. Existing studies rarely considered this situation due to the\nincompatibility of label-based supervision with complex conception\nrelationships. In this work, we advance the few-shot learning towards this more\nchallenging scenario, the semantic-based few-shot learning, and propose a\nmethod to address the paradigm by capturing the inner semantic relationships\nusing interactive psychometric learning. The experiment results on the\nCIFAR-100 dataset show the superiority of our method for the semantic-based\nfew-shot learning compared to the baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yin_L/0/1/0/all/0/1\">Lu Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menkovski_V/0/1/0/all/0/1\">Vlado Menkovski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pei_Y/0/1/0/all/0/1\">Yulong Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pechenizkiy_M/0/1/0/all/0/1\">Mykola Pechenizkiy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Deep Learning Based Workflow for Detection of Lung Nodules With Chest Radiograph. (arXiv:2112.10184v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2112.10184","description":"<p>PURPOSE: This study aimed to develop a deep learning-based tool to detect and\nlocalize lung nodules with chest radiographs(CXRs). We expected it to enhance\nthe efficiency of interpreting CXRs and reduce the possibilities of delayed\ndiagnosis of lung cancer.\n</p>\n<p>MATERIALS AND METHODS: We collected CXRs from NCKUH database and VBD, an\nopen-source medical image dataset, as our training and validation data. A\nnumber of CXRs from the Ministry of Health and Welfare(MOHW) database served as\nour test data. We built a segmentation model to identify lung areas from CXRs,\nand sliced them into 16 patches. Physicians labeled the CXRs by clicking the\npatches. These labeled patches were then used to train and fine-tune a deep\nneural network(DNN) model, classifying the patches as positive or negative.\nFinally, we test the DNN model with the lung patches of CXRs from MOHW.\n</p>\n<p>RESULTS: Our segmentation model identified the lung regions well from the\nwhole CXR. The Intersection over Union(IoU) between the ground truth and the\nsegmentation result was 0.9228. In addition, our DNN model achieved a\nsensitivity of 0.81, specificity of 0.82, and AUROC of 0.869 in 98 of 125\ncases. For the other 27 difficult cases, the sensitivity was 0.54, specificity\n0.494, and AUROC 0.682. Overall, we obtained a sensitivity of 0.78, specificity\nof 0.79, and AUROC 0.837.\n</p>\n<p>CONCLUSIONS: Our two-step workflow is comparable to state-of-the-art\nalgorithms in the sensitivity and specificity of localizing lung nodules from\nCXRs. Notably, our workflow provides an efficient way for specialists to label\nthe data, which is valuable for relevant researches because of the relative\nrarity of labeled medical image data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Tai_Y/0/1/0/all/0/1\">Yang Tai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fang_Y/0/1/0/all/0/1\">Yu-Wen Fang</a> (Same contribution), <a href=\"http://arxiv.org/find/eess/1/au:+Su_F/0/1/0/all/0/1\">Fang-Yi Su</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chiang_J/0/1/0/all/0/1\">Jung-Hsien Chiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"JoJoGAN: One Shot Face Stylization. (arXiv:2112.11641v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.11641","description":"<p>A style mapper applies some fixed style to its input images (so, for example,\ntaking faces to cartoons). This paper describes a simple procedure -- JoJoGAN\n-- to learn a style mapper from a single example of the style. JoJoGAN uses a\nGAN inversion procedure and StyleGAN's style-mixing property to produce a\nsubstantial paired dataset from a single example style. The paired dataset is\nthen used to fine-tune a StyleGAN. An image can then be style mapped by\nGAN-inversion followed by the fine-tuned StyleGAN. JoJoGAN needs just one\nreference and as little as 30 seconds of training time. JoJoGAN can use extreme\nstyle references (say, animal faces) successfully. Furthermore, one can control\nwhat aspects of the style are used and how much of the style is applied.\nQualitative and quantitative evaluation show that JoJoGAN produces high quality\nhigh resolution images that vastly outperform the current state-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chong_M/0/1/0/all/0/1\">Min Jin Chong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Forsyth_D/0/1/0/all/0/1\">David Forsyth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is it Possible to Predict MGMT Promoter Methylation from Brain Tumor MRI Scans using Deep Learning Models?. (arXiv:2201.06086v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2201.06086","description":"<p>Glioblastoma is a common brain malignancy that tends to occur in older adults\nand is almost always lethal. The effectiveness of chemotherapy, being the\nstandard treatment for most cancer types, can be improved if a particular\ngenetic sequence in the tumor known as MGMT promoter is methylated. However, to\nidentify the state of the MGMT promoter, the conventional approach is to\nperform a biopsy for genetic analysis, which is time and effort consuming. A\ncouple of recent publications proposed a connection between the MGMT promoter\nstate and the MRI scans of the tumor and hence suggested the use of deep\nlearning models for this purpose. Therefore, in this work, we use one of the\nmost extensive datasets, BraTS 2021, to study the potency of employing deep\nlearning solutions, including 2D and 3D CNN models and vision transformers.\nAfter conducting a thorough analysis of the models' performance, we concluded\nthat there seems to be no connection between the MRI scans and the state of the\nMGMT promoter.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Saeed_N/0/1/0/all/0/1\">Numan Saeed</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hardan_S/0/1/0/all/0/1\">Shahad Hardan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Abutalip_K/0/1/0/all/0/1\">Kudaibergen Abutalip</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yaqub_M/0/1/0/all/0/1\">Mohammad Yaqub</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MVPTR: Multi-Stage Vision-Language Pre-Training via Multi-Level Semantic Alignment. (arXiv:2201.12596v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.12596","description":"<p>In this paper, we propose a Multi-stage Vision-language Pre-TRaining (MVPTR)\nframework to learn cross-modality representation via multi-level semantic\nalignment. We introduce concepts in both modalities to construct two-level\nsemantic representations for language and vision. Based on the multi-level\ninput, we train the cross-modality model in two stages, namely, uni-modal\nlearning and cross-modal learning. The former stage enforces within-modality\ninteractions to learn multi-level semantics for each single modality. The\nlatter stage enforces interactions across modalities via both coarse-grain and\nfine-grain semantic alignment tasks. Image-text matching and masked language\nmodeling are then used to further optimize the pre-training model. Our model\ngenerates the-state-of-the-art results on several vision and language tasks.\nOur code is available at https://github.com/Junction4Nako/mvp_pytorch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zejun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1\">Zhihao Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tou_H/0/1/0/all/0/1\">Huaixiao Tou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhongyu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Probabilistically Robust Learning: Balancing Average- and Worst-case Performance. (arXiv:2202.01136v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2202.01136","description":"<p>Many of the successes of machine learning are based on minimizing an averaged\nloss function. However, it is well-known that this paradigm suffers from\nrobustness issues that hinder its applicability in safety-critical domains.\nThese issues are often addressed by training against worst-case perturbations\nof data, a technique known as adversarial training. Although empirically\neffective, adversarial training can be overly conservative, leading to\nunfavorable trade-offs between nominal performance and robustness. To this end,\nin this paper we propose a framework called probabilistic robustness that\nbridges the gap between the accurate, yet brittle average case and the robust,\nyet conservative worst case by enforcing robustness to most rather than to all\nperturbations. From a theoretical point of view, this framework overcomes the\ntrade-offs between the performance and the sample-complexity of worst-case and\naverage-case learning. From a practical point of view, we propose a novel\nalgorithm based on risk-aware optimization that effectively balances average-\nand worst-case performance at a considerably lower computational cost relative\nto adversarial training. Our results on MNIST, CIFAR-10, and SVHN illustrate\nthe advantages of this framework on the spectrum from average- to worst-case\nrobustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Robey_A/0/1/0/all/0/1\">Alexander Robey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chamon_L/0/1/0/all/0/1\">Luiz F. O. Chamon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pappas_G/0/1/0/all/0/1\">George J. Pappas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassani_H/0/1/0/all/0/1\">Hamed Hassani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simulation-to-Reality domain adaptation for offline 3D object annotation on pointclouds with correlation alignment. (arXiv:2202.02666v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.02666","description":"<p>Annotating objects with 3D bounding boxes in LiDAR pointclouds is a costly\nhuman driven process in an autonomous driving perception system. In this paper,\nwe present a method to semi-automatically annotate real-world pointclouds\ncollected by deployment vehicles using simulated data. We train a 3D object\ndetector model on labeled simulated data from CARLA jointly with real world\npointclouds from our target vehicle. The supervised object detection loss is\naugmented with a CORAL loss term to reduce the distance between labeled\nsimulated and unlabeled real pointcloud feature representations. The goal here\nis to learn representations that are invariant to simulated (labeled) and\nreal-world (unlabeled) target domains. We also provide an updated survey on\ndomain adaptation methods for pointclouds.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weishuang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiran_B/0/1/0/all/0/1\">B Ravi Kiran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gauthier_T/0/1/0/all/0/1\">Thomas Gauthier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mazouz_Y/0/1/0/all/0/1\">Yanis Mazouz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steger_T/0/1/0/all/0/1\">Theo Steger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-modal Sensor Fusion for Auto Driving Perception: A Survey. (arXiv:2202.02703v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.02703","description":"<p>Multi-modal fusion is a fundamental task for the perception of an autonomous\ndriving system, which has recently intrigued many researchers. However,\nachieving a rather good performance is not an easy task due to the noisy raw\ndata, underutilized information, and the misalignment of multi-modal sensors.\nIn this paper, we provide a literature review of the existing multi-modal-based\nmethods for perception tasks in autonomous driving. Generally, we make a\ndetailed analysis including over 50 papers leveraging perception sensors\nincluding LiDAR and camera trying to solve object detection and semantic\nsegmentation tasks. Different from traditional fusion methodology for\ncategorizing fusion models, we propose an innovative way that divides them into\ntwo major classes, four minor classes by a more reasonable taxonomy in the view\nof the fusion stage. Moreover, we dive deep into the current fusion methods,\nfocusing on the remaining problems and open-up discussions on the potential\nresearch opportunities. In conclusion, what we expect to do in this paper is to\npresent a new taxonomy of multi-modal fusion methods for the autonomous driving\nperception tasks and provoke thoughts of the fusion-based techniques in the\nfuture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Keli Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1\">Botian Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Siyuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yikang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FCM-DNN: diagnosing coronary artery disease by deep accuracy Fuzzy C-Means clustering model. (arXiv:2202.04645v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2202.04645","description":"<p>Cardiovascular disease is one of the most challenging diseases in middle-aged\nand older people, which causes high mortality. Coronary artery disease (CAD) is\nknown as a common cardiovascular disease. A standard clinical tool for\ndiagnosing CAD is angiography. The main challenges are dangerous side effects\nand high angiography costs. Today, the development of artificial\nintelligence-based methods is a valuable achievement for diagnosing disease.\nHence, in this paper, artificial intelligence methods such as neural network\n(NN), deep neural network (DNN), and Fuzzy C-Means clustering combined with\ndeep neural network (FCM-DNN) are developed for diagnosing CAD on a cardiac\nmagnetic resonance imaging (CMRI) dataset. The original dataset is used in two\ndifferent approaches. First, the labeled dataset is applied to the NN and DNN\nto create the NN and DNN models. Second, the labels are removed, and the\nunlabeled dataset is clustered via the FCM method, and then, the clustered\ndataset is fed to the DNN to create the FCM-DNN model. By utilizing the second\nclustering and modeling, the training process is improved, and consequently,\nthe accuracy is increased. As a result, the proposed FCM-DNN model achieves the\nbest performance with a 99.91% accuracy specifying 10 clusters, i.e., 5\nclusters for healthy subjects and 5 clusters for sick subjects, through the\n10-fold cross-validation technique compared to the NN and DNN models reaching\nthe accuracies of 92.18% and 99.63%, respectively. To the best of our\nknowledge, no study has been conducted for CAD diagnosis on the CMRI dataset\nusing artificial intelligence methods. The results confirm that the proposed\nFCM-DNN model can be helpful for scientific and research centers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Joloudari_J/0/1/0/all/0/1\">Javad Hassannataj Joloudari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Saadatfar_H/0/1/0/all/0/1\">Hamid Saadatfar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+GhasemiGol_M/0/1/0/all/0/1\">Mohammad GhasemiGol</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Alizadehsani_R/0/1/0/all/0/1\">Roohallah Alizadehsani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sani_Z/0/1/0/all/0/1\">Zahra Alizadeh Sani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hasanzadeh_F/0/1/0/all/0/1\">Fereshteh Hasanzadeh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hassannataj_E/0/1/0/all/0/1\">Edris Hassannataj</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sharifrazi_D/0/1/0/all/0/1\">Danial Sharifrazi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mansor_Z/0/1/0/all/0/1\">Zulkefli Mansor</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Do Vision Transformers Work?. (arXiv:2202.06709v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.06709","description":"<p>The success of multi-head self-attentions (MSAs) for computer vision is now\nindisputable. However, little is known about how MSAs work. We present\nfundamental explanations to help better understand the nature of MSAs. In\nparticular, we demonstrate the following properties of MSAs and Vision\nTransformers (ViTs): (1) MSAs improve not only accuracy but also generalization\nby flattening the loss landscapes. Such improvement is primarily attributable\nto their data specificity, not long-range dependency. On the other hand, ViTs\nsuffer from non-convex losses. Large datasets and loss landscape smoothing\nmethods alleviate this problem; (2) MSAs and Convs exhibit opposite behaviors.\nFor example, MSAs are low-pass filters, but Convs are high-pass filters.\nTherefore, MSAs and Convs are complementary; (3) Multi-stage neural networks\nbehave like a series connection of small individual models. In addition, MSAs\nat the end of a stage play a key role in prediction. Based on these insights,\nwe propose AlterNet, a model in which Conv blocks at the end of a stage are\nreplaced with MSA blocks. AlterNet outperforms CNNs not only in large data\nregimes but also in small data regimes. The code is available at\nhttps://github.com/xxxnell/how-do-vits-work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_N/0/1/0/all/0/1\">Namuk Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Songkuk Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Label fusion and training methods for reliable representation of inter-rater uncertainty. (arXiv:2202.07550v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2202.07550","description":"<p>Medical tasks are prone to inter-rater variability due to multiple factors\nsuch as image quality, professional experience and training, or guideline\nclarity. Training deep learning networks with annotations from multiple raters\nis a common practice that mitigates the model's bias towards a single expert.\nReliable models generating calibrated outputs and reflecting the inter-rater\ndisagreement are key to the integration of artificial intelligence in clinical\npractice. Various methods exist to take into account different expert labels.\nWe focus on comparing three label fusion methods: STAPLE, average of the\nrater's segmentation, and random sampling of each rater's segmentation during\ntraining. Each label fusion method is studied using both the conventional\ntraining framework and the recently published SoftSeg framework that limits\ninformation loss by treating the segmentation task as a regression. Our\nresults, across 10 data splittings on two public datasets, indicate that\nSoftSeg models, regardless of the ground truth fusion method, had better\ncalibration and preservation of the inter-rater rater variability compared with\ntheir conventional counterparts without impacting the segmentation performance.\nConventional models, i.e., trained with a Dice loss, with binary inputs, and\nsigmoid/softmax final activate, were overconfident and underestimated the\nuncertainty associated with inter-rater variability. Conversely, fusing labels\nby averaging with the SoftSeg framework led to underconfident outputs and\noverestimation of the rater disagreement. In terms of segmentation performance,\nthe best label fusion method was different for the two datasets studied,\nindicating this parameter might be task-dependent. However, SoftSeg had\nsegmentation performance systematically superior or equal to the conventionally\ntrained models and had the best calibration and preservation of the inter-rater\nvariability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lemay_A/0/1/0/all/0/1\">Andreanne Lemay</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gros_C/0/1/0/all/0/1\">Charley Gros</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cohen_Adad_J/0/1/0/all/0/1\">Julien Cohen-Adad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PMP-Net++: Point Cloud Completion by Transformer-Enhanced Multi-step Point Moving Paths. (arXiv:2202.09507v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.09507","description":"<p>Point cloud completion concerns to predict missing part for incomplete 3D\nshapes. A common strategy is to generate complete shape according to incomplete\ninput. However, unordered nature of point clouds will degrade generation of\nhigh-quality 3D shapes, as detailed topology and structure of unordered points\nare hard to be captured during the generative process using an extracted latent\ncode. We address this problem by formulating completion as point cloud\ndeformation process. Specifically, we design a novel neural network, named\nPMP-Net++, to mimic behavior of an earth mover. It moves each point of\nincomplete input to obtain a complete point cloud, where total distance of\npoint moving paths (PMPs) should be the shortest. Therefore, PMP-Net++ predicts\nunique PMP for each point according to constraint of point moving distances.\nThe network learns a strict and unique correspondence on point-level, and thus\nimproves quality of predicted complete shape. Moreover, since moving points\nheavily relies on per-point features learned by network, we further introduce a\ntransformer-enhanced representation learning network, which significantly\nimproves completion performance of PMP-Net++. We conduct comprehensive\nexperiments in shape completion, and further explore application on point cloud\nup-sampling, which demonstrate non-trivial improvement of PMP-Net++ over\nstate-of-the-art point cloud completion/up-sampling methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wen_X/0/1/0/all/0/1\">Xin Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_P/0/1/0/all/0/1\">Peng Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1\">Zhizhong Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yan-Pei Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_P/0/1/0/all/0/1\">Pengfei Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1\">Wen Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yu-Shen Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Going Deeper into Recognizing Actions in Dark Environments: A Comprehensive Benchmark Study. (arXiv:2202.09545v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.09545","description":"<p>While action recognition (AR) has gained large improvements with the\nintroduction of large-scale video datasets and the development of deep neural\nnetworks, AR models robust to challenging environments in real-world scenarios\nare still under-explored. We focus on the task of action recognition in dark\nenvironments, which can be applied to fields such as surveillance and\nautonomous driving at night. Intuitively, current deep networks along with\nvisual enhancement techniques should be able to handle AR in dark environments,\nhowever, it is observed that this is not always the case in practice. To dive\ndeeper into exploring solutions for AR in dark environments, we launched the\nUG2+ Challenge Track 2 (UG2-2) in IEEE CVPR 2021, with a goal of evaluating and\nadvancing the robustness of AR models in dark environments. The challenge\nbuilds and expands on top of a novel ARID dataset, the first dataset for the\ntask of dark video AR, and guides models to tackle such a task in both fully\nand semi-supervised manners. Baseline results utilizing current AR models and\nenhancement methods are reported, justifying the challenging nature of this\ntask with substantial room for improvements. Thanks to the active participation\nfrom the research community, notable advances have been made in participants'\nsolutions, while analysis of these solutions helped better identify possible\ndirections to tackle the challenge of AR in dark environments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yuecong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianfei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_H/0/1/0/all/0/1\">Haozhi Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1\">Jianxiong Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhenghua Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoli Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhengguo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qianwen Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparsity Winning Twice: Better Robust Generalization from More Efficient Training. (arXiv:2202.09844v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.09844","description":"<p>Recent studies demonstrate that deep networks, even robustified by the\nstate-of-the-art adversarial training (AT), still suffer from large robust\ngeneralization gaps, in addition to the much more expensive training costs than\nstandard training. In this paper, we investigate this intriguing problem from a\nnew perspective, i.e., injecting appropriate forms of sparsity during\nadversarial training. We introduce two alternatives for sparse adversarial\ntraining: (i) static sparsity, by leveraging recent results from the lottery\nticket hypothesis to identify critical sparse subnetworks arising from the\nearly training; (ii) dynamic sparsity, by allowing the sparse subnetwork to\nadaptively adjust its connectivity pattern (while sticking to the same sparsity\nratio) throughout training. We find both static and dynamic sparse methods to\nyield win-win: substantially shrinking the robust generalization gap and\nalleviating the robust overfitting, meanwhile significantly saving training and\ninference FLOPs. Extensive experiments validate our proposals with multiple\nnetwork architectures on diverse datasets, including CIFAR-10/100 and\nTiny-ImageNet. For example, our methods reduce robust generalization gap and\noverfitting by 34.44% and 4.02%, with comparable robust/standard accuracy\nboosts and 87.83%/87.82% training/inference FLOPs savings on CIFAR-100 with\nResNet-18. Besides, our approaches can be organically combined with existing\nregularizers, establishing new state-of-the-art results in AT. Codes are\navailable in https://github.com/VITA-Group/Sparsity-Win-Robust-Generalization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianlong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhenyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pengjun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balachandra_S/0/1/0/all/0/1\">Santosh Balachandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1\">Haoyu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zehao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Feature based Cross-slide Registration. (arXiv:2202.09971v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.09971","description":"<p>Cross-slide image analysis provides additional information by analysing the\nexpression of different biomarkers as compared to a single slide analysis.\nSlides stained with different biomarkers are analysed side by side which may\nreveal unknown relations between the different biomarkers. During the slide\npreparation, a tissue section may be placed at an arbitrary orientation as\ncompared to other sections of the same tissue block. The problem is compounded\nby the fact that tissue contents are likely to change from one section to the\nnext and there may be unique artefacts on some of the slides. This makes\nregistration of each section to a reference section of the same tissue block an\nimportant pre-requisite task before any cross-slide analysis. We propose a deep\nfeature based registration (DFBR) method which utilises data-driven features to\nestimate the rigid transformation. We adopted a multi-stage strategy for\nimproving the quality of registration. We also developed a visualisation tool\nto view registered pairs of WSIs at different magnifications. With the help of\nthis tool, one can apply a transformation on the fly without the need to\ngenerate transformed source WSI in a pyramidal form. We compared the\nperformance of data-driven features with that of hand-crafted features on the\nCOMET dataset. Our approach can align the images with low registration errors.\nGenerally, the success of non-rigid registration is dependent on the quality of\nrigid registration. To evaluate the efficacy of the DFBR method, the first two\nsteps of the ANHIR winner's framework are replaced with our DFBR to register\nchallenge provided image pairs. The modified framework produce comparable\nresults to that of challenge winning team.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Awan_R/0/1/0/all/0/1\">Ruqayya Awan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raza_S/0/1/0/all/0/1\">Shan E Ahmed Raza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lotz_J/0/1/0/all/0/1\">Johannes Lotz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weiss_N/0/1/0/all/0/1\">Nick Weiss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajpoot_N/0/1/0/all/0/1\">Nasir M. Rajpoot</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ABAW: Valence-Arousal Estimation, Expression Recognition, Action Unit Detection & Multi-Task Learning Challenges. (arXiv:2202.10659v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.10659","description":"<p>This paper describes the third Affective Behavior Analysis in-the-wild (ABAW)\nCompetition, held in conjunction with IEEE International Conference on Computer\nVision and Pattern Recognition (CVPR), 2022. The 3rd ABAW Competition is a\ncontinuation of the Competitions held at ICCV 2021, IEEE FG 2020 and IEEE CVPR\n2017 Conferences, and aims at automatically analyzing affect. This year the\nCompetition encompasses four Challenges: i) uni-task Valence-Arousal\nEstimation, ii) uni-task Expression Classification, iii) uni-task Action Unit\nDetection, and iv) Multi-Task-Learning. All the Challenges are based on a\ncommon benchmark database, Aff-Wild2, which is a large scale in-the-wild\ndatabase and the first one to be annotated in terms of valence-arousal,\nexpressions and action units. In this paper, we present the four Challenges,\nwith the utilized Competition corpora, we outline the evaluation metrics and\npresent the baseline systems along with their obtained results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kollias_D/0/1/0/all/0/1\">Dimitrios Kollias</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"One-shot Scene Graph Generation. (arXiv:2202.10824v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.10824","description":"<p>As a structured representation of the image content, the visual scene graph\n(visual relationship) acts as a bridge between computer vision and natural\nlanguage processing. Existing models on the scene graph generation task\nnotoriously require tens or hundreds of labeled samples. By contrast, human\nbeings can learn visual relationships from a few or even one example. Inspired\nby this, we design a task named One-Shot Scene Graph Generation, where each\nrelationship triplet (e.g., \"dog-has-head\") comes from only one labeled\nexample. The key insight is that rather than learning from scratch, one can\nutilize rich prior knowledge. In this paper, we propose Multiple Structured\nKnowledge (Relational Knowledge and Commonsense Knowledge) for the one-shot\nscene graph generation task. Specifically, the Relational Knowledge represents\nthe prior knowledge of relationships between entities extracted from the visual\ncontent, e.g., the visual relationships \"standing in\", \"sitting in\", and \"lying\nin\" may exist between \"dog\" and \"yard\", while the Commonsense Knowledge encodes\n\"sense-making\" knowledge like \"dog can guard yard\". By organizing these two\nkinds of knowledge in a graph structure, Graph Convolution Networks (GCNs) are\nused to extract knowledge-embedded semantic features of the entities. Besides,\ninstead of extracting isolated visual features from each entity generated by\nFaster R-CNN, we utilize an Instance Relation Transformer encoder to fully\nexplore their context information. Based on a constructed one-shot dataset, the\nexperimental results show that our method significantly outperforms existing\nstate-of-the-art methods by a large margin. Ablation studies also verify the\neffectiveness of the Instance Relation Transformer encoder and the Multiple\nStructured Knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yuyu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jingkuan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Lianli Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Heng Tao Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Auto-scaling Vision Transformers without Training. (arXiv:2202.11921v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2202.11921","description":"<p>This work targets automated designing and scaling of Vision Transformers\n(ViTs). The motivation comes from two pain spots: 1) the lack of efficient and\nprincipled methods for designing and scaling ViTs; 2) the tremendous\ncomputational cost of training ViT that is much heavier than its convolution\ncounterpart. To tackle these issues, we propose As-ViT, an auto-scaling\nframework for ViTs without training, which automatically discovers and scales\nup ViTs in an efficient and principled manner. Specifically, we first design a\n\"seed\" ViT topology by leveraging a training-free search process. This\nextremely fast search is fulfilled by a comprehensive study of ViT's network\ncomplexity, yielding a strong Kendall-tau correlation with ground-truth\naccuracies. Second, starting from the \"seed\" topology, we automate the scaling\nrule for ViTs by growing widths/depths to different ViT layers. This results in\na series of architectures with different numbers of parameters in a single run.\nFinally, based on the observation that ViTs can tolerate coarse tokenization in\nearly training stages, we propose a progressive tokenization strategy to train\nViTs faster and cheaper. As a unified framework, As-ViT achieves strong\nperformance on classification (83.5% top1 on ImageNet-1k) and detection (52.7%\nmAP on COCO) without any manual crafting nor scaling of ViT architectures: the\nend-to-end model design and scaling process cost only 12 hours on one V100 GPU.\nOur code is available at https://github.com/VITA-Group/AsViT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wuyang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1\">Xianzhi Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xiaodan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Denny Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Measuring CLEVRness: Blackbox testing of Visual Reasoning Models. (arXiv:2202.12162v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2202.12162","description":"<p>How can we measure the reasoning capabilities of intelligence systems? Visual\nquestion answering provides a convenient framework for testing the model's\nabilities by interrogating the model through questions about the scene.\nHowever, despite scores of various visual QA datasets and architectures, which\nsometimes yield even a super-human performance, the question of whether those\narchitectures can actually reason remains open to debate. To answer this, we\nextend the visual question answering framework and propose the following\nbehavioral test in the form of a two-player game. We consider black-box neural\nmodels of CLEVR. These models are trained on a diagnostic dataset benchmarking\nreasoning. Next, we train an adversarial player that re-configures the scene to\nfool the CLEVR model. We show that CLEVR models, which otherwise could perform\nat a human level, can easily be fooled by our agent. Our results put in doubt\nwhether data-driven approaches can do reasoning without exploiting the numerous\nbiases that are often present in those datasets. Finally, we also propose a\ncontrolled experiment measuring the efficiency of such models to learn and\nperform reasoning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mouselinos_S/0/1/0/all/0/1\">Spyridon Mouselinos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michalewski_H/0/1/0/all/0/1\">Henryk Michalewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malinowski_M/0/1/0/all/0/1\">Mateusz Malinowski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Factorizer: A Scalable Interpretable Approach to Context Modeling for Medical Image Segmentation. (arXiv:2202.12295v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2202.12295","description":"<p>Convolutional Neural Networks (CNNs) with U-shaped architectures have\ndominated medical image segmentation, which is crucial for various clinical\npurposes. However, the inherent locality of convolution makes CNNs fail to\nfully exploit global context, essential for better recognition of some\nstructures, e.g., brain lesions. Transformers have recently proved promising\nperformance on vision tasks, including semantic segmentation, mainly due to\ntheir capability of modeling long-range dependencies. Nevertheless, the\nquadratic complexity of attention makes existing Transformer-based models use\nself-attention layers only after somehow reducing the image resolution, which\nlimits the ability to capture global contexts present at higher resolutions.\nTherefore, this work introduces a family of models, dubbed Factorizer, which\nleverages the power of low-rank matrix factorization for constructing an\nend-to-end segmentation model. Specifically, we propose a linearly scalable\napproach to context modeling, formulating Nonnegative Matrix Factorization\n(NMF) as a differentiable layer integrated into a U-shaped architecture. The\nshifted window technique is also utilized in combination with NMF to\neffectively aggregate local information. Factorizers compete favorably with\nCNNs and Transformers in terms of accuracy, scalability, and interpretability,\nachieving state-of-the-art results on the BraTS dataset for brain tumor\nsegmentation, with Dice scores of 79.33%, 83.14%, and 90.16% for enhancing\ntumor, tumor core, and whole tumor, respectively. Highly meaningful NMF\ncomponents give an additional interpretability advantage to Factorizers over\nCNNs and Transformers. Moreover, our ablation studies reveal a distinctive\nfeature of Factorizers that enables a significant speed-up in inference for a\ntrained Factorizer without any extra steps and without sacrificing much\naccuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ashtari_P/0/1/0/all/0/1\">Pooya Ashtari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sima_D/0/1/0/all/0/1\">Diana Sima</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lathauwer_L/0/1/0/all/0/1\">Lieven De Lathauwer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sappey_Marinierd_D/0/1/0/all/0/1\">Dominique Sappey-Marinierd</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maes_F/0/1/0/all/0/1\">Frederik Maes</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huffel_S/0/1/0/all/0/1\">Sabine Van Huffel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NeuralFusion: Neural Volumetric Rendering under Human-object Interactions. (arXiv:2202.12825v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.12825","description":"<p>4D modeling of human-object interactions is critical for numerous\napplications. However, efficient volumetric capture and rendering of complex\ninteraction scenarios, especially from sparse inputs, remain challenging. In\nthis paper, we propose NeuralFusion, a neural approach for volumetric\nhuman-object capture and rendering using sparse consumer RGBD sensors. It\nmarries traditional non-rigid fusion with recent neural implicit modeling and\nblending advances, where the captured humans and objects are layerwise\ndisentangled. For geometry modeling, we propose a neural implicit inference\nscheme with non-rigid key-volume fusion, as well as a template-aid robust\nobject tracking pipeline. Our scheme enables detailed and complete geometry\ngeneration under complex interactions and occlusions. Moreover, we introduce a\nlayer-wise human-object texture rendering scheme, which combines volumetric and\nimage-based rendering in both spatial and temporal domains to obtain\nphoto-realistic results. Extensive experiments demonstrate the effectiveness\nand efficiency of our approach in synthesizing photo-realistic free-view\nresults under complex human-object interactions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yuheng Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Suyi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1\">Guoxing Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Z/0/1/0/all/0/1\">Zhuo Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_K/0/1/0/all/0/1\">Kaiwen Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Minye Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jingyi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lan Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-02-28T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/"}}]}]}