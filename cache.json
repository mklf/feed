{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-05-12T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Problems with Cosine as a Measure of Embedding Similarity for High Frequency Words. (arXiv:2205.05092v1 [cs.CL])","link":"http://arxiv.org/abs/2205.05092","description":"<p>Cosine similarity of contextual embeddings is used in many NLP tasks (e.g.,\nQA, IR, MT) and metrics (e.g., BERTScore). Here, we uncover systematic ways in\nwhich word similarities estimated by cosine over BERT embeddings are\nunderstated and trace this effect to training data frequency. We find that\nrelative to human judgements, cosine similarity underestimates the similarity\nof frequent words with other instances of the same word or other words across\ncontexts, even after controlling for polysemy and other factors. We conjecture\nthat this underestimation of similarity for high frequency words is due to\ndifferences in the representational geometry of high and low frequency words\nand provide a formal argument for the two-dimensional case.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kaitlyn Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ethayarajh_K/0/1/0/all/0/1\">Kawin Ethayarajh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Card_D/0/1/0/all/0/1\">Dallas Card</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurafsky_D/0/1/0/all/0/1\">Dan Jurafsky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Richer Countries and Richer Representations. (arXiv:2205.05093v1 [cs.CL])","link":"http://arxiv.org/abs/2205.05093","description":"<p>We examine whether some countries are more richly represented in embedding\nspace than others. We find that countries whose names occur with low frequency\nin training corpora are more likely to be tokenized into subwords, are less\nsemantically distinct in embedding space, and are less likely to be correctly\npredicted: e.g., Ghana (the correct answer and in-vocabulary) is not predicted\nfor, \"The country producing the most cocoa is [MASK].\". Although these\nperformance discrepancies and representational harms are due to frequency, we\nfind that frequency is highly correlated with a country's GDP; thus\nperpetuating historic power and wealth inequalities. We analyze the\neffectiveness of mitigation strategies; recommend that researchers report\ntraining word frequencies; and recommend future work for the community to\ndefine and design representational guarantees.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kaitlyn Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ethayarajh_K/0/1/0/all/0/1\">Kawin Ethayarajh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurafsky_D/0/1/0/all/0/1\">Dan Jurafsky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extracting Latent Steering Vectors from Pretrained Language Models. (arXiv:2205.05124v1 [cs.CL])","link":"http://arxiv.org/abs/2205.05124","description":"<p>Prior work on controllable text generation has focused on learning how to\ncontrol language models through trainable decoding, smart-prompt design, or\nfine-tuning based on a desired objective. We hypothesize that the information\nneeded to steer the model to generate a target sentence is already encoded\nwithin the model. Accordingly, we explore a different approach altogether:\nextracting latent vectors directly from pretrained language model decoders\nwithout fine-tuning. Experiments show that there exist steering vectors, which,\nwhen added to the hidden states of the language model, generate a target\nsentence nearly perfectly (&gt; 99 BLEU) for English sentences from a variety of\ndomains. We show that vector arithmetic can be used for unsupervised sentiment\ntransfer on the Yelp sentiment benchmark, with performance comparable to models\ntailored to this task. We find that distances between steering vectors reflect\nsentence similarity when evaluated on a textual similarity benchmark (STS-B),\noutperforming pooled hidden states of models. Finally, we present an analysis\nof the intrinsic properties of the steering vectors. Taken together, our\nresults suggest that frozen LMs can be effectively controlled through their\nlatent steering space.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Subramani_N/0/1/0/all/0/1\">Nishant Subramani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suresh_N/0/1/0/all/0/1\">Nivedita Suresh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peters_M/0/1/0/all/0/1\">Matthew E. Peters</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Human Language Modeling. (arXiv:2205.05128v1 [cs.CL])","link":"http://arxiv.org/abs/2205.05128","description":"<p>Natural language is generated by people, yet traditional language modeling\nviews words or documents as if generated independently. Here, we propose human\nlanguage modeling (HuLM), a hierarchical extension to the language modeling\nproblem whereby a human-level exists to connect sequences of documents (e.g.\nsocial media messages) and capture the notion that human language is moderated\nby changing human states. We introduce, HaRT, a large-scale transformer model\nfor the HuLM task, pre-trained on approximately 100,000 social media users, and\ndemonstrate its effectiveness in terms of both language modeling (perplexity)\nfor social media and fine-tuning for 4 downstream tasks spanning document- and\nuser-levels: stance detection, sentiment classification, age estimation, and\npersonality assessment. Results on all tasks meet or surpass the current\nstate-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Soni_N/0/1/0/all/0/1\">Nikita Soni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matero_M/0/1/0/all/0/1\">Matthew Matero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_N/0/1/0/all/0/1\">Niranjan Balasubramanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_H/0/1/0/all/0/1\">H. Andrew Schwartz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unifying Language Learning Paradigms. (arXiv:2205.05131v1 [cs.CL])","link":"http://arxiv.org/abs/2205.05131","description":"<p>Existing pre-trained models are generally geared towards a particular class\nof problems. To date, there seems to be still no consensus on what the right\narchitecture and pre-training setup should be. This paper presents a unified\nframework for pre-training models that are universally effective across\ndatasets and setups. We begin by disentangling architectural archetypes with\npre-training objectives -- two concepts that are commonly conflated. Next, we\npresent a generalized and unified perspective for self-supervision in NLP and\nshow how different pre-training objectives can be cast as one another and how\ninterpolating between different objectives can be effective. We then propose\nMixture-of-Denoisers (MoD), a pre-training objective that combines diverse\npre-training paradigms together. We furthermore introduce a notion of mode\nswitching, wherein downstream fine-tuning is associated with specific\npre-training schemes. We conduct extensive ablative experiments to compare\nmultiple pre-training objectives and find that our method pushes the\nPareto-frontier by outperforming T5 and/or GPT-like models across multiple\ndiverse setups. Finally, by scaling our model up to 20B parameters, we achieve\nSOTA performance on 50 well-established supervised NLP tasks ranging from\nlanguage generation (with automated and human evaluation), language\nunderstanding, text classification, question answering, commonsense reasoning,\nlong text reasoning, structured knowledge grounding and information retrieval.\nOur model also achieve strong results at in-context learning, outperforming\n175B GPT-3 on zero-shot SuperGLUE and tripling the performance of T5-XXL on\none-shot summarization. We release Flax-based T5X model checkpoints for the 20B\nmodel at\n\\url{https://github.com/google-research/google-research/tree/master/ul2}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1\">Mostafa Dehghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_V/0/1/0/all/0/1\">Vinh Q. Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_X/0/1/0/all/0/1\">Xavier Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahri_D/0/1/0/all/0/1\">Dara Bahri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuster_T/0/1/0/all/0/1\">Tal Schuster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Huaixiu Steven Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Houlsby_N/0/1/0/all/0/1\">Neil Houlsby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1\">Donald Metzler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sibylvariant Transformations for Robust Text Classification. (arXiv:2205.05137v1 [cs.CL])","link":"http://arxiv.org/abs/2205.05137","description":"<p>The vast majority of text transformation techniques in NLP are inherently\nlimited in their ability to expand input space coverage due to an implicit\nconstraint to preserve the original class label. In this work, we propose the\nnotion of sibylvariance (SIB) to describe the broader set of transforms that\nrelax the label-preserving constraint, knowably vary the expected class, and\nlead to significantly more diverse input distributions. We offer a unified\nframework to organize all data transformations, including two types of SIB: (1)\nTransmutations convert one discrete kind into another, (2) Mixture Mutations\nblend two or more classes together. To explore the role of sibylvariance within\nNLP, we implemented 41 text transformations, including several novel techniques\nlike Concept2Sentence and SentMix. Sibylvariance also enables a unique form of\nadaptive training that generates new input mixtures for the most confused class\npairs, challenging the learner to differentiate with greater nuance. Our\nexperiments on six benchmark datasets strongly support the efficacy of\nsibylvariance for generalization performance, defect detection, and adversarial\nrobustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Harel_Canada_F/0/1/0/all/0/1\">Fabrice Harel-Canada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gulzar_M/0/1/0/all/0/1\">Muhammad Ali Gulzar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Miryung Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reducing Activation Recomputation in Large Transformer Models. (arXiv:2205.05198v1 [cs.LG])","link":"http://arxiv.org/abs/2205.05198","description":"<p>Training large transformer models is one of the most important computational\nchallenges of modern AI. In this paper, we show how to significantly accelerate\ntraining of large transformer models by reducing activation recomputation.\nActivation recomputation is commonly used to work around memory capacity\nconstraints. Rather than storing activations for backpropagation, they are\ntraditionally recomputed, which saves memory but adds redundant compute. In\nthis work, we show most of this redundant compute is unnecessary because we can\nreduce memory consumption sufficiently without it. We present two novel yet\nvery simple techniques: sequence parallelism and selective activation\nrecomputation. In conjunction with tensor parallelism, these techniques almost\neliminate the need to recompute activations. We evaluate our approach on\nlanguage models up to one trillion parameters in scale and show that our method\nreduces activation memory by 5x, while reducing execution time overhead from\nactivation recomputation by over 90%. For example, when training a 530B\nparameter GPT-3 style model on 2240 NVIDIA A100 GPUs, we achieve a Model Flops\nUtilization of 54.2%, which is 29% faster than the 42.1% we achieve using\nrecomputation. Our implementation will be available in both Megatron-LM and\nNeMo-Megatron.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Korthikanti_V/0/1/0/all/0/1\">Vijay Korthikanti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Casper_J/0/1/0/all/0/1\">Jared Casper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lym_S/0/1/0/all/0/1\">Sangkug Lym</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McAfee_L/0/1/0/all/0/1\">Lawrence McAfee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andersch_M/0/1/0/all/0/1\">Michael Andersch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shoeybi_M/0/1/0/all/0/1\">Mohammad Shoeybi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Catanzaro_B/0/1/0/all/0/1\">Bryan Catanzaro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Separator-Transducer-Segmenter: Streaming Recognition and Segmentation of Multi-party Speech. (arXiv:2205.05199v1 [eess.AS])","link":"http://arxiv.org/abs/2205.05199","description":"<p>Streaming recognition and segmentation of multi-party conversations with\noverlapping speech is crucial for the next generation of voice assistant\napplications. In this work we address its challenges discovered in the previous\nwork on multi-turn recurrent neural network transducer (MT-RNN-T) with a novel\napproach, separator-transducer-segmenter (STS), that enables tighter\nintegration of speech separation, recognition and segmentation in a single\nmodel. First, we propose a new segmentation modeling strategy through\nstart-of-turn and end-of-turn tokens that improves segmentation without\nrecognition accuracy degradation. Second, we further improve both speech\nrecognition and segmentation accuracy through an emission regularization\nmethod, FastEmit, and multi-task training with speech activity information as\nan additional training signal. Third, we experiment with end-of-turn emission\nlatency penalty to improve end-point detection for each speaker turn. Finally,\nwe establish a novel framework for segmentation analysis of multi-party\nconversations through emission latency metrics. With our best model, we report\n4.6% abs. turn counting accuracy improvement and 17% rel. word error rate (WER)\nimprovement on LibriCSS dataset compared to the previously published work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Sklyar_I/0/1/0/all/0/1\">Ilya Sklyar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Piunova_A/0/1/0/all/0/1\">Anna Piunova</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Osendorfer_C/0/1/0/all/0/1\">Christian Osendorfer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Improved Zero-shot Voice Conversion with Conditional DSVAE. (arXiv:2205.05227v1 [eess.AS])","link":"http://arxiv.org/abs/2205.05227","description":"<p>Disentangling content and speaking style information is essential for\nzero-shot non-parallel voice conversion (VC). Our previous study investigated a\nnovel framework with disentangled sequential variational autoencoder (DSVAE) as\nthe backbone for information decomposition. We have demonstrated that\nsimultaneous disentangling content embedding and speaker embedding from one\nutterance is feasible for zero-shot VC. In this study, we continue the\ndirection by raising one concern about the prior distribution of content branch\nin the DSVAE baseline. We find the random initialized prior distribution will\nforce the content embedding to reduce the phonetic-structure information during\nthe learning process, which is not a desired property. Here, we seek to achieve\na better content embedding with more phonetic information preserved. We propose\nconditional DSVAE, a new model that enables content bias as a condition to the\nprior modeling and reshapes the content embedding sampled from the posterior\ndistribution. In our experiment on the VCTK dataset, we demonstrate that\ncontent embeddings derived from the conditional DSVAE overcome the randomness\nand achieve a much better phoneme classification accuracy, a stabilized\nvocalization and a better zero-shot VC performance compared with the\ncompetitive DSVAE baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lian_J/0/1/0/all/0/1\">Jiachen Lian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_C/0/1/0/all/0/1\">Chunlei Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Anumanchipalli_G/0/1/0/all/0/1\">Gopala Krishna Anumanchipalli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_D/0/1/0/all/0/1\">Dong Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Relational Triple Extraction: One Step is Enough. (arXiv:2205.05270v1 [cs.CL])","link":"http://arxiv.org/abs/2205.05270","description":"<p>Extracting relational triples from unstructured text is an essential task in\nnatural language processing and knowledge graph construction. Existing\napproaches usually contain two fundamental steps: (1) finding the boundary\npositions of head and tail entities; (2) concatenating specific tokens to form\ntriples. However, nearly all previous methods suffer from the problem of error\naccumulation, i.e., the boundary recognition error of each entity in step (1)\nwill be accumulated into the final combined triples. To solve the problem, in\nthis paper, we introduce a fresh perspective to revisit the triple extraction\ntask, and propose a simple but effective model, named DirectRel. Specifically,\nthe proposed model first generates candidate entities through enumerating token\nsequences in a sentence, and then transforms the triple extraction task into a\nlinking problem on a \"head $\\rightarrow$ tail\" bipartite graph. By doing so,\nall triples can be directly extracted in only one step. Extensive experimental\nresults on two widely used datasets demonstrate that the proposed model\nperforms better than the state-of-the-art baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shang_Y/0/1/0/all/0/1\">Yu-Ming Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Heyan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1\">Wei Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_X/0/1/0/all/0/1\">Xian-Ling Mao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"User Guide for KOTE: Korean Online Comments Emotions Dataset. (arXiv:2205.05300v1 [cs.CL])","link":"http://arxiv.org/abs/2205.05300","description":"<p>Sentiment analysis that classifies data into positive or negative has been\ndominantly used to recognize emotional aspects of texts, despite the deficit of\nthorough examination of emotional meanings. Recently, corpora labeled with more\nthan just valence are built to exceed this limit. However, most Korean emotion\ncorpora are small in the number of instances and cover a limited range of\nemotions. We introduce KOTE dataset. KOTE contains 50k (250k cases) Korean\nonline comments, each of which is manually labeled for 43 emotion labels or one\nspecial label (NO EMOTION) by crowdsourcing (Ps = 3,048). The emotion taxonomy\nof the 43 emotions is systematically established by cluster analysis of Korean\nemotion concepts expressed on word embedding space. After explaining how KOTE\nis developed, we also discuss the results of finetuning and analysis for social\ndiscrimination in the corpus.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jeon_D/0/1/0/all/0/1\">Duyoung Jeon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Junho Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1\">Cheongtag Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Unified Prompt Tuning for Few-shot Text Classification. (arXiv:2205.05313v1 [cs.CL])","link":"http://arxiv.org/abs/2205.05313","description":"<p>Prompt-based fine-tuning has boosted the performance of Pre-trained Language\nModels (PLMs) on few-shot text classification by employing task-specific\nprompts. Yet, PLMs are unfamiliar with prompt-style expressions during\npre-training, which limits the few-shot learning performance on downstream\ntasks. It would be desirable if the models can acquire some prompting knowledge\nbefore adaptation to specific NLP tasks. We present the Unified Prompt Tuning\n(UPT) framework, leading to better few-shot text classification for BERT-style\nmodels by explicitly capturing prompting semantics from non-target NLP\ndatasets. In UPT, a novel paradigm Prompt-Options-Verbalizer is proposed for\njoint prompt learning across different NLP tasks, forcing PLMs to capture\ntask-invariant prompting knowledge. We further design a self-supervised task\nnamed Knowledge-enhanced Selective Masked Language Modeling to improve the\nPLM's generalization abilities for accurate adaptation to previously unseen\ntasks. After multi-task learning across multiple tasks, the PLM can be better\nprompt-tuned towards any dissimilar target tasks in low-resourced settings.\nExperiments over a variety of NLP tasks show that UPT consistently outperforms\nstate-of-the-arts for prompt-based fine-tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_F/0/1/0/all/0/1\">Fuli Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_M/0/1/0/all/0/1\">Minghui Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Q/0/1/0/all/0/1\">Qiuhui Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Songfang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1\">Ming Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Efficient Summation Algorithm for the Accuracy, Convergence and Reproducibility of Parallel Numerical Methods. (arXiv:2205.05339v1 [cs.CL])","link":"http://arxiv.org/abs/2205.05339","description":"<p>Nowadays, parallel computing is ubiquitous in several application fields,\nboth in engineering and science. The computations rely on the floating-point\narithmetic specified by the IEEE754 Standard. In this context, an elementary\nbrick of computation, used everywhere, is the sum of a sequence of numbers.\nThis sum is subject to many numerical errors in floating-point arithmetic. To\nalleviate this issue, we have introduced a new parallel algorithm for summing a\nsequence of floating-point numbers. This algorithm which scales up easily with\nthe number of processors, adds numbers of the same exponents first. In this\narticle, our main contribution is an extensive analysis of its efficiency with\nrespect to several properties: accuracy, convergence and reproducibility. In\norder to show the usefulness of our algorithm, we have chosen a set of\nrepresentative numerical methods which are Simpson, Jacobi, LU factorization\nand the Iterated power method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Benmouhoub_F/0/1/0/all/0/1\">Farah Benmouhoub</a> (UPVD), <a href=\"http://arxiv.org/find/cs/1/au:+Garoche_P/0/1/0/all/0/1\">Pierre-Lo&#xef;c Garoche</a> (ENAC), <a href=\"http://arxiv.org/find/cs/1/au:+Martel_M/0/1/0/all/0/1\">Matthieu Martel</a> (UPVD)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pre-trained Language Models as Re-Annotators. (arXiv:2205.05368v1 [cs.CL])","link":"http://arxiv.org/abs/2205.05368","description":"<p>Annotation noise is widespread in datasets, but manually revising a flawed\ncorpus is time-consuming and error-prone. Hence, given the prior knowledge in\nPre-trained Language Models and the expected uniformity across all annotations,\nwe attempt to reduce annotation noise in the corpus through two tasks\nautomatically: (1) Annotation Inconsistency Detection that indicates the\ncredibility of annotations, and (2) Annotation Error Correction that rectifies\nthe abnormal annotations.\n</p>\n<p>We investigate how to acquire semantic sensitive annotation representations\nfrom Pre-trained Language Models, expecting to embed the examples with\nidentical annotations to the mutually adjacent positions even without\nfine-tuning. We proposed a novel credibility score to reveal the likelihood of\nannotation inconsistencies based on the neighbouring consistency. Then, we\nfine-tune the Pre-trained Language Models based classifier with\ncross-validation for annotation correction. The annotation corrector is further\nelaborated with two approaches: (1) soft labelling by Kernel Density Estimation\nand (2) a novel distant-peer contrastive loss.\n</p>\n<p>We study the re-annotation in relation extraction and create a new manually\nrevised dataset, Re-DocRED, for evaluating document-level re-annotation. The\nproposed credibility scores show promising agreement with human revisions,\nachieving a Binary F1 of 93.4 and 72.5 in detecting inconsistencies on TACRED\nand DocRED respectively. Moreover, the neighbour-aware classifiers based on\ndistant-peer contrastive learning and uncertain labels achieve Macro F1 up to\n66.2 and 57.8 in correcting annotations on TACRED and DocRED respectively.\nThese improvements are not merely theoretical: Rather, automatically denoised\ntraining sets demonstrate up to 3.6% performance improvement for\nstate-of-the-art relation extraction models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shu_C/0/1/0/all/0/1\">Chang Shu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AutoKE: An automatic knowledge embedding framework for scientific machine learning. (arXiv:2205.05390v1 [cs.LG])","link":"http://arxiv.org/abs/2205.05390","description":"<p>Imposing physical constraints on neural networks as a method of knowledge\nembedding has achieved great progress in solving physical problems described by\ngoverning equations. However, for many engineering problems, governing\nequations often have complex forms, including complex partial derivatives or\nstochastic physical fields, which results in significant inconveniences from\nthe perspective of implementation. In this paper, a scientific machine learning\nframework, called AutoKE, is proposed, and a reservoir flow problem is taken as\nan instance to demonstrate that this framework can effectively automate the\nprocess of embedding physical knowledge. In AutoKE, an emulator comprised of\ndeep neural networks (DNNs) is built for predicting the physical variables of\ninterest. An arbitrarily complex equation can be parsed and automatically\nconverted into a computational graph through the equation parser module, and\nthe fitness of the emulator to the governing equation is evaluated via\nautomatic differentiation. Furthermore, the fixed weights in the loss function\nare substituted with adaptive weights by incorporating the Lagrangian dual\nmethod. Neural architecture search (NAS) is also introduced into the AutoKE to\nselect an optimal network architecture of the emulator according to the\nspecific problem. Finally, we apply transfer learning to enhance the\nscalability of the emulator. In experiments, the framework is verified by a\nseries of physical problems in which it can automatically embed physical\nknowledge into an emulator without heavy hand-coding. The results demonstrate\nthat the emulator can not only make accurate predictions, but also be applied\nto similar problems with high efficiency via transfer learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_M/0/1/0/all/0/1\">Mengge Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuntian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongxiao Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Query-Based Keyphrase Extraction from Long Documents. (arXiv:2205.05391v1 [cs.CL])","link":"http://arxiv.org/abs/2205.05391","description":"<p>Transformer-based architectures in natural language processing force input\nsize limits that can be problematic when long documents need to be processed.\nThis paper overcomes this issue for keyphrase extraction by chunking the long\ndocuments while keeping a global context as a query defining the topic for\nwhich relevant keyphrases should be extracted. The developed system employs a\npre-trained BERT model and adapts it to estimate the probability that a given\ntext span forms a keyphrase. We experimented using various context sizes on two\npopular datasets, Inspec and SemEval, and a large novel dataset. The presented\nresults show that a shorter context with a query overcomes a longer one without\nthe query on long documents.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Docekal_M/0/1/0/all/0/1\">Martin Docekal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smrz_P/0/1/0/all/0/1\">Pavel Smrz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ALIGNMEET: A Comprehensive Tool for Meeting Annotation, Alignment, and Evaluation. (arXiv:2205.05433v1 [cs.CL])","link":"http://arxiv.org/abs/2205.05433","description":"<p>Summarization is a challenging problem, and even more challenging is to\nmanually create, correct, and evaluate the summaries. The severity of the\nproblem grows when the inputs are multi-party dialogues in a meeting setup. To\nfacilitate the research in this area, we present ALIGNMEET, a comprehensive\ntool for meeting annotation, alignment, and evaluation. The tool aims to\nprovide an efficient and clear interface for fast annotation while mitigating\nthe risk of introducing errors. Moreover, we add an evaluation mode that\nenables a comprehensive quality evaluation of meeting minutes. To the best of\nour knowledge, there is no such tool available. We release the tool as open\nsource. It is also directly installable from PyPI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Polak_P/0/1/0/all/0/1\">Peter Pol&#xe1;k</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1\">Muskaan Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nedoluzhko_A/0/1/0/all/0/1\">Anna Nedoluzhko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bojar_O/0/1/0/all/0/1\">Ond&#x159;ej Bojar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Building for Tomorrow: Assessing the Temporal Persistence of Text Classifiers. (arXiv:2205.05435v1 [cs.CL])","link":"http://arxiv.org/abs/2205.05435","description":"<p>Performance of text classification models can drop over time when new data to\nbe classified is more distant in time from the data used for training, due to\nnaturally occurring changes in the data, such as vocabulary change. A solution\nto this is to continually label new data to retrain the model, which is,\nhowever, often unaffordable to be performed regularly due to its associated\ncost. This raises important research questions on the design of text\nclassification models that are intended to persist over time: do all embedding\nmodels and classification algorithms exhibit similar performance drops over\ntime and is the performance drop more prominent in some tasks or datasets than\nothers? With the aim of answering these research questions, we perform\nlongitudinal classification experiments on three datasets spanning between 6\nand 19 years. Findings from these experiments inform the design of text\nclassification models with the aim of preserving performance over time,\ndiscussing the extent to which one can rely on classification models trained\nfrom temporally distant training data, as well as how the characteristics of\nthe dataset impact this.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alkhalifa_R/0/1/0/all/0/1\">Rabab Alkhalifa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kochkina_E/0/1/0/all/0/1\">Elena Kochkina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zubiaga_A/0/1/0/all/0/1\">Arkaitz Zubiaga</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Making Pre-trained Language Models Good Long-tailed Learners. (arXiv:2205.05461v1 [cs.CL])","link":"http://arxiv.org/abs/2205.05461","description":"<p>Prompt-tuning has shown appealing performance in few-shot classification by\nvirtue of its capability in effectively exploiting pre-trained knowledge. This\nmotivates us to check the hypothesis that prompt-tuning is also a promising\nchoice for long-tailed classification, since the tail classes are intuitively\nfew-shot ones. To achieve this aim, we conduct empirical studies to examine the\nhypothesis. The results demonstrate that prompt-tuning exactly makes\npre-trained language models at least good long-tailed learners. For intuitions\non why prompt-tuning can achieve good performance in long-tailed\nclassification, we carry out an in-depth analysis by progressively bridging the\ngap between prompt-tuning and commonly used fine-tuning. The summary is that\nthe classifier structure and parameterization form the key to making good\nlong-tailed learners, in comparison with the less important input structure.\nFinally, we verify the applicability of our finding to few-shot classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_L/0/1/0/all/0/1\">Lei Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Dawei Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Utilizing coarse-grained data in low-data settings for event extraction. (arXiv:2205.05468v1 [cs.CL])","link":"http://arxiv.org/abs/2205.05468","description":"<p>Annotating text data for event information extraction systems is hard,\nexpensive, and error-prone. We investigate the feasibility of integrating\ncoarse-grained data (document or sentence labels), which is far more feasible\nto obtain, instead of annotating more documents. We utilize a multi-task model\nwith two auxiliary tasks, document and sentence binary classification, in\naddition to the main task of token classification. We perform a series of\nexperiments with varying data regimes for the aforementioned integration.\nResults show that while introducing extra coarse-grained data offers greater\nimprovement and robustness, a gain is still possible with only the addition of\nnegative documents that have no information on any event.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mutlu_O/0/1/0/all/0/1\">Osman Mutlu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Clinical Prompt Learning with Frozen Language Models. (arXiv:2205.05535v1 [cs.CL])","link":"http://arxiv.org/abs/2205.05535","description":"<p>Prompt learning is a new paradigm in the Natural Language Processing (NLP)\nfield which has shown impressive performance on a number of natural language\ntasks with common benchmarking text datasets in full, few-shot, and zero-shot\ntrain-evaluation setups. Recently, it has even been observed that large but\nfrozen pre-trained language models (PLMs) with prompt learning outperform\nsmaller but fine-tuned models. However, as with many recent NLP trends, the\nperformance of even the largest PLMs such as GPT-3 do not perform well on\nspecialized domains (e.g. medical text), and the common practice to achieve\nState of the Art (SoTA) results still consists of pre-training and fine-tuning\nthe PLMs on downstream tasks. The reliance on fine-tuning large PLMs is\nproblematic in clinical settings where data is often held in non-GPU\nenvironments, and more resource efficient methods of training specialized\ndomain models is crucial. We investigated the viability of prompt learning on\nclinically meaningful decision tasks and directly compared with more\ntraditional fine-tuning methods. Results are partially in line with the prompt\nlearning literature, with prompt learning able to match or improve on\ntraditional fine-tuning with substantially fewer trainable parameters and\nrequiring less training data. We argue that prompt learning therefore provides\nlower computational resource costs applicable to clinical settings, that can\nserve as an alternative to fine-tuning ever increasing in size PLMs.\nComplementary code to reproduce experiments presented in this work can be found\nat: https://github.com/NtaylorOX/Public_Clinical_Prompt.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Taylor_N/0/1/0/all/0/1\">Niall Taylor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joyce_D/0/1/0/all/0/1\">Dan Joyce</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nevado_Holgado_A/0/1/0/all/0/1\">Alejo Nevado-Holgado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kormilitzin_A/0/1/0/all/0/1\">Andrey Kormilitzin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KETOD: Knowledge-Enriched Task-Oriented Dialogue. (arXiv:2205.05589v1 [cs.CL])","link":"http://arxiv.org/abs/2205.05589","description":"<p>Existing studies in dialogue system research mostly treat task-oriented\ndialogue and chit-chat as separate domains. Towards building a human-like\nassistant that can converse naturally and seamlessly with users, it is\nimportant to build a dialogue system that conducts both types of conversations\neffectively. In this work, we investigate how task-oriented dialogue and\nknowledge-grounded chit-chat can be effectively integrated into a single model.\nTo this end, we create a new dataset, KETOD (Knowledge-Enriched Task-Oriented\nDialogue), where we naturally enrich task-oriented dialogues with chit-chat\nbased on relevant entity knowledge. We also propose two new models,\nSimpleToDPlus and Combiner, for the proposed task. Experimental results on both\nautomatic and human evaluations show that the proposed methods can\nsignificantly improve the performance in knowledge-enriched response generation\nwhile maintaining a competitive task-oriented dialog performance. We believe\nour new dataset will be a valuable resource for future studies. Our dataset and\ncode are publicly available at \\url{https://github.com/facebookresearch/ketod}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhiyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moon_S/0/1/0/all/0/1\">Seungwhan Moon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sankar_C/0/1/0/all/0/1\">Chinnadhurai Sankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crook_P/0/1/0/all/0/1\">Paul Crook</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A neural prosody encoder for end-ro-end dialogue act classification. (arXiv:2205.05590v1 [cs.CL])","link":"http://arxiv.org/abs/2205.05590","description":"<p>Dialogue act classification (DAC) is a critical task for spoken language\nunderstanding in dialogue systems. Prosodic features such as energy and pitch\nhave been shown to be useful for DAC. Despite their importance, little research\nhas explored neural approaches to integrate prosodic features into end-to-end\n(E2E) DAC models which infer dialogue acts directly from audio signals. In this\nwork, we propose an E2E neural architecture that takes into account the need\nfor characterizing prosodic phenomena co-occurring at different levels inside\nan utterance. A novel part of this architecture is a learnable gating mechanism\nthat assesses the importance of prosodic features and selectively retains core\ninformation necessary for E2E DAC. Our proposed model improves DAC accuracy by\n1.07% absolute across three publicly available benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_K/0/1/0/all/0/1\">Kai Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knox_D/0/1/0/all/0/1\">Dillon Knox</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radfar_M/0/1/0/all/0/1\">Martin Radfar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1\">Thanh Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_M/0/1/0/all/0/1\">Markus Muller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strimel_G/0/1/0/all/0/1\">Grant P. Strimel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Susanj_N/0/1/0/all/0/1\">Nathan Susanj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mouchtaris_A/0/1/0/all/0/1\">Athanasios Mouchtaris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Omologo_M/0/1/0/all/0/1\">Maurizio Omologo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Identifying Moments of Change from Longitudinal User Text. (arXiv:2205.05593v1 [cs.CL])","link":"http://arxiv.org/abs/2205.05593","description":"<p>Identifying changes in individuals' behaviour and mood, as observed via\ncontent shared on online platforms, is increasingly gaining importance. Most\nresearch to-date on this topic focuses on either: (a) identifying individuals\nat risk or with a certain mental health condition given a batch of posts or (b)\nproviding equivalent labels at the post level. A disadvantage of such work is\nthe lack of a strong temporal component and the inability to make longitudinal\nassessments following an individual's trajectory and allowing timely\ninterventions. Here we define a new task, that of identifying moments of change\nin individuals on the basis of their shared content online. The changes we\nconsider are sudden shifts in mood (switches) or gradual mood progression\n(escalations). We have created detailed guidelines for capturing moments of\nchange and a corpus of 500 manually annotated user timelines (18.7K posts). We\nhave developed a variety of baseline models drawing inspiration from related\ntasks and show that the best performance is obtained through context aware\nsequential modelling. We also introduce new metrics for capturing rare events\nin temporal windows.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tsakalidis_A/0/1/0/all/0/1\">Adam Tsakalidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nanni_F/0/1/0/all/0/1\">Federico Nanni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hills_A/0/1/0/all/0/1\">Anthony Hills</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chim_J/0/1/0/all/0/1\">Jenny Chim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jiayu Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liakata_M/0/1/0/all/0/1\">Maria Liakata</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning. (arXiv:2205.05638v1 [cs.LG])","link":"http://arxiv.org/abs/2205.05638","description":"<p>Few-shot in-context learning (ICL) enables pre-trained language models to\nperform a previously-unseen task without any gradient-based training by feeding\na small number of training examples as part of the input. ICL incurs\nsubstantial computational, memory, and storage costs because it involves\nprocessing all of the training examples every time a prediction is made.\nParameter-efficient fine-tuning (e.g. adapter modules, prompt tuning, sparse\nupdate methods, etc.) offers an alternative paradigm where a small set of\nparameters are trained to enable a model to perform the new task. In this\npaper, we rigorously compare few-shot ICL and parameter-efficient fine-tuning\nand demonstrate that the latter offers better accuracy as well as dramatically\nlower computational costs. Along the way, we introduce a new\nparameter-efficient fine-tuning method called (IA)$^3$ that scales activations\nby learned vectors, attaining stronger performance while only introducing a\nrelatively tiny amount of new parameters. We also propose a simple recipe based\non the T0 model called T-Few that can be applied to new tasks without\ntask-specific tuning or modifications. We validate the effectiveness of T-Few\non completely unseen tasks by applying it to the RAFT benchmark, attaining\nsuper-human performance for the first time and outperforming the\nstate-of-the-art by 6% absolute. All of the code used in our experiments is\npublicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Haokun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tam_D/0/1/0/all/0/1\">Derek Tam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muqeeth_M/0/1/0/all/0/1\">Mohammed Muqeeth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohta_J/0/1/0/all/0/1\">Jay Mohta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Tenghao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raffel_C/0/1/0/all/0/1\">Colin Raffel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Aggregating Pairwise Semantic Differences for Few-Shot Claim Veracity Classification. (arXiv:2205.05646v1 [cs.CL])","link":"http://arxiv.org/abs/2205.05646","description":"<p>As part of an automated fact-checking pipeline, the claim veracity\nclassification task consists in determining if a claim is supported by an\nassociated piece of evidence. The complexity of gathering labelled\nclaim-evidence pairs leads to a scarcity of datasets, particularly when dealing\nwith new domains. In this paper, we introduce SEED, a novel vector-based method\nto few-shot claim veracity classification that aggregates pairwise semantic\ndifferences for claim-evidence pairs. We build on the hypothesis that we can\nsimulate class representative vectors that capture average semantic differences\nfor claim-evidence pairs in a class, which can then be used for classification\nof new instances. We compare the performance of our method with competitive\nbaselines including fine-tuned BERT/RoBERTa models, as well as the\nstate-of-the-art few-shot veracity classification method that leverages\nlanguage model perplexity. Experiments conducted on the FEVER and SCIFACT\ndatasets show consistent improvements over competitive baselines in few-shot\nsettings. Our code is available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1\">Xia Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zubiaga_A/0/1/0/all/0/1\">Arkaitz Zubiaga</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ontology-Based and Weakly Supervised Rare Disease Phenotyping from Clinical Notes. (arXiv:2205.05656v1 [cs.CL])","link":"http://arxiv.org/abs/2205.05656","description":"<p>Computational text phenotyping is the practice of identifying patients with\ncertain disorders and traits from clinical notes. Rare diseases are challenging\nto be identified due to few cases available for machine learning and the need\nfor data annotation from domain experts. We propose a method using ontologies\nand weak supervision, with recent pre-trained contextual representations from\nBi-directional Transformers (e.g. BERT). The ontology-based framework includes\ntwo steps: (i) Text-to-UMLS, extracting phenotypes by contextually linking\nmentions to concepts in Unified Medical Language System (UMLS), with a Named\nEntity Recognition and Linking (NER+L) tool, SemEHR, and weak supervision with\ncustomised rules and contextual mention representation; (ii) UMLS-to-ORDO,\nmatching UMLS concepts to rare diseases in Orphanet Rare Disease Ontology\n(ORDO). The weakly supervised approach is proposed to learn a phenotype\nconfirmation model to improve Text-to-UMLS linking, without annotated data from\ndomain experts. We evaluated the approach on three clinical datasets of\ndischarge summaries and radiology reports from two institutions in the US and\nthe UK. Our best weakly supervised method achieved 81.4% precision and 91.4%\nrecall on extracting rare disease UMLS phenotypes from MIMIC-III discharge\nsummaries. The overall pipeline processing clinical notes can surface rare\ndisease cases, mostly uncaptured in structured data (manually assigned ICD\ncodes). Results on radiology reports from MIMIC-III and NHS Tayside were\nconsistent with the discharge summaries. We discuss the usefulness of the weak\nsupervision approach and propose directions for future studies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Hang Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suarez_Paniagua_V/0/1/0/all/0/1\">V&#xed;ctor Su&#xe1;rez-Paniagua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Huayu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Minhong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Casey_A/0/1/0/all/0/1\">Arlene Casey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davidson_E/0/1/0/all/0/1\">Emma Davidson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiaoyan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alex_B/0/1/0/all/0/1\">Beatrice Alex</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Whiteley_W/0/1/0/all/0/1\">William Whiteley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Honghan Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Identifying concept libraries from language about object structure. (arXiv:2205.05666v1 [cs.CL])","link":"http://arxiv.org/abs/2205.05666","description":"<p>Our understanding of the visual world goes beyond naming objects,\nencompassing our ability to parse objects into meaningful parts, attributes,\nand relations. In this work, we leverage natural language descriptions for a\ndiverse set of 2K procedurally generated objects to identify the parts people\nuse and the principles leading these parts to be favored over others. We\nformalize our problem as search over a space of program libraries that contain\ndifferent part concepts, using tools from machine translation to evaluate how\nwell programs expressed in each library align to human language. By combining\nnaturalistic language at scale with structured program representations, we\ndiscover a fundamental information-theoretic tradeoff governing the part\nconcepts people name: people favor a lexicon that allows concise descriptions\nof each object, while also minimizing the size of the lexicon itself.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wong_C/0/1/0/all/0/1\">Catherine Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCarthy_W/0/1/0/all/0/1\">William P. McCarthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grand_G/0/1/0/all/0/1\">Gabriel Grand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Friedman_Y/0/1/0/all/0/1\">Yoni Friedman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1\">Joshua B. Tenenbaum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andreas_J/0/1/0/all/0/1\">Jacob Andreas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hawkins_R/0/1/0/all/0/1\">Robert D. Hawkins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1\">Judith E. Fan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Augmenting Poetry Composition with Verse by Verse. (arXiv:2103.17205v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.17205","description":"<p>We describe Verse by Verse, our experiment in augmenting the creative process\nof writing poetry with an AI. We have created a group of AI poets, styled after\nvarious American classic poets, that are able to offer as suggestions generated\nlines of verse while a user is composing a poem. In this paper, we describe the\nunderlying system to offer these suggestions. This includes a generative model,\nwhich is tasked with generating a large corpus of lines of verse offline and\nwhich are then stored in an index, and a dual-encoder model that is tasked with\nrecommending the next possible set of verses from our index given the previous\nline of verse.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Uthus_D/0/1/0/all/0/1\">David Uthus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Voitovich_M/0/1/0/all/0/1\">Maria Voitovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mical_R/0/1/0/all/0/1\">R.J. Mical</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"XLM-T: Multilingual Language Models in Twitter for Sentiment Analysis and Beyond. (arXiv:2104.12250v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.12250","description":"<p>Language models are ubiquitous in current NLP, and their multilingual\ncapacity has recently attracted considerable attention. However, current\nanalyses have almost exclusively focused on (multilingual variants of) standard\nbenchmarks, and have relied on clean pre-training and task-specific corpora as\nmultilingual signals. In this paper, we introduce XLM-T, a model to train and\nevaluate multilingual language models in Twitter. In this paper we provide: (1)\na new strong multilingual baseline consisting of an XLM-R (Conneau et al. 2020)\nmodel pre-trained on millions of tweets in over thirty languages, alongside\nstarter code to subsequently fine-tune on a target task; and (2) a set of\nunified sentiment analysis Twitter datasets in eight different languages and a\nXLM-T model fine-tuned on them.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Barbieri_F/0/1/0/all/0/1\">Francesco Barbieri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anke_L/0/1/0/all/0/1\">Luis Espinosa Anke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camacho_Collados_J/0/1/0/all/0/1\">Jose Camacho-Collados</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Re-evaluating Word Mover's Distance. (arXiv:2105.14403v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2105.14403","description":"<p>The word mover's distance (WMD) is a fundamental technique for measuring the\nsimilarity of two documents. As the crux of WMD, it can take advantage of the\nunderlying geometry of the word space by employing an optimal transport\nformulation. The original study on WMD reported that WMD outperforms classical\nbaselines such as bag-of-words (BOW) and TF-IDF by significant margins in\nvarious datasets. In this paper, we point out that the evaluation in the\noriginal study could be misleading. We re-evaluate the performances of WMD and\nthe classical baselines and find that the classical baselines are competitive\nwith WMD if we employ an appropriate preprocessing, i.e., L1 normalization. In\naddition, We introduce an analogy between WMD and L1-normalized BOW and find\nthat not only the performance of WMD but also the distance values resemble\nthose of BOW in high dimensional spaces.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sato_R/0/1/0/all/0/1\">Ryoma Sato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamada_M/0/1/0/all/0/1\">Makoto Yamada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kashima_H/0/1/0/all/0/1\">Hisashi Kashima</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EENLP: Cross-lingual Eastern European NLP Index. (arXiv:2108.02605v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.02605","description":"<p>Motivated by the sparsity of NLP resources for Eastern European languages, we\npresent a broad index of existing Eastern European language resources (90+\ndatasets and 45+ models) published as a github repository open for updates from\nthe community. Furthermore, to support the evaluation of commonsense reasoning\ntasks, we provide hand-crafted cross-lingual datasets for five different\nsemantic tasks (namely news categorization, paraphrase detection, Natural\nLanguage Inference (NLI) task, tweet sentiment detection, and news sentiment\ndetection) for some of the Eastern European languages. We perform several\nexperiments with the existing multilingual models on these datasets to define\nthe performance baselines and compare them to the existing results for other\nlanguages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tikhonov_A/0/1/0/all/0/1\">Alexey Tikhonov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malkhasov_A/0/1/0/all/0/1\">Alex Malkhasov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manoshin_A/0/1/0/all/0/1\">Andrey Manoshin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dima_G/0/1/0/all/0/1\">George Dima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cserhati_R/0/1/0/all/0/1\">R&#xe9;ka Cserh&#xe1;ti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asif_M/0/1/0/all/0/1\">Md.Sadek Hossain Asif</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sardi_M/0/1/0/all/0/1\">Matt S&#xe1;rdi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Topic Model Supervised by Understanding Map. (arXiv:2110.06043v11 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.06043","description":"<p>Inspired by the notion of Center of Mass in physics, an extension called\nSemantic Center of Mass (SCOM) is proposed, and used to discover the abstract\n\"topic\" of a document. The notion is under a framework model called\nUnderstanding Map Supervised Topic Model (UM-S-TM). The devising aim of UM-S-TM\nis to let both the document content and a semantic network -- specifically,\nUnderstanding Map -- play a role, in interpreting the meaning of a document.\nBased on different justifications, three possible methods are devised to\ndiscover the SCOM of a document. Some experiments on artificial documents and\nUnderstanding Maps are conducted to test their outcomes. In addition, its\nability of vectorization of documents and capturing sequential information are\ntested. We also compared UM-S-TM with probabilistic topic models like Latent\nDirichlet Allocation (LDA) and probabilistic Latent Semantic Analysis (pLSA).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Gangli Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Linguistic Frameworks Go Toe-to-Toe at Neuro-Symbolic Language Modeling. (arXiv:2112.07874v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.07874","description":"<p>We examine the extent to which, in principle, linguistic graph\nrepresentations can complement and improve neural language modeling. With an\nensemble setup consisting of a pretrained Transformer and ground-truth graphs\nfrom one of 7 different formalisms, we find that, overall, semantic\nconstituency structures are most useful to language modeling performance --\noutpacing syntactic constituency structures as well as syntactic and semantic\ndependency structures. Further, effects vary greatly depending on\npart-of-speech class. In sum, our findings point to promising tendencies in\nneuro-symbolic language modeling and invite future research quantifying the\ndesign choices made by different formalisms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Prange_J/0/1/0/all/0/1\">Jakob Prange</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schneider_N/0/1/0/all/0/1\">Nathan Schneider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1\">Lingpeng Kong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"QuALITY: Question Answering with Long Input Texts, Yes!. (arXiv:2112.08608v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.08608","description":"<p>To enable building and testing models on long-document comprehension, we\nintroduce QuALITY, a multiple-choice QA dataset with context passages in\nEnglish that have an average length of about 5,000 tokens, much longer than\ntypical current models can process. Unlike in prior work with passages, our\nquestions are written and validated by contributors who have read the entire\npassage, rather than relying on summaries or excerpts. In addition, only half\nof the questions are answerable by annotators working under tight time\nconstraints, indicating that skimming and simple search are not enough to\nconsistently perform well. Our baseline models perform poorly on this task\n(55.4%) and significantly lag behind human performance (93.5%).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pang_R/0/1/0/all/0/1\">Richard Yuanzhe Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parrish_A/0/1/0/all/0/1\">Alicia Parrish</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_N/0/1/0/all/0/1\">Nitish Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nangia_N/0/1/0/all/0/1\">Nikita Nangia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phang_J/0/1/0/all/0/1\">Jason Phang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1\">Angelica Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padmakumar_V/0/1/0/all/0/1\">Vishakh Padmakumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Johnny Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thompson_J/0/1/0/all/0/1\">Jana Thompson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">He He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bowman_S/0/1/0/all/0/1\">Samuel R. Bowman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Models in the Loop: Aiding Crowdworkers with Generative Annotation Assistants. (arXiv:2112.09062v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.09062","description":"<p>In Dynamic Adversarial Data Collection (DADC), human annotators are tasked\nwith finding examples that models struggle to predict correctly. Models trained\non DADC-collected training data have been shown to be more robust in\nadversarial and out-of-domain settings, and are considerably harder for humans\nto fool. However, DADC is more time-consuming than traditional data collection\nand thus more costly per annotated example. In this work, we examine whether we\ncan maintain the advantages of DADC, without incurring the additional cost. To\nthat end, we introduce Generative Annotation Assistants (GAAs),\ngenerator-in-the-loop models that provide real-time suggestions that annotators\ncan either approve, modify, or reject entirely. We collect training datasets in\ntwenty experimental settings and perform a detailed analysis of this approach\nfor the task of extractive question answering (QA) for both standard and\nadversarial data collection. We demonstrate that GAAs provide significant\nefficiency benefits with over a 30% annotation speed-up, while leading to over\na 5x improvement in model fooling rates. In addition, we find that using\nGAA-assisted training data leads to higher downstream model performance on a\nvariety of question answering tasks over adversarial data collection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bartolo_M/0/1/0/all/0/1\">Max Bartolo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thrush_T/0/1/0/all/0/1\">Tristan Thrush</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedel_S/0/1/0/all/0/1\">Sebastian Riedel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stenetorp_P/0/1/0/all/0/1\">Pontus Stenetorp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1\">Robin Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiela_D/0/1/0/all/0/1\">Douwe Kiela</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Multimodal German Dataset for Automatic Lip Reading Systems and Transfer Learning. (arXiv:2202.13403v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.13403","description":"<p>Large datasets as required for deep learning of lip reading do not exist in\nmany languages. In this paper we present the dataset GLips (German Lips)\nconsisting of 250,000 publicly available videos of the faces of speakers of the\nHessian Parliament, which was processed for word-level lip reading using an\nautomatic pipeline. The format is similar to that of the English language LRW\n(Lip Reading in the Wild) dataset, with each video encoding one word of\ninterest in a context of 1.16 seconds duration, which yields compatibility for\nstudying transfer learning between both datasets. By training a deep neural\nnetwork, we investigate whether lip reading has language-independent features,\nso that datasets of different languages can be used to improve lip reading\nmodels. We demonstrate learning from scratch and show that transfer learning\nfrom LRW to GLips and vice versa improves learning speed and performance, in\nparticular for the validation set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schwiebert_G/0/1/0/all/0/1\">Gerald Schwiebert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weber_C/0/1/0/all/0/1\">Cornelius Weber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1\">Leyuan Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siqueira_H/0/1/0/all/0/1\">Henrique Siqueira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wermter_S/0/1/0/all/0/1\">Stefan Wermter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Linking Theories and Methods in Cognitive Sciences via Joint Embedding of the Scientific Literature: The Example of Cognitive Control. (arXiv:2203.11016v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2203.11016","description":"<p>Traditionally, theory and practice of Cognitive Control are linked via\nliterature reviews by human domain experts. This approach, however, is\ninadequate to track the ever-growing literature. It may also be biased, and\nyield redundancies and confusion.\n</p>\n<p>Here we present an alternative approach. We performed automated text analyses\non a large body of scientific texts to create a joint representation of tasks\nand constructs. More specifically, 385,705 scientific abstracts were first\nmapped into an embedding space using a transformers-based language model.\nDocument embeddings were then used to identify a task-construct graph embedding\nthat grounds constructs on tasks and supports nuanced meaning of the constructs\nby taking advantage of constrained random walks in the graph.\n</p>\n<p>This joint task-construct graph embedding, can be queried to generate task\nbatteries targeting specific constructs, may reveal knowledge gaps in the\nliterature, and inspire new tasks and novel hypotheses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ansarinia_M/0/1/0/all/0/1\">Morteza Ansarinia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schrater_P/0/1/0/all/0/1\">Paul Schrater</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cardoso_Leite_P/0/1/0/all/0/1\">Pedro Cardoso-Leite</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CUNI-KIT System for Simultaneous Speech Translation Task at IWSLT 2022. (arXiv:2204.06028v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.06028","description":"<p>In this paper, we describe our submission to the Simultaneous Speech\nTranslation at IWSLT 2022. We explore strategies to utilize an offline model in\na simultaneous setting without the need to modify the original model. In our\nexperiments, we show that our onlinization algorithm is almost on par with the\noffline setting while being $3\\times$ faster than offline in terms of latency\non the test set. We also show that the onlinized offline model outperforms the\nbest IWSLT2021 simultaneous system in medium and high latency regimes and is\nalmost on par in the low latency regime. We make our system publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Polak_P/0/1/0/all/0/1\">Peter Pol&#xe1;k</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ngoc_N/0/1/0/all/0/1\">Ngoc-Quan Ngoc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tuan-Nam Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Danni Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mullov_C/0/1/0/all/0/1\">Carlos Mullov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niehues_J/0/1/0/all/0/1\">Jan Niehues</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bojar_O/0/1/0/all/0/1\">Ond&#x159;ej Bojar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waibel_A/0/1/0/all/0/1\">Alexander Waibel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fairly Accurate: Learning Optimal Accuracy vs. Fairness Tradeoffs for Hate Speech Detection. (arXiv:2204.07661v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.07661","description":"<p>Recent work has emphasized the importance of balancing competing objectives\nin model training (e.g., accuracy vs. fairness, or competing measures of\nfairness). Such trade-offs reflect a broader class of multi-objective\noptimization (MOO) problems in which optimization methods seek Pareto optimal\ntrade-offs between competing goals. In this work, we first introduce a\ndifferentiable measure that enables direct optimization of group fairness\n(specifically, balancing accuracy across groups) in model training. Next, we\ndemonstrate two model-agnostic MOO frameworks for learning Pareto optimal\nparameterizations over different groups of neural classification models. We\nevaluate our methods on the specific task of hate speech detection, in which\nprior work has shown lack of group fairness across speakers of different\nEnglish dialects. Empirical results across convolutional, sequential, and\ntransformer-based neural architectures show superior empirical accuracy vs.\nfairness trade-offs over prior work. More significantly, our measure enables\nthe Pareto machinery to ensure that each architecture achieves the best\npossible trade-off between fairness and accuracy w.r.t. the dataset, given\nuser-prescribed error tolerance bounds.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kovatchev_V/0/1/0/all/0/1\">Venelin Kovatchev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1\">Soumyajit Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1\">Anubrata Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lease_M/0/1/0/all/0/1\">Matthew Lease</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dialogue Meaning Representation for Task-Oriented Dialogue Systems. (arXiv:2204.10989v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.10989","description":"<p>Dialogue meaning representation formulates natural language utterance\nsemantics in their conversational context in an explicit and machine-readable\nform. Previous work typically follows the intent-slot framework, which is easy\nfor annotation yet limited on scalability for complex linguistic expressions. A\nline of works alleviates the representation issue by introducing hierarchical\nstructures but challenging to express complex compositional semantics, such as\nnegation and coreference. We propose Dialogue Meaning Representation (DMR), a\nflexible and easily extendable representation for task-oriented dialogue. Our\nrepresentation contains a set of nodes and edges with inheritance hierarchy to\nrepresent rich semantics for compositional semantics and task-specific\nconcepts. We annotated DMR-FastFood, a multi-turn dialogue dataset with more\nthan 70k utterances, with DMR. We propose two evaluation tasks to evaluate\ndifferent dialogue models, and further propose a novel coreference resolution\nmodel GNNCoref for the graph-based coreference resolution task. Experiments\nshow that DMR can be parsed well with pretrained Seq2Seq model, and GNNCoref\noutperforms the baseline models by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiangkun Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1\">Junqi Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1\">Hang Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1\">Qipeng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Original or Translated? A Causal Analysis of the Impact of Translationese on Machine Translation Performance. (arXiv:2205.02293v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.02293","description":"<p>Human-translated text displays distinct features from naturally written text\nin the same language. This phenomena, known as translationese, has been argued\nto confound the machine translation (MT) evaluation. Yet, we find that existing\nwork on translationese neglects some important factors and the conclusions are\nmostly correlational but not causal. In this work, we collect CausalMT, a\ndataset where the MT training data are also labeled with the human translation\ndirections. We inspect two critical factors, the train-test direction match\n(whether the human translation directions in the training and test sets are\naligned), and data-model direction match (whether the model learns in the same\ndirection as the human translation direction in the dataset). We show that\nthese two factors have a large causal effect on the MT performance, in addition\nto the test-model direction mismatch highlighted by existing work on the impact\nof translationese. In light of our findings, we provide a set of suggestions\nfor MT training and evaluation. Our code and data are at\nhttps://github.com/EdisonNi-hku/CausalMT\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ni_J/0/1/0/all/0/1\">Jingwei Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhijing Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freitag_M/0/1/0/all/0/1\">Markus Freitag</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conversational Analysis of Daily Dialog Data using Polite Emotional Dialogue Acts. (arXiv:2205.02921v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.02921","description":"<p>Many socio-linguistic cues are used in conversational analysis, such as\nemotion, sentiment, and dialogue acts. One of the fundamental cues is\npoliteness, which linguistically possesses properties such as social manners\nuseful in conversational analysis. This article presents findings of polite\nemotional dialogue act associations, where we can correlate the relationships\nbetween the socio-linguistic cues. We confirm our hypothesis that the\nutterances with the emotion classes Anger and Disgust are more likely to be\nimpolite. At the same time, Happiness and Sadness are more likely to be polite.\nA less expectable phenomenon occurs with dialogue acts Inform and Commissive\nwhich contain more polite utterances than Question and Directive. Finally, we\nconclude on the future work of these findings to extend the learning of social\nbehaviours using politeness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bothe_C/0/1/0/all/0/1\">Chandrakant Bothe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wermter_S/0/1/0/all/0/1\">Stefan Wermter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards a Progression-Aware Autonomous Dialogue Agent. (arXiv:2205.03692v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.03692","description":"<p>Recent advances in large-scale language modeling and generation have enabled\nthe creation of dialogue agents that exhibit human-like responses in a wide\nrange of conversational scenarios spanning a diverse set of tasks, from general\nchit-chat to focused goal-oriented discourse. While these agents excel at\ngenerating high-quality responses that are relevant to prior context, they\nsuffer from a lack of awareness of the overall direction in which the\nconversation is headed, and the likelihood of task success inherent therein.\nThus, we propose a framework in which dialogue agents can evaluate the\nprogression of a conversation toward or away from desired outcomes, and use\nthis signal to inform planning for subsequent responses. Our framework is\ncomposed of three key elements: (1) the notion of a \"global\" dialogue state\n(GDS) space, (2) a task-specific progression function (PF) computed in terms of\na conversation's trajectory through this space, and (3) a planning mechanism\nbased on dialogue rollouts by which an agent may use progression signals to\nselect its next response.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sanders_A/0/1/0/all/0/1\">Abraham Sanders</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strzalkowski_T/0/1/0/all/0/1\">Tomek Strzalkowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_M/0/1/0/all/0/1\">Mei Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_A/0/1/0/all/0/1\">Albert Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dey_D/0/1/0/all/0/1\">Deepanshu Dey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Braasch_J/0/1/0/all/0/1\">Jonas Braasch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dakuo Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Context-Aware Abbreviation Expansion Using Large Language Models. (arXiv:2205.03767v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.03767","description":"<p>Motivated by the need for accelerating text entry in augmentative and\nalternative communication (AAC) for people with severe motor impairments, we\npropose a paradigm in which phrases are abbreviated aggressively as primarily\nword-initial letters. Our approach is to expand the abbreviations into\nfull-phrase options by leveraging conversation context with the power of\npretrained large language models (LLMs). Through zero-shot, few-shot, and\nfine-tuning experiments on four public conversation datasets, we show that for\nreplies to the initial turn of a dialog, an LLM with 64B parameters is able to\nexactly expand over 70% of phrases with abbreviation length up to 10, leading\nto an effective keystroke saving rate of up to about 77% on these exact\nexpansions. Including a small amount of context in the form of a single\nconversation turn more than doubles abbreviation expansion accuracies compared\nto having no context, an effect that is more pronounced for longer phrases.\nAdditionally, the robustness of models against typo noise can be enhanced\nthrough fine-tuning on noisy data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_S/0/1/0/all/0/1\">Shanqing Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venugopalan_S/0/1/0/all/0/1\">Subhashini Venugopalan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tomanek_K/0/1/0/all/0/1\">Katrin Tomanek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayanan_A/0/1/0/all/0/1\">Ajit Narayanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morris_M/0/1/0/all/0/1\">Meredith Ringel Morris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brenner_M/0/1/0/all/0/1\">Michael P. Brenner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Answer Visual Questions from Web Videos. (arXiv:2205.05019v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.05019","description":"<p>Recent methods for visual question answering rely on large-scale annotated\ndatasets. Manual annotation of questions and answers for videos, however, is\ntedious, expensive and prevents scalability. In this work, we propose to avoid\nmanual annotation and generate a large-scale training dataset for video\nquestion answering making use of automatic cross-modal supervision. We leverage\na question generation transformer trained on text data and use it to generate\nquestion-answer pairs from transcribed video narrations. Given narrated videos,\nwe then automatically generate the HowToVQA69M dataset with 69M\nvideo-question-answer triplets. To handle the open vocabulary of diverse\nanswers in this dataset, we propose a training procedure based on a contrastive\nloss between a video-question multi-modal transformer and an answer\ntransformer. We introduce the zero-shot VideoQA task and the VideoQA feature\nprobe evaluation setting and show excellent results, in particular for rare\nanswers. Furthermore, our method achieves competitive results on MSRVTT-QA,\nActivityNet-QA, MSVD-QA and How2QA datasets. We also show that our VideoQA\ndataset generation approach generalizes to another source of web video and text\ndata. We use our method to generate the WebVidVQA3M dataset from the WebVid\ndataset, i.e., videos with alt-text annotations, and show its benefits for\ntraining VideoQA models. Finally, for a detailed evaluation we introduce iVQA,\na new VideoQA dataset with reduced language bias and high-quality manual\nannotations. Code, datasets and trained models are available at\nhttps://antoyang.github.io/just-ask.html\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1\">Antoine Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miech_A/0/1/0/all/0/1\">Antoine Miech</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sivic_J/0/1/0/all/0/1\">Josef Sivic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laptev_I/0/1/0/all/0/1\">Ivan Laptev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1\">Cordelia Schmid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-05-11T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"An Efficient Calculation of Quaternion Correlation of Signals and Color Images. (arXiv:2205.05113v1 [math.AC])","link":"http://arxiv.org/abs/2205.05113","description":"<p>Over the past century, a correlation has been an essential mathematical\ntechnique utilized in engineering sciences, including practically every\nsignal/image processing field. This paper describes an effective method of\ncalculating the correlation function of signals and color images in quaternion\nalgebra. We propose using the quaternions with a commutative multiplication\noperation and defining the corresponding correlation function in this\narithmetic. The correlation between quaternion signals and images can be\ncalculated by multiplying two quaternion DFTs of signals and images. The\ncomplexity of the correlation of color images is three times higher than in\ncomplex algebra.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/math/1/au:+Grigoryan_A/0/1/0/all/0/1\">Artyom M. Grigoryan</a>, <a href=\"http://arxiv.org/find/math/1/au:+Agaian_S/0/1/0/all/0/1\">Sos S. Agaian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep fusion of gray level co-occurrence matrices for lung nodule classification. (arXiv:2205.05123v1 [eess.IV])","link":"http://arxiv.org/abs/2205.05123","description":"<p>Lung cancer is a severe menace to human health, due to which millions of\npeople die because of late diagnoses of cancer; thus, it is vital to detect the\ndisease as early as possible. The Computerized chest analysis Tomography of\nscan is assumed to be one of the efficient solutions for detecting and\nclassifying lung nodules. The necessity of high accuracy of analyzing C.T. scan\nimages of the lung is considered as one of the crucial challenges in detecting\nand classifying lung cancer. A new long-short-term-memory (LSTM) based deep\nfusion structure, is introduced, where, the texture features computed from lung\nnodules through new volumetric grey-level-co-occurrence-matrices (GLCM)\ncomputations are applied to classify the nodules into: benign, malignant and\nambiguous. An improved Otsu segmentation method combined with the water strider\noptimization algorithm (WSA) is proposed to detect the lung nodules. Otsu-WSA\nthresholding can overcome the restrictions present in previous thresholding\nmethods. Extended experiments are run to assess this fusion structure by\nconsidering 2D-GLCM computations based 2D-slices fusion, and an approximation\nof this 3D-GLCM with volumetric 2.5D-GLCM computations-based LSTM fusion\nstructure. The proposed methods are trained and assessed through the LIDC-IDRI\ndataset, where 94.4%, 91.6%, and 95.8% Accuracy, sensitivity, and specificity\nare obtained, respectively for 2D-GLCM fusion and 97.33%, 96%, and 98%,\naccuracy, sensitivity, and specificity, respectively, for 2.5D-GLCM fusion. The\nyield of the same are 98.7%, 98%, and 99%, for the 3D-GLCM fusion. The obtained\nresults and analysis indicate that the WSA-Otsu method requires less execution\ntime and yields a more accurate thresholding process. It is found that 3D-GLCM\nbased LSTM outperforms its counterparts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Saihood_A/0/1/0/all/0/1\">Ahmed Saihood</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Karshenas_H/0/1/0/all/0/1\">Hossein Karshenas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nilchi_A/0/1/0/all/0/1\">AhmadReza Naghsh Nilchi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-Shot Image Classification Benchmarks are Too Far From Reality: Build Back Better with Semantic Task Sampling. (arXiv:2205.05155v1 [cs.CV])","link":"http://arxiv.org/abs/2205.05155","description":"<p>Every day, a new method is published to tackle Few-Shot Image Classification,\nshowing better and better performances on academic benchmarks. Nevertheless, we\nobserve that these current benchmarks do not accurately represent the real\nindustrial use cases that we encountered. In this work, through both\nqualitative and quantitative studies, we expose that the widely used benchmark\ntieredImageNet is strongly biased towards tasks composed of very semantically\ndissimilar classes e.g. bathtub, cabbage, pizza, schipperke, and cardoon. This\nmakes tieredImageNet (and similar benchmarks) irrelevant to evaluate the\nability of a model to solve real-life use cases usually involving more\nfine-grained classification. We mitigate this bias using semantic information\nabout the classes of tieredImageNet and generate an improved, balanced\nbenchmark. Going further, we also introduce a new benchmark for Few-Shot Image\nClassification using the Danish Fungi 2020 dataset. This benchmark proposes a\nwide variety of evaluation tasks with various fine-graininess. Moreover, this\nbenchmark includes many-way tasks (e.g. composed of 100 classes), which is a\nchallenging setting yet very common in industrial applications. Our experiments\nbring out the correlation between the difficulty of a task and the semantic\nsimilarity between its classes, as well as a heavy performance drop of\nstate-of-the-art methods on many-way few-shot classification, raising questions\nabout the scaling abilities of these methods. We hope that our work will\nencourage the community to further question the quality of standard evaluation\nprocesses and their relevance to real-life applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bennequin_E/0/1/0/all/0/1\">Etienne Bennequin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tami_M/0/1/0/all/0/1\">Myriam Tami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toubhans_A/0/1/0/all/0/1\">Antoine Toubhans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hudelot_C/0/1/0/all/0/1\">Celine Hudelot</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robustness of Humans and Machines on Object Recognition with Extreme Image Transformations. (arXiv:2205.05167v1 [cs.CV])","link":"http://arxiv.org/abs/2205.05167","description":"<p>Recent neural network architectures have claimed to explain data from the\nhuman visual cortex. Their demonstrated performance is however still limited by\nthe dependence on exploiting low-level features for solving visual tasks. This\nstrategy limits their performance in case of out-of-distribution/adversarial\ndata. Humans, meanwhile learn abstract concepts and are mostly unaffected by\neven extreme image distortions. Humans and networks employ strikingly different\nstrategies to solve visual tasks. To probe this, we introduce a novel set of\nimage transforms and evaluate humans and networks on an object recognition\ntask. We found performance for a few common networks quickly decreases while\nhumans are able to recognize objects with a high accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Crowder_D/0/1/0/all/0/1\">Dakarai Crowder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malik_G/0/1/0/all/0/1\">Girik Malik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Scale Space Radon Transform, Properties and Image Reconstruction. (arXiv:2205.05188v1 [cs.CV])","link":"http://arxiv.org/abs/2205.05188","description":"<p>Aware of the importance of the good behavior in the scale space that a\nmathematical transform must have, we depict, in this paper, the basic\nproperties and the inverse transform of the Scale Space Radon Transform (SSRT).\nTo reconstruct the image from SSRT sinogram, the Filtered backprojection (FBP)\ntechnique is used in two different ways: (1) Deconvolve SSRT to obtain the\nestimated Radon transform (RT) and then, reconstruct image using classical FBP\nor (2) Adapt FBP technique to SSRT so that the Radon projections spectrum used\nin classical FBP is replaced by SSRT and Wiener filtering, expressed in the\nfrequency domain. Comparison of image reconstruction techniques using SSRT and\nRT are performed on Shepp-Logan head phantom image. Using the Mean Absolute\nError (MAE) as image reconstruction quality measure, the preliminary results\npresent an outstanding performance for SSRT-based image reconstruction\ntechniques compared to the RT-based one. Furthermore, the method (2)\noutperforms the method (1) in terms of computation time and adaptability for\nhigh level of noise when fairly large Gaussian kernel is used.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nacereddine_N/0/1/0/all/0/1\">Nafaa Nacereddine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ziou_D/0/1/0/all/0/1\">Djemel Ziou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goumeidane_A/0/1/0/all/0/1\">Aicha Baya Goumeidane</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Student Collaboration Improves Self-Supervised Learning: Dual-Loss Adaptive Masked Autoencoder for Brain Cell Image Analysis. (arXiv:2205.05194v1 [cs.CV])","link":"http://arxiv.org/abs/2205.05194","description":"<p>Self-supervised learning leverages the underlying data structure as the\nsource of the supervisory signal without the need for human annotation effort.\nThis approach offers a practical solution to learning with a large amount of\nbiomedical data and limited annotation. Unlike other studies exploiting data\nvia multi-view (e.g., augmented images), this study presents a self-supervised\nDual-Loss Adaptive Masked Autoencoder (DAMA) algorithm established from the\nviewpoint of the information theory. Specifically, our objective function\nmaximizes the mutual information by minimizing the conditional entropy in\npixel-level reconstruction and feature-level regression. We further introduce\nan adaptive mask sampling strategy to maximize mutual information. We conduct\nextensive experiments on brain cell images to validate the proposed method.\nDAMA significantly outperforms both state-of-the-art self-supervised and\nsupervised methods on brain cells data and demonstrates competitive result on\nImageNet-1k. Code: https://github.com/hula-ai/DAMA\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ly_S/0/1/0/all/0/1\">Son T. Ly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Bai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vo_H/0/1/0/all/0/1\">Hung Q. Vo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maric_D/0/1/0/all/0/1\">Dragan Maric</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roysam_B/0/1/0/all/0/1\">Badri Roysam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Hien V. Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Best of Both Worlds: Multi-task Audio-Visual Automatic Speech Recognition and Active Speaker Detection. (arXiv:2205.05206v1 [eess.AS])","link":"http://arxiv.org/abs/2205.05206","description":"<p>Under noisy conditions, automatic speech recognition (ASR) can greatly\nbenefit from the addition of visual signals coming from a video of the\nspeaker's face. However, when multiple candidate speakers are visible this\ntraditionally requires solving a separate problem, namely active speaker\ndetection (ASD), which entails selecting at each moment in time which of the\nvisible faces corresponds to the audio. Recent work has shown that we can solve\nboth problems simultaneously by employing an attention mechanism over the\ncompeting video tracks of the speakers' faces, at the cost of sacrificing some\naccuracy on active speaker detection. This work closes this gap in active\nspeaker detection accuracy by presenting a single model that can be jointly\ntrained with a multi-task loss. By combining the two tasks during training we\nreduce the ASD classification accuracy by approximately 25%, while\nsimultaneously improving the ASR performance when compared to the multi-person\nbaseline trained exclusively for ASR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Braga_O/0/1/0/all/0/1\">Otavio Braga</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Siohan_O/0/1/0/all/0/1\">Olivier Siohan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DcnnGrasp: Towards Accurate Grasp Pattern Recognition with Adaptive Regularizer Learning. (arXiv:2205.05218v1 [cs.CV])","link":"http://arxiv.org/abs/2205.05218","description":"<p>The task of grasp pattern recognition aims to derive the applicable grasp\ntypes of an object according to the visual information. Current\nstate-of-the-art methods ignore category information of objects which is\ncrucial for grasp pattern recognition. This paper presents a novel dual-branch\nconvolutional neural network (DcnnGrasp) to achieve joint learning of object\ncategory classification and grasp pattern recognition. DcnnGrasp takes object\ncategory classification as an auxiliary task to improve the effectiveness of\ngrasp pattern recognition. Meanwhile, a new loss function called joint\ncross-entropy with an adaptive regularizer is derived through maximizing a\nposterior, which significantly improves the model performance. Besides, based\non the new loss function, a training strategy is proposed to maximize the\ncollaborative learning of the two tasks. The experiment was performed on five\nhousehold objects datasets including the RGB-D Object dataset, Hit-GPRec\ndataset, Amsterdam library of object images (ALOI), Columbia University Image\nLibrary (COIL-100), and MeganePro dataset 1. The experimental results\ndemonstrated that the proposed method can achieve competitive performance on\ngrasp pattern recognition with several state-of-the-art methods. Specifically,\nour method even outperformed the second-best one by nearly 15% in terms of\nglobal accuracy for the case of testing a novel object on the RGB-D Object\ndataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaoqin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Ziwei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Jingjing Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xianta Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Salient Object Detection via Bounding-box Supervision. (arXiv:2205.05245v1 [cs.CV])","link":"http://arxiv.org/abs/2205.05245","description":"<p>The success of fully supervised saliency detection models depends on a large\nnumber of pixel-wise labeling. In this paper, we work on bounding-box based\nweakly-supervised saliency detection to relieve the labeling effort. Given the\nbounding box annotation, we observe that pixels inside the bounding box may\ncontain extensive labeling noise. However, as a large amount of background is\nexcluded, the foreground bounding box region contains a less complex\nbackground, making it possible to perform handcrafted features-based saliency\ndetection with only the cropped foreground region. As the conventional\nhandcrafted features are not representative enough, leading to noisy saliency\nmaps, we further introduce structure-aware self-supervised loss to regularize\nthe structure of the prediction. Further, we claim that pixels outside the\nbounding box should be background, thus partial cross-entropy loss function can\nbe used to accurately localize the accurate background region. Experimental\nresults on six benchmark RGB saliency datasets illustrate the effectiveness of\nour model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_M/0/1/0/all/0/1\">Mengqi He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Wenxin Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Secure Federated Learning for Neuroimaging. (arXiv:2205.05249v1 [cs.LG])","link":"http://arxiv.org/abs/2205.05249","description":"<p>The amount of biomedical data continues to grow rapidly. However, the ability\nto collect data from multiple sites for joint analysis remains challenging due\nto security, privacy, and regulatory concerns. We present a Secure Federated\nLearning architecture, MetisFL, which enables distributed training of neural\nnetworks over multiple data sources without sharing data. Each site trains the\nneural network over its private data for some time, then shares the neural\nnetwork parameters (i.e., weights, gradients) with a Federation Controller,\nwhich in turn aggregates the local models, sends the resulting community model\nback to each site, and the process repeats. Our architecture provides strong\nsecurity and privacy. First, sample data never leaves a site. Second, neural\nparameters are encrypted before transmission and the community model is\ncomputed under fully-homomorphic encryption. Finally, we use\ninformation-theoretic methods to limit information leakage from the neural\nmodel to prevent a curious site from performing membership attacks. We\ndemonstrate this architecture in neuroimaging. Specifically, we investigate\ntraining neural models to classify Alzheimer's disease, and estimate Brain Age,\nfrom magnetic resonance imaging datasets distributed across multiple sites,\nincluding heterogeneous environments where sites have different amounts of\ndata, statistical distributions, and computational capabilities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stripelis_D/0/1/0/all/0/1\">Dimitris Stripelis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_U/0/1/0/all/0/1\">Umang Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saleem_H/0/1/0/all/0/1\">Hamza Saleem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhinagar_N/0/1/0/all/0/1\">Nikhil Dhinagar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghai_T/0/1/0/all/0/1\">Tanmay Ghai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_R/0/1/0/all/0/1\">Rafael Sanchez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anastasiou_C/0/1/0/all/0/1\">Chrysovalantis Anastasiou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asghar_A/0/1/0/all/0/1\">Armaghan Asghar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steeg_G/0/1/0/all/0/1\">Greg Ver Steeg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravi_S/0/1/0/all/0/1\">Srivatsan Ravi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naveed_M/0/1/0/all/0/1\">Muhammad Naveed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thompson_P/0/1/0/all/0/1\">Paul M. Thompson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ambite_J/0/1/0/all/0/1\">Jose Luis Ambite</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spatial-Temporal Space Hand-in-Hand: Spatial-Temporal Video Super-Resolution via Cycle-Projected Mutual Learning. (arXiv:2205.05264v1 [cs.CV])","link":"http://arxiv.org/abs/2205.05264","description":"<p>Spatial-Temporal Video Super-Resolution (ST-VSR) aims to generate\nsuper-resolved videos with higher resolution(HR) and higher frame rate (HFR).\nQuite intuitively, pioneering two-stage based methods complete ST-VSR by\ndirectly combining two sub-tasks: Spatial Video Super-Resolution (S-VSR) and\nTemporal Video Super-Resolution(T-VSR) but ignore the reciprocal relations\namong them. Specifically, 1) T-VSR to S-VSR: temporal correlations help\naccurate spatial detail representation with more clues; 2) S-VSR to T-VSR:\nabundant spatial information contributes to the refinement of temporal\nprediction. To this end, we propose a one-stage based Cycle-projected Mutual\nlearning network (CycMu-Net) for ST-VSR, which makes full use of\nspatial-temporal correlations via the mutual learning between S-VSR and T-VSR.\nSpecifically, we propose to exploit the mutual information among them via\niterative up-and-down projections, where the spatial and temporal features are\nfully fused and distilled, helping the high-quality video reconstruction.\nBesides extensive experiments on benchmark datasets, we also compare our\nproposed CycMu-Net with S-VSR and T-VSR tasks, demonstrating that our method\nsignificantly outperforms state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_M/0/1/0/all/0/1\">Mengshun Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_K/0/1/0/all/0/1\">Kui Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_L/0/1/0/all/0/1\">Liang Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jing Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Junjun Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zheng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AggPose: Deep Aggregation Vision Transformer for Infant Pose Estimation. (arXiv:2205.05277v1 [cs.CV])","link":"http://arxiv.org/abs/2205.05277","description":"<p>Movement and pose assessment of newborns lets experienced pediatricians\npredict neurodevelopmental disorders, allowing early intervention for related\ndiseases. However, most of the newest AI approaches for human pose estimation\nmethods focus on adults, lacking publicly benchmark for infant pose estimation.\nIn this paper, we fill this gap by proposing infant pose dataset and Deep\nAggregation Vision Transformer for human pose estimation, which introduces a\nfast trained full transformer framework without using convolution operations to\nextract features in the early stages. It generalizes Transformer + MLP to\nhigh-resolution deep layer aggregation within feature maps, thus enabling\ninformation fusion between different vision levels. We pre-train AggPose on\nCOCO pose dataset and apply it on our newly released large-scale infant pose\nestimation dataset. The results show that AggPose could effectively learn the\nmulti-scale features among different resolutions and significantly improve the\nperformance of infant pose estimation. We show that AggPose outperforms hybrid\nmodel HRFormer and TokenPose in the infant pose estimation dataset. Moreover,\nour AggPose outperforms HRFormer by 0.7% AP on COCO val pose estimation on\naverage. Our code is available at github.com/SZAR-LAB/AggPose.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1\">Xu Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoye Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Liya Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1\">Xuan Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zening Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_H/0/1/0/all/0/1\">Hongwu Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Jianguo Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ReFine: Re-randomization before Fine-tuning for Cross-domain Few-shot Learning. (arXiv:2205.05282v1 [cs.CV])","link":"http://arxiv.org/abs/2205.05282","description":"<p>Cross-domain few-shot learning (CD-FSL), where there are few target samples\nunder extreme differences between source and target domains, has recently\nattracted huge attention. For CD-FSL, recent studies generally have developed\ntransfer learning based approaches that pre-train a neural network on popular\nlabeled source domain datasets and then transfer it to target domain data.\nAlthough the labeled datasets may provide suitable initial parameters for the\ntarget data, the domain difference between the source and target might hinder\nthe fine-tuning on the target domain. This paper proposes a simple yet powerful\nmethod that re-randomizes the parameters fitted on the source domain before\nadapting to the target data. The re-randomization resets source-specific\nparameters of the source pre-trained model and thus facilitates fine-tuning on\nthe target domain, improving few-shot performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1\">Jaehoon Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sungnyun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_N/0/1/0/all/0/1\">Namgyu Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jin-Hwa Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1\">Hwanjun Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1\">Se-Young Yun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Invisible-to-Visible: Privacy-Aware Human Segmentation using Airborne Ultrasound via Collaborative Learning Probabilistic U-Net. (arXiv:2205.05293v1 [cs.CV])","link":"http://arxiv.org/abs/2205.05293","description":"<p>Color images are easy to understand visually and can acquire a great deal of\ninformation, such as color and texture. They are highly and widely used in\ntasks such as segmentation. On the other hand, in indoor person segmentation,\nit is necessary to collect person data considering privacy. We propose a new\ntask for human segmentation from invisible information, especially airborne\nultrasound. We first convert ultrasound waves to reflected ultrasound\ndirectional images (ultrasound images) to perform segmentation from invisible\ninformation. Although ultrasound images can roughly identify a person's\nlocation, the detailed shape is ambiguous. To address this problem, we propose\na collaborative learning probabilistic U-Net that uses ultrasound and\nsegmentation images simultaneously during training, closing the probabilistic\ndistributions between ultrasound and segmentation images by comparing the\nparameters of the latent spaces. In inference, only ultrasound images can be\nused to obtain segmentation results. As a result of performance verification,\nthe proposed method could estimate human segmentations more accurately than\nconventional probabilistic U-Net and other variational autoencoder models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tanigawa_R/0/1/0/all/0/1\">Risako Tanigawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ishii_Y/0/1/0/all/0/1\">Yasunori Ishii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kozuka_K/0/1/0/all/0/1\">Kazuki Kozuka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamashita_T/0/1/0/all/0/1\">Takayoshi Yamashita</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Arbitrary Shape Text Detection via Boundary Transformer. (arXiv:2205.05320v1 [cs.CV])","link":"http://arxiv.org/abs/2205.05320","description":"<p>Arbitrary shape text detection is a challenging task due to its complexity\nand variety, e.g, various scales, random rotations, and curve shapes. In this\npaper, we propose an arbitrary shape text detector with a boundary transformer,\nwhich can accurately and directly locate text boundaries without any\npost-processing. Our method mainly consists of a boundary proposal module and\nan iteratively optimized boundary transformer module. The boundary proposal\nmodule consisting of multi-layer dilated convolutions will compute important\nprior information (including classification map, distance field, and direction\nfield) for generating coarse boundary proposals meanwhile guiding the\noptimization of boundary transformer. The boundary transformer module adopts an\nencoder-decoder structure, in which the encoder is constructed by multi-layer\ntransformer blocks with residual connection while the decoder is a simple\nmulti-layer perceptron network (MLP). Under the guidance of prior information,\nthe boundary transformer module will gradually refine the coarse boundary\nproposals via boundary deformation in an iterative manner. Furthermore, we\npropose a novel boundary energy loss (BEL) which introduces an energy\nminimization constraint and an energy monotonically decreasing constraint for\nevery boundary optimization step. Extensive experiments on publicly available\nand challenging datasets demonstrate the state-of-the-art performance and\npromising efficiency of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shi-Xue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaobin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1\">Xu-Cheng Yin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Depth Completion: A Survey. (arXiv:2205.05335v1 [cs.CV])","link":"http://arxiv.org/abs/2205.05335","description":"<p>Depth completion aims at predicting dense pixel-wise depth from a sparse map\ncaptured from a depth sensor. It plays an essential role in various\napplications such as autonomous driving, 3D reconstruction, augmented reality,\nand robot navigation. Recent successes on the task have been demonstrated and\ndominated by deep learning based solutions. In this article, for the first\ntime, we provide a comprehensive literature review that helps readers better\ngrasp the research trends and clearly understand the current advances. We\ninvestigate the related studies from the design aspects of network\narchitectures, loss functions, benchmark datasets, and learning strategies with\na proposal of a novel taxonomy that categorizes existing methods. Besides, we\npresent a quantitative comparison of model performance on two widely used\nbenchmark datasets, including an indoor and an outdoor dataset. Finally, we\ndiscuss the challenges of prior works and provide readers with some insights\nfor future research directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Junjie Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_C/0/1/0/all/0/1\">Chenyu Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozay_M/0/1/0/all/0/1\">Mete Ozay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1\">Chenyou Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Q/0/1/0/all/0/1\">Qing Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Honghai Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_T/0/1/0/all/0/1\">Tin Lun Lam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AutoLC: Search Lightweight and Top-Performing Architecture for Remote Sensing Image Land-Cover Classification. (arXiv:2205.05369v1 [cs.CV])","link":"http://arxiv.org/abs/2205.05369","description":"<p>Land-cover classification has long been a hot and difficult challenge in\nremote sensing community. With massive High-resolution Remote Sensing (HRS)\nimages available, manually and automatically designed Convolutional Neural\nNetworks (CNNs) have already shown their great latent capacity on HRS\nland-cover classification in recent years. Especially, the former can achieve\nbetter performance while the latter is able to generate lightweight\narchitecture. Unfortunately, they both have shortcomings. On the one hand,\nbecause manual CNNs are almost proposed for natural image processing, it\nbecomes very redundant and inefficient to process HRS images. On the other\nhand, nascent Neural Architecture Search (NAS) techniques for dense prediction\ntasks are mainly based on encoder-decoder architecture, and just focus on the\nautomatic design of the encoder, which makes it still difficult to recover the\nrefined mapping when confronting complicated HRS scenes.\n</p>\n<p>To overcome their defects and tackle the HRS land-cover classification\nproblems better, we propose AutoLC which combines the advantages of two\nmethods. First, we devise a hierarchical search space and gain the lightweight\nencoder underlying gradient-based search strategy. Second, we meticulously\ndesign a lightweight but top-performing decoder that is adaptive to the\nsearched encoder of itself. Finally, experimental results on the LoveDA\nland-cover dataset demonstrate that our AutoLC method outperforms the\nstate-of-art manual and automatic methods with much less computational\nconsumption.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Chenyu Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Junjue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_A/0/1/0/all/0/1\">Ailong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1\">Yanfei Zhong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recurrent Encoder-Decoder Networks for Vessel Trajectory Prediction with Uncertainty Estimation. (arXiv:2205.05404v1 [cs.CV])","link":"http://arxiv.org/abs/2205.05404","description":"<p>Recent deep learning methods for vessel trajectory prediction are able to\nlearn complex maritime patterns from historical Automatic Identification System\n(AIS) data and accurately predict sequences of future vessel positions with a\nprediction horizon of several hours. However, in maritime surveillance\napplications, reliably quantifying the prediction uncertainty can be as\nimportant as obtaining high accuracy. This paper extends deep learning\nframeworks for trajectory prediction tasks by exploring how recurrent\nencoder-decoder neural networks can be tasked not only to predict but also to\nyield a corresponding prediction uncertainty via Bayesian modeling of epistemic\nand aleatoric uncertainties. We compare the prediction performance of two\ndifferent models based on labeled or unlabeled input data to highlight how\nuncertainty quantification and accuracy can be improved by using, if available,\nadditional information on the intention of the ship (e.g., its planned\ndestination).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Capobianco_S/0/1/0/all/0/1\">Samuele Capobianco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Forti_N/0/1/0/all/0/1\">Nicola Forti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Millefiori_L/0/1/0/all/0/1\">Leonardo M. Millefiori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Braca_P/0/1/0/all/0/1\">Paolo Braca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Willett_P/0/1/0/all/0/1\">Peter Willett</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Objective Method for Pedestrian Occlusion Level Classification. (arXiv:2205.05412v1 [cs.CV])","link":"http://arxiv.org/abs/2205.05412","description":"<p>Pedestrian detection is among the most safety-critical features of driver\nassistance systems for autonomous vehicles. One of the most complex detection\nchallenges is that of partial occlusion, where a target object is only\npartially available to the sensor due to obstruction by another foreground\nobject. A number of current pedestrian detection benchmarks provide annotation\nfor partial occlusion to assess algorithm performance in these scenarios,\nhowever each benchmark varies greatly in their definition of the occurrence and\nseverity of occlusion. In addition, current occlusion level annotation methods\ncontain a high degree of subjectivity by the human annotator. This can lead to\ninaccurate or inconsistent reporting of an algorithm's detection performance\nfor partially occluded pedestrians, depending on which benchmark is used. This\nresearch presents a novel, objective method for pedestrian occlusion level\nclassification for ground truth annotation. Occlusion level classification is\nachieved through the identification of visible pedestrian keypoints and through\nthe use of a novel, effective method of 2D body surface area estimation.\nExperimental results demonstrate that the proposed method reflects the\npixel-wise occlusion level of pedestrians in images and is effective for all\nforms of occlusion, including challenging edge cases such as self-occlusion,\ntruncation and inter-occluding pedestrians.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gilroy_S/0/1/0/all/0/1\">Shane Gilroy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glavin_M/0/1/0/all/0/1\">Martin Glavin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jones_E/0/1/0/all/0/1\">Edward Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mullins_D/0/1/0/all/0/1\">Darragh Mullins</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Label Logo Recognition and Retrieval based on Weighted Fusion of Neural Features. (arXiv:2205.05419v1 [cs.CV])","link":"http://arxiv.org/abs/2205.05419","description":"<p>Logo classification is a particular case of image classification, since these\nmay contain only text, images, or a combination of both. In this work, we\npropose a system for the multi-label classification and similarity search of\nlogo images. The method allows obtaining the most similar logos on the basis of\ntheir shape, color, business sector, semantics, general characteristics, or a\ncombination of such features established by the user. This is done by employing\na set of multi-label networks specialized in certain characteristics of logos.\nThe features extracted from these networks are combined to perform the\nsimilarity search according to the search criteria established. Since the text\nof logos is sometimes irrelevant for the classification, a preprocessing stage\nis carried out to remove it, thus improving the overall performance. The\nproposed approach is evaluated using the European Union Trademark (EUTM)\ndataset, structured with the hierarchical Vienna classification system, which\nincludes a series of metadata with which to index trademarks. We also make a\ncomparison between well known logo topologies and Vienna in order to help\ndesigners understand their correspondences. The experimentation carried out\nattained reliable performance results, both quantitatively and qualitatively,\nwhich outperformed the state-of-the-art results. In addition, since the\nsemantics and classification of brands can often be subjective, we also\nsurveyed graphic design students and professionals in order to assess the\nreliability of the proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bernabeu_M/0/1/0/all/0/1\">Marisa Bernabeu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gallego_A/0/1/0/all/0/1\">Antonio Javier Gallego</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pertusa_A/0/1/0/all/0/1\">Antonio Pertusa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RustSEG -- Automated segmentation of corrosion using deep learning. (arXiv:2205.05426v1 [cs.CV])","link":"http://arxiv.org/abs/2205.05426","description":"<p>The inspection of infrastructure for corrosion remains a task that is\ntypically performed manually by qualified engineers or inspectors. This task of\ninspection is laborious, slow, and often requires complex access. Recently,\ndeep learning based algorithms have revealed promise and performance in the\nautomatic detection of corrosion. However, to date, research regarding the\nsegmentation of images for automated corrosion detection has been limited, due\nto the lack of availability of per-pixel labelled data sets which are required\nfor model training. Herein, a novel deep learning approach (termed RustSEG) is\npresented, that can accurately segment images for automated corrosion\ndetection, without the requirement of per-pixel labelled data sets for\ntraining. The RustSEG method will first, using deep learning techniques,\ndetermine if corrosion is present in an image (i.e. a classification task), and\nthen if corrosion is present, the model will examine what pixels in the\noriginal image contributed to that classification decision. Finally, the method\ncan refine its predictions into a pixel-level segmentation mask. In ideal\ncases, the method is able to generate precise masks of corrosion in images,\ndemonstrating that the automated segmentation of corrosion without per-pixel\ntraining data is possible, addressing a significant hurdle in automated\ninfrastructure inspection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Burton_B/0/1/0/all/0/1\">B. Burton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nash_W/0/1/0/all/0/1\">W.T. Nash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Birbilis_N/0/1/0/all/0/1\">N. Birbilis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Continual Deepfake Detection Benchmark: Dataset, Methods, and Essentials. (arXiv:2205.05467v1 [cs.CV])","link":"http://arxiv.org/abs/2205.05467","description":"<p>There have been emerging a number of benchmarks and techniques for the\ndetection of deepfakes. However, very few works study the detection of\nincrementally appearing deepfakes in the real-world scenarios. To simulate the\nwild scenes, this paper suggests a continual deepfake detection benchmark\n(CDDB) over a new collection of deepfakes from both known and unknown\ngenerative models. The suggested CDDB designs multiple evaluations on the\ndetection over easy, hard, and long sequence of deepfake tasks, with a set of\nappropriate measures. In addition, we exploit multiple approaches to adapt\nmulticlass incremental learning methods, commonly used in the continual visual\nrecognition, to the continual deepfake detection problem. We evaluate several\nmethods, including the adapted ones, on the proposed CDDB. Within the proposed\nbenchmark, we explore some commonly known essentials of standard continual\nlearning. Our study provides new insights on these essentials in the context of\ncontinual deepfake detection. The suggested CDDB is clearly more challenging\nthan the existing benchmarks, which thus offers a suitable evaluation avenue to\nthe future research. Our benchmark dataset and the source code will be made\npublicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chuqiao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhiwu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paudel_D/0/1/0/all/0/1\">Danda Pani Paudel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yabin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahbazi_M/0/1/0/all/0/1\">Mohamad Shahbazi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_X/0/1/0/all/0/1\">Xiaopeng Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Supervised Distillation for Continual Representation Learning. (arXiv:2205.05476v1 [cs.CV])","link":"http://arxiv.org/abs/2205.05476","description":"<p>In this paper, we propose a novel training procedure for the continual\nrepresentation learning problem in which a neural network model is sequentially\nlearned to alleviate catastrophic forgetting in visual search tasks. Our\nmethod, called Contrastive Supervised Distillation (CSD), reduces feature\nforgetting while learning discriminative features. This is achieved by\nleveraging labels information in a distillation setting in which the student\nmodel is contrastively learned from the teacher model. Extensive experiments\nshow that CSD performs favorably in mitigating catastrophic forgetting by\noutperforming current state-of-the-art methods. Our results also provide\nfurther evidence that feature forgetting evaluated in visual retrieval tasks is\nnot as catastrophic as in classification tasks. Code at:\nhttps://github.com/NiccoBiondi/ContrastiveSupervisedDistillation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Barletti_T/0/1/0/all/0/1\">Tommaso Barletti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biondi_N/0/1/0/all/0/1\">Niccolo&#x27; Biondi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pernici_F/0/1/0/all/0/1\">Federico Pernici</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruni_M/0/1/0/all/0/1\">Matteo Bruni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bimbo_A/0/1/0/all/0/1\">Alberto Del Bimbo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scene Consistency Representation Learning for Video Scene Segmentation. (arXiv:2205.05487v1 [cs.CV])","link":"http://arxiv.org/abs/2205.05487","description":"<p>A long-term video, such as a movie or TV show, is composed of various scenes,\neach of which represents a series of shots sharing the same semantic story.\nSpotting the correct scene boundary from the long-term video is a challenging\ntask, since a model must understand the storyline of the video to figure out\nwhere a scene starts and ends. To this end, we propose an effective\nSelf-Supervised Learning (SSL) framework to learn better shot representations\nfrom unlabeled long-term videos. More specifically, we present an SSL scheme to\nachieve scene consistency, while exploring considerable data augmentation and\nshuffling methods to boost the model generalizability. Instead of explicitly\nlearning the scene boundary features as in the previous methods, we introduce a\nvanilla temporal model with less inductive bias to verify the quality of the\nshot features. Our method achieves the state-of-the-art performance on the task\nof Video Scene Segmentation. Additionally, we suggest a more fair and\nreasonable benchmark to evaluate the performance of Video Scene Segmentation\nmethods. The code is made available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Haoqian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Keyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yanan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_R/0/1/0/all/0/1\">Ruizhi Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_B/0/1/0/all/0/1\">Bo Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Haozhe Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1\">Weicheng Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Linlin Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning and Computer Vision Techniques for Microcirculation Analysis: A Review. (arXiv:2205.05493v1 [cs.CV])","link":"http://arxiv.org/abs/2205.05493","description":"<p>The analysis of microcirculation images has the potential to reveal early\nsigns of life-threatening diseases like sepsis. Quantifying the capillary\ndensity and the capillary distribution in microcirculation images can be used\nas a biological marker to assist critically ill patients. The quantification of\nthese biological markers is labor-intensive, time-consuming, and subject to\ninterobserver variability. Several computer vision techniques with varying\nperformance can be used to automate the analysis of these microcirculation\nimages in light of the stated challenges. In this paper, we present a survey of\nover 50 research papers and present the most relevant and promising computer\nvision algorithms to automate the analysis of microcirculation images.\nFurthermore, we present a survey of the methods currently used by other\nresearchers to automate the analysis of microcirculation images. This survey is\nof high clinical relevance because it acts as a guidebook of techniques for\nother researchers to develop their microcirculation analysis systems and\nalgorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abdou_M/0/1/0/all/0/1\">Maged Abdalla Helmy Mohamed Abdou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Truong_T/0/1/0/all/0/1\">Trung Tuyen Truong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jul_E/0/1/0/all/0/1\">Eric Jul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferreira_P/0/1/0/all/0/1\">Paulo Ferreira</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TextMatcher: Cross-Attentional Neural Network to Compare Image and Text. (arXiv:2205.05507v1 [cs.CV])","link":"http://arxiv.org/abs/2205.05507","description":"<p>We study a novel multimodal-learning problem, which we call text matching:\ngiven an image containing a single-line text and a candidate text\ntranscription, the goal is to assess whether the text represented in the image\ncorresponds to the candidate text. We devise the first machine-learning model\nspecifically designed for this problem. The proposed model, termed TextMatcher,\ncompares the two inputs by applying a cross-attention mechanism over the\nembedding representations of image and text, and it is trained in an end-to-end\nfashion. We extensively evaluate the empirical performance of TextMatcher on\nthe popular IAM dataset. Results attest that, compared to a baseline and\nexisting models designed for related problems, TextMatcher achieves higher\nperformance on a variety of configurations, while at the same time running\nfaster at inference time. We also showcase TextMatcher in a real-world\napplication scenario concerning the automatic processing of bank cheques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arrigoni_V/0/1/0/all/0/1\">Valentina Arrigoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Repele_L/0/1/0/all/0/1\">Luisa Repele</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saccavino_D/0/1/0/all/0/1\">Dario Marino Saccavino</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"READ: Large-Scale Neural Scene Rendering for Autonomous Driving. (arXiv:2205.05509v1 [cs.CV])","link":"http://arxiv.org/abs/2205.05509","description":"<p>Synthesizing free-view photo-realistic images is an important task in\nmultimedia. With the development of advanced driver assistance systems~(ADAS)\nand their applications in autonomous vehicles, experimenting with different\nscenarios becomes a challenge. Although the photo-realistic street scenes can\nbe synthesized by image-to-image translation methods, which cannot produce\ncoherent scenes due to the lack of 3D information. In this paper, a large-scale\nneural rendering method is proposed to synthesize the autonomous driving\nscene~(READ), which makes it possible to synthesize large-scale driving\nscenarios on a PC through a variety of sampling schemes. In order to represent\ndriving scenarios, we propose an {\\omega} rendering network to learn neural\ndescriptors from sparse point clouds. Our model can not only synthesize\nrealistic driving scenes but also stitch and edit driving scenes. Experiments\nshow that our model performs well in large-scale driving scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhuopeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zeyu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Ping Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Junbo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jianke Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Empirical Study Of Self-supervised Learning Approaches For Object Detection With Transformers. (arXiv:2205.05543v1 [cs.CV])","link":"http://arxiv.org/abs/2205.05543","description":"<p>Self-supervised learning (SSL) methods such as masked language modeling have\nshown massive performance gains by pretraining transformer models for a variety\nof natural language processing tasks. The follow-up research adapted similar\nmethods like masked image modeling in vision transformer and demonstrated\nimprovements in the image classification task. Such simple self-supervised\nmethods are not exhaustively studied for object detection transformers (DETR,\nDeformable DETR) as their transformer encoder modules take input in the\nconvolutional neural network (CNN) extracted feature space rather than the\nimage space as in general vision transformers. However, the CNN feature maps\nstill maintain the spatial relationship and we utilize this property to design\nself-supervised learning approaches to train the encoder of object detection\ntransformers in pretraining and multi-task learning settings. We explore common\nself-supervised methods based on image reconstruction, masked image modeling\nand jigsaw. Preliminary experiments in the iSAID dataset demonstrate faster\nconvergence of DETR in the initial epochs in both pretraining and multi-task\nlearning settings; nonetheless, similar improvement is not observed in the case\nof multi-task learning with Deformable DETR. The code for our experiments with\nDETR and Deformable DETR are available at https://github.com/gokulkarthik/detr\nand https://github.com/gokulkarthik/Deformable-DETR respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_G/0/1/0/all/0/1\">Gokul Karthik Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mullappilly_S/0/1/0/all/0/1\">Sahal Shaji Mullappilly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gehlot_A/0/1/0/all/0/1\">Abhishek Singh Gehlot</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CNN-LSTM Based Multimodal MRI and Clinical Data Fusion for Predicting Functional Outcome in Stroke Patients. (arXiv:2205.05545v1 [eess.IV])","link":"http://arxiv.org/abs/2205.05545","description":"<p>Clinical outcome prediction plays an important role in stroke patient\nmanagement. From a machine learning point-of-view, one of the main challenges\nis dealing with heterogeneous data at patient admission, i.e. the image data\nwhich are multidimensional and the clinical data which are scalars. In this\npaper, a multimodal convolutional neural network - long short-term memory\n(CNN-LSTM) based ensemble model is proposed. For each MR image module, a\ndedicated network provides preliminary prediction of the clinical outcome using\nthe modified Rankin scale (mRS). The final mRS score is obtained by merging the\npreliminary probabilities of each module dedicated to a specific type of MR\nimage weighted by the clinical metadata, here age or the National Institutes of\nHealth Stroke Scale (NIHSS). The experimental results demonstrate that the\nproposed model surpasses the baselines and offers an original way to\nautomatically encode the spatio-temporal context of MR images in a deep\nlearning architecture. The highest AUC (0.77) was achieved for the proposed\nmodel with NIHSS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hatami_N/0/1/0/all/0/1\">Nima Hatami</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cho_T/0/1/0/all/0/1\">Tae-Hee Cho</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mechtouff_L/0/1/0/all/0/1\">Laura Mechtouff</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Eker_O/0/1/0/all/0/1\">Omer Faruk Eker</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rousseau_D/0/1/0/all/0/1\">David Rousseau</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Frindel_C/0/1/0/all/0/1\">Carole Frindel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NMR: Neural Manifold Representation for Autonomous Driving. (arXiv:2205.05551v1 [cs.CV])","link":"http://arxiv.org/abs/2205.05551","description":"<p>Autonomous driving requires efficient reasoning about the Spatio-temporal\nnature of the semantics of the scene. Recent approaches have successfully\namalgamated the traditional modular architecture of an autonomous driving stack\ncomprising perception, prediction, and planning in an end-to-end trainable\nsystem. Such a system calls for a shared latent space embedding with\ninterpretable intermediate trainable projected representation. One such\nsuccessfully deployed representation is the Bird's-Eye View(BEV) representation\nof the scene in ego-frame. However, a fundamental assumption for an undistorted\nBEV is the local coplanarity of the world around the ego-vehicle. This\nassumption is highly restrictive, as roads, in general, do have gradients. The\nresulting distortions make path planning inefficient and incorrect. To overcome\nthis limitation, we propose Neural Manifold Representation (NMR), a\nrepresentation for the task of autonomous driving that learns to infer\nsemantics and predict way-points on a manifold over a finite horizon, centered\non the ego-vehicle. We do this using an iterative attention mechanism applied\non a latent high dimensional embedding of surround monocular images and partial\nego-vehicle state. This representation helps generate motion and behavior plans\nconsistent with and cognizant of the surface geometry. We propose a sampling\nalgorithm based on edge-adaptive coverage loss of BEV occupancy grid and\nassociated guidance flow field to generate the surface manifold while incurring\nminimal computational overhead. We aim to test the efficacy of our approach on\nCARLA and SYNTHIA-SF.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nair_U/0/1/0/all/0/1\">Unnikrishnan R. Nair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1\">Sarthak Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menon_M/0/1/0/all/0/1\">Midhun S. Menon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vidapanakal_S/0/1/0/all/0/1\">Srikanth Vidapanakal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Performance of a deep learning system for detection of referable diabetic retinopathy in real clinical settings. (arXiv:2205.05554v1 [eess.IV])","link":"http://arxiv.org/abs/2205.05554","description":"<p>Background: To determine the ability of a commercially available deep\nlearning system, RetCAD v.1.3.1 (Thirona, Nijmegen, The Netherlands) for the\nautomatic detection of referable diabetic retinopathy (DR) on a dataset of\ncolour fundus images acquired during routine clinical practice in a tertiary\nhospital screening program, analyzing the reduction of workload that can be\nreleased incorporating this artificial intelligence-based technology. Methods:\nEvaluation of the software was performed on a dataset of 7195 nonmydriatic\nfundus images from 6325 eyes of 3189 diabetic patients attending our screening\nprogram between February to December of 2019. The software generated a DR\nseverity score for each colour fundus image which was combined into an\neye-level score. This score was then compared with a reference standard as set\nby a human expert using receiver operating characteristic (ROC) curve analysis.\nResults: The artificial intelligence (AI) software achieved an area under the\nROC curve (AUC) value of 0.988 [0.981:0.993] for the detection of referable DR.\nAt the proposed operating point, the sensitivity of the RetCAD software for DR\nis 90.53% and specificity is 97.13%. A workload reduction of 96% could be\nachieved at the cost of only 6 false negatives. Conclusions: The AI software\ncorrectly identified the vast majority of referable DR cases, with a workload\nreduction of 96% of the cases that would need to be checked, while missing\nalmost no true cases, so it may therefore be used as an instrument for triage.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Sanchez_Gutierrez_V/0/1/0/all/0/1\">Ver&#xf3;nica S&#xe1;nchez-Guti&#xe9;rrez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hernandez_Martinez_P/0/1/0/all/0/1\">Paula Hern&#xe1;ndez-Mart&#xed;nez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Munoz_Negrete_F/0/1/0/all/0/1\">Francisco J. Mu&#xf1;oz-Negrete</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Engelberts_J/0/1/0/all/0/1\">Jonne Engelberts</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Luger_A/0/1/0/all/0/1\">Allison M. Luger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Grinsven_M/0/1/0/all/0/1\">Mark J.J.P. van Grinsven</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Review on Panoramic Imaging and Its Applications in Scene Understanding. (arXiv:2205.05570v1 [cs.CV])","link":"http://arxiv.org/abs/2205.05570","description":"<p>With the rapid development of high-speed communication and artificial\nintelligence technologies, human perception of real-world scenes is no longer\nlimited to the use of small Field of View (FoV) and low-dimensional scene\ndetection devices. Panoramic imaging emerges as the next generation of\ninnovative intelligent instruments for environmental perception and\nmeasurement. However, while satisfying the need for large-FoV photographic\nimaging, panoramic imaging instruments are expected to have high resolution, no\nblind area, miniaturization, and multi-dimensional intelligent perception, and\ncan be combined with artificial intelligence methods towards the next\ngeneration of intelligent instruments, enabling deeper understanding and more\nholistic perception of 360-degree real-world surrounding environments.\nFortunately, recent advances in freeform surfaces, thin-plate optics, and\nmetasurfaces provide innovative approaches to address human perception of the\nenvironment, offering promising ideas beyond conventional optical imaging. In\nthis review, we begin with introducing the basic principles of panoramic\nimaging systems, and then describe the architectures, features, and functions\nof various panoramic imaging systems. Afterwards, we discuss in detail the\nbroad application prospects and great design potential of freeform surfaces,\nthin-plate optics, and metasurfaces in panoramic imaging. We then provide a\ndetailed analysis on how these techniques can help enhance the performance of\npanoramic imaging systems. We further offer a detailed analysis of applications\nof panoramic imaging in scene understanding for autonomous driving and\nrobotics, spanning panoramic semantic image segmentation, panoramic depth\nestimation, panoramic visual localization, and so on. Finally, we cast a\nperspective on future potential and research directions for panoramic imaging\ninstruments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Shaohua Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kailun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Hao Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kaiwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1\">Jian Bai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Face Detection on Mobile: Five Implementations and Analysis. (arXiv:2205.05572v1 [cs.CV])","link":"http://arxiv.org/abs/2205.05572","description":"<p>In many practical cases face detection on smartphones or other highly\nportable devices is a necessity. Applications include mobile face access\ncontrol systems, driver status tracking, emotion recognition, etc. Mobile\ndevices have limited processing power and should have long-enough battery life\neven with face detection application running. Thus, striking the right balance\nbetween algorithm quality and complexity is crucial. In this work we adapt 5\nalgorithms to mobile. These algorithms are based on handcrafted or\nneural-network-based features and include: Viola-Jones (Haar cascade), LBP,\nHOG, MTCNN, BlazeFace. We analyze inference time of these algorithms on\ndifferent devices with different input image resolutions. We provide guidance,\nwhich algorithms are the best fit for mobile face access control systems and\npotentially other mobile applications. Interestingly, we note that cascaded\nalgorithms perform faster on scenes without faces, while BlazeFace is slower on\nempty scenes. Exploiting this behavior might be useful in practice.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khabarlak_K/0/1/0/all/0/1\">Kostiantyn Khabarlak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DoubleMatch: Improving Semi-Supervised Learning with Self-Supervision. (arXiv:2205.05575v1 [cs.LG])","link":"http://arxiv.org/abs/2205.05575","description":"<p>Following the success of supervised learning, semi-supervised learning (SSL)\nis now becoming increasingly popular. SSL is a family of methods, which in\naddition to a labeled training set, also use a sizable collection of unlabeled\ndata for fitting a model. Most of the recent successful SSL methods are based\non pseudo-labeling approaches: letting confident model predictions act as\ntraining labels. While these methods have shown impressive results on many\nbenchmark datasets, a drawback of this approach is that not all unlabeled data\nare used during training. We propose a new SSL algorithm, DoubleMatch, which\ncombines the pseudo-labeling technique with a self-supervised loss, enabling\nthe model to utilize all unlabeled data in the training process. We show that\nthis method achieves state-of-the-art accuracies on multiple benchmark datasets\nwhile also reducing training times compared to existing SSL methods. Code is\navailable at https://github.com/walline/doublematch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wallin_E/0/1/0/all/0/1\">Erik Wallin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Svensson_L/0/1/0/all/0/1\">Lennart Svensson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kahl_F/0/1/0/all/0/1\">Fredrik Kahl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hammarstrand_L/0/1/0/all/0/1\">Lars Hammarstrand</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TDT: Teaching Detectors to Track without Fully Annotated Videos. (arXiv:2205.05583v1 [cs.CV])","link":"http://arxiv.org/abs/2205.05583","description":"<p>Recently, one-stage trackers that use a joint model to predict both\ndetections and appearance embeddings in one forward pass received much\nattention and achieved state-of-the-art results on the Multi-Object Tracking\n(MOT) benchmarks. However, their success depends on the availability of videos\nthat are fully annotated with tracking data, which is expensive and hard to\nobtain. This can limit the model generalization. In comparison, the two-stage\napproach, which performs detection and embedding separately, is slower but\neasier to train as their data are easier to annotate. We propose to combine the\nbest of the two worlds through a data distillation approach. Specifically, we\nuse a teacher embedder, trained on Re-ID datasets, to generate pseudo\nappearance embedding labels for the detection datasets. Then, we use the\naugmented dataset to train a detector that is also capable of regressing these\npseudo-embeddings in a fully-convolutional fashion. Our proposed one-stage\nsolution matches the two-stage counterpart in quality but is 3 times faster.\nEven though the teacher embedder has not seen any tracking data during\ntraining, our proposed tracker achieves competitive performance with some\npopular trackers (e.g. JDE) trained with fully labeled tracking data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Shuzhi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1\">Guanhang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_C/0/1/0/all/0/1\">Chunhui Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fathy_M/0/1/0/all/0/1\">Mohammed E. Fathy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End Multi-Person Audio/Visual Automatic Speech Recognition. (arXiv:2205.05586v1 [eess.AS])","link":"http://arxiv.org/abs/2205.05586","description":"<p>Traditionally, audio-visual automatic speech recognition has been studied\nunder the assumption that the speaking face on the visual signal is the face\nmatching the audio. However, in a more realistic setting, when multiple faces\nare potentially on screen one needs to decide which face to feed to the A/V ASR\nsystem. The present work takes the recent progress of A/V ASR one step further\nand considers the scenario where multiple people are simultaneously on screen\n(multi-person A/V ASR). We propose a fully differentiable A/V ASR model that is\nable to handle multiple face tracks in a video. Instead of relying on two\nseparate models for speaker face selection and audio-visual ASR on a single\nface track, we introduce an attention layer to the ASR encoder that is able to\nsoft-select the appropriate face video track. Experiments carried out on an A/V\nsystem trained on over 30k hours of YouTube videos illustrate that the proposed\napproach can automatically select the proper face tracks with minor WER\ndegradation compared to an oracle selection of the speaking face while still\nshowing benefits of employing the visual signal instead of the audio alone.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Braga_O/0/1/0/all/0/1\">Otavio Braga</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Makino_T/0/1/0/all/0/1\">Takaki Makino</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Siohan_O/0/1/0/all/0/1\">Olivier Siohan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liao_H/0/1/0/all/0/1\">Hank Liao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video-ReTime: Learning Temporally Varying Speediness for Time Remapping. (arXiv:2205.05609v1 [cs.CV])","link":"http://arxiv.org/abs/2205.05609","description":"<p>We propose a method for generating a temporally remapped video that matches\nthe desired target duration while maximally preserving natural video dynamics.\nOur approach trains a neural network through self-supervision to recognize and\naccurately localize temporally varying changes in the video playback speed. To\nre-time videos, we 1. use the model to infer the slowness of individual video\nframes, and 2. optimize the temporal frame sub-sampling to be consistent with\nthe model's slowness predictions. We demonstrate that this model can detect\nplayback speed variations more accurately while also being orders of magnitude\nmore efficient than prior approaches. Furthermore, we propose an optimization\nfor video re-timing that enables precise control over the target duration and\nperforms more robustly on longer videos than prior methods. We evaluate the\nmodel quantitatively on artificially speed-up videos, through transfer to\naction recognition, and qualitatively through user studies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jenni_S/0/1/0/all/0/1\">Simon Jenni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woodson_M/0/1/0/all/0/1\">Markus Woodson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heilbron_F/0/1/0/all/0/1\">Fabian Caba Heilbron</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RepSR: Training Efficient VGG-style Super-Resolution Networks with Structural Re-Parameterization and Batch Normalization. (arXiv:2205.05671v1 [cs.CV])","link":"http://arxiv.org/abs/2205.05671","description":"<p>This paper explores training efficient VGG-style super-resolution (SR)\nnetworks with the structural re-parameterization technique. The general\npipeline of re-parameterization is to train networks with multi-branch topology\nfirst, and then merge them into standard 3x3 convolutions for efficient\ninference. In this work, we revisit those primary designs and investigate\nessential components for re-parameterizing SR networks. First of all, we find\nthat batch normalization (BN) is important to bring training non-linearity and\nimprove the final performance. However, BN is typically ignored in SR, as it\nusually degrades the performance and introduces unpleasant artifacts. We\ncarefully analyze the cause of BN issue and then propose a straightforward yet\neffective solution. In particular, we first train SR networks with mini-batch\nstatistics as usual, and then switch to using population statistics at the\nlater training period. While we have successfully re-introduced BN into SR, we\nfurther design a new re-parameterizable block tailored for SR, namely RepSR. It\nconsists of a clean residual path and two expand-and-squeeze convolution paths\nwith the modified BN. Extensive experiments demonstrate that our simple RepSR\nis capable of achieving superior performance to previous SR re-parameterization\nmethods among different model sizes. In addition, our RepSR can achieve a\nbetter trade-off between performance and actual running time (throughput) than\nprevious SR methods. Codes will be available at\nhttps://github.com/TencentARC/RepSR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xintao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1\">Chao Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1\">Ying Shan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NTIRE 2022 Challenge on Efficient Super-Resolution: Methods and Results. (arXiv:2205.05675v1 [cs.CV])","link":"http://arxiv.org/abs/2205.05675","description":"<p>This paper reviews the NTIRE 2022 challenge on efficient single image\nsuper-resolution with focus on the proposed solutions and results. The task of\nthe challenge was to super-resolve an input image with a magnification factor\nof $\\times$4 based on pairs of low and corresponding high resolution images.\nThe aim was to design a network for single image super-resolution that achieved\nimprovement of efficiency measured according to several metrics including\nruntime, parameters, FLOPs, activations, and memory consumption while at least\nmaintaining the PSNR of 29.00dB on DIV2K validation set. IMDN is set as the\nbaseline for efficiency measurement. The challenge had 3 tracks including the\nmain track (runtime), sub-track one (model complexity), and sub-track two\n(overall performance). In the main track, the practical runtime performance of\nthe submissions was evaluated. The rank of the teams were determined directly\nby the absolute value of the average runtime on the validation set and test\nset. In sub-track one, the number of parameters and FLOPs were considered. And\nthe individual rankings of the two metrics were summed up to determine a final\nranking in this track. In sub-track two, all of the five metrics mentioned in\nthe description of the challenge including runtime, parameter count, FLOPs,\nactivations, and memory consumption were considered. Similar to sub-track one,\nthe rankings of five metrics were summed up to determine a final ranking. The\nchallenge had 303 registered participants, and 43 teams made valid submissions.\nThey gauge the state-of-the-art in efficient single image super-resolution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yawei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Timofte_R/0/1/0/all/0/1\">Radu Timofte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_F/0/1/0/all/0/1\">Fangyuan Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mingxi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Songwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Z/0/1/0/all/0/1\">Zongcai Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Ding Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chenhui Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jingyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Q/0/1/0/all/0/1\">Qingrui Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zheyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yingqi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiangyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1\">Haoming Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1\">Chao Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Long Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Jinshan Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zong_Z/0/1/0/all/0/1\">Zhikai Zong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaoxiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hui_Z/0/1/0/all/0/1\">Zheng Hui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Tao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_P/0/1/0/all/0/1\">Peiran Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xuansong Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_X/0/1/0/all/0/1\">Xian-Sheng Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanbo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1\">Xiaozhong Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chuming Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_D/0/1/0/all/0/1\">Donghao Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1\">Ying Tai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengjie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhizhong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yuan Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1\">Shen Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Ziwei Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Lei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Z/0/1/0/all/0/1\">Zhihong Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu1_Q/0/1/0/all/0/1\">Qi Wu1</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Youwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1\">Haoqiang Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shuaicheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yuanfei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_M/0/1/0/all/0/1\">Meiguang Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hua Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinjian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_L/0/1/0/all/0/1\">Lingshun Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Gen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuanfan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Zuowei Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Lei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alexander_P/0/1/0/all/0/1\">Panaetov Alexander</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yucong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_M/0/1/0/all/0/1\">Minjie Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Li Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_L/0/1/0/all/0/1\">Lu Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zheyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1\">Hongbing Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1\">Yidong Cai</a>, et al. (45 additional authors not shown)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting Random Channel Pruning for Neural Network Compression. (arXiv:2205.05676v1 [cs.CV])","link":"http://arxiv.org/abs/2205.05676","description":"<p>Channel (or 3D filter) pruning serves as an effective way to accelerate the\ninference of neural networks. There has been a flurry of algorithms that try to\nsolve this practical problem, each being claimed effective in some ways. Yet, a\nbenchmark to compare those algorithms directly is lacking, mainly due to the\ncomplexity of the algorithms and some custom settings such as the particular\nnetwork configuration or training procedure. A fair benchmark is important for\nthe further development of channel pruning.\n</p>\n<p>Meanwhile, recent investigations reveal that the channel configurations\ndiscovered by pruning algorithms are at least as important as the pre-trained\nweights. This gives channel pruning a new role, namely searching the optimal\nchannel configuration. In this paper, we try to determine the channel\nconfiguration of the pruned models by random search. The proposed approach\nprovides a new way to compare different methods, namely how well they behave\ncompared with random pruning. We show that this simple strategy works quite\nwell compared with other channel pruning methods. We also show that under this\nsetting, there are surprisingly no clear winners among different channel\nimportance evaluation methods, which then may tilt the research efforts into\nadvanced channel configuration searching methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yawei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adamczewski_K/0/1/0/all/0/1\">Kamil Adamczewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1\">Shuhang Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Timofte_R/0/1/0/all/0/1\">Radu Timofte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HULC: 3D Human Motion Capture with Pose Manifold Sampling and Dense Contact Guidance. (arXiv:2205.05677v1 [cs.CV])","link":"http://arxiv.org/abs/2205.05677","description":"<p>Marker-less monocular 3D human motion capture (MoCap) with scene interactions\nis a challenging research topic relevant for extended reality, robotics and\nvirtual avatar generation. Due to the inherent depth ambiguity of monocular\nsettings, 3D motions captured with existing methods often contain severe\nartefacts such as incorrect body-scene inter-penetrations, jitter and body\nfloating. To tackle these issues, we propose HULC, a new approach for 3D human\nMoCap which is aware of the scene geometry. HULC estimates 3D poses and dense\nbody-environment surface contacts for improved 3D localisations, as well as the\nabsolute scale of the subject. Furthermore, we introduce a 3D pose trajectory\noptimisation based on a novel pose manifold sampling that resolves erroneous\nbody-environment inter-penetrations. Although the proposed method requires less\nstructured inputs compared to existing scene-aware monocular MoCap algorithms,\nit produces more physically-plausible poses: HULC significantly and\nconsistently outperforms the existing approaches in various experiments and on\ndifferent metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shimada_S/0/1/0/all/0/1\">Soshi Shimada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golyanik_V/0/1/0/all/0/1\">Vladislav Golyanik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_P/0/1/0/all/0/1\">Patrick P&#xe9;rez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Weipeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1\">Christian Theobalt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RISP: Rendering-Invariant State Predictor with Differentiable Simulation and Rendering for Cross-Domain Parameter Estimation. (arXiv:2205.05678v1 [cs.CV])","link":"http://arxiv.org/abs/2205.05678","description":"<p>This work considers identifying parameters characterizing a physical system's\ndynamic motion directly from a video whose rendering configurations are\ninaccessible. Existing solutions require massive training data or lack\ngeneralizability to unknown rendering configurations. We propose a novel\napproach that marries domain randomization and differentiable rendering\ngradients to address this problem. Our core idea is to train a\nrendering-invariant state-prediction (RISP) network that transforms image\ndifferences into state differences independent of rendering configurations,\ne.g., lighting, shadows, or material reflectance. To train this predictor, we\nformulate a new loss on rendering variances using gradients from differentiable\nrendering. Moreover, we present an efficient, second-order method to compute\nthe gradients of this loss, allowing it to be integrated seamlessly into modern\ndeep learning frameworks. We evaluate our method in rigid-body and\ndeformable-body simulation environments using four tasks: state estimation,\nsystem identification, imitation learning, and visuomotor control. We further\ndemonstrate the efficacy of our approach on a real-world example: inferring the\nstate and action sequences of a quadrotor from a video of its motion sequences.\nCompared with existing methods, our approach achieves significantly lower\nreconstruction errors and has better generalizability among unknown rendering\nconfigurations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_P/0/1/0/all/0/1\">Pingchuan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_T/0/1/0/all/0/1\">Tao Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1\">Joshua B. Tenenbaum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matusik_W/0/1/0/all/0/1\">Wojciech Matusik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1\">Chuang Gan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DDNet: Dual-path Decoder Network for Occlusion Relationship Reasoning. (arXiv:1911.11582v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1911.11582","description":"<p>Occlusion relationship reasoning based on convolution neural networks\nconsists of two subtasks: occlusion boundary extraction and occlusion\norientation inference. Due to the essential differences between the two\nsubtasks in the feature expression at the higher and lower stages, it is\nchallenging to carry on them simultaneously in one network. To address this\nissue, we propose a novel Dual-path Decoder Network, which uniformly extracts\nocclusion information at higher stages and separates into two paths to recover\nboundary and occlusion orientation respectively in lower stages. Besides,\nconsidering the restriction of occlusion orientation presentation to occlusion\norientation learning, we design a new orthogonal representation for occlusion\norientation and proposed the Orthogonal Orientation Regression loss which can\nget rid of the unfitness between occlusion representation and learning and\nfurther prompt the occlusion orientation learning. Finally, we apply a\nmulti-scale loss together with our proposed orientation regression loss to\nguide the boundary and orientation path learning respectively. Experiments\ndemonstrate that our proposed method achieves state-of-the-art results on PIOD\nand BSDS ownership datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_P/0/1/0/all/0/1\">Panhe Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_X/0/1/0/all/0/1\">Xuejing Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_L/0/1/0/all/0/1\">Lizhu Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Lei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chunpeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ming_A/0/1/0/all/0/1\">Anlong Ming</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DMT: Dynamic Mutual Training for Semi-Supervised Learning. (arXiv:2004.08514v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2004.08514","description":"<p>Recent semi-supervised learning methods use pseudo supervision as core idea,\nespecially self-training methods that generate pseudo labels. However, pseudo\nlabels are unreliable. Self-training methods usually rely on single model\nprediction confidence to filter low-confidence pseudo labels, thus remaining\nhigh-confidence errors and wasting many low-confidence correct labels. In this\npaper, we point out it is difficult for a model to counter its own errors.\nInstead, leveraging inter-model disagreement between different models is a key\nto locate pseudo label errors. With this new viewpoint, we propose mutual\ntraining between two different models by a dynamically re-weighted loss\nfunction, called Dynamic Mutual Training (DMT). We quantify inter-model\ndisagreement by comparing predictions from two different models to dynamically\nre-weight loss in training, where a larger disagreement indicates a possible\nerror and corresponds to a lower loss value. Extensive experiments show that\nDMT achieves state-of-the-art performance in both image classification and\nsemantic segmentation. Our codes are released at\nhttps://github.com/voldemortX/DST-CBC .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zhengyang Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qianyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1\">Qiqi Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xin Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1\">Guangliang Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xuequan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jianping Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lizhuang Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Asynchronous Kalman Filter for Hybrid Event Cameras. (arXiv:2012.05590v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.05590","description":"<p>Event cameras are ideally suited to capture HDR visual information without\nblur but perform poorly on static or slowly changing scenes. Conversely,\nconventional image sensors measure absolute intensity of slowly changing scenes\neffectively but do poorly on high dynamic range or quickly changing scenes. In\nthis paper, we present an event-based video reconstruction pipeline for High\nDynamic Range (HDR) scenarios. The proposed algorithm includes a frame\naugmentation pre-processing step that deblurs and temporally interpolates frame\ndata using events. The augmented frame and event data are then fused using a\nnovel asynchronous Kalman filter under a unifying uncertainty model for both\nsensors. Our experimental results are evaluated on both publicly available\ndatasets with challenging lighting conditions and fast motions and our new\ndataset with HDR reference. The proposed algorithm outperforms state-of-the-art\nmethods in both absolute intensity error (48% reduction) and image similarity\nindexes (average 11% improvement).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Ziwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_Y/0/1/0/all/0/1\">Yonhon Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scheerlinck_C/0/1/0/all/0/1\">Cedric Scheerlinck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahony_R/0/1/0/all/0/1\">Robert Mahony</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating Annotated Training Data for 6D Object Pose Estimation in Operational Environments with Minimal User Interaction. (arXiv:2103.09696v3 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2103.09696","description":"<p>Recently developed deep neural networks achieved state-of-the-art results in\nthe subject of 6D object pose estimation for robot manipulation. However, those\nsupervised deep learning methods require expensive annotated training data.\nCurrent methods for reducing those costs frequently use synthetic data from\nsimulations, but rely on expert knowledge and suffer from the \"domain gap\" when\nshifting to the real world. Here, we present a proof of concept for a novel\napproach of autonomously generating annotated training data for 6D object pose\nestimation. This approach is designed for learning new objects in operational\nenvironments while requiring little interaction and no expertise on the part of\nthe user. We evaluate our autonomous data generation approach in two grasping\nexperiments, where we archive a similar grasping success rate as related work\non a non autonomously generated data set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Koch_P/0/1/0/all/0/1\">Paul Koch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schluter_M/0/1/0/all/0/1\">Marian Schl&#xfc;ter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thill_S/0/1/0/all/0/1\">Serge Thill</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Shadow Generation for Composite Image in Real-world Scenes. (arXiv:2104.10338v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.10338","description":"<p>Image composition targets at inserting a foreground object into a background\nimage. Most previous image composition methods focus on adjusting the\nforeground to make it compatible with background while ignoring the shadow\neffect of foreground on the background. In this work, we focus on generating\nplausible shadow for the foreground object in the composite image. First, we\ncontribute a real-world shadow generation dataset DESOBA by generating\nsynthetic composite images based on paired real images and deshadowed images.\nThen, we propose a novel shadow generation network SGRNet, which consists of a\nshadow mask prediction stage and a shadow filling stage. In the shadow mask\nprediction stage, foreground and background information are thoroughly\ninteracted to generate foreground shadow mask. In the shadow filling stage,\nshadow parameters are predicted to fill the shadow area. Extensive experiments\non our DESOBA dataset and real composite images demonstrate the effectiveness\nof our proposed method. Our dataset and code are available at\nhttps://github.com/bcmi/Object-Shadow-Generation-Dataset-DESOBA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1\">Yan Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_L/0/1/0/all/0/1\">Li Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianfu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liqing Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Modal Transformer for Accelerated MR Imaging. (arXiv:2106.14248v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2106.14248","description":"<p>Accelerated multi-modal magnetic resonance (MR) imaging is a new and\neffective solution for fast MR imaging, providing superior performance in\nrestoring the target modality from its undersampled counterpart with guidance\nfrom an auxiliary modality. However, existing works simply combine the\nauxiliary modality as prior information, lacking in-depth investigations on the\npotential mechanisms for fusing different modalities. Further, they usually\nrely on the convolutional neural networks (CNNs), which is limited by the\nintrinsic locality in capturing the long-distance dependency. To this end, we\npropose a multi-modal transformer (MTrans), which is capable of transferring\nmulti-scale features from the target modality to the auxiliary modality, for\naccelerated MR imaging. To capture deep multi-modal information, our MTrans\nutilizes an improved multi-head attention mechanism, named cross attention\nmodule, which absorbs features from the auxiliary modality that contribute to\nthe target modality. Our framework provides three appealing benefits: (i) Our\nMTrans use an improved transformers for multi-modal MR imaging, affording more\nglobal information compared with existing CNN-based methods. (ii) A new cross\nattention module is proposed to exploit the useful information in each modality\nat different scales. The small patch in the target modality aims to keep more\nfine details, the large patch in the auxiliary modality aims to obtain\nhigh-level context features from the larger region and supplement the target\nmodality effectively. (iii) We evaluate MTrans with various accelerated\nmulti-modal MR imaging tasks, e.g., MR image reconstruction and\nsuper-resolution, where MTrans outperforms state-of-the-art methods on fastMRI\nand real-world clinical datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Feng_C/0/1/0/all/0/1\">Chun-Mei Feng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yan_Y/0/1/0/all/0/1\">Yunlu Yan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_G/0/1/0/all/0/1\">Geng Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Y/0/1/0/all/0/1\">Yong Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fu_H/0/1/0/all/0/1\">Huazhu Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Assessment of Deep Learning-based Heart Rate Estimation using Remote Photoplethysmography under Different Illuminations. (arXiv:2107.13193v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.13193","description":"<p>Remote photoplethysmography (rPPG) monitors heart rate without requiring\nphysical contact, which allows for a wide variety of applications. Deep\nlearning-based rPPG have demonstrated superior performance over the traditional\napproaches in controlled context. However, the lighting situation in indoor\nspace is typically complex, with uneven light distribution and frequent\nvariations in illumination. It lacks a fair comparison of different methods\nunder different illuminations using the same dataset. In this paper, we present\na public dataset, namely the BH-rPPG dataset, which contains data from thirty\nfive subjects under three illuminations: low, medium, and high illumination. We\nalso provide the ground truth heart rate measured by an oximeter. We evaluate\nthe performance of three deep learning-based methods (Deepphys, rPPGNet, and\nPhysnet) to that of four traditional methods (CHROM, GREEN, ICA, and POS) using\ntwo public datasets: the UBFC-rPPG dataset and the BH-rPPG dataset. The\nexperimental results demonstrate that traditional methods are generally more\nresistant to fluctuating illuminations. We found that the Physnet achieves\nlowest mean absolute error (MAE) among deep learning-based method under medium\nillumination, whereas the CHROM achieves 1.04 beats per minute (BPM),\noutperforming the Physnet by 80$\\%$. Additionally, we investigate potential\nmethods for improving performance of deep learning-based methods. We find that\nbrightness augmentation make model more robust to variation illumination. These\nfindings suggest that while developing deep learning-based heart rate\nestimation algorithms, illumination variation should be taken into account.\nThis work serves as a benchmark for rPPG performance evaluation and it opens a\npathway for future investigation into deep learning-based rPPG under\nillumination variations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Ze Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haofei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_F/0/1/0/all/0/1\">Feng Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stereo Hybrid Event-Frame (SHEF) Cameras for 3D Perception. (arXiv:2110.04988v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.04988","description":"<p>Stereo camera systems play an important role in robotics applications to\nperceive the 3D world. However, conventional cameras have drawbacks such as low\ndynamic range, motion blur and latency due to the underlying frame-based\nmechanism. Event cameras address these limitations as they report the\nbrightness changes of each pixel independently with a fine temporal resolution,\nbut they are unable to acquire absolute intensity information directly.\nAlthough integrated hybrid event-frame sensors (eg., DAVIS) are available, the\nquality of data is compromised by coupling at the pixel level in the circuit\nfabrication of such cameras. This paper proposes a stereo hybrid event-frame\n(SHEF) camera system that offers a sensor modality with separate high-quality\npure event and pure frame cameras, overcoming the limitations of each separate\nsensor and allowing for stereo depth estimation. We provide a SHEF dataset\ntargeted at evaluating disparity estimation algorithms and introduce a stereo\ndisparity estimation algorithm that uses edge information extracted from the\nevent stream correlated with the edge detected in the frame data. Our disparity\nestimation outperforms the state-of-the-art stereo matching algorithm on the\nSHEF dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Ziwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1\">Liyuan Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_Y/0/1/0/all/0/1\">Yonhon Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Z/0/1/0/all/0/1\">Zheyu Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahony_R/0/1/0/all/0/1\">Robert Mahony</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Synthesis of Diverse Weak Supervision Sources for Behavior Analysis. (arXiv:2111.15186v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2111.15186","description":"<p>Obtaining annotations for large training sets is expensive, especially in\nsettings where domain knowledge is required, such as behavior analysis. Weak\nsupervision has been studied to reduce annotation costs by using weak labels\nfrom task-specific labeling functions (LFs) to augment ground truth labels.\nHowever, domain experts still need to hand-craft different LFs for different\ntasks, limiting scalability. To reduce expert effort, we present AutoSWAP: a\nframework for automatically synthesizing data-efficient task-level LFs. The key\nto our approach is to efficiently represent expert knowledge in a reusable\ndomain-specific language and more general domain-level LFs, with which we use\nstate-of-the-art program synthesis techniques and a small labeled dataset to\ngenerate task-level LFs. Additionally, we propose a novel structural diversity\ncost that allows for efficient synthesis of diverse sets of LFs, further\nimproving AutoSWAP's performance. We evaluate AutoSWAP in three behavior\nanalysis domains and demonstrate that AutoSWAP outperforms existing approaches\nusing only a fraction of the data. Our results suggest that AutoSWAP is an\neffective way to automatically generate LFs that can significantly reduce\nexpert effort for behavior analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tseng_A/0/1/0/all/0/1\">Albert Tseng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jennifer J. Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_Y/0/1/0/all/0/1\">Yisong Yue</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated Distance Estimation for Wildlife Camera Trapping. (arXiv:2202.04613v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.04613","description":"<p>The ongoing biodiversity crisis calls for accurate estimation of animal\ndensity and abundance to identify sources of biodiversity decline and\neffectiveness of conservation interventions. Camera traps together with\nabundance estimation methods are often employed for this purpose. The necessary\ndistances between camera and observed animals are traditionally derived in a\nlaborious, fully manual or semi-automatic process. Both approaches require\nreference image material, which is both difficult to acquire and not available\nfor existing datasets. We propose a fully automatic approach we call AUtomated\nDIstance esTimation (AUDIT) to estimate camera-to-animal distances. We leverage\nexisting state-of-the-art relative monocular depth estimation and combine it\nwith a novel alignment procedure to estimate metric distances. AUDIT is fully\nautomated and requires neither the comparison of observations in camera trap\nimagery with reference images nor capturing of reference image material at all.\nAUDIT therefore relieves biologists and ecologists from a significant workload.\nWe evaluate AUDIT on a zoo scenario dataset unseen during training where we\nachieve a mean absolute distance estimation error over all animal instances of\nonly 0.9864 meters and mean relative error (REL) of 0.113. The code and usage\ninstructions are available at\nhttps://github.com/PJ-cs/DistanceEstimationTracking\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Johanns_P/0/1/0/all/0/1\">Peter Johanns</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haucke_T/0/1/0/all/0/1\">Timm Haucke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steinhage_V/0/1/0/all/0/1\">Volker Steinhage</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Do Vision Transformers Work?. (arXiv:2202.06709v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.06709","description":"<p>The success of multi-head self-attentions (MSAs) for computer vision is now\nindisputable. However, little is known about how MSAs work. We present\nfundamental explanations to help better understand the nature of MSAs. In\nparticular, we demonstrate the following properties of MSAs and Vision\nTransformers (ViTs): (1) MSAs improve not only accuracy but also generalization\nby flattening the loss landscapes. Such improvement is primarily attributable\nto their data specificity, not long-range dependency. On the other hand, ViTs\nsuffer from non-convex losses. Large datasets and loss landscape smoothing\nmethods alleviate this problem; (2) MSAs and Convs exhibit opposite behaviors.\nFor example, MSAs are low-pass filters, but Convs are high-pass filters.\nTherefore, MSAs and Convs are complementary; (3) Multi-stage neural networks\nbehave like a series connection of small individual models. In addition, MSAs\nat the end of a stage play a key role in prediction. Based on these insights,\nwe propose AlterNet, a model in which Conv blocks at the end of a stage are\nreplaced with MSA blocks. AlterNet outperforms CNNs not only in large data\nregimes but also in small data regimes. The code is available at\nhttps://github.com/xxxnell/how-do-vits-work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_N/0/1/0/all/0/1\">Namuk Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Songkuk Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Developing Imperceptible Adversarial Patches to Camouflage Military Assets From Computer Vision Enabled Technologies. (arXiv:2202.08892v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.08892","description":"<p>Convolutional neural networks (CNNs) have demonstrated rapid progress and a\nhigh level of success in object detection. However, recent evidence has\nhighlighted their vulnerability to adversarial attacks. These attacks are\ncalculated image perturbations or adversarial patches that result in object\nmisclassification or detection suppression. Traditional camouflage methods are\nimpractical when applied to disguise aircraft and other large mobile assets\nfrom autonomous detection in intelligence, surveillance and reconnaissance\ntechnologies and fifth generation missiles. In this paper we present a unique\nmethod that produces imperceptible patches capable of camouflaging large\nmilitary assets from computer vision-enabled technologies. We developed these\npatches by maximising object detection loss whilst limiting the patch's colour\nperceptibility. This work also aims to further the understanding of adversarial\nexamples and their effects on object detection algorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wise_C/0/1/0/all/0/1\">Chris Wise</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plested_J/0/1/0/all/0/1\">Jo Plested</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Multimodal German Dataset for Automatic Lip Reading Systems and Transfer Learning. (arXiv:2202.13403v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.13403","description":"<p>Large datasets as required for deep learning of lip reading do not exist in\nmany languages. In this paper we present the dataset GLips (German Lips)\nconsisting of 250,000 publicly available videos of the faces of speakers of the\nHessian Parliament, which was processed for word-level lip reading using an\nautomatic pipeline. The format is similar to that of the English language LRW\n(Lip Reading in the Wild) dataset, with each video encoding one word of\ninterest in a context of 1.16 seconds duration, which yields compatibility for\nstudying transfer learning between both datasets. By training a deep neural\nnetwork, we investigate whether lip reading has language-independent features,\nso that datasets of different languages can be used to improve lip reading\nmodels. We demonstrate learning from scratch and show that transfer learning\nfrom LRW to GLips and vice versa improves learning speed and performance, in\nparticular for the validation set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schwiebert_G/0/1/0/all/0/1\">Gerald Schwiebert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weber_C/0/1/0/all/0/1\">Cornelius Weber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1\">Leyuan Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siqueira_H/0/1/0/all/0/1\">Henrique Siqueira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wermter_S/0/1/0/all/0/1\">Stefan Wermter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Autoencoder Attractors for Uncertainty Estimation. (arXiv:2204.00382v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2204.00382","description":"<p>The reliability assessment of a machine learning model's prediction is an\nimportant quantity for the deployment in safety critical applications. Not only\ncan it be used to detect novel sceneries, either as out-of-distribution or\nanomaly sample, but it also helps to determine deficiencies in the training\ndata distribution. A lot of promising research directions have either proposed\ntraditional methods like Gaussian processes or extended deep learning based\napproaches, for example, by interpreting them from a Bayesian point of view. In\nthis work we propose a novel approach for uncertainty estimation based on\nautoencoder models: The recursive application of a previously trained\nautoencoder model can be interpreted as a dynamical system storing training\nexamples as attractors. While input images close to known samples will converge\nto the same or similar attractor, input samples containing unknown features are\nunstable and converge to different training samples by potentially removing or\nchanging characteristic features. The use of dropout during training and\ninference leads to a family of similar dynamical systems, each one being robust\non samples close to the training distribution but unstable on new features.\nEither the model reliably removes these features or the resulting instability\ncan be exploited to detect problematic input samples. We evaluate our approach\non several dataset combinations as well as on an industrial application for\noccupant classification in the vehicle interior for which we additionally\nrelease a new synthetic dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cruz_S/0/1/0/all/0/1\">Steve Dias Da Cruz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taetz_B/0/1/0/all/0/1\">Bertram Taetz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stifter_T/0/1/0/all/0/1\">Thomas Stifter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stricker_D/0/1/0/all/0/1\">Didier Stricker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sardino: Ultra-Fast Dynamic Ensemble for Secure Visual Sensing at Mobile Edge. (arXiv:2204.08189v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.08189","description":"<p>Adversarial example attack endangers the mobile edge systems such as vehicles\nand drones that adopt deep neural networks for visual sensing. This paper\npresents {\\em Sardino}, an active and dynamic defense approach that renews the\ninference ensemble at run time to develop security against the adaptive\nadversary who tries to exfiltrate the ensemble and construct the corresponding\neffective adversarial examples. By applying consistency check and data fusion\non the ensemble's predictions, Sardino can detect and thwart adversarial\ninputs. Compared with the training-based ensemble renewal, we use HyperNet to\nachieve {\\em one million times} acceleration and per-frame ensemble renewal\nthat presents the highest level of difficulty to the prerequisite exfiltration\nattacks. We design a run-time planner that maximizes the ensemble size in favor\nof security while maintaining the processing frame rate. Beyond adversarial\nexamples, Sardino can also address the issue of out-of-distribution inputs\neffectively. This paper presents extensive evaluation of Sardino's performance\nin counteracting adversarial examples and applies it to build a real-time\ncar-borne traffic sign recognition system. Live on-road tests show the built\nsystem's effectiveness in maintaining frame rate and detecting\nout-of-distribution inputs due to the false positives of a preceding YOLO-based\ntraffic sign detector.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_Q/0/1/0/all/0/1\">Qun Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zhenyu Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1\">Wenjie Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_R/0/1/0/all/0/1\">Rui Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The 6th AI City Challenge. (arXiv:2204.10380v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.10380","description":"<p>The 6th edition of the AI City Challenge specifically focuses on problems in\ntwo domains where there is tremendous unlocked potential at the intersection of\ncomputer vision and artificial intelligence: Intelligent Traffic Systems (ITS),\nand brick and mortar retail businesses. The four challenge tracks of the 2022\nAI City Challenge received participation requests from 254 teams across 27\ncountries. Track 1 addressed city-scale multi-target multi-camera (MTMC)\nvehicle tracking. Track 2 addressed natural-language-based vehicle track\nretrieval. Track 3 was a brand new track for naturalistic driving analysis,\nwhere the data were captured by several cameras mounted inside the vehicle\nfocusing on driver safety, and the task was to classify driver actions. Track 4\nwas another new track aiming to achieve retail store automated checkout using\nonly a single view camera. We released two leader boards for submissions based\non different methods, including a public leader board for the contest, where no\nuse of external data is allowed, and a general leader board for all submitted\nresults. The top performance of participating teams established strong\nbaselines and even outperformed the state-of-the-art in the proposed challenge\ntracks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Naphade_M/0/1/0/all/0/1\">Milind Naphade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anastasiu_D/0/1/0/all/0/1\">David C. Anastasiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1\">Zheng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1\">Ming-Ching Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yue Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Liang Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1\">Mohammed Shaiqur Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkatachalapathy_A/0/1/0/all/0/1\">Archana Venkatachalapathy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Anuj Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Q/0/1/0/all/0/1\">Qi Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ablavsky_V/0/1/0/all/0/1\">Vitaly Ablavsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sclaroff_S/0/1/0/all/0/1\">Stan Sclaroff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_P/0/1/0/all/0/1\">Pranamesh Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1\">Alice Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shangru Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chellappa_R/0/1/0/all/0/1\">Rama Chellappa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PolyLoss: A Polynomial Expansion Perspective of Classification Loss Functions. (arXiv:2204.12511v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.12511","description":"<p>Cross-entropy loss and focal loss are the most common choices when training\ndeep neural networks for classification problems. Generally speaking, however,\na good loss function can take on much more flexible forms, and should be\ntailored for different tasks and datasets. Motivated by how functions can be\napproximated via Taylor expansion, we propose a simple framework, named\nPolyLoss, to view and design loss functions as a linear combination of\npolynomial functions. Our PolyLoss allows the importance of different\npolynomial bases to be easily adjusted depending on the targeting tasks and\ndatasets, while naturally subsuming the aforementioned cross-entropy loss and\nfocal loss as special cases. Extensive experimental results show that the\noptimal choice within the PolyLoss is indeed dependent on the task and dataset.\nSimply by introducing one extra hyperparameter and adding one line of code, our\nPoly-1 formulation outperforms the cross-entropy loss and focal loss on 2D\nimage classification, instance segmentation, object detection, and 3D object\ndetection tasks, sometimes by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Leng_Z/0/1/0/all/0/1\">Zhaoqi Leng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1\">Mingxing Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chenxi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cubuk_E/0/1/0/all/0/1\">Ekin Dogus Cubuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xiaojie Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1\">Shuyang Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anguelov_D/0/1/0/all/0/1\">Dragomir Anguelov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pik-Fix: Restoring and Colorizing Old Photos. (arXiv:2205.01902v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.01902","description":"<p>Restoring and inpainting the visual memories that are present, but often\nimpaired, in old photos remains an intriguing but unsolved research topic.\nDecades-old photos often suffer from severe and commingled degradation such as\ncracks, defocus, and color-fading, which are difficult to treat individually\nand harder to repair when they interact. Deep learning presents a plausible\navenue, but the lack of large-scale datasets of old photos makes addressing\nthis restoration task very challenging. Here we present a novel reference-based\nend-to-end learning framework that is able to both repair and colorize old and\ndegraded pictures. Our proposed framework consists of three modules: a\nrestoration sub-network that conducts restoration from degradations, a\nsimilarity sub-network that performs color histogram matching and color\ntransfer, and a colorization subnet that learns to predict the chroma elements\nof images that have been conditioned on chromatic reference signals. The\noverall system makes use of color histogram priors from reference images, which\ngreatly reduces the need for large-scale training data. We have also created a\nfirst-of-a-kind public dataset of real old photos that are paired with ground\ntruth \"pristine\" photos that have been that have been manually restored by\nPhotoShop experts. We conducted extensive experiments on this dataset and\nsynthetic datasets, and found that our method significantly outperforms\nprevious state-of-the-art models using both qualitative comparisons and\nquantitative measurements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Runsheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhengzhong Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yuanqi Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xiaoyu Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinlong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1\">Zibo Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jiaqi Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bovik_A/0/1/0/all/0/1\">Alan Bovik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hongkai Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Iterative Geometry-Aware Cross Guidance Network for Stereo Image Inpainting. (arXiv:2205.03825v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.03825","description":"<p>Currently, single image inpainting has achieved promising results based on\ndeep convolutional neural networks. However, inpainting on stereo images with\nmissing regions has not been explored thoroughly, which is also a significant\nbut different problem. One crucial requirement for stereo image inpainting is\nstereo consistency. To achieve it, we propose an Iterative Geometry-Aware Cross\nGuidance Network (IGGNet). The IGGNet contains two key ingredients, i.e., a\nGeometry-Aware Attention (GAA) module and an Iterative Cross Guidance (ICG)\nstrategy. The GAA module relies on the epipolar geometry cues and learns the\ngeometry-aware guidance from one view to another, which is beneficial to make\nthe corresponding regions in two views consistent. However, learning guidance\nfrom co-existing missing regions is challenging. To address this issue, the ICG\nstrategy is proposed, which can alternately narrow down the missing regions of\nthe two views in an iterative manner. Experimental results demonstrate that our\nproposed network outperforms the latest stereo image inpainting model and\nstate-of-the-art single image inpainting models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1\">Ang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shanshan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qingjie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ke_Q/0/1/0/all/0/1\">Qiuhong Ke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improved Evaluation and Generation of Grid Layouts using Distance Preservation Quality and Linear Assignment Sorting. (arXiv:2205.04255v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.04255","description":"<p>Images sorted by similarity enables more images to be viewed simultaneously,\nand can be very useful for stock photo agencies or e-commerce applications.\nVisually sorted grid layouts attempt to arrange images so that their proximity\non the grid corresponds as closely as possible to their similarity. Various\nmetrics exist for evaluating such arrangements, but there is low experimental\nevidence on correlation between human perceived quality and metric value. We\npropose Distance Preservation Quality (DPQ) as a new metric to evaluate the\nquality of an arrangement. Extensive user testing revealed stronger correlation\nof DPQ with user-perceived quality and performance in image retrieval tasks\ncompared to other metrics. In addition, we introduce Fast Linear Assignment\nSorting (FLAS) as a new algorithm for creating visually sorted grid layouts.\nFLAS achieves very good sorting qualities while improving run time and\ncomputational resources.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Barthel_K/0/1/0/all/0/1\">Kai Uwe Barthel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hezel_N/0/1/0/all/0/1\">Nico Hezel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_K/0/1/0/all/0/1\">Klaus Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schall_K/0/1/0/all/0/1\">Konstantin Schall</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using Frequency Attention to Make Adversarial Patch Powerful Against Person Detector. (arXiv:2205.04638v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.04638","description":"<p>Deep neural networks (DNNs) are vulnerable to adversarial attacks. In\nparticular, object detectors may be attacked by applying a particular\nadversarial patch to the image. However, because the patch shrinks during\npreprocessing, most existing approaches that employ adversarial patches to\nattack object detectors would diminish the attack success rate on small and\nmedium targets. This paper proposes a Frequency Module(FRAN), a\nfrequency-domain attention module for guiding patch generation. This is the\nfirst study to introduce frequency domain attention to optimize the attack\ncapabilities of adversarial patches. Our method increases the attack success\nrates of small and medium targets by 4.18% and 3.89%, respectively, over the\nstate-of-the-art attack method for fooling the human detector while assaulting\nYOLOv3 without reducing the attack success rate of big targets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lei_X/0/1/0/all/0/1\">Xiaochun Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Chang Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zetao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1\">Zhaoting Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_X/0/1/0/all/0/1\">Xiang Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1\">Linjun Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"STDC-MA Network for Semantic Segmentation. (arXiv:2205.04639v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.04639","description":"<p>Semantic segmentation is applied extensively in autonomous driving and\nintelligent transportation with methods that highly demand spatial and semantic\ninformation. Here, an STDC-MA network is proposed to meet these demands. First,\nthe STDC-Seg structure is employed in STDC-MA to ensure a lightweight and\nefficient structure. Subsequently, the feature alignment module (FAM) is\napplied to understand the offset between high-level and low-level features,\nsolving the problem of pixel offset related to upsampling on the high-level\nfeature map. Our approach implements the effective fusion between high-level\nfeatures and low-level features. A hierarchical multiscale attention mechanism\nis adopted to reveal the relationship among attention regions from two\ndifferent input sizes of one image. Through this relationship, regions\nreceiving much attention are integrated into the segmentation results, thereby\nreducing the unfocused regions of the input image and improving the effective\nutilization of multiscale features. STDC- MA maintains the segmentation speed\nas an STDC-Seg network while improving the segmentation accuracy of small\nobjects. STDC-MA was verified on the verification set of Cityscapes. The\nsegmentation result of STDC-MA attained 76.81% mIOU with the input of 0.5x\nscale, 3.61% higher than STDC-Seg.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lei_X/0/1/0/all/0/1\">Xiaochun Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1\">Linjun Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zetao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1\">Zhaoting Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Chang Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jiaming Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OTFPF: Optimal Transport-Based Feature Pyramid Fusion Network for Brain Age Estimation with 3D Overlapped ConvNeXt. (arXiv:2205.04684v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.04684","description":"<p>Chronological age of healthy brain is able to be predicted using deep neural\nnetworks from T1-weighted magnetic resonance images (T1 MRIs), and the\npredicted brain age could serve as an effective biomarker for detecting\naging-related diseases or disorders. In this paper, we propose an end-to-end\nneural network architecture, referred to as optimal transport based feature\npyramid fusion (OTFPF) network, for the brain age estimation with T1 MRIs. The\nOTFPF consists of three types of modules: Optimal Transport based Feature\nPyramid Fusion (OTFPF) module, 3D overlapped ConvNeXt (3D OL-ConvNeXt) module\nand fusion module. These modules strengthen the OTFPF network's understanding\nof each brain's semi-multimodal and multi-level feature pyramid information,\nand significantly improve its estimation performances. Comparing with recent\nstate-of-the-art models, the proposed OTFPF converges faster and performs\nbetter. The experiments with 11,728 MRIs aged 3-97 years show that OTFPF\nnetwork could provide accurate brain age estimation, yielding mean absolute\nerror (MAE) of 2.097, Pearson's correlation coefficient (PCC) of 0.993 and\nSpearman's rank correlation coefficient (SRCC) of 0.989, between the estimated\nand chronological ages. Widespread quantitative experiments and ablation\nexperiments demonstrate the superiority and rationality of OTFPF network. The\ncodes and implement details will be released on GitHub:\nhttps://github.com/ZJU-Brain/OTFPF after final decision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yu Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yanyan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yalin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_S/0/1/0/all/0/1\">Shunjie Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_L/0/1/0/all/0/1\">Le Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1\">Xunzhao Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1\">Qianqian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yiyu Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuo_C/0/1/0/all/0/1\">Cheng Zhuo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Impact of Partial Occlusion on Pedestrian Detectability. (arXiv:2205.04812v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.04812","description":"<p>Robust detection of vulnerable road users is a safety critical requirement\nfor the deployment of autonomous vehicles in heterogeneous traffic. One of the\nmost complex outstanding challenges is that of partial occlusion where a target\nobject is only partially available to the sensor due to obstruction by another\nforeground object. A number of leading pedestrian detection benchmarks provide\nannotation for partial occlusion, however each benchmark varies greatly in\ntheir definition of the occurrence and severity of occlusion. Recent research\ndemonstrates that a high degree of subjectivity is used to classify occlusion\nlevel in these cases and occlusion is typically categorized into 2 to 3 broad\ncategories such as partially and heavily occluded. This can lead to inaccurate\nor inconsistent reporting of pedestrian detection model performance depending\non which benchmark is used. This research introduces a novel, objective\nbenchmark for partially occluded pedestrian detection to facilitate the\nobjective characterization of pedestrian detection models. Characterization is\ncarried out on seven popular pedestrian detection models for a range of\nocclusion levels from 0-99%. Results demonstrate that pedestrian detection\nperformance degrades, and the number of false negative detections increase as\npedestrian occlusion level increases. Of the seven popular pedestrian detection\nroutines characterized, CenterNet has the greatest overall performance,\nfollowed by SSDlite. RetinaNet has the lowest overall detection performance\nacross the range of occlusion levels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gilroy_S/0/1/0/all/0/1\">Shane Gilroy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mullins_D/0/1/0/all/0/1\">Darragh Mullins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jones_E/0/1/0/all/0/1\">Edward Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parsi_A/0/1/0/all/0/1\">Ashkan Parsi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glavin_M/0/1/0/all/0/1\">Martin Glavin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Answer Visual Questions from Web Videos. (arXiv:2205.05019v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.05019","description":"<p>Recent methods for visual question answering rely on large-scale annotated\ndatasets. Manual annotation of questions and answers for videos, however, is\ntedious, expensive and prevents scalability. In this work, we propose to avoid\nmanual annotation and generate a large-scale training dataset for video\nquestion answering making use of automatic cross-modal supervision. We leverage\na question generation transformer trained on text data and use it to generate\nquestion-answer pairs from transcribed video narrations. Given narrated videos,\nwe then automatically generate the HowToVQA69M dataset with 69M\nvideo-question-answer triplets. To handle the open vocabulary of diverse\nanswers in this dataset, we propose a training procedure based on a contrastive\nloss between a video-question multi-modal transformer and an answer\ntransformer. We introduce the zero-shot VideoQA task and the VideoQA feature\nprobe evaluation setting and show excellent results, in particular for rare\nanswers. Furthermore, our method achieves competitive results on MSRVTT-QA,\nActivityNet-QA, MSVD-QA and How2QA datasets. We also show that our VideoQA\ndataset generation approach generalizes to another source of web video and text\ndata. We use our method to generate the WebVidVQA3M dataset from the WebVid\ndataset, i.e., videos with alt-text annotations, and show its benefits for\ntraining VideoQA models. Finally, for a detailed evaluation we introduce iVQA,\na new VideoQA dataset with reduced language bias and high-quality manual\nannotations. Code, datasets and trained models are available at\nhttps://antoyang.github.io/just-ask.html\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1\">Antoine Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miech_A/0/1/0/all/0/1\">Antoine Miech</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sivic_J/0/1/0/all/0/1\">Josef Sivic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laptev_I/0/1/0/all/0/1\">Ivan Laptev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1\">Cordelia Schmid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-05-11T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}}]}]}