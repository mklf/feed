{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-06-01T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Parameter-Efficient and Student-Friendly Knowledge Distillation. (arXiv:2205.15308v1 [cs.LG])","link":"http://arxiv.org/abs/2205.15308","description":"<p>Knowledge distillation (KD) has been extensively employed to transfer the\nknowledge from a large teacher model to the smaller students, where the\nparameters of the teacher are fixed (or partially) during training. Recent\nstudies show that this mode may cause difficulties in knowledge transfer due to\nthe mismatched model capacities. To alleviate the mismatch problem,\nteacher-student joint training methods, e.g., online distillation, have been\nproposed, but it always requires expensive computational cost. In this paper,\nwe present a parameter-efficient and student-friendly knowledge distillation\nmethod, namely PESF-KD, to achieve efficient and sufficient knowledge transfer\nby updating relatively few partial parameters. Technically, we first\nmathematically formulate the mismatch as the sharpness gap between their\npredictive distributions, where we show such a gap can be narrowed with the\nappropriate smoothness of the soft label. Then, we introduce an adapter module\nfor the teacher and only update the adapter to obtain soft labels with\nappropriate smoothness. Experiments on a variety of benchmarks show that\nPESF-KD can significantly reduce the training cost while obtaining competitive\nresults compared to advanced online distillation methods. Code will be released\nupon acceptance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rao_J/0/1/0/all/0/1\">Jun Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1\">Xv Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1\">Liang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_S/0/1/0/all/0/1\">Shuhan Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StyleTTS: A Style-Based Generative Model for Natural and Diverse Text-to-Speech Synthesis. (arXiv:2205.15439v1 [eess.AS])","link":"http://arxiv.org/abs/2205.15439","description":"<p>Text-to-Speech (TTS) has recently seen great progress in synthesizing\nhigh-quality speech owing to the rapid development of parallel TTS systems, but\nproducing speech with naturalistic prosodic variations, speaking styles and\nemotional tones remains challenging. Moreover, since duration and speech are\ngenerated separately, parallel TTS models still have problems finding the best\nmonotonic alignments that are crucial for naturalistic speech synthesis. Here,\nwe propose StyleTTS, a style-based generative model for parallel TTS that can\nsynthesize diverse speech with natural prosody from a reference speech\nutterance. With novel Transferable Monotonic Aligner (TMA) and\nduration-invariant data augmentation schemes, our method significantly\noutperforms state-of-the-art models on both single and multi-speaker datasets\nin subjective tests of speech naturalness and speaker similarity. Through\nself-supervised learning of the speaking styles, our model can synthesize\nspeech with the same prosodic and emotional tone as any given reference speech\nwithout the need for explicitly labeling these categories.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yinghao Aaron Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Han_C/0/1/0/all/0/1\">Cong Han</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mesgarani_N/0/1/0/all/0/1\">Nima Mesgarani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analyzing Modality Robustness in Multimodal Sentiment Analysis. (arXiv:2205.15465v1 [cs.CL])","link":"http://arxiv.org/abs/2205.15465","description":"<p>Building robust multimodal models are crucial for achieving reliable\ndeployment in the wild. Despite its importance, less attention has been paid to\nidentifying and improving the robustness of Multimodal Sentiment Analysis (MSA)\nmodels. In this work, we hope to address that by (i) Proposing simple\ndiagnostic checks for modality robustness in a trained multimodal model. Using\nthese checks, we find MSA models to be highly sensitive to a single modality,\nwhich creates issues in their robustness; (ii) We analyze well-known robust\ntraining strategies to alleviate the issues. Critically, we observe that\nrobustness can be achieved without compromising on the original performance. We\nhope our extensive study-performed across five models and two benchmark\ndatasets-and proposed procedures would make robustness an integral component in\nMSA research. Our diagnostic checks and robust training solutions are simple to\nimplement and available at https://github. com/declare-lab/MSA-Robustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hazarika_D/0/1/0/all/0/1\">Devamanyu Hazarika</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yingting Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_B/0/1/0/all/0/1\">Bo Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shuai Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zimmermann_R/0/1/0/all/0/1\">Roger Zimmermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poria_S/0/1/0/all/0/1\">Soujanya Poria</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FinBERT-MRC: financial named entity recognition using BERT under the machine reading comprehension paradigm. (arXiv:2205.15485v1 [cs.CL])","link":"http://arxiv.org/abs/2205.15485","description":"<p>Financial named entity recognition (FinNER) from literature is a challenging\ntask in the field of financial text information extraction, which aims to\nextract a large amount of financial knowledge from unstructured texts. It is\nwidely accepted to use sequence tagging frameworks to implement FinNER tasks.\nHowever, such sequence tagging models cannot fully take advantage of the\nsemantic information in the texts. Instead, we formulate the FinNER task as a\nmachine reading comprehension (MRC) problem and propose a new model termed\nFinBERT-MRC. This formulation introduces significant prior information by\nutilizing well-designed queries, and extracts start index and end index of\ntarget entities without decoding modules such as conditional random fields\n(CRF). We conduct experiments on a publicly available Chinese financial dataset\nChFinAnn and a real-word bussiness dataset AdminPunish. FinBERT-MRC model\nachieves average F1 scores of 92.78% and 96.80% on the two datasets,\nrespectively, with average F1 gains +3.94% and +0.89% over some sequence\ntagging models including BiLSTM-CRF, BERT-Tagger, and BERT-CRF. The source code\nis available at https://github.com/zyz0000/FinBERT-MRC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuzhe Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hong Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Pre-Trained Language Models to Streamline Natural Language Interaction for Self-Tracking. (arXiv:2205.15503v1 [cs.CL])","link":"http://arxiv.org/abs/2205.15503","description":"<p>Current natural language interaction for self-tracking tools largely depends\non bespoke implementation optimized for a specific tracking theme and data\nformat, which is neither generalizable nor scalable to a tremendous design\nspace of self-tracking. However, training machine learning models in the\ncontext of self-tracking is challenging due to the wide variety of tracking\ntopics and data formats. In this paper, we propose a novel NLP task for\nself-tracking that extracts close- and open-ended information from a\nretrospective activity log described as a plain text, and a domain-agnostic,\nGPT-3-based NLU framework that performs this task. The framework augments the\nprompt using synthetic samples to transform the task into 10-shot learning, to\naddress a cold-start problem in bootstrapping a new tracking topic. Our\npreliminary evaluation suggests that our approach significantly outperforms the\nbaseline QA models. Going further, we discuss future application domains toward\nwhich the NLP and HCI researchers can collaborate.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Young-Ho Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sungdong Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1\">Minsuk Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sang-Woo Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ADAPT: Vision-Language Navigation with Modality-Aligned Action Prompts. (arXiv:2205.15509v1 [cs.CV])","link":"http://arxiv.org/abs/2205.15509","description":"<p>Vision-Language Navigation (VLN) is a challenging task that requires an\nembodied agent to perform action-level modality alignment, i.e., make\ninstruction-asked actions sequentially in complex visual environments. Most\nexisting VLN agents learn the instruction-path data directly and cannot\nsufficiently explore action-level alignment knowledge inside the multi-modal\ninputs. In this paper, we propose modAlity-aligneD Action PrompTs (ADAPT),\nwhich provides the VLN agent with action prompts to enable the explicit\nlearning of action-level modality alignment to pursue successful navigation.\nSpecifically, an action prompt is defined as a modality-aligned pair of an\nimage sub-prompt and a text sub-prompt, where the former is a single-view\nobservation and the latter is a phrase like ''walk past the chair''. When\nstarting navigation, the instruction-related action prompt set is retrieved\nfrom a pre-built action prompt base and passed through a prompt encoder to\nobtain the prompt feature. Then the prompt feature is concatenated with the\noriginal instruction feature and fed to a multi-layer transformer for action\nprediction. To collect high-quality action prompts into the prompt base, we use\nthe Contrastive Language-Image Pretraining (CLIP) model which has powerful\ncross-modality alignment ability. A modality alignment loss and a sequential\nconsistency loss are further introduced to enhance the alignment of the action\nprompt and enforce the agent to focus on the related prompt sequentially.\nExperimental results on both R2R and RxR show the superiority of ADAPT over\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Bingqian Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zicong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiwen Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jianzhuang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Event-Level Sentiment Analysis with Structured Arguments. (arXiv:2205.15511v1 [cs.CL])","link":"http://arxiv.org/abs/2205.15511","description":"<p>Previous studies about event-level sentiment analysis (SA) usually model the\nevent as a topic, a category or target terms, while the structured arguments\n(e.g., subject, object, time and location) that have potential effects on the\nsentiment are not well studied. In this paper, we redefine the task as\nstructured event-level SA and propose an End-to-End Event-level Sentiment\nAnalysis ($\\textit{E}^{3}\\textit{SA}$) approach to solve this issue.\nSpecifically, we explicitly extract and model the event structure information\nfor enhancing event-level SA. Extensive experiments demonstrate the great\nadvantages of our proposed approach over the state-of-the-art methods. Noting\nthe lack of the dataset, we also release a large-scale real-world dataset with\nevent arguments and sentiment labelling for promoting more\nresearches\\footnote{The dataset is available at\nhttps://github.com/zhangqi-here/E3SA}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Q/0/1/0/all/0/1\">Qinchun Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Liang He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Unified Framework for Emotion Identification and Generation in Dialogues. (arXiv:2205.15513v1 [cs.CL])","link":"http://arxiv.org/abs/2205.15513","description":"<p>Social chatbots have gained immense popularity, and their appeal lies not\njust in their capacity to respond to the diverse requests from users, but also\nin the ability to develop an emotional connection with users. To further\ndevelop and promote social chatbots, we need to concentrate on increasing user\ninteraction and take into account both the intellectual and emotional quotient\nin the conversational agents. In this paper, we propose a multi-task framework\nthat jointly identifies the emotion of a given dialogue and generates response\nin accordance to the identified emotion. We employ a BERT based network for\ncreating an empathetic system and use a mixed objective function that trains\nthe end-to-end network with both the classification and generation loss.\nExperimental results show that our proposed framework outperforms current\nstate-of-the-art models\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Madasu_A/0/1/0/all/0/1\">Avinash Madasu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Firdaus_M/0/1/0/all/0/1\">Mauajama Firdaus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eqbal_A/0/1/0/all/0/1\">Asif Eqbal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Knowledge-Enhanced Adversarial Model for Cross-lingual Structured Sentiment Analysis. (arXiv:2205.15514v1 [cs.CL])","link":"http://arxiv.org/abs/2205.15514","description":"<p>Structured sentiment analysis, which aims to extract the complex semantic\nstructures such as holders, expressions, targets, and polarities, has obtained\nwidespread attention from both industry and academia. Unfortunately, the\nexisting structured sentiment analysis datasets refer to a few languages and\nare relatively small, limiting neural network models' performance. In this\npaper, we focus on the cross-lingual structured sentiment analysis task, which\naims to transfer the knowledge from the source language to the target one.\nNotably, we propose a Knowledge-Enhanced Adversarial Model (\\texttt{KEAM}) with\nboth implicit distributed and explicit structural knowledge to enhance the\ncross-lingual transfer. First, we design an adversarial embedding adapter for\nlearning an informative and robust representation by capturing implicit\nsemantic information from diverse multi-lingual embeddings adaptively. Then, we\npropose a syntax GCN encoder to transfer the explicit semantic information\n(e.g., universal dependency tree) among multiple languages. We conduct\nexperiments on five datasets and compare \\texttt{KEAM} with both the supervised\nand unsupervised methods. The extensive experimental results show that our\n\\texttt{KEAM} model outperforms all the unsupervised baselines in various\nmetrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Q/0/1/0/all/0/1\">Qingchun Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jun Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Liang He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Refining Low-Resource Unsupervised Translation by Language Disentanglement of Multilingual Model. (arXiv:2205.15544v1 [cs.CL])","link":"http://arxiv.org/abs/2205.15544","description":"<p>Numerous recent work on unsupervised machine translation (UMT) implies that\ncompetent unsupervised translations of low-resource and unrelated languages,\nsuch as Nepali or Sinhala, are only possible if the model is trained in a\nmassive multilingual environment, where theses low-resource languages are mixed\nwith high-resource counterparts. Nonetheless, while the high-resource languages\ngreatly help kick-start the target low-resource translation tasks, the language\ndiscrepancy between them may hinder their further improvement. In this work, we\npropose a simple refinement procedure to disentangle languages from a\npre-trained multilingual UMT model for it to focus on only the target\nlow-resource task. Our method achieves the state of the art in the fully\nunsupervised translation tasks of English to Nepali, Sinhala, Gujarati,\nLatvian, Estonian and Kazakh, with BLEU score gains of 3.5, 3.5, 3.3, 4.1, 4.2,\nand 3.3, respectively. Our codebase is available at\nhttps://github.com/nxphi47/refine_unsup_multilingual_mt\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_X/0/1/0/all/0/1\">Xuan-Phi Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kui_W/0/1/0/all/0/1\">Wu Kui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aw_A/0/1/0/all/0/1\">Ai Ti Aw</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Multi-level Supervised Contrastive Learning Framework for Low-Resource Natural Language Inference. (arXiv:2205.15550v1 [cs.CL])","link":"http://arxiv.org/abs/2205.15550","description":"<p>Natural Language Inference (NLI) is a growingly essential task in natural\nlanguage understanding, which requires inferring the relationship between the\nsentence pairs (premise and hypothesis). Recently, low-resource natural\nlanguage inference has gained increasing attention, due to significant savings\nin manual annotation costs and a better fit with real-world scenarios. Existing\nworks fail to characterize discriminative representations between different\nclasses with limited training data, which may cause faults in label prediction.\nHere we propose a multi-level supervised contrastive learning framework named\nMultiSCL for low-resource natural language inference. MultiSCL leverages a\nsentence-level and pair-level contrastive learning objective to discriminate\nbetween different classes of sentence pairs by bringing those in one class\ntogether and pushing away those in different classes. MultiSCL adopts a data\naugmentation module that generates different views for input samples to better\nlearn the latent representation. The pair-level representation is obtained from\na cross attention module. We conduct extensive experiments on two public NLI\ndatasets in low-resource settings, and the accuracy of MultiSCL exceeds other\nmodels by 3.1% on average. Moreover, our method outperforms the previous\nstate-of-the-art method on cross-domain tasks of text classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shu&#x27;ang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xuming Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Li Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Aiwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1\">Lijie Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"hmBERT: Historical Multilingual Language Models for Named Entity Recognition. (arXiv:2205.15575v1 [cs.CL])","link":"http://arxiv.org/abs/2205.15575","description":"<p>Compared to standard Named Entity Recognition (NER), identifying persons,\nlocations, and organizations in historical texts forms a big challenge. To\nobtain machine-readable corpora, the historical text is usually scanned and\noptical character recognition (OCR) needs to be performed. As a result, the\nhistorical corpora contain errors. Also, entities like location or organization\ncan change over time, which poses another challenge. Overall historical texts\ncome with several peculiarities that differ greatly from modern texts and large\nlabeled corpora for training a neural tagger are hardly available for this\ndomain. In this work, we tackle NER for historical German, English, French,\nSwedish, and Finnish by training large historical language models. We\ncircumvent the need for labeled data by using unlabeled data for pretraining a\nlanguage model. hmBERT, a historical multilingual BERT-based language model is\nproposed, with different sizes of it being publicly released. Furthermore, we\nevaluate the capability of hmBERT by solving downstream NER as part of this\nyear's HIPE-2022 shared task and provide detailed analysis and insights. For\nthe Multilingual Classical Commentary coarse-grained NER challenge, our tagger\nHISTeria outperforms the other teams' models for two out of three languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schweter_S/0/1/0/all/0/1\">Stefan Schweter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marz_L/0/1/0/all/0/1\">Luisa M&#xe4;rz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_K/0/1/0/all/0/1\">Katharina Schmid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cano_E/0/1/0/all/0/1\">Erion &#xc7;ano</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Preparing an Endangered Language for the Digital Age: The Case of Judeo-Spanish. (arXiv:2205.15599v1 [cs.CL])","link":"http://arxiv.org/abs/2205.15599","description":"<p>We develop machine translation and speech synthesis systems to complement the\nefforts of revitalizing Judeo-Spanish, the exiled language of Sephardic Jews,\nwhich survived for centuries, but now faces the threat of extinction in the\ndigital age. Building on resources created by the Sephardic community of Turkey\nand elsewhere, we create corpora and tools that would help preserve this\nlanguage for future generations. For machine translation, we first develop a\nSpanish to Judeo-Spanish rule-based machine translation system, in order to\ngenerate large volumes of synthetic parallel data in the relevant language\npairs: Turkish, English and Spanish. Then, we train baseline neural machine\ntranslation engines using this synthetic data and authentic parallel data\ncreated from translations by the Sephardic community. For text-to-speech\nsynthesis, we present a 3.5 hour single speaker speech corpus for building a\nneural speech synthesis engine. Resources, model weights and online inference\nengines are shared publicly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Oktem_A/0/1/0/all/0/1\">Alp &#xd6;ktem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zevallos_R/0/1/0/all/0/1\">Rodolfo Zevallos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moslem_Y/0/1/0/all/0/1\">Yasmin Moslem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozturk_G/0/1/0/all/0/1\">G&#xfc;ne&#x15f; &#xd6;zt&#xfc;rk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarhon_K/0/1/0/all/0/1\">Karen &#x15e;arhon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"APPReddit: a Corpus of Reddit Posts Annotated for Appraisal. (arXiv:2205.15627v1 [cs.CL])","link":"http://arxiv.org/abs/2205.15627","description":"<p>Despite the large number of computational resources for emotion recognition,\nthere is a lack of data sets relying on appraisal models. According to\nAppraisal theories, emotions are the outcome of a multi-dimensional evaluation\nof events. In this paper, we present APPReddit, the first corpus of\nnon-experimental data annotated according to this theory. After describing its\ndevelopment, we compare our resource with enISEAR, a corpus of events created\nin an experimental setting and annotated for appraisal. Results show that the\ntwo corpora can be mapped notwithstanding different typologies of data and\nannotations schemes. A SVM model trained on APPReddit predicts four appraisal\ndimensions without significant loss. Merging both corpora in a single training\nset increases the prediction of 3 out of 4 dimensions. Such findings pave the\nway to a better performing classification model for appraisal prediction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stranisci_M/0/1/0/all/0/1\">Marco Antonio Stranisci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frenda_S/0/1/0/all/0/1\">Simona Frenda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ceccaldi_E/0/1/0/all/0/1\">Eleonora Ceccaldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basile_V/0/1/0/all/0/1\">Valerio Basile</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Damiano_R/0/1/0/all/0/1\">Rossana Damiano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patti_V/0/1/0/all/0/1\">Viviana Patti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NEWTS: A Corpus for News Topic-Focused Summarization. (arXiv:2205.15661v1 [cs.CL])","link":"http://arxiv.org/abs/2205.15661","description":"<p>Text summarization models are approaching human levels of fidelity. Existing\nbenchmarking corpora provide concordant pairs of full and abridged versions of\nWeb, news or, professional content. To date, all summarization datasets operate\nunder a one-size-fits-all paradigm that may not reflect the full range of\norganic summarization needs. Several recently proposed models (e.g., plug and\nplay language models) have the capacity to condition the generated summaries on\na desired range of themes. These capacities remain largely unused and\nunevaluated as there is no dedicated dataset that would support the task of\ntopic-focused summarization.\n</p>\n<p>This paper introduces the first topical summarization corpus NEWTS, based on\nthe well-known CNN/Dailymail dataset, and annotated via online crowd-sourcing.\nEach source article is paired with two reference summaries, each focusing on a\ndifferent theme of the source document. We evaluate a representative range of\nexisting techniques and analyze the effectiveness of different prompting\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bahrainian_S/0/1/0/all/0/1\">Seyed Ali Bahrainian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feucht_S/0/1/0/all/0/1\">Sheridan Feucht</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eickhoff_C/0/1/0/all/0/1\">Carsten Eickhoff</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Why are NLP Models Fumbling at Elementary Math? A Survey of Deep Learning based Word Problem Solvers. (arXiv:2205.15683v1 [cs.CL])","link":"http://arxiv.org/abs/2205.15683","description":"<p>From the latter half of the last decade, there has been a growing interest in\ndeveloping algorithms for automatically solving mathematical word problems\n(MWP). It is a challenging and unique task that demands blending surface level\ntext pattern recognition with mathematical reasoning. In spite of extensive\nresearch, we are still miles away from building robust representations of\nelementary math word problems and effective solutions for the general task. In\nthis paper, we critically examine the various models that have been developed\nfor solving word problems, their pros and cons and the challenges ahead. In the\nlast two years, a lot of deep learning models have recorded competing results\non benchmark datasets, making a critical and conceptual analysis of literature\nhighly useful at this juncture. We take a step back and analyse why, in spite\nof this abundance in scholarly interest, the predominantly used experiment and\ndataset designs continue to be a stumbling block. From the vantage point of\nhaving analyzed the literature closely, we also endeavour to provide a road-map\nfor future math word problem research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sundaram_S/0/1/0/all/0/1\">Sowmya S Sundaram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurajada_S/0/1/0/all/0/1\">Sairam Gurajada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fisichella_M/0/1/0/all/0/1\">Marco Fisichella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+P_D/0/1/0/all/0/1\">Deepak P</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abraham_S/0/1/0/all/0/1\">Savitha Sam Abraham</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Informational Space Based Semantic Analysis for Scientific Texts. (arXiv:2205.15696v1 [cs.CL])","link":"http://arxiv.org/abs/2205.15696","description":"<p>One major problem in Natural Language Processing is the automatic analysis\nand representation of human language. Human language is ambiguous and deeper\nunderstanding of semantics and creating human-to-machine interaction have\nrequired an effort in creating the schemes for act of communication and\nbuilding common-sense knowledge bases for the 'meaning' in texts. This paper\nintroduces computational methods for semantic analysis and the quantifying the\nmeaning of short scientific texts. Computational methods extracting semantic\nfeature are used to analyse the relations between texts of messages and\n'representations of situations' for a newly created large collection of\nscientific texts, Leicester Scientific Corpus. The representation of\nscientific-specific meaning is standardised by replacing the situation\nrepresentations, rather than psychological properties, with the vectors of some\nattributes: a list of scientific subject categories that the text belongs to.\nFirst, this paper introduces 'Meaning Space' in which the informational\nrepresentation of the meaning is extracted from the occurrence of the word in\ntexts across the scientific categories, i.e., the meaning of a word is\nrepresented by a vector of Relative Information Gain about the subject\ncategories. Then, the meaning space is statistically analysed for Leicester\nScientific Dictionary-Core and we investigate 'Principal Components of the\nMeaning' to describe the adequate dimensions of the meaning. The research in\nthis paper conducts the base for the geometric representation of the meaning of\ntexts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Suzen_N/0/1/0/all/0/1\">Neslihan Suzen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gorban_A/0/1/0/all/0/1\">Alexander N. Gorban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levesley_J/0/1/0/all/0/1\">Jeremy Levesley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirkes_E/0/1/0/all/0/1\">Evgeny M. Mirkes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual Transformers for Product Matching -- Experiments and a New Benchmark in Polish. (arXiv:2205.15712v1 [cs.CL])","link":"http://arxiv.org/abs/2205.15712","description":"<p>Product matching corresponds to the task of matching identical products\nacross different data sources. It typically employs available product features\nwhich, apart from being multimodal, i.e., comprised of various data types,\nmight be non-homogeneous and incomplete. The paper shows that pre-trained,\nmultilingual Transformer models, after fine-tuning, are suitable for solving\nthe product matching problem using textual features both in English and Polish\nlanguages. We tested multilingual mBERT and XLM-RoBERTa models in English on\nWeb Data Commons - training dataset and gold standard for large-scale product\nmatching. The obtained results show that these models perform similarly to the\nlatest solutions tested on this set, and in some cases, the results were even\nbetter.\n</p>\n<p>Additionally, we prepared a new dataset -- ProductMatch.pl -- that is\nentirely in Polish and based on offers in selected categories obtained from\nseveral online stores for the research purpose. It is the first open dataset\nfor product matching tasks in Polish, which allows comparing the effectiveness\nof the pre-trained models. Thus, we also showed the baseline results obtained\nby the fine-tuned mBERT and XLM-RoBERTa models on the Polish datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mo%7Bz%7Dd%7Bz%7Donek_M/0/1/0/all/0/1\">Micha&#x142; Mo{&#x17c;}d{&#x17c;}onek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wroblewska_A/0/1/0/all/0/1\">Anna Wr&#xf3;blewska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tkachuk_S/0/1/0/all/0/1\">Sergiy Tkachuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lukasik_S/0/1/0/all/0/1\">Szymon &#x141;ukasik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Don't Forget Cheap Training Signals Before Building Unsupervised Bilingual Word Embeddings. (arXiv:2205.15713v1 [cs.CL])","link":"http://arxiv.org/abs/2205.15713","description":"<p>Bilingual Word Embeddings (BWEs) are one of the cornerstones of cross-lingual\ntransfer of NLP models. They can be built using only monolingual corpora\nwithout supervision leading to numerous works focusing on unsupervised BWEs.\nHowever, most of the current approaches to build unsupervised BWEs do not\ncompare their results with methods based on easy-to-access cross-lingual\nsignals. In this paper, we argue that such signals should always be considered\nwhen developing unsupervised BWE methods. The two approaches we find most\neffective are: 1) using identical words as seed lexicons (which unsupervised\napproaches incorrectly assume are not available for orthographically distinct\nlanguage pairs) and 2) combining such lexicons with pairs extracted by matching\nromanized versions of words with an edit distance threshold. We experiment on\nthirteen non-Latin languages (and English) and show that such cheap signals\nwork well and that they outperform using more complex unsupervised methods on\ndistant language pairs such as Chinese, Japanese, Kannada, Tamil, and Thai. In\naddition, they are even competitive with the use of high-quality lexicons in\nsupervised approaches. Our results show that these training signals should not\nbe neglected when building BWEs, even for distant languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Severini_S/0/1/0/all/0/1\">Silvia Severini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hangya_V/0/1/0/all/0/1\">Viktor Hangya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabet_M/0/1/0/all/0/1\">Masoud Jalili Sabet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fraser_A/0/1/0/all/0/1\">Alexander Fraser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EMS: Efficient and Effective Massively Multilingual Sentence Representation Learning. (arXiv:2205.15744v1 [cs.CL])","link":"http://arxiv.org/abs/2205.15744","description":"<p>Massively multilingual sentence representation models, e.g., LASER,\nSBERT-distill, and LaBSE, help significantly improve cross-lingual downstream\ntasks. However, multiple training procedures, the use of a large amount of\ndata, or inefficient model architectures result in heavy computation to train a\nnew model according to our preferred languages and domains. To resolve this\nissue, we introduce efficient and effective massively multilingual sentence\nrepresentation learning (EMS), using cross-lingual sentence reconstruction\n(XTR) and sentence-level contrastive learning as training objectives. Compared\nwith related studies, the proposed model can be efficiently trained using\nsignificantly fewer parallel sentences and GPU computation resources without\ndepending on large-scale pre-trained models. Empirical results show that the\nproposed model significantly yields better or comparable results with regard to\nbi-text mining, zero-shot cross-lingual genre classification, and sentiment\nclassification. Ablative analyses demonstrate the effectiveness of each\ncomponent of the proposed model. We release the codes for model training and\nthe EMS pre-trained model, which supports 62 languages\n(https://github.com/Mao-KU/EMS).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mao_Z/0/1/0/all/0/1\">Zhuoyuan Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_C/0/1/0/all/0/1\">Chenhui Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurohashi_S/0/1/0/all/0/1\">Sadao Kurohashi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GateNLP-UShef at SemEval-2022 Task 8: Entity-Enriched Siamese Transformer for Multilingual News Article Similarity. (arXiv:2205.15812v1 [cs.CL])","link":"http://arxiv.org/abs/2205.15812","description":"<p>This paper describes the second-placed system on the leaderboard of\nSemEval-2022 Task 8: Multilingual News Article Similarity. We propose an\nentity-enriched Siamese Transformer which computes news article similarity\nbased on different sub-dimensions, such as the shared narrative, entities,\nlocation and time of the event discussed in the news article. Our system\nexploits a Siamese network architecture using a Transformer encoder to learn\ndocument-level representations for the purpose of capturing the narrative\ntogether with the auxiliary entity-based features extracted from the news\narticles. The intuition behind using all these features together is to capture\nthe similarity between news articles at different granularity levels and to\nassess the extent to which different news outlets write about \"the same\nevents\". Our experimental results and detailed ablation study demonstrate the\neffectiveness and the validity of our proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_I/0/1/0/all/0/1\">Iknoor Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yue Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thong_M/0/1/0/all/0/1\">Melissa Thong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scarton_C/0/1/0/all/0/1\">Carolina Scarton</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do self-supervised speech models develop human-like perception biases?. (arXiv:2205.15819v1 [cs.CL])","link":"http://arxiv.org/abs/2205.15819","description":"<p>Self-supervised models for speech processing form representational spaces\nwithout using any external labels. Increasingly, they appear to be a feasible\nway of at least partially eliminating costly manual annotations, a problem of\nparticular concern for low-resource languages. But what kind of\nrepresentational spaces do these models construct? Human perception specializes\nto the sounds of listeners' native languages. Does the same thing happen in\nself-supervised models? We examine the representational spaces of three kinds\nof state-of-the-art self-supervised models: wav2vec 2.0, HuBERT and contrastive\npredictive coding (CPC), and compare them with the perceptual spaces of\nFrench-speaking and English-speaking human listeners, both globally and taking\naccount of the behavioural differences between the two language groups. We show\nthat the CPC model shows a small native language effect, but that wav2vec 2.0\nand HuBERT seem to develop a universal speech perception space which is not\nlanguage specific. A comparison against the predictions of supervised phone\nrecognisers suggests that all three self-supervised models capture relatively\nfine-grained perceptual phenomena, while supervised models are better at\ncapturing coarser, phone-level, effects of listeners' native language, on\nperception.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Millet_J/0/1/0/all/0/1\">Juliette Millet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dunbar_E/0/1/0/all/0/1\">Ewan Dunbar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predicting non-native speech perception using the Perceptual Assimilation Model and state-of-the-art acoustic models. (arXiv:2205.15823v1 [cs.CL])","link":"http://arxiv.org/abs/2205.15823","description":"<p>Our native language influences the way we perceive speech sounds, affecting\nour ability to discriminate non-native sounds. We compare two ideas about the\ninfluence of the native language on speech perception: the Perceptual\nAssimilation Model, which appeals to a mental classification of sounds into\nnative phoneme categories, versus the idea that rich, fine-grained phonetic\nrepresentations tuned to the statistics of the native language, are sufficient.\nWe operationalize this idea using representations from two state-of-the-art\nspeech models, a Dirichlet process Gaussian mixture model and the more recent\nwav2vec 2.0 model. We present a new, open dataset of French- and\nEnglish-speaking participants' speech perception behaviour for 61 vowel sounds\nfrom six languages. We show that phoneme assimilation is a better predictor\nthan fine-grained phonetic modelling, both for the discrimination behaviour as\na whole, and for predicting differences in discriminability associated with\ndifferences in native language background. We also show that wav2vec 2.0, while\nnot good at capturing the effects of native language on speech perception, is\ncomplementary to information about native phoneme assimilation, and provides a\ngood model of low-level phonetic representations, supporting the idea that both\ncategorical and fine-grained perception are used during speech perception.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Millet_J/0/1/0/all/0/1\">Juliette Millet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chitoran_I/0/1/0/all/0/1\">Ioana Chitoran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dunbar_E/0/1/0/all/0/1\">Ewan Dunbar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LEXpander: applying colexification networks to automated lexicon expansion. (arXiv:2205.15850v1 [cs.CL])","link":"http://arxiv.org/abs/2205.15850","description":"<p>Recent approaches to text analysis from social media and other corpora rely\non word lists to detect topics, measure meaning, or to select relevant\ndocuments. These lists are often generated by applying computational lexicon\nexpansion methods to small, manually-curated sets of root words. Despite the\nwide use of this approach, we still lack an exhaustive comparative analysis of\nthe performance of lexicon expansion methods and how they can be improved with\nadditional linguistic data. In this work, we present LEXpander, a method for\nlexicon expansion that leverages novel data on colexification, i.e. semantic\nnetworks connecting words based on shared concepts and translations to other\nlanguages. We evaluate LEXpander in a benchmark including widely used methods\nfor lexicon expansion based on various word embedding models and synonym\nnetworks. We find that LEXpander outperforms existing approaches in terms of\nboth precision and the trade-off between precision and recall of generated word\nlists in a variety of tests. Our benchmark includes several linguistic\ncategories and sentiment variables in English and German. We also show that the\nexpanded word lists constitute a high-performing text analysis method in\napplication cases to various corpora. This way, LEXpander poses a systematic\nautomated solution to expand short lists of words into exhaustive and accurate\nword lists that can closely approximate word lists generated by experts in\npsychology and linguistics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Natale_A/0/1/0/all/0/1\">Anna Di Natale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_D/0/1/0/all/0/1\">David Garcia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers. (arXiv:2205.15868v1 [cs.CV])","link":"http://arxiv.org/abs/2205.15868","description":"<p>Large-scale pretrained transformers have created milestones in text (GPT-3)\nand text-to-image (DALL-E and CogView) generation. Its application to video\ngeneration is still facing many challenges: The potential huge computation cost\nmakes the training from scratch unaffordable; The scarcity and weak relevance\nof text-video datasets hinder the model understanding complex movement\nsemantics. In this work, we present 9B-parameter transformer CogVideo, trained\nby inheriting a pretrained text-to-image model, CogView2. We also propose\nmulti-frame-rate hierarchical training strategy to better align text and video\nclips. As (probably) the first open-source large-scale pretrained text-to-video\nmodel, CogVideo outperforms all publicly available models at a large margin in\nmachine and human evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hong_W/0/1/0/all/0/1\">Wenyi Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1\">Ming Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1\">Wendi Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xinghan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jie Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uzbek Sentiment Analysis based on local Restaurant Reviews. (arXiv:2205.15930v1 [cs.CL])","link":"http://arxiv.org/abs/2205.15930","description":"<p>Extracting useful information for sentiment analysis and classification\nproblems from a big amount of user-generated feedback, such as restaurant\nreviews, is a crucial task of natural language processing, which is not only\nfor customer satisfaction where it can give personalized services, but can also\ninfluence the further development of a company. In this paper, we present a\nwork done on collecting restaurant reviews data as a sentiment analysis dataset\nfor the Uzbek language, a member of the Turkic family which is heavily affected\nby the low-resource constraint, and provide some further analysis of the novel\ndataset by evaluation using different techniques, from logistic regression\nbased models, to support vector machines, and even deep learning models, such\nas recurrent neural networks, as well as convolutional neural networks. The\npaper includes detailed information on how the data was collected, how it was\npre-processed for better quality optimization, as well as experimental setups\nfor the evaluation process. The overall evaluation results indicate that by\nperforming pre-processing steps, such as stemming for agglutinative languages,\nthe system yields better results, eventually achieving 91% accuracy result in\nthe best performing model\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Matlatipov_S/0/1/0/all/0/1\">Sanatbek Matlatipov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahimboeva_H/0/1/0/all/0/1\">Hulkar Rahimboeva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajabov_J/0/1/0/all/0/1\">Jaloliddin Rajabov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuriyozov_E/0/1/0/all/0/1\">Elmurod Kuriyozov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hollywood Identity Bias Dataset: A Context Oriented Bias Analysis of Movie Dialogues. (arXiv:2205.15951v1 [cs.CL])","link":"http://arxiv.org/abs/2205.15951","description":"<p>Movies reflect society and also hold power to transform opinions. Social\nbiases and stereotypes present in movies can cause extensive damage due to\ntheir reach. These biases are not always found to be the need of storyline but\ncan creep in as the author's bias. Movie production houses would prefer to\nascertain that the bias present in a script is the story's demand. Today, when\ndeep learning models can give human-level accuracy in multiple tasks, having an\nAI solution to identify the biases present in the script at the writing stage\ncan help them avoid the inconvenience of stalled release, lawsuits, etc. Since\nAI solutions are data intensive and there exists no domain specific data to\naddress the problem of biases in scripts, we introduce a new dataset of movie\nscripts that are annotated for identity bias. The dataset contains dialogue\nturns annotated for (i) bias labels for seven categories, viz., gender,\nrace/ethnicity, religion, age, occupation, LGBTQ, and other, which contains\nbiases like body shaming, personality bias, etc. (ii) labels for sensitivity,\nstereotype, sentiment, emotion, emotion intensity, (iii) all labels annotated\nwith context awareness, (iv) target groups and reason for bias labels and (v)\nexpert-driven group-validation process for high quality annotations. We also\nreport various baseline performances for bias identification and category\ndetection on our dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sandhya Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_P/0/1/0/all/0/1\">Prapti Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahoo_N/0/1/0/all/0/1\">Nihar Sahoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mallela_N/0/1/0/all/0/1\">Niteesh Mallela</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_H/0/1/0/all/0/1\">Himanshu Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharyya_P/0/1/0/all/0/1\">Pushpak Bhattacharyya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savagaonkar_M/0/1/0/all/0/1\">Milind Savagaonkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nidhi/0/1/0/all/0/1\">Nidhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramnani_R/0/1/0/all/0/1\">Roshni Ramnani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maitra_A/0/1/0/all/0/1\">Anutosh Maitra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sengupta_S/0/1/0/all/0/1\">Shubhashis Sengupta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Graph -- Deep Learning: A Case Study in Question Answering in Aviation Safety Domain. (arXiv:2205.15952v1 [cs.CL])","link":"http://arxiv.org/abs/2205.15952","description":"<p>In the commercial aviation domain, there are a large number of documents,\nlike, accident reports (NTSB, ASRS) and regulatory directives (ADs). There is a\nneed for a system to access these diverse repositories efficiently in order to\nservice needs in the aviation industry, like maintenance, compliance, and\nsafety. In this paper, we propose a Knowledge Graph (KG) guided Deep Learning\n(DL) based Question Answering (QA) system for aviation safety. We construct a\nKnowledge Graph from Aircraft Accident reports and contribute this resource to\nthe community of researchers. The efficacy of this resource is tested and\nproved by the aforesaid QA system. Natural Language Queries constructed from\nthe documents mentioned above are converted into SPARQL (the interface language\nof the RDF graph database) queries and answered. On the DL side, we have two\ndifferent QA models: (i) BERT QA which is a pipeline of Passage Retrieval\n(Sentence-BERT based) and Question Answering (BERT based), and (ii) the\nrecently released GPT-3. We evaluate our system on a set of queries created\nfrom the accident reports. Our combined QA system achieves 9.3% increase in\naccuracy over GPT-3 and 40.3% increase over BERT QA. Thus, we infer that KG-DL\nperforms better than either singly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1\">Ankush Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gite_R/0/1/0/all/0/1\">Raj Gite</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laddha_S/0/1/0/all/0/1\">Shreya Laddha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharyya_P/0/1/0/all/0/1\">Pushpak Bhattacharyya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kar_S/0/1/0/all/0/1\">Satyanarayan Kar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ekbal_A/0/1/0/all/0/1\">Asif Ekbal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thind_P/0/1/0/all/0/1\">Prabhjit Thind</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zele_R/0/1/0/all/0/1\">Rajesh Zele</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shankar_R/0/1/0/all/0/1\">Ravi Shankar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NusaX: Multilingual Parallel Sentiment Dataset for 10 Indonesian Local Languages. (arXiv:2205.15960v1 [cs.CL])","link":"http://arxiv.org/abs/2205.15960","description":"<p>Natural language processing (NLP) has a significant impact on society via\ntechnologies such as machine translation and search engines. Despite its\nsuccess, NLP technology is only widely available for high-resource languages\nsuch as English and Chinese, while it remains inaccessible to many languages\ndue to the unavailability of data resources and benchmarks. In this work, we\nfocus on developing resources for languages in Indonesia. Despite being the\nsecond most linguistically diverse country, most languages in Indonesia are\ncategorized as endangered and some are even extinct. We develop the first-ever\nparallel resource for 10 low-resource languages in Indonesia. Our resource\nincludes datasets, a multi-task benchmark, and lexicons, as well as a parallel\nIndonesian-English dataset. We provide extensive analyses and describe the\nchallenges when creating such resources. We hope that our work can spark NLP\nresearch on Indonesian and other underrepresented languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Winata_G/0/1/0/all/0/1\">Genta Indra Winata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aji_A/0/1/0/all/0/1\">Alham Fikri Aji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cahyawijaya_S/0/1/0/all/0/1\">Samuel Cahyawijaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahendra_R/0/1/0/all/0/1\">Rahmad Mahendra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koto_F/0/1/0/all/0/1\">Fajri Koto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romadhony_A/0/1/0/all/0/1\">Ade Romadhony</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurniawan_K/0/1/0/all/0/1\">Kemal Kurniawan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moeljadi_D/0/1/0/all/0/1\">David Moeljadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prasojo_R/0/1/0/all/0/1\">Radityo Eko Prasojo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1\">Pascale Fung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baldwin_T/0/1/0/all/0/1\">Timothy Baldwin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lau_J/0/1/0/all/0/1\">Jey Han Lau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sennrich_R/0/1/0/all/0/1\">Rico Sennrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruder_S/0/1/0/all/0/1\">Sebastian Ruder</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cluster-based Evaluation of Automatically Generated Text. (arXiv:2205.16001v1 [cs.CL])","link":"http://arxiv.org/abs/2205.16001","description":"<p>While probabilistic language generators have improved dramatically over the\nlast few years, the automatic evaluation metrics used to assess them have not\nkept pace with this progress. In the domain of language generation, a good\nmetric must correlate highly with human judgements. Yet, with few exceptions,\nthere is a lack of such metrics in the literature. In this work, we analyse the\ngeneral paradigm of language generator evaluation. We first discuss the\ncomputational and qualitative issues with using automatic evaluation metrics\nthat operate on probability distributions over strings, the backbone of most\nlanguage generators. We then propose the use of distributions over clusters\ninstead, where we cluster strings based on their text embeddings (obtained from\na pretrained language model). While we find the biases introduced by this\nsubstitution to be quite strong, we observe that, empirically, this methodology\nleads to metric estimators with higher correlation with human judgements, while\nsimultaneously reducing estimator variance. We finish the paper with a probing\nanalysis, which leads us to conclude that -- by encoding syntactic- and\ncoherence-level features of text, while ignoring surface-level features --\nthese clusters may simply be better equipped to evaluate state-of-the-art\nlanguage models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pimentel_T/0/1/0/all/0/1\">Tiago Pimentel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meister_C/0/1/0/all/0/1\">Clara Meister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Retriever and Go Beyond: A Thesis Proposal. (arXiv:2205.16005v1 [cs.CL])","link":"http://arxiv.org/abs/2205.16005","description":"<p>Information Retriever (IR) aims to find the relevant documents (e.g.\nsnippets, passages, and articles) to a given query at large scale. IR plays an\nimportant role in many tasks such as open domain question answering and\ndialogue systems, where external knowledge is needed. In the past, searching\nalgorithms based on term matching have been widely used. Recently, neural-based\nalgorithms (termed as neural retrievers) have gained more attention which can\nmitigate the limitations of traditional methods. Regardless of the success\nachieved by neural retrievers, they still face many challenges, e.g. suffering\nfrom a small amount of training data and failing to answer simple\nentity-centric questions. Furthermore, most of the existing neural retrievers\nare developed for pure-text query. This prevents them from handling\nmulti-modality queries (i.e. the query is composed of textual description and\nimages). This proposal has two goals. First, we introduce methods to address\nthe abovementioned issues of neural retrievers from three angles, new model\narchitectures, IR-oriented pretraining tasks, and generating large scale\ntraining data. Second, we identify the future research direction and propose\npotential corresponding solution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_M/0/1/0/all/0/1\">Man Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Topic Model via Optimal Transport. (arXiv:2008.13537v3 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2008.13537","description":"<p>Recently, Neural Topic Models (NTMs) inspired by variational autoencoders\nhave obtained increasingly research interest due to their promising results on\ntext analysis. However, it is usually hard for existing NTMs to achieve good\ndocument representation and coherent/diverse topics at the same time. Moreover,\nthey often degrade their performance severely on short documents. The\nrequirement of reparameterisation could also comprise their training quality\nand model flexibility. To address these shortcomings, we present a new neural\ntopic model via the theory of optimal transport (OT). Specifically, we propose\nto learn the topic distribution of a document by directly minimising its OT\ndistance to the document's word distributions. Importantly, the cost matrix of\nthe OT distance models the weights between topics and words, which is\nconstructed by the distances between topics and words in an embedding space.\nOur proposed model can be trained efficiently with a differentiable loss.\nExtensive experiments show that our framework significantly outperforms the\nstate-of-the-art NTMs on discovering more coherent and diverse topics and\nderiving better document representations for both regular and short texts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">He Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phung_D/0/1/0/all/0/1\">Dinh Phung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huynh_V/0/1/0/all/0/1\">Viet Huynh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1\">Trung Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buntine_W/0/1/0/all/0/1\">Wray Buntine</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Compositional Generalization in Multilingual Semantic Parsing over Wikidata. (arXiv:2108.03509v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.03509","description":"<p>Semantic parsing (SP) allows humans to leverage vast knowledge resources\nthrough natural interaction. However, parsers are mostly designed for and\nevaluated on English resources, such as CFQ (Keysers et al., 2020), the current\nstandard benchmark based on English data generated from grammar rules and\noriented towards Freebase, an outdated knowledge base. We propose a method for\ncreating a multilingual, parallel dataset of question-query pairs, grounded in\nWikidata. We introduce such a dataset, which we call Multilingual Compositional\nWikidata Questions (MCWQ), and use it to analyze the compositional\ngeneralization of semantic parsers in Hebrew, Kannada, Chinese and English.\nWhile within-language generalization is comparable across languages,\nexperiments on zero-shot cross-lingual transfer demonstrate that cross-lingual\ncompositional generalization fails, even with state-of-the-art pretrained\nmultilingual encoders. Furthermore, our methodology, dataset and results will\nfacilitate future research on SP in more realistic and diverse settings than\nhas been possible with existing resources.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cui_R/0/1/0/all/0/1\">Ruixiang Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aralikatte_R/0/1/0/all/0/1\">Rahul Aralikatte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lent_H/0/1/0/all/0/1\">Heather Lent</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hershcovich_D/0/1/0/all/0/1\">Daniel Hershcovich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Ground Visual Objects for Visual Dialog. (arXiv:2109.06013v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.06013","description":"<p>Visual dialog is challenging since it needs to answer a series of coherent\nquestions based on understanding the visual environment. How to ground related\nvisual objects is one of the key problems. Previous studies utilize the\nquestion and history to attend to the image and achieve satisfactory\nperformance, however these methods are not sufficient to locate related visual\nobjects without any guidance. The inappropriate grounding of visual objects\nprohibits the performance of visual dialog models. In this paper, we propose a\nnovel approach to Learn to Ground visual objects for visual dialog, which\nemploys a novel visual objects grounding mechanism where both prior and\nposterior distributions over visual objects are used to facilitate visual\nobjects grounding. Specifically, a posterior distribution over visual objects\nis inferred from both context (history and questions) and answers, and it\nensures the appropriate grounding of visual objects during the training\nprocess. Meanwhile, a prior distribution, which is inferred from context only,\nis used to approximate the posterior distribution so that appropriate visual\nobjects can be grounded even without answers during the inference process.\nExperimental results on the VisDial v0.9 and v1.0 datasets demonstrate that our\napproach improves the previous strong models in both generative and\ndiscriminative settings by a significant margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Feilong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiuyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Can Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SPARQLing Database Queries from Intermediate Question Decompositions. (arXiv:2109.06162v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.06162","description":"<p>To translate natural language questions into executable database queries,\nmost approaches rely on a fully annotated training set. Annotating a large\ndataset with queries is difficult as it requires query-language expertise. We\nreduce this burden using grounded in databases intermediate question\nrepresentations. These representations are simpler to collect and were\noriginally crowdsourced within the Break dataset (Wolfson et al., 2020). Our\npipeline consists of two parts: a neural semantic parser that converts natural\nlanguage questions into the intermediate representations and a non-trainable\ntranspiler to the SPARQL query language (a standard language for accessing\nknowledge graphs and semantic web). We chose SPARQL because its queries are\nstructurally closer to our intermediate representations (compared to SQL). We\nobserve that the execution accuracy of queries constructed by our model on the\nchallenging Spider dataset is comparable with the state-of-the-art text-to-SQL\nmethods trained with annotated SQL queries. Our code and data are publicly\navailable (see https://github.com/yandex-research/sparqling-queries).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saparina_I/0/1/0/all/0/1\">Irina Saparina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osokin_A/0/1/0/all/0/1\">Anton Osokin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GoG: Relation-aware Graph-over-Graph Network for Visual Dialog. (arXiv:2109.08475v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.08475","description":"<p>Visual dialog, which aims to hold a meaningful conversation with humans about\na given image, is a challenging task that requires models to reason the complex\ndependencies among visual content, dialog history, and current questions. Graph\nneural networks are recently applied to model the implicit relations between\nobjects in an image or dialog. However, they neglect the importance of 1)\ncoreference relations among dialog history and dependency relations between\nwords for the question representation; and 2) the representation of the image\nbased on the fully represented question. Therefore, we propose a novel\nrelation-aware graph-over-graph network (GoG) for visual dialog. Specifically,\nGoG consists of three sequential graphs: 1) H-Graph, which aims to capture\ncoreference relations among dialog history; 2) History-aware Q-Graph, which\naims to fully understand the question through capturing dependency relations\nbetween words based on coreference resolution on the dialog history; and 3)\nQuestion-aware I-Graph, which aims to capture the relations between objects in\nan image based on fully question representation. As an additional feature\nrepresentation module, we add GoG to the existing visual dialogue model.\nExperimental results show that our model outperforms the strong baseline in\nboth generative and discriminative settings by a significant margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Feilong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiuyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MixQG: Neural Question Generation with Mixed Answer Types. (arXiv:2110.08175v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.08175","description":"<p>Asking good questions is an essential ability for both human and machine\nintelligence. However, existing neural question generation approaches mainly\nfocus on the short factoid type of answers. In this paper, we propose a neural\nquestion generator, MixQG, to bridge this gap. We combine 9 question answering\ndatasets with diverse answer types, including yes/no, multiple-choice,\nextractive, and abstractive answers, to train a single generative model. We\nshow with empirical results that our model outperforms existing work in both\nseen and unseen domains and can generate questions with different cognitive\nlevels when conditioned on different answer types. Our code is released and\nwell-integrated with the Huggingface library to facilitate various downstream\napplications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Murakhovska_L/0/1/0/all/0/1\">Lidiya Murakhovs&#x27;ka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chien-Sheng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laban_P/0/1/0/all/0/1\">Philippe Laban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_T/0/1/0/all/0/1\">Tong Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenhao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BitextEdit: Automatic Bitext Editing for Improved Low-Resource Machine Translation. (arXiv:2111.06787v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.06787","description":"<p>Mined bitexts can contain imperfect translations that yield unreliable\ntraining signals for Neural Machine Translation (NMT). While filtering such\npairs out is known to improve final model quality, we argue that it is\nsuboptimal in low-resource conditions where even mined data can be limited. In\nour work, we propose instead, to refine the mined bitexts via automatic\nediting: given a sentence in a language xf, and a possibly imperfect\ntranslation of it xe, our model generates a revised version xf' or xe' that\nyields a more equivalent translation pair (i.e., &lt;xf, xe'&gt; or &lt;xf', xe&gt;). We\nuse a simple editing strategy by (1) mining potentially imperfect translations\nfor each sentence in a given bitext, (2) learning a model to reconstruct the\noriginal translations and translate, in a multi-task fashion. Experiments\ndemonstrate that our approach successfully improves the quality of CCMatrix\nmined bitext for 5 low-resource language-pairs and 10 translation directions by\nup to ~ 8 BLEU points, in most cases improving upon a competitive\nback-translation baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Briakou_E/0/1/0/all/0/1\">Eleftheria Briakou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sida I. Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghazvininejad_M/0/1/0/all/0/1\">Marjan Ghazvininejad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Making sense of electrical vehicle discussions using sentiment analysis on closely related news and user comments. (arXiv:2112.12327v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.12327","description":"<p>We used a token-wise and document-wise sentiment analysis using both\nunsupervised and supervised models applied to both news and user reviews\ndataset. And our token-wise sentiment analysis found a statistically\nsignificant difference in sentiment between the two groups (both of which were\nvery large N), our document-wise supervised sentiment analysis found no\nsignificant difference in sentiment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xuan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Everts_J/0/1/0/all/0/1\">Josh Everts</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Model-Agnostic Multitask Fine-tuning for Few-shot Vision-Language Transfer Learning. (arXiv:2203.04904v2 [cs.MM] UPDATED)","link":"http://arxiv.org/abs/2203.04904","description":"<p>Despite achieving state-of-the-art zero-shot performance, existing\nvision-language models, e.g., CLIP, still fall short of domain-specific\nclassification tasks, e.g., Fungi Classification. In the context of few-shot\ntransfer learning, traditional fine-tuning fails to prevent highly expressive\nmodel from exploiting spurious correlations in the training data. On the other\nhand, although model-agnostic meta-learning (MAML) presents as a natural\nalternative for transfer learning, the expensive computation due to implicit\nsecond-order optimization limits its use in large-scale models and datasets. In\nthis work we aim to further improve the generalization of existing\nvision-language models on unseen tasks via a simple yet efficient fine-tuning\nstrategy based on uniform task sampling. We term our method as Model-Agnostic\nMultitask Fine-tuning (MAMF). Compared with MAML, MAMF discards the bi-level\noptimization and uses only first-order gradients, which makes it easily\nscalable and computationally efficient. Due to the uniform task sampling\nprocedure, MAMF consistently outperforms the classical fine-tuning method for\nfew-shot transfer learning on five benchmark datasets. Empirically, we further\ndiscover that the effectiveness of first-order MAML is highly dependent on the\nzero-shot performance of the pretrained model, and our simple algorithm can\noutperform first-order MAML on more challenging datasets with low zero-shot\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhenhailong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Manling Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Han Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revise and Resubmit: An Intertextual Model of Text-based Collaboration in Peer Review. (arXiv:2204.10805v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.10805","description":"<p>Peer review is a key component of the publishing process in most fields of\nscience. The increasing submission rates put a strain on reviewing quality and\nefficiency, motivating the development of applications to support the reviewing\nand editorial work. While existing NLP studies focus on the analysis of\nindividual texts, editorial assistance often requires modeling interactions\nbetween pairs of texts -- yet general frameworks and datasets to support this\nscenario are missing. Relationships between texts are the core object of the\nintertextuality theory -- a family of approaches in literary studies not yet\noperationalized in NLP. Inspired by prior theoretical work, we propose the\nfirst intertextual model of text-based collaboration, which encompasses three\nmajor phenomena that make up a full iteration of the review-revise-and-resubmit\ncycle: pragmatic tagging, linking and long-document version alignment. While\npeer review is used across the fields of science and publication formats,\nexisting datasets solely focus on conference-style review in computer science.\nAddressing this, we instantiate our proposed model in the first annotated\nmulti-domain corpus in journal-style post-publication open peer review, and\nprovide detailed insights into the practical aspects of intertextual\nannotation. Our resource is a major step towards multi-domain, fine-grained\napplications of NLP in editorial support for peer review, and our intertextual\nframework paves the path for general-purpose modeling of text-based\ncollaboration. Our corpus and accompanying code are publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kuznetsov_I/0/1/0/all/0/1\">Ilia Kuznetsov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buchmann_J/0/1/0/all/0/1\">Jan Buchmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eichler_M/0/1/0/all/0/1\">Max Eichler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Models Can See: Plugging Visual Controls in Text Generation. (arXiv:2205.02655v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.02655","description":"<p>Generative language models (LMs) such as GPT-2/3 can be prompted to generate\ntext with remarkable quality. While they are designed for text-prompted\ngeneration, it remains an open question how the generation process could be\nguided by modalities beyond text such as images. In this work, we propose a\ntraining-free framework, called MAGIC (iMAge-Guided text generatIon with CLIP),\nfor plugging in visual controls in the generation process and enabling LMs to\nperform multimodal tasks (e.g., image captioning) in a zero-shot manner. MAGIC\nis a simple yet efficient plug-and-play framework, which directly combines an\noff-the-shelf LM (i.e., GPT-2) and an image-text matching model (i.e., CLIP)\nfor image-grounded text generation. During decoding, MAGIC influences the\ngeneration of the LM by introducing a CLIP-induced score, called magic score,\nwhich regularizes the generated result to be semantically related to a given\nimage while being coherent to the previously generated context. Notably, the\nproposed decoding scheme does not involve any gradient update operation,\ntherefore being computationally efficient. On the challenging task of zero-shot\nimage captioning, MAGIC outperforms the state-of-the-art method by notable\nmargins with a nearly 27 times decoding speedup. MAGIC is a flexible framework\nand is theoretically compatible with any text generation tasks that incorporate\nimage grounding. In the experiments, we showcase that it is also capable of\nperforming visually grounded story generation given both an image and a text\nprompt.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yixuan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_T/0/1/0/all/0/1\">Tian Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yahui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fangyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yogatama_D/0/1/0/all/0/1\">Dani Yogatama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1\">Lingpeng Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collier_N/0/1/0/all/0/1\">Nigel Collier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Use of Transformer-Based Models for Word-Level Transliteration of the Book of the Dean of Lismore. (arXiv:2205.11370v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.11370","description":"<p>The Book of the Dean of Lismore (BDL) is a 16th-century Scottish Gaelic\nmanuscript written in a non-standard orthography. In this work, we outline the\nproblem of transliterating the text of the BDL into a standardised orthography,\nand perform exploratory experiments using Transformer-based models for this\ntask. In particular, we focus on the task of word-level transliteration, and\nachieve a character-level BLEU score of 54.15 with our best model, a BART\narchitecture pre-trained on the text of Scottish Gaelic Wikipedia and then\nfine-tuned on around 2,000 word-level parallel examples. Our initial\nexperiments give promising results, but we highlight the shortcomings of our\nmodel, and discuss directions for future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gow_Smith_E/0/1/0/all/0/1\">Edward Gow-Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McConville_M/0/1/0/all/0/1\">Mark McConville</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gillies_W/0/1/0/all/0/1\">William Gillies</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scott_J/0/1/0/all/0/1\">Jade Scott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maolalaigh_R/0/1/0/all/0/1\">Roibeard &#xd3; Maolalaigh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HiVLP: Hierarchical Vision-Language Pre-Training for Fast Image-Text Retrieval. (arXiv:2205.12105v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.12105","description":"<p>In the past few years, the emergence of vision-language pre-training (VLP)\nhas brought cross-modal retrieval to a new era. However, due to the latency and\ncomputation demand, it is commonly challenging to apply VLP in a real-time\nonline retrieval system. To alleviate the defect, this paper proposes a\n\\textbf{Hi}erarchical \\textbf{V}ision-\\textbf{}Language \\textbf{P}re-Training\n(\\textbf{HiVLP}) for fast Image-Text Retrieval (ITR). Specifically, we design a\nnovel hierarchical retrieval objective, which uses the representation of\ndifferent dimensions for coarse-to-fine ITR, i.e., using low-dimensional\nrepresentation for large-scale coarse retrieval and high-dimensional\nrepresentation for small-scale fine retrieval. We evaluate our proposed HiVLP\non two popular image-text retrieval benchmarks, i.e., Flickr30k and COCO.\nExtensive experiments demonstrate that our HiVLP not only has fast inference\nspeed but also can be easily scaled to large-scale ITR scenarios. The detailed\nresults show that HiVLP is $1,427$$\\sim$$120,649\\times$ faster than the\nfusion-based model UNITER and 2$\\sim$5 faster than the fastest embedding-based\nmodel LightingDot in different candidate scenarios. It also achieves about +4.9\nAR on COCO and +3.8 AR on Flickr30K than LightingDot and achieves comparable\nperformance with the state-of-the-art (SOTA) fusion-based model METER.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Feilong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiuyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jiaxin Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Duzhen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1\">Jianlong Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What Dense Graph Do You Need for Self-Attention?. (arXiv:2205.14014v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2205.14014","description":"<p>Transformers have made progress in miscellaneous tasks, but suffer from\nquadratic computational and memory complexities. Recent works propose sparse\nTransformers with attention on sparse graphs to reduce complexity and remain\nstrong performance. While effective, the crucial parts of how dense a graph\nneeds to be to perform well are not fully explored. In this paper, we propose\nNormalized Information Payload (NIP), a graph scoring function measuring\ninformation transfer on graph, which provides an analysis tool for trade-offs\nbetween performance and complexity. Guided by this theoretical analysis, we\npresent Hypercube Transformer, a sparse Transformer that models token\ninteractions in a hypercube and shows comparable or even better results with\nvanilla Transformer while yielding $O(N\\log N)$ complexity with sequence length\n$N$. Experiments on tasks requiring various sequence lengths lay validation for\nour graph function well.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuxing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chu-Tak Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1\">Qipeng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1\">Zhangyue Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yunhua Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"L3Cube-MahaNLP: Marathi Natural Language Processing Datasets, Models, and Library. (arXiv:2205.14728v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.14728","description":"<p>Despite being the third most popular language in India, the Marathi language\nlacks useful NLP resources. Moreover, popular NLP libraries do not have support\nfor the Marathi language. With L3Cube-MahaNLP, we aim to build resources and a\nlibrary for Marathi natural language processing. We present datasets and\ntransformer models for supervised tasks like sentiment analysis, named entity\nrecognition, and hate speech detection. We have also published a monolingual\nMarathi corpus for unsupervised language modeling tasks. Overall we present\nMahaCorpus, MahaSent, MahaNER, and MahaHate datasets and their corresponding\nMahaBERT models fine-tuned on these datasets. We aim to move ahead of benchmark\ndatasets and prepare useful resources for Marathi. The resources are available\nat https://github.com/l3cube-pune/MarathiNLP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Joshi_R/0/1/0/all/0/1\">Raviraj Joshi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Duplex Conversation: Towards Human-like Interaction in Spoken Dialogue System. (arXiv:2205.15060v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.15060","description":"<p>In this paper, we present Duplex Conversation, a multi-turn, multimodal\nspoken dialogue system that enables telephone-based agents to interact with\ncustomers like a human. We use the concept of full-duplex in telecommunication\nto demonstrate what a human-like interactive experience should be and how to\nachieve smooth turn-taking through three subtasks: user state detection,\nbackchannel selection, and barge-in detection. Besides, we propose\nsemi-supervised learning with multimodal data augmentation to leverage\nunlabeled data to increase model generalization. Experimental results on three\nsub-tasks show that the proposed method achieves consistent improvements\ncompared with baselines. We deploy the Duplex Conversation to Alibaba\nintelligent customer service and share lessons learned in production. Online\nA/B experiments show that the proposed system can significantly reduce response\nlatency by 50%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Ting-En Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuchuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yongbin Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-05-31T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Parameter-Efficient and Student-Friendly Knowledge Distillation. (arXiv:2205.15308v1 [cs.LG])","link":"http://arxiv.org/abs/2205.15308","description":"<p>Knowledge distillation (KD) has been extensively employed to transfer the\nknowledge from a large teacher model to the smaller students, where the\nparameters of the teacher are fixed (or partially) during training. Recent\nstudies show that this mode may cause difficulties in knowledge transfer due to\nthe mismatched model capacities. To alleviate the mismatch problem,\nteacher-student joint training methods, e.g., online distillation, have been\nproposed, but it always requires expensive computational cost. In this paper,\nwe present a parameter-efficient and student-friendly knowledge distillation\nmethod, namely PESF-KD, to achieve efficient and sufficient knowledge transfer\nby updating relatively few partial parameters. Technically, we first\nmathematically formulate the mismatch as the sharpness gap between their\npredictive distributions, where we show such a gap can be narrowed with the\nappropriate smoothness of the soft label. Then, we introduce an adapter module\nfor the teacher and only update the adapter to obtain soft labels with\nappropriate smoothness. Experiments on a variety of benchmarks show that\nPESF-KD can significantly reduce the training cost while obtaining competitive\nresults compared to advanced online distillation methods. Code will be released\nupon acceptance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rao_J/0/1/0/all/0/1\">Jun Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1\">Xv Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1\">Liang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_S/0/1/0/all/0/1\">Shuhan Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Searching for the Essence of Adversarial Perturbations. (arXiv:2205.15357v1 [cs.LG])","link":"http://arxiv.org/abs/2205.15357","description":"<p>Neural networks have achieved the state-of-the-art performance on various\nmachine learning fields, yet the incorporation of malicious perturbations with\ninput data (adversarial example) is able to fool neural networks' predictions.\nThis would lead to potential risks in real-world applications, for example,\nauto piloting and facial recognition. However, the reason for the existence of\nadversarial examples remains controversial. Here we demonstrate that\nadversarial perturbations contain human-recognizable information, which is the\nkey conspirator responsible for a neural network's erroneous prediction. This\nconcept of human-recognizable information allows us to explain key features\nrelated to adversarial perturbations, which include the existence of\nadversarial examples, the transferability among different neural networks, and\nthe increased neural network interpretability for adversarial training. Two\nunique properties in adversarial perturbations that fool neural networks are\nuncovered: masking and generation. A special class, the complementary class, is\nidentified when neural networks classify input images. The human-recognizable\ninformation contained in adversarial perturbations allows researchers to gain\ninsight on the working principles of neural networks and may lead to develop\ntechniques that detect/defense adversarial attacks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Menn_D/0/1/0/all/0/1\">Dennis Y. Menn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting Audio Pattern Recognition for Asthma Medication Adherence: Evaluation with the RDA Benchmark Suite. (arXiv:2205.15360v1 [cs.SD])","link":"http://arxiv.org/abs/2205.15360","description":"<p>Asthma is a common, usually long-term respiratory disease with negative\nimpact on society and the economy worldwide. Treatment involves using medical\ndevices (inhalers) that distribute medication to the airways, and its\nefficiency depends on the precision of the inhalation technique. Health\nmonitoring systems equipped with sensors and embedded with sound signal\ndetection enable the recognition of drug actuation and could be powerful tools\nfor reliable audio content analysis. This paper revisits audio pattern\nrecognition and machine learning techniques for asthma medication adherence\nassessment and presents the Respiratory and Drug Actuation (RDA)\nSuite(https://gitlab.com/vvr/monitoring-medication-adherence/rda-benchmark) for\nbenchmarking and further research. The RDA Suite includes a set of tools for\naudio processing, feature extraction and classification and is provided along\nwith a dataset consisting of respiratory and drug actuation sounds. The\nclassification models in RDA are implemented based on conventional and advanced\nmachine learning and deep network architectures. This study provides a\ncomparative evaluation of the implemented approaches, examines potential\nimprovements and discusses challenges and future tendencies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fakotakis_N/0/1/0/all/0/1\">Nikos D. Fakotakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nousias_S/0/1/0/all/0/1\">Stavros Nousias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arvanitis_G/0/1/0/all/0/1\">Gerasimos Arvanitis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zacharaki_E/0/1/0/all/0/1\">Evangelia I. Zacharaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moustakas_K/0/1/0/all/0/1\">Konstantinos Moustakas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TubeFormer-DeepLab: Video Mask Transformer. (arXiv:2205.15361v1 [cs.CV])","link":"http://arxiv.org/abs/2205.15361","description":"<p>We present TubeFormer-DeepLab, the first attempt to tackle multiple core\nvideo segmentation tasks in a unified manner. Different video segmentation\ntasks (e.g., video semantic/instance/panoptic segmentation) are usually\nconsidered as distinct problems. State-of-the-art models adopted in the\nseparate communities have diverged, and radically different approaches dominate\nin each task. By contrast, we make a crucial observation that video\nsegmentation tasks could be generally formulated as the problem of assigning\ndifferent predicted labels to video tubes (where a tube is obtained by linking\nsegmentation masks along the time axis) and the labels may encode different\nvalues depending on the target task. The observation motivates us to develop\nTubeFormer-DeepLab, a simple and effective video mask transformer model that is\nwidely applicable to multiple video segmentation tasks. TubeFormer-DeepLab\ndirectly predicts video tubes with task-specific labels (either pure semantic\ncategories, or both semantic categories and instance identities), which not\nonly significantly simplifies video segmentation models, but also advances\nstate-of-the-art results on multiple video segmentation benchmarks\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Dahun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jun Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huiyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_S/0/1/0/all/0/1\">Siyuan Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1\">Qihang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hong-Seok Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adam_H/0/1/0/all/0/1\">Hartwig Adam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kweon_I/0/1/0/all/0/1\">In So Kweon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liang-Chieh Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dictionary Learning with Accumulator Neurons. (arXiv:2205.15386v1 [cs.CV])","link":"http://arxiv.org/abs/2205.15386","description":"<p>The Locally Competitive Algorithm (LCA) uses local competition between\nnon-spiking leaky integrator neurons to infer sparse representations, allowing\nfor potentially real-time execution on massively parallel neuromorphic\narchitectures such as Intel's Loihi processor. Here, we focus on the problem of\ninferring sparse representations from streaming video using dictionaries of\nspatiotemporal features optimized in an unsupervised manner for sparse\nreconstruction. Non-spiking LCA has previously been used to achieve\nunsupervised learning of spatiotemporal dictionaries composed of convolutional\nkernels from raw, unlabeled video. We demonstrate how unsupervised dictionary\nlearning with spiking LCA (\\hbox{S-LCA}) can be efficiently implemented using\naccumulator neurons, which combine a conventional leaky-integrate-and-fire\n(\\hbox{LIF}) spike generator with an additional state variable that is used to\nminimize the difference between the integrated input and the spiking output. We\ndemonstrate dictionary learning across a wide range of dynamical regimes, from\ngraded to intermittent spiking, for inferring sparse representations of both\nstatic images drawn from the CIFAR database as well as video frames captured\nfrom a DVS camera. On a classification task that requires identification of the\nsuite from a deck of cards being rapidly flipped through as viewed by a DVS\ncamera, we find essentially no degradation in performance as the LCA model used\nto infer sparse spatiotemporal representations migrates from graded to spiking.\nWe conclude that accumulator neurons are likely to provide a powerful enabling\ncomponent of future neuromorphic hardware for implementing online unsupervised\nlearning of spatiotemporal dictionaries optimized for sparse reconstruction of\nstreaming video from event based DVS cameras.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Parpart_G/0/1/0/all/0/1\">Gavin Parpart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_C/0/1/0/all/0/1\">Carlos Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stewart_T/0/1/0/all/0/1\">Terrence C. Stewart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_E/0/1/0/all/0/1\">Edward Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rego_J/0/1/0/all/0/1\">Jocelyn Rego</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OBrien_A/0/1/0/all/0/1\">Andrew O&#x27;Brien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nesbit_S/0/1/0/all/0/1\">Steven Nesbit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kenyon_G/0/1/0/all/0/1\">Garrett T. Kenyon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watkins_Y/0/1/0/all/0/1\">Yijing Watkins</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VoGE: A Differentiable Volume Renderer using Gaussian Ellipsoids for Analysis-by-Synthesis. (arXiv:2205.15401v1 [cs.GR])","link":"http://arxiv.org/abs/2205.15401","description":"<p>Differentiable rendering allows the application of computer graphics on\nvision tasks, e.g. object pose and shape fitting, via analysis-by-synthesis,\nwhere gradients at occluded regions are important when inverting the rendering\nprocess. To obtain those gradients, state-of-the-art (SoTA) differentiable\nrenderers use rasterization to collect a set of nearest components for each\npixel and aggregate them based on the viewing distance. In this paper, we\npropose VoGE, which uses ray tracing to capture nearest components with their\nvolume density distributions on the rays and aggregates via integral of the\nvolume densities based on Gaussian ellipsoids, which brings more efficient and\nstable gradients. To efficiently render via VoGE, we propose an approximate\nclose-form solution for the volume density aggregation and a coarse-to-fine\nrendering strategy. Finally, we provide a CUDA implementation of VoGE, which\ngives a competitive rendering speed in comparison to PyTorch3D. Quantitative\nand qualitative experiment results show VoGE outperforms SoTA counterparts when\napplied to various vision tasks,e.g., object pose estimation, shape/texture\nfitting, and occlusion reasoning. The VoGE library and demos are available at\nhttps://github.com/Angtian/VoGE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1\">Angtian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kortylewski_A/0/1/0/all/0/1\">Adam Kortylewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1\">Alan Yuille</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gator: Customizable Channel Pruning of Neural Networks with Gating. (arXiv:2205.15404v1 [cs.CV])","link":"http://arxiv.org/abs/2205.15404","description":"<p>The rise of neural network (NN) applications has prompted an increased\ninterest in compression, with a particular focus on channel pruning, which does\nnot require any additional hardware. Most pruning methods employ either\nsingle-layer operations or global schemes to determine which channels to remove\nfollowed by fine-tuning of the network. In this paper we present Gator, a\nchannel-pruning method which temporarily adds learned gating mechanisms for\npruning of individual channels, and which is trained with an additional\nauxiliary loss, aimed at reducing the computational cost due to memory,\n(theoretical) speedup (in terms of FLOPs), and practical, hardware-specific\nspeedup. Gator introduces a new formulation of dependencies between NN layers\nwhich, in contrast to most previous methods, enables pruning of non-sequential\nparts, such as layers on ResNet's highway, and even removing entire ResNet\nblocks. Gator's pruning for ResNet-50 trained on ImageNet produces\nstate-of-the-art (SOTA) results, such as 50% FLOPs reduction with only\n0.4%-drop in top-5 accuracy. Also, Gator outperforms previous pruning models,\nin terms of GPU latency by running 1.4 times faster. Furthermore, Gator\nachieves improved top-5 accuracy results, compared to MobileNetV2 and\nSqueezeNet, for similar runtimes. The source code of this work is available at:\nhttps://github.com/EliPassov/gator.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Passov_E/0/1/0/all/0/1\">Eli Passov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+David_E/0/1/0/all/0/1\">Eli O. David</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Netanyahu_N/0/1/0/all/0/1\">Nathan S. Netanyahu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Grid HTM: Hierarchical Temporal Memory for Anomaly Detection in Videos. (arXiv:2205.15407v1 [cs.CV])","link":"http://arxiv.org/abs/2205.15407","description":"<p>The interest for video anomaly detection systems has gained traction for the\npast few years. The current approaches use deep learning to perform anomaly\ndetection in videos, but this approach has multiple problems. For starters,\ndeep learning in general has issues with noise, concept drift, explainability,\nand training data volumes. Additionally, anomaly detection in itself is a\ncomplex task and faces challenges such as unknowness, heterogeneity, and class\nimbalance. Anomaly detection using deep learning is therefore mainly\nconstrained to generative models such as generative adversarial networks and\nautoencoders due to their unsupervised nature, but even they suffer from\ngeneral deep learning issues and are hard to train properly. In this paper, we\nexplore the capabilities of the Hierarchical Temporal Memory (HTM) algorithm to\nperform anomaly detection in videos, as it has favorable properties such as\nnoise tolerance and online learning which combats concept drift. We introduce a\nnovel version of HTM, namely, Grid HTM, which is an HTM-based architecture\nspecifically for anomaly detection in complex videos such as surveillance\nfootage.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Monakhov_V/0/1/0/all/0/1\">Vladimir Monakhov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thambawita_V/0/1/0/all/0/1\">Vajira Thambawita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Halvorsen_P/0/1/0/all/0/1\">P&#xe5;l Halvorsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riegler_M/0/1/0/all/0/1\">Michael A. Riegler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LiDAR-aid Inertial Poser: Large-scale Human Motion Capture by Sparse Inertial and LiDAR Sensors. (arXiv:2205.15410v1 [cs.CV])","link":"http://arxiv.org/abs/2205.15410","description":"<p>We propose a multi-sensor fusion method for capturing challenging 3D human\nmotions with accurate consecutive local poses and global trajectories in\nlarge-scale scenarios, only using a single LiDAR and 4 IMUs. Specifically, to\nfully utilize the global geometry information captured by LiDAR and local\ndynamic motions captured by IMUs, we design a two-stage pose estimator in a\ncoarse-to-fine manner, where point clouds provide the coarse body shape and IMU\nmeasurements optimize the local actions. Furthermore, considering the\ntranslation deviation caused by the view-dependent partial point cloud, we\npropose a pose-guided translation corrector. It predicts the offset between\ncaptured points and the real root locations, which makes the consecutive\nmovements and trajectories more precise and natural. Extensive quantitative and\nqualitative experiments demonstrate the capability of our approach for\ncompelling motion capture in large-scale scenarios, which outperforms other\nmethods by an obvious margin. We will release our code and captured dataset to\nstimulate future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chengfeng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Yiming Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yannan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cong_P/0/1/0/all/0/1\">Peishan Cong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1\">Han Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jingyi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yuexin Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PolypConnect: Image inpainting for generating realistic gastrointestinal tract images with polyps. (arXiv:2205.15413v1 [eess.IV])","link":"http://arxiv.org/abs/2205.15413","description":"<p>Early identification of a polyp in the lower gastrointestinal (GI) tract can\nlead to prevention of life-threatening colorectal cancer. Developing\ncomputer-aided diagnosis (CAD) systems to detect polyps can improve detection\naccuracy and efficiency and save the time of the domain experts called\nendoscopists. Lack of annotated data is a common challenge when building CAD\nsystems. Generating synthetic medical data is an active research area to\novercome the problem of having relatively few true positive cases in the\nmedical domain. To be able to efficiently train machine learning (ML) models,\nwhich are the core of CAD systems, a considerable amount of data should be\nused. In this respect, we propose the PolypConnect pipeline, which can convert\nnon-polyp images into polyp images to increase the size of training datasets\nfor training. We present the whole pipeline with quantitative and qualitative\nevaluations involving endoscopists. The polyp segmentation model trained using\nsynthetic data, and real data shows a 5.1% improvement of mean intersection\nover union (mIOU), compared to the model trained only using real data. The\ncodes of all the experiments are available on GitHub to reproduce the results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Fagereng_J/0/1/0/all/0/1\">Jan Andre Fagereng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Thambawita_V/0/1/0/all/0/1\">Vajira Thambawita</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Storaas_A/0/1/0/all/0/1\">Andrea M. Stor&#xe5;s</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Parasa_S/0/1/0/all/0/1\">Sravanthi Parasa</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lange_T/0/1/0/all/0/1\">Thomas de Lange</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Halvorsen_P/0/1/0/all/0/1\">P&#xe5;l Halvorsen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Riegler_M/0/1/0/all/0/1\">Michael A. Riegler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fitting and recognition of geometric primitives in segmented 3D point clouds using a localized voting procedure. (arXiv:2205.15426v1 [cs.CV])","link":"http://arxiv.org/abs/2205.15426","description":"<p>The automatic creation of geometric models from point clouds has numerous\napplications in CAD (e.g., reverse engineering, manufacturing, assembling) and,\nmore in general, in shape modelling and processing. Given a segmented point\ncloud representing a man-made object, we propose a method for recognizing\nsimple geometric primitives and their interrelationships. Our approach is based\non the Hough transform (HT) for its ability to deal with noise, missing parts\nand outliers. In our method we introduce a novel technique for processing\nsegmented point clouds that, through a voting procedure, is able to provide an\ninitial estimate of the geometric parameters characterizing each primitive\ntype. By using these estimates, we localize the search of the optimal solution\nin a dimensionally-reduced parameter space thus making it efficient to extend\nthe HT to more primitives than those that are generally found in the\nliterature, i.e. planes and spheres. Then, we extract a number of geometric\ndescriptors that uniquely characterize a segment, and, on the basis of these\ndescriptors, we show how to aggregate parts of primitives (segments).\nExperiments on both synthetic and industrial scans reveal the robustness of the\nprimitive fitting method and its effectiveness for inferring relations among\nsegments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Raffo_A/0/1/0/all/0/1\">Andrea Raffo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romanengo_C/0/1/0/all/0/1\">Chiara Romanengo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Falcidieno_B/0/1/0/all/0/1\">Bianca Falcidieno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biasotti_S/0/1/0/all/0/1\">Silvia Biasotti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Segmentation Consistency Training: Out-of-Distribution Generalization for Medical Image Segmentation. (arXiv:2205.15428v1 [cs.CV])","link":"http://arxiv.org/abs/2205.15428","description":"<p>Generalizability is seen as one of the major challenges in deep learning, in\nparticular in the domain of medical imaging, where a change of hospital or in\nimaging routines can lead to a complete failure of a model. To tackle this, we\nintroduce Consistency Training, a training procedure and alternative to data\naugmentation based on maximizing models' prediction consistency across\naugmented and unaugmented data in order to facilitate better\nout-of-distribution generalization. To this end, we develop a novel\nregion-based segmentation loss function called Segmentation Inconsistency Loss\n(SIL), which considers the differences between pairs of augmented and\nunaugmented predictions and labels. We demonstrate that Consistency Training\noutperforms conventional data augmentation on several out-of-distribution\ndatasets on polyp segmentation, a popular medical task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Torpmann_Hagen_B/0/1/0/all/0/1\">Birk Torpmann-Hagen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thambawita_V/0/1/0/all/0/1\">Vajira Thambawita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glette_K/0/1/0/all/0/1\">Kyrre Glette</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Halvorsen_P/0/1/0/all/0/1\">P&#xe5;l Halvorsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riegler_M/0/1/0/all/0/1\">Michael A. Riegler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Advances in Transformers and CNN for Skin Lesion Diagnosis on Small Datasets. (arXiv:2205.15442v1 [cs.CV])","link":"http://arxiv.org/abs/2205.15442","description":"<p>Skin cancer is one of the most common types of cancer in the world. Different\ncomputer-aided diagnosis systems have been proposed to tackle skin lesion\ndiagnosis, most of them based in deep convolutional neural networks. However,\nrecent advances in computer vision achieved state-of-art results in many tasks,\nnotably Transformer-based networks. We explore and evaluate advances in\ncomputer vision architectures, training methods and multimodal feature fusion\nfor skin lesion diagnosis task. Experiments show that PiT ($0.800 \\pm 0.006$),\nCoaT ($0.780 \\pm 0.024$) and ViT ($0.771 \\pm 0.018$) backbone models with\nMetaBlock fusion achieved state-of-art results for balanced accuracy metric in\nPAD-UFES-20 dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lima_L/0/1/0/all/0/1\">Leandro M. de Lima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krohling_R/0/1/0/all/0/1\">Renato A. Krohling</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continual Object Detection: A review of definitions, strategies, and challenges. (arXiv:2205.15445v1 [cs.CV])","link":"http://arxiv.org/abs/2205.15445","description":"<p>The field of Continual Learning investigates the ability to learn consecutive\ntasks without losing performance on those previously learned. Its focus has\nbeen mainly on incremental classification tasks. We believe that research in\ncontinual object detection deserves even more attention due to its vast range\nof applications in robotics and autonomous vehicles. This scenario is more\ncomplex than conventional classification given the occurrence of instances of\nclasses that are unknown at the time, but can appear in subsequent tasks as a\nnew class to be learned, resulting in missing annotations and conflicts with\nthe background label. In this review, we analyze the current strategies\nproposed to tackle the problem of class-incremental object detection. Our main\ncontributions are: (1) a short and systematic review of the methods that\npropose solutions to traditional incremental object detection scenarios; (2) A\ncomprehensive evaluation of the existing approaches using a new metric to\nquantify the stability and plasticity of each technique in a standard way; (3)\nan overview of the current trends within continual object detection and a\ndiscussion of possible future research directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Menezes_A/0/1/0/all/0/1\">Angelo G. Menezes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moura_G/0/1/0/all/0/1\">Gustavo de Moura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alves_C/0/1/0/all/0/1\">C&#xe9;zanne Alves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carvalho_A/0/1/0/all/0/1\">Andr&#xe9; C. P. L. F. de Carvalho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HeatER: An Efficient and Unified Network for Human Reconstruction via Heatmap-based TransformER. (arXiv:2205.15448v1 [cs.CV])","link":"http://arxiv.org/abs/2205.15448","description":"<p>Recently, vision transformers have shown great success in 2D human pose\nestimation (2D HPE), 3D human pose estimation (3D HPE), and human mesh\nreconstruction (HMR) tasks. In these tasks, heatmap representations of the\nhuman structural information are often extracted first from the image by a CNN,\nand then further processed with a transformer architecture to provide the final\nHPE or HMR estimation. However, existing transformer architectures are not able\nto process these heatmap inputs directly, forcing an unnatural flattening of\nthe features prior to input. Furthermore, much of the performance benefit in\nrecent HPE and HMR methods has come at the cost of ever-increasing computation\nand memory needs. Therefore, to simultaneously address these problems, we\npropose HeatER, a novel transformer design which preserves the inherent\nstructure of heatmap representations when modeling attention while reducing the\nmemory and computational costs. Taking advantage of HeatER, we build a unified\nand efficient network for 2D HPE, 3D HPE, and HMR tasks. A heatmap\nreconstruction module is applied to improve the robustness of the estimated\nhuman pose and mesh. Extensive experiments demonstrate the effectiveness of\nHeatER on various human pose and mesh datasets. For instance, HeatER\noutperforms the SOTA method MeshGraphormer by requiring 5% of Params and 16% of\nMACs on Human3.6M and 3DPW datasets. Code will be publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Ce Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mendieta_M/0/1/0/all/0/1\">Matias Mendieta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Taojiannan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MVMO: A Multi-Object Dataset for Wide Baseline Multi-View Semantic Segmentation. (arXiv:2205.15452v1 [cs.CV])","link":"http://arxiv.org/abs/2205.15452","description":"<p>We present MVMO (Multi-View, Multi-Object dataset): a synthetic dataset of\n116,000 scenes containing randomly placed objects of 10 distinct classes and\ncaptured from 25 camera locations in the upper hemisphere. MVMO comprises\nphotorealistic, path-traced image renders, together with semantic segmentation\nground truth for every view. Unlike existing multi-view datasets, MVMO features\nwide baselines between cameras and high density of objects, which lead to large\ndisparities, heavy occlusions and view-dependent object appearance. Single view\nsemantic segmentation is hindered by self and inter-object occlusions that\ncould benefit from additional viewpoints. Therefore, we expect that MVMO will\npropel research in multi-view semantic segmentation and cross-view semantic\ntransfer. We also provide baselines that show that new research is needed in\nsuch fields to exploit the complementary information of multi-view setups.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alvarez_Gila_A/0/1/0/all/0/1\">Aitor Alvarez-Gila</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weijer_J/0/1/0/all/0/1\">Joost van de Weijer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaxing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garrote_E/0/1/0/all/0/1\">Estibaliz Garrote</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Registering Image Volumes using 3D SIFT and Discrete SP-Symmetry. (arXiv:2205.15456v1 [cs.CV])","link":"http://arxiv.org/abs/2205.15456","description":"<p>This paper proposes to extend local image features in 3D to include\ninvariance to discrete symmetry including inversion of spatial axes and image\ncontrast. A binary feature sign $s \\in \\{-1,+1\\}$ is defined as the sign of the\nLaplacian operator $\\nabla^2$, and used to obtain a descriptor that is\ninvariant to image sign inversion $s \\rightarrow -s$ and 3D parity transforms\n$(x,y,z)\\rightarrow(-x,-y,-z)$, i.e. SP-invariant or SP-symmetric. SP-symmetry\napplies to arbitrary scalar image fields $I: R^3 \\rightarrow R^1$ mapping 3D\ncoordinates $(x,y,z) \\in R^3$ to scalar intensity $I(x,y,z) \\in R^1$,\ngeneralizing the well-known charge conjugation and parity symmetry\n(CP-symmetry) applying to elementary charged particles. Feature orientation is\nmodeled as a set of discrete states corresponding to potential axis\nreflections, independently of image contrast inversion. Two primary axis\nvectors are derived from image observations and potentially subject to\nreflection, and a third axis is an axial vector defined by the right-hand rule.\nAugmenting local feature properties with sign in addition to standard\n(location, scale, orientation) geometry leads to descriptors that are invariant\nto coordinate reflections and intensity contrast inversion. Feature properties\nare factored in to probabilistic point-based registration as symmetric kernels,\nbased on a model of binary feature correspondence. Experiments using the\nwell-known coherent point drift (CPD) algorithm demonstrate that SIFT-CPD\nkernels achieve the most accurate and rapid registration of the human brain and\nCT chest, including multiple MRI modalities of differing intensity contrast,\nand abnormal local variations such as tumors or occlusions. SIFT-CPD image\nregistration is invariant to global scaling, rotation and translation and image\nintensity inversions of the input data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chauvin_L/0/1/0/all/0/1\">Laurent Chauvin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wells_W/0/1/0/all/0/1\">William Wells III</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toews_M/0/1/0/all/0/1\">Matthew Toews</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-Shot Diffusion Models. (arXiv:2205.15463v1 [cs.CV])","link":"http://arxiv.org/abs/2205.15463","description":"<p>Denoising diffusion probabilistic models (DDPM) are powerful hierarchical\nlatent variable models with remarkable sample generation quality and training\nstability. These properties can be attributed to parameter sharing in the\ngenerative hierarchy, as well as a parameter-free diffusion-based inference\nprocedure. In this paper, we present Few-Shot Diffusion Models (FSDM), a\nframework for few-shot generation leveraging conditional DDPMs. FSDMs are\ntrained to adapt the generative process conditioned on a small set of images\nfrom a given class by aggregating image patch information using a set-based\nVision Transformer (ViT). At test time, the model is able to generate samples\nfrom previously unseen classes conditioned on as few as 5 samples from that\nclass. We empirically show that FSDM can perform few-shot generation and\ntransfer to new datasets. We benchmark variants of our method on complex vision\ndatasets for few-shot learning and compare to unconditional and conditional\nDDPM baselines. Additionally, we show how conditioning the model on patch-based\ninput set information improves training convergence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Giannone_G/0/1/0/all/0/1\">Giorgio Giannone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nielsen_D/0/1/0/all/0/1\">Didrik Nielsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winther_O/0/1/0/all/0/1\">Ole Winther</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GCoNet+: A Stronger Group Collaborative Co-Salient Object Detector. (arXiv:2205.15469v1 [cs.CV])","link":"http://arxiv.org/abs/2205.15469","description":"<p>In this paper, we present a novel end-to-end group collaborative learning\nnetwork, termed GCoNet+, which can effectively and efficiently (250 fps)\nidentify co-salient objects in natural scenes. The proposed GCoNet+ achieves\nthe new state-of-the-art performance for co-salient object detection (CoSOD)\nthrough mining consensus representations based on the following two essential\ncriteria: 1) intra-group compactness to better formulate the consistency among\nco-salient objects by capturing their inherent shared attributes using our\nnovel group affinity module (GAM); 2) inter-group separability to effectively\nsuppress the influence of noisy objects on the output by introducing our new\ngroup collaborating module (GCM) conditioning on the inconsistent consensus. To\nfurther improve the accuracy, we design a series of simple yet effective\ncomponents as follows: i) a recurrent auxiliary classification module (RACM)\npromoting the model learning at the semantic level; ii) a confidence\nenhancement module (CEM) helping the model to improve the quality of the final\npredictions; and iii) a group-based symmetric triplet (GST) loss guiding the\nmodel to learn more discriminative features. Extensive experiments on three\nchallenging benchmarks, i.e., CoCA, CoSOD3k, and CoSal2015, demonstrate that\nour GCoNet+ outperforms the existing 12 cutting-edge models. Code has been\nreleased at https://github.com/ZhengPeng7/GCoNet_plus.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_P/0/1/0/all/0/1\">Peng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1\">Huazhu Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1\">Deng-Ping Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Q/0/1/0/all/0/1\">Qi Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1\">Jie Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Introduction of a tree-based technique for efficient and real-time label retrieval in the object tracking system. (arXiv:2205.15477v1 [cs.CV])","link":"http://arxiv.org/abs/2205.15477","description":"<p>This paper addresses the issue of the real-time tracking quality of moving\nobjects in large-scale video surveillance systems. During the tracking process,\nthe system assigns an identifier or label to each tracked object to distinguish\nit from other objects. In such a mission, it is essential to keep this\nidentifier for the same objects, whatever the area, the time of their\nappearance, or the detecting camera. This is to conserve as much information\nabout the tracking object as possible, decrease the number of ID switching\n(ID-Sw), and increase the quality of object tracking. To accomplish object\nlabeling, a massive amount of data collected by the cameras must be searched to\nretrieve the most similar (nearest neighbor) object identifier. Although this\ntask is simple, it becomes very complex in large-scale video surveillance\nnetworks, where the data becomes very large. In this case, the label retrieval\ntime increases significantly with this increase, which negatively affects the\nperformance of the real-time tracking system. To avoid such problems, we\npropose a new solution to automatically label multiple objects for efficient\nreal-time tracking using the indexing mechanism. This mechanism organizes the\nmetadata of the objects extracted during the detection and tracking phase in an\nAdaptive BCCF-tree. The main advantage of this structure is: its ability to\nindex massive metadata generated by multi-cameras, its logarithmic search\ncomplexity, which implicitly reduces the search response time, and its quality\nof research results, which ensure coherent labeling of the tracked objects. The\nsystem load is distributed through a new Internet of Video Things\ninfrastructure-based architecture to improve data processing and real-time\nobject tracking performance. The experimental evaluation was conducted on a\npublicly available dataset generated by multi-camera containing different crowd\nactivities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Benrazek_A/0/1/0/all/0/1\">Ala-Eddine Benrazek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kouahla_Z/0/1/0/all/0/1\">Zineddine Kouahla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farou_B/0/1/0/all/0/1\">Brahim Farou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seridi_H/0/1/0/all/0/1\">Hamid Seridi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allele_I/0/1/0/all/0/1\">Imane Allele</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Joint Spatial-Temporal and Appearance Modeling with Transformer for Multiple Object Tracking. (arXiv:2205.15495v1 [cs.CV])","link":"http://arxiv.org/abs/2205.15495","description":"<p>The recent trend in multiple object tracking (MOT) is heading towards\nleveraging deep learning to boost the tracking performance. In this paper, we\npropose a novel solution named TransSTAM, which leverages Transformer to\neffectively model both the appearance features of each object and the\nspatial-temporal relationships among objects. TransSTAM consists of two major\nparts: (1) The encoder utilizes the powerful self-attention mechanism of\nTransformer to learn discriminative features for each tracklet; (2) The decoder\nadopts the standard cross-attention mechanism to model the affinities between\nthe tracklets and the detections by taking both spatial-temporal and appearance\nfeatures into account. TransSTAM has two major advantages: (1) It is solely\nbased on the encoder-decoder architecture and enjoys a compact network design,\nhence being computationally efficient; (2) It can effectively learn\nspatial-temporal and appearance features within one model, hence achieving\nbetter tracking accuracy. The proposed method is evaluated on multiple public\nbenchmarks including MOT16, MOT17, and MOT20, and it achieves a clear\nperformance improvement in both IDF1 and HOTA with respect to previous\nstate-of-the-art approaches on all the benchmarks. Our code is available at\n\\url{https://github.com/icicle4/TranSTAM}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dai_P/0/1/0/all/0/1\">Peng Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yiqiang Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weng_R/0/1/0/all/0/1\">Renliang Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Changshui Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ADAPT: Vision-Language Navigation with Modality-Aligned Action Prompts. (arXiv:2205.15509v1 [cs.CV])","link":"http://arxiv.org/abs/2205.15509","description":"<p>Vision-Language Navigation (VLN) is a challenging task that requires an\nembodied agent to perform action-level modality alignment, i.e., make\ninstruction-asked actions sequentially in complex visual environments. Most\nexisting VLN agents learn the instruction-path data directly and cannot\nsufficiently explore action-level alignment knowledge inside the multi-modal\ninputs. In this paper, we propose modAlity-aligneD Action PrompTs (ADAPT),\nwhich provides the VLN agent with action prompts to enable the explicit\nlearning of action-level modality alignment to pursue successful navigation.\nSpecifically, an action prompt is defined as a modality-aligned pair of an\nimage sub-prompt and a text sub-prompt, where the former is a single-view\nobservation and the latter is a phrase like ''walk past the chair''. When\nstarting navigation, the instruction-related action prompt set is retrieved\nfrom a pre-built action prompt base and passed through a prompt encoder to\nobtain the prompt feature. Then the prompt feature is concatenated with the\noriginal instruction feature and fed to a multi-layer transformer for action\nprediction. To collect high-quality action prompts into the prompt base, we use\nthe Contrastive Language-Image Pretraining (CLIP) model which has powerful\ncross-modality alignment ability. A modality alignment loss and a sequential\nconsistency loss are further introduced to enhance the alignment of the action\nprompt and enforce the agent to focus on the related prompt sequentially.\nExperimental results on both R2R and RxR show the superiority of ADAPT over\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Bingqian Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zicong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiwen Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jianzhuang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IDE-3D: Interactive Disentangled Editing for High-Resolution 3D-aware Portrait Synthesis. (arXiv:2205.15517v1 [cs.CV])","link":"http://arxiv.org/abs/2205.15517","description":"<p>Existing 3D-aware facial generation methods face a dilemma in quality versus\neditability: they either generate editable results in low resolution or\nhigh-quality ones with no editing flexibility. In this work, we propose a new\napproach that brings the best of both worlds together. Our system consists of\nthree major components: (1) a 3D-semantics-aware generative model that produces\nview-consistent, disentangled face images and semantic masks; (2) a hybrid GAN\ninversion approach that initialize the latent codes from the semantic and\ntexture encoder, and further optimized them for faithful reconstruction; and\n(3) a canonical editor that enables efficient manipulation of semantic masks in\ncanonical view and product high-quality editing results. Our approach is\ncompetent for many applications, e.g. free-view face drawing, editing, and\nstyle control. Both quantitative and qualitative results show that our method\nreaches the state-of-the-art in terms of photorealism, faithfulness, and\nefficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jingxiang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yichun Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lizhen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yebin Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Variational Transfer Learning using Cross-Domain Latent Modulation. (arXiv:2205.15523v1 [cs.LG])","link":"http://arxiv.org/abs/2205.15523","description":"<p>To successfully apply trained neural network models to new domains, powerful\ntransfer learning solutions are essential. We propose to introduce a novel\ncross-domain latent modulation mechanism to a variational autoencoder framework\nso as to achieve effective transfer learning. Our key idea is to procure deep\nrepresentations from one data domain and use it to influence the\nreparameterization of the latent variable of another domain. Specifically, deep\nrepresentations of the source and target domains are first extracted by a\nunified inference model and aligned by employing gradient reversal. The learned\ndeep representations are then cross-modulated to the latent encoding of the\nalternative domain, where consistency constraints are also applied. In the\nempirical validation that includes a number of transfer learning benchmark\ntasks for unsupervised domain adaptation and image-to-image translation, our\nmodel demonstrates competitive performance, which is also supported by evidence\nobtained from visualization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1\">Jinyong Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jeremiah D. Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cranefield_S/0/1/0/all/0/1\">Stephen Cranefield</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Din_X/0/1/0/all/0/1\">Xuejie Din</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pseudo-Data based Self-Supervised Federated Learning for Classification of Histopathological Images. (arXiv:2205.15530v1 [cs.CV])","link":"http://arxiv.org/abs/2205.15530","description":"<p>Computer-aided diagnosis (CAD) can help pathologists improve diagnostic\naccuracy together with consistency and repeatability for cancers. However, the\nCAD models trained with the histopathological images only from a single center\n(hospital) generally suffer from the generalization problem due to the\nstraining inconsistencies among different centers. In this work, we propose a\npseudo-data based self-supervised federated learning (FL) framework, named\nSSL-FT-BT, to improve both the diagnostic accuracy and generalization of CAD\nmodels. Specifically, the pseudo histopathological images are generated from\neach center, which contains inherent and specific properties corresponding to\nthe real images in this center, but does not include the privacy information.\nThese pseudo images are then shared in the central server for self-supervised\nlearning (SSL). A multi-task SSL is then designed to fully learn both the\ncenter-specific information and common inherent representation according to the\ndata characteristics. Moreover, a novel Barlow Twins based FL (FL-BT) algorithm\nis proposed to improve the local training for the CAD model in each center by\nconducting contrastive learning, which benefits the optimization of the global\nmodel in the FL procedure. The experimental results on three public\nhistopathological image datasets indicate the effectiveness of the proposed\nSSL-FL-BT on both diagnostic accuracy and generalization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jun Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuanming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xiangmin Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1\">Saisai Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ying_S/0/1/0/all/0/1\">Shihui Ying</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"itKD: Interchange Transfer-based Knowledge Distillation for 3D Object Detection. (arXiv:2205.15531v1 [cs.CV])","link":"http://arxiv.org/abs/2205.15531","description":"<p>Recently, point-cloud based 3D object detectors have achieved remarkable\nprogress. However, most studies are limited to the development of deep learning\narchitectures for improving only their accuracy. In this paper, we propose an\nautoencoder-style framework comprising channel-wise compression and\ndecompression via interchange transfer for knowledge distillation. To learn the\nmap-view feature of a teacher network, the features from a teacher and student\nnetwork are independently passed through the shared autoencoder; here, we use a\ncompressed representation loss that binds the channel-wised compression\nknowledge from both the networks as a kind of regularization. The decompressed\nfeatures are transferred in opposite directions to reduce the gap in the\ninterchange reconstructions. Lastly, we present an attentive head loss for\nmatching the pivotal detection information drawn by the multi-head\nself-attention mechanism. Through extensive experiments, we verify that our\nmethod can learn the lightweight model that is well-aligned with the 3D point\ncloud detection task and we demonstrate its superiority using the well-known\npublic datasets Waymo and nuScenes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cho_H/0/1/0/all/0/1\">Hyeon Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Junyong Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baek_G/0/1/0/all/0/1\">Geonwoo Baek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_W/0/1/0/all/0/1\">Wonjun Hwang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gluing Neural Networks Symbolically Through Hyperdimensional Computing. (arXiv:2205.15534v1 [cs.SC])","link":"http://arxiv.org/abs/2205.15534","description":"<p>Hyperdimensional Computing affords simple, yet powerful operations to create\nlong Hyperdimensional Vectors (hypervectors) that can efficiently encode\ninformation, be used for learning, and are dynamic enough to be modified on the\nfly. In this paper, we explore the notion of using binary hypervectors to\ndirectly encode the final, classifying output signals of neural networks in\norder to fuse differing networks together at the symbolic level. This allows\nmultiple neural networks to work together to solve a problem, with little\nadditional overhead. Output signals just before classification are encoded as\nhypervectors and bundled together through consensus summation to train a\nclassification hypervector. This process can be performed iteratively and even\non single neural networks by instead making a consensus of multiple\nclassification hypervectors. We find that this outperforms the state of the\nart, or is on a par with it, while using very little overhead, as hypervector\noperations are extremely fast and efficient in comparison to the neural\nnetworks. This consensus process can learn online and even grow or lose models\nin real time. Hypervectors act as memories that can be stored, and even further\nbundled together over time, affording life long learning capabilities.\nAdditionally, this consensus structure inherits the benefits of\nHyperdimensional Computing, without sacrificing the performance of modern\nMachine Learning. This technique can be extrapolated to virtually any neural\nmodel, and requires little modification to employ - one simply requires\nrecording the output signals of networks when presented with a testing example.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sutor_P/0/1/0/all/0/1\">Peter Sutor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_D/0/1/0/all/0/1\">Dehao Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Summers_Stay_D/0/1/0/all/0/1\">Douglas Summers-Stay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fermuller_C/0/1/0/all/0/1\">Cornelia Fermuller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aloimonos_Y/0/1/0/all/0/1\">Yiannis Aloimonos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepDefacer: Automatic Removal of Facial Features via U-Net Image Segmentation. (arXiv:2205.15536v1 [cs.CV])","link":"http://arxiv.org/abs/2205.15536","description":"<p>Recent advancements in the field of magnetic resonance imaging (MRI) have\nenabled large-scale collaboration among clinicians and researchers for\nneuroimaging tasks. However, researchers are often forced to use outdated and\nslow software to anonymize MRI images for publication. These programs\nspecifically perform expensive mathematical operations over 3D images that\nrapidly slow down anonymization speed as an image's volume increases in size.\nIn this paper, we introduce DeepDefacer, an application of deep learning to MRI\nanonymization that uses a streamlined 3D U-Net network to mask facial regions\nin MRI images with a significant increase in speed over traditional\nde-identification software. We train DeepDefacer on MRI images from the Brain\nDevelopment Organization (IXI) and International Consortium for Brain Mapping\n(ICBM) and quantitatively evaluate our model against a baseline 3D U-Net model\nwith regards to Dice, recall, and precision scores. We also evaluate\nDeepDefacer against Pydeface, a traditional defacing application, with regards\nto speed on a range of CPU and GPU devices and qualitatively evaluate our\nmodel's defaced output versus the ground truth images produced by Pydeface. We\nprovide a link to a PyPi program at the end of this manuscript to encourage\nfurther research into the application of deep learning to MRI anonymization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khazane_A/0/1/0/all/0/1\">Anish Khazane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoachuck_J/0/1/0/all/0/1\">Julien Hoachuck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gorgolewski_K/0/1/0/all/0/1\">Krzysztof J. Gorgolewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poldrack_R/0/1/0/all/0/1\">Russell A. Poldrack</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AI-based automated Meibomian gland segmentation, classification and reflection correction in infrared Meibography. (arXiv:2205.15543v1 [q-bio.QM])","link":"http://arxiv.org/abs/2205.15543","description":"<p>Purpose: Develop a deep learning-based automated method to segment meibomian\nglands (MG) and eyelids, quantitatively analyze the MG area and MG ratio,\nestimate the meiboscore, and remove specular reflections from infrared images.\nMethods: A total of 1600 meibography images were captured in a clinical\nsetting. 1000 images were precisely annotated with multiple revisions by\ninvestigators and graded 6 times by meibomian gland dysfunction (MGD) experts.\nTwo deep learning (DL) models were trained separately to segment areas of the\nMG and eyelid. Those segmentation were used to estimate MG ratio and\nmeiboscores using a classification-based DL model. A generative adversarial\nnetwork was implemented to remove specular reflections from original images.\nResults: The mean ratio of MG calculated by investigator annotation and DL\nsegmentation was consistent 26.23% vs 25.12% in the upper eyelids and 32.34%\nvs. 32.29% in the lower eyelids, respectively. Our DL model achieved 73.01%\naccuracy for meiboscore classification on validation set and 59.17% accuracy\nwhen tested on images from independent center, compared to 53.44% validation\naccuracy by MGD experts. The DL-based approach successfully removes reflection\nfrom the original MG images without affecting meiboscore grading. Conclusions:\nDL with infrared meibography provides a fully automated, fast quantitative\nevaluation of MG morphology (MG Segmentation, MG area, MG ratio, and\nmeiboscore) which are sufficiently accurate for diagnosing dry eye disease.\nAlso, the DL removes specular reflection from images to be used by\nophthalmologists for distraction-free assessment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Saha_R/0/1/0/all/0/1\">Ripon Kumar Saha</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Chowdhury_A/0/1/0/all/0/1\">A. M. Mahmud Chowdhury</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Na_K/0/1/0/all/0/1\">Kyung-Sun Na</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Hwang_G/0/1/0/all/0/1\">Gyu Deok Hwang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Eom_Y/0/1/0/all/0/1\">Youngsub Eom</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Kim_J/0/1/0/all/0/1\">Jaeyoung Kim</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Jeon_H/0/1/0/all/0/1\">Hae-Gon Jeon</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Hwang_H/0/1/0/all/0/1\">Ho Sik Hwang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Chung_E/0/1/0/all/0/1\">Euiheon Chung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mask2Hand: Learning to Predict the 3D Hand Pose and Shape from Shadow. (arXiv:2205.15553v1 [cs.CV])","link":"http://arxiv.org/abs/2205.15553","description":"<p>We present a self-trainable method, Mask2Hand, which learns to solve the\nchallenging task of predicting 3D hand pose and shape from a 2D binary mask of\nhand silhouette/shadow without additional manually-annotated data. Given the\nintrinsic camera parameters and the parametric hand model in the camera space,\nwe adopt the differentiable rendering technique to project 3D estimations onto\nthe 2D binary silhouette space. By applying a tailored combination of losses\nbetween the rendered silhouette and the input binary mask, we are able to\nintegrate the self-guidance mechanism into our end-to-end optimization process\nfor constraining global mesh registration and hand pose estimation. The\nexperiments show that our method, which takes a single binary mask as the\ninput, can achieve comparable prediction accuracy on both unaligned and aligned\nsettings as state-of-the-art methods that require RGB or depth inputs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_L/0/1/0/all/0/1\">Li-Jen Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1\">Yu-Cheng Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chia-Hui Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hwann-Tzong Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"iFS-RCNN: An Incremental Few-shot Instance Segmenter. (arXiv:2205.15562v1 [cs.CV])","link":"http://arxiv.org/abs/2205.15562","description":"<p>This paper addresses incremental few-shot instance segmentation, where a few\nexamples of new object classes arrive when access to training examples of old\nclasses is not available anymore, and the goal is to perform well on both old\nand new classes. We make two contributions by extending the common Mask-RCNN\nframework in its second stage -- namely, we specify a new object class\nclassifier based on the probit function and a new uncertainty-guided\nbounding-box predictor. The former leverages Bayesian learning to address a\npaucity of training examples of new classes. The latter learns not only to\npredict object bounding boxes but also to estimate the uncertainty of the\nprediction as guidance for bounding box refinement. We also specify two new\nloss functions in terms of the estimated object-class distribution and\nbounding-box uncertainty. Our contributions produce significant performance\ngains on the COCO dataset over the state of the art -- specifically, the gain\nof +6 on the new classes and +16 on the old classes in the AP instance\nsegmentation metric. Furthermore, we are the first to evaluate the incremental\nfew-shot setting on the more challenging LVIS dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Khoi Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Todorovic_S/0/1/0/all/0/1\">Sinisa Todorovic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sub-Image Histogram Equalization using Coot Optimization Algorithm for Segmentation and Parameter Selection. (arXiv:2205.15565v1 [cs.CV])","link":"http://arxiv.org/abs/2205.15565","description":"<p>Contrast enhancement is very important in terms of assessing images in an\nobjective way. Contrast enhancement is also significant for various algorithms\nincluding supervised and unsupervised algorithms for accurate classification of\nsamples. Some contrast enhancement algorithms solve this problem by addressing\nthe low contrast issue. Mean and variance based sub-image histogram\nequalization (MVSIHE) algorithm is one of these contrast enhancements methods\nproposed in the literature. It has different parameters which need to be tuned\nin order to achieve optimum results. With this motivation, in this study, we\nemployed one of the most recent optimization algorithms, namely, coot\noptimization algorithm (COA) for selecting appropriate parameters for the\nMVSIHE algorithm. Blind/referenceless image spatial quality evaluator (BRISQUE)\nand natural image quality evaluator (NIQE) metrics are used for evaluating\nfitness of the coot swarm population. The results show that the proposed method\ncan be used in the field of biomedical image processing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kuran_E/0/1/0/all/0/1\">Emre Can Kuran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuran_U/0/1/0/all/0/1\">Umut Kuran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Er_M/0/1/0/all/0/1\">Mehmet Bilal Er</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical Spherical CNNs with Lifting-based Adaptive Wavelets for Pooling and Unpooling. (arXiv:2205.15571v1 [cs.CV])","link":"http://arxiv.org/abs/2205.15571","description":"<p>Pooling and unpooling are two essential operations in constructing\nhierarchical spherical convolutional neural networks (HS-CNNs) for\ncomprehensive feature learning in the spherical domain. Most existing models\nemploy downsampling-based pooling, which will inevitably incur information loss\nand cannot adapt to different spherical signals and tasks. Besides, the\npreserved information after pooling cannot be well restored by the subsequent\nunpooling to characterize the desirable features for a task. In this paper, we\npropose a novel framework of HS-CNNs with a lifting structure to learn adaptive\nspherical wavelets for pooling and unpooling, dubbed LiftHS-CNN, which ensures\na more efficient hierarchical feature learning for both image- and pixel-level\ntasks. Specifically, adaptive spherical wavelets are learned with a lifting\nstructure that consists of trainable lifting operators (i.e., update and\npredict operators). With this learnable lifting structure, we can adaptively\npartition a signal into two sub-bands containing low- and high-frequency\ncomponents, respectively, and thus generate a better down-scaled representation\nfor pooling by preserving more information in the low-frequency sub-band. The\nupdate and predict operators are parameterized with graph-based attention to\njointly consider the signal's characteristics and the underlying geometries. We\nfurther show that particular properties are promised by the learned wavelets,\nensuring the spatial-frequency localization for better exploiting the signal's\ncorrelation in both spatial and frequency domains. We then propose an unpooling\noperation that is invertible to the lifting-based pooling, where an inverse\nwavelet transform is performed by using the learned lifting operators to\nrestore an up-scaled representation. Extensive empirical evaluations on various\nspherical domain tasks validate the superiority of the proposed LiftHS-CNN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mingxing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chenglin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1\">Wenrui Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Siheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1\">Junni Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frossard_P/0/1/0/all/0/1\">Pascal Frossard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1\">Hongkai Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3PSDF: Three-Pole Signed Distance Function for Learning Surfaces with Arbitrary Topologies. (arXiv:2205.15572v1 [cs.CV])","link":"http://arxiv.org/abs/2205.15572","description":"<p>Recent advances in learning 3D shapes using neural implicit functions have\nachieved impressive results by breaking the previous barrier of resolution and\ndiversity for varying topologies. However, most of such approaches are limited\nto closed surfaces as they require the space to be divided into inside and\noutside. More recent works based on unsigned distance function have been\nproposed to handle complex geometry containing both the open and closed\nsurfaces. Nonetheless, as their direct outputs are point clouds, robustly\nobtaining high-quality meshing results from discrete points remains an open\nquestion. We present a novel learnable implicit representation, called the\nthree-pole signed distance function (3PSDF), that can represent non-watertight\n3D shapes with arbitrary topologies while supporting easy field-to-mesh\nconversion using the classic Marching Cubes algorithm. The key to our method is\nthe introduction of a new sign, the NULL sign, in addition to the conventional\nin and out labels. The existence of the null sign could stop the formation of a\nclosed isosurface derived from the bisector of the in/out regions. Further, we\npropose a dedicated learning framework to effectively learn 3PSDF without\nworrying about the vanishing gradient due to the null labels. Experimental\nresults show that our approach outperforms the previous state-of-the-art\nmethods in a wide range of benchmarks both quantitatively and qualitatively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weikai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Cheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Weiyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Bo Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MontageGAN: Generation and Assembly of Multiple Components by GANs. (arXiv:2205.15577v1 [cs.CV])","link":"http://arxiv.org/abs/2205.15577","description":"<p>A multi-layer image is more valuable than a single-layer image from a graphic\ndesigner's perspective. However, most of the proposed image generation methods\nso far focus on single-layer images. In this paper, we propose MontageGAN,\nwhich is a Generative Adversarial Networks (GAN) framework for generating\nmulti-layer images. Our method utilized a two-step approach consisting of local\nGANs and global GAN. Each local GAN learns to generate a specific image layer,\nand the global GAN learns the placement of each generated image layer. Through\nour experiments, we show the ability of our method to generate multi-layer\nimages and estimate the placement of the generated image layers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shee_C/0/1/0/all/0/1\">Chean Fei Shee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uchida_S/0/1/0/all/0/1\">Seiichi Uchida</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Effective Fusion Method to Enhance the Robustness of CNN. (arXiv:2205.15582v1 [cs.CV])","link":"http://arxiv.org/abs/2205.15582","description":"<p>With the development of technology rapidly, applications of convolutional\nneural networks have improved the convenience of our life. However, in image\nclassification field, it has been found that when some perturbations are added\nto images, the CNN would misclassify it. Thus various defense methods have been\nproposed. The previous approach only considered how to incorporate modules in\nthe network to improve robustness, but did not focus on the way the modules\nwere incorporated. In this paper, we design a new fusion method to enhance the\nrobustness of CNN. We use a dot product-based approach to add the denoising\nmodule to ResNet18 and the attention mechanism to further improve the\nrobustness of the model. The experimental results on CIFAR10 have shown that\nour method is effective and better than the state-of-the-art methods under the\nattack of FGSM and PGD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yating Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_Z/0/1/0/all/0/1\">Zhichao Lian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Decomposing NeRF for Editing via Feature Field Distillation. (arXiv:2205.15585v1 [cs.CV])","link":"http://arxiv.org/abs/2205.15585","description":"<p>Emerging neural radiance fields (NeRF) are a promising scene representation\nfor computer graphics, enabling high-quality 3D reconstruction and novel view\nsynthesis from image observations. However, editing a scene represented by a\nNeRF is challenging, as the underlying connectionist representations such as\nMLPs or voxel grids are not object-centric or compositional. In particular, it\nhas been difficult to selectively edit specific regions or objects. In this\nwork, we tackle the problem of semantic scene decomposition of NeRFs to enable\nquery-based local editing of the represented 3D scenes. We propose to distill\nthe knowledge of off-the-shelf, self-supervised 2D image feature extractors\nsuch as CLIP-LSeg or DINO into a 3D feature field optimized in parallel to the\nradiance field. Given a user-specified query of various modalities such as\ntext, an image patch, or a point-and-click selection, 3D feature fields\nsemantically decompose 3D space without the need for re-training and enable us\nto semantically select and edit regions in the radiance field. Our experiments\nvalidate that the distilled feature fields (DFFs) can transfer recent progress\nin 2D vision and language foundation models to 3D scene representations,\nenabling convincing 3D segmentation and selective editing of emerging neural\ngraphics representations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kobayashi_S/0/1/0/all/0/1\">Sosuke Kobayashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsumoto_E/0/1/0/all/0/1\">Eiichi Matsumoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sitzmann_V/0/1/0/all/0/1\">Vincent Sitzmann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Novel View Synthesis for High-fidelity Headshot Scenes. (arXiv:2205.15595v1 [cs.CV])","link":"http://arxiv.org/abs/2205.15595","description":"<p>Rendering scenes with a high-quality human face from arbitrary viewpoints is\na practical and useful technique for many real-world applications. Recently,\nNeural Radiance Fields (NeRF), a rendering technique that uses neural networks\nto approximate classical ray tracing, have been considered as one of the\npromising approaches for synthesizing novel views from a sparse set of images.\nWe find that NeRF can render new views while maintaining geometric consistency,\nbut it does not properly maintain skin details, such as moles and pores. These\ndetails are important particularly for faces because when we look at an image\nof a face, we are much more sensitive to details than when we look at other\nobjects. On the other hand, 3D Morpable Models (3DMMs) based on traditional\nmeshes and textures can perform well in terms of skin detail despite that it\nhas less precise geometry and cannot cover the head and the entire scene with\nbackground. Based on these observations, we propose a method to use both NeRF\nand 3DMM to synthesize a high-fidelity novel view of a scene with a face. Our\nmethod learns a Generative Adversarial Network (GAN) to mix a NeRF-synthesized\nimage and a 3DMM-rendered image and produces a photorealistic scene with a face\npreserving the skin details. Experiments with various real-world scenes\ndemonstrate the effectiveness of our approach. The code will be available on\nhttps://github.com/showlab/headshot .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tsutsui_S/0/1/0/all/0/1\">Satoshi Tsutsui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_W/0/1/0/all/0/1\">Weijia Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Sijing Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yunyi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1\">Murong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shou_M/0/1/0/all/0/1\">Mike Zheng Shou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative Aging of Brain Images with Diffeomorphic Registration. (arXiv:2205.15607v1 [eess.IV])","link":"http://arxiv.org/abs/2205.15607","description":"<p>Analyzing and predicting brain aging is essential for early prognosis and\naccurate diagnosis of cognitive diseases. The technique of neuroimaging, such\nas Magnetic Resonance Imaging (MRI), provides a noninvasive means of observing\nthe aging process within the brain. With longitudinal image data collection,\ndata-intensive Artificial Intelligence (AI) algorithms have been used to\nexamine brain aging. However, existing state-of-the-art algorithms tend to be\nrestricted to group-level predictions and suffer from unreal predictions. This\npaper proposes a methodology for generating longitudinal MRI scans that capture\nsubject-specific neurodegeneration and retain anatomical plausibility in aging.\nThe proposed methodology is developed within the framework of diffeomorphic\nregistration and relies on three key novel technological advances to generate\nsubject-level anatomically plausible predictions: i) a computationally\nefficient and individualized generative framework based on registration; ii) an\naging generative module based on biological linear aging progression; iii) a\nquality control module to fit registration for generation task. Our methodology\nwas evaluated on 2662 T1-weighted (T1-w) MRI scans from 796 participants from\nthree different cohorts. First, we applied 6 commonly used criteria to\ndemonstrate the aging simulation ability of the proposed methodology; Secondly,\nwe evaluated the quality of the synthetic images using quantitative\nmeasurements and qualitative assessment by a neuroradiologist. Overall, the\nexperimental results show that the proposed method can produce anatomically\nplausible predictions that can be used to enhance longitudinal datasets, in\nturn enabling data-hungry AI-driven healthcare tools.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Fu_J/0/1/0/all/0/1\">Jingru Fu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tzortzakakis_A/0/1/0/all/0/1\">Antonios Tzortzakakis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Barroso_J/0/1/0/all/0/1\">Jos&#xe9; Barroso</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Westman_E/0/1/0/all/0/1\">Eric Westman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ferreira_D/0/1/0/all/0/1\">Daniel Ferreira</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Moreno_R/0/1/0/all/0/1\">Rodrigo Moreno</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly-supervised Action Transition Learning for Stochastic Human Motion Prediction. (arXiv:2205.15608v1 [cs.CV])","link":"http://arxiv.org/abs/2205.15608","description":"<p>We introduce the task of action-driven stochastic human motion prediction,\nwhich aims to predict multiple plausible future motions given a sequence of\naction labels and a short motion history. This differs from existing works,\nwhich predict motions that either do not respect any specific action category,\nor follow a single action label. In particular, addressing this task requires\ntackling two challenges: The transitions between the different actions must be\nsmooth; the length of the predicted motion depends on the action sequence and\nvaries significantly across samples. As we cannot realistically expect training\ndata to cover sufficiently diverse action transitions and motion lengths, we\npropose an effective training strategy consisting of combining multiple motions\nfrom different actions and introducing a weak form of supervision to encourage\nsmooth transitions. We then design a VAE-based model conditioned on both the\nobserved motion and the action label sequence, allowing us to generate multiple\nplausible future motions of varying length. We illustrate the generality of our\napproach by exploring its use with two different temporal encoding models,\nnamely RNNs and Transformers. Our approach outperforms baseline models\nconstructed by adapting state-of-the-art single action-conditioned motion\ngeneration methods and stochastic human motion prediction approaches to our new\ntask of action-driven stochastic motion prediction. Our code is available at\nhttps://github.com/wei-mao-2019/WAT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mao_W/0/1/0/all/0/1\">Wei Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Miaomiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salzmann_M/0/1/0/all/0/1\">Mathieu Salzmann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bag of Tricks for Domain Adaptive Multi-Object Tracking. (arXiv:2205.15609v1 [cs.CV])","link":"http://arxiv.org/abs/2205.15609","description":"<p>In this paper, SIA_Track is presented which is developed by a research team\nfrom SI Analytics. The proposed method was built from pre-existing detector and\ntracker under the tracking-by-detection paradigm. The tracker we used is an\nonline tracker that merely links newly received detections with existing\ntracks. The core part of our method is training procedure of the object\ndetector where synthetic and unlabeled real data were only used for training.\nTo maximize the performance on real data, we first propose to use\npseudo-labeling that generates imperfect labels for real data using a model\ntrained with synthetic dataset. After that model soups scheme was applied to\naggregate weights produced during iterative pseudo-labeling. Besides,\ncross-domain mixed sampling also helped to increase detection performance on\nreal data. Our method, SIA_Track, takes the first place on MOTSynth2MOT17 track\nat BMTT 2022 challenge. The code is available on\nhttps://github.com/SIAnalytics/BMTT2022_SIA_track.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Seo_M/0/1/0/all/0/1\">Minseok Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryu_J/0/1/0/all/0/1\">Jeongwon Ryu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_K/0/1/0/all/0/1\">Kwangjin Yoon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Centroid Supervision Alleviates Domain Shift in Medical Image Classification. (arXiv:2205.15658v1 [cs.CV])","link":"http://arxiv.org/abs/2205.15658","description":"<p>Deep learning based medical imaging classification models usually suffer from\nthe domain shift problem, where the classification performance drops when\ntraining data and real-world data differ in imaging equipment manufacturer,\nimage acquisition protocol, patient populations, etc. We propose Feature\nCentroid Contrast Learning (FCCL), which can improve target domain\nclassification performance by extra supervision during training with\ncontrastive loss between instance and class centroid. Compared with current\nunsupervised domain adaptation and domain generalization methods, FCCL performs\nbetter while only requires labeled image data from a single source domain and\nno target domain. We verify through extensive experiments that FCCL can achieve\nsuperior performance on at least three imaging modalities, i.e. fundus\nphotographs, dermatoscopic images, and H &amp; E tissue images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wenshuo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Dalu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Binghong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yehui Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Junde Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaorong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Haifeng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yanwu Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ViT-BEVSeg: A Hierarchical Transformer Network for Monocular Birds-Eye-View Segmentation. (arXiv:2205.15667v1 [cs.CV])","link":"http://arxiv.org/abs/2205.15667","description":"<p>Generating a detailed near-field perceptual model of the environment is an\nimportant and challenging problem in both self-driving vehicles and autonomous\nmobile robotics. A Bird Eye View (BEV) map, providing a panoptic\nrepresentation, is a commonly used approach that provides a simplified 2D\nrepresentation of the vehicle surroundings with accurate semantic level\nsegmentation for many downstream tasks. Current state-of-the art approaches to\ngenerate BEV-maps employ a Convolutional Neural Network (CNN) backbone to\ncreate feature-maps which are passed through a spatial transformer to project\nthe derived features onto the BEV coordinate frame. In this paper, we evaluate\nthe use of vision transformers (ViT) as a backbone architecture to generate BEV\nmaps. Our network architecture, ViT-BEVSeg, employs standard vision\ntransformers to generate a multi-scale representation of the input image. The\nresulting representation is then provided as an input to a spatial transformer\ndecoder module which outputs segmentation maps in the BEV grid. We evaluate our\napproach on the nuScenes dataset demonstrating a considerable improvement in\nthe performance relative to state-of-the-art approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dutta_P/0/1/0/all/0/1\">Pramit Dutta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sistu_G/0/1/0/all/0/1\">Ganesh Sistu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yogamani_S/0/1/0/all/0/1\">Senthil Yogamani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galvan_E/0/1/0/all/0/1\">Edgar Galv&#xe1;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McDonald_J/0/1/0/all/0/1\">John McDonald</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Augmentation-Aware Self-Supervision for Data-Efficient GAN Training. (arXiv:2205.15677v1 [cs.LG])","link":"http://arxiv.org/abs/2205.15677","description":"<p>Training generative adversarial networks (GANs) with limited data is valuable\nbut challenging because discriminators are prone to over-fitting in such\nsituations. Recently proposed differentiable data augmentation techniques for\ndiscriminators demonstrate improved data efficiency of training GANs. However,\nthe naive data augmentation introduces undesired invariance to augmentation\ninto the discriminator. The invariance may degrade the representation learning\nability of the discriminator, thereby affecting the generative modeling\nperformance of the generator. To mitigate the invariance while inheriting the\nbenefits of data augmentation, we propose a novel augmentation-aware\nself-supervised discriminator that predicts the parameter of augmentation given\nthe augmented and original data. Moreover, the prediction task is required to\ndistinguishable between real data and generated data since they are different\nduring training. We further encourage the generator to learn from the proposed\ndiscriminator by generating augmentation-predictable real data. We compare the\nproposed method with state-of-the-arts across the class-conditional BigGAN and\nunconditional StyleGAN2 architectures on CIFAR-10/100 and several low-shot\ndatasets, respectively. Experimental results show a significantly improved\ngeneration performance of our method over competing methods for training\ndata-efficient GANs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Liang Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Q/0/1/0/all/0/1\">Qi Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Huawei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1\">Siyuan Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoshuang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xueqi Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Relation-aware Graph Network Proliferation. (arXiv:2205.15678v1 [cs.LG])","link":"http://arxiv.org/abs/2205.15678","description":"<p>Graph neural architecture search has sparked much attention as Graph Neural\nNetworks (GNNs) have shown powerful reasoning capability in many relational\ntasks. However, the currently used graph search space overemphasizes learning\nnode features and neglects mining hierarchical relational information.\nMoreover, due to diverse mechanisms in the message passing, the graph search\nspace is much larger than that of CNNs. This hinders the straightforward\napplication of classical search strategies for exploring complicated graph\nsearch space. We propose Automatic Relation-aware Graph Network Proliferation\n(ARGNP) for efficiently searching GNNs with a relation-guided message passing\nmechanism. Specifically, we first devise a novel dual relation-aware graph\nsearch space that comprises both node and relation learning operations. These\noperations can extract hierarchical node/relational information and provide\nanisotropic guidance for message passing on a graph. Second, analogous to cell\nproliferation, we design a network proliferation search paradigm to\nprogressively determine the GNN architectures by iteratively performing network\ndivision and differentiation. The experiments on six datasets for four graph\nlearning tasks demonstrate that GNNs produced by our method are superior to the\ncurrent state-of-the-art hand-crafted and search-based GNNs. Codes are\navailable at https://github.com/phython96/ARGNP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_S/0/1/0/all/0/1\">Shaofei Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xinzhe Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jiebo Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1\">Zheng-Jun Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qingming Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Learning for Building Damage Assessment from Large-scale xBD Satellite Imagery Benchmark Datasets. (arXiv:2205.15688v1 [cs.CV])","link":"http://arxiv.org/abs/2205.15688","description":"<p>In the field of post-disaster assessment, for timely and accurate rescue and\nlocalization after a disaster, people need to know the location of damaged\nbuildings. In deep learning, some scholars have proposed methods to make\nautomatic and highly accurate building damage assessments by remote sensing\nimages, which are proved to be more efficient than assessment by domain\nexperts. However, due to the lack of a large amount of labeled data, these\nkinds of tasks can suffer from being able to do an accurate assessment, as the\nefficiency of deep learning models relies highly on labeled data. Although\nexisting semi-supervised and unsupervised studies have made breakthroughs in\nthis area, none of them has completely solved this problem. Therefore, we\npropose adopting a self-supervised comparative learning approach to address the\ntask without the requirement of labeled data. We constructed a novel asymmetric\ntwin network architecture and tested its performance on the xBD dataset.\nExperiment results of our model show the improvement compared to baseline and\ncommonly used methods. We also demonstrated the potential of self-supervised\nmethods for building damage recognition awareness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xia_Z/0/1/0/all/0/1\">Zaishuo Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zelin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yanbing Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jinze Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adriano_B/0/1/0/all/0/1\">Bruno Adriano</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Progressive Multi-scale Consistent Network for Multi-class Fundus Lesion Segmentation. (arXiv:2205.15720v1 [eess.IV])","link":"http://arxiv.org/abs/2205.15720","description":"<p>Effectively integrating multi-scale information is of considerable\nsignificance for the challenging multi-class segmentation of fundus lesions\nbecause different lesions vary significantly in scales and shapes. Several\nmethods have been proposed to successfully handle the multi-scale object\nsegmentation. However, two issues are not considered in previous studies. The\nfirst is the lack of interaction between adjacent feature levels, and this will\nlead to the deviation of high-level features from low-level features and the\nloss of detailed cues. The second is the conflict between the low-level and\nhigh-level features, this occurs because they learn different scales of\nfeatures, thereby confusing the model and decreasing the accuracy of the final\nprediction. In this paper, we propose a progressive multi-scale consistent\nnetwork (PMCNet) that integrates the proposed progressive feature fusion (PFF)\nblock and dynamic attention block (DAB) to address the aforementioned issues.\nSpecifically, PFF block progressively integrates multi-scale features from\nadjacent encoding layers, facilitating feature learning of each layer by\naggregating fine-grained details and high-level semantics. As features at\ndifferent scales should be consistent, DAB is designed to dynamically learn the\nattentive cues from the fused features at different scales, thus aiming to\nsmooth the essential conflicts existing in multi-scale features. The two\nproposed PFF and DAB blocks can be integrated with the off-the-shelf backbone\nnetworks to address the two issues of multi-scale and feature inconsistency in\nthe multi-class segmentation of fundus lesions, which will produce better\nfeature representation in the feature space. Experimental results on three\npublic datasets indicate that the proposed method is more effective than recent\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+He_A/0/1/0/all/0/1\">Along He</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_K/0/1/0/all/0/1\">Kai Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_T/0/1/0/all/0/1\">Tao Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bo_W/0/1/0/all/0/1\">Wang Bo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kang_H/0/1/0/all/0/1\">Hong Kang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fu_H/0/1/0/all/0/1\">Huazhu Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"One Loss for Quantization: Deep Hashing with Discrete Wasserstein Distributional Matching. (arXiv:2205.15721v1 [cs.CV])","link":"http://arxiv.org/abs/2205.15721","description":"<p>Image hashing is a principled approximate nearest neighbor approach to find\nsimilar items to a query in a large collection of images. Hashing aims to learn\na binary-output function that maps an image to a binary vector. For optimal\nretrieval performance, producing balanced hash codes with low-quantization\nerror to bridge the gap between the learning stage's continuous relaxation and\nthe inference stage's discrete quantization is important. However, in the\nexisting deep supervised hashing methods, coding balance and low-quantization\nerror are difficult to achieve and involve several losses. We argue that this\nis because the existing quantization approaches in these methods are\nheuristically constructed and not effective to achieve these objectives. This\npaper considers an alternative approach to learning the quantization\nconstraints. The task of learning balanced codes with low quantization error is\nre-formulated as matching the learned distribution of the continuous codes to a\npre-defined discrete, uniform distribution. This is equivalent to minimizing\nthe distance between two distributions. We then propose a computationally\nefficient distributional distance by leveraging the discrete property of the\nhash functions. This distributional distance is a valid distance and enjoys\nlower time and sample complexities. The proposed single-loss quantization\nobjective can be integrated into any existing supervised hashing method to\nimprove code balance and quantization error. Experiments confirm that the\nproposed approach substantially improves the performance of several\nrepresentative hashing~methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Doan_K/0/1/0/all/0/1\">Khoa D. Doan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_P/0/1/0/all/0/1\">Peng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Ping Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeVRF: Fast Deformable Voxel Radiance Fields for Dynamic Scenes. (arXiv:2205.15723v1 [cs.CV])","link":"http://arxiv.org/abs/2205.15723","description":"<p>Modeling dynamic scenes is important for many applications such as virtual\nreality and telepresence. Despite achieving unprecedented fidelity for novel\nview synthesis in dynamic scenes, existing methods based on Neural Radiance\nFields (NeRF) suffer from slow convergence (i.e., model training time measured\nin days). In this paper, we present DeVRF, a novel representation to accelerate\nlearning dynamic radiance fields. The core of DeVRF is to model both the 3D\ncanonical space and 4D deformation field of a dynamic, non-rigid scene with\nexplicit and discrete voxel-based representations. However, it is quite\nchallenging to train such a representation which has a large number of model\nparameters, often resulting in overfitting issues. To overcome this challenge,\nwe devise a novel static-to-dynamic learning paradigm together with a new data\ncapture setup that is convenient to deploy in practice. This paradigm unlocks\nefficient learning of deformable radiance fields via utilizing the 3D\nvolumetric canonical space learnt from multi-view static images to ease the\nlearning of 4D voxel deformation field with only few-view dynamic sequences. To\nfurther improve the efficiency of our DeVRF and its synthesized novel view's\nquality, we conduct thorough explorations and identify a set of strategies. We\nevaluate DeVRF on both synthetic and real-world dynamic scenes with different\ntypes of deformation. Experiments demonstrate that DeVRF achieves two orders of\nmagnitude speedup (100x faster) with on-par high-fidelity results compared to\nthe previous state-of-the-art approaches. The code and dataset will be released\nin https://github.com/showlab/DeVRF.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jia-Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yan-Pei Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_W/0/1/0/all/0/1\">Weijia Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenqiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">David Junhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keppo_J/0/1/0/all/0/1\">Jussi Keppo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1\">Ying Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qie_X/0/1/0/all/0/1\">Xiaohu Qie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shou_M/0/1/0/all/0/1\">Mike Zheng Shou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformers for Multi-Object Tracking on Point Clouds. (arXiv:2205.15730v1 [cs.CV])","link":"http://arxiv.org/abs/2205.15730","description":"<p>We present TransMOT, a novel transformer-based end-to-end trainable online\ntracker and detector for point cloud data. The model utilizes a cross- and a\nself-attention mechanism and is applicable to lidar data in an automotive\ncontext, as well as other data types, such as radar. Both track management and\nthe detection of new tracks are performed by the same transformer decoder\nmodule and the tracker state is encoded in feature space. With this approach,\nwe make use of the rich latent space of the detector for tracking rather than\nrelying on low-dimensional bounding boxes. Still, we are able to retain some of\nthe desirable properties of traditional Kalman-filter based approaches, such as\nan ability to handle sensor input at arbitrary timesteps or to compensate frame\nskips. This is possible due to a novel module that transforms the track\ninformation from one frame to the next on feature-level and thereby fulfills a\nsimilar task as the prediction step of a Kalman filter. Results are presented\non the challenging real-world dataset nuScenes, where the proposed model\noutperforms its Kalman filter-based tracking baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ruppel_F/0/1/0/all/0/1\">Felicia Ruppel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faion_F/0/1/0/all/0/1\">Florian Faion</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glaser_C/0/1/0/all/0/1\">Claudius Gl&#xe4;ser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dietmayer_K/0/1/0/all/0/1\">Klaus Dietmayer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Omni-Granular Ego-Semantic Propagation for Self-Supervised Graph Representation Learning. (arXiv:2205.15746v1 [cs.LG])","link":"http://arxiv.org/abs/2205.15746","description":"<p>Unsupervised/self-supervised graph representation learning is critical for\ndownstream node- and graph-level classification tasks. Global structure of\ngraphs helps discriminating representations and existing methods mainly utilize\nthe global structure by imposing additional supervisions. However, their global\nsemantics are usually invariant for all nodes/graphs and they fail to\nexplicitly embed the global semantics to enrich the representations. In this\npaper, we propose Omni-Granular Ego-Semantic Propagation for Self-Supervised\nGraph Representation Learning (OEPG). Specifically, we introduce\ninstance-adaptive global-aware ego-semantic descriptors, leveraging the first-\nand second-order feature differences between each node/graph and hierarchical\nglobal clusters of the entire graph dataset. The descriptors can be explicitly\nintegrated into local graph convolution as new neighbor nodes. Besides, we\ndesign an omni-granular normalization on the whole scales and hierarchies of\nthe ego-semantic to assign attentional weight to each descriptor from an\nomni-granular perspective. Specialized pretext tasks and cross-iteration\nmomentum update are further developed for local-global mutual adaptation. In\ndownstream tasks, OEPG consistently achieves the best performance with a 2%~6%\naccuracy gain on multiple datasets cross scales and domains. Notably, OEPG also\ngeneralizes to quantity- and topology-imbalance scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Ling Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_S/0/1/0/all/0/1\">Shenda Hong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial synthesis based data-augmentation for code-switched spoken language identification. (arXiv:2205.15747v1 [eess.AS])","link":"http://arxiv.org/abs/2205.15747","description":"<p>Spoken Language Identification (LID) is an important sub-task of Automatic\nSpeech Recognition(ASR) that is used to classify the language(s) in an audio\nsegment. Automatic LID plays an useful role in multilingual countries. In\nvarious countries, identifying a language becomes hard, due to the multilingual\nscenario where two or more than two languages are mixed together during\nconversation. Such phenomenon of speech is called as code-mixing or\ncode-switching. This nature is followed not only in India but also in many\nAsian countries. Such code-mixed data is hard to find, which further reduces\nthe capabilities of the spoken LID. Due to the lack of avalibility of this\ncode-mixed data, it becomes a minority class in LID task. Hence, this work\nprimarily addresses this problem using data augmentation as a solution on the\nminority code-switched class. This study focuses on Indic language code-mixed\nwith English. Spoken LID is performed on Hindi, code-mixed with English. This\nresearch proposes Generative Adversarial Network (GAN) based data augmentation\ntechnique performed using Mel spectrograms for audio data. GANs have already\nbeen proven to be accurate in representing the real data distribution in the\nimage domain. Proposed research exploits these capabilities of GANs in speech\ndomains such as speech classification, automatic speech recognition,etc. GANs\nare trained to generate Mel spectrograms of the minority code-mixed class which\nare then used to augment data for the classifier. Utilizing GANs give an\noverall improvement on Unweighted Average Recall by an amount of 3.5\\% as\ncompared to a Convolutional Recurrent Neural Network (CRNN) classifier used as\nthe baseline reference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Shastri_P/0/1/0/all/0/1\">Parth Shastri</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Patil_C/0/1/0/all/0/1\">Chirag Patil</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wanere_P/0/1/0/all/0/1\">Poorval Wanere</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mahajan_D/0/1/0/all/0/1\">Dr. Shrinivas Mahajan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bhatt_D/0/1/0/all/0/1\">Dr. Abhishek Bhatt</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sailor_D/0/1/0/all/0/1\">Dr. Hardik Sailor</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Non-Iterative Recovery from Nonlinear Observations using Generative Models. (arXiv:2205.15749v1 [cs.LG])","link":"http://arxiv.org/abs/2205.15749","description":"<p>In this paper, we aim to estimate the direction of an underlying signal from\nits nonlinear observations following the semi-parametric single index model\n(SIM). Unlike conventional compressed sensing where the signal is assumed to be\nsparse, we assume that the signal lies in the range of an $L$-Lipschitz\ncontinuous generative model with bounded $k$-dimensional inputs. This is mainly\nmotivated by the tremendous success of deep generative models in various real\napplications. Our reconstruction method is non-iterative (though approximating\nthe projection step may use an iterative procedure) and highly efficient, and\nit is shown to attain the near-optimal statistical rate of order $\\sqrt{(k \\log\nL)/m}$, where $m$ is the number of measurements. We consider two specific\ninstances of the SIM, namely noisy $1$-bit and cubic measurement models, and\nperform experiments on image datasets to demonstrate the efficacy of our\nmethod. In particular, for the noisy $1$-bit measurement model, we show that\nour non-iterative method significantly outperforms a state-of-the-art iterative\nmethod in terms of both accuracy and efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiulong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhaoqiang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating the Role of Image Retrieval for Visual Localization -- An exhaustive benchmark. (arXiv:2205.15761v1 [cs.CV])","link":"http://arxiv.org/abs/2205.15761","description":"<p>Visual localization, i.e., camera pose estimation in a known scene, is a core\ncomponent of technologies such as autonomous driving and augmented reality.\nState-of-the-art localization approaches often rely on image retrieval\ntechniques for one of two purposes: (1) provide an approximate pose estimate or\n(2) determine which parts of the scene are potentially visible in a given query\nimage. It is common practice to use state-of-the-art image retrieval algorithms\nfor both of them. These algorithms are often trained for the goal of retrieving\nthe same landmark under a large range of viewpoint changes which often differs\nfrom the requirements of visual localization. In order to investigate the\nconsequences for visual localization, this paper focuses on understanding the\nrole of image retrieval for multiple visual localization paradigms. First, we\nintroduce a novel benchmark setup and compare state-of-the-art retrieval\nrepresentations on multiple datasets using localization performance as metric.\nSecond, we investigate several definitions of \"ground truth\" for image\nretrieval. Using these definitions as upper bounds for the visual localization\nparadigms, we show that there is still sgnificant room for improvement. Third,\nusing these tools and in-depth analysis, we show that retrieval performance on\nclassical landmark retrieval or place recognition tasks correlates only for\nsome but not all paradigms to localization performance. Finally, we analyze the\neffects of blur and dynamic scenes in the images. We conclude that there is a\nneed for retrieval approaches specifically designed for localization paradigms.\nOur benchmark and evaluation protocols are available at\nhttps://github.com/naver/kapture-localization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Humenberger_M/0/1/0/all/0/1\">Martin Humenberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cabon_Y/0/1/0/all/0/1\">Yohann Cabon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pion_N/0/1/0/all/0/1\">No&#xe9; Pion</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weinzaepfel_P/0/1/0/all/0/1\">Philippe Weinzaepfel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Donghwan Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guerin_N/0/1/0/all/0/1\">Nicolas Gu&#xe9;rin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sattler_T/0/1/0/all/0/1\">Torsten Sattler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Csurka_G/0/1/0/all/0/1\">Gabriela Csurka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SymFormer: End-to-end symbolic regression using transformer-based architecture. (arXiv:2205.15764v1 [cs.CV])","link":"http://arxiv.org/abs/2205.15764","description":"<p>Novel view synthesis is a long-standing problem. In this work, we consider a\nvariant of the problem where we are given only a few context views sparsely\ncovering a scene or an object. The goal is to predict novel viewpoints in the\nscene, which requires learning priors. The current state of the art is based on\nNeural Radiance Fields (NeRFs), and while achieving impressive results, the\nmethods suffer from long training times as they require evaluating thousands of\n3D point samples via a deep neural network for each image. We propose a 2D-only\nmethod that maps multiple context views and a query pose to a new image in a\nsingle pass of a neural network. Our model uses a two-stage architecture\nconsisting of a codebook and a transformer model. The codebook is used to embed\nindividual images into a smaller latent space, and the transformer solves the\nview synthesis task in this more compact space. To train our model efficiently,\nwe introduce a novel branching attention mechanism that allows us to use the\nsame model not only for neural rendering but also for camera pose estimation.\nExperimental results on real-world scenes show that our approach is competitive\ncompared to NeRF-based methods while not reasoning in 3D, and it is faster to\ntrain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vastl/0/1/0/all/0/1\">Vastl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin/0/1/0/all/0/1\">Martin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulh%5C%27anek/0/1/0/all/0/1\">Kulh&#xe1;nek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jon%5C%27a%5Cv%7Bs%7D/0/1/0/all/0/1\">Jon&#xe1;&#x161;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kubal%5C%27ik/0/1/0/all/0/1\">Kubal&#xed;k</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji%5Cv%7Br%7D%5C%27i/0/1/0/all/0/1\">Ji&#x159;&#xed;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Derner/0/1/0/all/0/1\">Derner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erik/0/1/0/all/0/1\">Erik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Babu%5Cv%7Bs%7Dka/0/1/0/all/0/1\">Babu&#x161;ka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robert/0/1/0/all/0/1\">Robert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SAMURAI: Shape And Material from Unconstrained Real-world Arbitrary Image collections. (arXiv:2205.15768v1 [cs.CV])","link":"http://arxiv.org/abs/2205.15768","description":"<p>Inverse rendering of an object under entirely unknown capture conditions is a\nfundamental challenge in computer vision and graphics. Neural approaches such\nas NeRF have achieved photorealistic results on novel view synthesis, but they\nrequire known camera poses. Solving this problem with unknown camera poses is\nhighly challenging as it requires joint optimization over shape, radiance, and\npose. This problem is exacerbated when the input images are captured in the\nwild with varying backgrounds and illuminations. Standard pose estimation\ntechniques fail in such image collections in the wild due to very few estimated\ncorrespondences across images. Furthermore, NeRF cannot relight a scene under\nany illumination, as it operates on radiance (the product of reflectance and\nillumination). We propose a joint optimization framework to estimate the shape,\nBRDF, and per-image camera pose and illumination. Our method works on\nin-the-wild online image collections of an object and produces relightable 3D\nassets for several use-cases such as AR/VR. To our knowledge, our method is the\nfirst to tackle this severely unconstrained task with minimal user interaction.\nProject page: https://markboss.me/publication/2022-samurai/ Video:\nhttps://youtu.be/LlYuGDjXp-8\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Boss_M/0/1/0/all/0/1\">Mark Boss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Engelhardt_A/0/1/0/all/0/1\">Andreas Engelhardt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kar_A/0/1/0/all/0/1\">Abhishek Kar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuanzhen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_D/0/1/0/all/0/1\">Deqing Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barron_J/0/1/0/all/0/1\">Jonathan T. Barron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lensch_H/0/1/0/all/0/1\">Hendrik P. A. Lensch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jampani_V/0/1/0/all/0/1\">Varun Jampani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Concept-level Debugging of Part-Prototype Networks. (arXiv:2205.15769v1 [cs.LG])","link":"http://arxiv.org/abs/2205.15769","description":"<p>Part-prototype Networks (ProtoPNets) are concept-based classifiers designed\nto achieve the same performance as black-box models without compromising\ntransparency. ProtoPNets compute predictions based on similarity to\nclass-specific part-prototypes learned to recognize parts of training examples,\nmaking it easy to faithfully determine what examples are responsible for any\ntarget prediction and why. However, like other models, they are prone to\npicking up confounds and shortcuts from the data, thus suffering from\ncompromised prediction accuracy and limited generalization. We propose\nProtoPDebug, an effective concept-level debugger for ProtoPNets in which a\nhuman supervisor, guided by the model's explanations, supplies feedback in the\nform of what part-prototypes must be forgotten or kept, and the model is\nfine-tuned to align with this supervision. An extensive empirical evaluation on\nsynthetic and real-world data shows that ProtoPDebug outperforms\nstate-of-the-art debuggers for a fraction of the annotation cost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bontempelli_A/0/1/0/all/0/1\">Andrea Bontempelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teso_S/0/1/0/all/0/1\">Stefano Teso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giunchiglia_F/0/1/0/all/0/1\">Fausto Giunchiglia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Passerini_A/0/1/0/all/0/1\">Andrea Passerini</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The hybrid approach -- Convolutional Neural Networks and Expectation Maximization Algorithm -- for Tomographic Reconstruction of Hyperspectral Images. (arXiv:2205.15772v1 [eess.IV])","link":"http://arxiv.org/abs/2205.15772","description":"<p>We present a simple but novel hybrid approach to hyperspectral data cube\nreconstruction from computed tomography imaging spectrometry (CTIS) images that\nsequentially combines neural networks and the iterative Expectation\nMaximization (EM) algorithm. We train and test the ability of the method to\nreconstruct data cubes of $100\\times100\\times25$ and $100\\times100\\times100$\nvoxels, corresponding to 25 and 100 spectral channels, from simulated CTIS\nimages generated by our CTIS simulator. The hybrid approach utilizes the\ninherent strength of the Convolutional Neural Network (CNN) with regard to\nnoise and its ability to yield consistent reconstructions and make use of the\nEM algorithm's ability to generalize to spectral images of any object without\ntraining. The hybrid approach achieves better performance than both the CNNs\nand EM alone for seen (included in CNN training) and unseen (excluded from CNN\ntraining) cubes for both the 25- and 100-channel cases. For the 25 spectral\nchannels, the improvements from CNN to the hybrid model (CNN + EM) in terms of\nthe mean-squared errors are between 14-26%. For 100 spectral channels, the\nimprovements between 19-40% are attained with the largest improvement of 40%\nfor the unseen data, to which the CNNs are not exposed during the training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ahlebaek_M/0/1/0/all/0/1\">Mads J. Ahleb&#xe6;k</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Peters_M/0/1/0/all/0/1\">Mads S. Peters</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_W/0/1/0/all/0/1\">Wei-Chih Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Frandsen_M/0/1/0/all/0/1\">Mads T. Frandsen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Eriksen_R/0/1/0/all/0/1\">Ren&#xe9; L. Eriksen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jorgensen_B/0/1/0/all/0/1\">Bjarke J&#xf8;rgensen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Co-Training for Unsupervised Domain Adaptation of Semantic Segmentation Models. (arXiv:2205.15781v1 [cs.CV])","link":"http://arxiv.org/abs/2205.15781","description":"<p>Semantic image segmentation is addressed by training deep models. Since\nsupervised training draws to a curse of human-based image labeling, using\nsynthetic images with automatically generated ground truth together with\nunlabeled real-world images is a promising alternative. This implies to address\nan unsupervised domain adaptation (UDA) problem. In this paper, we proposed a\nnew co-training process for synth-to-real UDA of semantic segmentation models.\nFirst, we design a self-training procedure which provides two initial models.\nThen, we keep training these models in a collaborative manner for obtaining the\nfinal model. The overall process treats the deep models as black boxes and\ndrives their collaboration at the level of pseudo-labeled target images, {\\ie},\nneither modifying loss functions is required, nor explicit feature alignment.\nWe test our proposal on standard synthetic and real-world datasets. Our\nco-training shows improvements of 15-20 percentage points of mIoU over\nbaselines, so establishing new state-of-the-art results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gomez_J/0/1/0/all/0/1\">Jose L. G&#xf3;mez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Villalonga_G/0/1/0/all/0/1\">Gabriel Villalonga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lopez_A/0/1/0/all/0/1\">Antonio M. L&#xf3;pez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Deep Fake Detection for Trial Courts. (arXiv:2205.15792v1 [cs.CV])","link":"http://arxiv.org/abs/2205.15792","description":"<p>Recently, image manipulation has achieved rapid growth due to the advancement\nof sophisticated image editing tools. A recent surge of generated fake imagery\nand videos using neural networks is DeepFake. DeepFake algorithms can create\nfake images and videos that humans cannot distinguish from authentic ones.\n(GANs) have been extensively used for creating realistic images without\naccessing the original images. Therefore, it is become essential to detect fake\nvideos to avoid spreading false information. This paper presents a survey of\nmethods used to detect DeepFakes and datasets available for detecting DeepFakes\nin the literature to date. We present extensive discussions and research trends\nrelated to DeepFake technologies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Celebi_N/0/1/0/all/0/1\">Naciye Celebi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qingzhong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karatoprak_M/0/1/0/all/0/1\">Muhammed Karatoprak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrasting quadratic assignments for set-based representation learning. (arXiv:2205.15814v1 [cs.CV])","link":"http://arxiv.org/abs/2205.15814","description":"<p>The standard approach to contrastive learning is to maximize the agreement\nbetween different views of the data. The views are ordered in pairs, such that\nthey are either positive, encoding different views of the same object, or\nnegative, corresponding to views of different objects. The supervisory signal\ncomes from maximizing the total similarity over positive pairs, while the\nnegative pairs are needed to avoid collapse. In this work, we note that the\napproach of considering individual pairs cannot account for both intra-set and\ninter-set similarities when the sets are formed from the views of the data. It\nthus limits the information content of the supervisory signal available to\ntrain representations. We propose to go beyond contrasting individual pairs of\nobjects by focusing on contrasting objects as sets. For this, we use\ncombinatorial quadratic assignment theory designed to evaluate set and graph\nsimilarities and derive set-contrastive objective as a regularizer for\ncontrastive learning methods. We conduct experiments and demonstrate that our\nmethod improves learned representations for the tasks of metric learning and\nself-supervised classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moskalev_A/0/1/0/all/0/1\">Artem Moskalev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sosnovik_I/0/1/0/all/0/1\">Ivan Sosnovik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fischer_V/0/1/0/all/0/1\">Volker Fischer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smeulders_A/0/1/0/all/0/1\">Arnold Smeulders</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Image Representation Learning with Deep Latent Particles. (arXiv:2205.15821v1 [cs.CV])","link":"http://arxiv.org/abs/2205.15821","description":"<p>We propose a new representation of visual data that disentangles object\nposition from appearance. Our method, termed Deep Latent Particles (DLP),\ndecomposes the visual input into low-dimensional latent ``particles'', where\neach particle is described by its spatial location and features of its\nsurrounding region. To drive learning of such representations, we follow a\nVAE-based approach and introduce a prior for particle positions based on a\nspatial-softmax architecture, and a modification of the evidence lower bound\nloss inspired by the Chamfer distance between particles. We demonstrate that\nour DLP representations are useful for downstream tasks such as unsupervised\nkeypoint (KP) detection, image manipulation, and video prediction for scenes\ncomposed of multiple dynamic objects. In addition, we show that our\nprobabilistic interpretation of the problem naturally provides uncertainty\nestimates for particle locations, which can be used for model selection, among\nother tasks. Videos and code are available:\nhttps://taldatech.github.io/deep-latent-particles-web/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Daniel_T/0/1/0/all/0/1\">Tal Daniel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tamar_A/0/1/0/all/0/1\">Aviv Tamar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Surface Analysis with Vision Transformers. (arXiv:2205.15836v1 [cs.CV])","link":"http://arxiv.org/abs/2205.15836","description":"<p>The extension of convolutional neural networks (CNNs) to non-Euclidean\ngeometries has led to multiple frameworks for studying manifolds. Many of those\nmethods have shown design limitations resulting in poor modelling of long-range\nassociations, as the generalisation of convolutions to irregular surfaces is\nnon-trivial. Recent state-of-the-art performance of Vision Transformers (ViTs)\ndemonstrates that a general-purpose architecture, which implements\nself-attention, could replace the local feature learning operations of CNNs.\nMotivated by the success of attention-modelling in computer vision, we extend\nViTs to surfaces by reformulating the task of surface learning as a\nsequence-to-sequence problem and propose a patching mechanism for surface\nmeshes. We validate the performance of the proposed Surface Vision Transformer\n(SiT) on two brain age prediction tasks in the developing Human Connectome\nProject (dHCP) dataset and investigate the impact of pre-training on model\nperformance. Experiments show that the SiT outperforms many surface CNNs, while\nindicating some evidence of general transformation invariance. Code available\nat https://github.com/metrics-lab/surface-vision-transformers\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dahan_S/0/1/0/all/0/1\">Simon Dahan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_L/0/1/0/all/0/1\">Logan Z. J. Williams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fawaz_A/0/1/0/all/0/1\">Abdulah Fawaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rueckert_D/0/1/0/all/0/1\">Daniel Rueckert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robinson_E/0/1/0/all/0/1\">Emma C. Robinson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"D$^2$NeRF: Self-Supervised Decoupling of Dynamic and Static Objects from a Monocular Video. (arXiv:2205.15838v1 [cs.CV])","link":"http://arxiv.org/abs/2205.15838","description":"<p>Given a monocular video, segmenting and decoupling dynamic objects while\nrecovering the static environment is a widely studied problem in machine\nintelligence. Existing solutions usually approach this problem in the image\ndomain, limiting their performance and understanding of the environment. We\nintroduce Decoupled Dynamic Neural Radiance Field (D$^2$NeRF), a\nself-supervised approach that takes a monocular video and learns a 3D scene\nrepresentation which decouples moving objects, including their shadows, from\nthe static background. Our method represents the moving objects and the static\nbackground by two separate neural radiance fields with only one allowing for\ntemporal changes. A naive implementation of this approach leads to the dynamic\ncomponent taking over the static one as the representation of the former is\ninherently more general and prone to overfitting. To this end, we propose a\nnovel loss to promote correct separation of phenomena. We further propose a\nshadow field network to detect and decouple dynamically moving shadows. We\nintroduce a new dataset containing various dynamic objects and shadows and\ndemonstrate that our method can achieve better performance than\nstate-of-the-art approaches in decoupling dynamic and static 3D objects,\nocclusion and shadow removal, and image segmentation for moving objects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tianhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_F/0/1/0/all/0/1\">Fangcheng Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tagliasacchi_A/0/1/0/all/0/1\">Andrea Tagliasacchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cole_F/0/1/0/all/0/1\">Forrester Cole</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oztireli_C/0/1/0/all/0/1\">Cengiz Oztireli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Geo-Neus: Geometry-Consistent Neural Implicit Surfaces Learning for Multi-view Reconstruction. (arXiv:2205.15848v1 [cs.CV])","link":"http://arxiv.org/abs/2205.15848","description":"<p>Recently, neural implicit surfaces learning by volume rendering has become\npopular for multi-view reconstruction. However, one key challenge remains:\nexisting approaches lack explicit multi-view geometry constraints, hence\nusually fail to generate geometry consistent surface reconstruction. To address\nthis challenge, we propose geometry-consistent neural implicit surfaces\nlearning for multi-view reconstruction. We theoretically analyze that there\nexists a gap between the volume rendering integral and point-based signed\ndistance function (SDF) modeling. To bridge this gap, we directly locate the\nzero-level set of SDF networks and explicitly perform multi-view geometry\noptimization by leveraging the sparse geometry from structure from motion (SFM)\nand photometric consistency in multi-view stereo. This makes our SDF\noptimization unbiased and allows the multi-view geometry constraints to focus\non the true surface optimization. Extensive experiments show that our proposed\nmethod achieves high-quality surface reconstruction in both complex thin\nstructures and large smooth regions, thus outperforming the state-of-the-arts\nby a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_Q/0/1/0/all/0/1\">Qiancheng Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qingshan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ong_Y/0/1/0/all/0/1\">Yew-Soon Ong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_W/0/1/0/all/0/1\">Wenbing Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Snapture -- A Novel Neural Architecture for Combined Static and Dynamic Hand Gesture Recognition. (arXiv:2205.15862v1 [cs.CV])","link":"http://arxiv.org/abs/2205.15862","description":"<p>As robots are expected to get more involved in people's everyday lives,\nframeworks that enable intuitive user interfaces are in demand. Hand gesture\nrecognition systems provide a natural way of communication and, thus, are an\nintegral part of seamless Human-Robot Interaction (HRI). Recent years have\nwitnessed an immense evolution of computational models powered by deep\nlearning. However, state-of-the-art models fall short in expanding across\ndifferent gesture domains, such as emblems and co-speech. In this paper, we\npropose a novel hybrid hand gesture recognition system. Our architecture\nenables learning both static and dynamic gestures: by capturing a so-called\n\"snapshot\" of the gesture performance at its peak, we integrate the hand pose\nalong with the dynamic movement. Moreover, we present a method for analyzing\nthe motion profile of a gesture to uncover its dynamic characteristics and\nwhich allows regulating a static channel based on the amount of motion. Our\nevaluation demonstrates the superiority of our approach on two gesture\nbenchmarks compared to a CNNLSTM baseline. We also provide an analysis on a\ngesture class basis that unveils the potential of our Snapture architecture for\nperformance improvements. Thanks to its modular implementation, our framework\nallows the integration of other multimodal data like facial expressions and\nhead tracking, which are important cues in HRI scenarios, into one\narchitecture. Thus, our work contributes both to gesture recognition research\nand machine learning applications for non-verbal communication with robots.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ali_H/0/1/0/all/0/1\">Hassan Ali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jirak_D/0/1/0/all/0/1\">Doreen Jirak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wermter_S/0/1/0/all/0/1\">Stefan Wermter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Braille Letter Reading: A Benchmark for Spatio-Temporal Pattern Recognition on Neuromorphic Hardware. (arXiv:2205.15864v1 [cs.CV])","link":"http://arxiv.org/abs/2205.15864","description":"<p>Spatio-temporal pattern recognition is a fundamental ability of the brain\nwhich is required for numerous real-world applications. Recent deep learning\napproaches have reached outstanding accuracy in such tasks, but their\nimplementation on conventional embedded solutions is still very computationally\nand energy expensive. Tactile sensing in robotic applications is a\nrepresentative example where real-time processing and energy-efficiency are\nrequired. Following a brain-inspired computing approach, we propose a new\nbenchmark for spatio-temporal tactile pattern recognition at the edge through\nbraille letters reading. We recorded a new braille letters dataset based on the\ncapacitive tactile sensors/fingertip of the iCub robot, then we investigated\nthe importance of temporal information and the impact of event-based encoding\nfor spike-based/event-based computation. Afterwards, we trained and compared\nfeed-forward and recurrent spiking neural networks (SNNs) offline using\nback-propagation through time with surrogate gradients, then we deployed them\non the Intel Loihi neuromorphic chip for fast and efficient inference. We\nconfronted our approach to standard classifiers, in particular to a Long\nShort-Term Memory (LSTM) deployed on the embedded Nvidia Jetson GPU in terms of\nclassification accuracy, power/energy consumption and computational delay. Our\nresults show that the LSTM outperforms the recurrent SNN in terms of accuracy\nby 14%. However, the recurrent SNN on Loihi is 237 times more energy-efficient\nthan the LSTM on Jetson, requiring an average power of only 31mW. This work\nproposes a new benchmark for tactile sensing and highlights the challenges and\nopportunities of event-based encoding, neuromorphic hardware and spike-based\ncomputing for spatio-temporal pattern recognition at the edge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Muller_Cleve_S/0/1/0/all/0/1\">Simon F Muller-Cleve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fra_V/0/1/0/all/0/1\">Vittorio Fra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khacef_L/0/1/0/all/0/1\">Lyes Khacef</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pequeno_Zurro_A/0/1/0/all/0/1\">Alejandro Pequeno-Zurro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klepatsch_D/0/1/0/all/0/1\">Daniel Klepatsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Forno_E/0/1/0/all/0/1\">Evelina Forno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ivanovich_D/0/1/0/all/0/1\">Diego G Ivanovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rastogi_S/0/1/0/all/0/1\">Shavika Rastogi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Urgese_G/0/1/0/all/0/1\">Gianvito Urgese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zenke_F/0/1/0/all/0/1\">Friedemann Zenke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bartolozzi_C/0/1/0/all/0/1\">Chiara Bartolozzi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Review of Mobile Mapping Systems: From Sensors to Applications. (arXiv:2205.15865v1 [cs.CV])","link":"http://arxiv.org/abs/2205.15865","description":"<p>The evolution of mobile mapping systems (MMSs) has gained more attention in\nthe past few decades. MMSs have been widely used to provide valuable assets in\ndifferent applications. This has been facilitated by the wide availability of\nlow-cost sensors, the advances in computational resources, the maturity of the\nmapping algorithms, and the need for accurate and on-demand geographic\ninformation system (GIS) data and digital maps. Many MMSs combine hybrid\nsensors to provide a more informative, robust, and stable solution by\ncomplementing each other. In this paper, we present a comprehensive review of\nthe modern MMSs by focusing on 1) the types of sensors and platforms, where we\ndiscuss their capabilities, limitations, and also provide a comprehensive\noverview of recent MMS technologies available in the market, 2) highlighting\nthe general workflow to process any MMS data, 3) identifying the different use\ncases of mobile mapping technology by reviewing some of the common\napplications, and 4) presenting a discussion on the benefits, challenges, and\nshare our views on the potential research directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Elhashash_M/0/1/0/all/0/1\">Mostafa Elhashash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Albanwan_H/0/1/0/all/0/1\">Hessah Albanwan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_R/0/1/0/all/0/1\">Rongjun Qin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Median Pixel Difference Convolutional Network for Robust Face Recognition. (arXiv:2205.15867v1 [cs.CV])","link":"http://arxiv.org/abs/2205.15867","description":"<p>Face recognition is one of the most active tasks in computer vision and has\nbeen widely used in the real world. With great advances made in convolutional\nneural networks (CNN), lots of face recognition algorithms have achieved high\naccuracy on various face datasets. However, existing face recognition\nalgorithms based on CNNs are vulnerable to noise. Noise corrupted image\npatterns could lead to false activations, significantly decreasing face\nrecognition accuracy in noisy situations. To equip CNNs with built-in\nrobustness to noise of different levels, we proposed a Median Pixel Difference\nConvolutional Network (MeDiNet) by replacing some traditional convolutional\nlayers with the proposed novel Median Pixel Difference Convolutional Layer\n(MeDiConv) layer. The proposed MeDiNet integrates the idea of traditional\nmultiscale median filtering with deep CNNs. The MeDiNet is tested on the four\nface datasets (LFW, CA-LFW, CP-LFW, and YTF) with versatile settings on blur\nkernels, noise intensities, scales, and JPEG quality factors. Extensive\nexperiments show that our MeDiNet can effectively remove noisy pixels in the\nfeature map and suppress the negative impact of noise, leading to achieving\nlimited accuracy loss under these practical noises compared with the standard\nCNN under clean conditions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiehua Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Z/0/1/0/all/0/1\">Zhuo Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Li Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers. (arXiv:2205.15868v1 [cs.CV])","link":"http://arxiv.org/abs/2205.15868","description":"<p>Large-scale pretrained transformers have created milestones in text (GPT-3)\nand text-to-image (DALL-E and CogView) generation. Its application to video\ngeneration is still facing many challenges: The potential huge computation cost\nmakes the training from scratch unaffordable; The scarcity and weak relevance\nof text-video datasets hinder the model understanding complex movement\nsemantics. In this work, we present 9B-parameter transformer CogVideo, trained\nby inheriting a pretrained text-to-image model, CogView2. We also propose\nmulti-frame-rate hierarchical training strategy to better align text and video\nclips. As (probably) the first open-source large-scale pretrained text-to-video\nmodel, CogVideo outperforms all publicly available models at a large margin in\nmachine and human evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hong_W/0/1/0/all/0/1\">Wenyi Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1\">Ming Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1\">Wendi Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xinghan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jie Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D-model ShapeNet Core Classification using Meta-Semantic Learning. (arXiv:2205.15869v1 [cs.CV])","link":"http://arxiv.org/abs/2205.15869","description":"<p>Understanding 3D point cloud models for learning purposes has become an\nimperative challenge for real-world identification such as autonomous driving\nsystems. A wide variety of solutions using deep learning have been proposed for\npoint cloud segmentation, object detection, and classification. These methods,\nhowever, often require a considerable number of model parameters and are\ncomputationally expensive. We study a semantic dimension of given 3D data\npoints and propose an efficient method called Meta-Semantic Learning\n(Meta-SeL). Meta-SeL is an integrated framework that leverages two input 3D\nlocal points (input 3D models and part-segmentation labels), providing a time\nand cost-efficient, and precise projection model for a number of 3D recognition\ntasks. The results indicate that Meta-SeL yields competitive performance in\ncomparison with other complex state-of-the-art work. Moreover, being random\nshuffle invariant, Meta-SeL is resilient to translation as well as jittering\nnoise.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohammadi_F/0/1/0/all/0/1\">Farid Ghareh Mohammadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Cheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shenavarmasouleh_F/0/1/0/all/0/1\">Farzan Shenavarmasouleh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amini_M/0/1/0/all/0/1\">M. Hadi Amini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morkos_B/0/1/0/all/0/1\">Beshoy Morkos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arabnia_H/0/1/0/all/0/1\">Hamid R. Arabnia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FaIRCoP: Facial Image Retrieval using Contrastive Personalization. (arXiv:2205.15870v1 [cs.CV])","link":"http://arxiv.org/abs/2205.15870","description":"<p>Retrieving facial images from attributes plays a vital role in various\nsystems such as face recognition and suspect identification. Compared to other\nimage retrieval tasks, facial image retrieval is more challenging due to the\nhigh subjectivity involved in describing a person's facial features. Existing\nmethods do so by comparing specific characteristics from the user's mental\nimage against the suggested images via high-level supervision such as using\nnatural language. In contrast, we propose a method that uses a relatively\nsimpler form of binary supervision by utilizing the user's feedback to label\nimages as either similar or dissimilar to the target image. Such supervision\nenables us to exploit the contrastive learning paradigm for encapsulating each\nuser's personalized notion of similarity. For this, we propose a novel loss\nfunction optimized online via user feedback. We validate the efficacy of our\nproposed approach using a carefully designed testbed to simulate user feedback\nand a large-scale user study. Our experiments demonstrate that our method\niteratively improves personalization, leading to faster convergence and\nenhanced recommendation relevance, thereby, improving user satisfaction. Our\nproposed framework is also equipped with a user-friendly web interface with a\nreal-time experience for facial image retrieval.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_D/0/1/0/all/0/1\">Devansh Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saini_A/0/1/0/all/0/1\">Aditya Saini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhasin_D/0/1/0/all/0/1\">Drishti Bhasin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhagat_S/0/1/0/all/0/1\">Sarthak Bhagat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uppal_S/0/1/0/all/0/1\">Shagun Uppal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_R/0/1/0/all/0/1\">Rishi Raj Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumaraguru_P/0/1/0/all/0/1\">Ponnurangam Kumaraguru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1\">Rajiv Ratn Shah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Keypoints to Object Landmarks via Self-Training Correspondence: A novel approach to Unsupervised Landmark Discovery. (arXiv:2205.15895v1 [cs.CV])","link":"http://arxiv.org/abs/2205.15895","description":"<p>This paper proposes a novel paradigm for the unsupervised learning of object\nlandmark detectors. Contrary to existing methods that build on auxiliary tasks\nsuch as image generation or equivariance, we propose a self-training approach\nwhere, departing from generic keypoints, a landmark detector and descriptor is\ntrained to improve itself, tuning the keypoints into distinctive landmarks. To\nthis end, we propose an iterative algorithm that alternates between producing\nnew pseudo-labels through feature clustering and learning distinctive features\nfor each pseudo-class through contrastive learning. With a shared backbone for\nthe landmark detector and descriptor, the keypoint locations progressively\nconverge to stable landmarks, filtering those less stable. Compared to previous\nworks, our approach can learn points that are more flexible in terms of\ncapturing large viewpoint changes. We validate our method on a variety of\ndifficult datasets, including LS3D, BBCPose, Human3.6M and PennAction,\nachieving new state of the art results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mallis_D/0/1/0/all/0/1\">Dimitrios Mallis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_E/0/1/0/all/0/1\">Enrique Sanchez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bell_M/0/1/0/all/0/1\">Matt Bell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tzimiropoulos_G/0/1/0/all/0/1\">Georgios Tzimiropoulos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inferring 3D change detection from bitemporal optical images. (arXiv:2205.15903v1 [eess.IV])","link":"http://arxiv.org/abs/2205.15903","description":"<p>Change detection is one of the most active research areas in Remote Sensing\n(RS). Most of the recently developed change detection methods are based on deep\nlearning (DL) algorithms. This kind of algorithms is generally focused on\ngenerating two-dimensional (2D) change maps, thus only identifying planimetric\nchanges in land use/land cover (LULC) and not considering nor returning any\ninformation on the corresponding elevation changes. Our work goes one step\nfurther, proposing two novel networks, able to solve simultaneously the 2D and\n3D CD tasks, and the 3DCD dataset, a novel and freely available dataset\nprecisely designed for this multitask. Particularly, the aim of this work is to\nlay the foundations for the development of DL algorithms able to automatically\ninfer an elevation (3D) CD map -- together with a standard 2D CD map --,\nstarting only from a pair of bitemporal optical images. The proposed\narchitectures, to perform the task described before, consist of a\ntransformer-based network, the MultiTask Bitemporal Images Transformer (MTBIT),\nand a deep convolutional network, the Siamese ResUNet (SUNet). Particularly,\nMTBIT is a transformer-based architecture, based on a semantic tokenizer. SUNet\ninstead combines, in a siamese encoder, skip connections and residual layers to\nlearn rich features, capable to solve efficiently the proposed task. These\nmodels are, thus, able to obtain 3D CD maps from two optical images taken at\ndifferent time instants, without the need to rely directly on elevation data\nduring the inference step. Encouraging results, obtained on the novel 3DCD\ndataset, are shown. The code and the 3DCD dataset are available at\n\\url{https://sites.google.com/uniroma1.it/3dchangedetection/home-page}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Marsocci_V/0/1/0/all/0/1\">Valerio Marsocci</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Coletta_V/0/1/0/all/0/1\">Virginia Coletta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ravanelli_R/0/1/0/all/0/1\">Roberta Ravanelli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Scardapane_S/0/1/0/all/0/1\">Simone Scardapane</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Crespi_M/0/1/0/all/0/1\">Mattia Crespi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SAR Despeckling Using Overcomplete Convolutional Networks. (arXiv:2205.15906v1 [cs.CV])","link":"http://arxiv.org/abs/2205.15906","description":"<p>Synthetic Aperture Radar (SAR) despeckling is an important problem in remote\nsensing as speckle degrades SAR images, affecting downstream tasks like\ndetection and segmentation. Recent studies show that convolutional neural\nnetworks(CNNs) outperform classical despeckling methods. Traditional CNNs try\nto increase the receptive field size as the network goes deeper, thus\nextracting global features. However,speckle is relatively small, and increasing\nreceptive field does not help in extracting speckle features. This study\nemploys an overcomplete CNN architecture to focus on learning low-level\nfeatures by restricting the receptive field. The proposed network consists of\nan overcomplete branch to focus on the local structures and an undercomplete\nbranch that focuses on the global structures. We show that the proposed network\nimproves despeckling performance compared to recent despeckling methods on\nsynthetic and real SAR images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Perera_M/0/1/0/all/0/1\">Malsha V. Perera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bandara_W/0/1/0/all/0/1\">Wele Gedara Chaminda Bandara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valanarasu_J/0/1/0/all/0/1\">Jeya Maria Jose Valanarasu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1\">Vishal M. Patel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Competitive Method for Dog Nose-print Re-identification. (arXiv:2205.15934v1 [cs.CV])","link":"http://arxiv.org/abs/2205.15934","description":"<p>Vision-based pattern identification (such as face, fingerprint, iris etc.)\nhas been successfully applied in human biometrics for a long history. However,\ndog nose-print authentication is a challenging problem since the lack of a\nlarge amount of labeled data. For that, this paper presents our proposed\nmethods for dog nose-print authentication (Re-ID) task in CVPR 2022 pet\nbiometric challenge. First, considering the problem that each class only with\nfew samples in the training set, we propose an automatic offline data\naugmentation strategy. Then, for the difference in sample styles between the\ntraining and test datasets, we employ joint cross-entropy, triplet and\npair-wise circle losses function for network optimization. Finally, with\nmultiple models ensembled adopted, our methods achieve 86.67\\% AUC on the test\nset. Codes are available at https://github.com/muzishen/Pet-ReID-IMAG.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_F/0/1/0/all/0/1\">Fei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zijun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_X/0/1/0/all/0/1\">Xiaode Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiayi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1\">Xiaoyu Du</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Skeleton-based Action Recognition via Temporal-Channel Aggregation. (arXiv:2205.15936v1 [cs.CV])","link":"http://arxiv.org/abs/2205.15936","description":"<p>Skeleton-based action recognition methods are limited by the semantic\nextraction of spatio-temporal skeletal maps. However, current methods have\ndifficulty in effectively combining features from both temporal and spatial\ngraph dimensions and tend to be thick on one side and thin on the other. In\nthis paper, we propose a Temporal-Channel Aggregation Graph Convolutional\nNetworks (TCA-GCN) to learn spatial and temporal topologies dynamically and\nefficiently aggregate topological features in different temporal and channel\ndimensions for skeleton-based action recognition. We use the Temporal\nAggregation module to learn temporal dimensional features and the Channel\nAggregation module to efficiently combine spatial dynamic topological features\nlearned using Channel-wise with temporal dynamic topological features. In\naddition, we extract multi-scale skeletal features on temporal modeling and\nfuse them with priori skeletal knowledge with an attention mechanism. Extensive\nexperiments show that our model results outperform state-of-the-art methods on\nthe NTU RGB+D, NTU RGB+D 120, and NW-UCLA datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shengqin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongji Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Fenglin Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1\">Minghao Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yu Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Voxel Field Fusion for 3D Object Detection. (arXiv:2205.15938v1 [cs.CV])","link":"http://arxiv.org/abs/2205.15938","description":"<p>In this work, we present a conceptually simple yet effective framework for\ncross-modality 3D object detection, named voxel field fusion. The proposed\napproach aims to maintain cross-modality consistency by representing and fusing\naugmented image features as a ray in the voxel field. To this end, the\nlearnable sampler is first designed to sample vital features from the image\nplane that are projected to the voxel grid in a point-to-ray manner, which\nmaintains the consistency in feature representation with spatial context. In\naddition, ray-wise fusion is conducted to fuse features with the supplemental\ncontext in the constructed voxel field. We further develop mixed augmentor to\nalign feature-variant transformations, which bridges the modality gap in data\naugmentation. The proposed framework is demonstrated to achieve consistent\ngains in various benchmarks and outperforms previous fusion-based methods on\nKITTI and nuScenes datasets. Code is made available at\nhttps://github.com/dvlab-research/VFF.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1\">Xiaojuan Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yukang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zeming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\">Jiaya Jia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Memory-efficient Segmentation of High-resolution Volumetric MicroCT Images. (arXiv:2205.15941v1 [eess.IV])","link":"http://arxiv.org/abs/2205.15941","description":"<p>In recent years, 3D convolutional neural networks have become the dominant\napproach for volumetric medical image segmentation. However, compared to their\n2D counterparts, 3D networks introduce substantially more training parameters\nand higher requirement for the GPU memory. This has become a major limiting\nfactor for designing and training 3D networks for high-resolution volumetric\nimages. In this work, we propose a novel memory-efficient network architecture\nfor 3D high-resolution image segmentation. The network incorporates both global\nand local features via a two-stage U-net-based cascaded framework and at the\nfirst stage, a memory-efficient U-net (meU-net) is developed. The features\nlearnt at the two stages are connected via post-concatenation, which further\nimproves the information flow. The proposed segmentation method is evaluated on\nan ultra high-resolution microCT dataset with typically 250 million voxels per\nvolume. Experiments show that it outperforms state-of-the-art 3D segmentation\nmethods in terms of both segmentation accuracy and memory efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yuan Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Blackie_L/0/1/0/all/0/1\">Laura Blackie</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Miguel_Aliaga_I/0/1/0/all/0/1\">Irene Miguel-Aliaga</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bai_W/0/1/0/all/0/1\">Wenjia Bai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Two-Dimensional Quantum Material Identification via Self-Attention and Soft-labeling in Deep Learning. (arXiv:2205.15948v1 [cs.CV])","link":"http://arxiv.org/abs/2205.15948","description":"<p>In quantum machine field, detecting two-dimensional (2D) materials in Silicon\nchips is one of the most critical problems. Instance segmentation can be\nconsidered as a potential approach to solve this problem. However, similar to\nother deep learning methods, the instance segmentation requires a large scale\ntraining dataset and high quality annotation in order to achieve a considerable\nperformance. In practice, preparing the training dataset is a challenge since\nannotators have to deal with a large image, e.g 2K resolution, and extremely\ndense objects in this problem. In this work, we present a novel method to\ntackle the problem of missing annotation in instance segmentation in 2D quantum\nmaterial identification. We propose a new mechanism for automatically detecting\nfalse negative objects and an attention based loss strategy to reduce the\nnegative impact of these objects contributing to the overall loss function. We\nexperiment on the 2D material detection datasets, and the experiments show our\nmethod outperforms previous works.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_X/0/1/0/all/0/1\">Xuan Bac Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bisht_A/0/1/0/all/0/1\">Apoorva Bisht</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Churchill_H/0/1/0/all/0/1\">Hugh Churchill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luu_K/0/1/0/all/0/1\">Khoa Luu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CropMix: Sampling a Rich Input Distribution via Multi-Scale Cropping. (arXiv:2205.15955v1 [cs.CV])","link":"http://arxiv.org/abs/2205.15955","description":"<p>We present a simple method, CropMix, for the purpose of producing a rich\ninput distribution from the original dataset distribution. Unlike single random\ncropping, which may inadvertently capture only limited information, or\nirrelevant information, like pure background, unrelated objects, etc, we crop\nan image multiple times using distinct crop scales, thereby ensuring that\nmulti-scale information is captured. The new input distribution, serving as\ntraining data, useful for a number of vision tasks, is then formed by simply\nmixing multiple cropped views. We first demonstrate that CropMix can be\nseamlessly applied to virtually any training recipe and neural network\narchitecture performing classification tasks. CropMix is shown to improve the\nperformance of image classifiers on several benchmark tasks across-the-board\nwithout sacrificing computational simplicity and efficiency. Moreover, we show\nthat CropMix is of benefit to both contrastive learning and masked image\nmodeling towards more powerful representations, where preferable results are\nachieved when learned representations are transferred to downstream tasks. Code\nis available at GitHub.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Junlin Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petersson_L/0/1/0/all/0/1\">Lars Petersson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongdong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reid_I/0/1/0/all/0/1\">Ian Reid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FedHarmony: Unlearning Scanner Bias with Distributed Data. (arXiv:2205.15970v1 [cs.LG])","link":"http://arxiv.org/abs/2205.15970","description":"<p>The ability to combine data across scanners and studies is vital for\nneuroimaging, to increase both statistical power and the representation of\nbiological variability. However, combining datasets across sites leads to two\nchallenges: first, an increase in undesirable non-biological variance due to\nscanner and acquisition differences - the harmonisation problem - and second,\ndata privacy concerns due to the inherently personal nature of medical imaging\ndata, meaning that sharing them across sites may risk violation of privacy\nlaws. To overcome these restrictions, we propose FedHarmony: a harmonisation\nframework operating in the federated learning paradigm. We show that to remove\nthe scanner-specific effects, we only need to share the mean and standard\ndeviation of the learned features, helping to protect individual subjects'\nprivacy. We demonstrate our approach across a range of realistic data\nscenarios, using real multi-site data from the ABIDE dataset, thus showing the\npotential utility of our method for MRI harmonisation across studies. Our code\nis available at https://github.com/nkdinsdale/FedHarmony.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dinsdale_N/0/1/0/all/0/1\">Nicola K Dinsdale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jenkinson_M/0/1/0/all/0/1\">Mark Jenkinson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Namburete_A/0/1/0/all/0/1\">Ana IL Namburete</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text2Human: Text-Driven Controllable Human Image Generation. (arXiv:2205.15996v1 [cs.CV])","link":"http://arxiv.org/abs/2205.15996","description":"<p>Generating high-quality and diverse human images is an important yet\nchallenging task in vision and graphics. However, existing generative models\noften fall short under the high diversity of clothing shapes and textures.\nFurthermore, the generation process is even desired to be intuitively\ncontrollable for layman users. In this work, we present a text-driven\ncontrollable framework, Text2Human, for a high-quality and diverse human\ngeneration. We synthesize full-body human images starting from a given human\npose with two dedicated steps. 1) With some texts describing the shapes of\nclothes, the given human pose is first translated to a human parsing map. 2)\nThe final human image is then generated by providing the system with more\nattributes about the textures of clothes. Specifically, to model the diversity\nof clothing textures, we build a hierarchical texture-aware codebook that\nstores multi-scale neural representations for each type of texture. The\ncodebook at the coarse level includes the structural representations of\ntextures, while the codebook at the fine level focuses on the details of\ntextures. To make use of the learned hierarchical codebook to synthesize\ndesired images, a diffusion-based transformer sampler with mixture of experts\nis firstly employed to sample indices from the coarsest level of the codebook,\nwhich then is used to predict the indices of the codebook at finer levels. The\npredicted indices at different levels are translated to human images by the\ndecoder learned accompanied with hierarchical codebooks. The use of\nmixture-of-experts allows for the generated image conditioned on the\nfine-grained text input. The prediction for finer level indices refines the\nquality of clothing textures. Extensive quantitative and qualitative\nevaluations demonstrate that our proposed framework can generate more diverse\nand realistic human images compared to state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yuming Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shuai Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_H/0/1/0/all/0/1\">Haonan Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wayne Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1\">Chen Change Loy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TransFuser: Imitation with Transformer-Based Sensor Fusion for Autonomous Driving. (arXiv:2205.15997v1 [cs.CV])","link":"http://arxiv.org/abs/2205.15997","description":"<p>How should we integrate representations from complementary sensors for\nautonomous driving? Geometry-based fusion has shown promise for perception\n(e.g. object detection, motion forecasting). However, in the context of\nend-to-end driving, we find that imitation learning based on existing sensor\nfusion methods underperforms in complex driving scenarios with a high density\nof dynamic agents. Therefore, we propose TransFuser, a mechanism to integrate\nimage and LiDAR representations using self-attention. Our approach uses\ntransformer modules at multiple resolutions to fuse perspective view and bird's\neye view feature maps. We experimentally validate its efficacy on a challenging\nnew benchmark with long routes and dense traffic, as well as the official\nleaderboard of the CARLA urban driving simulator. At the time of submission,\nTransFuser outperforms all prior work on the CARLA leaderboard in terms of\ndriving score by a large margin. Compared to geometry-based fusion, TransFuser\nreduces the average collisions per kilometer by 48%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chitta_K/0/1/0/all/0/1\">Kashyap Chitta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prakash_A/0/1/0/all/0/1\">Aditya Prakash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaeger_B/0/1/0/all/0/1\">Bernhard Jaeger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zehao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Renz_K/0/1/0/all/0/1\">Katrin Renz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geiger_A/0/1/0/all/0/1\">Andreas Geiger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cascade Luminance and Chrominance for Image Retouching: More Like Artist. (arXiv:2205.15999v1 [cs.CV])","link":"http://arxiv.org/abs/2205.15999","description":"<p>Photo retouching aims to adjust the luminance, contrast, and saturation of\nthe image to make it more human aesthetically desirable. However, artists'\nactions in photo retouching are difficult to quantitatively analyze. By\ninvestigating their retouching behaviors, we propose a two-stage network that\nbrightens images first and then enriches them in the chrominance plane. Six\npieces of useful information from image EXIF are picked as the network's\ncondition input. Additionally, hue palette loss is added to make the image more\nvibrant. Based on the above three aspects, Luminance-Chrominance Cascading\nNet(LCCNet) makes the machine learning problem of mimicking artists in photo\nretouching more reasonable. Experiments show that our method is effective on\nthe benchmark MIT-Adobe FiveK dataset, and achieves state-of-the-art\nperformance for both quantitative and qualitative evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1\">Hailong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Sibo Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1\">Xi Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1\">Chenyu Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xingyue Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What Knowledge Gets Distilled in Knowledge Distillation?. (arXiv:2205.16004v1 [cs.CV])","link":"http://arxiv.org/abs/2205.16004","description":"<p>Knowledge distillation aims to transfer useful information from a teacher\nnetwork to a student network, with the primary goal of improving the student's\nperformance for the task at hand. Over the years, there has a been a deluge of\nnovel techniques and use cases of knowledge distillation. Yet, despite the\nvarious improvements, there seems to be a glaring gap in the community's\nfundamental understanding of the process. Specifically, what is the knowledge\nthat gets distilled in knowledge distillation? In other words, in what ways\ndoes the student become similar to the teacher? Does it start to localize\nobjects in the same way? Does it get fooled by the same adversarial samples?\nDoes its data invariance properties become similar? Our work presents a\ncomprehensive study to try to answer these questions and more. Our results,\nusing image classification as a case study and three state-of-the-art knowledge\ndistillation techniques, show that knowledge distillation methods can indeed\nindirectly distill other kinds of properties beyond improving task performance.\nBy exploring these questions, we hope for our work to provide a clearer picture\nof what happens during knowledge distillation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ojha_U/0/1/0/all/0/1\">Utkarsh Ojha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yong Jae Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improved Vector Quantized Diffusion Models. (arXiv:2205.16007v1 [cs.CV])","link":"http://arxiv.org/abs/2205.16007","description":"<p>Vector quantized diffusion (VQ-Diffusion) is a powerful generative model for\ntext-to-image synthesis, but sometimes can still generate low-quality samples\nor weakly correlated images with text input. We find these issues are mainly\ndue to the flawed sampling strategy. In this paper, we propose two important\ntechniques to further improve the sample quality of VQ-Diffusion. 1) We explore\nclassifier-free guidance sampling for discrete denoising diffusion model and\npropose a more general and effective implementation of classifier-free\nguidance. 2) We present a high-quality inference strategy to alleviate the\njoint distribution issue in VQ-Diffusion. Finally, we conduct experiments on\nvarious datasets to validate their effectiveness and show that the improved\nVQ-Diffusion suppresses the vanilla version by large margins. We achieve an\n8.44 FID score on MSCOCO, surpassing VQ-Diffusion by 5.42 FID score. When\ntrained on ImageNet, we dramatically improve the FID score from 11.89 to 4.83,\ndemonstrating the superiority of our proposed techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1\">Zhicong Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1\">Shuyang Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Jianmin Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_F/0/1/0/all/0/1\">Fang Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Kymatio: Scattering Transforms in Python. (arXiv:1812.11214v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/1812.11214","description":"<p>The wavelet scattering transform is an invariant signal representation\nsuitable for many signal processing and machine learning applications. We\npresent the Kymatio software package, an easy-to-use, high-performance Python\nimplementation of the scattering transform in 1D, 2D, and 3D that is compatible\nwith modern deep learning frameworks. All transforms may be executed on a GPU\n(in addition to CPU), offering a considerable speed up over CPU\nimplementations. The package also has a small memory footprint, resulting\ninefficient memory usage. The source code, documentation, and examples are\navailable undera BSD license at https://www.kymat.io/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Andreux_M/0/1/0/all/0/1\">Mathieu Andreux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angles_T/0/1/0/all/0/1\">Tom&#xe1;s Angles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Exarchakis_G/0/1/0/all/0/1\">Georgios Exarchakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leonarduzzi_R/0/1/0/all/0/1\">Roberto Leonarduzzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rochette_G/0/1/0/all/0/1\">Gaspar Rochette</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thiry_L/0/1/0/all/0/1\">Louis Thiry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zarka_J/0/1/0/all/0/1\">John Zarka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mallat_S/0/1/0/all/0/1\">St&#xe9;phane Mallat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+anden_J/0/1/0/all/0/1\">Joakim and&#xe9;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belilovsky_E/0/1/0/all/0/1\">Eugene Belilovsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruna_J/0/1/0/all/0/1\">Joan Bruna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lostanlen_V/0/1/0/all/0/1\">Vincent Lostanlen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhary_M/0/1/0/all/0/1\">Muawiz Chaudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hirn_M/0/1/0/all/0/1\">Matthew J. Hirn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oyallon_E/0/1/0/all/0/1\">Edouard Oyallon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Sixin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cella_C/0/1/0/all/0/1\">Carmine Cella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eickenberg_M/0/1/0/all/0/1\">Michael Eickenberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PDE-based Group Equivariant Convolutional Neural Networks. (arXiv:2001.09046v6 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2001.09046","description":"<p>We present a PDE-based framework that generalizes Group equivariant\nConvolutional Neural Networks (G-CNNs). In this framework, a network layer is\nseen as a set of PDE-solvers where geometrically meaningful PDE-coefficients\nbecome the layer's trainable weights. Formulating our PDEs on homogeneous\nspaces allows these networks to be designed with built-in symmetries such as\nrotation in addition to the standard translation equivariance of CNNs.\n</p>\n<p>Having all the desired symmetries included in the design obviates the need to\ninclude them by means of costly techniques such as data augmentation. We will\ndiscuss our PDE-based G-CNNs (PDE-G-CNNs) in a general homogeneous space\nsetting while also going into the specifics of our primary case of interest:\nroto-translation equivariance.\n</p>\n<p>We solve the PDE of interest by a combination of linear group convolutions\nand non-linear morphological group convolutions with analytic kernel\napproximations that we underpin with formal theorems. Our kernel approximations\nallow for fast GPU-implementation of the PDE-solvers, we release our\nimplementation with this article in the form of the LieTorch extension to\nPyTorch, available at https://gitlab.com/bsmetsjr/lietorch . Just like for\nlinear convolution a morphological convolution is specified by a kernel that we\ntrain in our PDE-G-CNNs. In PDE-G-CNNs we do not use non-linearities such as\nmax/min-pooling and ReLUs as they are already subsumed by morphological\nconvolutions.\n</p>\n<p>We present a set of experiments to demonstrate the strength of the proposed\nPDE-G-CNNs in increasing the performance of deep learning based imaging\napplications with far fewer parameters than traditional CNNs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Smets_B/0/1/0/all/0/1\">Bart Smets</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Portegies_J/0/1/0/all/0/1\">Jim Portegies</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bekkers_E/0/1/0/all/0/1\">Erik Bekkers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duits_R/0/1/0/all/0/1\">Remco Duits</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CoRe: Color Regression for Multicolor Fashion Garments. (arXiv:2010.02849v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2010.02849","description":"<p>Developing deep networks that analyze fashion garments has many real-world\napplications. Among all fashion attributes, color is one of the most important\nyet challenging to detect. Existing approaches are classification-based and\nthus cannot go beyond the list of discrete predefined color names. In this\npaper, we handle color detection as a regression problem to predict the exact\nRGB values. That's why in addition to a first color classifier, we include a\nsecond regression stage for refinement in our newly proposed architecture. This\nsecond step combines two attention models: the first depends on the type of\nclothing, the second depends on the color previously detected by the\nclassifier. Our final prediction is the weighted spatial pooling over the image\npixels RGB values, where the illumination has been corrected. This architecture\nis modular and easily expanded to detect the RGBs of all colors in a multicolor\ngarment. In our experiments, we show the benefits of each component of our\narchitecture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rame_A/0/1/0/all/0/1\">Alexandre Rame</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Douillard_A/0/1/0/all/0/1\">Arthur Douillard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ollion_C/0/1/0/all/0/1\">Charles Ollion</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalized Few-shot Semantic Segmentation. (arXiv:2010.05210v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2010.05210","description":"<p>Training semantic segmentation models requires a large amount of finely\nannotated data, making it hard to quickly adapt to novel classes not satisfying\nthis condition. Few-Shot Segmentation (FS-Seg) tackles this problem with many\nconstraints. In this paper, we introduce a new benchmark, called Generalized\nFew-Shot Semantic Segmentation (GFS-Seg), to analyze the generalization ability\nof simultaneously segmenting the novel categories with very few examples and\nthe base categories with sufficient examples. It is the first study showing\nthat previous representative state-of-the-art FS-Seg methods fall short in\nGFS-Seg and the performance discrepancy mainly comes from the constrained\nsetting of FS-Seg. To make GFS-Seg tractable, we set up a GFS-Seg baseline that\nachieves decent performance without structural change on the original model.\nThen, since context is essential for semantic segmentation, we propose the\nContext-Aware Prototype Learning (CAPL) that significantly improves performance\nby 1) leveraging the co-occurrence prior knowledge from support samples, and 2)\ndynamically enriching contextual information to the classifier, conditioned on\nthe content of each query image. Both two contributions are experimentally\nshown to have substantial practical merit. Extensive experiments on Pascal-VOC\nand COCO manifest the effectiveness of CAPL, and CAPL generalizes well to\nFS-Seg by achieving competitive performance. Code is available at\nhttps://github.com/dvlab-research/GFS-Seg.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1\">Zhuotao Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_X/0/1/0/all/0/1\">Xin Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Li Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_M/0/1/0/all/0/1\">Michelle Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hengshuang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\">Jiaya Jia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating Outdoor Recognition Performance of Infrared Beacons for Infrastructure-based Localization. (arXiv:2104.09335v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.09335","description":"<p>This paper demonstrates a system comprised of infrared beacons and a camera\nequipped with an optical band-pass filter. Our system can reliably detect and\nidentify individual beacons at 100m distance regardless of lighting conditions.\nWe describe the camera and beacon design as well as the image processing\npipeline in detail. In our experiments, we investigate and demonstrate the\nability of the system to recognize our beacons in both daytime and nighttime\nconditions. High precision localization is a key enabler for automated vehicles\nbut remains unsolved, despite strong recent improvements. Our low-cost,\ninfrastructure-based approach is a potential step towards solving the\nlocalization problem. All datasets are made available here\nhttps://embedded.rwth-aachen.de/doku.php?id=forschung:mobility:infralocalization:concept.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kampmann_A/0/1/0/all/0/1\">Alexandru Kampmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lamberti_M/0/1/0/all/0/1\">Michael Lamberti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petrovic_N/0/1/0/all/0/1\">Nikola Petrovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kowalewski_S/0/1/0/all/0/1\">Stefan Kowalewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alrifaee_B/0/1/0/all/0/1\">Bassam Alrifaee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simulated Adversarial Testing of Face Recognition Models. (arXiv:2106.04569v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.04569","description":"<p>Most machine learning models are validated and tested on fixed datasets. This\ncan give an incomplete picture of the capabilities and weaknesses of the model.\nSuch weaknesses can be revealed at test time in the real world. The risks\ninvolved in such failures can be loss of profits, loss of time or even loss of\nlife in certain critical applications. In order to alleviate this issue,\nsimulators can be controlled in a fine-grained manner using interpretable\nparameters to explore the semantic image manifold. In this work, we propose a\nframework for learning how to test machine learning algorithms using simulators\nin an adversarial manner in order to find weaknesses in the model before\ndeploying it in critical scenarios. We apply this method in a face recognition\nsetup. We show that certain weaknesses of models trained on real data can be\ndiscovered using simulated samples. Using our proposed method, we can find\nadversarial synthetic faces that fool contemporary face recognition models.\nThis demonstrates the fact that these models have weaknesses that are not\nmeasured by commonly used validation datasets. We hypothesize that this type of\nadversarial examples are not isolated, but usually lie in connected spaces in\nthe latent space of the simulator. We present a method to find these\nadversarial regions as opposed to the typical adversarial points found in the\nadversarial example literature.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ruiz_N/0/1/0/all/0/1\">Nataniel Ruiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kortylewski_A/0/1/0/all/0/1\">Adam Kortylewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_W/0/1/0/all/0/1\">Weichao Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1\">Cihang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bargal_S/0/1/0/all/0/1\">Sarah Adel Bargal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1\">Alan Yuille</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sclaroff_S/0/1/0/all/0/1\">Stan Sclaroff</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the potential of sequential and non-sequential regression models for Sentinel-1-based biomass prediction in Tanzanian miombo forests. (arXiv:2106.15020v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.15020","description":"<p>This study derives regression models for above-ground biomass (AGB)\nestimation in miombo woodlands of Tanzania that utilise the high availability\nand low cost of Sentinel-1 data. The limited forest canopy penetration of\nC-band SAR sensors along with the sparseness of available ground truth restrict\ntheir usefulness in traditional AGB regression models. Therefore, we propose to\nuse AGB predictions based on airborne laser scanning (ALS) data as a surrogate\nresponse variable for SAR data. This dramatically increases the available\ntraining data and opens for flexible regression models that capture fine-scale\nAGB dynamics. This becomes a sequential modelling approach, where the first\nregression stage has linked in situ data to ALS data and produced the AGB\nprediction map; We perform the subsequent stage, where this map is related to\nSentinel-1 data. We develop a traditional, parametric regression model and\nalternative non-parametric models for this stage. The latter uses a conditional\ngenerative adversarial network (cGAN) to translate Sentinel-1 images into\nALS-based AGB prediction maps. The convolution filters in the neural networks\nmake them contextual. We compare the sequential models to traditional,\nnon-sequential regression models, all trained on limited AGB ground reference\ndata. Results show that our newly proposed non-sequential Sentinel-1-based\nregression model performs better quantitatively than the sequential models, but\nachieves less sensitivity to fine-scale AGB dynamics. The contextual cGAN-based\nsequential models best reproduce the distribution of ALS-based AGB predictions.\nThey also reach a lower RMSE against in situ AGB data than the parametric\nsequential model, indicating a potential for further development.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bjork_S/0/1/0/all/0/1\">Sara Bj&#xf6;rk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anfinsen_S/0/1/0/all/0/1\">Stian Normann Anfinsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naesset_E/0/1/0/all/0/1\">Erik N&#xe6;sset</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gobakken_T/0/1/0/all/0/1\">Terje Gobakken</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zahabu_E/0/1/0/all/0/1\">Eliakimu Zahabu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Creating synthetic night-time visible-light meteorological satellite images using the GAN method. (arXiv:2108.04330v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.04330","description":"<p>Meteorology satellite visible light images is critical for meteorology\nsupport and forecast. However, there is no such kind of data during night time.\nTo overcome this, we propose a method based on deep learning to create\nsynthetic satellite visible light images during night. Specifically, to produce\nmore realistic products, we train a Generative Adversarial Networks (GAN) model\nto generate visible light images given the corresponding satellite infrared\nimages and numerical weather prediction(NWP) products. To better model the\nnonlinear relationship from infrared data and NWP products to visible light\nimages, we propose to use the channel-wise attention mechanics, e.g., SEBlock\nto quantitative weight the input channels. The experiments based on the ECMWF\nNWP products and FY-4A meteorology satellite visible light and infrared\nchannels date show that the proposed methods can be effective to create\nrealistic synthetic satellite visible light images during night.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_W/0/1/0/all/0/1\">Wencong Cheng</a> (Beijing Aviation Meteorological Institute)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Consistent Relative Confidence and Label-Free Model Selection for Convolutional Neural Networks. (arXiv:2108.11845v9 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.11845","description":"<p>In this paper, we are concerned with image classification with deep\nconvolutional neural networks (CNNs). We focus on the following question: given\na set of candidate CNN models, how to select the right one with the best\ngeneralization property for the current task? Current model selection methods\nall require access to a batch of labeled data for computing a pre-specified\nperformance metric, such as the cross-entropy loss, the classification error\nrate and the negative log-likelihood. In many practical cases, labels are not\navailable in time as labeling itself is a time-consuming and expensive task. To\nthis end, we propose an approach to CNN model selection using only unlabeled\ndata. We develop this method based on a principle termed consistent relative\nconfidence. Experimental results on benchmark datasets demonstrate the\neffectiveness and efficiency of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bin Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Eyes Tell All: Irregular Pupil Shapes Reveal GAN-generated Faces. (arXiv:2109.00162v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.00162","description":"<p>Generative adversary network (GAN) generated high-realistic human faces have\nbeen used as profile images for fake social media accounts and are visually\nchallenging to discern from real ones. In this work, we show that GAN-generated\nfaces can be exposed via irregular pupil shapes. This phenomenon is caused by\nthe lack of physiological constraints in the GAN models. We demonstrate that\nsuch artifacts exist widely in high-quality GAN-generated faces and further\ndescribe an automatic method to extract the pupils from two eyes and analysis\ntheir shapes for exposing the GAN-generated faces. Qualitative and quantitative\nevaluations of our method suggest its simplicity and effectiveness in\ndistinguishing GAN-generated faces.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Hui Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1\">Ming-Ching Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1\">Siwei Lyu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Ground Visual Objects for Visual Dialog. (arXiv:2109.06013v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.06013","description":"<p>Visual dialog is challenging since it needs to answer a series of coherent\nquestions based on understanding the visual environment. How to ground related\nvisual objects is one of the key problems. Previous studies utilize the\nquestion and history to attend to the image and achieve satisfactory\nperformance, however these methods are not sufficient to locate related visual\nobjects without any guidance. The inappropriate grounding of visual objects\nprohibits the performance of visual dialog models. In this paper, we propose a\nnovel approach to Learn to Ground visual objects for visual dialog, which\nemploys a novel visual objects grounding mechanism where both prior and\nposterior distributions over visual objects are used to facilitate visual\nobjects grounding. Specifically, a posterior distribution over visual objects\nis inferred from both context (history and questions) and answers, and it\nensures the appropriate grounding of visual objects during the training\nprocess. Meanwhile, a prior distribution, which is inferred from context only,\nis used to approximate the posterior distribution so that appropriate visual\nobjects can be grounded even without answers during the inference process.\nExperimental results on the VisDial v0.9 and v1.0 datasets demonstrate that our\napproach improves the previous strong models in both generative and\ndiscriminative settings by a significant margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Feilong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiuyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Can Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Visual Navigation under Partial Observability. (arXiv:2109.07752v3 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2109.07752","description":"<p>How can a robot navigate successfully in rich and diverse environments,\nindoors or outdoors, along office corridors or trails on the grassland, on the\nflat ground or the staircase? To this end, this work aims to address three\nchallenges: (i) complex visual observations, (ii) partial observability of\nlocal visual sensing, and (iii) multimodal robot behaviors conditioned on both\nthe local environment and the global navigation objective. We propose to train\na neural network (NN) controller for local navigation via imitation learning.\nTo tackle complex visual observations, we extract multi-scale spatial\nrepresentations through CNNs. To tackle partial observability, we aggregate\nmulti-scale spatial information over time and encode it in LSTMs. To learn\nmultimodal behaviors, we use a separate memory module for each behavior mode.\nImportantly, we integrate the multiple neural network modules into a unified\ncontroller that achieves robust performance for visual navigation in complex,\npartially observable environments. We implemented the controller on the\nquadrupedal Spot robot and evaluated it on three challenging tasks: adversarial\npedestrian avoidance, blind-spot obstacle avoidance, and elevator riding. The\nexperiments show that the proposed NN architecture significantly improves\nnavigation performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ai_B/0/1/0/all/0/1\">Bo Ai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1\">Wei Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinay/0/1/0/all/0/1\">Vinay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_D/0/1/0/all/0/1\">David Hsu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GoG: Relation-aware Graph-over-Graph Network for Visual Dialog. (arXiv:2109.08475v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.08475","description":"<p>Visual dialog, which aims to hold a meaningful conversation with humans about\na given image, is a challenging task that requires models to reason the complex\ndependencies among visual content, dialog history, and current questions. Graph\nneural networks are recently applied to model the implicit relations between\nobjects in an image or dialog. However, they neglect the importance of 1)\ncoreference relations among dialog history and dependency relations between\nwords for the question representation; and 2) the representation of the image\nbased on the fully represented question. Therefore, we propose a novel\nrelation-aware graph-over-graph network (GoG) for visual dialog. Specifically,\nGoG consists of three sequential graphs: 1) H-Graph, which aims to capture\ncoreference relations among dialog history; 2) History-aware Q-Graph, which\naims to fully understand the question through capturing dependency relations\nbetween words based on coreference resolution on the dialog history; and 3)\nQuestion-aware I-Graph, which aims to capture the relations between objects in\nan image based on fully question representation. As an additional feature\nrepresentation module, we add GoG to the existing visual dialogue model.\nExperimental results show that our model outperforms the strong baseline in\nboth generative and discriminative settings by a significant margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Feilong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiuyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KD-VLP: Improving End-to-End Vision-and-Language Pretraining with Object Knowledge Distillation. (arXiv:2109.10504v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.10504","description":"<p>Self-supervised vision-and-language pretraining (VLP) aims to learn\ntransferable multi-modal representations from large-scale image-text data and\nto achieve strong performances on a broad scope of vision-language tasks after\nfinetuning. Previous mainstream VLP approaches typically adopt a two-step\nstrategy relying on external object detectors to encode images in a multi-modal\nTransformer framework, which suffer from restrictive object concept space,\nlimited image context and inefficient computation. In this paper, we propose an\nobject-aware end-to-end VLP framework, which directly feeds image grid features\nfrom CNNs into the Transformer and learns the multi-modal representations\njointly. More importantly, we propose to perform object knowledge distillation\nto facilitate learning cross-modal alignment at different semantic levels. To\nachieve that, we design two novel pretext tasks by taking object features and\ntheir semantic labels from external detectors as supervision: 1.) Object-guided\nmasked vision modeling task focuses on enforcing object-aware representation\nlearning in the multi-modal Transformer; 2.) Phrase-region alignment task aims\nto improve cross-modal alignment by utilizing the similarities between noun\nphrases and object labels in the linguistic space. Extensive experiments on a\nwide range of vision-language tasks demonstrate the efficacy of our proposed\nframework, and we achieve competitive or superior performances over the\nexisting pretraining strategies. The code is available in supplementary\nmaterials.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yongfei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chenfei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tseng_S/0/1/0/all/0/1\">Shao-yen Tseng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lal_V/0/1/0/all/0/1\">Vasudev Lal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xuming He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RWN: Robust Watermarking Network for Image Cropping Localization. (arXiv:2110.05687v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.05687","description":"<p>Image cropping can be maliciously used to manipulate the layout of an image\nand alter the underlying meaning. Previous image crop detection schemes only\npredicts whether an image has been cropped, ignoring which part of the image is\ncropped. This paper presents a novel robust watermarking network (RWN) for\nimage crop localization. We train an anti-crop processor (ACP) that embeds a\nwatermark into a target image. The visually indistinguishable protected image\nis then posted on the social network instead of the original image. At the\nrecipient's side, ACP extracts the watermark from the attacked image, and we\nconduct feature matching on the original and extracted watermark to locate the\nposition of the crop in the original image plane. We further extend our scheme\nto detect tampering attack on the attacked image. Besides, we explore a simple\nyet efficient method (JPEG-Mixup) to improve the generalization of JPEG\nrobustness. According to our comprehensive experiments, RWN is the first to\nprovide high-accuracy and robust image crop localization. Besides, the accuracy\nof tamper detection is comparable with many state-of-the-art passive-based\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ying_Q/0/1/0/all/0/1\">Qichao Ying</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaoxiao Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Z/0/1/0/all/0/1\">Zhenxing Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinpeng Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LoveDA: A Remote Sensing Land-Cover Dataset for Domain Adaptive Semantic Segmentation. (arXiv:2110.08733v6 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.08733","description":"<p>Deep learning approaches have shown promising results in remote sensing high\nspatial resolution (HSR) land-cover mapping. However, urban and rural scenes\ncan show completely different geographical landscapes, and the inadequate\ngeneralizability of these algorithms hinders city-level or national-level\nmapping. Most of the existing HSR land-cover datasets mainly promote the\nresearch of learning semantic representation, thereby ignoring the model\ntransferability. In this paper, we introduce the Land-cOVEr Domain Adaptive\nsemantic segmentation (LoveDA) dataset to advance semantic and transferable\nlearning. The LoveDA dataset contains 5987 HSR images with 166768 annotated\nobjects from three different cities. Compared to the existing datasets, the\nLoveDA dataset encompasses two domains (urban and rural), which brings\nconsiderable challenges due to the: 1) multi-scale objects; 2) complex\nbackground samples; and 3) inconsistent class distributions. The LoveDA dataset\nis suitable for both land-cover semantic segmentation and unsupervised domain\nadaptation (UDA) tasks. Accordingly, we benchmarked the LoveDA dataset on\neleven semantic segmentation methods and eight UDA methods. Some exploratory\nstudies including multi-scale architectures and strategies, additional\nbackground supervision, and pseudo-label analysis were also carried out to\naddress these challenges. The code and data are available at\nhttps://github.com/Junjue-Wang/LoveDA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Junjue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zhuo Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_A/0/1/0/all/0/1\">Ailong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiaoyan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1\">Yanfei Zhong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dense Prediction with Attentive Feature Aggregation. (arXiv:2111.00770v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.00770","description":"<p>Aggregating information from features across different layers is an essential\noperation for dense prediction models. Despite its limited expressiveness,\nfeature concatenation dominates the choice of aggregation operations. In this\npaper, we introduce Attentive Feature Aggregation (AFA) to fuse different\nnetwork layers with more expressive non-linear operations. AFA exploits both\nspatial and channel attention to compute weighted average of the layer\nactivations. Inspired by neural volume rendering, we extend AFA with\nScale-Space Rendering (SSR) to perform late fusion of multi-scale predictions.\nAFA is applicable to a wide range of existing network designs. Our experiments\nshow consistent and significant improvements on challenging semantic\nsegmentation benchmarks, including Cityscapes, BDD100K, and Mapillary Vistas,\nat negligible computational and parameter overhead. In particular, AFA improves\nthe performance of the Deep Layer Aggregation (DLA) model by nearly 6% mIoU on\nCityscapes. Our experimental analyses show that AFA learns to progressively\nrefine segmentation maps and to improve boundary details, leading to new\nstate-of-the-art results on boundary detection benchmarks on BSDS500 and\nNYUDv2. Code and video resources are available at <a href=\"http://vis.xyz/pub/dla-afa.\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yung-Hsu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Thomas E. Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Min Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bulo_S/0/1/0/all/0/1\">Samuel Rota Bul&#xf2;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kontschieder_P/0/1/0/all/0/1\">Peter Kontschieder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fisher Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Representational Alignment with Human Perception Using Identically Represented Inputs. (arXiv:2111.14726v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.14726","description":"<p>We contribute to the study of the quality of learned representations. In many\ndomains, an important evaluation criterion for safe and trustworthy deep\nlearning is how well the invariances captured by representations of deep neural\nnetworks (DNNs) are shared with humans. We identify challenges in measuring\nthese invariances. Prior works used gradient-based methods to generate\n\\textit{identically represented inputs} (IRIs), \\ie, inputs which have similar\nrepresentations (on a given layer) of a neural network. If these IRIs look\n`similar' to humans then a neural network's learned invariances are said to\nalign with human perception. However, we show that prior studies on the\nalignment of invariances between DNNs and humans are `biased' by the specific\nloss function used to generate IRIs. We show how different loss functions can\nlead to different takeaways about a model's shared invariances with humans. We\nshow that under an \\textit{adversarial} IRI~generation process all models\nappear to have very little shared invariance with humans. We conduct an\nin-depth investigation of how different components of the deep learning\npipeline contribute to learning models that have good alignment with human's\ninvariances. We find that architectures with residual connections trained using\na self-supervised contrastive loss with $\\ell_p$ ball adversarial data\naugmentation tend to learn the most human-like invariances.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nanda_V/0/1/0/all/0/1\">Vedant Nanda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majumdar_A/0/1/0/all/0/1\">Ayan Majumdar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolling_C/0/1/0/all/0/1\">Camila Kolling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dickerson_J/0/1/0/all/0/1\">John P. Dickerson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gummadi_K/0/1/0/all/0/1\">Krishna P. Gummadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Love_B/0/1/0/all/0/1\">Bradley C. Love</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weller_A/0/1/0/all/0/1\">Adrian Weller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multiview Transformers for Video Recognition. (arXiv:2201.04288v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.04288","description":"<p>Video understanding requires reasoning at multiple spatiotemporal resolutions\n-- from short fine-grained motions to events taking place over longer\ndurations. Although transformer architectures have recently advanced the\nstate-of-the-art, they have not explicitly modelled different spatiotemporal\nresolutions. To this end, we present Multiview Transformers for Video\nRecognition (MTV). Our model consists of separate encoders to represent\ndifferent views of the input video with lateral connections to fuse information\nacross views. We present thorough ablation studies of our model and show that\nMTV consistently performs better than single-view counterparts in terms of\naccuracy and computational cost across a range of model sizes. Furthermore, we\nachieve state-of-the-art results on six standard datasets, and improve even\nfurther with large-scale pretraining. Code and checkpoints are available at:\nhttps://github.com/google-research/scenic/tree/main/scenic/projects/mtv.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Shen Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_X/0/1/0/all/0/1\">Xuehan Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arnab_A/0/1/0/all/0/1\">Anurag Arnab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhichao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Chen Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1\">Cordelia Schmid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DICP: Doppler Iterative Closest Point Algorithm. (arXiv:2201.11944v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2201.11944","description":"<p>In this paper, we present a novel algorithm for point cloud registration for\nrange sensors capable of measuring per-return instantaneous radial velocity:\nDoppler ICP. Existing variants of ICP that solely rely on geometry or other\nfeatures generally fail to estimate the motion of the sensor correctly in\nscenarios that have non-distinctive features and/or repetitive geometric\nstructures such as hallways, tunnels, highways, and bridges. We propose a new\nDoppler velocity objective function that exploits the compatibility of each\npoint's Doppler measurement and the sensor's current motion estimate. We\njointly optimize the Doppler velocity objective function and the geometric\nobjective function which sufficiently constrains the point cloud alignment\nproblem even in feature-denied environments. Furthermore, the correspondence\nmatches used for the alignment are improved by pruning away the points from\ndynamic targets which generally degrade the ICP solution. We evaluate our\nmethod on data collected from real sensors and from simulation. Our results\nshow that with the added Doppler velocity residual terms, our method achieves a\nsignificant improvement in registration accuracy along with faster convergence,\non average, when compared to classical point-to-plane ICP that solely relies on\ngeometric residuals.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hexsel_B/0/1/0/all/0/1\">Bruno Hexsel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vhavle_H/0/1/0/all/0/1\">Heethesh Vhavle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yi Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MVP-Net: Multiple View Pointwise Semantic Segmentation of Large-Scale Point Clouds. (arXiv:2201.12769v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.12769","description":"<p>Semantic segmentation of 3D point cloud is an essential task for autonomous\ndriving environment perception. The pipeline of most pointwise point cloud\nsemantic segmentation methods includes points sampling, neighbor searching,\nfeature aggregation, and classification. Neighbor searching method like\nK-nearest neighbors algorithm, KNN, has been widely applied. However, the\ncomplexity of KNN is always a bottleneck of efficiency. In this paper, we\npropose an end-to-end neural architecture, Multiple View Pointwise Net,\nMVP-Net, to efficiently and directly infer large-scale outdoor point cloud\nwithout KNN or any complex pre/postprocessing. Instead, assumption-based space\nfilling curves and multi-rotation of point cloud methods are introduced to\npoint feature aggregation and receptive field expanding. Numerical experiments\nshow that the proposed MVP-Net is 11 times faster than the most efficient\npointwise semantic segmentation method RandLA-Net and achieves the same\naccuracy on the large-scale benchmark SemanticKITTI dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1\">Chuanyu Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaohan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_N/0/1/0/all/0/1\">Nuo Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Han Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_S/0/1/0/all/0/1\">Shengguang Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Pu Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A General Deep Learning framework for Neuron Instance Segmentation based on Efficient UNet and Morphological Post-processing. (arXiv:2202.08682v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2202.08682","description":"<p>Recent studies have demonstrated the superiority of deep learning in medical\nimage analysis, especially in cell instance segmentation, a fundamental step\nfor many biological studies. However, the excellent performance of the neural\nnetworks requires training on large, unbiased dataset and annotations, which is\nlabor-intensive and expertise-demanding. This paper presents an end-to-end\nframework to automatically detect and segment NeuN stained neuronal cells on\nhistological images using only point annotations. Unlike traditional nuclei\nsegmentation with point annotation, we propose using point annotation and\nbinary segmentation to synthesize pixel-level annotations. The synthetic masks\nare used as the ground truth to train the neural network, a U-Net-like\narchitecture with a state-of-the-art network, EfficientNet, as the encoder.\nValidation results show the superiority of our model compared to other recent\nmethods. In addition, we investigated multiple post-processing schemes and\nproposed an original strategy to convert the probability map into segmented\ninstances using ultimate erosion and dynamic reconstruction. This approach is\neasy to configure and outperforms other classical post-processing techniques.\nThis work aims to develop a robust and efficient framework for analyzing\nneurons using optical microscopic data, which can be used in preclinical\nbiological studies and, more specifically, in the context of neurodegenerative\ndiseases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wu_H/0/1/0/all/0/1\">Huaqian Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Souedet_N/0/1/0/all/0/1\">Nicolas Souedet</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jan_C/0/1/0/all/0/1\">Caroline Jan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Clouchoux_C/0/1/0/all/0/1\">C&#xe9;dric Clouchoux</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Delzescaux_T/0/1/0/all/0/1\">Thierry Delzescaux</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Structured Pruning is All You Need for Pruning CNNs at Initialization. (arXiv:2203.02549v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.02549","description":"<p>Pruning is a popular technique for reducing the model size and computational\ncost of convolutional neural networks (CNNs). However, a slow retraining or\nfine-tuning procedure is often required to recover the accuracy loss caused by\npruning. Recently, a new research direction on weight pruning,\npruning-at-initialization (PAI), is proposed to directly prune CNNs before\ntraining so that fine-tuning or retraining can be avoided. While PAI has shown\npromising results in reducing the model size, existing approaches rely on\nfine-grained weight pruning which requires unstructured sparse matrix\ncomputation, making it difficult to achieve real speedup in practice unless the\nsparsity is very high. This work is the first to show that fine-grained weight\npruning is in fact not necessary for PAI. Instead, the layerwise compression\nratio is the main critical factor to determine the accuracy of a CNN model\npruned at initialization. Based on this key observation, we propose\nPreCropping, a structured hardware-efficient model compression scheme.\nPreCropping directly compresses the model at the channel level following the\nlayerwise compression ratio. Compared to weight pruning, the proposed scheme is\nregular and dense in both storage and computation without sacrificing accuracy.\nIn addition, since PreCropping compresses CNNs at initialization, the\ncomputational and memory costs of CNNs are reduced for both training and\ninference on commodity hardware. We empirically demonstrate our approaches on\nseveral modern CNN architectures, including ResNet, ShuffleNet, and MobileNet\nfor both CIFAR-10 and ImageNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1\">Yaohui Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_W/0/1/0/all/0/1\">Weizhe Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hongzheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suh_G/0/1/0/all/0/1\">G. Edward Suh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sa_C/0/1/0/all/0/1\">Christopher De Sa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhiru Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DrawingInStyles: Portrait Image Generation and Editing with Spatially Conditioned StyleGAN. (arXiv:2203.02762v3 [cs.GR] UPDATED)","link":"http://arxiv.org/abs/2203.02762","description":"<p>The research topic of sketch-to-portrait generation has witnessed a boost of\nprogress with deep learning techniques. The recently proposed StyleGAN\narchitectures achieve state-of-the-art generation ability but the original\nStyleGAN is not friendly for sketch-based creation due to its unconditional\ngeneration nature. To address this issue, we propose a direct conditioning\nstrategy to better preserve the spatial information under the StyleGAN\nframework. Specifically, we introduce Spatially Conditioned StyleGAN\n(SC-StyleGAN for short), which explicitly injects spatial constraints to the\noriginal StyleGAN generation process. We explore two input modalities, sketches\nand semantic maps, which together allow users to express desired generation\nresults more precisely and easily. Based on SC-StyleGAN, we present\nDrawingInStyles, a novel drawing interface for non-professional users to easily\nproduce high-quality, photo-realistic face images with precise control, either\nfrom scratch or editing existing ones. Qualitative and quantitative evaluations\nshow the superior generation ability of our method to existing and alternative\nsolutions. The usability and expressiveness of our system are confirmed by a\nuser study.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_W/0/1/0/all/0/1\">Wanchao Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Hui Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shu-Yu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Lin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1\">Hongbo Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Model-Agnostic Multitask Fine-tuning for Few-shot Vision-Language Transfer Learning. (arXiv:2203.04904v2 [cs.MM] UPDATED)","link":"http://arxiv.org/abs/2203.04904","description":"<p>Despite achieving state-of-the-art zero-shot performance, existing\nvision-language models, e.g., CLIP, still fall short of domain-specific\nclassification tasks, e.g., Fungi Classification. In the context of few-shot\ntransfer learning, traditional fine-tuning fails to prevent highly expressive\nmodel from exploiting spurious correlations in the training data. On the other\nhand, although model-agnostic meta-learning (MAML) presents as a natural\nalternative for transfer learning, the expensive computation due to implicit\nsecond-order optimization limits its use in large-scale models and datasets. In\nthis work we aim to further improve the generalization of existing\nvision-language models on unseen tasks via a simple yet efficient fine-tuning\nstrategy based on uniform task sampling. We term our method as Model-Agnostic\nMultitask Fine-tuning (MAMF). Compared with MAML, MAMF discards the bi-level\noptimization and uses only first-order gradients, which makes it easily\nscalable and computationally efficient. Due to the uniform task sampling\nprocedure, MAMF consistently outperforms the classical fine-tuning method for\nfew-shot transfer learning on five benchmark datasets. Empirically, we further\ndiscover that the effectiveness of first-order MAML is highly dependent on the\nzero-shot performance of the pretrained model, and our simple algorithm can\noutperform first-order MAML on more challenging datasets with low zero-shot\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhenhailong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Manling Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Han Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-supervised Semantic Segmentation with Error Localization Network. (arXiv:2204.02078v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.02078","description":"<p>This paper studies semi-supervised learning of semantic segmentation, which\nassumes that only a small portion of training images are labeled and the others\nremain unlabeled. The unlabeled images are usually assigned pseudo labels to be\nused in training, which however often causes the risk of performance\ndegradation due to the confirmation bias towards errors on the pseudo labels.\nWe present a novel method that resolves this chronic issue of pseudo labeling.\nAt the heart of our method lies error localization network (ELN), an auxiliary\nmodule that takes an image and its segmentation prediction as input and\nidentifies pixels whose pseudo labels are likely to be wrong. ELN enables\nsemi-supervised learning to be robust against inaccurate pseudo labels by\ndisregarding label noises during training and can be naturally integrated with\nself-training and contrastive learning. Moreover, we introduce a new learning\nstrategy for ELN that simulates plausible and diverse segmentation errors\nduring training of ELN to enhance its generalization. Our method is evaluated\non PASCAL VOC 2012 and Cityscapes, where it outperforms all existing methods in\nevery evaluation setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kwon_D/0/1/0/all/0/1\">Donghyeon Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_S/0/1/0/all/0/1\">Suha Kwak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CAIPI in Practice: Towards Explainable Interactive Medical Image Classification. (arXiv:2204.02661v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2204.02661","description":"<p>Would you trust physicians if they cannot explain their decisions to you?\nMedical diagnostics using machine learning gained enormously in importance\nwithin the last decade. However, without further enhancements many\nstate-of-the-art machine learning methods are not suitable for medical\napplication. The most important reasons are insufficient data set quality and\nthe black-box behavior of machine learning algorithms such as Deep Learning\nmodels. Consequently, end-users cannot correct the model's decisions and the\ncorresponding explanations. The latter is crucial for the trustworthiness of\nmachine learning in the medical domain. The research field explainable\ninteractive machine learning searches for methods that address both\nshortcomings. This paper extends the explainable and interactive CAIPI\nalgorithm and provides an interface to simplify human-in-the-loop approaches\nfor image classification. The interface enables the end-user (1) to investigate\nand (2) to correct the model's prediction and explanation, and (3) to influence\nthe data set quality. After CAIPI optimization with only a single\ncounterexample per iteration, the model achieves an accuracy of $97.48\\%$ on\nthe Medical MNIST and $95.02\\%$ on the Fashion MNIST. This accuracy is\napproximately equal to state-of-the-art Deep Learning optimization procedures.\nBesides, CAIPI reduces the labeling effort by approximately $80\\%$.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Slany_E/0/1/0/all/0/1\">Emanuel Slany</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ott_Y/0/1/0/all/0/1\">Yannik Ott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scheele_S/0/1/0/all/0/1\">Stephan Scheele</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paulus_J/0/1/0/all/0/1\">Jan Paulus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_U/0/1/0/all/0/1\">Ute Schmid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Permutation-Invariant Relational Network for Multi-person 3D Pose Estimation. (arXiv:2204.04913v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.04913","description":"<p>The recovery of multi-person 3D poses from a single RGB image is a severely\nill-conditioned problem due to the inherent 2D-3D depth ambiguity, inter-person\nocclusions, and body truncations. To tackle these issues, recent works have\nshown promising results by simultaneously reasoning for different people.\nHowever, in most cases this is done by only considering pairwise person\ninteractions, hindering thus a holistic scene representation able to capture\nlong-range interactions. This is addressed by approaches that jointly process\nall people in the scene, although they require defining one of the individuals\nas a reference and a pre-defined person ordering, being sensitive to this\nchoice. In this paper, we overcome both these limitations, and we propose an\napproach for multi-person 3D pose estimation that captures long-range\ninteractions independently of the input order. For this purpose, we build a\nresidual-like permutation-invariant network that successfully refines\npotentially corrupted initial 3D poses estimated by an off-the-shelf detector.\nThe residual function is learned via Set Transformer blocks, that model the\ninteractions among all initial poses, no matter their ordering or number. A\nthorough evaluation demonstrates that our approach is able to boost the\nperformance of the initially estimated 3D poses by large margins, achieving\nstate-of-the-art results on standardized benchmarks. Additionally, the proposed\nmodule works in a computationally efficient manner and can be potentially used\nas a drop-in complement for any 3D pose detector in multi-people scenes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ugrinovic_N/0/1/0/all/0/1\">Nicolas Ugrinovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruiz_A/0/1/0/all/0/1\">Adria Ruiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agudo_A/0/1/0/all/0/1\">Antonio Agudo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanfeliu_A/0/1/0/all/0/1\">Alberto Sanfeliu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreno_Noguer_F/0/1/0/all/0/1\">Francesc Moreno-Noguer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MultiEarth 2022 -- Multimodal Learning for Earth and Environment Workshop and Challenge. (arXiv:2204.07649v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.07649","description":"<p>The Multimodal Learning for Earth and Environment Challenge (MultiEarth 2022)\nwill be the first competition aimed at the monitoring and analysis of\ndeforestation in the Amazon rainforest at any time and in any weather\nconditions. The goal of the Challenge is to provide a common benchmark for\nmultimodal information processing and to bring together the earth and\nenvironmental science communities as well as multimodal representation learning\ncommunities to compare the relative merits of the various multimodal learning\nmethods to deforestation estimation under well-defined and strictly comparable\nconditions. MultiEarth 2022 will have three sub-challenges: 1) matrix\ncompletion, 2) deforestation estimation, and 3) image-to-image translation.\nThis paper presents the challenge guidelines, datasets, and evaluation metrics\nfor the three sub-challenges. Our challenge website is available at\nhttps://sites.google.com/view/rainforest-challenge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cha_M/0/1/0/all/0/1\">Miriam Cha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kuan Wei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_M/0/1/0/all/0/1\">Morgan Schmidt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angelides_G/0/1/0/all/0/1\">Gregory Angelides</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamilton_M/0/1/0/all/0/1\">Mark Hamilton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_S/0/1/0/all/0/1\">Sam Goldberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cabrera_A/0/1/0/all/0/1\">Armando Cabrera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Isola_P/0/1/0/all/0/1\">Phillip Isola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perron_T/0/1/0/all/0/1\">Taylor Perron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freeman_B/0/1/0/all/0/1\">Bill Freeman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yen-Chen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swenson_B/0/1/0/all/0/1\">Brandon Swenson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piou_J/0/1/0/all/0/1\">Jean Piou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weighted Bayesian Gaussian Mixture Model for Roadside LiDAR Object Detection. (arXiv:2204.09804v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.09804","description":"<p>Background modeling is widely used for intelligent surveillance systems to\ndetect the moving targets by subtracting the static background components. Most\nroadside LiDAR object detection methods filter out foreground points by\ncomparing new data points to pre-trained background references based on\ndescriptive statistics over many frames (e.g., voxel density, slopes, maximum\ndistance). These solutions are not efficient under heavy traffic, and parameter\nvalues are hard to transfer from one scenario to another. In early studies, the\nprobabilistic background modeling methods widely used for the video-based\nsystem were considered not suitable for roadside LiDAR surveillance systems due\nto the sparse and unstructured point clouds data. In this paper, the raw LiDAR\ndata were transformed into a structured representation based on the elevation\nand azimuth value of each LiDAR point. With this high-order tensor\nrepresentation, we break the barrier to allow the efficient Gaussian Mixture\nModel (GMM) method for roadside LiDAR background modeling. The Bayesian\nNonParametric (BNP) approach integrates the intensity value and 3D measurements\nto fully exploit the measurement data using both 3D and intensity info. The\nproposed method was compared against two state-of-the-art roadside LiDAR\nbackground models and evaluated at point, object, and path levels,\ndemonstrating better robustness under heavy traffic and challenging weather.\nThis multimodal Weighted Bayesian GMM method is capable of handling dynamic\nbackgrounds with noisy measurements and substantially enhances the\ninfrastructure-based LiDAR object detection, whereby various 3D modeling for\nsmart city applications could be created.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianya Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_P/0/1/0/all/0/1\">Peter J. Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yi Ge</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Models Can See: Plugging Visual Controls in Text Generation. (arXiv:2205.02655v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.02655","description":"<p>Generative language models (LMs) such as GPT-2/3 can be prompted to generate\ntext with remarkable quality. While they are designed for text-prompted\ngeneration, it remains an open question how the generation process could be\nguided by modalities beyond text such as images. In this work, we propose a\ntraining-free framework, called MAGIC (iMAge-Guided text generatIon with CLIP),\nfor plugging in visual controls in the generation process and enabling LMs to\nperform multimodal tasks (e.g., image captioning) in a zero-shot manner. MAGIC\nis a simple yet efficient plug-and-play framework, which directly combines an\noff-the-shelf LM (i.e., GPT-2) and an image-text matching model (i.e., CLIP)\nfor image-grounded text generation. During decoding, MAGIC influences the\ngeneration of the LM by introducing a CLIP-induced score, called magic score,\nwhich regularizes the generated result to be semantically related to a given\nimage while being coherent to the previously generated context. Notably, the\nproposed decoding scheme does not involve any gradient update operation,\ntherefore being computationally efficient. On the challenging task of zero-shot\nimage captioning, MAGIC outperforms the state-of-the-art method by notable\nmargins with a nearly 27 times decoding speedup. MAGIC is a flexible framework\nand is theoretically compatible with any text generation tasks that incorporate\nimage grounding. In the experiments, we showcase that it is also capable of\nperforming visually grounded story generation given both an image and a text\nprompt.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yixuan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_T/0/1/0/all/0/1\">Tian Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yahui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fangyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yogatama_D/0/1/0/all/0/1\">Dani Yogatama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1\">Lingpeng Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collier_N/0/1/0/all/0/1\">Nigel Collier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Augmentations: An Insight into their Effectiveness on Convolution Neural Networks. (arXiv:2205.04064v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2205.04064","description":"<p>Augmentations are the key factor in determining the performance of any neural\nnetwork as they provide a model with a critical edge in boosting its\nperformance. Their ability to boost a model's robustness depends on two\nfactors, viz-a-viz, the model architecture, and the type of augmentations.\nAugmentations are very specific to a dataset, and it is not imperative that all\nkinds of augmentation would necessarily produce a positive effect on a model's\nperformance. Hence there is a need to identify augmentations that perform\nconsistently well across a variety of datasets and also remain invariant to the\ntype of architecture, convolutions, and the number of parameters used. Hence\nthere is a need to identify augmentations that perform consistently well across\na variety of datasets and also remain invariant to the type of architecture,\nconvolutions, and the number of parameters used. This paper evaluates the\neffect of parameters using 3x3 and depth-wise separable convolutions on\ndifferent augmentation techniques on MNIST, FMNIST, and CIFAR10 datasets.\nStatistical Evidence shows that techniques such as Cutouts and Random\nhorizontal flip were consistent on both parametrically low and high\narchitectures. Depth-wise separable convolutions outperformed 3x3 convolutions\nat higher parameters due to their ability to create deeper networks.\nAugmentations resulted in bridging the accuracy gap between the 3x3 and\ndepth-wise separable convolutions, thus establishing their role in model\ngeneralization. At higher number augmentations did not produce a significant\nchange in performance. The synergistic effect of multiple augmentations at\nhigher parameters, with antagonistic effect at lower parameters, was also\nevaluated. The work proves that a delicate balance between architectural\nsupremacy and augmentations needs to be achieved to enhance a model's\nperformance in any given deep learning task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ethiraj_S/0/1/0/all/0/1\">Sabeesh Ethiraj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bolla_B/0/1/0/all/0/1\">Bharath Kumar Bolla</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Impact of Partial Occlusion on Pedestrian Detectability. (arXiv:2205.04812v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.04812","description":"<p>Robust detection of vulnerable road users is a safety critical requirement\nfor the deployment of autonomous vehicles in heterogeneous traffic. One of the\nmost complex outstanding challenges is that of partial occlusion where a target\nobject is only partially available to the sensor due to obstruction by another\nforeground object. A number of leading pedestrian detection benchmarks provide\nannotation for partial occlusion, however each benchmark varies greatly in\ntheir definition of the occurrence and severity of occlusion. Recent research\ndemonstrates that a high degree of subjectivity is used to classify occlusion\nlevel in these cases and occlusion is typically categorized into 2 to 3 broad\ncategories such as partially and heavily occluded. This can lead to inaccurate\nor inconsistent reporting of pedestrian detection model performance depending\non which benchmark is used. This research introduces a novel, objective\nbenchmark for partially occluded pedestrian detection to facilitate the\nobjective characterization of pedestrian detection models. Characterization is\ncarried out on seven popular pedestrian detection models for a range of\nocclusion levels from 0-99%. Results demonstrate that pedestrian detection\nperformance degrades, and the number of false negative detections increase as\npedestrian occlusion level increases. Of the seven popular pedestrian detection\nroutines characterized, CenterNet has the greatest overall performance,\nfollowed by SSDlite. RetinaNet has the lowest overall detection performance\nacross the range of occlusion levels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gilroy_S/0/1/0/all/0/1\">Shane Gilroy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mullins_D/0/1/0/all/0/1\">Darragh Mullins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jones_E/0/1/0/all/0/1\">Edward Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parsi_A/0/1/0/all/0/1\">Ashkan Parsi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glavin_M/0/1/0/all/0/1\">Martin Glavin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Objective Method for Pedestrian Occlusion Level Classification. (arXiv:2205.05412v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.05412","description":"<p>Pedestrian detection is among the most safety-critical features of driver\nassistance systems for autonomous vehicles. One of the most complex detection\nchallenges is that of partial occlusion, where a target object is only\npartially available to the sensor due to obstruction by another foreground\nobject. A number of current pedestrian detection benchmarks provide annotation\nfor partial occlusion to assess algorithm performance in these scenarios,\nhowever each benchmark varies greatly in their definition of the occurrence and\nseverity of occlusion. In addition, current occlusion level annotation methods\ncontain a high degree of subjectivity by the human annotator. This can lead to\ninaccurate or inconsistent reporting of an algorithm's detection performance\nfor partially occluded pedestrians, depending on which benchmark is used. This\nresearch presents a novel, objective method for pedestrian occlusion level\nclassification for ground truth annotation. Occlusion level classification is\nachieved through the identification of visible pedestrian keypoints and through\nthe use of a novel, effective method of 2D body surface area estimation.\nExperimental results demonstrate that the proposed method reflects the\npixel-wise occlusion level of pedestrians in images and is effective for all\nforms of occlusion, including challenging edge cases such as self-occlusion,\ntruncation and inter-occluding pedestrians.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gilroy_S/0/1/0/all/0/1\">Shane Gilroy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glavin_M/0/1/0/all/0/1\">Martin Glavin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jones_E/0/1/0/all/0/1\">Edward Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mullins_D/0/1/0/all/0/1\">Darragh Mullins</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PillarNet: Real-Time and High-Performance Pillar-based 3D Object Detection. (arXiv:2205.07403v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.07403","description":"<p>Real-time and high-performance 3D object detection is of critical importance\nfor autonomous driving. Recent top-performing 3D object detectors mainly rely\non point-based or 3D voxel-based convolutions, which are both computationally\ninefficient for onboard deployment. In contrast, pillar-based methods use\nsolely 2D convolutions, which consume less computation resources, but they lag\nfar behind their voxel-based counterparts in detection accuracy. In this paper,\nby examining the primary performance gap between pillar- and voxel-based\ndetectors, we develop a real-time and high-performance pillar-based detector,\ndubbed PillarNet. The proposed PillarNet consists of a powerful encoder network\nfor effective pillar feature learning, a neck network for spatial-semantic\nfeature fusion and the commonly used detect head. Using only 2D convolutions,\nPillarNet is flexible to an optional pillar size and compatible with classical\n2D CNN backbones, such as VGGNet and ResNet.Additionally, PillarNet benefits\nfrom our designed orientation-decoupled IoU regression loss along with the\nIoU-aware prediction branch. Extensive experimental results on large-scale\nnuScenes Dataset and Waymo Open Dataset demonstrate that the proposed PillarNet\nperforms well over the state-of-the-art 3D detectors in terms of effectiveness\nand efficiency. The source code is available at\nhttps://github.com/agent-sgs/PillarNet.git.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_G/0/1/0/all/0/1\">Guangsheng Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruifeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Chao Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ColonFormer: An Efficient Transformer based Method for Colon Polyp Segmentation. (arXiv:2205.08473v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.08473","description":"<p>Identifying polyps is challenging for automatic analysis of endoscopic images\nin computer-aided clinical support systems. Models based on convolutional\nnetworks (CNN), transformers, and their combinations have been proposed to\nsegment polyps with promising results. However, those approaches have\nlimitations either in modeling the local appearance of the polyps only or lack\nof multi-level features for spatial dependency in the decoding process. This\npaper proposes a novel network, namely ColonFormer, to address these\nlimitations. ColonFormer is an encoder-decoder architecture capable of modeling\nlong-range semantic information at both encoder and decoder branches. The\nencoder is a lightweight architecture based on transformers for modeling global\nsemantic relations at multi scales. The decoder is a hierarchical network\nstructure designed for learning multi-level features to enrich feature\nrepresentation. Besides, a refinement module is added with a new skip\nconnection technique to refine the boundary of polyp objects in the global map\nfor accurate segmentation. Extensive experiments have been conducted on five\npopular benchmark datasets for polyp segmentation, including Kvasir, CVC-Clinic\nDB, CVC-ColonDB, CVC-T, and ETIS-Larib. Experimental results show that our\nColonFormer outperforms other state-of-the-art methods on all benchmark\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Duc_N/0/1/0/all/0/1\">Nguyen Thanh Duc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oanh_N/0/1/0/all/0/1\">Nguyen Thi Oanh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thuy_N/0/1/0/all/0/1\">Nguyen Thi Thuy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Triet_T/0/1/0/all/0/1\">Tran Minh Triet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sang_D/0/1/0/all/0/1\">Dinh Viet Sang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bayesian Convolutional Neural Networks for Limited Data Hyperspectral Remote Sensing Image Classification. (arXiv:2205.09250v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.09250","description":"<p>Employing deep neural networks for Hyperspectral remote sensing (HSRS) image\nclassification is a challenging task. HSRS images have high dimensionality and\na large number of channels with substantial redundancy between channels. In\naddition, the training data for classifying HSRS images is limited and the\namount of available training data is much smaller compared to other\nclassification tasks. These factors complicate the training process of deep\nneural networks with many parameters and cause them to not perform well even\ncompared to conventional models. Moreover, convolutional neural networks\nproduce over-confident predictions, which is highly undesirable considering the\naforementioned problem.\n</p>\n<p>In this work, we use for HSRS image classification a special class of deep\nneural networks, namely a Bayesian neural network (BNN). To the extent of our\nknowledge, this is the first time that BNNs are used in HSRS image\nclassification. BNNs inherently provide a measure for uncertainty. We perform\nextensive experiments on the Pavia Centre, Salinas, and Botswana datasets. We\nshow that a BNN outperforms a standard convolutional neural network (CNN) and\nan off-the-shelf Random Forest (RF). Further experiments underline that the BNN\nis more stable and robust to model pruning, and that the uncertainty is higher\nfor samples with higher expected prediction error.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Joshaghani_M/0/1/0/all/0/1\">Mohammad Joshaghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davari_A/0/1/0/all/0/1\">Amirabbas Davari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hatamian_F/0/1/0/all/0/1\">Faezeh Nejati Hatamian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maier_A/0/1/0/all/0/1\">Andreas Maier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riess_C/0/1/0/all/0/1\">Christian Riess</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discovering Dynamic Functional Brain Networks via Spatial and Channel-wise Attention. (arXiv:2205.09576v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.09576","description":"<p>Using deep learning models to recognize functional brain networks (FBNs) in\nfunctional magnetic resonance imaging (fMRI) has been attracting increasing\ninterest recently. However, most existing work focuses on detecting static FBNs\nfrom entire fMRI signals, such as correlation-based functional connectivity.\nSliding-window is a widely used strategy to capture the dynamics of FBNs, but\nit is still limited in representing intrinsic functional interactive dynamics\nat each time step. And the number of FBNs usually need to be set manually. More\nover, due to the complexity of dynamic interactions in brain, traditional\nlinear and shallow models are insufficient in identifying complex and spatially\noverlapped FBNs across each time step. In this paper, we propose a novel\nSpatial and Channel-wise Attention Autoencoder (SCAAE) for discovering FBNs\ndynamically. The core idea of SCAAE is to apply attention mechanism to FBNs\nconstruction. Specifically, we designed two attention modules: 1) spatial-wise\nattention (SA) module to discover FBNs in the spatial domain and 2) a\nchannel-wise attention (CA) module to weigh the channels for selecting the FBNs\nautomatically. We evaluated our approach on ADHD200 dataset and our results\nindicate that the proposed SCAAE method can effectively recover the dynamic\nchanges of the FBNs at each fMRI time step, without using sliding windows. More\nimportantly, our proposed hybrid attention modules (SA and CA) do not enforce\nassumptions of linearity and independence as previous methods, and thus provide\na novel approach to better understanding dynamic functional brain networks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yiheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_E/0/1/0/all/0/1\">Enjie Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_M/0/1/0/all/0/1\">Mengshen He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhengliang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shijie Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xintao Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_D/0/1/0/all/0/1\">Dajiang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_B/0/1/0/all/0/1\">Bao Ge</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HiVLP: Hierarchical Vision-Language Pre-Training for Fast Image-Text Retrieval. (arXiv:2205.12105v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.12105","description":"<p>In the past few years, the emergence of vision-language pre-training (VLP)\nhas brought cross-modal retrieval to a new era. However, due to the latency and\ncomputation demand, it is commonly challenging to apply VLP in a real-time\nonline retrieval system. To alleviate the defect, this paper proposes a\n\\textbf{Hi}erarchical \\textbf{V}ision-\\textbf{}Language \\textbf{P}re-Training\n(\\textbf{HiVLP}) for fast Image-Text Retrieval (ITR). Specifically, we design a\nnovel hierarchical retrieval objective, which uses the representation of\ndifferent dimensions for coarse-to-fine ITR, i.e., using low-dimensional\nrepresentation for large-scale coarse retrieval and high-dimensional\nrepresentation for small-scale fine retrieval. We evaluate our proposed HiVLP\non two popular image-text retrieval benchmarks, i.e., Flickr30k and COCO.\nExtensive experiments demonstrate that our HiVLP not only has fast inference\nspeed but also can be easily scaled to large-scale ITR scenarios. The detailed\nresults show that HiVLP is $1,427$$\\sim$$120,649\\times$ faster than the\nfusion-based model UNITER and 2$\\sim$5 faster than the fastest embedding-based\nmodel LightingDot in different candidate scenarios. It also achieves about +4.9\nAR on COCO and +3.8 AR on Flickr30K than LightingDot and achieves comparable\nperformance with the state-of-the-art (SOTA) fusion-based model METER.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Feilong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiuyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jiaxin Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Duzhen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1\">Jianlong Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Real-Time Video Deblurring via Lightweight Motion Compensation. (arXiv:2205.12634v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.12634","description":"<p>While motion compensation greatly improves video deblurring quality,\nseparately performing motion compensation and video deblurring demands huge\ncomputational overhead. This paper proposes a real-time video deblurring\nframework consisting of a lightweight multi-task unit that supports both video\ndeblurring and motion compensation in an efficient way. The multi-task unit is\nspecifically designed to handle large portions of the two tasks using a single\nshared network, and consists of a multi-task detail network and simple networks\nfor deblurring and motion compensation. The multi-task unit minimizes the cost\nof incorporating motion compensation into video deblurring and enables\nreal-time deblurring. Moreover, by stacking multiple multi-task units, our\nframework provides flexible control between the cost and deblurring quality. We\nexperimentally validate the state-of-the-art deblurring quality of our\napproach, which runs at a much faster speed compared to previous methods, and\nshow practical real-time performance (30.99dB@30fps measured in the DVD\ndataset).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Son_H/0/1/0/all/0/1\">Hyeongseok Son</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Junyong Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1\">Sunghyun Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seungyong Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"V-Doc : Visual questions answers with Documents. (arXiv:2205.13724v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2205.13724","description":"<p>We propose V-Doc, a question-answering tool using document images and PDF,\nmainly for researchers and general non-deep learning experts looking to\ngenerate, process, and understand the document visual question answering tasks.\nThe V-Doc supports generating and using both extractive and abstractive\nquestion-answer pairs using documents images. The extractive QA selects a\nsubset of tokens or phrases from the document contents to predict the answers,\nwhile the abstractive QA recognises the language in the content and generates\nthe answer based on the trained model. Both aspects are crucial to\nunderstanding the documents, especially in an image format. We include a\ndetailed scenario of question generation for the abstractive QA task. V-Doc\nsupports a wide range of datasets and models, and is highly extensible through\na declarative, framework-agnostic platform.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yihao Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhe Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Runlin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yanhang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xianru Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yuzhong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_H/0/1/0/all/0/1\">Hyunsuk Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Soyeon Caren Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video2StyleGAN: Disentangling Local and Global Variations in a Video. (arXiv:2205.13996v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.13996","description":"<p>Image editing using a pretrained StyleGAN generator has emerged as a powerful\nparadigm for facial editing, providing disentangled controls over age,\nexpression, illumination, etc. However, the approach cannot be directly adopted\nfor video manipulations. We hypothesize that the main missing ingredient is the\nlack of fine-grained and disentangled control over face location, face pose,\nand local facial expressions. In this work, we demonstrate that such a\nfine-grained control is indeed achievable using pretrained StyleGAN by working\nacross multiple (latent) spaces (namely, the positional space, the W+ space,\nand the S space) and combining the optimization results across the multiple\nspaces. Building on this enabling component, we introduce Video2StyleGAN that\ntakes a target image and driving video(s) to reenact the local and global\nlocations and expressions from the driving video in the identity of the target\nimage. We evaluate the effectiveness of our method over multiple challenging\nscenarios and demonstrate clear improvements over alternative approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abdal_R/0/1/0/all/0/1\">Rameen Abdal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_P/0/1/0/all/0/1\">Peihao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitra_N/0/1/0/all/0/1\">Niloy J. Mitra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wonka_P/0/1/0/all/0/1\">Peter Wonka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Task Learning with Multi-query Transformer for Dense Prediction. (arXiv:2205.14354v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.14354","description":"<p>Previous multi-task dense prediction studies developed complex pipelines such\nas multi-modal distillations in multiple stages or searching for task\nrelational contexts for each task. The core insight beyond these methods is to\nmaximize the mutual effects between each task. Inspired by the recent\nquery-based Transformers, we propose a simpler pipeline named Multi-Query\nTransformer (MQTransformer) that is equipped with multiple queries from\ndifferent tasks to facilitate the reasoning among multiple tasks and simplify\nthe cross task pipeline. Instead of modeling the dense per-pixel context among\ndifferent tasks, we seek a task-specific proxy to perform cross-task reasoning\nvia multiple queries where each query encodes the task-related context. The\nMQTransformer is composed of three key components: shared encoder, cross task\nattention and shared decoder. We first model each task with a task-relevant and\nscale-aware query, and then both the image feature output by the feature\nextractor and the task-relevant query feature are fed into the shared encoder,\nthus encoding the query feature from the image feature. Secondly, we design a\ncross task attention module to reason the dependencies among multiple tasks and\nfeature scales from two perspectives including different tasks of the same\nscale and different scales of the same task. Then we use a shared decoder to\ngradually refine the image features with the reasoned query features from\ndifferent tasks. Extensive experiment results on two dense prediction datasets\n(NYUD-v2 and PASCAL-Context) show that the proposed method is an effective\napproach and achieves the state-of-the-art result. Code will be available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yangyang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiangtai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1\">Haobo Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yibo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_Y/0/1/0/all/0/1\">Yunhai Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lefei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PSNet: Fast Data Structuring for Hierarchical Deep Learning on Point Cloud. (arXiv:2205.14965v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.14965","description":"<p>In order to retain more feature information of local areas on a point cloud,\nlocal grouping and subsampling are the necessary data structuring steps in most\nhierarchical deep learning models. Due to the disorder nature of the points in\na point cloud, the significant time cost may be consumed when grouping and\nsubsampling the points, which consequently results in poor scalability. This\npaper proposes a fast data structuring method called PSNet (Point Structuring\nNet). PSNet transforms the spatial features of the points and matches them to\nthe features of local areas in a point cloud. PSNet achieves grouping and\nsampling at the same time while the existing methods process sampling and\ngrouping in two separate steps (such as using FPS plus kNN). PSNet performs\nfeature transformation pointwise while the existing methods uses the spatial\nrelationship among the points as the reference for grouping. Thanks to these\nfeatures, PSNet has two important advantages: 1) the grouping and sampling\nresults obtained by PSNet is stable and permutation invariant; and 2) PSNet can\nbe easily parallelized. PSNet can replace the data structuring methods in the\nmainstream point cloud deep learning models in a plug-and-play manner. We have\nconducted extensive experiments. The results show that PSNet can improve the\ntraining and inference speed significantly while maintaining the model\naccuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Luyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Ligang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jinjin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xie Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deblurring Photographs of Characters Using Deep Neural Networks. (arXiv:2205.15053v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.15053","description":"<p>In this paper, we present our approach for the Helsinki Deblur Challenge\n(HDC2021). The task of this challenge is to deblur images of characters without\nknowing the point spread function (PSF). The organizers provided a dataset of\npairs of sharp and blurred images. Our method consists of three steps: First,\nwe estimate a warping transformation of the images to align the sharp images\nwith the blurred ones. Next, we estimate the PSF using a quasi-Newton method.\nThe estimated PSF allows to generate additional pairs of sharp and blurred\nimages. Finally, we train a deep convolutional neural network to reconstruct\nthe sharp images from the blurred images. Our method is able to successfully\nreconstruct images from the first 10 stages of the HDC 2021 data. Our code is\navailable at https://github.com/hhu-machine-learning/hdc2021-psfnn.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Germer_T/0/1/0/all/0/1\">Thomas Germer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uelwer_T/0/1/0/all/0/1\">Tobias Uelwer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harmeling_S/0/1/0/all/0/1\">Stefan Harmeling</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EAMM: One-Shot Emotional Talking Face via Audio-Based Emotion-Aware Motion Model. (arXiv:2205.15278v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.15278","description":"<p>Although significant progress has been made to audio-driven talking face\ngeneration, existing methods either neglect facial emotion or cannot be applied\nto arbitrary subjects. In this paper, we propose the Emotion-Aware Motion Model\n(EAMM) to generate one-shot emotional talking faces by involving an emotion\nsource video. Specifically, we first propose an Audio2Facial-Dynamics module,\nwhich renders talking faces from audio-driven unsupervised zero- and\nfirst-order key-points motion. Then through exploring the motion model's\nproperties, we further propose an Implicit Emotion Displacement Learner to\nrepresent emotion-related facial dynamics as linearly additive displacements to\nthe previously acquired motion representations. Comprehensive experiments\ndemonstrate that by incorporating the results from both modules, our method can\ngenerate satisfactory talking face results on arbitrary subjects with realistic\nemotion patterns.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1\">Xinya Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kaisiyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qianyi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wayne Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1\">Feng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1\">Xun Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-Shot and Few-Shot Learning for Lung Cancer Multi-Label Classification using Vision Transformer. (arXiv:2205.15290v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.15290","description":"<p>Lung cancer is the leading cause of cancer-related death worldwide. Lung\nadenocarcinoma (LUAD) and lung squamous cell carcinoma (LUSC) are the most\ncommon histologic subtypes of non-small-cell lung cancer (NSCLC). Histology is\nan essential tool for lung cancer diagnosis. Pathologists make classifications\naccording to the dominant subtypes. Although morphology remains the standard\nfor diagnosis, significant tool needs to be developed to elucidate the\ndiagnosis. In our study, we utilize the pre-trained Vision Transformer (ViT)\nmodel to classify multiple label lung cancer on histologic slices (from dataset\nLC25000), in both Zero-Shot and Few-Shot settings. Then we compare the\nperformance of Zero-Shot and Few-Shot ViT on accuracy, precision, recall,\nsensitivity and specificity. Our study show that the pre-trained ViT model has\na good performance in Zero-Shot setting, a competitive accuracy ($99.87\\%$) in\nFew-Shot setting ({epoch = 1}) and an optimal result ($100.00\\%$ on both\nvalidation set and test set) in Few-Shot seeting ({epoch = 5}).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_F/0/1/0/all/0/1\">Fu-Ming Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1\">Yingfang Fan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-05-31T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/"}}]}]}