{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-07-20T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Using attention methods to predict judicial outcomes. (arXiv:2207.08823v1 [cs.LG])","link":"http://arxiv.org/abs/2207.08823","description":"<p>Legal Judgment Prediction is one of the most acclaimed fields for the\ncombined area of NLP, AI, and Law. By legal prediction we mean an intelligent\nsystems capable to predict specific judicial characteristics, such as judicial\noutcome, a judicial class, predict an specific case. In this research, we have\nused AI classifiers to predict judicial outcomes in the Brazilian legal system.\nFor this purpose, we developed a text crawler to extract data from the official\nBrazilian electronic legal systems. These texts formed a dataset of\nsecond-degree murder and active corruption cases. We applied different\nclassifiers, such as Support Vector Machines and Neural Networks, to predict\njudicial outcomes by analyzing textual features from the dataset. Our research\nshowed that Regression Trees, Gated Recurring Units and Hierarchical Attention\nNetworks presented higher metrics for different subsets. As a final goal, we\nexplored the weights of one of the algorithms, the Hierarchical Attention\nNetworks, to find a sample of the most important words used to absolve or\nconvict defendants.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bertalan_V/0/1/0/all/0/1\">Vithor Gomes Ferreira Bertalan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruiz_E/0/1/0/all/0/1\">Evandro Eduardo Seron Ruiz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Sequence Models for Text Classification Tasks. (arXiv:2207.08880v1 [cs.CL])","link":"http://arxiv.org/abs/2207.08880","description":"<p>The exponential growth of data generated on the Internet in the current\ninformation age is a driving force for the digital economy. Extraction of\ninformation is the major value in an accumulated big data. Big data dependency\non statistical analysis and hand-engineered rules machine learning algorithms\nare overwhelmed with vast complexities inherent in human languages. Natural\nLanguage Processing (NLP) is equipping machines to understand these human\ndiverse and complicated languages. Text Classification is an NLP task which\nautomatically identifies patterns based on predefined or undefined labeled\nsets. Common text classification application includes information retrieval,\nmodeling news topic, theme extraction, sentiment analysis, and spam detection.\nIn texts, some sequences of words depend on the previous or next word sequences\nto make full meaning; this is a challenging dependency task that requires the\nmachine to be able to store some previous important information to impact\nfuture meaning. Sequence models such as RNN, GRU, and LSTM is a breakthrough\nfor tasks with long-range dependencies. As such, we applied these models to\nBinary and Multi-class classification. Results generated were excellent with\nmost of the models performing within the range of 80% and 94%. However, this\nresult is not exhaustive as we believe there is room for improvement if\nmachines are to compete with humans.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abdullahi_S/0/1/0/all/0/1\">Saheed Salahudeen Abdullahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yiming_S/0/1/0/all/0/1\">Sun Yiming</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muhammad_S/0/1/0/all/0/1\">Shamsuddeen Hassan Muhammad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mustapha_A/0/1/0/all/0/1\">Abdulrasheed Mustapha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aminu_A/0/1/0/all/0/1\">Ahmad Muhammad Aminu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdullahi_A/0/1/0/all/0/1\">Abdulkadir Abdullahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bello_M/0/1/0/all/0/1\">Musa Bello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aliyu_S/0/1/0/all/0/1\">Saminu Mohammad Aliyu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MRCLens: an MRC Dataset Bias Detection Toolkit. (arXiv:2207.08943v1 [cs.CL])","link":"http://arxiv.org/abs/2207.08943","description":"<p>Many recent neural models have shown remarkable empirical results in Machine\nReading Comprehension, but evidence suggests sometimes the models take\nadvantage of dataset biases to predict and fail to generalize on out-of-sample\ndata. While many other approaches have been proposed to address this issue from\nthe computation perspective such as new architectures or training procedures,\nwe believe a method that allows researchers to discover biases, and adjust the\ndata or the models in an earlier stage will be beneficial. Thus, we introduce\nMRCLens, a toolkit that detects whether biases exist before users train the\nfull model. For the convenience of introducing the toolkit, we also provide a\ncategorization of common biases in MRC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1\">Yifan Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haohan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1\">Eric P. Xing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Selection Bias Induced Spurious Correlations in Large Language Models. (arXiv:2207.08982v1 [cs.CL])","link":"http://arxiv.org/abs/2207.08982","description":"<p>In this work we show how large language models (LLMs) can learn statistical\ndependencies between otherwise unconditionally independent variables due to\ndataset selection bias. To demonstrate the effect, we developed a masked gender\ntask that can be applied to BERT-family models to reveal spurious correlations\nbetween predicted gender pronouns and a variety of seemingly gender-neutral\nvariables like date and location, on pre-trained (unmodified) BERT and RoBERTa\nlarge models. Finally, we provide an online demo, inviting readers to\nexperiment further.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+McMilin_E/0/1/0/all/0/1\">Emily McMilin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Training Large-Vocabulary Neural Language Models by Private Federated Learning for Resource-Constrained Devices. (arXiv:2207.08988v1 [cs.LG])","link":"http://arxiv.org/abs/2207.08988","description":"<p>Federated Learning (FL) is a technique to train models using data distributed\nacross devices. Differential Privacy (DP) provides a formal privacy guarantee\nfor sensitive data. Our goal is to train a large neural network language model\n(NNLM) on compute-constrained devices while preserving privacy using FL and DP.\nHowever, the DP-noise introduced to the model increases as the model size\ngrows, which often prevents convergence. We propose Partial Embedding Updates\n(PEU), a novel technique to decrease noise by decreasing payload size.\nFurthermore, we adopt Low Rank Adaptation (LoRA) and Noise Contrastive\nEstimation (NCE) to reduce the memory demands of large models on\ncompute-constrained devices. This combination of techniques makes it possible\nto train large-vocabulary language models while preserving accuracy and\nprivacy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mingbin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1\">Congzheng Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Ye Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_N/0/1/0/all/0/1\">Neha Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Granqvist_F/0/1/0/all/0/1\">Filip Granqvist</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dalen_R/0/1/0/all/0/1\">Rogier van Dalen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Argueta_A/0/1/0/all/0/1\">Arturo Argueta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Shiyi Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yaqiao Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Leo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walia_A/0/1/0/all/0/1\">Anmol Walia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_A/0/1/0/all/0/1\">Alex Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PiC: A Phrase-in-Context Dataset for Phrase Understanding and Semantic Search. (arXiv:2207.09068v1 [cs.CL])","link":"http://arxiv.org/abs/2207.09068","description":"<p>Since BERT (Devlin et al., 2018), learning contextualized word embeddings has\nbeen a de-facto standard in NLP. However, the progress of learning\ncontextualized phrase embeddings is hindered by the lack of a human-annotated,\nphrase-in-context benchmark. To fill this gap, we propose PiC - a dataset of\n~28K of noun phrases accompanied by their contextual Wikipedia pages and a\nsuite of three tasks of increasing difficulty for evaluating the quality of\nphrase embeddings. We find that training on our dataset improves ranking\nmodels' accuracy and remarkably pushes Question Answering (QA) models to\nnear-human accuracy which is 95% Exact Match (EM) on semantic search given a\nquery phrase and a passage. Interestingly, we find evidence that such\nimpressive performance is because the QA models learn to better capture the\ncommon meaning of a phrase regardless of its actual context. That is, on our\nPhrase Sense Disambiguation (PSD) task, SotA model accuracy drops substantially\n(60% EM), failing to differentiate between two different senses of the same\nphrase under two different contexts. Further results on our 3-task PiC\nbenchmark reveal that learning contextualized phrase embeddings remains an\ninteresting, open challenge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pham_T/0/1/0/all/0/1\">Thang M. Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Seunghyun Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1\">Trung Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1\">Anh Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual Transformer Encoders: a Word-Level Task-Agnostic Evaluation. (arXiv:2207.09076v1 [cs.CL])","link":"http://arxiv.org/abs/2207.09076","description":"<p>Some Transformer-based models can perform cross-lingual transfer learning:\nthose models can be trained on a specific task in one language and give\nrelatively good results on the same task in another language, despite having\nbeen pre-trained on monolingual tasks only. But, there is no consensus yet on\nwhether those transformer-based models learn universal patterns across\nlanguages. We propose a word-level task-agnostic method to evaluate the\nalignment of contextualized representations built by such models. We show that\nour method provides more accurate translated word pairs than previous methods\nto evaluate word-level alignment. And our results show that some inner layers\nof multilingual Transformer-based models outperform other explicitly aligned\nrepresentations, and even more so according to a stricter definition of\nmultilingual alignment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gaschi_F/0/1/0/all/0/1\">F&#xe9;lix Gaschi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plesse_F/0/1/0/all/0/1\">Fran&#xe7;ois Plesse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rastin_P/0/1/0/all/0/1\">Parisa Rastin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toussaint_Y/0/1/0/all/0/1\">Yannick Toussaint</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ILASR: Privacy-Preserving Incremental Learning for AutomaticSpeech Recognition at Production Scale. (arXiv:2207.09078v1 [cs.CL])","link":"http://arxiv.org/abs/2207.09078","description":"<p>Incremental learning is one paradigm to enable model building and updating at\nscale with streaming data. For end-to-end automatic speech recognition (ASR)\ntasks, the absence of human annotated labels along with the need for privacy\npreserving policies for model building makes it a daunting challenge. Motivated\nby these challenges, in this paper we use a cloud based framework for\nproduction systems to demonstrate insights from privacy preserving incremental\nlearning for automatic speech recognition (ILASR). By privacy preserving, we\nmean, usage of ephemeral data which are not human annotated. This system is a\nstep forward for production levelASR models for incremental/continual learning\nthat offers near real-time test-bed for experimentation in the cloud for\nend-to-end ASR, while adhering to privacy-preserving policies. We show that the\nproposed system can improve the production models significantly(3%) over a new\ntime period of six months even in the absence of human annotated labels with\nvarying levels of weak supervision and large batch sizes in incremental\nlearning. This improvement is 20% over test sets with new words and phrases in\nthe new time period. We demonstrate the effectiveness of model building in a\nprivacy-preserving incremental fashion for ASR while further exploring the\nutility of having an effective teacher model and use of large batch sizes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chennupati_G/0/1/0/all/0/1\">Gopinath Chennupati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_M/0/1/0/all/0/1\">Milind Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chadha_G/0/1/0/all/0/1\">Gurpreet Chadha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eakin_A/0/1/0/all/0/1\">Aaron Eakin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raju_A/0/1/0/all/0/1\">Anirudh Raju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tiwari_G/0/1/0/all/0/1\">Gautam Tiwari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahu_A/0/1/0/all/0/1\">Anit Kumar Sahu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rastrow_A/0/1/0/all/0/1\">Ariya Rastrow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Droppo_J/0/1/0/all/0/1\">Jasha Droppo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oberlin_A/0/1/0/all/0/1\">Andy Oberlin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nandanoor_B/0/1/0/all/0/1\">Buddha Nandanoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkataramanan_P/0/1/0/all/0/1\">Prahalad Venkataramanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zheng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sitpure_P/0/1/0/all/0/1\">Pankaj Sitpure</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Relational Future Captioning Model for Explaining Likely Collisions in Daily Tasks. (arXiv:2207.09083v1 [cs.RO])","link":"http://arxiv.org/abs/2207.09083","description":"<p>Domestic service robots that support daily tasks are a promising solution for\nelderly or disabled people. It is crucial for domestic service robots to\nexplain the collision risk before they perform actions. In this paper, our aim\nis to generate a caption about a future event. We propose the Relational Future\nCaptioning Model (RFCM), a crossmodal language generation model for the future\ncaptioning task. The RFCM has the Relational Self-Attention Encoder to extract\nthe relationships between events more effectively than the conventional\nself-attention in transformers. We conducted comparison experiments, and the\nresults show the RFCM outperforms a baseline method on two datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kambara_M/0/1/0/all/0/1\">Motonari Kambara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugiura_K/0/1/0/all/0/1\">Komei Sugiura</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can You Fool AI by Doing a 180? $\\unicode{x2013}$ A Case Study on Authorship Analysis of Texts by Arata Osada. (arXiv:2207.09085v1 [cs.CL])","link":"http://arxiv.org/abs/2207.09085","description":"<p>This paper is our attempt at answering a twofold question covering the areas\nof ethics and authorship analysis. Firstly, since the methods used for\nperforming authorship analysis imply that an author can be recognized by the\ncontent he or she creates, we were interested in finding out whether it would\nbe possible for an author identification system to correctly attribute works to\nauthors if in the course of years they have undergone a major psychological\ntransition. Secondly, and from the point of view of the evolution of an\nauthor's ethical values, we checked what it would mean if the authorship\nattribution system encounters difficulties in detecting single authorship. We\nset out to answer those questions through performing a binary authorship\nanalysis task using a text classifier based on a pre-trained transformer model\nand a baseline method relying on conventional similarity metrics. For the test\nset, we chose works of Arata Osada, a Japanese educator and specialist in the\nhistory of education, with half of them being books written before the World\nWar II and another half in the 1950s, in between which he underwent a\ntransformation in terms of political opinions. As a result, we were able to\nconfirm that in the case of texts authored by Arata Osada in a time span of\nmore than 10 years, while the classification accuracy drops by a large margin\nand is substantially lower than for texts by other non-fiction writers,\nconfidence scores of the predictions remain at a similar level as in the case\nof a shorter time span, indicating that the classifier was in many instances\ntricked into deciding that texts written over a time span of multiple years\nwere actually written by two different people, which in turn leads us to\nbelieve that such a change can affect authorship analysis, and that historical\nevents have great impact on a person's ethical outlook as expressed in their\nwritings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nieuwazny_J/0/1/0/all/0/1\">Jagna Nieuwazny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nowakowski_K/0/1/0/all/0/1\">Karol Nowakowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ptaszynski_M/0/1/0/all/0/1\">Michal Ptaszynski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masui_F/0/1/0/all/0/1\">Fumito Masui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MoEC: Mixture of Expert Clusters. (arXiv:2207.09094v1 [cs.CL])","link":"http://arxiv.org/abs/2207.09094","description":"<p>Sparsely Mixture of Experts (MoE) has received great interest due to its\npromising scaling capability with affordable computational overhead. MoE\nconverts dense layers into sparse experts, and utilizes a gated routing network\nto make experts conditionally activated. However, as the number of experts\ngrows, MoE with outrageous parameters suffers from overfitting and sparse data\nallocation. Such problems are especially severe on tasks with limited data,\nthus hindering the progress for MoE models to improve performance by scaling\nup. In this work, we propose Mixture of Expert Clusters - a general approach to\nenable expert layers to learn more diverse and appropriate knowledge by\nimposing variance-based constraints on the routing stage. We further propose a\ncluster-level expert dropout strategy specifically designed for the expert\ncluster structure. Our experiments reveal that MoEC could improve performance\non machine translation and natural language understanding tasks, and raise the\nperformance upper bound for scaling up experts under limited data. We also\nverify that MoEC plays a positive role in mitigating overfitting and sparse\ndata allocation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yuan Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shaohan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analyzing Bagging Methods for Language Models. (arXiv:2207.09099v1 [cs.CL])","link":"http://arxiv.org/abs/2207.09099","description":"<p>Modern language models leverage increasingly large numbers of parameters to\nachieve performance on natural language understanding tasks. Ensembling these\nmodels in specific configurations for downstream tasks show even further\nperformance improvements. In this paper, we perform an analysis of bagging\nlanguage models and compare single language models to bagged ensembles that are\nroughly equivalent in terms of final model size. We explore an array of model\nbagging configurations for natural language understanding tasks with final\nensemble sizes ranging from 300M parameters to 1.5B parameters and determine\nthat our ensembling methods are at best roughly equivalent to single LM\nbaselines. We note other positive effects of bagging and pruning in specific\nscenarios according to findings in our experiments such as variance reduction\nand minor performance improvements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Islam_P/0/1/0/all/0/1\">Pranab Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khosla_S/0/1/0/all/0/1\">Shaan Khosla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lok_A/0/1/0/all/0/1\">Arthur Lok</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saxena_M/0/1/0/all/0/1\">Mudit Saxena</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Usability of Transformers-based models for a French Question-Answering task. (arXiv:2207.09150v1 [cs.CL])","link":"http://arxiv.org/abs/2207.09150","description":"<p>For many tasks, state-of-the-art results have been achieved with\nTransformer-based architectures, resulting in a paradigmatic shift in practices\nfrom the use of task-specific architectures to the fine-tuning of pre-trained\nlanguage models. The ongoing trend consists in training models with an\never-increasing amount of data and parameters, which requires considerable\nresources. It leads to a strong search to improve resource efficiency based on\nalgorithmic and hardware improvements evaluated only for English. This raises\nquestions about their usability when applied to small-scale learning problems,\nfor which a limited amount of training data is available, especially for\nunder-resourced languages tasks. The lack of appropriately sized corpora is a\nhindrance to applying data-driven and transfer learning-based approaches with\nstrong instability cases. In this paper, we establish a state-of-the-art of the\nefforts dedicated to the usability of Transformer-based models and propose to\nevaluate these improvements on the question-answering performances of French\nlanguage which have few resources. We address the instability relating to data\nscarcity by investigating various training strategies with data augmentation,\nhyperparameters optimization and cross-lingual transfer. We also introduce a\nnew compact model for French FrALBERT which proves to be competitive in\nlow-resource settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cattan_O/0/1/0/all/0/1\">Oralie Cattan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Servan_C/0/1/0/all/0/1\">Christophe Servan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosset_S/0/1/0/all/0/1\">Sophie Rosset</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Benchmarking Transformers-based models on French Spoken Language Understanding tasks. (arXiv:2207.09152v1 [cs.CL])","link":"http://arxiv.org/abs/2207.09152","description":"<p>In the last five years, the rise of the self-attentional Transformer-based\narchitectures led to state-of-the-art performances over many natural language\ntasks. Although these approaches are increasingly popular, they require large\namounts of data and computational resources. There is still a substantial need\nfor benchmarking methodologies ever upwards on under-resourced languages in\ndata-scarce application conditions. Most pre-trained language models were\nmassively studied using the English language and only a few of them were\nevaluated on French. In this paper, we propose a unified benchmark, focused on\nevaluating models quality and their ecological impact on two well-known French\nspoken language understanding tasks. Especially we benchmark thirteen\nwell-established Transformer-based models on the two available spoken language\nunderstanding tasks for French: MEDIA and ATIS-FR. Within this framework, we\nshow that compact models can reach comparable results to bigger ones while\ntheir ecological impact is considerably lower. However, this assumption is\nnuanced and depends on the considered compression method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cattan_O/0/1/0/all/0/1\">Oralie Cattan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghannay_S/0/1/0/all/0/1\">Sahar Ghannay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Servan_C/0/1/0/all/0/1\">Christophe Servan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosset_S/0/1/0/all/0/1\">Sophie Rosset</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the cross-lingual transferability of multilingual prototypical models across NLU tasks. (arXiv:2207.09157v1 [cs.CL])","link":"http://arxiv.org/abs/2207.09157","description":"<p>Supervised deep learning-based approaches have been applied to task-oriented\ndialog and have proven to be effective for limited domain and language\napplications when a sufficient number of training examples are available. In\npractice, these approaches suffer from the drawbacks of domain-driven design\nand under-resourced languages. Domain and language models are supposed to grow\nand change as the problem space evolves. On one hand, research on transfer\nlearning has demonstrated the cross-lingual ability of multilingual\nTransformers-based models to learn semantically rich representations. On the\nother, in addition to the above approaches, meta-learning have enabled the\ndevelopment of task and language learning algorithms capable of far\ngeneralization. Through this context, this article proposes to investigate the\ncross-lingual transferability of using synergistically few-shot learning with\nprototypical neural networks and multilingual Transformers-based models.\nExperiments in natural language understanding tasks on MultiATIS++ corpus shows\nthat our approach substantially improves the observed transfer learning\nperformances between the low and the high resource languages. More generally\nour approach confirms that the meaningful latent space learned in a given\nlanguage can be can be generalized to unseen and under-resourced ones using\nmeta-learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cattan_O/0/1/0/all/0/1\">Oralie Cattan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Servan_C/0/1/0/all/0/1\">Christophe Servan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosset_S/0/1/0/all/0/1\">Sophie Rosset</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Urdu Speech and Text Based Sentiment Analyzer. (arXiv:2207.09163v1 [cs.CL])","link":"http://arxiv.org/abs/2207.09163","description":"<p>Discovering what other people think has always been a key aspect of our\ninformation-gathering strategy. People can now actively utilize information\ntechnology to seek out and comprehend the ideas of others, thanks to the\nincreased availability and popularity of opinion-rich resources such as online\nreview sites and personal blogs. Because of its crucial function in\nunderstanding people's opinions, sentiment analysis (SA) is a crucial task.\nExisting research, on the other hand, is primarily focused on the English\nlanguage, with just a small amount of study devoted to low-resource languages.\nFor sentiment analysis, this work presented a new multi-class Urdu dataset\nbased on user evaluations. The tweeter website was used to get Urdu dataset.\nOur proposed dataset includes 10,000 reviews that have been carefully\nclassified into two categories by human experts: positive, negative. The\nprimary purpose of this research is to construct a manually annotated dataset\nfor Urdu sentiment analysis and to establish the baseline result. Five\ndifferent lexicon- and rule-based algorithms including Naivebayes, Stanza,\nTextblob, Vader, and Flair are employed and the experimental results show that\nFlair with an accuracy of 70% outperforms other tested algorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_W/0/1/0/all/0/1\">Waqar Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Edalati_M/0/1/0/all/0/1\">Maryam Edalati</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contextual Similarity is More Valuable than Character Similarity: Curriculum Learning for Chinese Spell Checking. (arXiv:2207.09217v1 [cs.CL])","link":"http://arxiv.org/abs/2207.09217","description":"<p>Chinese Spell Checking (CSC) task aims to detect and correct Chinese spelling\nerrors. In recent years, related researches focus on introducing the character\nsimilarity from confusion set to enhance the CSC models, ignoring the context\nof characters that contain richer information. To make better use of contextual\nsimilarity, we propose a simple yet effective curriculum learning framework for\nthe CSC task. With the help of our designed model-agnostic framework, existing\nCSC models will be trained from easy to difficult as humans learn Chinese\ncharacters and achieve further performance improvements. Extensive experiments\nand detailed analyses on widely used SIGHAN datasets show that our method\noutperforms previous state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Ding Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yinghui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qingyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shirong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yangning Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yunbo Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Hai-Tao Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Formal Algorithms for Transformers. (arXiv:2207.09238v1 [cs.LG])","link":"http://arxiv.org/abs/2207.09238","description":"<p>This document aims to be a self-contained, mathematically precise overview of\ntransformer architectures and algorithms (*not* results). It covers what\ntransformers are, how they are trained, what they are used for, their key\narchitectural components, and a preview of the most prominent models. The\nreader is assumed to be familiar with basic ML terminology and simpler neural\nnetwork architectures such as MLPs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Phuong_M/0/1/0/all/0/1\">Mary Phuong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hutter_M/0/1/0/all/0/1\">Marcus Hutter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lifelong Pretraining: Continually Adapting Language Models to Emerging Corpora. (arXiv:2110.08534v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.08534","description":"<p>Pretrained language models (PTLMs) are typically learned over a large, static\ncorpus and further fine-tuned for various downstream tasks. However, when\ndeployed in the real world, a PTLM-based model must deal with data\ndistributions that deviate from what the PTLM was initially trained on. In this\npaper, we study a lifelong language model pretraining challenge where a PTLM is\ncontinually updated so as to adapt to emerging data. Over a domain-incremental\nresearch paper stream and a chronologically-ordered tweet stream, we\nincrementally pretrain a PTLM with different continual learning algorithms, and\nkeep track of the downstream task performance (after fine-tuning). We evaluate\nPTLM's ability to adapt to new corpora while retaining learned knowledge in\nearlier corpora. Our experiments show distillation-based approaches to be most\neffective in retaining downstream performance in earlier domains. The\nalgorithms also improve knowledge transfer, allowing models to achieve better\ndownstream performance over the latest data, and improve temporal\ngeneralization when distribution gaps exist between training and evaluation\nbecause of time. We believe our problem formulation, methods, and analysis will\ninspire future studies towards continual pretraining of language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1\">Xisen Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dejiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Henghui Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_W/0/1/0/all/0/1\">Wei Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shang-Wen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xiaokai Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arnold_A/0/1/0/all/0/1\">Andrew Arnold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Object-Centric Unsupervised Image Captioning. (arXiv:2112.00969v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.00969","description":"<p>Image captioning is a longstanding problem in the field of computer vision\nand natural language processing. To date, researchers have produced impressive\nstate-of-the-art performance in the age of deep learning. Most of these\nstate-of-the-art, however, requires large volume of annotated image-caption\npairs in order to train their models. When given an image dataset of interests,\npractitioner needs to annotate the caption for each image in the training set\nand this process needs to happen for each newly collected image dataset. In\nthis paper, we explore the task of unsupervised image captioning which utilizes\nunpaired images and texts to train the model so that the texts can come from\ndifferent sources than the images. A main school of research on this topic that\nhas been shown to be effective is to construct pairs from the images and texts\nin the training set according to their overlap of objects. Unlike in the\nsupervised setting, these constructed pairings are however not guaranteed to\nhave fully overlapping set of objects. Our work in this paper overcomes this by\nharvesting objects corresponding to a given sentence from the training set,\neven if they don't belong to the same image. When used as input to a\ntransformer, such mixture of objects enables larger if not full object\ncoverage, and when supervised by the corresponding sentence, produced results\nthat outperform current state of the art unsupervised methods by a significant\nmargin. Building upon this finding, we further show that (1) additional\ninformation on relationship between objects and attributes of objects also\nhelps in boosting performance; and (2) our method also extends well to\nnon-English image captioning, which usually suffers from a scarcer level of\nannotations. Our findings are supported by strong empirical results. Our code\nis available at https://github.com/zihangm/obj-centric-unsup-caption.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1\">Zihang Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">David Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1\">Xuefei Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_A/0/1/0/all/0/1\">Ashish Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1\">Ser-Nam Lim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Speech Recognition for Speech Assessment of Persian Preschool Children. (arXiv:2203.12886v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.12886","description":"<p>Preschool evaluation is crucial because it gives teachers and parents\ninfluential knowledge about children's growth and development. The COVID-19\npandemic has highlighted the necessity of online assessment for preschool\nchildren. One of the areas that should be tested is their ability to speak.\nEmploying Automatic Speech Recognition(ASR) system is useless, since they of\npre-trained on voices, that are different from children's voices in terms of\nfrequency and amplitude. We constructed an ASR for our cognitive test system to\nsolve this issue using the Wav2Vec 2.0 model with a new pre-training objective\ncalled Random Frequency Pitch(RFP). In addition, we used our new dataset to\nfine-tune our model for Meaningless Words(MW) and Rapid Automatic Naming(RAN)\ntests. Our new approach reaches a Word Error Rate(WER) of 6.45 on the Persian\nsection of the CommonVoice dataset. Furthermore, our novel methodology produces\npositive outcomes in zero- and few-shot scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abaskohi_A/0/1/0/all/0/1\">Amirhossein Abaskohi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mortazavi_F/0/1/0/all/0/1\">Fatemeh Mortazavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moradi_H/0/1/0/all/0/1\">Hadi Moradi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FactGraph: Evaluating Factuality in Summarization with Semantic Graph Representations. (arXiv:2204.06508v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.06508","description":"<p>Despite recent improvements in abstractive summarization, most current\napproaches generate summaries that are not factually consistent with the source\ndocument, severely restricting their trust and usage in real-world\napplications. Recent works have shown promising improvements in factuality\nerror identification using text or dependency arc entailments; however, they do\nnot consider the entire semantic graph simultaneously. To this end, we propose\nFactGraph, a method that decomposes the document and the summary into\nstructured meaning representations (MR), which are more suitable for factuality\nevaluation. MRs describe core semantic concepts and their relations,\naggregating the main content in both document and summary in a canonical form,\nand reducing data sparsity. FactGraph encodes such graphs using a graph encoder\naugmented with structure-aware adapters to capture interactions among the\nconcepts based on the graph connectivity, along with text representations using\nan adapter-based text encoder. Experiments on different benchmarks for\nevaluating factuality show that FactGraph outperforms previous approaches by up\nto 15%. Furthermore, FactGraph improves performance on identifying content\nverifiability errors and better captures subsentence-level factual\ninconsistencies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ribeiro_L/0/1/0/all/0/1\">Leonardo F. R. Ribeiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Mengwen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dreyer_M/0/1/0/all/0/1\">Markus Dreyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking. (arXiv:2204.08387v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.08387","description":"<p>Self-supervised pre-training techniques have achieved remarkable progress in\nDocument AI. Most multimodal pre-trained models use a masked language modeling\nobjective to learn bidirectional representations on the text modality, but they\ndiffer in pre-training objectives for the image modality. This discrepancy adds\ndifficulty to multimodal representation learning. In this paper, we propose\n\\textbf{LayoutLMv3} to pre-train multimodal Transformers for Document AI with\nunified text and image masking. Additionally, LayoutLMv3 is pre-trained with a\nword-patch alignment objective to learn cross-modal alignment by predicting\nwhether the corresponding image patch of a text word is masked. The simple\nunified architecture and training objectives make LayoutLMv3 a general-purpose\npre-trained model for both text-centric and image-centric Document AI tasks.\nExperimental results show that LayoutLMv3 achieves state-of-the-art performance\nnot only in text-centric tasks, including form understanding, receipt\nunderstanding, and document visual question answering, but also in\nimage-centric tasks such as document image classification and document layout\nanalysis. The code and models are publicly available at\n\\url{https://aka.ms/layoutlmv3}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yupan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_T/0/1/0/all/0/1\">Tengchao Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1\">Lei Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yutong Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Effective Few-Shot Named Entity Linking by Meta-Learning. (arXiv:2207.05280v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2207.05280","description":"<p>Entity linking aims to link ambiguous mentions to their corresponding\nentities in a knowledge base, which is significant and fundamental for various\ndownstream applications, e.g., knowledge base completion, question answering,\nand information extraction. While great efforts have been devoted to this task,\nmost of these studies follow the assumption that large-scale labeled data is\navailable. However, when the labeled data is insufficient for specific domains\ndue to labor-intensive annotation work, the performance of existing algorithms\nwill suffer an intolerable decline. In this paper, we endeavor to solve the\nproblem of few-shot entity linking, which only requires a minimal amount of\nin-domain labeled data and is more practical in real situations. Specifically,\nwe firstly propose a novel weak supervision strategy to generate non-trivial\nsynthetic entity-mention pairs based on mention rewriting. Since the quality of\nthe synthetic data has a critical impact on effective model training, we\nfurther design a meta-learning mechanism to assign different weights to each\nsynthetic entity-mention pair automatically. Through this way, we can\nprofoundly exploit rich and precious semantic information to derive a\nwell-trained entity linking model under the few-shot setting. The experiments\non real-world datasets show that the proposed method can extensively improve\nthe state-of-the-art few-shot entity linking model and achieve impressive\nperformance when only a small amount of labeled data is available. Moreover, we\nalso demonstrate the outstanding ability of the model's transferability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiuxing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhengyan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1\">Ning Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1\">Haitao Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianyong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Synergistic Compilation Workflow for Tackling Crosstalk in Quantum Machines. (arXiv:2207.05751v2 [quant-ph] UPDATED)","link":"http://arxiv.org/abs/2207.05751","description":"<p>Near-term quantum systems tend to be noisy. Crosstalk noise has been\nrecognized as one of several major types of noises in superconducting Noisy\nIntermediate-Scale Quantum (NISQ) devices. Crosstalk arises from the concurrent\nexecution of two-qubit gates on nearby qubits, such as \\texttt{CX}. It might\nsignificantly raise the error rate of gates in comparison to running them\nindividually. Crosstalk can be mitigated through scheduling or hardware machine\ntuning. Prior scientific studies, however, manage crosstalk at a really late\nphase in the compilation process, usually after hardware mapping is done. It\nmay miss great opportunities of optimizing algorithm logic, routing, and\ncrosstalk at the same time. In this paper, we push the envelope by considering\nall these factors simultaneously at the very early compilation stage. We\npropose a crosstalk-aware quantum program compilation framework called CQC that\ncan enhance crosstalk mitigation while achieving satisfactory circuit depth.\nMoreover, we identify opportunities for translation from intermediate\nrepresentation to the circuit for application-specific crosstalk mitigation,\nfor instance, the \\texttt{CX} ladder construction in variational quantum\neigensolvers (VQE). Evaluations through simulation and on real IBM-Q devices\nshow that our framework can significantly reduce the error rate by up to\n6$\\times$, with only $\\sim$60\\% circuit depth compared to state-of-the-art gate\nscheduling approaches. In particular, for VQE, we demonstrate 49\\% circuit\ndepth reduction with 9.6\\% fidelity improvement over prior art on the H4\nmolecule using IBMQ Guadalupe. Our CQC framework will be released on GitHub.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/quant-ph/1/au:+Hua_F/0/1/0/all/0/1\">Fei Hua</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Jin_Y/0/1/0/all/0/1\">Yuwei Jin</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Li_A/0/1/0/all/0/1\">Ang Li</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Chen_Y/0/1/0/all/0/1\">Yanhao Chen</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Zhang_C/0/1/0/all/0/1\">Chi Zhang</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Hayes_A/0/1/0/all/0/1\">Ari Hayes</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Gao_H/0/1/0/all/0/1\">Hang Gao</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Zhang_E/0/1/0/all/0/1\">Eddy Z. Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Context Pattern Generation for Entity Set Expansion. (arXiv:2207.08087v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2207.08087","description":"<p>Entity Set Expansion (ESE) is a valuable task that aims to find entities of\nthe target semantic class described by given seed entities. Various NLP and IR\ndownstream applications have benefited from ESE due to its ability to discover\nknowledge. Although existing bootstrapping methods have achieved great\nprogress, most of them still rely on manually pre-defined context patterns. A\nnon-negligible shortcoming of the pre-defined context patterns is that they\ncannot be flexibly generalized to all kinds of semantic classes, and we call\nthis phenomenon as \"semantic sensitivity\". To address this problem, we devise a\ncontext pattern generation module that utilizes autoregressive language models\n(e.g., GPT-2) to automatically generate high-quality context patterns for\nentities. In addition, we propose the GAPA, a novel ESE framework that\nleverages the aforementioned GenerAted PAtterns to expand target entities.\nExtensive experiments and detailed analyses on three widely used datasets\ndemonstrate the effectiveness of our method. All the codes of our experiments\nwill be available for reproducibility.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yinghui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shulin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qingyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yangning Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Ruiyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yunbo Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Hai-Tao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Ying Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-07-19T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Global-Local Stepwise Generative Network for Ultra High-Resolution Image Restoration. (arXiv:2207.08808v1 [cs.CV])","link":"http://arxiv.org/abs/2207.08808","description":"<p>While the research on image background restoration from regular size of\ndegraded images has achieved remarkable progress, restoring ultra\nhigh-resolution (e.g., 4K) images remains an extremely challenging task due to\nthe explosion of computational complexity and memory usage, as well as the\ndeficiency of annotated data. In this paper we present a novel model for ultra\nhigh-resolution image restoration, referred to as the Global-Local Stepwise\nGenerative Network (GLSGN), which employs a stepwise restoring strategy\ninvolving four restoring pathways: three local pathways and one global pathway.\nThe local pathways focus on conducting image restoration in a fine-grained\nmanner over local but high-resolution image patches, while the global pathway\nperforms image restoration coarsely on the scale-down but intact image to\nprovide cues for the local pathways in a global view including semantics and\nnoise patterns. To smooth the mutual collaboration between these four pathways,\nour GLSGN is designed to ensure the inter-pathway consistency in four aspects\nin terms of low-level content, perceptual attention, restoring intensity and\nhigh-level semantics, respectively. As another major contribution of this work,\nwe also introduce the first ultra high-resolution dataset to date for both\nreflection removal and rain streak removal, comprising 4,670 real-world and\nsynthetic images. Extensive experiments across three typical tasks for image\nbackground restoration, including image reflection removal, image rain streak\nremoval and image dehazing, show that our GLSGN consistently outperforms\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1\">Xin Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Haobo Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pei_W/0/1/0/all/0/1\">Wenjie Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Fanglin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">David Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_G/0/1/0/all/0/1\">Guangming Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Audio Input Generates Continuous Frames to Synthesize Facial Video Using Generative Adiversarial Networks. (arXiv:2207.08813v1 [cs.SD])","link":"http://arxiv.org/abs/2207.08813","description":"<p>This paper presents a simple method for speech videos generation based on\naudio: given a piece of audio, we can generate a video of the target face\nspeaking this audio. We propose Generative Adversarial Networks (GAN) with cut\nspeech audio input as condition and use Convolutional Gate Recurrent Unit (GRU)\nin generator and discriminator. Our model is trained by exploiting the short\naudio and the frames in this duration. For training, we cut the audio and\nextract the face in the corresponding frames. We designed a simple encoder and\ncompare the generated frames using GAN with and without GRU. We use GRU for\ntemporally coherent frames and the results show that short audio can produce\nrelatively realistic output results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hanhaodi Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prior-Guided Adversarial Initialization for Fast Adversarial Training. (arXiv:2207.08859v1 [cs.CV])","link":"http://arxiv.org/abs/2207.08859","description":"<p>Fast adversarial training (FAT) effectively improves the efficiency of\nstandard adversarial training (SAT). However, initial FAT encounters\ncatastrophic overfitting, i.e.,the robust accuracy against adversarial attacks\nsuddenly and dramatically decreases. Though several FAT variants spare no\neffort to prevent overfitting, they sacrifice much calculation cost. In this\npaper, we explore the difference between the training processes of SAT and FAT\nand observe that the attack success rate of adversarial examples (AEs) of FAT\ngets worse gradually in the late training stage, resulting in overfitting. The\nAEs are generated by the fast gradient sign method (FGSM) with a zero or random\ninitialization. Based on the observation, we propose a prior-guided FGSM\ninitialization method to avoid overfitting after investigating several\ninitialization strategies, improving the quality of the AEs during the whole\ntraining process. The initialization is formed by leveraging historically\ngenerated AEs without additional calculation cost. We further provide a\ntheoretical analysis for the proposed initialization method. We also propose a\nsimple yet effective regularizer based on the prior-guided initialization,i.e.,\nthe currently generated perturbation should not deviate too much from the\nprior-guided initialization. The regularizer adopts both historical and current\nadversarial perturbations to guide the model learning. Evaluations on four\ndatasets demonstrate that the proposed method can prevent catastrophic\noverfitting and outperform state-of-the-art FAT methods. The code is released\nat https://github.com/jiaxiaojunQAQ/FGSM-PGI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1\">Xiaojun Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xingxing Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Baoyuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Ke Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1\">Xiaochun Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Romanus: Robust Task Offloading in Modular Multi-Sensor Autonomous Driving Systems. (arXiv:2207.08865v1 [cs.DC])","link":"http://arxiv.org/abs/2207.08865","description":"<p>Due to the high performance and safety requirements of self-driving\napplications, the complexity of modern autonomous driving systems (ADS) has\nbeen growing, instigating the need for more sophisticated hardware which could\nadd to the energy footprint of the ADS platform. Addressing this, edge\ncomputing is poised to encompass self-driving applications, enabling the\ncompute-intensive autonomy-related tasks to be offloaded for processing at\ncompute-capable edge servers. Nonetheless, the intricate hardware architecture\nof ADS platforms, in addition to the stringent robustness demands, set forth\ncomplications for task offloading which are unique to autonomous driving.\nHence, we present $ROMANUS$, a methodology for robust and efficient task\noffloading for modular ADS platforms with multi-sensor processing pipelines.\nOur methodology entails two phases: (i) the introduction of efficient\noffloading points along the execution path of the involved deep learning\nmodels, and (ii) the implementation of a runtime solution based on Deep\nReinforcement Learning to adapt the operating mode according to variations in\nthe perceived road scene complexity, network connectivity, and server load.\nExperiments on the object detection use case demonstrated that our approach is\n14.99% more energy-efficient than pure local execution while achieving a 77.06%\nreduction in risky behavior from a robust-agnostic offloading baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Luke Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Odema_M/0/1/0/all/0/1\">Mohanad Odema</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faruque_M/0/1/0/all/0/1\">Mohammad Abdullah Al Faruque</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FLAIR: Federated Learning Annotated Image Repository. (arXiv:2207.08869v1 [cs.LG])","link":"http://arxiv.org/abs/2207.08869","description":"<p>Cross-device federated learning is an emerging machine learning (ML) paradigm\nwhere a large population of devices collectively train an ML model while the\ndata remains on the devices. This research field has a unique set of practical\nchallenges, and to systematically make advances, new datasets curated to be\ncompatible with this paradigm are needed. Existing federated learning\nbenchmarks in the image domain do not accurately capture the scale and\nheterogeneity of many real-world use cases. We introduce FLAIR, a challenging\nlarge-scale annotated image dataset for multi-label classification suitable for\nfederated learning. FLAIR has 429,078 images from 51,414 Flickr users and\ncaptures many of the intricacies typically encountered in federated learning,\nsuch as heterogeneous user data and a long-tailed label distribution. We\nimplement multiple baselines in different learning setups for different tasks\non this dataset. We believe FLAIR can serve as a challenging benchmark for\nadvancing the state-of-the art in federated learning. Dataset access and the\ncode for the benchmark are available at\n\\url{https://github.com/apple/ml-flair}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1\">Congzheng Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Granqvist_F/0/1/0/all/0/1\">Filip Granqvist</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talwar_K/0/1/0/all/0/1\">Kunal Talwar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prior Knowledge Guided Unsupervised Domain Adaptation. (arXiv:2207.08877v1 [cs.LG])","link":"http://arxiv.org/abs/2207.08877","description":"<p>The waive of labels in the target domain makes Unsupervised Domain Adaptation\n(UDA) an attractive technique in many real-world applications, though it also\nbrings great challenges as model adaptation becomes harder without labeled\ntarget data. In this paper, we address this issue by seeking compensation from\ntarget domain prior knowledge, which is often (partially) available in\npractice, e.g., from human expertise. This leads to a novel yet practical\nsetting where in addition to the training data, some prior knowledge about the\ntarget class distribution are available. We term the setting as\nKnowledge-guided Unsupervised Domain Adaptation (KUDA). In particular, we\nconsider two specific types of prior knowledge about the class distribution in\nthe target domain: Unary Bound that describes the lower and upper bounds of\nindividual class probabilities, and Binary Relationship that describes the\nrelations between two class probabilities. We propose a general rectification\nmodule that uses such prior knowledge to refine model generated pseudo labels.\nThe module is formulated as a Zero-One Programming problem derived from the\nprior knowledge and a smooth regularizer. It can be easily plugged into\nself-training based UDA methods, and we combine it with two state-of-the-art\nmethods, SHOT and DINE. Empirical results on four benchmarks confirm that the\nrectification module clearly improves the quality of pseudo labels, which in\nturn benefits the self-training stage. With the guidance from prior knowledge,\nthe performances of both methods are substantially boosted. We expect our work\nto inspire further investigations in integrating prior knowledge in UDA. Code\nis available at https://github.com/tsun/KUDA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1\">Tao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Cheng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_H/0/1/0/all/0/1\">Haibin Ling</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A hierarchical semantic segmentation framework for computer vision-based bridge damage detection. (arXiv:2207.08878v1 [cs.CV])","link":"http://arxiv.org/abs/2207.08878","description":"<p>Computer vision-based damage detection using remote cameras and unmanned\naerial vehicles (UAVs) enables efficient and low-cost bridge health monitoring\nthat reduces labor costs and the needs for sensor installation and maintenance.\nBy leveraging recent semantic image segmentation approaches, we are able to\nfind regions of critical structural components and recognize damage at the\npixel level using images as the only input. However, existing methods perform\npoorly when detecting small damages (e.g., cracks and exposed rebars) and thin\nobjects with limited image samples, especially when the components of interest\nare highly imbalanced. To this end, this paper introduces a semantic\nsegmentation framework that imposes the hierarchical semantic relationship\nbetween component category and damage types. For example, certain concrete\ncracks only present on bridge columns and therefore the non-column region will\nbe masked out when detecting such damages. In this way, the damage detection\nmodel could focus on learning features from possible damaged regions only and\navoid the effects of other irrelevant regions. We also utilize multi-scale\naugmentation that provides views with different scales that preserves\ncontextual information of each image without losing the ability of handling\nsmall and thin objects. Furthermore, the proposed framework employs important\nsampling that repeatedly samples images containing rare components (e.g.,\nrailway sleeper and exposed rebars) to provide more data samples, which\naddresses the imbalanced data challenge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jingxiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yujie Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bingqing Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NeuForm: Adaptive Overfitting for Neural Shape Editing. (arXiv:2207.08890v1 [cs.CV])","link":"http://arxiv.org/abs/2207.08890","description":"<p>Neural representations are popular for representing shapes, as they can be\nlearned form sensor data and used for data cleanup, model completion, shape\nediting, and shape synthesis. Current neural representations can be categorized\nas either overfitting to a single object instance, or representing a collection\nof objects. However, neither allows accurate editing of neural scene\nrepresentations: on the one hand, methods that overfit objects achieve highly\naccurate reconstructions, but do not generalize to unseen object configurations\nand thus cannot support editing; on the other hand, methods that represent a\nfamily of objects with variations do generalize but produce only approximate\nreconstructions. We propose NEUFORM to combine the advantages of both\noverfitted and generalizable representations by adaptively using the one most\nappropriate for each shape region: the overfitted representation where reliable\ndata is available, and the generalizable representation everywhere else. We\nachieve this with a carefully designed architecture and an approach that blends\nthe network weights of the two representations, avoiding seams and other\nartifacts. We demonstrate edits that successfully reconfigure parts of\nhuman-designed shapes, such as chairs, tables, and lamps, while preserving\nsemantic integrity and the accuracy of an overfitted shape representation. We\ncompare with two state-of-the-art competitors and demonstrate clear\nimprovements in terms of plausibility and fidelity of the resultant edits.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Connor Z. Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitra_N/0/1/0/all/0/1\">Niloy J. Mitra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wetzstein_G/0/1/0/all/0/1\">Gordon Wetzstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1\">Leonidas Guibas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guerrero_P/0/1/0/all/0/1\">Paul Guerrero</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conditional DETR V2: Efficient Detection Transformer with Box Queries. (arXiv:2207.08914v1 [cs.CV])","link":"http://arxiv.org/abs/2207.08914","description":"<p>In this paper, we are interested in Detection Transformer (DETR), an\nend-to-end object detection approach based on a transformer encoder-decoder\narchitecture without hand-crafted postprocessing, such as NMS. Inspired by\nConditional DETR, an improved DETR with fast training convergence, that\npresented box queries (originally called spatial queries) for internal decoder\nlayers, we reformulate the object query into the format of the box query that\nis a composition of the embeddings of the reference point and the\ntransformation of the box with respect to the reference point. This\nreformulation indicates the connection between the object query in DETR and the\nanchor box that is widely studied in Faster R-CNN. Furthermore, we learn the\nbox queries from the image content, further improving the detection quality of\nConditional DETR still with fast training convergence. In addition, we adopt\nthe idea of axial self-attention to save the memory cost and accelerate the\nencoder. The resulting detector, called Conditional DETR V2, achieves better\nresults than Conditional DETR, saves the memory cost and runs more efficiently.\nFor example, for the DC$5$-ResNet-$50$ backbone, our approach achieves $44.8$\nAP with $16.4$ FPS on the COCO $val$ set and compared to Conditional DETR, it\nruns $1.6\\times$ faster, saves $74$\\% of the overall memory cost, and improves\n$1.0$ AP score.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaokang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Fangyun Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_G/0/1/0/all/0/1\">Gang Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingdong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recognizing Hand Use and Hand Role at Home After Stroke from Egocentric Video. (arXiv:2207.08920v1 [cs.CV])","link":"http://arxiv.org/abs/2207.08920","description":"<p>Introduction: Hand function is a central determinant of independence after\nstroke. Measuring hand use in the home environment is necessary to evaluate the\nimpact of new interventions, and calls for novel wearable technologies.\nEgocentric video can capture hand-object interactions in context, as well as\nshow how more-affected hands are used during bilateral tasks (for stabilization\nor manipulation). Automated methods are required to extract this information.\nObjective: To use artificial intelligence-based computer vision to classify\nhand use and hand role from egocentric videos recorded at home after stroke.\nMethods: Twenty-one stroke survivors participated in the study. A random forest\nclassifier, a SlowFast neural network, and the Hand Object Detector neural\nnetwork were applied to identify hand use and hand role at home.\nLeave-One-Subject-Out-Cross-Validation (LOSOCV) was used to evaluate the\nperformance of the three models. Between-group differences of the models were\ncalculated based on the Mathews correlation coefficient (MCC). Results: For\nhand use detection, the Hand Object Detector had significantly higher\nperformance than the other models. The macro average MCCs using this model in\nthe LOSOCV were 0.50 +- 0.23 for the more-affected hands and 0.58 +- 0.18 for\nthe less-affected hands. Hand role classification had macro average MCCs in the\nLOSOCV that were close to zero for all models. Conclusion: Using egocentric\nvideo to capture the hand use of stroke survivors at home is feasible. Pose\nestimation to track finger movements may be beneficial to classifying hand\nroles in the future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tsai_M/0/1/0/all/0/1\">Meng-Fen Tsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rosalie H. Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zariffa_J/0/1/0/all/0/1\">Jo&#x15b;e Zariffa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"I2I: Image to Icosahedral Projection for $\\mathrm{SO}(3)$ Object Reasoning from Single-View Images. (arXiv:2207.08925v1 [cs.CV])","link":"http://arxiv.org/abs/2207.08925","description":"<p>Reasoning about 3D objects based on 2D images is challenging due to large\nvariations in appearance caused by viewing the object from different\norientations. Ideally, our model would be invariant or equivariant to changes\nin object pose. Unfortunately, this is typically not possible with 2D image\ninput because we do not have an a priori model of how the image would change\nunder out-of-plane object rotations. The only $\\mathrm{SO}(3)$-equivariant\nmodels that currently exist require point cloud input rather than 2D images. In\nthis paper, we propose a novel model architecture based on icosahedral group\nconvolution that reasons in $\\mathrm{SO(3)}$ by projecting the input image onto\nan icosahedron. As a result of this projection, the model is approximately\nequivariant to rotation in $\\mathrm{SO}(3)$. We apply this model to an object\npose estimation task and find that it outperforms reasonable baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Klee_D/0/1/0/all/0/1\">David Klee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biza_O/0/1/0/all/0/1\">Ondrej Biza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Platt_R/0/1/0/all/0/1\">Robert Platt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walters_R/0/1/0/all/0/1\">Robin Walters</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Easy Batch Normalization. (arXiv:2207.08940v1 [cs.LG])","link":"http://arxiv.org/abs/2207.08940","description":"<p>It was shown that adversarial examples improve object recognition. But what\nabout their opposite side, easy examples? Easy examples are samples that the\nmachine learning model classifies correctly with high confidence. In our paper,\nwe are making the first step toward exploring the potential benefits of using\neasy examples in the training procedure of neural networks. We propose to use\nan auxiliary batch normalization for easy examples for the standard and robust\naccuracy improvement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Asadulaev_A/0/1/0/all/0/1\">Arip Asadulaev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panfilov_A/0/1/0/all/0/1\">Alexander Panfilov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Filchenkov_A/0/1/0/all/0/1\">Andrey Filchenkov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robustar: Interactive Toolbox Supporting Precise Data Annotation for Robust Vision Learning. (arXiv:2207.08944v1 [cs.CV])","link":"http://arxiv.org/abs/2207.08944","description":"<p>We introduce the initial release of our software Robustar, which aims to\nimprove the robustness of vision classification machine learning models through\na data-driven perspective. Building upon the recent understanding that the lack\nof machine learning model's robustness is the tendency of the model's learning\nof spurious features, we aim to solve this problem from its root at the data\nperspective by removing the spurious features from the data before training. In\nparticular, we introduce a software that helps the users to better prepare the\ndata for training image classification models by allowing the users to annotate\nthe spurious features at the pixel level of images. To facilitate this process,\nour software also leverages recent advances to help identify potential images\nand pixels worthy of attention and to continue the training with newly\nannotated data. Our software is hosted at the GitHub Repository\nhttps://github.com/HaohanWang/Robustar.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chonghan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haohan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1\">Leyang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1\">Shuguang Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jingcheng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xinnuo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Linjing Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1\">Eric P. Xing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-step domain adaptation by adversarial attack to $\\mathcal{H} \\Delta \\mathcal{H}$-divergence. (arXiv:2207.08948v1 [cs.LG])","link":"http://arxiv.org/abs/2207.08948","description":"<p>Adversarial examples are transferable between different models. In our paper,\nwe propose to use this property for multi-step domain adaptation. In\nunsupervised domain adaptation settings, we demonstrate that replacing the\nsource domain with adversarial examples to $\\mathcal{H} \\Delta\n\\mathcal{H}$-divergence can improve source classifier accuracy on the target\ndomain. Our method can be connected to most domain adaptation techniques. We\nconducted a range of experiments and achieved improvement in accuracy on Digits\nand Office-Home datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Asadulaev_A/0/1/0/all/0/1\">Arip Asadulaev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panfilov_A/0/1/0/all/0/1\">Alexander Panfilov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Filchenkov_A/0/1/0/all/0/1\">Andrey Filchenkov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MonoIndoor++:Towards Better Practice of Self-Supervised Monocular Depth Estimation for Indoor Environments. (arXiv:2207.08951v1 [cs.CV])","link":"http://arxiv.org/abs/2207.08951","description":"<p>Self-supervised monocular depth estimation has seen significant progress in\nrecent years, especially in outdoor environments. However, depth prediction\nresults are not satisfying in indoor scenes where most of the existing data are\ncaptured with hand-held devices. As compared to outdoor environments,\nestimating depth of monocular videos for indoor environments, using\nself-supervised methods, results in two additional challenges: (i) the depth\nrange of indoor video sequences varies a lot across different frames, making it\ndifficult for the depth network to induce consistent depth cues for training;\n(ii) the indoor sequences recorded with handheld devices often contain much\nmore rotational motions, which cause difficulties for the pose network to\npredict accurate relative camera poses. In this work, we propose a novel\nframework-MonoIndoor++ by giving special considerations to those challenges and\nconsolidating a set of good practices for improving the performance of\nself-supervised monocular depth estimation for indoor environments. First, a\ndepth factorization module with transformer-based scale regression network is\nproposed to estimate a global depth scale factor explicitly, and the predicted\nscale factor can indicate the maximum depth values. Second, rather than using a\nsingle-stage pose estimation strategy as in previous methods, we propose to\nutilize a residual pose estimation module to estimate relative camera poses\nacross consecutive frames iteratively. Third, to incorporate extensive\ncoordinates guidance for our residual pose estimation module, we propose to\nperform coordinate convolutional encoding directly over the inputs to pose\nnetworks. The proposed method is validated on a variety of benchmark indoor\ndatasets, i.e., EuRoC MAV, NYUv2, ScanNet and 7-Scenes, demonstrating the\nstate-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Runze Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_P/0/1/0/all/0/1\">Pan Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhanu_B/0/1/0/all/0/1\">Bir Bhanu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploiting Unlabeled Data with Vision and Language Models for Object Detection. (arXiv:2207.08954v1 [cs.CV])","link":"http://arxiv.org/abs/2207.08954","description":"<p>Building robust and generic object detection frameworks requires scaling to\nlarger label spaces and bigger training datasets. However, it is prohibitively\ncostly to acquire annotations for thousands of categories at a large scale. We\npropose a novel method that leverages the rich semantics available in recent\nvision and language models to localize and classify objects in unlabeled\nimages, effectively generating pseudo labels for object detection. Starting\nwith a generic and class-agnostic region proposal mechanism, we use vision and\nlanguage models to categorize each region of an image into any object category\nthat is required for downstream tasks. We demonstrate the value of the\ngenerated pseudo labels in two specific tasks, open-vocabulary detection, where\na model needs to generalize to unseen object categories, and semi-supervised\nobject detection, where additional unlabeled images can be used to improve the\nmodel. Our empirical evaluation shows the effectiveness of the pseudo labels in\nboth tasks, where we outperform competitive baselines and achieve a novel\nstate-of-the-art for open-vocabulary object detection. Our code is available at\nhttps://github.com/xiaofeng94/VL-PLM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shiyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhixing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schulter_S/0/1/0/all/0/1\">Samuel Schulter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Long Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+G_V/0/1/0/all/0/1\">Vijay Kumar B.G</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stathopoulos_A/0/1/0/all/0/1\">Anastasis Stathopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandraker_M/0/1/0/all/0/1\">Manmohan Chandraker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metaxas_D/0/1/0/all/0/1\">Dimitris Metaxas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Space-time Video Super-resolution via Spatial-temporal Feature Interaction. (arXiv:2207.08960v1 [cs.CV])","link":"http://arxiv.org/abs/2207.08960","description":"<p>The target of space-time video super-resolution (STVSR) is to increase both\nthe frame rate (also referred to as the temporal resolution) and the spatial\nresolution of a given video. Recent approaches solve STVSR with end-to-end deep\nneural networks. A popular solution is to first increase the frame rate of the\nvideo; then perform feature refinement among different frame features; and last\nincrease the spatial resolutions of these features. The temporal correlation\namong features of different frames is carefully exploited in this process. The\nspatial correlation among features of different (spatial) resolutions, despite\nbeing also very important, is however not emphasized. In this paper, we propose\na spatial-temporal feature interaction network to enhance STVSR by exploiting\nboth spatial and temporal correlations among features of different frames and\nspatial resolutions. Specifically, the spatial-temporal frame interpolation\nmodule is introduced to interpolate low- and high-resolution intermediate frame\nfeatures simultaneously and interactively. The spatial-temporal local and\nglobal refinement modules are respectively deployed afterwards to exploit the\nspatial-temporal correlation among different features for their refinement.\nFinally, a novel motion consistency loss is employed to enhance the motion\ncontinuity among reconstructed frames. We conduct experiments on three standard\nbenchmarks, Vid4, Vimeo-90K and Adobe240, and the results demonstrate that our\nmethod improves the state of the art methods by a considerable margin. Our\ncodes will be available at\nhttps://github.com/yuezijie/STINet-Space-time-Video-Super-resolution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yue_Z/0/1/0/all/0/1\">Zijie Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_M/0/1/0/all/0/1\">Miaojing Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1\">Shuai Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shanlin Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Superficial White Matter Analysis: An Efficient Point-cloud-based Deep Learning Framework with Supervised Contrastive Learning for Consistent Tractography Parcellation across Populations and dMRI Acquisitions. (arXiv:2207.08975v1 [eess.IV])","link":"http://arxiv.org/abs/2207.08975","description":"<p>Diffusion MRI tractography is an advanced imaging technique that enables in\nvivo mapping of the brain's white matter connections. White matter parcellation\nclassifies tractography streamlines into clusters or anatomically meaningful\ntracts. It enables quantification and visualization of whole-brain\ntractography. Currently, most parcellation methods focus on the deep white\nmatter (DWM), whereas fewer methods address the superficial white matter (SWM)\ndue to its complexity. We propose a novel two-stage deep-learning-based\nframework, Superficial White Matter Analysis (SupWMA), that performs an\nefficient and consistent parcellation of 198 SWM clusters from whole-brain\ntractography. A point-cloud-based network is adapted to our SWM parcellation\ntask, and supervised contrastive learning enables more discriminative\nrepresentations between plausible streamlines and outliers for SWM. We train\nour model on a large-scale tractography dataset including streamline samples\nfrom labeled SWM clusters and anatomically implausible streamline samples, and\nwe perform testing on six independently acquired datasets of different ages and\nhealth conditions (including neonates and patients with space-occupying brain\ntumors). Compared to several state-of-the-art methods, SupWMA obtains highly\nconsistent and accurate SWM parcellation results on all datasets, showing good\ngeneralization across the lifespan in health and disease. In addition, the\ncomputational speed of SupWMA is much faster than other methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Xue_T/0/1/0/all/0/1\">Tengfei Xue</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_F/0/1/0/all/0/1\">Fan Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_C/0/1/0/all/0/1\">Chaoyi Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1\">Yuqian Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Song_Y/0/1/0/all/0/1\">Yang Song</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Golby_A/0/1/0/all/0/1\">Alexandra J. Golby</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Makris_N/0/1/0/all/0/1\">Nikos Makris</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rathi_Y/0/1/0/all/0/1\">Yogesh Rathi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cai_W/0/1/0/all/0/1\">Weidong Cai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+ODonnell_L/0/1/0/all/0/1\">Lauren J. O&#x27;Donnell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SelectionConv: Convolutional Neural Networks for Non-rectilinear Image Data. (arXiv:2207.08979v1 [cs.CV])","link":"http://arxiv.org/abs/2207.08979","description":"<p>Convolutional Neural Networks have revolutionized vision applications. There\nare image domains and representations, however, that cannot be handled by\nstandard CNNs (e.g., spherical images, superpixels). Such data are usually\nprocessed using networks and algorithms specialized for each type. In this\nwork, we show that it may not always be necessary to use specialized neural\nnetworks to operate on such spaces. Instead, we introduce a new structured\ngraph convolution operator that can copy 2D convolution weights, transferring\nthe capabilities of already trained traditional CNNs to our new graph network.\nThis network can then operate on any data that can be represented as a\npositional graph. By converting non-rectilinear data to a graph, we can apply\nthese convolutions on these irregular image domains without requiring training\non large domain-specific datasets. Results of transferring pre-trained image\nnetworks for segmentation, stylization, and depth prediction are demonstrated\nfor a variety of such data forms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hart_D/0/1/0/all/0/1\">David Hart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Whitney_M/0/1/0/all/0/1\">Michael Whitney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morse_B/0/1/0/all/0/1\">Bryan Morse</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeformIrisNet: An Identity-Preserving Model of Iris Texture Deformation. (arXiv:2207.08980v1 [cs.CV])","link":"http://arxiv.org/abs/2207.08980","description":"<p>Nonlinear iris texture deformations due to pupil size variations are one of\nthe main factors responsible for within-class variance of genuine comparison\nscores in iris recognition. In dominant approaches to iris recognition, the\nsize of a ring-shaped iris region is linearly scaled to a canonical rectangle,\nused further in encoding and matching. However, the biological complexity of\niris sphincter and dilator muscles causes the movements of iris features to be\nnonlinear in a function of pupil size, and not solely organized along radial\npaths. Alternatively to the existing theoretical models based on biomechanics\nof iris musculature, in this paper we propose a novel deep autoencoder-based\nmodel that can effectively learn complex movements of iris texture features\ndirectly from the data. The proposed model takes two inputs, (a) an\nISO-compliant near-infrared iris image with initial pupil size, and (b) the\nbinary mask defining the target shape of the iris. The model makes all the\nnecessary nonlinear deformations to the iris texture to match the shape of iris\nin image (a) with the shape provided by the target mask (b). The\nidentity-preservation component of the loss function helps the model in finding\ndeformations that preserve identity and not only visual realism of generated\nsamples. We also demonstrate two immediate applications of this model: better\ncompensation for iris texture deformations in iris recognition algorithms,\ncompared to linear models, and creation of generative algorithm that can aid\nhuman forensic examiners, who may need to compare iris images with large\ndifference in pupil dilation. We offer the source codes and model weights\navailable along with this paper.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1\">Siamul Karim Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tinsley_P/0/1/0/all/0/1\">Patrick Tinsley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Czajka_A/0/1/0/all/0/1\">Adam Czajka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Capabilities, Limitations and Challenges of Style Transfer with CycleGANs: A Study on Automatic Ring Design Generation. (arXiv:2207.08989v1 [cs.CV])","link":"http://arxiv.org/abs/2207.08989","description":"<p>Rendering programs have changed the design process completely as they permit\nto see how the products will look before they are fabricated. However, the\nrendering process is complicated and takes a significant amount of time, not\nonly in the rendering itself but in the setting of the scene as well.\nMaterials, lights and cameras need to be set in order to get the best quality\nresults. Nevertheless, the optimal output may not be obtained in the first\nrender. This all makes the rendering process a tedious process. Since\nGoodfellow et al. introduced Generative Adversarial Networks (GANs) in 2014\n[1], they have been used to generate computer-assigned synthetic data, from\nnon-existing human faces to medical data analysis or image style transfer. GANs\nhave been used to transfer image textures from one domain to another. However,\npaired data from both domains was needed. When Zhu et al. introduced the\nCycleGAN model, the elimination of this expensive constraint permitted\ntransforming one image from one domain into another, without the need for\npaired data. This work validates the applicability of CycleGANs on style\ntransfer from an initial sketch to a final render in 2D that represents a 3D\ndesign, a step that is paramount in every product design process. We inquiry\nthe possibilities of including CycleGANs as part of the design pipeline, more\nprecisely, applied to the rendering of ring designs. Our contribution entails a\ncrucial part of the process as it allows the customer to see the final product\nbefore buying. This work sets a basis for future research, showing the\npossibilities of GANs in design and establishing a starting point for novel\napplications to approach crafts design.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pedroso_T/0/1/0/all/0/1\">Tomas Cabezon Pedroso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ser_J/0/1/0/all/0/1\">Javier Del Ser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diaz_Rodriguez_N/0/1/0/all/0/1\">Natalia Diaz-Rodr&#x131;guez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Structure from Action: Learning Interactions for Articulated Object 3D Structure Discovery. (arXiv:2207.08997v1 [cs.CV])","link":"http://arxiv.org/abs/2207.08997","description":"<p>Articulated objects are abundant in daily life. Discovering their parts,\njoints, and kinematics is crucial for robots to interact with these objects. We\nintroduce Structure from Action (SfA), a framework that discovers the 3D part\ngeometry and joint parameters of unseen articulated objects via a sequence of\ninferred interactions. Our key insight is that 3D interaction and perception\nshould be considered in conjunction to construct 3D articulated CAD models,\nespecially in the case of categories not seen during training. By selecting\ninformative interactions, SfA discovers parts and reveals initially occluded\nsurfaces, like the inside of a closed drawer. By aggregating visual\nobservations in 3D, SfA accurately segments multiple parts, reconstructs part\ngeometry, and infers all joint parameters in a canonical coordinate frame. Our\nexperiments demonstrate that a single SfA model trained in simulation can\ngeneralize to many unseen object categories with unknown kinematic structures\nand to real-world objects. Code and data will be publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nie_N/0/1/0/all/0/1\">Neil Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gadre_S/0/1/0/all/0/1\">Samir Yitzhak Gadre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ehsani_K/0/1/0/all/0/1\">Kiana Ehsani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Shuran Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discovering novel systemic biomarkers in photos of the external eye. (arXiv:2207.08998v1 [eess.IV])","link":"http://arxiv.org/abs/2207.08998","description":"<p>External eye photos were recently shown to reveal signs of diabetic retinal\ndisease and elevated HbA1c. In this paper, we evaluate if external eye photos\ncontain information about additional systemic medical conditions. We developed\na deep learning system (DLS) that takes external eye photos as input and\npredicts multiple systemic parameters, such as those related to the liver\n(albumin, AST); kidney (eGFR estimated using the race-free 2021 CKD-EPI\ncreatinine equation, the urine ACR); bone &amp; mineral (calcium); thyroid (TSH);\nand blood count (Hgb, WBC, platelets). Development leveraged 151,237 images\nfrom 49,015 patients with diabetes undergoing diabetic eye screening in 11\nsites across Los Angeles county, CA. Evaluation focused on 9 pre-specified\nsystemic parameters and leveraged 3 validation sets (A, B, C) spanning 28,869\npatients with and without diabetes undergoing eye screening in 3 independent\nsites in Los Angeles County, CA, and the greater Atlanta area, GA. We compared\nagainst baseline models incorporating available clinicodemographic variables\n(e.g. age, sex, race/ethnicity, years with diabetes). Relative to the baseline,\nthe DLS achieved statistically significant superior performance at detecting\nAST&gt;36, calcium&lt;8.6, eGFR&lt;60, Hgb&lt;11, platelets&lt;150, ACR&gt;=300, and WBC&lt;4 on\nvalidation set A (a patient population similar to the development sets), where\nthe AUC of DLS exceeded that of the baseline by 5.2-19.4%. On validation sets B\nand C, with substantial patient population differences compared to the\ndevelopment sets, the DLS outperformed the baseline for ACR&gt;=300 and Hgb&lt;11 by\n7.3-13.2%. Our findings provide further evidence that external eye photos\ncontain important biomarkers of systemic health spanning multiple organ\nsystems. Further work is needed to investigate whether and how these biomarkers\ncan be translated into clinical impact.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Babenko_B/0/1/0/all/0/1\">Boris Babenko</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Traynis_I/0/1/0/all/0/1\">Ilana Traynis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_C/0/1/0/all/0/1\">Christina Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Singh_P/0/1/0/all/0/1\">Preeti Singh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Uddin_A/0/1/0/all/0/1\">Akib Uddin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cuadros_J/0/1/0/all/0/1\">Jorge Cuadros</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Daskivich_L/0/1/0/all/0/1\">Lauren P. Daskivich</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maa_A/0/1/0/all/0/1\">April Y. Maa</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_R/0/1/0/all/0/1\">Ramasamy Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kang_E/0/1/0/all/0/1\">Eugene Yu-Chuan Kang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Matias_Y/0/1/0/all/0/1\">Yossi Matias</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Corrado_G/0/1/0/all/0/1\">Greg S. Corrado</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Peng_L/0/1/0/all/0/1\">Lily Peng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Webster_D/0/1/0/all/0/1\">Dale R. Webster</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Semturs_C/0/1/0/all/0/1\">Christopher Semturs</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Krause_J/0/1/0/all/0/1\">Jonathan Krause</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Varadarajan_A/0/1/0/all/0/1\">Avinash V. Varadarajan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hammel_N/0/1/0/all/0/1\">Naama Hammel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yun Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SS-MFAR : Semi-supervised Multi-task Facial Affect Recognition. (arXiv:2207.09012v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09012","description":"<p>Automatic affect recognition has applications in many areas such as\neducation, gaming, software development, automotives, medical care, etc. but it\nis non trivial task to achieve appreciable performance on in-the-wild data\nsets. In-the-wild data sets though represent real-world scenarios better than\nsynthetic data sets, the former ones suffer from the problem of incomplete\nlabels. Inspired by semi-supervised learning, in this paper, we introduce our\nsubmission to the Multi-Task-Learning Challenge at the 4th Affective Behavior\nAnalysis in-the-wild (ABAW) 2022 Competition. The three tasks that are\nconsidered in this challenge are valence-arousal(VA) estimation, classification\nof expressions into 6 basic (anger, disgust, fear, happiness, sadness,\nsurprise), neutral, and the 'other' category and 12 action units(AU) numbered\nAU-\\{1,2,4,6,7,10,12,15,23,24,25,26\\}. Our method Semi-supervised Multi-task\nFacial Affect Recognition titled \\textbf{SS-MFAR} uses a deep residual network\nwith task specific classifiers for each of the tasks along with adaptive\nthresholds for each expression class and semi-supervised learning for the\nincomplete labels. Source code is available at\nhttps://github.com/1980x/ABAW2022DMACS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gera_D/0/1/0/all/0/1\">Darshan Gera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_B/0/1/0/all/0/1\">Badveeti Naveen Siva Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_B/0/1/0/all/0/1\">Bobbili Veerendra Raj Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_S/0/1/0/all/0/1\">S Balasubramanian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Structure-aware Editable Morphable Model for 3D Facial Detail Animation and Manipulation. (arXiv:2207.09019v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09019","description":"<p>Morphable models are essential for the statistical modeling of 3D faces.\nPrevious works on morphable models mostly focus on large-scale facial geometry\nbut ignore facial details. This paper augments morphable models in representing\nfacial details by learning a Structure-aware Editable Morphable Model (SEMM).\nSEMM introduces a detail structure representation based on the distance field\nof wrinkle lines, jointly modeled with detail displacements to establish better\ncorrespondences and enable intuitive manipulation of wrinkle structure.\nBesides, SEMM introduces two transformation modules to translate expression\nblendshape weights and age values into changes in latent space, allowing\neffective semantic detail editing while maintaining identity. Extensive\nexperiments demonstrate that the proposed model compactly represents facial\ndetails, outperforms previous methods in expression animation qualitatively and\nquantitatively, and achieves effective age editing and wrinkle line editing of\nfacial details. Code and model are available at\nhttps://github.com/gerwang/facial-detail-manipulation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ling_J/0/1/0/all/0/1\">Jingwang Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhibo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_M/0/1/0/all/0/1\">Ming Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Quan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1\">Chen Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1\">Feng Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Indoor Localization for Personalized Ambient Assisted Living of Multiple Users in Multi-Floor Smart Environments. (arXiv:2207.09025v1 [cs.AI])","link":"http://arxiv.org/abs/2207.09025","description":"<p>This paper presents a multifunctional interdisciplinary framework that makes\nfour scientific contributions towards the development of personalized ambient\nassisted living, with a specific focus to address the different and dynamic\nneeds of the diverse aging population in the future of smart living\nenvironments. First, it presents a probabilistic reasoning-based mathematical\napproach to model all possible forms of user interactions for any activity\narising from the user diversity of multiple users in such environments. Second,\nit presents a system that uses this approach with a machine learning method to\nmodel individual user profiles and user-specific user interactions for\ndetecting the dynamic indoor location of each specific user. Third, to address\nthe need to develop highly accurate indoor localization systems for increased\ntrust, reliance, and seamless user acceptance, the framework introduces a novel\nmethodology where two boosting approaches Gradient Boosting and the AdaBoost\nalgorithm are integrated and used on a decision tree-based learning model to\nperform indoor localization. Fourth, the framework introduces two novel\nfunctionalities to provide semantic context to indoor localization in terms of\ndetecting each user's floor-specific location as well as tracking whether a\nspecific user was located inside or outside a given spatial region in a\nmulti-floor-based indoor setting. These novel functionalities of the proposed\nframework were tested on a dataset of localization-related Big Data collected\nfrom 18 different users who navigated in 3 buildings consisting of 5 floors and\n254 indoor spatial regions. The results show that this approach of indoor\nlocalization for personalized AAL that models each specific user always\nachieves higher accuracy as compared to the traditional approach of modeling an\naverage user.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thakur_N/0/1/0/all/0/1\">Nirmalya Thakur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1\">Chia Y. Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ML-BPM: Multi-teacher Learning with Bidirectional Photometric Mixing for Open Compound Domain Adaptation in Semantic Segmentation. (arXiv:2207.09045v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09045","description":"<p>Open compound domain adaptation (OCDA) considers the target domain as the\ncompound of multiple unknown homogeneous subdomains. The goal of OCDA is to\nminimize the domain gap between the labeled source domain and the unlabeled\ncompound target domain, which benefits the model generalization to the unseen\ndomains. Current OCDA for semantic segmentation methods adopt manual domain\nseparation and employ a single model to simultaneously adapt to all the target\nsubdomains. However, adapting to a target subdomain might hinder the model from\nadapting to other dissimilar target subdomains, which leads to limited\nperformance. In this work, we introduce a multi-teacher framework with\nbidirectional photometric mixing to separately adapt to every target subdomain.\nFirst, we present an automatic domain separation to find the optimal number of\nsubdomains. On this basis, we propose a multi-teacher framework in which each\nteacher model uses bidirectional photometric mixing to adapt to one target\nsubdomain. Furthermore, we conduct an adaptive distillation to learn a student\nmodel and apply consistency regularization to improve the student\ngeneralization. Experimental results on benchmark datasets show the efficacy of\nthe proposed approach for both the compound domain and the open domains against\nexisting state-of-the-art approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_F/0/1/0/all/0/1\">Fei Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hur_S/0/1/0/all/0/1\">Sungsu Hur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seokju Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Junsik Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kweon_I/0/1/0/all/0/1\">In So Kweon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Prototype Mask for Occluded Person Re-Identification. (arXiv:2207.09046v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09046","description":"<p>Although person re-identification has achieved an impressive improvement in\nrecent years, the common occlusion case caused by different obstacles is still\nan unsettled issue in real application scenarios. Existing methods mainly\naddress this issue by employing body clues provided by an extra network to\ndistinguish the visible part. Nevertheless, the inevitable domain gap between\nthe assistant model and the ReID datasets has highly increased the difficulty\nto obtain an effective and efficient model. To escape from the extra\npre-trained networks and achieve an automatic alignment in an end-to-end\ntrainable network, we propose a novel Dynamic Prototype Mask (DPM) based on two\nself-evident prior knowledge. Specifically, we first devise a Hierarchical Mask\nGenerator which utilizes the hierarchical semantic to select the visible\npattern space between the high-quality holistic prototype and the feature\nrepresentation of the occluded input image. Under this condition, the occluded\nrepresentation could be well aligned in a selected subspace spontaneously.\nThen, to enrich the feature representation of the high-quality holistic\nprototype and provide a more complete feature space, we introduce a Head Enrich\nModule to encourage different heads to aggregate different patterns\nrepresentation in the whole image. Extensive experimental evaluations conducted\non occluded and holistic person re-identification benchmarks demonstrate the\nsuperior performance of the DPM over the state-of-the-art methods. The code is\nreleased at https://github.com/stone96123/DPM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_L/0/1/0/all/0/1\">Lei Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_P/0/1/0/all/0/1\">Pingyang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yongjian Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TTVFI: Learning Trajectory-Aware Transformer for Video Frame Interpolation. (arXiv:2207.09048v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09048","description":"<p>Video frame interpolation (VFI) aims to synthesize an intermediate frame\nbetween two consecutive frames. State-of-the-art approaches usually adopt a\ntwo-step solution, which includes 1) generating locally-warped pixels by\nflow-based motion estimations, 2) blending the warped pixels to form a full\nframe through deep neural synthesis networks. However, due to the inconsistent\nwarping from the two consecutive frames, the warped features for new frames are\nusually not aligned, which leads to distorted and blurred frames, especially\nwhen large and complex motions occur. To solve this issue, in this paper we\npropose a novel Trajectory-aware Transformer for Video Frame Interpolation\n(TTVFI). In particular, we formulate the warped features with inconsistent\nmotions as query tokens, and formulate relevant regions in a motion trajectory\nfrom two original consecutive frames into keys and values. Self-attention is\nlearned on relevant tokens along the trajectory to blend the pristine features\ninto intermediate frames through end-to-end training. Experimental results\ndemonstrate that our method outperforms other state-of-the-art methods in four\nwidely-used VFI benchmarks. Both code and pre-trained models will be released\nsoon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chengxu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Huan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jianlong Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_X/0/1/0/all/0/1\">Xueming Qian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RepBNN: towards a precise Binary Neural Network with Enhanced Feature Map via Repeating. (arXiv:2207.09049v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09049","description":"<p>Binary neural network (BNN) is an extreme quantization version of\nconvolutional neural networks (CNNs) with all features and weights mapped to\njust 1-bit. Although BNN saves a lot of memory and computation demand to make\nCNN applicable on edge or mobile devices, BNN suffers the drop of network\nperformance due to the reduced representation capability after binarization. In\nthis paper, we propose a new replaceable and easy-to-use convolution module\nRepConv, which enhances feature maps through replicating input or output along\nchannel dimension by $\\beta$ times without extra cost on the number of\nparameters and convolutional computation. We also define a set of RepTran rules\nto use RepConv throughout BNN modules like binary convolution, fully connected\nlayer and batch normalization. Experiments demonstrate that after the RepTran\ntransformation, a set of highly cited BNNs have achieved universally better\nperformance than the original BNN versions. For example, the Top-1 accuracy of\nRep-ReCU-ResNet-20, i.e., a RepBconv enhanced ReCU-ResNet-20, reaches 88.97% on\nCIFAR-10, which is 1.47% higher than that of the original network. And\nRep-AdamBNN-ReActNet-A achieves 71.342% Top-1 accuracy on ImageNet, a fresh\nstate-of-the-art result of BNNs. Code and models are available\nat:https://github.com/imfinethanks/Rep_AdamBNN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xulong Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Z/0/1/0/all/0/1\">Zhi Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jiaxuan Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_K/0/1/0/all/0/1\">Keqi Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yaru Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xuanyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hao Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Balanced Contrastive Learning for Long-Tailed Visual Recognition. (arXiv:2207.09052v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09052","description":"<p>Real-world data typically follow a long-tailed distribution, where a few\nmajority categories occupy most of the data while most minority categories\ncontain a limited number of samples. Classification models minimizing\ncross-entropy struggle to represent and classify the tail classes. Although the\nproblem of learning unbiased classifiers has been well studied, methods for\nrepresenting imbalanced data are under-explored. In this paper, we focus on\nrepresentation learning for imbalanced data. Recently, supervised contrastive\nlearning has shown promising performance on balanced data recently. However,\nthrough our theoretical analysis, we find that for long-tailed data, it fails\nto form a regular simplex which is an ideal geometric configuration for\nrepresentation learning. To correct the optimization behavior of SCL and\nfurther improve the performance of long-tailed visual recognition, we propose a\nnovel loss for balanced contrastive learning (BCL). Compared with SCL, we have\ntwo improvements in BCL: class-averaging, which balances the gradient\ncontribution of negative classes; class-complement, which allows all classes to\nappear in every mini-batch. The proposed balanced contrastive learning (BCL)\nmethod satisfies the condition of forming a regular simplex and assists the\noptimization of cross-entropy. Equipped with BCL, the proposed two-branch\nframework can obtain a stronger feature representation and achieve competitive\nperformance on long-tailed benchmark datasets such as CIFAR-10-LT,\nCIFAR-100-LT, ImageNet-LT, and iNaturalist2018. Our code is available at\n\\href{https://github.com/FlamieZhu/BCL}{this URL}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jianggang/0/1/0/all/0/1\">Jianggang</a>, Zhu, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng/0/1/0/all/0/1\">Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang/0/1/0/all/0/1\">Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jingjing/0/1/0/all/0/1\">Jingjing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen/0/1/0/all/0/1\">Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phoebe_Y/0/1/0/all/0/1\">Yi-Ping Phoebe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen/0/1/0/all/0/1\">Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu-Gang/0/1/0/all/0/1\">Yu-Gang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang/0/1/0/all/0/1\">Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Box-supervised Instance Segmentation with Level Set Evolution. (arXiv:2207.09055v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09055","description":"<p>In contrast to the fully supervised methods using pixel-wise mask labels,\nbox-supervised instance segmentation takes advantage of the simple box\nannotations, which has recently attracted a lot of research attentions. In this\npaper, we propose a novel single-shot box-supervised instance segmentation\napproach, which integrates the classical level set model with deep neural\nnetwork delicately. Specifically, our proposed method iteratively learns a\nseries of level sets through a continuous Chan-Vese energy-based function in an\nend-to-end fashion. A simple mask supervised SOLOv2 model is adapted to predict\nthe instance-aware mask map as the level set for each instance. Both the input\nimage and its deep features are employed as the input data to evolve the level\nset curves, where a box projection function is employed to obtain the initial\nboundary. By minimizing the fully differentiable energy function, the level set\nfor each instance is iteratively optimized within its corresponding bounding\nbox annotation. The experimental results on four challenging benchmarks\ndemonstrate the leading performance of our proposed approach to robust instance\nsegmentation in various scenarios. The code is available at:\nhttps://github.com/LiWentomng/boxlevelset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wentong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jianke Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_M/0/1/0/all/0/1\">Miaomiao Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_X/0/1/0/all/0/1\">Xiansheng Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-shot Open-set Recognition Using Background as Unknowns. (arXiv:2207.09059v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09059","description":"<p>Few-shot open-set recognition aims to classify both seen and novel images\ngiven only limited training data of seen classes. The challenge of this task is\nthat the model is required not only to learn a discriminative classifier to\nclassify the pre-defined classes with few training data but also to reject\ninputs from unseen classes that never appear at training time. In this paper,\nwe propose to solve the problem from two novel aspects. First, instead of\nlearning the decision boundaries between seen classes, as is done in standard\nclose-set classification, we reserve space for unseen classes, such that images\nlocated in these areas are recognized as the unseen classes. Second, to\neffectively learn such decision boundaries, we propose to utilize the\nbackground features from seen classes. As these background regions do not\nsignificantly contribute to the decision of close-set classification, it is\nnatural to use them as the pseudo unseen classes for classifier learning. Our\nextensive experiments show that our proposed method not only outperforms\nmultiple baselines but also sets new state-of-the-art results on three popular\nbenchmarks, namely tieredImageNet, miniImageNet, and Caltech-USCD\nBirds-200-2011 (CUB).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_N/0/1/0/all/0/1\">Nan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1\">Guosheng Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Moment Centralization based Gradient Descent Optimizers for Convolutional Neural Networks. (arXiv:2207.09066v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09066","description":"<p>Convolutional neural networks (CNNs) have shown very appealing performance\nfor many computer vision applications. The training of CNNs is generally\nperformed using stochastic gradient descent (SGD) based optimization\ntechniques. The adaptive momentum-based SGD optimizers are the recent trends.\nHowever, the existing optimizers are not able to maintain a zero mean in the\nfirst-order moment and struggle with optimization. In this paper, we propose a\nmoment centralization-based SGD optimizer for CNNs. Specifically, we impose the\nzero mean constraints on the first-order moment explicitly. The proposed moment\ncentralization is generic in nature and can be integrated with any of the\nexisting adaptive momentum-based optimizers. The proposed idea is tested with\nthree state-of-the-art optimization techniques, including Adam, Radam, and\nAdabelief on benchmark CIFAR10, CIFAR100, and TinyImageNet datasets for image\nclassification. The performance of the existing optimizers is generally\nimproved when integrated with the proposed moment centralization. Further, The\nresults of the proposed moment centralization are also better than the existing\ngradient centralization. The analytical analysis using the toy example shows\nthat the proposed method leads to a shorter and smoother optimization\ntrajectory. The source code is made publicly available at\n\\url{https://github.com/sumanthsadhu/MC-optimizer}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sadu_S/0/1/0/all/0/1\">Sumanth Sadu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dubey_S/0/1/0/all/0/1\">Shiv Ram Dubey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sreeja_S/0/1/0/all/0/1\">SR Sreeja</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Time Is MattEr: Temporal Self-supervision for Video Transformers. (arXiv:2207.09067v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09067","description":"<p>Understanding temporal dynamics of video is an essential aspect of learning\nbetter video representations. Recently, transformer-based architectural designs\nhave been extensively explored for video tasks due to their capability to\ncapture long-term dependency of input sequences. However, we found that these\nVideo Transformers are still biased to learn spatial dynamics rather than\ntemporal ones, and debiasing the spurious correlation is critical for their\nperformance. Based on the observations, we design simple yet effective\nself-supervised tasks for video models to learn temporal dynamics better.\nSpecifically, for debiasing the spatial bias, our method learns the temporal\norder of video frames as extra self-supervision and enforces the randomly\nshuffled frames to have low-confidence outputs. Also, our method learns the\ntemporal flow direction of video tokens among consecutive frames for enhancing\nthe correlation toward temporal dynamics. Under various video action\nrecognition tasks, we demonstrate the effectiveness of our method and its\ncompatibility with state-of-the-art Video Transformers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1\">Sukmin Yun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jaehyung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_D/0/1/0/all/0/1\">Dongyoon Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1\">Hwanjun Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ha_J/0/1/0/all/0/1\">Jung-Woo Ha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1\">Jinwoo Shin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Context Unaware Knowledge Distillation for Image Retrieval. (arXiv:2207.09070v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09070","description":"<p>Existing data-dependent hashing methods use large backbone networks with\nmillions of parameters and are computationally complex. Existing knowledge\ndistillation methods use logits and other features of the deep (teacher) model\nand as knowledge for the compact (student) model, which requires the teacher's\nnetwork to be fine-tuned on the context in parallel with the student model on\nthe context. Training teacher on the target context requires more time and\ncomputational resources. In this paper, we propose context unaware knowledge\ndistillation that uses the knowledge of the teacher model without fine-tuning\nit on the target context. We also propose a new efficient student model\narchitecture for knowledge distillation. The proposed approach follows a\ntwo-step process. The first step involves pre-training the student model with\nthe help of context unaware knowledge distillation from the teacher model. The\nsecond step involves fine-tuning the student model on the context of image\nretrieval. In order to show the efficacy of the proposed approach, we compare\nthe retrieval results, no. of parameters and no. of operations of the student\nmodels with the teacher models under different retrieval frameworks, including\ndeep cauchy hashing (DCH) and central similarity quantization (CSQ). The\nexperimental results confirm that the proposed approach provides a promising\ntrade-off between the retrieval results and efficiency. The code used in this\npaper is released publicly at \\url{https://github.com/satoru2001/CUKDFIR}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Reddy_B/0/1/0/all/0/1\">Bytasandram Yaswanth Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dubey_S/0/1/0/all/0/1\">Shiv Ram Dubey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanodiya_R/0/1/0/all/0/1\">Rakesh Kumar Sanodiya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karn_R/0/1/0/all/0/1\">Ravi Ranjan Prasad Karn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Incremental Task Learning with Incremental Rank Updates. (arXiv:2207.09074v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09074","description":"<p>Incremental Task learning (ITL) is a category of continual learning that\nseeks to train a single network for multiple tasks (one after another), where\ntraining data for each task is only available during the training of that task.\nNeural networks tend to forget older tasks when they are trained for the newer\ntasks; this property is often known as catastrophic forgetting. To address this\nissue, ITL methods use episodic memory, parameter regularization, masking and\npruning, or extensible network structures. In this paper, we propose a new\nincremental task learning framework based on low-rank factorization. In\nparticular, we represent the network weights for each layer as a linear\ncombination of several rank-1 matrices. To update the network for a new task,\nwe learn a rank-1 (or low-rank) matrix and add that to the weights of every\nlayer. We also introduce an additional selector vector that assigns different\nweights to the low-rank matrices learned for the previous tasks. We show that\nour approach performs better than the current state-of-the-art methods in terms\nof accuracy and forgetting. Our method also offers better memory efficiency\ncompared to episodic memory- and mask-based approaches. Our code will be\navailable at https://github.com/CSIPlab/task-increment-rank-update.git\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hyder_R/0/1/0/all/0/1\">Rakib Hyder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_K/0/1/0/all/0/1\">Ken Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_B/0/1/0/all/0/1\">Boyu Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Markopoulos_P/0/1/0/all/0/1\">Panos Markopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prater_Bennette_A/0/1/0/all/0/1\">Ashley Prater-Bennette</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asif_M/0/1/0/all/0/1\">M. Salman Asif</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Relational Future Captioning Model for Explaining Likely Collisions in Daily Tasks. (arXiv:2207.09083v1 [cs.RO])","link":"http://arxiv.org/abs/2207.09083","description":"<p>Domestic service robots that support daily tasks are a promising solution for\nelderly or disabled people. It is crucial for domestic service robots to\nexplain the collision risk before they perform actions. In this paper, our aim\nis to generate a caption about a future event. We propose the Relational Future\nCaptioning Model (RFCM), a crossmodal language generation model for the future\ncaptioning task. The RFCM has the Relational Self-Attention Encoder to extract\nthe relationships between events more effectively than the conventional\nself-attention in transformers. We conducted comparison experiments, and the\nresults show the RFCM outperforms a baseline method on two datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kambara_M/0/1/0/all/0/1\">Motonari Kambara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugiura_K/0/1/0/all/0/1\">Komei Sugiura</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dual Adaptive Transformations for Weakly Supervised Point Cloud Segmentation. (arXiv:2207.09084v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09084","description":"<p>Weakly supervised point cloud segmentation, i.e. semantically segmenting a\npoint cloud with only a few labeled points in the whole 3D scene, is highly\ndesirable due to the heavy burden of collecting abundant dense annotations for\nthe model training. However, existing methods remain challenging to accurately\nsegment 3D point clouds since limited annotated data may lead to insufficient\nguidance for label propagation to unlabeled data. Considering the\nsmoothness-based methods have achieved promising progress, in this paper, we\nadvocate applying the consistency constraint under various perturbations to\neffectively regularize unlabeled 3D points. Specifically, we propose a novel\nDAT (\\textbf{D}ual \\textbf{A}daptive \\textbf{T}ransformations) model for weakly\nsupervised point cloud segmentation, where the dual adaptive transformations\nare performed via an adversarial strategy at both point-level and region-level,\naiming at enforcing the local and structural smoothness constraints on 3D point\nclouds. We evaluate our proposed DAT model with two popular backbones on the\nlarge-scale S3DIS and ScanNet-V2 datasets. Extensive experiments demonstrate\nthat our model can effectively leverage the unlabeled 3D points and achieve\nsignificant performance gains on both datasets, setting new state-of-the-art\nperformance for weakly supervised point cloud segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhonghua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yicheng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1\">Guosheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jianfei Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1\">Chen Qian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MHR-Net: Multiple-Hypothesis Reconstruction of Non-Rigid Shapes from 2D Views. (arXiv:2207.09086v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09086","description":"<p>We propose MHR-Net, a novel method for recovering Non-Rigid Shapes from\nMotion (NRSfM). MHR-Net aims to find a set of reasonable reconstructions for a\n2D view, and it also selects the most likely reconstruction from the set. To\ndeal with the challenging unsupervised generation of non-rigid shapes, we\ndevelop a new Deterministic Basis and Stochastic Deformation scheme in MHR-Net.\nThe non-rigid shape is first expressed as the sum of a coarse shape basis and a\nflexible shape deformation, then multiple hypotheses are generated with\nuncertainty modeling of the deformation part. MHR-Net is optimized with\nreprojection loss on the basis and the best hypothesis. Furthermore, we design\na new Procrustean Residual Loss, which reduces the rigid rotations between\nsimilar shapes and further improves the performance. Experiments show that\nMHR-Net achieves state-of-the-art reconstruction accuracy on Human3.6M, SURREAL\nand 300-VW datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_H/0/1/0/all/0/1\">Haitian Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_J/0/1/0/all/0/1\">Jiaxu Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MONet: Multi-scale Overlap Network for Duplication Detection in Biomedical Images. (arXiv:2207.09107v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09107","description":"<p>Manipulation of biomedical images to misrepresent experimental results has\nplagued the biomedical community for a while. Recent interest in the problem\nled to the curation of a dataset and associated tasks to promote the\ndevelopment of biomedical forensic methods. Of these, the largest manipulation\ndetection task focuses on the detection of duplicated regions between images.\nTraditional computer-vision based forensic models trained on natural images are\nnot designed to overcome the challenges presented by biomedical images. We\npropose a multi-scale overlap detection model to detect duplicated image\nregions. Our model is structured to find duplication hierarchically, so as to\nreduce the number of patch operations. It achieves state-of-the-art performance\noverall and on multiple biomedical image categories.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sabir_E/0/1/0/all/0/1\">Ekraam Sabir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nandi_S/0/1/0/all/0/1\">Soumyaroop Nandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+AbdAlmageed_W/0/1/0/all/0/1\">Wael AbdAlmageed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Natarajan_P/0/1/0/all/0/1\">Prem Natarajan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"eCDT: Event Clustering for Simultaneous Feature Detection and Tracking-. (arXiv:2207.09108v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09108","description":"<p>Contrary to other standard cameras, event cameras interpret the world in an\nentirely different manner; as a collection of asynchronous events. Despite\nevent camera's unique data output, many event feature detection and tracking\nalgorithms have shown significant progress by making detours to frame-based\ndata representations. This paper questions the need to do so and proposes a\nnovel event data-friendly method that achieve simultaneous feature detection\nand tracking, called event Clustering-based Detection and Tracking (eCDT). Our\nmethod employs a novel clustering method, named as k-NN Classifier-based\nSpatial Clustering and Applications with Noise (KCSCAN), to cluster adjacent\npolarity events to retrieve event trajectories.With the aid of a Head and Tail\nDescriptor Matching process, event clusters that reappear in a different\npolarity are continually tracked, elongating the feature tracks. Thanks to our\nclustering approach in spatio-temporal space, our method automatically solves\nfeature detection and feature tracking simultaneously. Also, eCDT can extract\nfeature tracks at any frequency with an adjustable time window, which does not\ncorrupt the high temporal resolution of the original event data. Our method\nachieves 30% better feature tracking ages compared with the state-of-the-art\napproach while also having a low error approximately equal to it.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Sumin Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yeeun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_H/0/1/0/all/0/1\">Hyungtae Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_A/0/1/0/all/0/1\">Alex Junho Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Myung_H/0/1/0/all/0/1\">Hyun Myung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Expert-LaSTS: Expert-Knowledge Guided Latent Space for Traffic Scenarios. (arXiv:2207.09120v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09120","description":"<p>Clustering traffic scenarios and detecting novel scenario types are required\nfor scenario-based testing of autonomous vehicles. These tasks benefit from\neither good similarity measures or good representations for the traffic\nscenarios. In this work, an expert-knowledge aided representation learning for\ntraffic scenarios is presented. The latent space so formed is used for\nsuccessful clustering and novel scenario type detection. Expert-knowledge is\nused to define objectives that the latent representations of traffic scenarios\nshall fulfill. It is presented, how the network architecture and loss is\ndesigned from these objectives, thereby incorporating expert-knowledge. An\nautomatic mining strategy for traffic scenarios is presented, such that no\nmanual labeling is required. Results show the performance advantage compared to\nbaseline methods. Additionally, extensive analysis of the latent space is\nperformed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wurst_J/0/1/0/all/0/1\">Jonas Wurst</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_L/0/1/0/all/0/1\">Lakshman Balasubramanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Botsch_M/0/1/0/all/0/1\">Michael Botsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Utschick_W/0/1/0/all/0/1\">Wolfgang Utschick</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Shrinking the Semantic Gap: Spatial Pooling of Local Moment Invariants for Copy-Move Forgery Detection. (arXiv:2207.09135v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09135","description":"<p>Copy-move forgery is a manipulation of copying and pasting specific patches\nfrom and to an image, with potentially illegal or unethical uses. Recent\nadvances in the forensic methods for copy-move forgery have shown increasing\nsuccess in detection accuracy and robustness. However, for images with high\nself-similarity or strong signal corruption, the existing algorithms often\nexhibit inefficient processes and unreliable results. This is mainly due to the\ninherent semantic gap between low-level visual representation and high-level\nsemantic concept. In this paper, we present a very first study of trying to\nmitigate the semantic gap problem in copy-move forgery detection, with spatial\npooling of local moment invariants for midlevel image representation. Our\ndetection method expands the traditional works on two aspects: 1) we introduce\nthe bag-of-visual-words model into this field for the first time, may meaning a\nnew perspective of forensic study; 2) we propose a word-to-phrase feature\ndescription and matching pipeline, covering the spatial structure and visual\nsaliency information of digital images. Extensive experimental results show the\nsuperior performance of our framework over state-of-the-art algorithms in\novercoming the related problems caused by the semantic gap.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhiqiu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_S/0/1/0/all/0/1\">Shuren Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yaoshen Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_G/0/1/0/all/0/1\">Guohua Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ParticleSfM: Exploiting Dense Point Trajectories for Localizing Moving Cameras in the Wild. (arXiv:2207.09137v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09137","description":"<p>Estimating the pose of a moving camera from monocular video is a challenging\nproblem, especially due to the presence of moving objects in dynamic\nenvironments, where the performance of existing camera pose estimation methods\nare susceptible to pixels that are not geometrically consistent. To tackle this\nchallenge, we present a robust dense indirect structure-from-motion method for\nvideos that is based on dense correspondence initialized from pairwise optical\nflow. Our key idea is to optimize long-range video correspondence as dense\npoint trajectories and use it to learn robust estimation of motion\nsegmentation. A novel neural network architecture is proposed for processing\nirregular point trajectory data. Camera poses are then estimated and optimized\nwith global bundle adjustment over the portion of long-range point trajectories\nthat are classified as static. Experiments on MPI Sintel dataset show that our\nsystem produces significantly more accurate camera trajectories compared to\nexisting state-of-the-art methods. In addition, our method is able to retain\nreasonable accuracy of camera poses on fully static scenes, which consistently\noutperforms strong state-of-the-art dense correspondence based methods with\nend-to-end deep learning, demonstrating the potential of dense indirect methods\nbased on optical flow and point trajectories. As the point trajectory\nrepresentation is general, we further present results and comparisons on\nin-the-wild monocular videos with complex motion of dynamic objects. Code is\navailable at https://github.com/bytedance/particle-sfm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shaohui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Hengkai Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong-Jin Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What Matters for 3D Scene Flow Network. (arXiv:2207.09143v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09143","description":"<p>3D scene flow estimation from point clouds is a low-level 3D motion\nperception task in computer vision. Flow embedding is a commonly used technique\nin scene flow estimation, and it encodes the point motion between two\nconsecutive frames. Thus, it is critical for the flow embeddings to capture the\ncorrect overall direction of the motion. However, previous works only search\nlocally to determine a soft correspondence, ignoring the distant points that\nturn out to be the actual matching ones. In addition, the estimated\ncorrespondence is usually from the forward direction of the adjacent point\nclouds, and may not be consistent with the estimated correspondence acquired\nfrom the backward direction. To tackle these problems, we propose a novel\nall-to-all flow embedding layer with backward reliability validation during the\ninitial scene flow estimation. Besides, we investigate and compare several\ndesign choices in key components of the 3D scene flow network, including the\npoint similarity calculation, input elements of predictor, and predictor &amp;\nrefinement level design. After carefully choosing the most effective designs,\nwe are able to present a model that achieves the state-of-the-art performance\non FlyingThings3D and KITTI Scene Flow datasets. Our proposed model surpasses\nall existing methods by at least 38.2% on FlyingThings3D dataset and 24.7% on\nKITTI Scene Flow dataset for EPE3D metric. We release our codes at\nhttps://github.com/IRMVLab/3DFlow.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guangming Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yunzhe Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhe Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yiyang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tomizuka_M/0/1/0/all/0/1\">Masayoshi Tomizuka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_W/0/1/0/all/0/1\">Wei Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hesheng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Mutual Modulation for Self-Supervised Cross-Modal Super-Resolution. (arXiv:2207.09156v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09156","description":"<p>Self-supervised cross-modal super-resolution (SR) can overcome the difficulty\nof acquiring paired training data, but is challenging because only\nlow-resolution (LR) source and high-resolution (HR) guide images from different\nmodalities are available. Existing methods utilize pseudo or weak supervision\nin LR space and thus deliver results that are blurry or not faithful to the\nsource modality. To address this issue, we present a mutual modulation SR\n(MMSR) model, which tackles the task by a mutual modulation strategy, including\na source-to-guide modulation and a guide-to-source modulation. In these\nmodulations, we develop cross-domain adaptive filters to fully exploit\ncross-modal spatial dependency and help induce the source to emulate the\nresolution of the guide and induce the guide to mimic the modality\ncharacteristics of the source. Moreover, we adopt a cycle consistency\nconstraint to train MMSR in a fully self-supervised manner. Experiments on\nvarious tasks demonstrate the state-of-the-art performance of our MMSR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xiaoyu Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yokoya_N/0/1/0/all/0/1\">Naoto Yokoya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Longguang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uezato_T/0/1/0/all/0/1\">Tatsumi Uezato</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FedX: Unsupervised Federated Learning with Cross Knowledge Distillation. (arXiv:2207.09158v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09158","description":"<p>This paper presents FedX, an unsupervised federated learning framework. Our\nmodel learns unbiased representation from decentralized and heterogeneous local\ndata. It employs a two-sided knowledge distillation with contrastive learning\nas a core component, allowing the federated system to function without\nrequiring clients to share any data features. Furthermore, its adaptable\narchitecture can be used as an add-on module for existing unsupervised\nalgorithms in federated settings. Experiments show that our model improves\nperformance significantly (1.58--5.52pp) on five unsupervised algorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Sungwon Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Sungwon Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fangzhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sundong Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chuhan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xing Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cha_M/0/1/0/all/0/1\">Meeyoung Cha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Single Stage Virtual Try-on via Deformable Attention Flows. (arXiv:2207.09161v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09161","description":"<p>Virtual try-on aims to generate a photo-realistic fitting result given an\nin-shop garment and a reference person image. Existing methods usually build up\nmulti-stage frameworks to deal with clothes warping and body blending\nrespectively, or rely heavily on intermediate parser-based labels which may be\nnoisy or even inaccurate. To solve the above challenges, we propose a\nsingle-stage try-on framework by developing a novel Deformable Attention Flow\n(DAFlow), which applies the deformable attention scheme to multi-flow\nestimation. With pose keypoints as the guidance only, the self- and\ncross-deformable attention flows are estimated for the reference person and the\ngarment images, respectively. By sampling multiple flow fields, the\nfeature-level and pixel-level information from different semantic areas are\nsimultaneously extracted and merged through the attention mechanism. It enables\nclothes warping and body synthesizing at the same time which leads to\nphoto-realistic results in an end-to-end manner. Extensive experiments on two\ntry-on datasets demonstrate that our proposed method achieves state-of-the-art\nperformance both qualitatively and quantitatively. Furthermore, additional\nexperiments on the other two image editing tasks illustrate the versatility of\nour method for multi-view synthesis and image animation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1\">Shuai Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Huiling Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhikang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hongxia Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Global and Local Features through Gaussian Mixture Models on Image Semantic Segmentation. (arXiv:2207.09162v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09162","description":"<p>The semantic segmentation task aims at dense classification at the pixel-wise\nlevel. Deep models exhibited progress in tackling this task. However, one\nremaining problem with these approaches is the loss of spatial precision, often\nproduced at the segmented objects' boundaries. Our proposed model addresses\nthis problem by providing an internal structure for the feature representations\nwhile extracting a global representation that supports the former. To fit the\ninternal structure, during training, we predict a Gaussian Mixture Model from\nthe data, which, merged with the skip connections and the decoding stage, helps\navoid wrong inductive biases. Furthermore, our results show that we can improve\nsemantic segmentation by providing both learning representations (global and\nlocal) with a clustering behavior and combining them. Finally, we present\nresults demonstrating our advances in Cityscapes and Synthia datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saire_D/0/1/0/all/0/1\">Darwin Saire</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rivera_A/0/1/0/all/0/1\">Ad&#xed;n Ram&#xed;rez Rivera</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Multi-Stage Framework for the 2022 Multi-Structure Segmentation for Renal Cancer Treatment. (arXiv:2207.09165v1 [eess.IV])","link":"http://arxiv.org/abs/2207.09165","description":"<p>Three-dimensional (3D) kidney parsing on computed tomography angiography\n(CTA) images is of great clinical significance. Automatic segmentation of\nkidney, renal tumor, renal vein and renal artery benefits a lot on\nsurgery-based renal cancer treatment. In this paper, we propose a new\nnnhra-unet network, and use a multi-stage framework which is based on it to\nsegment the multi-structure of kidney and participate in the KiPA2022\nchallenge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yusheng Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhongchen Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_L/0/1/0/all/0/1\">Lisheng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervision Can Be a Good Few-Shot Learner. (arXiv:2207.09176v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09176","description":"<p>Existing few-shot learning (FSL) methods rely on training with a large\nlabeled dataset, which prevents them from leveraging abundant unlabeled data.\nFrom an information-theoretic perspective, we propose an effective unsupervised\nFSL method, learning representations with self-supervision. Following the\nInfoMax principle, our method learns comprehensive representations by capturing\nthe intrinsic structure of the data. Specifically, we maximize the mutual\ninformation (MI) of instances and their representations with a low-bias MI\nestimator to perform self-supervised pre-training. Rather than supervised\npre-training focusing on the discriminable features of the seen classes, our\nself-supervised model has less bias toward the seen classes, resulting in\nbetter generalization for unseen classes. We explain that supervised\npre-training and self-supervised pre-training are actually maximizing different\nMI objectives. Extensive experiments are further conducted to analyze their FSL\nperformance with various training settings. Surprisingly, the results show that\nself-supervised pre-training can outperform supervised pre-training under the\nappropriate conditions. Compared with state-of-the-art FSL methods, our\napproach achieves comparable performance on widely used FSL benchmarks without\nany labels of the base classes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yuning Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1\">Liangjian Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jianzhuang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yajing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_X/0/1/0/all/0/1\">Xinmei Tian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NDF: Neural Deformable Fields for Dynamic Human Modelling. (arXiv:2207.09193v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09193","description":"<p>We propose Neural Deformable Fields (NDF), a new representation for dynamic\nhuman digitization from a multi-view video. Recent works proposed to represent\na dynamic human body with shared canonical neural radiance fields which links\nto the observation space with deformation fields estimations. However, the\nlearned canonical representation is static and the current design of the\ndeformation fields is not able to represent large movements or detailed\ngeometry changes. In this paper, we propose to learn a neural deformable field\nwrapped around a fitted parametric body model to represent the dynamic human.\nThe NDF is spatially aligned by the underlying reference surface. A neural\nnetwork is then learned to map pose to the dynamics of NDF. The proposed NDF\nrepresentation can synthesize the digitized performer with novel views and\nnovel poses with a detailed and reasonable dynamic appearance. Experiments show\nthat our method significantly outperforms recent human synthesis methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruiqi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jie Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Disentangled Content Information for Face Forgery Detection. (arXiv:2207.09202v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09202","description":"<p>Convolutional neural network based face forgery detection methods have\nachieved remarkable results during training, but struggled to maintain\ncomparable performance during testing. We observe that the detector is prone to\nfocus more on content information than artifact traces, suggesting that the\ndetector is sensitive to the intrinsic bias of the dataset, which leads to\nsevere overfitting. Motivated by this key observation, we design an easily\nembeddable disentanglement framework for content information removal, and\nfurther propose a Content Consistency Constraint (C2C) and a Global\nRepresentation Contrastive Constraint (GRCC) to enhance the independence of\ndisentangled features. Furthermore, we cleverly construct two unbalanced\ndatasets to investigate the impact of the content bias. Extensive\nvisualizations and experiments demonstrate that our framework can not only\nignore the interference of content information, but also guide the detector to\nmine suspicious artifact traces and achieve competitive performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jiahao Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Huafeng Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_W/0/1/0/all/0/1\">Weihong Deng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VoloGAN: Adversarial Domain Adaptation for Synthetic Depth Data. (arXiv:2207.09204v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09204","description":"<p>We present VoloGAN, an adversarial domain adaptation network that translates\nsynthetic RGB-D images of a high-quality 3D model of a person, into RGB-D\nimages that could be generated with a consumer depth sensor. This system is\nespecially useful to generate high amount training data for single-view 3D\nreconstruction algorithms replicating the real-world capture conditions, being\nable to imitate the style of different sensor types, for the same high-end 3D\nmodel database. The network uses a CycleGAN framework with a U-Net architecture\nfor the generator and a discriminator inspired by SIV-GAN. We use different\noptimizers and learning rate schedules to train the generator and the\ndiscriminator. We further construct a loss function that considers image\nchannels individually and, among other metrics, evaluates the structural\nsimilarity. We demonstrate that CycleGANs can be used to apply adversarial\ndomain adaptation of synthetic 3D data to train a volumetric video generator\nmodel having only few training samples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kirch_S/0/1/0/all/0/1\">Sascha Kirch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pages_R/0/1/0/all/0/1\">Rafael Pag&#xe9;s</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arnaldo_S/0/1/0/all/0/1\">Sergio Arnaldo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_S/0/1/0/all/0/1\">Sergio Mart&#xed;n</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KinD-LCE Curve Estimation And Retinex Fusion On Low-Light Image. (arXiv:2207.09210v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09210","description":"<p>The problems of low light image noise and chromatic aberration is a\nchallenging problem for tasks such as object detection, semantic segmentation,\ninstance segmentation, etc. In this paper, we propose the algorithm for low\nillumination enhancement. KinD-LCE uses the light curve estimation module in\nthe network structure to enhance the illumination map in the Retinex decomposed\nimage, which improves the image brightness; we proposed the illumination map\nand reflection map fusion module to restore the restored image details and\nreduce the detail loss. Finally, we included a total variation loss function to\neliminate noise. Our method uses the GladNet dataset as the training set, and\nthe LOL dataset as the test set and is validated using ExDark as the dataset\nfor downstream tasks. Extensive Experiments on the benchmarks demonstrate the\nadvantages of our method and are close to the state-of-the-art results, which\nachieve a PSNR of 19.7216 and SSIM of 0.8213 in terms of metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lei_X/0/1/0/all/0/1\">Xiaochun Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Junlin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zetao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mai_W/0/1/0/all/0/1\">Weiliang Mai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1\">Zhaoting Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Chang Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1\">Linjun Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_Z/0/1/0/all/0/1\">Ziqi Shan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image Super-Resolution with Deep Dictionary. (arXiv:2207.09228v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09228","description":"<p>Since the first success of Dong et al., the deep-learning-based approach has\nbecome dominant in the field of single-image super-resolution. This replaces\nall the handcrafted image processing steps of traditional sparse-coding-based\nmethods with a deep neural network. In contrast to sparse-coding-based methods,\nwhich explicitly create high/low-resolution dictionaries, the dictionaries in\ndeep-learning-based methods are implicitly acquired as a nonlinear combination\nof multiple convolutions. One disadvantage of deep-learning-based methods is\nthat their performance is degraded for images created differently from the\ntraining dataset (out-of-domain images). We propose an end-to-end\nsuper-resolution network with a deep dictionary (SRDD), where a high-resolution\ndictionary is explicitly learned without sacrificing the advantages of deep\nlearning. Extensive experiments show that explicit learning of high-resolution\ndictionary makes the network more robust for out-of-domain test images while\nmaintaining the performance of the in-domain test images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maeda_S/0/1/0/all/0/1\">Shunta Maeda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IDET: Iterative Difference-Enhanced Transformers for High-Quality Change Detection. (arXiv:2207.09240v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09240","description":"<p>Change detection (CD) aims to detect change regions within an image pair\ncaptured at different times, playing a significant role for diverse real-world\napplications. Nevertheless, most of existing works focus on designing advanced\nnetwork architectures to map the feature difference to the final change map\nwhile ignoring the influence of the quality of the feature difference. In this\npaper, we study the CD from a new perspective, i.e., how to optimize the\nfeature difference to highlight changes and suppress unchanged regions, and\npropose a novel module denoted as iterative difference-enhanced transformers\n(IDET). IDET contains three transformers: two transformers for extracting the\nlong-range information of the two images and one transformer for enhancing the\nfeature difference. In contrast to the previous transformers, the third\ntransformer takes the outputs of the first two transformers to guide the\nenhancement of the feature difference iteratively. To achieve more effective\nrefinement, we further propose the multi-scale IDET-based change detection that\nuses multi-scale representations of the images for multiple feature difference\nrefinements and proposes a coarse-to-fine fusion strategy to combine all\nrefinements. Our final CD method outperforms seven state-of-the-art methods on\nsix large-scale datasets under diverse application scenarios, which\ndemonstrates the importance of feature difference enhancements and the\neffectiveness of IDET.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1\">Rui Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ruofei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1\">Qing Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuxiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_W/0/1/0/all/0/1\">Wei Fan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Don't Stop Learning: Towards Continual Learning for the CLIP Model. (arXiv:2207.09248v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09248","description":"<p>The Contrastive Language-Image Pre-training (CLIP) Model is a recently\nproposed large-scale pre-train model which attracts increasing attention in the\ncomputer vision community. Benefiting from its gigantic image-text training\nset, the CLIP model has learned outstanding capabilities in zero-shot learning\nand image-text matching. To boost the recognition performance of CLIP on some\ntarget visual concepts, it is often desirable to further update the CLIP model\nby fine-tuning some classes-of-interest on extra training data. This operation,\nhowever, raises an important concern: will the update hurt the zero-shot\nlearning or image-text matching capability of the CLIP, i.e., the catastrophic\nforgetting issue? If yes, could existing continual learning algorithms be\nadapted to alleviate the risk of catastrophic forgetting? To answer these\nquestions, this work conducts a systemic study on the continual learning issue\nof the CLIP model. We construct evaluation protocols to measure the impact of\nfine-tuning updates and explore different ways to upgrade existing continual\nlearning methods to mitigate the forgetting issue of the CLIP model. Our study\nreveals the particular challenges of CLIP continual learning problem and lays a\nfoundation for further researches. Moreover, we propose a new algorithm, dubbed\nLearning without Forgetting via Replayed Vocabulary (VR-LwF), which shows exact\neffectiveness for alleviating the forgetting issue of the CLIP model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yuxuan Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lingqiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_C/0/1/0/all/0/1\">Chunna Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jingyuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1\">Haoxuan Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Action Quality Assessment with Temporal Parsing Transformer. (arXiv:2207.09270v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09270","description":"<p>Action Quality Assessment(AQA) is important for action understanding and\nresolving the task poses unique challenges due to subtle visual differences.\nExisting state-of-the-art methods typically rely on the holistic video\nrepresentations for score regression or ranking, which limits the\ngeneralization to capture fine-grained intra-class variation. To overcome the\nabove limitation, we propose a temporal parsing transformer to decompose the\nholistic feature into temporal part-level representations. Specifically, we\nutilize a set of learnable queries to represent the atomic temporal patterns\nfor a specific action. Our decoding process converts the frame representations\nto a fixed number of temporally ordered part representations. To obtain the\nquality score, we adopt the state-of-the-art contrastive regression based on\nthe part representations. Since existing AQA datasets do not provide temporal\npart-level labels or partitions, we propose two novel loss functions on the\ncross attention responses of the decoder: a ranking loss to ensure the\nlearnable queries to satisfy the temporal order in cross attention and a\nsparsity loss to encourage the part representations to be more discriminative.\nExtensive experiments show that our proposed method outperforms prior work on\nthree public AQA benchmarks by a considerable margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yang Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Desen Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Songyang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_E/0/1/0/all/0/1\">Errui Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_Y/0/1/0/all/0/1\">Yu Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_Y/0/1/0/all/0/1\">Yang Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingdong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploiting Inter-Sample Affinity for Knowability-Aware Universal Domain Adaptation. (arXiv:2207.09280v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09280","description":"<p>Universal domain adaptation (UDA) aims to transfer the knowledge of common\nclasses from source domain to target domain without any prior knowledge on the\nlabel set, which requires to distinguish the unknown samples from the known\nones in the target domain. Recent methods preferred to increase the\ninter-sample affinity within a known class, while they ignored the inter-sample\naffinity between the unknown samples and the known ones. This paper reveals\nthat exploiting such inter-sample affinity can significantly improve the\nperformance of UDA and proposes a knowability-aware UDA framework based on it.\nFirst, we estimate the knowability of each target sample by searching its\nneighboring samples in the source domain. Then, we propose an auto-thresholding\nscheme applied to the estimated knowability to determine whether a target\nsample is unknown or known. Next, in addition to increasing the inter-sample\naffinity within each known class like previous methods, we design new losses\nbased on the estimated knowability to reduce the inter-sample affinity between\nthe unknown target samples and the known ones. Finally, experiments on four\npublic datasets demonstrate that our method significantly outperforms existing\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yifan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_R/0/1/0/all/0/1\">Ran Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D Room Layout Estimation from a Cubemap of Panorama Image via Deep Manhattan Hough Transform. (arXiv:2207.09291v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09291","description":"<p>Significant geometric structures can be compactly described by global\nwireframes in the estimation of 3D room layout from a single panoramic image.\nBased on this observation, we present an alternative approach to estimate the\nwalls in 3D space by modeling long-range geometric patterns in a learnable\nHough Transform block. We transform the image feature from a cubemap tile to\nthe Hough space of a Manhattan world and directly map the feature to the\ngeometric output. The convolutional layers not only learn the local\ngradient-like line features, but also utilize the global information to\nsuccessfully predict occluded walls with a simple network structure. Unlike\nmost previous work, the predictions are performed individually on each cubemap\ntile, and then assembled to get the layout estimation. Experimental results\nshow that we achieve comparable results with recent state-of-the-art in\nprediction accuracy and performance. Code is available at\nhttps://github.com/Starrah/DMH-Net.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yining Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_C/0/1/0/all/0/1\">Chao Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_Z/0/1/0/all/0/1\">Zhou Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yue Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Caltech Fish Counting Dataset: A Benchmark for Multiple-Object Tracking and Counting. (arXiv:2207.09295v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09295","description":"<p>We present the Caltech Fish Counting Dataset (CFC), a large-scale dataset for\ndetecting, tracking, and counting fish in sonar videos. We identify sonar\nvideos as a rich source of data for advancing low signal-to-noise computer\nvision applications and tackling domain generalization in multiple-object\ntracking (MOT) and counting. In comparison to existing MOT and counting\ndatasets, which are largely restricted to videos of people and vehicles in\ncities, CFC is sourced from a natural-world domain where targets are not easily\nresolvable and appearance features cannot be easily leveraged for target\nre-identification. With over half a million annotations in over 1,500 videos\nsourced from seven different sonar cameras, CFC allows researchers to train MOT\nand counting algorithms and evaluate generalization performance at unseen test\nlocations. We perform extensive baseline experiments and identify key\nchallenges and opportunities for advancing the state of the art in\ngeneralization in MOT and counting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kay_J/0/1/0/all/0/1\">Justin Kay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulits_P/0/1/0/all/0/1\">Peter Kulits</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stathatos_S/0/1/0/all/0/1\">Suzanne Stathatos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Siqi Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Young_E/0/1/0/all/0/1\">Erik Young</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beery_S/0/1/0/all/0/1\">Sara Beery</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horn_G/0/1/0/all/0/1\">Grant Van Horn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perona_P/0/1/0/all/0/1\">Pietro Perona</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Semantic Statistics Matching (D2SM) Denoising Network. (arXiv:2207.09302v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09302","description":"<p>The ultimate aim of image restoration like denoising is to find an exact\ncorrelation between the noisy and clear image domains. But the optimization of\nend-to-end denoising learning like pixel-wise losses is performed in a\nsample-to-sample manner, which ignores the intrinsic correlation of images,\nespecially semantics. In this paper, we introduce the Deep Semantic Statistics\nMatching (D2SM) Denoising Network. It exploits semantic features of pretrained\nclassification networks, then it implicitly matches the probabilistic\ndistribution of clear images at the semantic feature space. By learning to\npreserve the semantic distribution of denoised images, we empirically find our\nmethod significantly improves the denoising capabilities of networks, and the\ndenoised results can be better understood by high-level vision tasks.\nComprehensive experiments conducted on the noisy Cityscapes dataset demonstrate\nthe superiority of our method on both the denoising performance and semantic\nsegmentation accuracy. Moreover, the performance improvement observed on our\nextended tasks including super-resolution and dehazing experiments shows its\npotentiality as a new general plug-and-play component.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mei_K/0/1/0/all/0/1\">Kangfu Mei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1\">Vishal M. Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1\">Rui Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DH-AUG: DH Forward Kinematics Model Driven Augmentation for 3D Human Pose Estimation. (arXiv:2207.09303v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09303","description":"<p>Due to the lack of diversity of datasets, the generalization ability of the\npose estimator is poor. To solve this problem, we propose a pose augmentation\nsolution via DH forward kinematics model, which we call DH-AUG. We observe that\nthe previous work is all based on single-frame pose augmentation, if it is\ndirectly applied to video pose estimator, there will be several previously\nignored problems: (i) angle ambiguity in bone rotation (multiple solutions);\n(ii) the generated skeleton video lacks movement continuity. To solve these\nproblems, we propose a special generator based on DH forward kinematics model,\nwhich is called DH-generator. Extensive experiments demonstrate that DH-AUG can\ngreatly increase the generalization ability of the video pose estimator. In\naddition, when applied to a single-frame 3D pose estimator, our method\noutperforms the previous best pose augmentation method. The source code has\nbeen released at\nhttps://github.com/hlz0606/DH-AUG-DH-Forward-Kinematics-Model-Driven-Augmentation-for-3D-Human-Pose-Estimation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Linzhi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jiahao Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_W/0/1/0/all/0/1\">Weihong Deng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Trustworthy Healthcare AI: Attention-Based Feature Learning for COVID-19 Screening With Chest Radiography. (arXiv:2207.09312v1 [eess.IV])","link":"http://arxiv.org/abs/2207.09312","description":"<p>Building AI models with trustworthiness is important especially in regulated\nareas such as healthcare. In tackling COVID-19, previous work uses\nconvolutional neural networks as the backbone architecture, which has shown to\nbe prone to over-caution and overconfidence in making decisions, rendering them\nless trustworthy -- a crucial flaw in the context of medical imaging. In this\nstudy, we propose a feature learning approach using Vision Transformers, which\nuse an attention-based mechanism, and examine the representation learning\ncapability of Transformers as a new backbone architecture for medical imaging.\nThrough the task of classifying COVID-19 chest radiographs, we investigate into\nwhether generalization capabilities benefit solely from Vision Transformers'\narchitectural advances. Quantitative and qualitative evaluations are conducted\non the trustworthiness of the models, through the use of \"trust score\"\ncomputation and a visual explainability technique. We conclude that the\nattention-based feature learning approach is promising in building trustworthy\ndeep learning models for healthcare.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ma_K/0/1/0/all/0/1\">Kai Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xi_P/0/1/0/all/0/1\">Pengcheng Xi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Habashy_K/0/1/0/all/0/1\">Karim Habashy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ebadi_A/0/1/0/all/0/1\">Ashkan Ebadi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tremblay_S/0/1/0/all/0/1\">St&#xe9;phane Tremblay</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wong_A/0/1/0/all/0/1\">Alexander Wong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Content-aware Scalable Deep Compressed Sensing. (arXiv:2207.09313v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09313","description":"<p>To more efficiently address image compressed sensing (CS) problems, we\npresent a novel content-aware scalable network dubbed CASNet which collectively\nachieves adaptive sampling rate allocation, fine granular scalability and\nhigh-quality reconstruction. We first adopt a data-driven saliency detector to\nevaluate the importances of different image regions and propose a\nsaliency-based block ratio aggregation (BRA) strategy for sampling rate\nallocation. A unified learnable generating matrix is then developed to produce\nsampling matrix of any CS ratio with an ordered structure. Being equipped with\nthe optimization-inspired recovery subnet guided by saliency information and a\nmulti-block training scheme preventing blocking artifacts, CASNet jointly\nreconstructs the image blocks sampled at various sampling rates with one single\nmodel. To accelerate training convergence and improve network robustness, we\npropose an SVD-based initialization scheme and a random transformation\nenhancement (RTE) strategy, which are extensible without introducing extra\nparameters. All the CASNet components can be combined and learned end-to-end.\nWe further provide a four-stage implementation for evaluation and practical\ndeployments. Experiments demonstrate that CASNet outperforms other CS networks\nby a large margin, validating the collaboration and mutual supports among its\ncomponents and strategies. Codes are available at\nhttps://github.com/Guaishou74851/CASNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jian Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Interactive Object Segmentation Through a Singulation-and-Grasping Approach. (arXiv:2207.09314v1 [cs.RO])","link":"http://arxiv.org/abs/2207.09314","description":"<p>Instance segmentation with unseen objects is a challenging problem in\nunstructured environments. To solve this problem, we propose a robot learning\napproach to actively interact with novel objects and collect each object's\ntraining label for further fine-tuning to improve the segmentation model\nperformance, while avoiding the time-consuming process of manually labeling a\ndataset. The Singulation-and-Grasping (SaG) policy is trained through\nend-to-end reinforcement learning. Given a cluttered pile of objects, our\napproach chooses pushing and grasping motions to break the clutter and conducts\nobject-agnostic grasping for which the SaG policy takes as input the visual\nobservations and imperfect segmentation. We decompose the problem into three\nsubtasks: (1) the object singulation subtask aims to separate the objects from\neach other, which creates more space that alleviates the difficulty of (2) the\ncollision-free grasping subtask; (3) the mask generation subtask to obtain the\nself-labeled ground truth masks by using an optical flow-based binary\nclassifier and motion cue post-processing for transfer learning. Our system\nachieves 70% singulation success rate in simulated cluttered scenes. The\ninteractive segmentation of our system achieves 87.8%, 73.9%, and 69.3% average\nprecision for toy blocks, YCB objects in simulation and real-world novel\nobjects, respectively, which outperforms several baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Houjian Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_C/0/1/0/all/0/1\">Changhyun Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking IoU-based Optimization for Single-stage 3D Object Detection. (arXiv:2207.09332v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09332","description":"<p>Since Intersection-over-Union (IoU) based optimization maintains the\nconsistency of the final IoU prediction metric and losses, it has been widely\nused in both regression and classification branches of single-stage 2D object\ndetectors. Recently, several 3D object detection methods adopt IoU-based\noptimization and directly replace the 2D IoU with 3D IoU. However, such a\ndirect computation in 3D is very costly due to the complex implementation and\ninefficient backward operations. Moreover, 3D IoU-based optimization is\nsub-optimal as it is sensitive to rotation and thus can cause training\ninstability and detection performance deterioration. In this paper, we propose\na novel Rotation-Decoupled IoU (RDIoU) method that can mitigate the\nrotation-sensitivity issue, and produce more efficient optimization objectives\ncompared with 3D IoU during the training stage. Specifically, our RDIoU\nsimplifies the complex interactions of regression parameters by decoupling the\nrotation variable as an independent term, yet preserving the geometry of 3D\nIoU. By incorporating RDIoU into both the regression and classification\nbranches, the network is encouraged to learn more precise bounding boxes and\nconcurrently overcome the misalignment issue between classification and\nregression. Extensive experiments on the benchmark KITTI and Waymo Open Dataset\nvalidate that our RDIoU method can bring substantial improvement for the\nsingle-stage 3D object detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sheng_H/0/1/0/all/0/1\">Hualian Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_S/0/1/0/all/0/1\">Sijia Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_N/0/1/0/all/0/1\">Na Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_B/0/1/0/all/0/1\">Bing Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jianqiang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_X/0/1/0/all/0/1\">Xian-Sheng Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1\">Min-Jian Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Gim Hee Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncertainty in Contrastive Learning: On the Predictability of Downstream Performance. (arXiv:2207.09336v1 [cs.LG])","link":"http://arxiv.org/abs/2207.09336","description":"<p>The superior performance of some of today's state-of-the-art deep learning\nmodels is to some extent owed to extensive (self-)supervised contrastive\npretraining on large-scale datasets. In contrastive learning, the network is\npresented with pairs of positive (similar) and negative (dissimilar) datapoints\nand is trained to find an embedding vector for each datapoint, i.e., a\nrepresentation, which can be further fine-tuned for various downstream tasks.\nIn order to safely deploy these models in critical decision-making systems, it\nis crucial to equip them with a measure of their uncertainty or reliability.\nHowever, due to the pairwise nature of training a contrastive model, and the\nlack of absolute labels on the output (an abstract embedding vector), adapting\nconventional uncertainty estimation techniques to such models is non-trivial.\nIn this work, we study whether the uncertainty of such a representation can be\nquantified for a single datapoint in a meaningful way. In other words, we\nexplore if the downstream performance on a given datapoint is predictable,\ndirectly from its pre-trained embedding. We show that this goal can be achieved\nby directly estimating the distribution of the training data in the embedding\nspace and accounting for the local consistency of the representations. Our\nexperiments show that this notion of uncertainty for an embedding vector often\nstrongly correlates with its downstream accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ardeshir_S/0/1/0/all/0/1\">Shervin Ardeshir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azizan_N/0/1/0/all/0/1\">Navid Azizan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Representation Learning with Transformer: A Sequence-to-Sequence Perspective. (arXiv:2207.09339v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09339","description":"<p>Visual representation learning is the key of solving various vision problems.\nRelying on the seminal grid structure priors, convolutional neural networks\n(CNNs) have been the de facto standard architectures of most deep vision\nmodels. For instance, classical semantic segmentation methods often adopt a\nfully-convolutional network (FCN) with an encoder-decoder architecture. The\nencoder progressively reduces the spatial resolution and learns more abstract\nvisual concepts with larger receptive fields. Since context modeling is\ncritical for segmentation, the latest efforts have been focused on increasing\nthe receptive field, through either dilated (i.e., atrous) convolutions or\ninserting attention modules. However, the FCN-based architecture remains\nunchanged. In this paper, we aim to provide an alternative perspective by\ntreating visual representation learning generally as a sequence-to-sequence\nprediction task. Specifically, we deploy a pure Transformer to encode an image\nas a sequence of patches, without local convolution and resolution reduction.\nWith the global context modeled in every layer of the Transformer, stronger\nvisual representation can be learned for better tackling vision tasks. In\nparticular, our segmentation model, termed as SEgmentation TRansformer (SETR),\nexcels on ADE20K (50.28% mIoU, the first position in the test leaderboard on\nthe day of submission), Pascal Context (55.83% mIoU) and reaches competitive\nresults on Cityscapes. Further, we formulate a family of Hierarchical\nLocal-Global (HLG) Transformers characterized by local attention within windows\nand global-attention across windows in a hierarchical and pyramidal\narchitecture. Extensive experiments show that our method achieves appealing\nperformance on a variety of visual recognition tasks (e.g., image\nclassification, object detection and instance segmentation and semantic\nsegmentation).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Sixiao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiachen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xinxuan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiatian Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yanwei Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1\">Tao Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jianfeng Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Computer Vision to the Rescue: Infant Postural Symmetry Estimation from Incongruent Annotations. (arXiv:2207.09352v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09352","description":"<p>Bilateral postural symmetry plays a key role as a potential risk marker for\nautism spectrum disorder (ASD) and as a symptom of congenital muscular\ntorticollis (CMT) in infants, but current methods of assessing symmetry require\nlaborious clinical expert assessments. In this paper, we develop a computer\nvision based infant symmetry assessment system, leveraging 3D human pose\nestimation for infants. Evaluation and calibration of our system against ground\ntruth assessments is complicated by our findings from a survey of human ratings\nof angle and symmetry, that such ratings exhibit low inter-rater reliability.\nTo rectify this, we develop a Bayesian estimator of the ground truth derived\nfrom a probabilistic graphical model of fallible human raters. We show that the\n3D infant pose estimation model can achieve 68% area under the receiver\noperating characteristic curve performance in predicting the Bayesian aggregate\nlabels, compared to only 61% from a 2D infant pose estimation model and 60%\nfrom a 3D adult pose estimation model, highlighting the importance of 3D poses\nand infant domain knowledge in assessing infant body symmetry. Our survey\nanalysis also suggests that human ratings are susceptible to higher levels of\nbias and inconsistency, and hence our final 3D pose-based symmetry assessment\nsystem is calibrated but not directly supervised by Bayesian aggregate human\nratings, yielding higher levels of consistency and lower levels of inter-limb\nassessment bias.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaofei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_M/0/1/0/all/0/1\">Michael Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luan_L/0/1/0/all/0/1\">Lingfei Luan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tunik_B/0/1/0/all/0/1\">Bethany Tunik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ostadabbas_S/0/1/0/all/0/1\">Sarah Ostadabbas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cycle Encoding of a StyleGAN Encoder for Improved Reconstruction and Editability. (arXiv:2207.09367v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09367","description":"<p>GAN inversion aims to invert an input image into the latent space of a\npre-trained GAN. Despite the recent advances in GAN inversion, there remain\nchallenges to mitigate the tradeoff between distortion and editability, i.e.\nreconstructing the input image accurately and editing the inverted image with a\nsmall visual quality drop. The recently proposed pivotal tuning model makes\nsignificant progress towards reconstruction and editability, by using a\ntwo-step approach that first inverts the input image into a latent code, called\npivot code, and then alters the generator so that the input image can be\naccurately mapped into the pivot code. Here, we show that both reconstruction\nand editability can be improved by a proper design of the pivot code. We\npresent a simple yet effective method, named cycle encoding, for a high-quality\npivot code. The key idea of our method is to progressively train an encoder in\nvarying spaces according to a cycle scheme: W-&gt;W+-&gt;W. This training methodology\npreserves the properties of both W and W+ spaces, i.e. high editability of W\nand low distortion of W+. To further decrease the distortion, we also propose\nto refine the pivot code with an optimization-based method, where a\nregularization term is introduced to reduce the degradation in editability.\nQualitative and quantitative comparisons to several state-of-the-art methods\ndemonstrate the superiority of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mao_X/0/1/0/all/0/1\">Xudong Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1\">Liujuan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gnanha_A/0/1/0/all/0/1\">Aurele T. Gnanha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhenguo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emotion Recognition based on Multi-Task Learning Framework in the ABAW4 Challenge. (arXiv:2207.09373v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09373","description":"<p>This paper presents our submission to the Multi-Task Learning (MTL) Challenge\nof the 4th Affective Behavior Analysis in-the-wild (ABAW) competition. Based on\nvisual feature representations, we utilize three types of temporal encoder to\ncapture the temporal context information in the video, including the\ntransformer based encoder, LSTM based encoder and GRU based encoder. With the\ntemporal context-aware representations, we employ multi-task framework to\npredict the valence, arousal, expression and AU values of the images. In\naddition, smoothing processing is applied to refine the initial valence and\narousal predictions, and a model ensemble strategy is used to combine multiple\nresults from different model setups. Our system achieves the performance of\n$1.742$ on MTL Challenge validation dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tenggan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chuanhe Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaolong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuchen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_L/0/1/0/all/0/1\">Liyu Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Lei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1\">Wenqiang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Fengyuan Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image Synthesis with Disentangled Attributes for Chest X-Ray Nodule Augmentation and Detection. (arXiv:2207.09389v1 [eess.IV])","link":"http://arxiv.org/abs/2207.09389","description":"<p>Lung nodule detection in chest X-ray (CXR) images is common to early\nscreening of lung cancers. Deep-learning-based Computer-Assisted Diagnosis\n(CAD) systems can support radiologists for nodule screening in CXR. However, it\nrequires large-scale and diverse medical data with high-quality annotations to\ntrain such robust and accurate CADs. To alleviate the limited availability of\nsuch datasets, lung nodule synthesis methods are proposed for the sake of data\naugmentation. Nevertheless, previous methods lack the ability to generate\nnodules that are realistic with the size attribute desired by the detector. To\naddress this issue, we introduce a novel lung nodule synthesis framework in\nthis paper, which decomposes nodule attributes into three main aspects\nincluding shape, size, and texture, respectively. A GAN-based Shape Generator\nfirstly models nodule shapes by generating diverse shape masks. The following\nSize Modulation then enables quantitative control on the diameters of the\ngenerated nodule shapes in pixel-level granularity. A coarse-to-fine gated\nconvolutional Texture Generator finally synthesizes visually plausible nodule\ntextures conditioned on the modulated shape masks. Moreover, we propose to\nsynthesize nodule CXR images by controlling the disentangled nodule attributes\nfor data augmentation, in order to better compensate for the nodules that are\neasily missed in the detection task. Our experiments demonstrate the enhanced\nimage quality, diversity, and controllability of the proposed lung nodule\nsynthesis framework. We also validate the effectiveness of our data\naugmentation on greatly improving nodule detection performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Shen_Z/0/1/0/all/0/1\">Zhenrong Shen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ouyang_X/0/1/0/all/0/1\">Xi Ouyang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiao_B/0/1/0/all/0/1\">Bin Xiao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheng_J/0/1/0/all/0/1\">Jie-Zhi Cheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Q/0/1/0/all/0/1\">Qian Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shen_D/0/1/0/all/0/1\">Dinggang Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RCLane: Relay Chain Prediction for Lane Detection. (arXiv:2207.09399v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09399","description":"<p>Lane detection is an important component of many real-world autonomous\nsystems. Despite a wide variety of lane detection approaches have been\nproposed, reporting steady benchmark improvements over time, lane detection\nremains a largely unsolved problem. This is because most of the existing lane\ndetection methods either treat the lane detection as a dense prediction or a\ndetection task, few of them consider the unique topologies (Y-shape,\nFork-shape, nearly horizontal lane) of the lane markers, which leads to\nsub-optimal solution. In this paper, we present a new method for lane detection\nbased on relay chain prediction. Specifically, our model predicts a\nsegmentation map to classify the foreground and background region. For each\npixel point in the foreground region, we go through the forward branch and\nbackward branch to recover the whole lane. Each branch decodes a transfer map\nand a distance map to produce the direction moving to the next point, and how\nmany steps to progressively predict a relay station (next point). As such, our\nmodel is able to capture the keypoints along the lanes. Despite its simplicity,\nour strategy allows us to establish new state-of-the-art on four major\nbenchmarks including TuSimple, CULane, CurveLanes and LLAMAS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shenghua Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_X/0/1/0/all/0/1\">Xinyue Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1\">Bin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yanwei Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_X/0/1/0/all/0/1\">Xiangyang Xue</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Det6D: A Ground-Aware Full-Pose 3D Object Detector for Improving Terrain Robustness. (arXiv:2207.09412v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09412","description":"<p>Accurate 3D object detection with LiDAR is critical for autonomous driving.\nExisting research is all based on the flat-world assumption. However, the\nactual road can be complex with steep sections, which breaks the premise.\nCurrent methods suffer from performance degradation in this case due to\ndifficulty correctly detecting objects on sloped terrain. In this work, we\npropose Det6D, the first full-degree-of-freedom 3D object detector without\nspatial and postural limitations, to improve terrain robustness. We choose the\npoint-based framework by founding their capability of detecting objects in the\nentire spatial range. To predict full-degree poses, including pitch and roll,\nwe design a ground-aware orientation branch that leverages the local ground\nconstraints. Given the difficulty of long-tail non-flat scene data collection\nand 6D pose annotation, we present Slope-Aug, a data augmentation method for\nsynthesizing non-flat terrain from existing datasets recorded in flat scenes.\nExperiments on various datasets demonstrate the effectiveness and robustness of\nour method in different terrains. We further conducted an extended experiment\nto explore how the network predicts the two extra poses. The proposed modules\nare plug-and-play for existing point-based frameworks. The code is available at\nhttps://github.com/HITSZ-NRSL/De6D.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_J/0/1/0/all/0/1\">Junyuan Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haoyao Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SphereFed: Hyperspherical Federated Learning. (arXiv:2207.09413v1 [cs.LG])","link":"http://arxiv.org/abs/2207.09413","description":"<p>Federated Learning aims at training a global model from multiple\ndecentralized devices (i.e. clients) without exchanging their private local\ndata. A key challenge is the handling of non-i.i.d. (independent identically\ndistributed) data across multiple clients that may induce disparities of their\nlocal features. We introduce the Hyperspherical Federated Learning (SphereFed)\nframework to address the non-i.i.d. issue by constraining learned\nrepresentations of data points to be on a unit hypersphere shared by clients.\nSpecifically, all clients learn their local representations by minimizing the\nloss with respect to a fixed classifier whose weights span the unit\nhypersphere. After federated training in improving the global model, this\nclassifier is further calibrated with a closed-form solution by minimizing a\nmean squared loss. We show that the calibration solution can be computed\nefficiently and distributedly without direct access of local data. Extensive\nexperiments indicate that our SphereFed approach is able to improve the\naccuracy of multiple existing federated learning algorithms by a considerable\nmargin (up to 6% on challenging datasets) with enhanced computation and\ncommunication efficiency across datasets and model architectures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xin Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Sai Qian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1\">Ang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kung_H/0/1/0/all/0/1\">H.T. Kung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Geometric Features Informed Multi-person Human-object Interaction Recognition in Videos. (arXiv:2207.09425v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09425","description":"<p>Human-Object Interaction (HOI) recognition in videos is important for\nanalyzing human activity. Most existing work focusing on visual features\nusually suffer from occlusion in the real-world scenarios. Such a problem will\nbe further complicated when multiple people and objects are involved in HOIs.\nConsider that geometric features such as human pose and object position provide\nmeaningful information to understand HOIs, we argue to combine the benefits of\nboth visual and geometric features in HOI recognition, and propose a novel\nTwo-level Geometric feature-informed Graph Convolutional Network (2G-GCN). The\ngeometric-level graph models the interdependency between geometric features of\nhumans and objects, while the fusion-level graph further fuses them with visual\nfeatures of humans and objects. To demonstrate the novelty and effectiveness of\nour method in challenging scenarios, we propose a new multi-person HOI dataset\n(MPHOI-72). Extensive experiments on MPHOI-72 (multi-person HOI), CAD-120\n(single-human HOI) and Bimanual Actions (two-hand HOI) datasets demonstrate our\nsuperior performance compared to state-of-the-arts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiao_T/0/1/0/all/0/1\">Tanqiu Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Men_Q/0/1/0/all/0/1\">Qianhui Men</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Frederick W. B. Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kubotani_Y/0/1/0/all/0/1\">Yoshiki Kubotani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morishima_S/0/1/0/all/0/1\">Shigeo Morishima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shum_H/0/1/0/all/0/1\">Hubert P. H. Shum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Theseus: A Library for Differentiable Nonlinear Optimization. (arXiv:2207.09442v1 [cs.RO])","link":"http://arxiv.org/abs/2207.09442","description":"<p>We present Theseus, an efficient application-agnostic open source library for\ndifferentiable nonlinear least squares (DNLS) optimization built on PyTorch,\nproviding a common framework for end-to-end structured learning in robotics and\nvision. Existing DNLS implementations are application specific and do not\nalways incorporate many ingredients important for efficiency. Theseus is\napplication-agnostic, as we illustrate with several example applications that\nare built using the same underlying differentiable components, such as\nsecond-order optimizers, standard costs functions, and Lie groups. For\nefficiency, Theseus incorporates support for sparse solvers, automatic\nvectorization, batching, GPU acceleration, and gradient computation with\nimplicit differentiation and direct loss minimization. We do extensive\nperformance evaluation in a set of applications, demonstrating significant\nefficiency gains and better scalability when these features are incorporated.\nProject page: https://sites.google.com/view/theseus-ai\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pineda_L/0/1/0/all/0/1\">Luis Pineda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_T/0/1/0/all/0/1\">Taosha Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Monge_M/0/1/0/all/0/1\">Maurizio Monge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkataraman_S/0/1/0/all/0/1\">Shobha Venkataraman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sodhi_P/0/1/0/all/0/1\">Paloma Sodhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1\">Ricky Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ortiz_J/0/1/0/all/0/1\">Joseph Ortiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DeTone_D/0/1/0/all/0/1\">Daniel DeTone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1\">Austin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anderson_S/0/1/0/all/0/1\">Stuart Anderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Jing Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amos_B/0/1/0/all/0/1\">Brandon Amos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukadam_M/0/1/0/all/0/1\">Mustafa Mukadam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PoserNet: Refining Relative Camera Poses Exploiting Object Detections. (arXiv:2207.09445v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09445","description":"<p>The estimation of the camera poses associated with a set of images commonly\nrelies on feature matches between the images. In contrast, we are the first to\naddress this challenge by using objectness regions to guide the pose estimation\nproblem rather than explicit semantic object detections. We propose Pose\nRefiner Network (PoserNet) a light-weight Graph Neural Network to refine the\napproximate pair-wise relative camera poses. PoserNet exploits associations\nbetween the objectness regions - concisely expressed as bounding boxes - across\nmultiple views to globally refine sparsely connected view graphs. We evaluate\non the 7-Scenes dataset across varied sizes of graphs and show how this process\ncan be beneficial to optimisation-based Motion Averaging algorithms improving\nthe median error on the rotation by 62 degrees with respect to the initial\nestimates obtained based on bounding boxes. Code and data are available at\nhttps://github.com/IIT-PAVIS/PoserNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Taiana_M/0/1/0/all/0/1\">Matteo Taiana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toso_M/0/1/0/all/0/1\">Matteo Toso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+James_S/0/1/0/all/0/1\">Stuart James</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bue_A/0/1/0/all/0/1\">Alessio Del Bue</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ShapeCrafter: A Recursive Text-Conditioned 3D Shape Generation Model. (arXiv:2207.09446v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09446","description":"<p>We present ShapeCrafter, a neural network for recursive text-conditioned 3D\nshape generation. Existing methods to generate text-conditioned 3D shapes\nconsume an entire text prompt to generate a 3D shape in a single step. However,\nhumans tend to describe shapes recursively-we may start with an initial\ndescription and progressively add details based on intermediate results. To\ncapture this recursive process, we introduce a method to generate a 3D shape\ndistribution, conditioned on an initial phrase, that gradually evolves as more\nphrases are added. Since existing datasets are insufficient for training this\napproach, we present Text2Shape++, a large dataset of 369K shape-text pairs\nthat supports recursive shape generation. To capture local details that are\noften used to refine shape descriptions, we build on top of vector-quantized\ndeep implicit functions that generate a distribution of high-quality shapes.\nResults show that our method can generate shapes consistent with text\ndescriptions, and shapes evolve gradually as more phrases are added. Our method\nsupports shape editing, extrapolation, and can enable new applications in\nhuman-machine collaboration for creative design.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_R/0/1/0/all/0/1\">Rao Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_X/0/1/0/all/0/1\">Xiao Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yiwen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ritchie_D/0/1/0/all/0/1\">Daniel Ritchie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sridhar_S/0/1/0/all/0/1\">Srinath Sridhar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Human-to-Robot Imitation in the Wild. (arXiv:2207.09450v1 [cs.RO])","link":"http://arxiv.org/abs/2207.09450","description":"<p>We approach the problem of learning by watching humans in the wild. While\ntraditional approaches in Imitation and Reinforcement Learning are promising\nfor learning in the real world, they are either sample inefficient or are\nconstrained to lab settings. Meanwhile, there has been a lot of success in\nprocessing passive, unstructured human data. We propose tackling this problem\nvia an efficient one-shot robot learning algorithm, centered around learning\nfrom a third-person perspective. We call our method WHIRL: In-the-Wild Human\nImitating Robot Learning. WHIRL extracts a prior over the intent of the human\ndemonstrator, using it to initialize our agent's policy. We introduce an\nefficient real-world policy learning scheme that improves using interactions.\nOur key contributions are a simple sampling-based policy optimization approach,\na novel objective function for aligning human and robot videos as well as an\nexploration method to boost sample efficiency. We show one-shot generalization\nand success in real-world settings, including 20 different manipulation tasks\nin the wild. Videos and talk at https://human2robot.github.io\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bahl_S/0/1/0/all/0/1\">Shikhar Bahl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Abhinav Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pathak_D/0/1/0/all/0/1\">Deepak Pathak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A New Perspective on Stabilizing GANs training: Direct Adversarial Training. (arXiv:2008.09041v5 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2008.09041","description":"<p>Generative Adversarial Networks (GANs) are the most popular image generation\nmodels that have achieved remarkable progress on various computer vision tasks.\nHowever, training instability is still one of the open problems for all\nGAN-based algorithms. Quite a number of methods have been proposed to stabilize\nthe training of GANs, the focuses of which were respectively put on the loss\nfunctions, regularization and normalization technologies, training algorithms,\nand model architectures. Different from the above methods, in this paper, a new\nperspective on stabilizing GANs training is presented. It is found that\nsometimes the images produced by the generator act like adversarial examples of\nthe discriminator during the training process, which may be part of the reason\ncausing the unstable training of GANs. With this finding, we propose the Direct\nAdversarial Training (DAT) method to stabilize the training process of GANs.\nFurthermore, we prove that the DAT method is able to minimize the Lipschitz\nconstant of the discriminator adaptively. The advanced performance of DAT is\nverified on multiple loss functions, network architectures, hyper-parameters,\nand datasets. Specifically, DAT achieves significant improvements of 11.5% FID\non CIFAR-100 unconditional generation based on SSGAN, 10.5% FID on STL-10\nunconditional generation based on SSGAN, and 13.2% FID on LSUN-Bedroom\nunconditional generation based on SSGAN. Code will be available at\nhttps://github.com/iceli1007/DAT-GAN\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1\">Ziqiang Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xia_P/0/1/0/all/0/1\">Pengfei Xia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tao_R/0/1/0/all/0/1\">Rentuo Tao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Niu_H/0/1/0/all/0/1\">Hongjing Niu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Learning the Right Attention Point for Feature Enhancement. (arXiv:2012.06257v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.06257","description":"<p>We present a novel attention-based mechanism to learn enhanced point features\nfor point cloud processing tasks, e.g., classification and segmentation. Unlike\nprior works, which were trained to optimize the weights of a pre-selected set\nof attention points, our approach learns to locate the best attention points to\nmaximize the performance of a specific task, e.g., point cloud classification.\nImportantly, we advocate the use of single attention point to facilitate\nsemantic understanding in point feature learning. Specifically, we formulate a\nnew and simple convolution, which combines convolutional features from an input\npoint and its corresponding learned attention point, or LAP, for short. Our\nattention mechanism can be easily incorporated into state-of-the-art point\ncloud classification and segmentation networks. Extensive experiments on common\nbenchmarks such as ModelNet40, ShapeNetPart, and S3DIS all demonstrate that our\nLAP-enabled networks consistently outperform the respective original networks,\nas well as other competitive alternatives, which employ multiple attention\npoints, either pre-selected or learned under our LAP framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Liqiang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1\">Pengdi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1\">Chi-Wing Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kai Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hui Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explainability of deep vision-based autonomous driving systems: Review and challenges. (arXiv:2101.05307v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.05307","description":"<p>This survey reviews explainability methods for vision-based self-driving\nsystems trained with behavior cloning. The concept of explainability has\nseveral facets and the need for explainability is strong in driving, a\nsafety-critical application. Gathering contributions from several research\nfields, namely computer vision, deep learning, autonomous driving, explainable\nAI (X-AI), this survey tackles several points. First, it discusses definitions,\ncontext, and motivation for gaining more interpretability and explainability\nfrom self-driving systems, as well as the challenges that are specific to this\napplication. Second, methods providing explanations to a black-box self-driving\nsystem in a post-hoc fashion are comprehensively organized and detailed. Third,\napproaches from the literature that aim at building more interpretable\nself-driving systems by design are presented and discussed in detail. Finally,\nremaining open-challenges and potential future research directions are\nidentified and examined.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zablocki_E/0/1/0/all/0/1\">&#xc9;loi Zablocki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ben_Younes_H/0/1/0/all/0/1\">H&#xe9;di Ben-Younes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_P/0/1/0/all/0/1\">Patrick P&#xe9;rez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cord_M/0/1/0/all/0/1\">Matthieu Cord</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MOTR: End-to-End Multiple-Object Tracking with Transformer. (arXiv:2105.03247v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.03247","description":"<p>Temporal modeling of objects is a key challenge in multiple object tracking\n(MOT). Existing methods track by associating detections through motion-based\nand appearance-based similarity heuristics. The post-processing nature of\nassociation prevents end-to-end exploitation of temporal variations in video\nsequence. In this paper, we propose MOTR, which extends DETR and introduces\ntrack query to model the tracked instances in the entire video. Track query is\ntransferred and updated frame-by-frame to perform iterative prediction over\ntime. We propose tracklet-aware label assignment to train track queries and\nnewborn object queries. We further propose temporal aggregation network and\ncollective average loss to enhance temporal relation modeling. Experimental\nresults on DanceTrack show that MOTR significantly outperforms state-of-the-art\nmethod, ByteTrack by 6.5% on HOTA metric. On MOT17, MOTR outperforms our\nconcurrent works, TrackFormer and TransTrack, on association performance. MOTR\ncan serve as a stronger baseline for future research on temporal modeling and\nTransformer-based trackers. Code is available at\nhttps://github.com/megvii-research/MOTR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_F/0/1/0/all/0/1\">Fangao Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_B/0/1/0/all/0/1\">Bin Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tiancai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yichen Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"When Deep Classifiers Agree: Analyzing Correlations between Learning Order and Image Statistics. (arXiv:2105.08997v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2105.08997","description":"<p>Although a plethora of architectural variants for deep classification has\nbeen introduced over time, recent works have found empirical evidence towards\nsimilarities in their training process. It has been hypothesized that neural\nnetworks converge not only to similar representations, but also exhibit a\nnotion of empirical agreement on which data instances are learned first.\nFollowing in the latter works$'$ footsteps, we define a metric to quantify the\nrelationship between such classification agreement over time, and posit that\nthe agreement phenomenon can be mapped to core statistics of the investigated\ndataset. We empirically corroborate this hypothesis across the CIFAR10, Pascal,\nImageNet and KTH-TIPS2 datasets. Our findings indicate that agreement seems to\nbe independent of specific architectures, training hyper-parameters or labels,\nalbeit follows an ordering according to image statistics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pliushch_I/0/1/0/all/0/1\">Iuliia Pliushch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mundt_M/0/1/0/all/0/1\">Martin Mundt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lupp_N/0/1/0/all/0/1\">Nicolas Lupp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramesh_V/0/1/0/all/0/1\">Visvanathan Ramesh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Collaboration of Experts: Achieving 80% Top-1 Accuracy on ImageNet with 100M FLOPs. (arXiv:2107.03815v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.03815","description":"<p>In this paper, we propose a Collaboration of Experts (CoE) framework to pool\ntogether the expertise of multiple networks towards a common aim. Each expert\nis an individual network with expertise on a unique portion of the dataset,\nwhich enhances the collective capacity. Given a sample, an expert is selected\nby the delegator, which simultaneously outputs a rough prediction to support\nearly termination. To fulfill this framework, we propose three modules to impel\neach model to play its role, namely weight generation module (WGM), label\ngeneration module (LGM) and variance calculation module (VCM). Our method\nachieves the state-of-the-art performance on ImageNet, 80.7% top-1 accuracy\nwith 194M FLOPs. Combined with PWLU activation function and CondConv, CoE\nfurther achieves the accuracy of 80.0% with only 100M FLOPs for the first time.\nMore importantly, our method is hardware friendly and achieves a 3-6x speedup\ncompared with some existing conditional computation approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yikang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1\">Zhao Zhong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Capturing, Reconstructing, and Simulating: the UrbanScene3D Dataset. (arXiv:2107.04286v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.04286","description":"<p>We present UrbanScene3D, a large-scale data platform for research of urban\nscene perception and reconstruction. UrbanScene3D contains over 128k\nhigh-resolution images covering 16 scenes including large-scale real urban\nregions and synthetic cities with 136 km^2 area in total. The dataset also\ncontains high-precision LiDAR scans and hundreds of image sets with different\nobservation patterns, which provide a comprehensive benchmark to design and\nevaluate aerial path planning and 3D reconstruction algorithms. In addition,\nthe dataset, which is built on Unreal Engine and Airsim simulator together with\nthe manually annotated unique instance label for each building in the dataset,\nenables the generation of all kinds of data, e.g., 2D depth maps, 2D/3D\nbounding boxes, and 3D point cloud/mesh segmentations, etc. The simulator with\nphysical engine and lighting system not only produce variety of data but also\nenable users to simulate cars or drones in the proposed urban environment for\nfuture research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Liqiang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yilin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yue Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1\">Xingguang Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_K/0/1/0/all/0/1\">Ke Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hui Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust outlier detection by de-biasing VAE likelihoods. (arXiv:2108.08760v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2108.08760","description":"<p>Deep networks often make confident, yet, incorrect, predictions when tested\nwith outlier data that is far removed from their training distributions.\nLikelihoods computed by deep generative models (DGMs) are a candidate metric\nfor outlier detection with unlabeled data. Yet, previous studies have shown\nthat DGM likelihoods are unreliable and can be easily biased by simple\ntransformations to input data. Here, we examine outlier detection with\nvariational autoencoders (VAEs), among the simplest of DGMs. We propose novel\nanalytical and algorithmic approaches to ameliorate key biases with VAE\nlikelihoods. Our bias corrections are sample-specific, computationally\ninexpensive, and readily computed for various decoder visible distributions.\nNext, we show that a well-known image pre-processing technique -- contrast\nstretching -- extends the effectiveness of bias correction to further improve\noutlier detection. Our approach achieves state-of-the-art accuracies with nine\ngrayscale and natural image datasets, and demonstrates significant advantages\n-- both with speed and performance -- over four recent, competing approaches.\nIn summary, lightweight remedies suffice to achieve robust outlier detection\nwith VAEs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chauhan_K/0/1/0/all/0/1\">Kushal Chauhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+U_B/0/1/0/all/0/1\">Barath Mohan U</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shenoy_P/0/1/0/all/0/1\">Pradeep Shenoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_M/0/1/0/all/0/1\">Manish Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sridharan_D/0/1/0/all/0/1\">Devarajan Sridharan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GaitPrivacyON: Privacy-Preserving Mobile Gait Biometrics using Unsupervised Learning. (arXiv:2110.03967v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.03967","description":"<p>Numerous studies in the literature have already shown the potential of\nbiometrics on mobile devices for authentication purposes. However, it has been\nshown that, the learning processes associated to biometric systems might expose\nsensitive personal information about the subjects. This study proposes\nGaitPrivacyON, a novel mobile gait biometrics verification approach that\nprovides accurate authentication results while preserving the sensitive\ninformation of the subject. It comprises two modules: i) a convolutional\nAutoencoder that transforms attributes of the biometric raw data, such as the\ngender or the activity being performed, into a new privacy-preserving\nrepresentation; and ii) a mobile gait verification system based on the\ncombination of Convolutional Neural Networks (CNNs) and Recurrent Neural\nNetworks (RNNs) with a Siamese architecture. The main advantage of\nGaitPrivacyON is that the first module (convolutional Autoencoder) is trained\nin an unsupervised way, without specifying the sensitive attributes of the\nsubject to protect. The experimental results achieved using two popular\ndatabases (MotionSense and MobiAct) suggest the potential of GaitPrivacyON to\nsignificantly improve the privacy of the subject while keeping user\nauthentication results higher than 99% Area Under the Curve (AUC). To the best\nof our knowledge, this is the first mobile gait verification approach that\nconsiders privacy-preserving methods trained in an unsupervised way.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Delgado_Santos_P/0/1/0/all/0/1\">Paula Delgado-Santos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tolosana_R/0/1/0/all/0/1\">Ruben Tolosana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guest_R/0/1/0/all/0/1\">Richard Guest</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vera_R/0/1/0/all/0/1\">Ruben Vera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deravi_F/0/1/0/all/0/1\">Farzin Deravi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morales_A/0/1/0/all/0/1\">Aythami Morales</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Focus Your Distribution: Coarse-to-Fine Non-Contrastive Learning for Anomaly Detection and Localization. (arXiv:2110.04538v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.04538","description":"<p>The essence of unsupervised anomaly detection is to learn the compact\ndistribution of normal samples and detect outliers as anomalies in testing.\nMeanwhile, the anomalies in real-world are usually subtle and fine-grained in a\nhigh-resolution image especially for industrial applications. Towards this end,\nwe propose a novel framework for unsupervised anomaly detection and\nlocalization. Our method aims at learning dense and compact distribution from\nnormal images with a coarse-to-fine alignment process. The coarse alignment\nstage standardizes the pixel-wise position of objects in both image and feature\nlevels. The fine alignment stage then densely maximizes the similarity of\nfeatures among all corresponding locations in a batch. To facilitate the\nlearning with only normal images, we propose a new pretext task called\nnon-contrastive learning for the fine alignment stage. Non-contrastive learning\nextracts robust and discriminating normal image representations without making\nassumptions on abnormal samples, and it thus empowers our model to generalize\nto various anomalous scenarios. Extensive experiments on two typical industrial\ndatasets of MVTec AD and BenTech AD demonstrate that our framework is effective\nin detecting various real-world defects and achieves a new state-of-the-art in\nindustrial unsupervised anomaly detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Ye Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_R/0/1/0/all/0/1\">Rui Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_T/0/1/0/all/0/1\">Tianpeng Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1\">Rui Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Liwei Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Content Preserving Image Translation with Texture Co-occurrence and Spatial Self-Similarity for Texture Debiasing and Domain Adaptation. (arXiv:2110.07920v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.07920","description":"<p>Models trained on datasets with texture bias usually perform poorly on\nout-of-distribution samples since biased representations are embedded into the\nmodel. Recently, various image translation and debiasing methods have attempted\nto disentangle texture biased representations for downstream tasks, but\naccurately discarding biased features without altering other relevant\ninformation is still challenging. In this paper, we propose a novel framework\nthat leverages image translation to generate additional training images using\nthe content of a source image and the texture of a target image with a\ndifferent bias property to explicitly mitigate texture bias when training a\nmodel on a target task. Our model ensures texture similarity between the target\nand generated images via a texture co-occurrence loss while preserving content\ndetails from source images with a spatial self-similarity loss. Both the\ngenerated and original training images are combined to train improved\nclassification or segmentation models robust to inconsistent texture bias.\nEvaluation on five classification- and two segmentation-datasets with known\ntexture biases demonstrates the utility of our method, and reports significant\nimprovements over recent state-of-the-art methods in all cases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1\">Myeongkyun Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Won_D/0/1/0/all/0/1\">Dongkyu Won</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luna_M/0/1/0/all/0/1\">Miguel Luna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chikontwe_P/0/1/0/all/0/1\">Philip Chikontwe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_K/0/1/0/all/0/1\">Kyung Soo Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahn_J/0/1/0/all/0/1\">June Hong Ahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Sang Hyun Park</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Keypoint Representations: Modeling Keypoints and Poses as Objects for Multi-Person Human Pose Estimation. (arXiv:2111.08557v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.08557","description":"<p>In keypoint estimation tasks such as human pose estimation, heatmap-based\nregression is the dominant approach despite possessing notable drawbacks:\nheatmaps intrinsically suffer from quantization error and require excessive\ncomputation to generate and post-process. Motivated to find a more efficient\nsolution, we propose to model individual keypoints and sets of spatially\nrelated keypoints (i.e., poses) as objects within a dense single-stage\nanchor-based detection framework. Hence, we call our method KAPAO (pronounced\n\"Ka-Pow\"), for Keypoints And Poses As Objects. KAPAO is applied to the problem\nof single-stage multi-person human pose estimation by simultaneously detecting\nhuman pose and keypoint objects and fusing the detections to exploit the\nstrengths of both object representations. In experiments, we observe that KAPAO\nis faster and more accurate than previous methods, which suffer greatly from\nheatmap post-processing. The accuracy-speed trade-off is especially favourable\nin the practical setting when not using test-time augmentation. Source code:\nhttps://github.com/wmcnally/kapao.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+McNally_W/0/1/0/all/0/1\">William McNally</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vats_K/0/1/0/all/0/1\">Kanav Vats</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1\">Alexander Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McPhee_J/0/1/0/all/0/1\">John McPhee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"STEEX: Steering Counterfactual Explanations with Semantics. (arXiv:2111.09094v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.09094","description":"<p>As deep learning models are increasingly used in safety-critical\napplications, explainability and trustworthiness become major concerns. For\nsimple images, such as low-resolution face portraits, synthesizing visual\ncounterfactual explanations has recently been proposed as a way to uncover the\ndecision mechanisms of a trained classification model. In this work, we address\nthe problem of producing counterfactual explanations for high-quality images\nand complex scenes. Leveraging recent semantic-to-image models, we propose a\nnew generative counterfactual explanation framework that produces plausible and\nsparse modifications which preserve the overall scene structure. Furthermore,\nwe introduce the concept of \"region-targeted counterfactual explanations\", and\na corresponding framework, where users can guide the generation of\ncounterfactuals by specifying a set of semantic regions of the query image the\nexplanation must be about. Extensive experiments are conducted on challenging\ndatasets including high-quality portraits (CelebAMask-HQ) and driving scenes\n(BDD100k). Code is available at https://github.com/valeoai/STEEX\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jacob_P/0/1/0/all/0/1\">Paul Jacob</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zablocki_E/0/1/0/all/0/1\">&#xc9;loi Zablocki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ben_Younes_H/0/1/0/all/0/1\">H&#xe9;di Ben-Younes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Micka&#xeb;l Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_P/0/1/0/all/0/1\">Patrick P&#xe9;rez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cord_M/0/1/0/all/0/1\">Matthieu Cord</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Class-agnostic Object Detection with Multi-modal Transformer. (arXiv:2111.11430v6 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.11430","description":"<p>What constitutes an object? This has been a long-standing question in\ncomputer vision. Towards this goal, numerous learning-free and learning-based\napproaches have been developed to score objectness. However, they generally do\nnot scale well across new domains and novel objects. In this paper, we advocate\nthat existing methods lack a top-down supervision signal governed by\nhuman-understandable semantics. For the first time in literature, we\ndemonstrate that Multi-modal Vision Transformers (MViT) trained with aligned\nimage-text pairs can effectively bridge this gap. Our extensive experiments\nacross various domains and novel objects show the state-of-the-art performance\nof MViTs to localize generic objects in images. Based on the observation that\nexisting MViTs do not include multi-scale feature processing and usually\nrequire longer training schedules, we develop an efficient MViT architecture\nusing multi-scale deformable attention and late vision-language fusion. We show\nthe significance of MViT proposals in a diverse range of applications including\nopen-world object detection, salient and camouflage object detection,\nsupervised and self-supervised detection tasks. Further, MViTs can adaptively\ngenerate proposals given a specific language query and thus offer enhanced\ninteractability. Code: \\url{https://git.io/J1HPY}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maaz_M/0/1/0/all/0/1\">Muhammad Maaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rasheed_H/0/1/0/all/0/1\">Hanoona Rasheed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1\">Salman Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1\">Fahad Shahbaz Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anwer_R/0/1/0/all/0/1\">Rao Muhammad Anwer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Ming-Hsuan Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Facial Depth and Normal Estimation using Single Dual-Pixel Camera. (arXiv:2111.12928v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.12928","description":"<p>Many mobile manufacturers recently have adopted Dual-Pixel (DP) sensors in\ntheir flagship models for faster auto-focus and aesthetic image captures.\nDespite their advantages, research on their usage for 3D facial understanding\nhas been limited due to the lack of datasets and algorithmic designs that\nexploit parallax in DP images. This is because the baseline of sub-aperture\nimages is extremely narrow and parallax exists in the defocus blur region. In\nthis paper, we introduce a DP-oriented Depth/Normal network that reconstructs\nthe 3D facial geometry. For this purpose, we collect a DP facial data with more\nthan 135K images for 101 persons captured with our multi-camera structured\nlight systems. It contains the corresponding ground-truth 3D models including\ndepth map and surface normal in metric scale. Our dataset allows the proposed\nmatching network to be generalized for 3D facial depth/normal estimation. The\nproposed network consists of two novel modules: Adaptive Sampling Module and\nAdaptive Normal Module, which are specialized in handling the defocus blur in\nDP images. Finally, the proposed method achieves state-of-the-art performances\nover recent DP-based depth/normal estimation methods. We also demonstrate the\napplicability of the estimated depth/normal to face spoofing and relighting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1\">Minjun Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choe_J/0/1/0/all/0/1\">Jaesung Choe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ha_H/0/1/0/all/0/1\">Hyowon Ha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeon_H/0/1/0/all/0/1\">Hae-Gon Jeon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Im_S/0/1/0/all/0/1\">Sunghoon Im</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kweon_I/0/1/0/all/0/1\">In So Kweon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_K/0/1/0/all/0/1\">KuK-Jin Yoon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VL-LTR: Learning Class-wise Visual-Linguistic Representation for Long-Tailed Visual Recognition. (arXiv:2111.13579v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.13579","description":"<p>Deep learning-based models encounter challenges when processing long-tailed\ndata in the real world. Existing solutions usually employ some balancing\nstrategies or transfer learning to deal with the class imbalance problem, based\non the image modality. In this work, we present a visual-linguistic long-tailed\nrecognition framework, termed VL-LTR, and conduct empirical studies on the\nbenefits of introducing text modality for long-tailed recognition (LTR).\nCompared to existing approaches, the proposed VL-LTR has the following merits.\n(1) Our method can not only learn visual representation from images but also\nlearn corresponding linguistic representation from noisy class-level text\ndescriptions collected from the Internet; (2) Our method can effectively use\nthe learned visual-linguistic representation to improve the visual recognition\nperformance, especially for classes with fewer image samples. We also conduct\nextensive experiments and set the new state-of-the-art performance on\nwidely-used LTR benchmarks. Notably, our method achieves 77.2% overall accuracy\non ImageNet-LT, which significantly outperforms the previous best method by\nover 17 points, and is close to the prevailing performance training on the full\nImageNet. Code is available at https://github.com/ChangyaoTian/VL-LTR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_C/0/1/0/all/0/1\">Changyao Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenhai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xizhou Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1\">Jifeng Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"$\\ell_\\infty$-Robustness and Beyond: Unleashing Efficient Adversarial Training. (arXiv:2112.00378v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2112.00378","description":"<p>Neural networks are vulnerable to adversarial attacks: adding well-crafted,\nimperceptible perturbations to their input can modify their output. Adversarial\ntraining is one of the most effective approaches in training robust models\nagainst such attacks. However, it is much slower than vanilla training of\nneural networks since it needs to construct adversarial examples for the entire\ntraining data at every iteration, hampering its effectiveness. Recently, Fast\nAdversarial Training (FAT) was proposed that can obtain robust models\nefficiently. However, the reasons behind its success are not fully understood,\nand more importantly, it can only train robust models for $\\ell_\\infty$-bounded\nattacks as it uses FGSM during training. In this paper, by leveraging the\ntheory of coreset selection, we show how selecting a small subset of training\ndata provides a general, more principled approach toward reducing the time\ncomplexity of robust training. Unlike existing methods, our approach can be\nadapted to a wide variety of training objectives, including TRADES,\n$\\ell_p$-PGD, and Perceptual Adversarial Training (PAT). Our experimental\nresults indicate that our approach speeds up adversarial training by 2-3 times\nwhile experiencing a slight reduction in the clean and robust accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dolatabadi_H/0/1/0/all/0/1\">Hadi M. Dolatabadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erfani_S/0/1/0/all/0/1\">Sarah Erfani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leckie_C/0/1/0/all/0/1\">Christopher Leckie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Object-Centric Unsupervised Image Captioning. (arXiv:2112.00969v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.00969","description":"<p>Image captioning is a longstanding problem in the field of computer vision\nand natural language processing. To date, researchers have produced impressive\nstate-of-the-art performance in the age of deep learning. Most of these\nstate-of-the-art, however, requires large volume of annotated image-caption\npairs in order to train their models. When given an image dataset of interests,\npractitioner needs to annotate the caption for each image in the training set\nand this process needs to happen for each newly collected image dataset. In\nthis paper, we explore the task of unsupervised image captioning which utilizes\nunpaired images and texts to train the model so that the texts can come from\ndifferent sources than the images. A main school of research on this topic that\nhas been shown to be effective is to construct pairs from the images and texts\nin the training set according to their overlap of objects. Unlike in the\nsupervised setting, these constructed pairings are however not guaranteed to\nhave fully overlapping set of objects. Our work in this paper overcomes this by\nharvesting objects corresponding to a given sentence from the training set,\neven if they don't belong to the same image. When used as input to a\ntransformer, such mixture of objects enables larger if not full object\ncoverage, and when supervised by the corresponding sentence, produced results\nthat outperform current state of the art unsupervised methods by a significant\nmargin. Building upon this finding, we further show that (1) additional\ninformation on relationship between objects and attributes of objects also\nhelps in boosting performance; and (2) our method also extends well to\nnon-English image captioning, which usually suffers from a scarcer level of\nannotations. Our findings are supported by strong empirical results. Our code\nis available at https://github.com/zihangm/obj-centric-unsup-caption.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1\">Zihang Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">David Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1\">Xuefei Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_A/0/1/0/all/0/1\">Ashish Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1\">Ser-Nam Lim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TISE: Bag of Metrics for Text-to-Image Synthesis Evaluation. (arXiv:2112.01398v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.01398","description":"<p>In this paper, we conduct a study on the state-of-the-art methods for\ntext-to-image synthesis and propose a framework to evaluate these methods. We\nconsider syntheses where an image contains a single or multiple objects. Our\nstudy outlines several issues in the current evaluation pipeline: (i) for image\nquality assessment, a commonly used metric, e.g., Inception Score (IS), is\noften either miscalibrated for the single-object case or misused for the\nmulti-object case; (ii) for text relevance and object accuracy assessment,\nthere is an overfitting phenomenon in the existing R-precision (RP) and\nSemantic Object Accuracy (SOA) metrics, respectively; (iii) for multi-object\ncase, many vital factors for evaluation, e.g., object fidelity, positional\nalignment, counting alignment, are largely dismissed; (iv) the ranking of the\nmethods based on current metrics is highly inconsistent with real images. To\novercome these issues, we propose a combined set of existing and new metrics to\nsystematically evaluate the methods. For existing metrics, we offer an improved\nversion of IS named IS* by using temperature scaling to calibrate the\nconfidence of the classifier used by IS; we also propose a solution to mitigate\nthe overfitting issues of RP and SOA. For new metrics, we develop counting\nalignment, positional alignment, object-centric IS, and object-centric FID\nmetrics for evaluating the multi-object case. We show that benchmarking with\nour bag of metrics results in a highly consistent ranking among existing\nmethods that is well-aligned with human evaluation. As a by-product, we create\nAttnGAN++, a simple but strong baseline for the benchmark by stabilizing the\ntraining of AttnGAN using spectral normalization. We also release our toolbox,\nso-called TISE, for advocating fair and consistent evaluation of text-to-image\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dinh_T/0/1/0/all/0/1\">Tan M. Dinh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_R/0/1/0/all/0/1\">Rang Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_B/0/1/0/all/0/1\">Binh-Son Hua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CA-SSL: Class-Agnostic Semi-Supervised Learning for Detection and Segmentation. (arXiv:2112.04966v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.04966","description":"<p>To improve instance-level detection/segmentation performance, existing\nself-supervised and semi-supervised methods extract either task-unrelated or\ntask-specific training signals from unlabeled data. We show that these two\napproaches, at the two extreme ends of the task-specificity spectrum, are\nsuboptimal for the task performance. Utilizing too little task-specific\ntraining signals causes underfitting to the ground-truth labels of downstream\ntasks, while the opposite causes overfitting to the ground-truth labels. To\nthis end, we propose a novel Class-Agnostic Semi-Supervised Learning (CA-SSL)\nframework to achieve a more favorable task-specificity balance in extracting\ntraining signals from unlabeled data. CA-SSL has three training stages that act\non either ground-truth labels (labeled data) or pseudo labels (unlabeled data).\nThis decoupling strategy avoids the complicated scheme in traditional SSL\nmethods that balances the contributions from both data types. Especially, we\nintroduce a warmup training stage to achieve a more optimal balance in task\nspecificity by ignoring class information in the pseudo labels, while\npreserving localization training signals. As a result, our warmup model can\nbetter avoid underfitting/overfitting when fine-tuned on the ground-truth\nlabels in detection and segmentation tasks. Using 3.6M unlabeled data, we\nachieve a significant performance gain of 4.7% over ImageNet-pretrained\nbaseline on FCOS object detection. In addition, our warmup model demonstrates\nexcellent transferability to other detection and segmentation frameworks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qi_L/0/1/0/all/0/1\">Lu Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuen_J/0/1/0/all/0/1\">Jason Kuen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhe Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jiuxiang Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_F/0/1/0/all/0/1\">Fengyun Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1\">Weidong Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Z/0/1/0/all/0/1\">Zhen Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Ming-Hsuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\">Jiaya Jia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FEAR: Fast, Efficient, Accurate and Robust Visual Tracker. (arXiv:2112.07957v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.07957","description":"<p>We present FEAR, a family of fast, efficient, accurate, and robust Siamese\nvisual trackers. We present a novel and efficient way to benefit from\ndual-template representation for object model adaption, which incorporates\ntemporal information with only a single learnable parameter. We further improve\nthe tracker architecture with a pixel-wise fusion block. By plugging-in\nsophisticated backbones with the abovementioned modules, FEAR-M and FEAR-L\ntrackers surpass most Siamese trackers on several academic benchmarks in both\naccuracy and efficiency. Employed with the lightweight backbone, the optimized\nversion FEAR-XS offers more than 10 times faster tracking than current Siamese\ntrackers while maintaining near state-of-the-art results. FEAR-XS tracker is\n2.4x smaller and 4.3x faster than LightTrack with superior accuracy. In\naddition, we expand the definition of the model efficiency by introducing FEAR\nbenchmark that assesses energy consumption and execution speed. We show that\nenergy consumption is a limiting factor for trackers on mobile devices. Source\ncode, pretrained models, and evaluation protocol are available at\nhttps://github.com/PinataFarms/FEARTracker.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Borsuk_V/0/1/0/all/0/1\">Vasyl Borsuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vei_R/0/1/0/all/0/1\">Roman Vei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kupyn_O/0/1/0/all/0/1\">Orest Kupyn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martyniuk_T/0/1/0/all/0/1\">Tetiana Martyniuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krashenyi_I/0/1/0/all/0/1\">Igor Krashenyi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matas_J/0/1/0/all/0/1\">Ji&#x159;i Matas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comprehensive Analysis of the Object Detection Pipeline on UAVs. (arXiv:2203.00306v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2203.00306","description":"<p>An object detection pipeline comprises a camera that captures the scene and\nan object detector that processes these images. The quality of the images\ndirectly affects the performance of the object detector. Many works nowadays\nfocus either on improving the image quality or improving the object detection\nmodels independently, but neglect the importance of joint optimization of the\ntwo subsystems. The goal of this paper is to tune the detection throughput and\naccuracy of existing object detectors in the remote sensing scenario by\nfocusing on optimizing the input images tailored to the object detector. To\nachieve this, we empirically analyze the influence of two selected camera\ncalibration parameters (camera distortion correction and gamma correction) and\nfive image parameters (quantization, compression, resolution, color model,\nadditional channels) for these applications. For our experiments, we utilize\nthree UAV data sets from different domains and a mixture of large and small\nstate-of-the-art object detector models to provide an extensive evaluation of\nthe influence of the pipeline parameters. Finally, we realize an object\ndetection pipeline prototype on an embedded platform for an UAV and give a best\npractice recommendation for building object detection pipelines based on our\nfindings. We show that not all parameters have an equal impact on detection\naccuracy and data throughput, and that by using a suitable compromise between\nparameters we are able to achieve higher detection accuracy for lightweight\nobject detection models, while keeping the same data throughput.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Varga_L/0/1/0/all/0/1\">Leon Amadeus Varga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koch_S/0/1/0/all/0/1\">Sebastian Koch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zell_A/0/1/0/all/0/1\">Andreas Zell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Hierarchical Graph Representation for Large-Scale Zero-Shot Image Classification. (arXiv:2203.01386v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.01386","description":"<p>The main question we address in this paper is how to scale up visual\nrecognition of unseen classes, also known as zero-shot learning, to tens of\nthousands of categories as in the ImageNet-21K benchmark. At this scale,\nespecially with many fine-grained categories included in ImageNet-21K, it is\ncritical to learn quality visual semantic representations that are\ndiscriminative enough to recognize unseen classes and distinguish them from\nseen ones. We propose a \\emph{H}ierarchical \\emph{G}raphical knowledge\n\\emph{R}epresentation framework for the confidence-based classification method,\ndubbed as HGR-Net. Our experimental results demonstrate that HGR-Net can grasp\nclass inheritance relations by utilizing hierarchical conceptual knowledge. Our\nmethod significantly outperformed all existing techniques, boosting the\nperformance by 7\\% compared to the runner-up approach on the ImageNet-21K\nbenchmark. We show that HGR-Net is learning-efficient in few-shot scenarios. We\nalso analyzed our method on smaller datasets like ImageNet-21K-P, 2-hops and\n3-hops, demonstrating its generalization ability. Our benchmark and code are\navailable at https://kaiyi.me/p/hgrnet.html.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yi_K/0/1/0/all/0/1\">Kai Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xiaoqian Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gou_Y/0/1/0/all/0/1\">Yunhao Gou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elhoseiny_M/0/1/0/all/0/1\">Mohamed Elhoseiny</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DiT: Self-supervised Pre-training for Document Image Transformer. (arXiv:2203.02378v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.02378","description":"<p>Image Transformer has recently achieved significant progress for natural\nimage understanding, either using supervised (ViT, DeiT, etc.) or\nself-supervised (BEiT, MAE, etc.) pre-training techniques. In this paper, we\npropose \\textbf{DiT}, a self-supervised pre-trained \\textbf{D}ocument\n\\textbf{I}mage \\textbf{T}ransformer model using large-scale unlabeled text\nimages for Document AI tasks, which is essential since no supervised\ncounterparts ever exist due to the lack of human-labeled document images. We\nleverage DiT as the backbone network in a variety of vision-based Document AI\ntasks, including document image classification, document layout analysis, table\ndetection as well as text detection for OCR. Experiment results have\nillustrated that the self-supervised pre-trained DiT model achieves new\nstate-of-the-art results on these downstream tasks, e.g. document image\nclassification (91.11 $\\rightarrow$ 92.69), document layout analysis (91.0\n$\\rightarrow$ 94.9), table detection (94.23 $\\rightarrow$ 96.55) and text\ndetection for OCR (93.07 $\\rightarrow$ 94.29). The code and pre-trained models\nare publicly available at \\url{https://aka.ms/msdit}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junlong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yiheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_T/0/1/0/all/0/1\">Tengchao Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1\">Lei Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Cha Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CF-ViT: A General Coarse-to-Fine Method for Vision Transformer. (arXiv:2203.03821v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.03821","description":"<p>Vision Transformers (ViT) have made many breakthroughs in computer vision\ntasks. However, considerable redundancy arises in the spatial dimension of an\ninput image, leading to massive computational costs. Therefore, We propose a\ncoarse-to-fine vision transformer (CF-ViT) to relieve computational burden\nwhile retaining performance in this paper. Our proposed CF-ViT is motivated by\ntwo important observations in modern ViT models: (1) The coarse-grained patch\nsplitting can locate informative regions of an input image. (2) Most images can\nbe well recognized by a ViT model in a small-length token sequence. Therefore,\nour CF-ViT implements network inference in a two-stage manner. At coarse\ninference stage, an input image is split into a small-length patch sequence for\na computationally economical classification. If not well recognized, the\ninformative patches are identified and further re-split in a fine-grained\ngranularity. Extensive experiments demonstrate the efficacy of our CF-ViT. For\nexample, without any compromise on performance, CF-ViT reduces 53% FLOPs of\nLV-ViT, and also achieves 2.01x throughput.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mengzhao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Mingbao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Ke Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yunhang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yongjian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_F/0/1/0/all/0/1\">Fei Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quasi-Balanced Self-Training on Noise-Aware Synthesis of Object Point Clouds for Closing Domain Gap. (arXiv:2203.03833v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.03833","description":"<p>Semantic analyses of object point clouds are largely driven by releasing of\nbenchmarking datasets, including synthetic ones whose instances are sampled\nfrom object CAD models. However, learning from synthetic data may not\ngeneralize to practical scenarios, where point clouds are typically incomplete,\nnon-uniformly distributed, and noisy. Such a challenge of Simulation-to-Reality\n(Sim2Real) domain gap could be mitigated via learning algorithms of domain\nadaptation; however, we argue that generation of synthetic point clouds via\nmore physically realistic rendering is a powerful alternative, as systematic\nnon-uniform noise patterns can be captured. To this end, we propose an\nintegrated scheme consisting of physically realistic synthesis of object point\nclouds via rendering stereo images via projection of speckle patterns onto CAD\nmodels and a novel quasi-balanced self-training designed for more balanced data\ndistribution by sparsity-driven selection of pseudo labeled samples for long\ntailed classes. Experiment results can verify the effectiveness of our method\nas well as both of its modules for unsupervised domain adaptation on point\ncloud classification, achieving the state-of-the-art performance. Source codes\nand the SpeckleNet synthetic dataset are available at\nhttps://github.com/Gorilla-Lab-SCUT/QS3.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yongwei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zihao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_L/0/1/0/all/0/1\">Longkun Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Ke Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_K/0/1/0/all/0/1\">Kui Jia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VoViT: Low Latency Graph-based Audio-Visual Voice Separation Transformer. (arXiv:2203.04099v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2203.04099","description":"<p>This paper presents an audio-visual approach for voice separation which\nproduces state-of-the-art results at a low latency in two scenarios: speech and\nsinging voice. The model is based on a two-stage network. Motion cues are\nobtained with a lightweight graph convolutional network that processes face\nlandmarks. Then, both audio and motion features are fed to an audio-visual\ntransformer which produces a fairly good estimation of the isolated target\nsource. In a second stage, the predominant voice is enhanced with an audio-only\nnetwork. We present different ablation studies and comparison to\nstate-of-the-art methods. Finally, we explore the transferability of models\ntrained for speech separation in the task of singing voice separation. The\ndemos, code, and weights are available in https://ipcv.github.io/VoViT/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Montesinos_J/0/1/0/all/0/1\">Juan F. Montesinos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadandale_V/0/1/0/all/0/1\">Venkatesh S. Kadandale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haro_G/0/1/0/all/0/1\">Gloria Haro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PETR: Position Embedding Transformation for Multi-View 3D Object Detection. (arXiv:2203.05625v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.05625","description":"<p>In this paper, we develop position embedding transformation (PETR) for\nmulti-view 3D object detection. PETR encodes the position information of 3D\ncoordinates into image features, producing the 3D position-aware features.\nObject query can perceive the 3D position-aware features and perform end-to-end\nobject detection. PETR achieves state-of-the-art performance (50.4% NDS and\n44.1% mAP) on standard nuScenes dataset and ranks 1st place on the benchmark.\nIt can serve as a simple yet strong baseline for future research. Code is\navailable at \\url{https://github.com/megvii-research/PETR}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yingfei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tiancai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CODA: A Real-World Road Corner Case Dataset for Object Detection in Autonomous Driving. (arXiv:2203.07724v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.07724","description":"<p>Contemporary deep-learning object detection methods for autonomous driving\nusually assume prefixed categories of common traffic participants, such as\npedestrians and cars. Most existing detectors are unable to detect uncommon\nobjects and corner cases (e.g., a dog crossing a street), which may lead to\nsevere accidents in some situations, making the timeline for the real-world\napplication of reliable autonomous driving uncertain. One main reason that\nimpedes the development of truly reliably self-driving systems is the lack of\npublic datasets for evaluating the performance of object detectors on corner\ncases. Hence, we introduce a challenging dataset named CODA that exposes this\ncritical problem of vision-based detectors. The dataset consists of 1500\ncarefully selected real-world driving scenes, each containing four object-level\ncorner cases (on average), spanning more than 30 object categories. On CODA,\nthe performance of standard object detectors trained on large-scale autonomous\ndriving datasets significantly drops to no more than 12.8% in mAR. Moreover, we\nexperiment with the state-of-the-art open-world object detector and find that\nit also fails to reliably identify the novel objects in CODA, suggesting that a\nrobust perception system for autonomous driving is probably still far from\nreach. We expect our CODA dataset to facilitate further research in reliable\ndetection for real-world autonomous driving. Our dataset will be released at\nhttps://coda-dataset.github.io.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kaican Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haoyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_L/0/1/0/all/0/1\">Lanqing Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_C/0/1/0/all/0/1\">Chaoqiang Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jianhua Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yukuai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chunjing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeung_D/0/1/0/all/0/1\">Dit-Yan Yeung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenguo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hang Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"REALY: Rethinking the Evaluation of 3D Face Reconstruction. (arXiv:2203.09729v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.09729","description":"<p>The evaluation of 3D face reconstruction results typically relies on a rigid\nshape alignment between the estimated 3D model and the ground-truth scan. We\nobserve that aligning two shapes with different reference points can largely\naffect the evaluation results. This poses difficulties for precisely diagnosing\nand improving a 3D face reconstruction method. In this paper, we propose a\nnovel evaluation approach with a new benchmark REALY, consists of 100 globally\naligned face scans with accurate facial keypoints, high-quality region masks,\nand topology-consistent meshes. Our approach performs region-wise shape\nalignment and leads to more accurate, bidirectional correspondences during\ncomputing the shape errors. The fine-grained, region-wise evaluation results\nprovide us detailed understandings about the performance of state-of-the-art 3D\nface reconstruction methods. For example, our experiments on single-image based\nreconstruction methods reveal that DECA performs the best on nose regions,\nwhile GANFit performs better on cheek regions. Besides, a new and high-quality\n3DMM basis, HIFI3D++, is further derived using the same procedure as we\nconstruct REALY to align and retopologize several 3D face datasets. We will\nrelease REALY, HIFI3D++, and our new evaluation pipeline at\nhttps://realy3dface.com.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chai_Z/0/1/0/all/0/1\">Zenghao Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haoxian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Jing Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_D/0/1/0/all/0/1\">Di Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhengzhuo Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhe_X/0/1/0/all/0/1\">Xuefei Zhe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1\">Chun Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_L/0/1/0/all/0/1\">Linchao Bao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ARM: Any-Time Super-Resolution Method. (arXiv:2203.10812v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.10812","description":"<p>This paper proposes an Any-time super-Resolution Method (ARM) to tackle the\nover-parameterized single image super-resolution (SISR) models. Our ARM is\nmotivated by three observations: (1) The performance of different image patches\nvaries with SISR networks of different sizes. (2) There is a tradeoff between\ncomputation overhead and performance of the reconstructed image. (3) Given an\ninput image, its edge information can be an effective option to estimate its\nPSNR. Subsequently, we train an ARM supernet containing SISR subnets of\ndifferent sizes to deal with image patches of various complexity. To that\neffect, we construct an Edge-to-PSNR lookup table that maps the edge score of\nan image patch to the PSNR performance for each subnet, together with a set of\ncomputation costs for the subnets. In the inference, the image patches are\nindividually distributed to different subnets for a better\ncomputation-performance tradeoff. Moreover, each SISR subnet shares weights of\nthe ARM supernet, thus no extra parameters are introduced. The setting of\nmultiple subnets can well adapt the computational cost of SISR model to the\ndynamically available hardware resources, allowing the SISR task to be in\nservice at any time. Extensive experiments on resolution datasets of different\nsizes with popular SISR networks as backbones verify the effectiveness and the\nversatility of our ARM. The source code is available at\nhttps://github.com/chenbong/ARM-Net.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bohong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Mingbao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheng_K/0/1/0/all/0/1\">Kekai Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mengdan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Peixian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Ke Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1\">Liujuan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PersFormer: 3D Lane Detection via Perspective Transformer and the OpenLane Benchmark. (arXiv:2203.11089v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.11089","description":"<p>Methods for 3D lane detection have been recently proposed to address the\nissue of inaccurate lane layouts in many autonomous driving scenarios\n(uphill/downhill, bump, etc.). Previous work struggled in complex cases due to\ntheir simple designs of the spatial transformation between front view and\nbird's eye view (BEV) and the lack of a realistic dataset. Towards these\nissues, we present PersFormer: an end-to-end monocular 3D lane detector with a\nnovel Transformer-based spatial feature transformation module. Our model\ngenerates BEV features by attending to related front-view local regions with\ncamera parameters as a reference. PersFormer adopts a unified 2D/3D anchor\ndesign and an auxiliary task to detect 2D/3D lanes simultaneously, enhancing\nthe feature consistency and sharing the benefits of multi-task learning.\nMoreover, we release one of the first large-scale real-world 3D lane datasets:\nOpenLane, with high-quality annotation and scenario diversity. OpenLane\ncontains 200,000 frames, over 880,000 instance-level lanes, 14 lane categories,\nalong with scene tags and the closed-in-path object annotations to encourage\nthe development of lane detection and more industrial-related autonomous\ndriving methods. We show that PersFormer significantly outperforms competitive\nbaselines in the 3D lane detection task on our new OpenLane dataset as well as\nApollo 3D Lane Synthetic dataset, and is also on par with state-of-the-art\nalgorithms in the 2D task on OpenLane. The project page is available at\nhttps://github.com/OpenPerceptionX/PersFormer_3DLane and OpenLane dataset is\nprovided at https://github.com/OpenPerceptionX/OpenLane.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Li Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sima_C/0/1/0/all/0/1\">Chonghao Sima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zehan Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jiajie Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1\">Xiangwei Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_C/0/1/0/all/0/1\">Conghui He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jianping Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Junchi Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Physics-based Learning of Parameterized Thermodynamics from Real-time Thermography. (arXiv:2203.13148v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.13148","description":"<p>Progress in automatic control of thermal processes and real-time estimation\nof heat penetration into live tissue has long been limited by the difficulty of\nobtaining high-fidelity thermodynamic models. Traditionally, in complex\nthermodynamic systems, it is often infeasible to estimate the thermophysical\nparameters of spatiotemporally varying processes, forcing the adoption of\nmodel-free control architectures. This comes at the cost of losing any\nrobustness guarantees, and implies a need for extensive real-life testing. In\nrecent years, however, infrared cameras and other thermographic equipment have\nbecome readily applicable to these processes, allowing for a real-time,\nnon-invasive means of sensing the thermal state of a process. In this work, we\npresent a novel physics-based approach to learning a thermal process's dynamics\ndirectly from such real-time thermographic data, while focusing attention on\nregions with high thermal activity. We call this process, which applies to any\nhigher-dimensional scalar field, attention-based noise robust averaging (ANRA).\nGiven a partial-differential equation model structure, we show that our\napproach is robust against noise, and can be used to initialize optimization\nroutines to further refine parameter estimates. We demonstrate our method on\nseveral simulation examples, as well as by applying it to electrosurgical\nthermal response data on in vivo porcine skin tissue.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+El_Kebir_H/0/1/0/all/0/1\">Hamza El-Kebir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yongseok Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bentsman_J/0/1/0/all/0/1\">Joseph Bentsman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Primitive-based Shape Abstraction via Nonparametric Bayesian Inference. (arXiv:2203.14714v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.14714","description":"<p>3D shape abstraction has drawn great interest over the years. Apart from\nlow-level representations such as meshes and voxels, researchers also seek to\nsemantically abstract complex objects with basic geometric primitives. Recent\ndeep learning methods rely heavily on datasets, with limited generality to\nunseen categories. Furthermore, abstracting an object accurately yet with a\nsmall number of primitives still remains a challenge. In this paper, we propose\na novel non-parametric Bayesian statistical method to infer an abstraction,\nconsisting of an unknown number of geometric primitives, from a point cloud. We\nmodel the generation of points as observations sampled from an infinite mixture\nof Gaussian Superquadric Taper Models (GSTM). Our approach formulates the\nabstraction as a clustering problem, in which: 1) each point is assigned to a\ncluster via the Chinese Restaurant Process (CRP); 2) a primitive representation\nis optimized for each cluster, and 3) a merging post-process is incorporated to\nprovide a concise representation. We conduct extensive experiments on two\ndatasets. The results indicate that our method outperforms the state-of-the-art\nin terms of accuracy and is generalizable to various types of objects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuwei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weixiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruan_S/0/1/0/all/0/1\">Sipu Ruan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chirikjian_G/0/1/0/all/0/1\">Gregory S. Chirikjian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quantized GAN for Complex Music Generation from Dance Videos. (arXiv:2204.00604v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.00604","description":"<p>We present Dance2Music-GAN (D2M-GAN), a novel adversarial multi-modal\nframework that generates complex musical samples conditioned on dance videos.\nOur proposed framework takes dance video frames and human body motions as\ninput, and learns to generate music samples that plausibly accompany the\ncorresponding input. Unlike most existing conditional music generation works\nthat generate specific types of mono-instrumental sounds using symbolic audio\nrepresentations (e.g., MIDI), and that usually rely on pre-defined musical\nsynthesizers, in this work we generate dance music in complex styles (e.g.,\npop, breaking, etc.) by employing a Vector Quantized (VQ) audio representation,\nand leverage both its generality and high abstraction capacity of its symbolic\nand continuous counterparts. By performing an extensive set of experiments on\nmultiple datasets, and following a comprehensive evaluation protocol, we assess\nthe generative qualities of our proposal against alternatives. The attained\nquantitative results, which measure the music consistency, beats\ncorrespondence, and music diversity, demonstrate the effectiveness of our\nproposed method. Last but not least, we curate a challenging dance-music\ndataset of in-the-wild TikTok videos, which we use to further demonstrate the\nefficacy of our approach in real-world applications -- and which we hope to\nserve as a starting point for relevant future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Ye Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olszewski_K/0/1/0/all/0/1\">Kyle Olszewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Achlioptas_P/0/1/0/all/0/1\">Panos Achlioptas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_M/0/1/0/all/0/1\">Menglei Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yan Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tulyakov_S/0/1/0/all/0/1\">Sergey Tulyakov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Style-Hallucinated Dual Consistency Learning for Domain Generalized Semantic Segmentation. (arXiv:2204.02548v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.02548","description":"<p>In this paper, we study the task of synthetic-to-real domain generalized\nsemantic segmentation, which aims to learn a model that is robust to unseen\nreal-world scenes using only synthetic data. The large domain shift between\nsynthetic and real-world data, including the limited source environmental\nvariations and the large distribution gap between synthetic and real-world\ndata, significantly hinders the model performance on unseen real-world scenes.\nIn this work, we propose the Style-HAllucinated Dual consistEncy learning\n(SHADE) framework to handle such domain shift. Specifically, SHADE is\nconstructed based on two consistency constraints, Style Consistency (SC) and\nRetrospection Consistency (RC). SC enriches the source situations and\nencourages the model to learn consistent representation across\nstyle-diversified samples. RC leverages real-world knowledge to prevent the\nmodel from overfitting to synthetic data and thus largely keeps the\nrepresentation consistent between the synthetic and real-world models.\nFurthermore, we present a novel style hallucination module (SHM) to generate\nstyle-diversified samples that are essential to consistency learning. SHM\nselects basis styles from the source distribution, enabling the model to\ndynamically generate diverse and realistic samples during training. Experiments\nshow that our SHADE yields significant improvement and outperforms\nstate-of-the-art methods by 5.05% and 8.35% on the average mIoU of three\nreal-world datasets on single- and multi-source settings, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yuyang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1\">Zhun Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_N/0/1/0/all/0/1\">Na Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1\">Nicu Sebe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Gim Hee Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unidirectional Video Denoising by Mimicking Backward Recurrent Modules with Look-ahead Forward Ones. (arXiv:2204.05532v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.05532","description":"<p>While significant progress has been made in deep video denoising, it remains\nvery challenging for exploiting historical and future frames. Bidirectional\nrecurrent networks (BiRNN) have exhibited appealing performance in several\nvideo restoration tasks. However, BiRNN is intrinsically offline because it\nuses backward recurrent modules to propagate from the last to current frames,\nwhich causes high latency and large memory consumption. To address the offline\nissue of BiRNN, we present a novel recurrent network consisting of forward and\nlook-ahead recurrent modules for unidirectional video denoising. Particularly,\nlook-ahead module is an elaborate forward module for leveraging information\nfrom near-future frames. When denoising the current frame, the hidden features\nby forward and look-ahead recurrent modules are combined, thereby making it\nfeasible to exploit both historical and near-future frames. Due to the scene\nmotion between non-neighboring frames, border pixels missing may occur when\nwarping look-ahead feature from near-future frame to current frame, which can\nbe largely alleviated by incorporating forward warping and proposed border\nenlargement. Experiments show that our method achieves state-of-the-art\nperformance with constant latency and memory consumption. Code is avaliable at\nhttps://github.com/nagejacob/FloRNN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiaohe Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_Z/0/1/0/all/0/1\">Zhenxin Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_W/0/1/0/all/0/1\">Wangmeng Zuo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking. (arXiv:2204.08387v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.08387","description":"<p>Self-supervised pre-training techniques have achieved remarkable progress in\nDocument AI. Most multimodal pre-trained models use a masked language modeling\nobjective to learn bidirectional representations on the text modality, but they\ndiffer in pre-training objectives for the image modality. This discrepancy adds\ndifficulty to multimodal representation learning. In this paper, we propose\n\\textbf{LayoutLMv3} to pre-train multimodal Transformers for Document AI with\nunified text and image masking. Additionally, LayoutLMv3 is pre-trained with a\nword-patch alignment objective to learn cross-modal alignment by predicting\nwhether the corresponding image patch of a text word is masked. The simple\nunified architecture and training objectives make LayoutLMv3 a general-purpose\npre-trained model for both text-centric and image-centric Document AI tasks.\nExperimental results show that LayoutLMv3 achieves state-of-the-art performance\nnot only in text-centric tasks, including form understanding, receipt\nunderstanding, and document visual question answering, but also in\nimage-centric tasks such as document image classification and document layout\nanalysis. The code and models are publicly available at\n\\url{https://aka.ms/layoutlmv3}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yupan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_T/0/1/0/all/0/1\">Tengchao Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1\">Lei Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yutong Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GIMO: Gaze-Informed Human Motion Prediction in Context. (arXiv:2204.09443v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.09443","description":"<p>Predicting human motion is critical for assistive robots and AR/VR\napplications, where the interaction with humans needs to be safe and\ncomfortable. Meanwhile, an accurate prediction depends on understanding both\nthe scene context and human intentions. Even though many works study\nscene-aware human motion prediction, the latter is largely underexplored due to\nthe lack of ego-centric views that disclose human intent and the limited\ndiversity in motion and scenes. To reduce the gap, we propose a large-scale\nhuman motion dataset that delivers high-quality body pose sequences, scene\nscans, as well as ego-centric views with the eye gaze that serves as a\nsurrogate for inferring human intent. By employing inertial sensors for motion\ncapture, our data collection is not tied to specific scenes, which further\nboosts the motion dynamics observed from our subjects. We perform an extensive\nstudy of the benefits of leveraging the eye gaze for ego-centric human motion\nprediction with various state-of-the-art architectures. Moreover, to realize\nthe full potential of the gaze, we propose a novel network architecture that\nenables bidirectional communication between the gaze and motion branches. Our\nnetwork achieves the top performance in human motion prediction on the proposed\ndataset, thanks to the intent information from eye gaze and the denoised gaze\nfeature modulated by the motion. Code and data can be found at\nhttps://github.com/y-zheng18/GIMO.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yang Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yanchao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mo_K/0/1/0/all/0/1\">Kaichun Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiaman Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yebin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">C. Karen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1\">Leonidas J. Guibas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Parametric Level-sets Enhanced To Improve Reconstruction (PaLEnTIR). (arXiv:2204.09815v2 [math.NA] UPDATED)","link":"http://arxiv.org/abs/2204.09815","description":"<p>In this paper, we consider the restoration and reconstruction of piecewise\nconstant objects in two and three dimensions using PaLEnTIR, a significantly\nenhanced Parametric level set (PaLS) model relative to the current\nstate-of-the-art. The primary contribution of this paper is a new PaLS\nformulation which requires only a single level set function to recover a scene\nwith piecewise constant objects possessing multiple unknown contrasts. Our\nmodel offers distinct advantages over current approaches to the multi-contrast,\nmulti-object problem, all of which require multiple level sets and explicit\nestimation of the contrast magnitudes. Given upper and lower bounds on the\ncontrast, our approach is able to recover objects with any distribution of\ncontrasts and eliminates the need to know either the number of contrasts in a\ngiven scene or their values. We provide an iterative process for finding these\nspace-varying contrast limits. Relative to most PaLS methods which employ\nradial basis functions (RBFs), our model makes use of non-isotropic basis\nfunctions, thereby expanding the class of shapes that a PaLS model of a given\ncomplexity can approximate. Finally, PaLEnTIR improves the conditioning of the\nJacobian matrix required as part of the parameter identification process and\nconsequently accelerates the optimization methods by controlling the magnitude\nof the PaLS expansion coefficients, fixing the centers of the basis functions,\nand the uniqueness of parametric to image mappings provided by the new\nparameterization. We demonstrate the performance of the new approach using both\n2D and 3D variants of X-ray computed tomography, diffuse optical tomography\n(DOT), denoising, deconvolution problems. Application to experimental sparse CT\ndata and simulated data with different types of noise are performed to further\nvalidate the proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/math/1/au:+Ozsar_E/0/1/0/all/0/1\">Ege Ozsar</a>, <a href=\"http://arxiv.org/find/math/1/au:+Kilmer_M/0/1/0/all/0/1\">Misha Kilmer</a>, <a href=\"http://arxiv.org/find/math/1/au:+Miller_E/0/1/0/all/0/1\">Eric Miller</a>, <a href=\"http://arxiv.org/find/math/1/au:+Sturler_E/0/1/0/all/0/1\">Eric de Sturler</a>, <a href=\"http://arxiv.org/find/math/1/au:+Saibaba_A/0/1/0/all/0/1\">Arvind Saibaba</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MMRotate: A Rotated Object Detection Benchmark using PyTorch. (arXiv:2204.13317v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.13317","description":"<p>We present an open-source toolbox, named MMRotate, which provides a coherent\nalgorithm framework of training, inferring, and evaluation for the popular\nrotated object detection algorithm based on deep learning. MMRotate implements\n18 state-of-the-art algorithms and supports the three most frequently used\nangle definition methods. To facilitate future research and industrial\napplications of rotated object detection-related problems, we also provide a\nlarge number of trained models and detailed benchmarks to give insights into\nthe performance of rotated object detection. MMRotate is publicly released at\nhttps://github.com/open-mmlab/mmrotate.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yue Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xue Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Gefan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiabao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yanyi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Liping Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xue Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xingzhao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Junchi Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_C/0/1/0/all/0/1\">Chengqi Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kai Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Overview of Color Transfer and Style Transfer for Images and Videos. (arXiv:2204.13339v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.13339","description":"<p>Image or video appearance features (e.g., color, texture, tone, illumination,\nand so on) reflect one's visual perception and direct impression of an image or\nvideo. Given a source image (video) and a target image (video), the image\n(video) color transfer technique aims to process the color of the source image\nor video (note that the source image or video is also referred to the reference\nimage or video in some literature) to make it look like that of the target\nimage or video, i.e., transferring the appearance of the target image or video\nto that of the source image or video, which can thereby change one's perception\nof the source image or video. As an extension of color transfer, style transfer\nrefers to rendering the content of a target image or video in the style of an\nartist with either a style sample or a set of images through a style transfer\nmodel. As an emerging field, the study of style transfer has attracted the\nattention of a large number of researchers. After decades of development, it\nhas become a highly interdisciplinary research with a variety of artistic\nexpression styles can be achieved. This paper provides an overview of color\ntransfer and style transfer methods over the past years.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shiguang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reliable Label Correction is a Good Booster When Learning with Extremely Noisy Labels. (arXiv:2205.00186v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.00186","description":"<p>Learning with noisy labels has aroused much research interest since data\nannotations, especially for large-scale datasets, may be inevitably imperfect.\nRecent approaches resort to a semi-supervised learning problem by dividing\ntraining samples into clean and noisy sets. This paradigm, however, is prone to\nsignificant degeneration under heavy label noise, as the number of clean\nsamples is too small for conventional methods to behave well. In this paper, we\nintroduce a novel framework, termed as LC-Booster, to explicitly tackle\nlearning under extreme noise. The core idea of LC-Booster is to incorporate\nlabel correction into the sample selection, so that more purified samples,\nthrough the reliable label correction, can be utilized for training, thereby\nalleviating the confirmation bias. Experiments show that LC-Booster advances\nstate-of-the-art results on several noisy-label benchmarks, including CIFAR-10,\nCIFAR-100, Clothing1M and WebVision. Remarkably, under the extreme 90\\% noise\nratio, LC-Booster achieves 92.9\\% and 48.4\\% accuracy on CIFAR-10 and\nCIFAR-100, surpassing state-of-the-art methods by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1\">Xiangyu Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shuo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianfei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zheng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinchao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1\">Yang You</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D Object Detection with a Self-supervised Lidar Scene Flow Backbone. (arXiv:2205.00705v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.00705","description":"<p>State-of-the-art lidar-based 3D object detection methods rely on supervised\nlearning and large labeled datasets. However, annotating lidar data is\nresource-consuming, and depending only on supervised learning limits the\napplicability of trained models. Self-supervised training strategies can\nalleviate these issues by learning a general point cloud backbone model for\ndownstream 3D vision tasks. Against this backdrop, we show the relationship\nbetween self-supervised multi-frame flow representations and single-frame 3D\ndetection hypotheses. Our main contribution leverages learned flow and motion\nrepresentations and combines a self-supervised backbone with a supervised 3D\ndetection head. First, a self-supervised scene flow estimation model is trained\nwith cycle consistency. Then, the point cloud encoder of this model is used as\nthe backbone of a single-frame 3D object detection head model. This second 3D\nobject detection model learns to utilize motion representations to distinguish\ndynamic objects exhibiting different movement patterns. Experiments on KITTI\nand nuScenes benchmarks show that the proposed self-supervised pre-training\nincreases 3D detection performance significantly.\nhttps://github.com/emecercelik/ssl-3d-detection.git\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yurtsever_E/0/1/0/all/0/1\">Ekim Yurtsever</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ercelik_E/0/1/0/all/0/1\">Eme&#xe7; Er&#xe7;elik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Mingyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhijie Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hanzhen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Topcam_P/0/1/0/all/0/1\">P&#x131;nar Top&#xe7;am</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Listl_M/0/1/0/all/0/1\">Maximilian Listl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cayli_Y/0/1/0/all/0/1\">Y&#x131;lmaz Kaan &#xc7;ayl&#x131;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knoll_A/0/1/0/all/0/1\">Alois Knoll</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Causal Balancing for Domain Generalization. (arXiv:2206.05263v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2206.05263","description":"<p>While machine learning models rapidly advance the state-of-the-art on various\nreal-world tasks, out-of-domain (OOD) generalization remains a challenging\nproblem given the vulnerability of these models to spurious correlations. We\npropose a causally-motivated balanced mini-batch sampling strategy to transform\nthe observed train distribution to a balanced distribution that is free of\nspurious correlations. We argue that the Bayes optimal classifier trained on\nsuch balanced distribution is minimax optimal across a diverse enough\nenvironment space. We also provide an identifiability guarantee of the latent\nvariable model of the proposed underlying data generation process with\ninvariant causal mechanisms, by utilizing enough number of train environments.\nExperiments are conducted on three domain generalization datasets,\ndemonstrating empirically that our balanced mini-batch sampling strategy\nimproves the performance of four different established domain generalization\nmodel baselines compared to the random mini-batch sampling strategy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saxon_M/0/1/0/all/0/1\">Michael Saxon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiachen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongyang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Patch-level Representation Learning for Self-supervised Vision Transformers. (arXiv:2206.07990v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.07990","description":"<p>Recent self-supervised learning (SSL) methods have shown impressive results\nin learning visual representations from unlabeled images. This paper aims to\nimprove their performance further by utilizing the architectural advantages of\nthe underlying neural network, as the current state-of-the-art visual pretext\ntasks for SSL do not enjoy the benefit, i.e., they are architecture-agnostic.\nIn particular, we focus on Vision Transformers (ViTs), which have gained much\nattention recently as a better architectural choice, often outperforming\nconvolutional networks for various visual tasks. The unique characteristic of\nViT is that it takes a sequence of disjoint patches from an image and processes\npatch-level representations internally. Inspired by this, we design a simple\nyet effective visual pretext task, coined SelfPatch, for learning better\npatch-level representations. To be specific, we enforce invariance against each\npatch and its neighbors, i.e., each patch treats similar neighboring patches as\npositive samples. Consequently, training ViTs with SelfPatch learns more\nsemantically meaningful relations among patches (without using human-annotated\nlabels), which can be beneficial, in particular, to downstream tasks of a dense\nprediction type. Despite its simplicity, we demonstrate that it can\nsignificantly improve the performance of existing SSL methods for various\nvisual tasks, including object detection and semantic segmentation.\nSpecifically, SelfPatch significantly improves the recent self-supervised ViT,\nDINO, by achieving +1.3 AP on COCO object detection, +1.2 AP on COCO instance\nsegmentation, and +2.9 mIoU on ADE20K semantic segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1\">Sukmin Yun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hankook Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jaehyung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1\">Jinwoo Shin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-end Learning for Image-based Detection of Molecular Alterations in Digital Pathology. (arXiv:2207.00095v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.00095","description":"<p>Current approaches for classification of whole slide images (WSI) in digital\npathology predominantly utilize a two-stage learning pipeline. The first stage\nidentifies areas of interest (e.g. tumor tissue), while the second stage\nprocesses cropped tiles from these areas in a supervised fashion. During\ninference, a large number of tiles are combined into a unified prediction for\nthe entire slide. A major drawback of such approaches is the requirement for\ntask-specific auxiliary labels which are not acquired in clinical routine. We\npropose a novel learning pipeline for WSI classification that is trainable\nend-to-end and does not require any auxiliary annotations. We apply our\napproach to predict molecular alterations for a number of different use-cases,\nincluding detection of microsatellite instability in colorectal tumors and\nprediction of specific mutations for colon, lung, and breast cancer cases from\nThe Cancer Genome Atlas. Results reach AUC scores of up to 94% and are shown to\nbe competitive with state of the art two-stage pipelines. We believe our\napproach can facilitate future research in digital pathology and contribute to\nsolve a large range of problems around the prediction of cancer phenotypes,\nhopefully enabling personalized therapies for more patients in future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Teichmann_M/0/1/0/all/0/1\">Marvin Teichmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aichert_A/0/1/0/all/0/1\">Andre Aichert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bohnenberger_H/0/1/0/all/0/1\">Hanibal Bohnenberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strobel_P/0/1/0/all/0/1\">Philipp Str&#xf6;bel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heimann_T/0/1/0/all/0/1\">Tobias Heimann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LaTeRF: Label and Text Driven Object Radiance Fields. (arXiv:2207.01583v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.01583","description":"<p>Obtaining 3D object representations is important for creating photo-realistic\nsimulations and for collecting AR and VR assets. Neural fields have shown their\neffectiveness in learning a continuous volumetric representation of a scene\nfrom 2D images, but acquiring object representations from these models with\nweak supervision remains an open challenge. In this paper we introduce LaTeRF,\na method for extracting an object of interest from a scene given 2D images of\nthe entire scene, known camera poses, a natural language description of the\nobject, and a set of point-labels of object and non-object points in the input\nimages. To faithfully extract the object from the scene, LaTeRF extends the\nNeRF formulation with an additional `objectness' probability at each 3D point.\nAdditionally, we leverage the rich latent space of a pre-trained CLIP model\ncombined with our differentiable object renderer, to inpaint the occluded parts\nof the object. We demonstrate high-fidelity object extraction on both synthetic\nand real-world datasets and justify our design choices through an extensive\nablation study.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mirzaei_A/0/1/0/all/0/1\">Ashkan Mirzaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kant_Y/0/1/0/all/0/1\">Yash Kant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kelly_J/0/1/0/all/0/1\">Jonathan Kelly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gilitschenski_I/0/1/0/all/0/1\">Igor Gilitschenski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Accelerating Score-based Generative Models with Preconditioned Diffusion Sampling. (arXiv:2207.02196v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.02196","description":"<p>Score-based generative models (SGMs) have recently emerged as a promising\nclass of generative models. However, a fundamental limitation is that their\ninference is very slow due to a need for many (e.g., 2000) iterations of\nsequential computations. An intuitive acceleration method is to reduce the\nsampling iterations which however causes severe performance degradation. We\ninvestigate this problem by viewing the diffusion sampling process as a\nMetropolis adjusted Langevin algorithm, which helps reveal the underlying cause\nto be ill-conditioned curvature. Under this insight, we propose a\nmodel-agnostic preconditioned diffusion sampling (PDS) method that leverages\nmatrix preconditioning to alleviate the aforementioned problem. Crucially, PDS\nis proven theoretically to converge to the original target distribution of a\nSGM, no need for retraining. Extensive experiments on three image datasets with\na variety of resolutions and diversity validate that PDS consistently\naccelerates off-the-shelf SGMs whilst maintaining the synthesis quality. In\nparticular, PDS can accelerate by up to 29x on more challenging high resolution\n(1024x1024) image generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1\">Hengyuan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiatian Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jianfeng Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dense Teacher: Dense Pseudo-Labels for Semi-supervised Object Detection. (arXiv:2207.02541v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.02541","description":"<p>To date, the most powerful semi-supervised object detectors (SS-OD) are based\non pseudo-boxes, which need a sequence of post-processing with fine-tuned\nhyper-parameters. In this work, we propose replacing the sparse pseudo-boxes\nwith the dense prediction as a united and straightforward form of pseudo-label.\nCompared to the pseudo-boxes, our Dense Pseudo-Label (DPL) does not involve any\npost-processing method, thus retaining richer information. We also introduce a\nregion selection technique to highlight the key information while suppressing\nthe noise carried by dense labels. We name our proposed SS-OD algorithm that\nleverages the DPL as Dense Teacher. On COCO and VOC, Dense Teacher shows\nsuperior performance under various settings compared with the pseudo-box-based\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hongyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Z/0/1/0/all/0/1\">Zheng Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Songtao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_W/0/1/0/all/0/1\">Weixin Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zeming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Haiyan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CCPL: Contrastive Coherence Preserving Loss for Versatile Style Transfer. (arXiv:2207.04808v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.04808","description":"<p>In this paper, we aim to devise a universally versatile style transfer method\ncapable of performing artistic, photo-realistic, and video style transfer\njointly, without seeing videos during training. Previous single-frame methods\nassume a strong constraint on the whole image to maintain temporal consistency,\nwhich could be violated in many cases. Instead, we make a mild and reasonable\nassumption that global inconsistency is dominated by local inconsistencies and\ndevise a generic Contrastive Coherence Preserving Loss (CCPL) applied to local\npatches. CCPL can preserve the coherence of the content source during style\ntransfer without degrading stylization. Moreover, it owns a neighbor-regulating\nmechanism, resulting in a vast reduction of local distortions and considerable\nvisual quality improvement. Aside from its superior performance on versatile\nstyle transfer, it can be easily extended to other tasks, such as\nimage-to-image translation. Besides, to better fuse content and style features,\nwe propose Simple Covariance Transformation (SCT) to effectively align\nsecond-order statistics of the content feature with the style feature.\nExperiments demonstrate the effectiveness of the resulting model for versatile\nstyle transfer, when armed with CCPL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zijie Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1\">Junping Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xiang Bai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Compound Prototype Matching for Few-shot Action Recognition. (arXiv:2207.05515v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.05515","description":"<p>Few-shot action recognition aims to recognize novel action classes using only\na small number of labeled training samples. In this work, we propose a novel\napproach that first summarizes each video into compound prototypes consisting\nof a group of global prototypes and a group of focused prototypes, and then\ncompares video similarity based on the prototypes. Each global prototype is\nencouraged to summarize a specific aspect from the entire video, for example,\nthe start/evolution of the action. Since no clear annotation is provided for\nthe global prototypes, we use a group of focused prototypes to focus on certain\ntimestamps in the video. We compare video similarity by matching the compound\nprototypes between the support and query videos. The global prototypes are\ndirectly matched to compare videos from the same perspective, for example, to\ncompare whether two actions start similarly. For the focused prototypes, since\nactions have various temporal variations in the videos, we apply bipartite\nmatching to allow the comparison of actions with different temporal positions\nand shifts. Experiments demonstrate that our proposed method achieves\nstate-of-the-art results on multiple benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lijin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yifei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sato_Y/0/1/0/all/0/1\">Yoichi Sato</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Structure PLP-SLAM: Efficient Sparse Mapping and Localization using Point, Line and Plane for Monocular, RGB-D and Stereo Cameras. (arXiv:2207.06058v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.06058","description":"<p>This paper demonstrates a visual SLAM system that utilizes point and line\ncloud for robust camera localization, simultaneously, with an embedded\npiece-wise planar reconstruction (PPR) module which in all provides a\nstructural map. To build a scale consistent map in parallel with tracking, such\nas employing a single camera brings the challenge of reconstructing geometric\nprimitives with scale ambiguity, and further introduces the difficulty in graph\noptimization of bundle adjustment (BA). We address these problems by proposing\nseveral run-time optimizations on the reconstructed lines and planes. The\nsystem is then extended with depth and stereo sensors based on the design of\nthe monocular framework. The results show that our proposed SLAM tightly\nincorporates the semantic features to boost both frontend tracking as well as\nbackend optimization. We evaluate our system exhaustively on various datasets,\nand open-source our code for the community\n(https://github.com/PeterFWS/Structure-PLP-SLAM).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shu_F/0/1/0/all/0/1\">Fangwen Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiaxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pagani_A/0/1/0/all/0/1\">Alain Pagani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stricker_D/0/1/0/all/0/1\">Didier Stricker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Privacy-Preserving Face Recognition with Learnable Privacy Budgets in Frequency Domain. (arXiv:2207.07316v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.07316","description":"<p>Face recognition technology has been used in many fields due to its high\nrecognition accuracy, including the face unlocking of mobile devices, community\naccess control systems, and city surveillance. As the current high accuracy is\nguaranteed by very deep network structures, facial images often need to be\ntransmitted to third-party servers with high computational power for inference.\nHowever, facial images visually reveal the user's identity information. In this\nprocess, both untrusted service providers and malicious users can significantly\nincrease the risk of a personal privacy breach. Current privacy-preserving\napproaches to face recognition are often accompanied by many side effects, such\nas a significant increase in inference time or a noticeable decrease in\nrecognition accuracy. This paper proposes a privacy-preserving face recognition\nmethod using differential privacy in the frequency domain. Due to the\nutilization of differential privacy, it offers a guarantee of privacy in\ntheory. Meanwhile, the loss of accuracy is very slight. This method first\nconverts the original image to the frequency domain and removes the direct\ncomponent termed DC. Then a privacy budget allocation method can be learned\nbased on the loss of the back-end face recognition network within the\ndifferential privacy framework. Finally, it adds the corresponding noise to the\nfrequency domain features. Our method performs very well with several classical\nface recognition test sets according to the extensive experiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ji_J/0/1/0/all/0/1\">Jiazhen Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yuge Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiaxiang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xingkun Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1\">Shouhong Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">ShengChuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1\">Liujuan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Long-Term Spatial-Temporal Graphs for Active Speaker Detection. (arXiv:2207.07783v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.07783","description":"<p>Active speaker detection (ASD) in videos with multiple speakers is a\nchallenging task as it requires learning effective audiovisual features and\nspatial-temporal correlations over long temporal windows. In this paper, we\npresent SPELL, a novel spatial-temporal graph learning framework that can solve\ncomplex tasks such as ASD. To this end, each person in a video frame is first\nencoded in a unique node for that frame. Nodes corresponding to a single person\nacross frames are connected to encode their temporal dynamics. Nodes within a\nframe are also connected to encode inter-person relationships. Thus, SPELL\nreduces ASD to a node classification task. Importantly, SPELL is able to reason\nover long temporal contexts for all nodes without relying on computationally\nexpensive fully connected graph neural networks. Through extensive experiments\non the AVA-ActiveSpeaker dataset, we demonstrate that learning graph-based\nrepresentations can significantly improve the active speaker detection\nperformance owing to its explicit spatial and temporal structure. SPELL\noutperforms all previous state-of-the-art approaches while requiring\nsignificantly lower memory and computational resources. Our code is publicly\navailable at https://github.com/SRA2/SPELL\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Min_K/0/1/0/all/0/1\">Kyle Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1\">Sourya Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tripathi_S/0/1/0/all/0/1\">Subarna Tripathi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guha_T/0/1/0/all/0/1\">Tanaya Guha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majumdar_S/0/1/0/all/0/1\">Somdeb Majumdar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RCRN: Real-world Character Image Restoration Network via Skeleton Extraction. (arXiv:2207.07795v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.07795","description":"<p>Constructing high-quality character image datasets is challenging because\nreal-world images are often affected by image degradation. There are\nlimitations when applying current image restoration methods to such real-world\ncharacter images, since (i) the categories of noise in character images are\ndifferent from those in general images; (ii) real-world character images\nusually contain more complex image degradation, e.g., mixed noise at different\nnoise levels. To address these problems, we propose a real-world character\nrestoration network (RCRN) to effectively restore degraded character images,\nwhere character skeleton information and scale-ensemble feature extraction are\nutilized to obtain better restoration performance. The proposed method consists\nof a skeleton extractor (SENet) and a character image restorer (CiRNet). SENet\naims to preserve the structural consistency of the character and normalize\ncomplex noise. Then, CiRNet reconstructs clean images from degraded character\nimages and their skeletons. Due to the lack of benchmarks for real-world\ncharacter image restoration, we constructed a dataset containing 1,606\ncharacter images with real-world degradation to evaluate the validity of the\nproposed method. The experimental results demonstrate that RCRN outperforms\nstate-of-the-art methods quantitatively and qualitatively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_D/0/1/0/all/0/1\">Daqian Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diao_X/0/1/0/all/0/1\">Xiaolei Diao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaomin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_H/0/1/0/all/0/1\">Hao Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hao Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CharFormer: A Glyph Fusion based Attentive Framework for High-precision Character Image Denoising. (arXiv:2207.07798v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.07798","description":"<p>Degraded images commonly exist in the general sources of character images,\nleading to unsatisfactory character recognition results. Existing methods have\ndedicated efforts to restoring degraded character images. However, the\ndenoising results obtained by these methods do not appear to improve character\nrecognition performance. This is mainly because current methods only focus on\npixel-level information and ignore critical features of a character, such as\nits glyph, resulting in character-glyph damage during the denoising process. In\nthis paper, we introduce a novel generic framework based on glyph fusion and\nattention mechanisms, i.e., CharFormer, for precisely recovering character\nimages without changing their inherent glyphs. Unlike existing frameworks,\nCharFormer introduces a parallel target task for capturing additional\ninformation and injecting it into the image denoising backbone, which will\nmaintain the consistency of character glyphs during character image denoising.\nMoreover, we utilize attention-based networks for global-local feature\ninteraction, which will help to deal with blind denoising and enhance denoising\nperformance. We compare CharFormer with state-of-the-art methods on multiple\ndatasets. The experimental results show the superiority of CharFormer\nquantitatively and qualitatively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_D/0/1/0/all/0/1\">Daqian Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diao_X/0/1/0/all/0/1\">Xiaolei Diao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_L/0/1/0/all/0/1\">Lida Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chi_Y/0/1/0/all/0/1\">Yang Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chuntao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hao Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Structural Prior Guided Generative Adversarial Transformers for Low-Light Image Enhancement. (arXiv:2207.07828v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.07828","description":"<p>We propose an effective Structural Prior guided Generative Adversarial\nTransformer (SPGAT) to solve low-light image enhancement. Our SPGAT mainly\ncontains a generator with two discriminators and a structural prior estimator\n(SPE). The generator is based on a U-shaped Transformer which is used to\nexplore non-local information for better clear image restoration. The SPE is\nused to explore useful structures from images to guide the generator for better\nstructural detail estimation. To generate more realistic images, we develop a\nnew structural prior guided adversarial learning method by building the skip\nconnections between the generator and discriminators so that the discriminators\ncan better discriminate between real and fake features. Finally, we propose a\nparallel windows-based Swin Transformer block to aggregate different level\nhierarchical features for high-quality image restoration. Experimental results\ndemonstrate that the proposed SPGAT performs favorably against recent\nstate-of-the-art methods on both synthetic and real-world datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Cong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Jinshan Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiao-Ming Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast-MoCo: Boost Momentum-based Contrastive Learning with Combinatorial Patches. (arXiv:2207.08220v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.08220","description":"<p>Contrastive-based self-supervised learning methods achieved great success in\nrecent years. However, self-supervision requires extremely long training epochs\n(e.g., 800 epochs for MoCo v3) to achieve promising results, which is\nunacceptable for the general academic community and hinders the development of\nthis topic. This work revisits the momentum-based contrastive learning\nframeworks and identifies the inefficiency in which two augmented views\ngenerate only one positive pair. We propose Fast-MoCo - a novel framework that\nutilizes combinatorial patches to construct multiple positive pairs from two\naugmented views, which provides abundant supervision signals that bring\nsignificant acceleration with neglectable extra computational cost. Fast-MoCo\ntrained with 100 epochs achieves 73.5% linear evaluation accuracy, similar to\nMoCo v3 (ResNet-50 backbone) trained with 800 epochs. Extra training (200\nepochs) further improves the result to 75.1%, which is on par with\nstate-of-the-art methods. Experiments on several downstream tasks also confirm\nthe effectiveness of Fast-MoCo.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ci_Y/0/1/0/all/0/1\">Yuanzheng Ci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_L/0/1/0/all/0/1\">Lei Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1\">Wanli Ouyang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Show Me What I Like: Detecting User-Specific Video Highlights Using Content-Based Multi-Head Attention. (arXiv:2207.08352v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.08352","description":"<p>We propose a method to detect individualized highlights for users on given\ntarget videos based on their preferred highlight clips marked on previous\nvideos they have watched. Our method explicitly leverages the contents of both\nthe preferred clips and the target videos using pre-trained features for the\nobjects and the human activities. We design a multi-head attention mechanism to\nadaptively weigh the preferred clips based on their object- and\nhuman-activity-based contents, and fuse them using these weights into a single\nfeature representation for each user. We compute similarities between these\nper-user feature representations and the per-frame features computed from the\ndesired target videos to estimate the user-specific highlight clips from the\ntarget videos. We test our method on a large-scale highlight detection dataset\ncontaining the annotated highlights of individual users. Compared to current\nbaselines, we observe an absolute improvement of 2-4% in the mean average\nprecision of the detected highlights. We also perform extensive ablation\nexperiments on the number of preferred highlight clips associated with each\nuser as well as on the object- and human-activity-based feature representations\nto validate that our method is indeed both content-based and user-specific.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_U/0/1/0/all/0/1\">Uttaran Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1\">Gang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petrangeli_S/0/1/0/all/0/1\">Stefano Petrangeli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swaminathan_V/0/1/0/all/0/1\">Viswanathan Swaminathan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1\">Dinesh Manocha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Open-world Semantic Segmentation via Contrasting and Clustering Vision-Language Embedding. (arXiv:2207.08455v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.08455","description":"<p>To bridge the gap between supervised semantic segmentation and real-world\napplications that acquires one model to recognize arbitrary new concepts,\nrecent zero-shot segmentation attracts a lot of attention by exploring the\nrelationships between unseen and seen object categories, yet requiring large\namounts of densely-annotated data with diverse base classes. In this paper, we\npropose a new open-world semantic segmentation pipeline that makes the first\nattempt to learn to segment semantic objects of various open-world categories\nwithout any efforts on dense annotations, by purely exploiting the\nimage-caption data that naturally exist on the Internet. Our method,\nVision-language-driven Semantic Segmentation (ViL-Seg), employs an image and a\ntext encoder to generate visual and text embeddings for the image-caption data,\nwith two core components that endow its segmentation ability: First, the image\nencoder is jointly trained with a vision-based contrasting and a cross-modal\ncontrasting, which encourage the visual embeddings to preserve both\nfine-grained semantics and high-level category information that are crucial for\nthe segmentation task. Furthermore, an online clustering head is devised over\nthe image encoder, which allows to dynamically segment the visual embeddings\ninto distinct semantic groups such that they can be classified by comparing\nwith various text embeddings to complete our segmentation pipeline. Experiments\nshow that without using any data with dense annotations, our method can\ndirectly segment objects of arbitrary categories, outperforming zero-shot\nsegmentation methods that require data labeling on three benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Quande Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Youpeng Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jianhua Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chunjing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Segmenting white matter hyperintensities on isotropic three-dimensional Fluid Attenuated Inversion Recovery magnetic resonance images: A comparison of Deep learning tools on a Norwegian national imaging database. (arXiv:2207.08467v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2207.08467","description":"<p>Automated segmentation of white matter hyperintensities (WMHs) is an\nessential step in neuroimaging analysis of Magnetic Resonance Imaging (MRI).\nFluid Attenuated Inversion Recovery (FLAIR-weighted) is an MRI contrast that is\nparticularly useful to visualize and quantify WMHs, a hallmark of cerebral\nsmall vessel disease and Alzheimer's disease (AD). Clinical MRI protocols\nmigrate to a three-dimensional (3D) FLAIR-weighted acquisition to enable high\nspatial resolution in all three voxel dimensions. The current study details the\ndeployment of deep learning tools to enable automated WMH segmentation and\ncharacterization from 3D FLAIR-weighted images acquired as part of a national\nAD imaging initiative.\n</p>\n<p>Among 642 participants (283 male, mean age: (65.18 +/- 9.33) years) from the\nDDI study, two in-house networks were trained and validated across five\nnational collection sites. Three models were tested on a held-out subset of the\ninternal data from the 642 participants and an external dataset with 29 cases\nfrom an international collaborator. These test sets were evaluated\nindependently. Five established WMH performance metrics were used for\ncomparison against ground truth human-in-the-loop segmentation.\n</p>\n<p>Results of the three networks tested, the 3D nnU-Net had the best performance\nwith an average dice similarity coefficient score of 0.78 +/- 0.10, performing\nbetter than both the in-house developed 2.5D model and the SOTA Deep Bayesian\nnetwork.\n</p>\n<p>With the increasing use of 3D FLAIR-weighted images in MRI protocols, our\nresults suggest that WMH segmentation models can be trained on 3D data and\nyield WMH segmentation performance that is comparable to or better than\nstate-of-the-art without the need for including T1-weighted image series.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Roevang_M/0/1/0/all/0/1\">Martin Soria Roevang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Selnes_P/0/1/0/all/0/1\">Per Selnes</a>, <a href=\"http://arxiv.org/find/eess/1/au:+MacIntosh_B/0/1/0/all/0/1\">Bradley John MacIntosh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Groote_I/0/1/0/all/0/1\">Inge Rasmus Groote</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Paalhaugen_L/0/1/0/all/0/1\">Lene Paalhaugen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sudre_C/0/1/0/all/0/1\">Carole Sudre</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fladby_T/0/1/0/all/0/1\">Tormod Fladby</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bjoernerud_A/0/1/0/all/0/1\">Atle Bjoernerud</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical Feature Alignment Network for Unsupervised Video Object Segmentation. (arXiv:2207.08485v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.08485","description":"<p>Optical flow is an easily conceived and precious cue for advancing\nunsupervised video object segmentation (UVOS). Most of the previous methods\ndirectly extract and fuse the motion and appearance features for segmenting\ntarget objects in the UVOS setting. However, optical flow is intrinsically an\ninstantaneous velocity of all pixels among consecutive frames, thus making the\nmotion features not aligned well with the primary objects among the\ncorresponding frames. To solve the above challenge, we propose a concise,\npractical, and efficient architecture for appearance and motion feature\nalignment, dubbed hierarchical feature alignment network (HFAN). Specifically,\nthe key merits in HFAN are the sequential Feature AlignMent (FAM) module and\nthe Feature AdaptaTion (FAT) module, which are leveraged for processing the\nappearance and motion features hierarchically. FAM is capable of aligning both\nappearance and motion features with the primary object semantic\nrepresentations, respectively. Further, FAT is explicitly designed for the\nadaptive fusion of appearance and motion features to achieve a desirable\ntrade-off between cross-modal features. Extensive experiments demonstrate the\neffectiveness of the proposed HFAN, which reaches a new state-of-the-art\nperformance on DAVIS-16, achieving 88.7 $\\mathcal{J}\\&amp;\\mathcal{F}$ Mean, i.e.,\na relative improvement of 3.5% over the best published result.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pei_G/0/1/0/all/0/1\">Gensheng Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_F/0/1/0/all/0/1\">Fumin Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yazhou Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_G/0/1/0/all/0/1\">Guo-Sen Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1\">Zhenmin Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jinhui Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Study of the performance and scalability of federated learning for medical imaging with intermittent clients. (arXiv:2207.08581v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2207.08581","description":"<p>Federated learning is a data decentralization privacy-preserving technique\nused to perform machine or deep learning in a secure way. In this paper we\npresent theoretical aspects about federated learning, such as the presentation\nof an aggregation operator, different types of federated learning, and issues\nto be taken into account in relation to the distribution of data from the\nclients, together with the exhaustive analysis of a use case where the number\nof clients varies. Specifically, a use case of medical image analysis is\nproposed, using chest X-ray images obtained from an open data repository. In\naddition to the advantages related to privacy, improvements in predictions (in\nterms of accuracy and area under the curve) and reduction of execution times\nwill be studied with respect to the classical case (the centralized approach).\nDifferent clients will be simulated from the training data, selected in an\nunbalanced manner, i.e., they do not all have the same number of data. The\nresults of considering three or ten clients are exposed and compared between\nthem and against the centralized case. Two approaches to follow will be\nanalyzed in the case of intermittent clients, as in a real scenario some\nclients may leave the training, and some new ones may enter the training. The\nevolution of the results for the test set in terms of accuracy, area under the\ncurve and execution time is shown as the number of clients into which the\noriginal data is divided increases. Finally, improvements and future work in\nthe field are proposed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Diaz_J/0/1/0/all/0/1\">Judith S&#xe1;inz-Pardo D&#xed;az</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_A/0/1/0/all/0/1\">&#xc1;lvaro L&#xf3;pez Garc&#xed;a</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FakeCLR: Exploring Contrastive Learning for Solving Latent Discontinuity in Data-Efficient GANs. (arXiv:2207.08630v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.08630","description":"<p>Data-Efficient GANs (DE-GANs), which aim to learn generative models with a\nlimited amount of training data, encounter several challenges for generating\nhigh-quality samples. Since data augmentation strategies have largely\nalleviated the training instability, how to further improve the generative\nperformance of DE-GANs becomes a hotspot. Recently, contrastive learning has\nshown the great potential of increasing the synthesis quality of DE-GANs, yet\nrelated principles are not well explored. In this paper, we revisit and compare\ndifferent contrastive learning strategies in DE-GANs, and identify (i) the\ncurrent bottleneck of generative performance is the discontinuity of latent\nspace; (ii) compared to other contrastive learning strategies,\nInstance-perturbation works towards latent space continuity, which brings the\nmajor improvement to DE-GANs. Based on these observations, we propose FakeCLR,\nwhich only applies contrastive learning on perturbed fake samples, and devises\nthree related training techniques: Noise-related Latent Augmentation,\nDiversity-aware Queue, and Forgetting Factor of Queue. Our experimental results\nmanifest the new state of the arts on both few-shot generation and limited-data\ngeneration. On multiple datasets, FakeCLR acquires more than 15% FID\nimprovement compared to existing DE-GANs. Code is available at\nhttps://github.com/iceli1007/FakeCLR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Ziqiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chaoyue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Heliang Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-07-19T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/"}}]}]}