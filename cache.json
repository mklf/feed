{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-05-02T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"HiNER: A Large Hindi Named Entity Recognition Dataset. (arXiv:2204.13743v1 [cs.CL])","link":"http://arxiv.org/abs/2204.13743","description":"<p>Named Entity Recognition (NER) is a foundational NLP task that aims to\nprovide class labels like Person, Location, Organisation, Time, and Number to\nwords in free text. Named Entities can also be multi-word expressions where the\nadditional I-O-B annotation information helps label them during the NER\nannotation process. While English and European languages have considerable\nannotated data for the NER task, Indian languages lack on that front -- both in\nterms of quantity and following annotation standards. This paper releases a\nsignificantly sized standard-abiding Hindi NER dataset containing 109,146\nsentences and 2,220,856 tokens, annotated with 11 tags. We discuss the dataset\nstatistics in all their essential detail and provide an in-depth analysis of\nthe NER tag-set used with our data. The statistics of tag-set in our dataset\nshow a healthy per-tag distribution, especially for prominent classes like\nPerson, Location and Organisation. Since the proof of resource-effectiveness is\nin building models with the resource and testing the model on benchmark data\nand against the leader-board entries in shared tasks, we do the same with the\naforesaid data. We use different language models to perform the sequence\nlabelling task for NER and show the efficacy of our data by performing a\ncomparative evaluation with models trained on another dataset available for the\nHindi NER task. Our dataset helps achieve a weighted F1 score of 88.78 with all\nthe tags and 92.22 when we collapse the tag-set, as discussed in the paper. To\nthe best of our knowledge, no available dataset meets the standards of volume\n(amount) and variability (diversity), as far as Hindi NER is concerned. We fill\nthis gap through this work, which we hope will significantly help NLP for\nHindi. We release this dataset with our code and models at\nhttps://github.com/cfiltnlp/HiNER\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Murthy_R/0/1/0/all/0/1\">Rudra Murthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharjee_P/0/1/0/all/0/1\">Pallab Bhattacharjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharnagat_R/0/1/0/all/0/1\">Rahul Sharnagat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khatri_J/0/1/0/all/0/1\">Jyotsana Khatri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanojia_D/0/1/0/all/0/1\">Diptesh Kanojia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharyya_P/0/1/0/all/0/1\">Pushpak Bhattacharyya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CAVES: A Dataset to facilitate Explainable Classification and Summarization of Concerns towards COVID Vaccines. (arXiv:2204.13746v1 [cs.CL])","link":"http://arxiv.org/abs/2204.13746","description":"<p>Convincing people to get vaccinated against COVID-19 is a key societal\nchallenge in the present times. As a first step towards this goal, many prior\nworks have relied on social media analysis to understand the specific concerns\nthat people have towards these vaccines, such as potential side-effects,\nineffectiveness, political factors, and so on. Though there are datasets that\nbroadly classify social media posts into Anti-vax and Pro-Vax labels, there is\nno dataset (to our knowledge) that labels social media posts according to the\nspecific anti-vaccine concerns mentioned in the posts. In this paper, we have\ncurated CAVES, the first large-scale dataset containing about 10k COVID-19\nanti-vaccine tweets labelled into various specific anti-vaccine concerns in a\nmulti-label setting. This is also the first multi-label classification dataset\nthat provides explanations for each of the labels. Additionally, the dataset\nalso provides class-wise summaries of all the tweets. We also perform\npreliminary experiments on the dataset and show that this is a very challenging\ndataset for multi-label explainable classification and tweet summarization, as\nis evident by the moderate scores achieved by some state-of-the-art models. Our\ndataset and codes are available at: https://github.com/sohampoddar26/caves-data\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Poddar_S/0/1/0/all/0/1\">Soham Poddar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samad_A/0/1/0/all/0/1\">Azlaan Mustafa Samad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_R/0/1/0/all/0/1\">Rajdeep Mukherjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganguly_N/0/1/0/all/0/1\">Niloy Ganguly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Saptarshi Ghosh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Split for Automatic Bias Detection. (arXiv:2204.13749v1 [cs.LG])","link":"http://arxiv.org/abs/2204.13749","description":"<p>Classifiers are biased when trained on biased datasets. As a remedy, we\npropose Learning to Split (ls), an algorithm for automatic bias detection.\nGiven a dataset with input-label pairs, ls learns to split this dataset so that\npredictors trained on the training split generalize poorly to the testing\nsplit. This performance gap provides a proxy for measuring the degree of bias\nin the learned features and can therefore be used to reduce biases. Identifying\nnon-generalizable splits is challenging as we don't have any explicit\nannotations about how to split. In this work, we show that the prediction\ncorrectness of the testing example can be used as a source of weak supervision:\ngeneralization performance will drop if we move examples that are predicted\ncorrectly away from the testing split, leaving only those that are\nmispredicted. We evaluate our approach on Beer Review, Waterbirds, CelebA and\nMNLI. Empirical results show that ls is able to generate astonishingly\nchallenging splits that correlate with human-identified biases. Moreover, we\ndemonstrate that combining robust learning algorithms (such as group DRO) with\nsplits identified by ls enables automatic de-biasing. Compared with previous\nstate-of-the-arts, we substantially improves the worst-group performance (23.4%\non average) when the source of biases is unknown during training and\nvalidation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bao_Y/0/1/0/all/0/1\">Yujia Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barzilay_R/0/1/0/all/0/1\">Regina Barzilay</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Faithful to the Document or to the World? Mitigating Hallucinations via Entity-linked Knowledge in Abstractive Summarization. (arXiv:2204.13761v1 [cs.CL])","link":"http://arxiv.org/abs/2204.13761","description":"<p>Despite recent advances in abstractive summarization, current summarization\nsystems still suffer from content hallucinations where models generate text\nthat is either irrelevant or contradictory to the source document. However,\nprior work has been predicated on the assumption that any generated facts not\nappearing explicitly in the source are undesired hallucinations. Methods have\nbeen proposed to address this scenario by ultimately improving `faithfulness'\nto the source document, but in reality, there is a large portion of entities in\nthe gold reference targets that are not directly in the source. In this work,\nwe show that these entities are not aberrations, but they instead require\nutilizing external world knowledge to infer reasoning paths from entities in\nthe source. We show that by utilizing an external knowledge base, we can\nimprove the faithfulness of summaries without simply making them more\nextractive, and additionally, we show that external knowledge bases linked from\nthe source can benefit the factuality of generated summaries.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yue Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wieting_J/0/1/0/all/0/1\">John Wieting</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verga_P/0/1/0/all/0/1\">Pat Verga</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inferring Implicit Relations with Language Models. (arXiv:2204.13778v1 [cs.CL])","link":"http://arxiv.org/abs/2204.13778","description":"<p>A prominent challenge for modern language understanding systems is the\nability to answer implicit reasoning questions, where the required reasoning\nsteps for answering the question are not mentioned in the text explicitly. In\nthis work, we investigate why current models struggle with implicit reasoning\nquestion answering (QA) tasks, by decoupling inference of reasoning steps from\ntheir execution. We define a new task of implicit relation inference and\nconstruct a benchmark, IMPLICITRELATIONS, where given a question, a model\nshould output a list of concept-relation pairs, where the relations describe\nthe implicit reasoning steps required for answering the question. Using\nIMPLICITRELATIONS, we evaluate models from the GPT-3 family and find that,\nwhile these models struggle on the implicit reasoning QA task, they often\nsucceed at inferring implicit relations. This suggests that the bottleneck for\nanswering implicit reasoning questions is in the ability of language models to\nretrieve and reason over information rather than to plan an accurate reasoning\nstrategy\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Katz_U/0/1/0/all/0/1\">Uri Katz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geva_M/0/1/0/all/0/1\">Mor Geva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berant_J/0/1/0/all/0/1\">Jonathan Berant</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Instilling Type Knowledge in Language Models via Multi-Task QA. (arXiv:2204.13796v1 [cs.CL])","link":"http://arxiv.org/abs/2204.13796","description":"<p>Understanding human language often necessitates understanding entities and\ntheir place in a taxonomy of knowledge -- their types. Previous methods to\nlearn entity types rely on training classifiers on datasets with coarse, noisy,\nand incomplete labels. We introduce a method to instill fine-grained type\nknowledge in language models with text-to-text pre-training on type-centric\nquestions leveraging knowledge base documents and knowledge graphs. We create\nthe WikiWiki dataset: entities and passages from 10M Wikipedia articles linked\nto the Wikidata knowledge graph with 41K types. Models trained on WikiWiki\nachieve state-of-the-art performance in zero-shot dialog state tracking\nbenchmarks, accurately infer entity types in Wikipedia articles, and can\ndiscover new types deemed useful by human judges.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sridhar_M/0/1/0/all/0/1\">Mukund Sridhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prakash_C/0/1/0/all/0/1\">Chandana Satya Prakash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Jin Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamza_W/0/1/0/all/0/1\">Wael Hamza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1\">Julian McAuley</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating writing style as a contributor to gender gaps in science and technology. (arXiv:2204.13805v1 [cs.CY])","link":"http://arxiv.org/abs/2204.13805","description":"<p>While universalism is a foundational principle of science, a growing stream\nof research finds that scientific contributions are evaluated differently\ndepending on the gender of the author, with women tending to receive fewer\ncitations relative to men, even for work of comparable quality. Strikingly,\nresearch also suggests that these gender gaps are visible even under blinded\nreview, wherein the evaluator is not aware of the gender of the author. In this\narticle, we consider whether gender differences in writing styles -- how men\nand women communicate their work -- may contribute to these observed gender\ngaps. We ground our investigation in a previously established framework for\ncharacterizing the linguistic style of written text, which distinguishes\nbetween two sets of features -- informational (i.e., features that emphasize\nfacts) and involved (i.e., features that emphasize relationships). Using a\nlarge, matched sample of academic papers and patents, we find significant\ndifferences in writing style by gender; women use more involved features in\ntheir writing, a pattern that holds universally across fields. The magnitude of\nthe effect varies across fields, with larger gender differences observed in the\nsocial sciences and arts humanities and smaller gaps in the physical sciences\nand technology. Subsequently, we show that gender differences in writing style\nmay have parallels in reading preferences; papers and patents with more\ninformational features tend to be cited more by men, while those with more\ninvolved features tend to be cited more by women, even after controlling for\nthe gender of the author, inventor, and patent attorney. Our findings suggest\nthat formal written text is not devoid of personal character, which could\ncontribute to bias in evaluation, thereby compromising the norm of\nuniversalism.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Levitskaya_E/0/1/0/all/0/1\">Ekaterina Levitskaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kedrick_K/0/1/0/all/0/1\">Kara Kedrick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Funk_R/0/1/0/all/0/1\">Russell J. Funk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Repro: An Open-Source Library for Improving the Reproducibility and Usability of Publicly Available Research Code. (arXiv:2204.13848v1 [cs.CL])","link":"http://arxiv.org/abs/2204.13848","description":"<p>We introduce Repro, an open-source library which aims at improving the\nreproducibility and usability of research code. The library provides a\nlightweight Python API for running software released by researchers within\nDocker containers which contain the exact required runtime configuration and\ndependencies for the code. Because the environment setup for each package is\nhandled by Docker, users do not have to do any configuration themselves. Once\nRepro is installed, users can run the code for the 30+ papers currently\nsupported by the library. We hope researchers see the value provided to others\nby including their research code in Repro and consider adding support for their\nown research code.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deutsch_D/0/1/0/all/0/1\">Daniel Deutsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1\">Dan Roth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Textual Adversarial Examples Based on Distributional Characteristics of Data Representations. (arXiv:2204.13853v1 [cs.CL])","link":"http://arxiv.org/abs/2204.13853","description":"<p>Although deep neural networks have achieved state-of-the-art performance in\nvarious machine learning tasks, adversarial examples, constructed by adding\nsmall non-random perturbations to correctly classified inputs, successfully\nfool highly expressive deep classifiers into incorrect predictions. Approaches\nto adversarial attacks in natural language tasks have boomed in the last five\nyears using character-level, word-level, phrase-level, or sentence-level\ntextual perturbations. While there is some work in NLP on defending against\nsuch attacks through proactive methods, like adversarial training, there is to\nour knowledge no effective general reactive approaches to defence via detection\nof textual adversarial examples such as is found in the image processing\nliterature. In this paper, we propose two new reactive methods for NLP to fill\nthis gap, which unlike the few limited application baselines from NLP are based\nentirely on distribution characteristics of learned representations: we adapt\none from the image processing literature (Local Intrinsic Dimensionality\n(LID)), and propose a novel one (MultiDistance Representation Ensemble Method\n(MDRE)). Adapted LID and MDRE obtain state-of-the-art results on\ncharacter-level, word-level, and phrase-level attacks on the IMDB dataset as\nwell as on the later two with respect to the MultiNLI dataset. For future\nresearch, we publish our code.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1\">Na Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dras_M/0/1/0/all/0/1\">Mark Dras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Emma Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision-Language Pre-Training for Boosting Scene Text Detectors. (arXiv:2204.13867v1 [cs.CV])","link":"http://arxiv.org/abs/2204.13867","description":"<p>Recently, vision-language joint representation learning has proven to be\nhighly effective in various scenarios. In this paper, we specifically adapt\nvision-language joint learning for scene text detection, a task that\nintrinsically involves cross-modal interaction between the two modalities:\nvision and language, since text is the written form of language. Concretely, we\npropose to learn contextualized, joint representations through vision-language\npre-training, for the sake of enhancing the performance of scene text\ndetectors. Towards this end, we devise a pre-training architecture with an\nimage encoder, a text encoder and a cross-modal encoder, as well as three\npretext tasks: image-text contrastive learning (ITC), masked language modeling\n(MLM) and word-in-image prediction (WIP). The pre-trained model is able to\nproduce more informative representations with richer semantics, which could\nreadily benefit existing scene text detectors (such as EAST and PSENet) in the\ndown-stream text detection task. Extensive experiments on standard benchmarks\ndemonstrate that the proposed paradigm can significantly improve the\nperformance of various representative text detectors, outperforming previous\npre-training approaches. The code and pre-trained models will be publicly\nreleased.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Sibo Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_J/0/1/0/all/0/1\">Jianqiang Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhibo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jun Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_W/0/1/0/all/0/1\">Wenqing Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xiang Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_C/0/1/0/all/0/1\">Cong Yao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Por Qu\\'e N\\~ao Utiliser Alla Spr{\\aa}k? Mixed Training with Gradient Optimization in Few-Shot Cross-Lingual Transfer. (arXiv:2204.13869v1 [cs.CL])","link":"http://arxiv.org/abs/2204.13869","description":"<p>The current state-of-the-art for few-shot cross-lingual transfer learning\nfirst trains on abundant labeled data in the source language and then\nfine-tunes with a few examples on the target language, termed target-adapting.\nThough this has been demonstrated to work on a variety of tasks, in this paper\nwe show some deficiencies of this approach and propose a one-step mixed\ntraining method that trains on both source and target data with\n\\textit{stochastic gradient surgery}, a novel gradient-level optimization.\nUnlike the previous studies that focus on one language at a time when\ntarget-adapting, we use one model to handle all target languages simultaneously\nto avoid excessively language-specific models. Moreover, we discuss the\nunreality of utilizing large target development sets for model selection in\nprevious literature. We further show that our method is both development-free\nfor target languages, and is also able to escape from overfitting issues. We\nconduct a large-scale experiment on 4 diverse NLP tasks across up to 48\nlanguages. Our proposed method achieves state-of-the-art performance on all\ntasks and outperforms target-adapting by a large margin, especially for\nlanguages that are linguistically distant from the source language, e.g., 7.36%\nF1 absolute gain on average for the NER task, up to 17.60% on Punjabi.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Haoran Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murray_K/0/1/0/all/0/1\">Kenton Murray</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OA-Mine: Open-World Attribute Mining for E-Commerce Products with Weak Supervision. (arXiv:2204.13874v1 [cs.CL])","link":"http://arxiv.org/abs/2204.13874","description":"<p>Automatic extraction of product attributes from their textual descriptions is\nessential for online shopper experience. One inherent challenge of this task is\nthe emerging nature of e-commerce products -- we see new types of products with\ntheir unique set of new attributes constantly. Most prior works on this matter\nmine new values for a set of known attributes but cannot handle new attributes\nthat arose from constantly changing data. In this work, we study the attribute\nmining problem in an open-world setting to extract novel attributes and their\nvalues. Instead of providing comprehensive training data, the user only needs\nto provide a few examples for a few known attribute types as weak supervision.\nWe propose a principled framework that first generates attribute value\ncandidates and then groups them into clusters of attributes. The candidate\ngeneration step probes a pre-trained language model to extract phrases from\nproduct titles. Then, an attribute-aware fine-tuning method optimizes a\nmultitask objective and shapes the language model representation to be\nattribute-discriminative. Finally, we discover new attributes and values\nthrough the self-ensemble of our framework, which handles the open-world\nchallenge. We run extensive experiments on a large distantly annotated\ndevelopment set and a gold standard human-annotated test set that we collected.\nOur model significantly outperforms strong baselines and can generalize to\nunseen attributes and product types.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinyang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chenwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xin Luna Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_J/0/1/0/all/0/1\">Jingbo Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faloutsos_C/0/1/0/all/0/1\">Christos Faloutsos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leaner and Faster: Two-Stage Model Compression for Lightweight Text-Image Retrieval. (arXiv:2204.13913v1 [cs.CV])","link":"http://arxiv.org/abs/2204.13913","description":"<p>Current text-image approaches (e.g., CLIP) typically adopt dual-encoder\narchitecture using pre-trained vision-language representation. However, these\nmodels still pose non-trivial memory requirements and substantial incremental\nindexing time, which makes them less practical on mobile devices. In this\npaper, we present an effective two-stage framework to compress large\npre-trained dual-encoder for lightweight text-image retrieval. The resulting\nmodel is smaller (39% of the original), faster (1.6x/2.9x for processing\nimage/text respectively), yet performs on par with or better than the original\nfull model on Flickr30K and MSCOCO benchmarks. We also open-source an\naccompanying realistic mobile image search application.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1\">Siyu Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1\">Kenny Q. Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Czech Dataset for Cross-lingual Subjectivity Classification. (arXiv:2204.13915v1 [cs.CL])","link":"http://arxiv.org/abs/2204.13915","description":"<p>In this paper, we introduce a new Czech subjectivity dataset of 10k manually\nannotated subjective and objective sentences from movie reviews and\ndescriptions. Our prime motivation is to provide a reliable dataset that can be\nused with the existing English dataset as a benchmark to test the ability of\npre-trained multilingual models to transfer knowledge between Czech and English\nand vice versa. Two annotators annotated the dataset reaching 0.83 of the\nCohen's \\k{appa} inter-annotator agreement. To the best of our knowledge, this\nis the first subjectivity dataset for the Czech language. We also created an\nadditional dataset that consists of 200k automatically labeled sentences. Both\ndatasets are freely available for research purposes. Furthermore, we fine-tune\nfive pre-trained BERT-like models to set a monolingual baseline for the new\ndataset and we achieve 93.56% of accuracy. We fine-tune models on the existing\nEnglish dataset for which we obtained results that are on par with the current\nstate-of-the-art results. Finally, we perform zero-shot cross-lingual\nsubjectivity classification between Czech and English to verify the usability\nof our dataset as the cross-lingual benchmark. We compare and discuss the\ncross-lingual and monolingual results and the ability of multilingual models to\ntransfer knowledge between languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Priban_P/0/1/0/all/0/1\">Pavel P&#x159;ib&#xe1;&#x148;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steinberger_J/0/1/0/all/0/1\">Josef Steinberger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"QRelScore: Better Evaluating Generated Questions with Deeper Understanding of Context-aware Relevance. (arXiv:2204.13921v1 [cs.CL])","link":"http://arxiv.org/abs/2204.13921","description":"<p>Existing metrics for assessing question generation not only require costly\nhuman reference but also fail to take into account the input context of\ngeneration, rendering the lack of deep understanding of the relevance between\nthe generated questions and input contexts. As a result, they may wrongly\npenalize a legitimate and reasonable candidate question when it (i) involves\ncomplicated reasoning with the context or (ii) can be grounded by multiple\nevidences in the context. In this paper, we propose $\\textbf{QRelScore}$, a\ncontext-aware $\\underline{\\textbf{Rel}}$evance evaluation metric for\n$\\underline{\\textbf{Q}}$uestion Generation. Based on off-the-shelf language\nmodels such as BERT and GPT2, QRelScore employs both word-level hierarchical\nmatching and sentence-level prompt-based generation to cope with the\ncomplicated reasoning and diverse generation from multiple evidences,\nrespectively. Compared with existing metrics, our experiments demonstrate that\nQRelScore is able to achieve a higher correlation with human judgments while\nbeing much more robust to adversarial samples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoqiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Siliang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lingfei Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KERMIT -- A Transformer-Based Approach for Knowledge Graph Matching. (arXiv:2204.13931v1 [cs.CL])","link":"http://arxiv.org/abs/2204.13931","description":"<p>One of the strongest signals for automated matching of knowledge graphs and\nontologies are textual concept descriptions. With the rise of transformer-based\nlanguage models, text comparison based on meaning (rather than lexical\nfeatures) is available to researchers. However, performing pairwise comparisons\nof all textual descriptions of concepts in two knowledge graphs is expensive\nand scales quadratically (or even worse if concepts have more than one\ndescription). To overcome this problem, we follow a two-step approach: we first\ngenerate matching candidates using a pre-trained sentence transformer (so\ncalled bi-encoder). In a second step, we use fine-tuned transformer\ncross-encoders to generate the best candidates. We evaluate our approach on\nmultiple datasets and show that it is feasible and produces competitive\nresults.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hertling_S/0/1/0/all/0/1\">Sven Hertling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Portisch_J/0/1/0/all/0/1\">Jan Portisch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paulheim_H/0/1/0/all/0/1\">Heiko Paulheim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"User Experience Design for Automatic Credibility Assessment of News Content About COVID-19. (arXiv:2204.13943v1 [cs.CL])","link":"http://arxiv.org/abs/2204.13943","description":"<p>The increasingly rapid spread of information about COVID-19 on the web calls\nfor automatic measures of quality assurance. In that context, we check the\ncredibility of news content using selected linguistic features. We present two\nempirical studies to evaluate the usability of graphical interfaces that offer\nsuch credibility assessment. In a moderated qualitative interview with six\nparticipants, we identify rating scale, sub-criteria and algorithm authorship\nas important predictors of the usability. A subsequent quantitative online\nsurvey with 50 participants reveals a conflict between transparency and\nconciseness in the interface design, as well as a perceived hierarchy of\nmetadata: the authorship of a news text is more important than the authorship\nof the credibility algorithm used to assess the content quality. Finally, we\nmake suggestions for future research, such as proactively documenting\ncredibility-related metadata for Natural Language Processing and Language\nTechnology services and establishing an explicit hierarchical taxonomy of\nusability predictors for automatic credibility assessment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schulz_K/0/1/0/all/0/1\">Konstantin Schulz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rauenbusch_J/0/1/0/all/0/1\">Jens Rauenbusch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fillies_J/0/1/0/all/0/1\">Jan Fillies</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rutenburg_L/0/1/0/all/0/1\">Lisa Rutenburg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karvelas_D/0/1/0/all/0/1\">Dimitrios Karvelas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rehm_G/0/1/0/all/0/1\">Georg Rehm</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"\"My nose is running.\"\"Are you also coughing?\": Building A Medical Diagnosis Agent with Interpretable Inquiry Logics. (arXiv:2204.13953v1 [cs.CL])","link":"http://arxiv.org/abs/2204.13953","description":"<p>With the rise of telemedicine, the task of developing Dialogue Systems for\nMedical Diagnosis (DSMD) has received much attention in recent years. Different\nfrom early researches that needed to rely on extra human resources and\nexpertise to help construct the system, recent researches focused on how to\nbuild DSMD in a purely data-driven manner. However, the previous data-driven\nDSMD methods largely overlooked the system interpretability, which is critical\nfor a medical application, and they also suffered from the data sparsity issue\nat the same time. In this paper, we explore how to bring interpretability to\ndata-driven DSMD. Specifically, we propose a more interpretable decision\nprocess to implement the dialogue manager of DSMD by reasonably mimicking real\ndoctors' inquiry logics, and we devise a model with highly transparent\ncomponents to conduct the inference. Moreover, we collect a new DSMD dataset,\nwhich has a much larger scale, more diverse patterns and is of higher quality\nthan the existing ones. The experiments show that our method obtains 7.7%,\n10.0%, 3.0% absolute improvement in diagnosis accuracy respectively on three\ndatasets, demonstrating the effectiveness of its rational decision process and\nmodel design. Our codes and the GMD-12 dataset are available at\nhttps://github.com/lwgkzl/BR-Agent.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenge Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yi Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tangi_J/0/1/0/all/0/1\">Jianheng Tangi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yafei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1\">Ruihui Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenjie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yefeng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PIE: a Parameter and Inference Efficient Solution for Large Scale Knowledge Graph Embedding Reasoning. (arXiv:2204.13957v1 [cs.CL])","link":"http://arxiv.org/abs/2204.13957","description":"<p>Knowledge graph (KG) embedding methods which map entities and relations to\nunique embeddings in the KG have shown promising results on many reasoning\ntasks. However, the same embedding dimension for both dense entities and sparse\nentities will cause either over parameterization (sparse entities) or under\nfitting (dense entities). Normally, a large dimension is set to get better\nperformance. Meanwhile, the inference time grows log-linearly with the number\nof entities for all entities are traversed and compared. Both the parameter and\ninference become challenges when working with huge amounts of entities. Thus,\nwe propose PIE, a \\textbf{p}arameter and \\textbf{i}nference \\textbf{e}fficient\nsolution. Inspired from tensor decomposition methods, we find that decompose\nentity embedding matrix into low rank matrices can reduce more than half of the\nparameters while maintaining comparable performance. To accelerate model\ninference, we propose a self-supervised auxiliary task, which can be seen as\nfine-grained entity typing. By randomly masking and recovering entities'\nconnected relations, the task learns the co-occurrence of entity and relations.\nUtilizing the fine grained typing, we can filter unrelated entities during\ninference and get targets with possibly sub-linear time requirement.\nExperiments on link prediction benchmarks demonstrate the proposed key\ncapabilities. Moreover, we prove effectiveness of the proposed solution on the\nOpen Graph Benchmark large scale challenge dataset WikiKG90Mv2 and achieve the\nstate of the art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chao_L/0/1/0/all/0/1\">Linlin Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Taifeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_W/0/1/0/all/0/1\">Wei Chu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Making sense of violence risk predictions using clinical notes. (arXiv:2204.13976v1 [cs.LG])","link":"http://arxiv.org/abs/2204.13976","description":"<p>Violence risk assessment in psychiatric institutions enables interventions to\navoid violence incidents. Clinical notes written by practitioners and available\nin electronic health records (EHR) are valuable resources that are seldom used\nto their full potential. Previous studies have attempted to assess violence\nrisk in psychiatric patients using such notes, with acceptable performance.\nHowever, they do not explain why classification works and how it can be\nimproved. We explore two methods to better understand the quality of a\nclassifier in the context of clinical note analysis: random forests using topic\nmodels, and choice of evaluation metric. These methods allow us to understand\nboth our data and our methodology more profoundly, setting up the groundwork to\nwork on improved models that build upon this understanding. This is\nparticularly important when it comes to the generalizability of evaluated\nclassifiers to new data, a trustworthiness problem that is of great interest\ndue to the increased availability of new data in electronic format.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mosteiro_P/0/1/0/all/0/1\">Pablo Mosteiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rijcken_E/0/1/0/all/0/1\">Emil Rijcken</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zervanou_K/0/1/0/all/0/1\">Kalliopi Zervanou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaymak_U/0/1/0/all/0/1\">Uzay Kaymak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scheepers_F/0/1/0/all/0/1\">Floortje Scheepers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spruit_M/0/1/0/all/0/1\">Marco Spruit</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ExaASC: A General Target-Based Stance Detection Corpus in Arabic Language. (arXiv:2204.13979v1 [cs.CL])","link":"http://arxiv.org/abs/2204.13979","description":"<p>Target-based Stance Detection is the task of finding a stance toward a\ntarget. Twitter is one of the primary sources of political discussions in\nsocial media and one of the best resources to analyze Stance toward entities.\nThis work proposes a new method toward Target-based Stance detection by using\nthe stance of replies toward a most important and arguing target in source\ntweet. This target is detected with respect to the source tweet itself and not\nlimited to a set of pre-defined targets which is the usual approach of the\ncurrent state-of-the-art methods. Our proposed new attitude resulted in a new\ncorpus called ExaASC for the Arabic Language, one of the low resource languages\nin this field. In the end, we used BERT to evaluate our corpus and reached a\n70.69 Macro F-score. This shows that our data and model can work in a general\nTarget-base Stance Detection system. The corpus is publicly available1.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jaziriyan_M/0/1/0/all/0/1\">Mohammad Mehdi Jaziriyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akbari_A/0/1/0/all/0/1\">Ahmad Akbari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karbasi_H/0/1/0/all/0/1\">Hamed Karbasi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prompt Engineering for Text-Based Generative Art. (arXiv:2204.13988v1 [cs.MM])","link":"http://arxiv.org/abs/2204.13988","description":"<p>Text-based generative art has seen an explosion of interest in 2021. Online\ncommunities around text-based generative art as a novel digital medium have\nquickly emerged. This short paper identifies five types of prompt modifiers\nused by practitioners in the community of text-based generative art based on a\n3-month ethnographic study on Twitter. The novel taxonomy of prompt modifiers\nprovides researchers a conceptual starting point for investigating the\npractices of text-based generative art, but also may help practitioners of\ntext-based generative art improve their images. The paper concludes with a\ndiscussion of research opportunities in the space of text-based generative art\nand the broader implications of prompt engineering from the perspective of\nhuman-AI interaction in future applications beyond the use case of text-based\ngenerative art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Oppenlaender_J/0/1/0/all/0/1\">Jonas Oppenlaender</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Backdoor Attacks in Federated Learning by Rare Embeddings and Gradient Ensembling. (arXiv:2204.14017v1 [cs.LG])","link":"http://arxiv.org/abs/2204.14017","description":"<p>Recent advances in federated learning have demonstrated its promising\ncapability to learn on decentralized datasets. However, a considerable amount\nof work has raised concerns due to the potential risks of adversaries\nparticipating in the framework to poison the global model for an adversarial\npurpose. This paper investigates the feasibility of model poisoning for\nbackdoor attacks through \\textit{rare word embeddings of NLP models} in text\nclassification and sequence-to-sequence tasks. In text classification, less\nthan 1\\% of adversary clients suffices to manipulate the model output without\nany drop in the performance of clean sentences. For a less complex dataset, a\nmere 0.1\\% of adversary clients is enough to poison the global model\neffectively. We also propose a technique specialized in the federated learning\nscheme called gradient ensemble, which enhances the backdoor performance in all\nexperimental settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yoo_K/0/1/0/all/0/1\">KiYoon Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_N/0/1/0/all/0/1\">Nojun Kwak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-shot learning for medical text: A systematic review. (arXiv:2204.14081v1 [cs.CL])","link":"http://arxiv.org/abs/2204.14081","description":"<p>Objective: Few-shot learning (FSL) methods require small numbers of labeled\ninstances for training. As many medical topics have limited annotated textual\ndata in practical settings, FSL-based natural language processing (NLP) methods\nhold substantial promise. We aimed to conduct a systematic review to explore\nthe state of FSL methods for medical NLP. Materials and Methods: We searched\nfor articles published between January 2016 and August 2021 using\nPubMed/Medline, Embase, ACL Anthology, and IEEE Xplore Digital Library. To\nidentify the latest relevant methods, we also searched other sources such as\npreprint servers (eg., medRxiv) via Google Scholar. We included all articles\nthat involved FSL and any type of medical text. We abstracted articles based on\ndata source(s), aim(s), training set size(s), primary method(s)/approach(es),\nand evaluation method(s). Results: 31 studies met our inclusion criteria-all\npublished after 2018; 22 (71%) since 2020. Concept extraction/named entity\nrecognition was the most frequently addressed task (13/31; 42%), followed by\ntext classification (10/31; 32%). Twenty-one (68%) studies reconstructed\nexisting datasets to create few-shot scenarios synthetically, and MIMIC-III was\nthe most frequently used dataset (7/31; 23%). Common methods included FSL with\nattention mechanisms (12/31; 39%), prototypical networks (8/31; 26%), and\nmeta-learning (6/31; 19%). Discussion: Despite the potential for FSL in\nbiomedical NLP, progress has been limited compared to domain-independent FSL.\nThis may be due to the paucity of standardized, public datasets, and the\nrelative underperformance of FSL methods on biomedical topics. Creation and\nrelease of specialized datasets for biomedical FSL may aid method development\nby enabling comparative analyses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yao Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yuting Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yuan-Chi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Garadi_M/0/1/0/all/0/1\">Mohammed Ali Al-Garadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarker_A/0/1/0/all/0/1\">Abeed Sarker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Climate and Weather: Inspecting Depression Detection via Emotion Recognition. (arXiv:2204.14099v1 [cs.CL])","link":"http://arxiv.org/abs/2204.14099","description":"<p>Automatic depression detection has attracted increasing amount of attention\nbut remains a challenging task. Psychological research suggests that depressive\nmood is closely related with emotion expression and perception, which motivates\nthe investigation of whether knowledge of emotion recognition can be\ntransferred for depression detection. This paper uses pretrained features\nextracted from the emotion recognition model for depression detection, further\nfuses emotion modality with audio and text to form multimodal depression\ndetection. The proposed emotion transfer improves depression detection\nperformance on DAIC-WOZ as well as increases the training stability. The\nanalysis of how the emotion expressed by depressed individuals is further\nperceived provides clues for further understanding of the relationship between\ndepression and emotion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Mengyue Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_K/0/1/0/all/0/1\">Kai Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TEMOS: Generating diverse human motions from textual descriptions. (arXiv:2204.14109v1 [cs.CV])","link":"http://arxiv.org/abs/2204.14109","description":"<p>We address the problem of generating diverse 3D human motions from textual\ndescriptions. This challenging task requires joint modeling of both modalities:\nunderstanding and extracting useful human-centric information from the text,\nand then generating plausible and realistic sequences of human poses. In\ncontrast to most previous work which focuses on generating a single,\ndeterministic, motion from a textual description, we design a variational\napproach that can produce multiple diverse human motions. We propose TEMOS, a\ntext-conditioned generative model leveraging variational autoencoder (VAE)\ntraining with human motion data, in combination with a text encoder that\nproduces distribution parameters compatible with the VAE latent space. We show\nthat TEMOS framework can produce both skeleton-based animations as in prior\nwork, as well more expressive SMPL body motions. We evaluate our approach on\nthe KIT Motion-Language benchmark and, despite being relatively\nstraightforward, demonstrate significant improvements over the state of the\nart. Code and models are available on our project page.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Petrovich_M/0/1/0/all/0/1\">Mathis Petrovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Black_M/0/1/0/all/0/1\">Michael J. Black</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varol_G/0/1/0/all/0/1\">G&#xfc;l Varol</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Developmental Negation Processing in Transformer Language Models. (arXiv:2204.14114v1 [cs.CL])","link":"http://arxiv.org/abs/2204.14114","description":"<p>Reasoning using negation is known to be difficult for transformer-based\nlanguage models. While previous studies have used the tools of\npsycholinguistics to probe a transformer's ability to reason over negation,\nnone have focused on the types of negation studied in developmental psychology.\nWe explore how well transformers can process such categories of negation, by\nframing the problem as a natural language inference (NLI) task. We curate a set\nof diagnostic questions for our target categories from popular NLI datasets and\nevaluate how well a suite of models reason over them. We find that models\nperform consistently better only on certain categories, suggesting clear\ndistinctions in how they are processed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Laverghetta_A/0/1/0/all/0/1\">Antonio Laverghetta Jr.</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Licato_J/0/1/0/all/0/1\">John Licato</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning from Natural Language Feedback. (arXiv:2204.14146v1 [cs.CL])","link":"http://arxiv.org/abs/2204.14146","description":"<p>Pretrained language models often do not perform tasks in ways that are in\nline with our preferences, e.g., generating offensive text or factually\nincorrect summaries. Recent work approaches the above issue by learning from a\nsimple form of human evaluation: comparisons between pairs of model-generated\ntask outputs. Comparison feedback conveys limited information about human\npreferences per human evaluation. Here, we propose to learn from natural\nlanguage feedback, which conveys more information per human evaluation. We\nlearn from language feedback on model outputs using a three-step learning\nalgorithm. First, we condition the language model on the initial output and\nfeedback to generate many refinements. Second, we choose the refinement with\nthe highest similarity to the feedback. Third, we finetune a language model to\nmaximize the likelihood of the chosen refinement given the input. In synthetic\nexperiments, we first evaluate whether language models accurately incorporate\nfeedback to produce refinements, finding that only large language models (175B\nparameters) do so. Using only 100 samples of human-written feedback, our\nlearning algorithm finetunes a GPT-3 model to roughly human-level\nsummarization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Scheurer_J/0/1/0/all/0/1\">J&#xe9;r&#xe9;my Scheurer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Campos_J/0/1/0/all/0/1\">Jon Ander Campos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_J/0/1/0/all/0/1\">Jun Shern Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1\">Angelica Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1\">Kyunghyun Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_E/0/1/0/all/0/1\">Ethan Perez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OPERA:Operation-Pivoted Discrete Reasoning over Text. (arXiv:2204.14166v1 [cs.CL])","link":"http://arxiv.org/abs/2204.14166","description":"<p>Machine reading comprehension (MRC) that requires discrete reasoning\ninvolving symbolic operations, e.g., addition, sorting, and counting, is a\nchallenging task. According to this nature, semantic parsing-based methods\npredict interpretable but complex logical forms. However, logical form\ngeneration is nontrivial and even a little perturbation in a logical form will\nlead to wrong answers. To alleviate this issue, multi-predictor -based methods\nare proposed to directly predict different types of answers and achieve\nimprovements. However, they ignore the utilization of symbolic operations and\nencounter a lack of reasoning ability and interpretability. To inherit the\nadvantages of these two types of methods, we propose OPERA, an\noperation-pivoted discrete reasoning framework, where lightweight symbolic\noperations (compared with logical forms) as neural modules are utilized to\nfacilitate the reasoning ability and interpretability. Specifically, operations\nare first selected and then softly executed to simulate the answer reasoning\nprocedure. Extensive experiments on both DROP and RACENum datasets show the\nreasoning ability of OPERA. Moreover, further analysis verifies its\ninterpretability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yongwei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Junwei Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_C/0/1/0/all/0/1\">Chaoqun Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Haipeng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jiahui Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yifan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jing Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Youzheng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaodong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tiejun Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TemporalWiki: A Lifelong Benchmark for Training and Evaluating Ever-Evolving Language Models. (arXiv:2204.14211v1 [cs.CL])","link":"http://arxiv.org/abs/2204.14211","description":"<p>Language Models (LMs) become outdated as the world changes; they often fail\nto perform tasks requiring recent factual information which was absent or\ndifferent during training, a phenomenon called temporal misalignment. This is\nespecially a challenging problem because the research community still lacks a\ncoherent dataset for assessing the adaptability of LMs to frequently-updated\nknowledge corpus such as Wikipedia. To this end, we introduce TemporalWiki, a\nlifelong benchmark for ever-evolving LMs that utilizes the difference between\nconsecutive snapshots of English Wikipedia and English Wikidata for training\nand evaluation, respectively. The benchmark hence allows researchers to\nperiodically track an LM's ability to retain previous knowledge and acquire\nupdated/new knowledge at each point in time. We also find that training an LM\non the diff data through continual learning methods achieves similar or better\nperplexity than on the entire snapshot in our benchmark with 12 times less\ncomputational cost, which verifies that factual knowledge in LMs can be safely\nupdated with minimal training data via continual learning. The dataset and the\ncode are available at https://github.com/joeljang/temporalwiki .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jang_J/0/1/0/all/0/1\">Joel Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_S/0/1/0/all/0/1\">Seonghyeon Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Changho Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sohee Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1\">Joongbo Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Janghoon Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1\">Gyeonghun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_M/0/1/0/all/0/1\">Minjoon Seo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modular Domain Adaptation. (arXiv:2204.14213v1 [cs.CL])","link":"http://arxiv.org/abs/2204.14213","description":"<p>Off-the-shelf models are widely used by computational social science\nresearchers to measure properties of text, such as sentiment. However, without\naccess to source data it is difficult to account for domain shift, which\nrepresents a threat to validity. Here, we treat domain adaptation as a modular\nprocess that involves separate model producers and model consumers, and show\nhow they can independently cooperate to facilitate more accurate measurements\nof text. We introduce two lightweight techniques for this scenario, and\ndemonstrate that they reliably increase out-of-domain accuracy on four\nmulti-domain text classification datasets when used with linear and contextual\nembedding models. We conclude with recommendations for model producers and\nconsumers, and release models and replication code to accompany this paper.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Junshen K. Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Card_D/0/1/0/all/0/1\">Dallas Card</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurafsky_D/0/1/0/all/0/1\">Dan Jurafsky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Training Naturalized Semantic Parsers with Very Little Data. (arXiv:2204.14243v1 [cs.CL])","link":"http://arxiv.org/abs/2204.14243","description":"<p>Semantic parsing is an important NLP problem, particularly for voice\nassistants such as Alexa and Google Assistant. State-of-the-art (SOTA) semantic\nparsers are seq2seq architectures based on large language models that have been\npretrained on vast amounts of text. To better leverage that pretraining, recent\nwork has explored a reformulation of semantic parsing whereby the output\nsequences are themselves natural language sentences, but in a controlled\nfragment of natural language. This approach delivers strong results,\nparticularly for few-shot semantic parsing, which is of key importance in\npractice and the focus of our paper. We push this line of work forward by\nintroducing an automated methodology that delivers very significant additional\nimprovements by utilizing modest amounts of unannotated data, which is\ntypically easy to obtain. Our method is based on a novel synthesis of four\ntechniques: joint training with auxiliary unsupervised tasks; constrained\ndecoding; self-training; and paraphrasing. We show that this method delivers\nnew SOTA few-shot performance on the Overnight dataset, particularly in very\nlow-resource settings, and very compelling few-shot results on a new semantic\nparsing dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rongali_S/0/1/0/all/0/1\">Subendhu Rongali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arkoudas_K/0/1/0/all/0/1\">Konstantine Arkoudas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rubino_M/0/1/0/all/0/1\">Melanie Rubino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamza_W/0/1/0/all/0/1\">Wael Hamza</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Handling and Presenting Harmful Text. (arXiv:2204.14256v1 [cs.CL])","link":"http://arxiv.org/abs/2204.14256","description":"<p>Textual data can pose a risk of serious harm. These harms can be categorised\nalong three axes: (1) the harm type (e.g. misinformation, hate speech or racial\nstereotypes) (2) whether it is \\textit{elicited} as a feature of the research\ndesign from directly studying harmful content (e.g. training a hate speech\nclassifier or auditing unfiltered large-scale datasets) versus\n\\textit{spuriously} invoked from working on unrelated problems (e.g. language\ngeneration or part of speech tagging) but with datasets that nonetheless\ncontain harmful content, and (3) who it affects, from the humans\n(mis)represented in the data to those handling or labelling the data to readers\nand reviewers of publications produced from the data. It is an unsolved problem\nin NLP as to how textual harms should be handled, presented, and discussed;\nbut, stopping work on content which poses a risk of harm is untenable.\nAccordingly, we provide practical advice and introduce \\textsc{HarmCheck}, a\nresource for reflecting on research into textual harms. We hope our work\nencourages ethical, responsible, and respectful research in the NLP community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Derczynski_L/0/1/0/all/0/1\">Leon Derczynski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirk_H/0/1/0/all/0/1\">Hannah Rose Kirk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Birhane_A/0/1/0/all/0/1\">Abeba Birhane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vidgen_B/0/1/0/all/0/1\">Bertie Vidgen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Polyglot Prompt: Multilingual Multitask PrompTraining. (arXiv:2204.14264v1 [cs.CL])","link":"http://arxiv.org/abs/2204.14264","description":"<p>This paper aims for a potential architectural breakthrough for multilingual\nlearning and asks: could different tasks from different languages be modeled in\na monolithic framework (without any task/language-specific module)? The benefit\nof achieving this is not only that systems trained on low resources scenario\ncan be assisted by more other languages and tasks, but opening new doors for\nfuture multilingual research. We approach this goal by developing a learning\nframework Polyglot Prompt, where prompting methods are introduced to learn a\nunified semantic space for different languages and tasks after proper\nmultilingual prompt engineering. Experimentally, we perform a comprehensive\nevaluation on 6 tasks (topic classification, sentiment classification, named\nentity recognition, question answering, natural language inference,\nsummarization), 24 datasets, and 49 languages, which shows the efficacy of\nmultilingual multitask prompting training and suggests several interesting\nobservations. e.g., English prompts are polyglots since directly applying them\nto task samples in other languages could result in a better improvement. We\nalso present an interpretable multilingual evaluation methodology and show how\nthe proposed framework, multilingual multitask prompt training, works. We\nrelease all datasets prompted in the best setting and will release our code\nsoon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jinlan Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_S/0/1/0/all/0/1\">See-Kiong Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Pengfei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Robust is Neural Machine Translation to Language Imbalance in Multilingual Tokenizer Training?. (arXiv:2204.14268v1 [cs.CL])","link":"http://arxiv.org/abs/2204.14268","description":"<p>A multilingual tokenizer is a fundamental component of multilingual neural\nmachine translation. It is trained from a multilingual corpus. Since a skewed\ndata distribution is considered to be harmful, a sampling strategy is usually\nused to balance languages in the corpus. However, few works have systematically\nanswered how language imbalance in tokenizer training affects downstream\nperformance. In this work, we analyze how translation performance changes as\nthe data ratios among languages vary in the tokenizer training corpus. We find\nthat while relatively better performance is often observed when languages are\nmore equally sampled, the downstream performance is more robust to language\nimbalance than we usually expected. Two features, UNK rate and closeness to the\ncharacter level, can warn of poor downstream performance before performing the\ntask. We also distinguish language sampling for tokenizer training from\nsampling for model training and show that the model is more sensitive to the\nlatter.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shiyue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhary_V/0/1/0/all/0/1\">Vishrav Chaudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_N/0/1/0/all/0/1\">Naman Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cross_J/0/1/0/all/0/1\">James Cross</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wenzek_G/0/1/0/all/0/1\">Guillaume Wenzek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guzman_F/0/1/0/all/0/1\">Francisco Guzman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-end Spoken Conversational Question Answering: Task, Dataset and Model. (arXiv:2204.14272v1 [cs.CL])","link":"http://arxiv.org/abs/2204.14272","description":"<p>In spoken question answering, the systems are designed to answer questions\nfrom contiguous text spans within the related speech transcripts. However, the\nmost natural way that human seek or test their knowledge is via human\nconversations. Therefore, we propose a new Spoken Conversational Question\nAnswering task (SCQA), aiming at enabling the systems to model complex dialogue\nflows given the speech documents. In this task, our main objective is to build\nthe system to deal with conversational questions based on the audio recordings,\nand to explore the plausibility of providing more cues from different\nmodalities with systems in information gathering. To this end, instead of\ndirectly adopting automatically generated speech transcripts with highly noisy\ndata, we propose a novel unified data distillation approach, DDNet, which\neffectively ingests cross-modal information to achieve fine-grained\nrepresentations of the speech and language modalities. Moreover, we propose a\nsimple and novel mechanism, termed Dual Attention, by encouraging better\nalignments between audio and text to ease the process of knowledge transfer. To\nevaluate the capacity of SCQA systems in a dialogue-style interaction, we\nassemble a Spoken Conversational Question Answering (Spoken-CoQA) dataset with\nmore than 40k question-answer pairs from 4k conversations. The performance of\nthe existing state-of-the-art methods significantly degrade on our dataset,\nhence demonstrating the necessity of cross-modal information integration. Our\nexperimental results demonstrate that our proposed method achieves superior\nperformance in spoken conversational question answering tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1\">Chenyu You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fenglin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1\">Shen Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuexian Zou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What Makes a Good and Useful Summary? Incorporating Users in Automatic Summarization Research. (arXiv:2012.07619v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2012.07619","description":"<p>Automatic text summarization has enjoyed great progress over the years and is\nused in numerous applications, impacting the lives of many. Despite this\ndevelopment, there is little research that meaningfully investigates how the\ncurrent research focus in automatic summarization aligns with users' needs. To\nbridge this gap, we propose a survey methodology that can be used to\ninvestigate the needs of users of automatically generated summaries.\nImportantly, these needs are dependent on the target group. Hence, we design\nour survey in such a way that it can be easily adjusted to investigate\ndifferent user groups. In this work we focus on university students, who make\nextensive use of summaries during their studies. We find that the current\nresearch directions of the automatic summarization community do not fully align\nwith students' needs. Motivated by our findings, we present ways to mitigate\nthis mismatch in future research on automatic summarization: we propose\nresearch directions that impact the design, the development and the evaluation\nof automatically generated summaries.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hoeve_M/0/1/0/all/0/1\">Maartje ter Hoeve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiseleva_J/0/1/0/all/0/1\">Julia Kiseleva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rijke_M/0/1/0/all/0/1\">Maarten de Rijke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Identifying Machine-Paraphrased Plagiarism. (arXiv:2103.11909v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.11909","description":"<p>Employing paraphrasing tools to conceal plagiarized text is a severe threat\nto academic integrity. To enable the detection of machine-paraphrased text, we\nevaluate the effectiveness of five pre-trained word embedding models combined\nwith machine learning classifiers and state-of-the-art neural language models.\nWe analyze preprints of research papers, graduation theses, and Wikipedia\narticles, which we paraphrased using different configurations of the tools\nSpinBot and SpinnerChief. The best performing technique, Longformer, achieved\nan average F1 score of 80.99% (F1=99.68% for SpinBot and F1=71.64% for\nSpinnerChief cases), while human evaluators achieved F1=78.4% for SpinBot and\nF1=65.6% for SpinnerChief cases. We show that the automated classification\nalleviates shortcomings of widely-used text-matching systems, such as Turnitin\nand PlagScan. To facilitate future research, all data, code, and two web\napplications showcasing our contributions are openly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wahle_J/0/1/0/all/0/1\">Jan Philip Wahle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruas_T/0/1/0/all/0/1\">Terry Ruas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foltynek_T/0/1/0/all/0/1\">Tom&#xe1;&#x161; Folt&#xfd;nek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meuschke_N/0/1/0/all/0/1\">Norman Meuschke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gipp_B/0/1/0/all/0/1\">Bela Gipp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are Neural Language Models Good Plagiarists? A Benchmark for Neural Paraphrase Detection. (arXiv:2103.12450v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.12450","description":"<p>The rise of language models such as BERT allows for high-quality text\nparaphrasing. This is a problem to academic integrity, as it is difficult to\ndifferentiate between original and machine-generated content. We propose a\nbenchmark consisting of paraphrased articles using recent language models\nrelying on the Transformer architecture. Our contribution fosters future\nresearch of paraphrase detection systems as it offers a large collection of\naligned original and paraphrased documents, a study regarding its structure,\nclassification experiments with state-of-the-art systems, and we make our\nfindings publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wahle_J/0/1/0/all/0/1\">Jan Philip Wahle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruas_T/0/1/0/all/0/1\">Terry Ruas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meuschke_N/0/1/0/all/0/1\">Norman Meuschke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gipp_B/0/1/0/all/0/1\">Bela Gipp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Linguistic Dependencies and Statistical Dependence. (arXiv:2104.08685v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08685","description":"<p>Are pairs of words that tend to occur together also likely to stand in a\nlinguistic dependency? This empirical question is motivated by a long history\nof literature in cognitive science, psycholinguistics, and NLP. In this work we\ncontribute an extensive analysis of the relationship between linguistic\ndependencies and statistical dependence between words. Improving on previous\nwork, we introduce the use of large pretrained language models to compute\ncontextualized estimates of the pointwise mutual information between words\n(CPMI). For multiple models and languages, we extract dependency trees which\nmaximize CPMI, and compare to gold standard linguistic dependencies. Overall,\nwe find that CPMI dependencies achieve an unlabelled undirected attachment\nscore of at most $\\approx 0.5$. While far above chance, and consistently above\na non-contextualized PMI baseline, this score is generally comparable to a\nsimple baseline formed by connecting adjacent words. We analyze which kinds of\nlinguistic dependencies are best captured in CPMI dependencies, and also find\nmarked differences between the estimates of the large pretrained language\nmodels, illustrating how their different training schemes affect the type of\ndependencies they capture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hoover_J/0/1/0/all/0/1\">Jacob Louis Hoover</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sordoni_A/0/1/0/all/0/1\">Alessandro Sordoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_W/0/1/0/all/0/1\">Wenyu Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+ODonnell_T/0/1/0/all/0/1\">Timothy J. O&#x27;Donnell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Post-hoc Interpretability for Neural NLP: A Survey. (arXiv:2108.04840v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.04840","description":"<p>Neural networks for NLP are becoming increasingly complex and widespread, and\nthere is a growing concern if these models are responsible to use. Explaining\nmodels helps to address the safety and ethical concerns and is essential for\naccountability. Interpretability serves to provide these explanations in terms\nthat are understandable to humans. Additionally, post-hoc methods provide\nexplanations after a model is learned and are generally model-agnostic. This\nsurvey provides a categorization of how recent post-hoc interpretability\nmethods communicate explanations to humans, it discusses each method in-depth,\nand how they are validated, as the latter is often a common concern.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Madsen_A/0/1/0/all/0/1\">Andreas Madsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1\">Siva Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandar_S/0/1/0/all/0/1\">Sarath Chandar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsolved Problems in ML Safety. (arXiv:2109.13916v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2109.13916","description":"<p>Machine learning (ML) systems are rapidly increasing in size, are acquiring\nnew capabilities, and are increasingly deployed in high-stakes settings. As\nwith other powerful technologies, safety for ML should be a leading research\npriority. In response to emerging safety challenges in ML, such as those\nintroduced by recent large-scale models, we provide a new roadmap for ML Safety\nand refine the technical problems that the field needs to address. We present\nfour problems ready for research, namely withstanding hazards (\"Robustness\"),\nidentifying hazards (\"Monitoring\"), reducing inherent model hazards\n(\"Alignment\"), and reducing systemic hazards (\"Systemic Safety\"). Throughout,\nwe clarify each problem's motivation and provide concrete research directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hendrycks_D/0/1/0/all/0/1\">Dan Hendrycks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carlini_N/0/1/0/all/0/1\">Nicholas Carlini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schulman_J/0/1/0/all/0/1\">John Schulman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steinhardt_J/0/1/0/all/0/1\">Jacob Steinhardt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Impact of Temporal Representations on Metaphor Detection. (arXiv:2111.03320v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.03320","description":"<p>State-of-the-art approaches for metaphor detection compare their literal - or\ncore - meaning and their contextual meaning using metaphor classifiers based on\nneural networks. However, metaphorical expressions evolve over time due to\nvarious reasons, such as cultural and societal impact. Metaphorical expressions\nare known to co-evolve with language and literal word meanings, and even drive,\nto some extent, this evolution. This poses the question of whether different,\npossibly time-specific, representations of literal meanings may impact the\nmetaphor detection task. To the best of our knowledge, this is the first study\nthat examines the metaphor detection task with a detailed exploratory analysis\nwhere different temporal and static word embeddings are used to account for\ndifferent representations of literal meanings. Our experimental analysis is\nbased on three popular benchmarks used for metaphor detection and word\nembeddings extracted from different corpora and temporally aligned using\ndifferent state-of-the-art approaches. The results suggest that the usage of\ndifferent static word embedding methods does impact the metaphor detection task\nand some temporal word embeddings slightly outperform static methods. However,\nthe results also suggest that temporal word embeddings may provide\nrepresentations of the core meaning of the metaphor even too close to their\ncontextual meaning, thus confusing the classifier. Overall, the interaction\nbetween temporal language evolution and metaphor detection appears tiny in the\nbenchmark datasets used in our experiments. This suggests that future work for\nthe computational analysis of this important linguistic phenomenon should first\nstart by creating a new dataset where this interaction is better represented.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ottolina_G/0/1/0/all/0/1\">Giorgio Ottolina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palmonari_M/0/1/0/all/0/1\">Matteo Palmonari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alam_M/0/1/0/all/0/1\">Mehwish Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vimercati_M/0/1/0/all/0/1\">Manuel Vimercati</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AnswerSumm: A Manually-Curated Dataset and Pipeline for Answer Summarization. (arXiv:2111.06474v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.06474","description":"<p>Community Question Answering (CQA) fora such as Stack Overflow and Yahoo!\nAnswers contain a rich resource of answers to a wide range of community-based\nquestions. Each question thread can receive a large number of answers with\ndifferent perspectives. One goal of answer summarization is to produce a\nsummary that reflects the range of answer perspectives. A major obstacle for\nthis task is the absence of a dataset to provide supervision for producing such\nsummaries. Recent works propose heuristics to create such data, but these are\noften noisy and do not cover all answer perspectives present. This work\nintroduces a novel dataset of 4,631 CQA threads for answer summarization\ncurated by professional linguists. Our pipeline gathers annotations for all\nsubtasks of answer summarization, including relevant answer sentence selection,\ngrouping these sentences based on perspectives, summarizing each perspective,\nand producing an overall summary. We analyze and benchmark state-of-the-art\nmodels on these subtasks and introduce a novel unsupervised approach for\nmulti-perspective data augmentation that boosts summarization performance\naccording to automatic evaluation. Finally, we propose reinforcement learning\nrewards to improve factual consistency and answer coverage and analyze areas\nfor improvement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fabbri_A/0/1/0/all/0/1\">Alexander R. Fabbri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiaojian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyer_S/0/1/0/all/0/1\">Srini Iyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haoran Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diab_M/0/1/0/all/0/1\">Mona Diab</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Testing the Generalization of Neural Language Models for COVID-19 Misinformation Detection. (arXiv:2111.07819v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.07819","description":"<p>A drastic rise in potentially life-threatening misinformation has been a\nby-product of the COVID-19 pandemic. Computational support to identify false\ninformation within the massive body of data on the topic is crucial to prevent\nharm. Researchers proposed many methods for flagging online misinformation\nrelated to COVID-19. However, these methods predominantly target specific\ncontent types (e.g., news) or platforms (e.g., Twitter). The methods'\ncapabilities to generalize were largely unclear so far. We evaluate fifteen\nTransformer-based models on five COVID-19 misinformation datasets that include\nsocial media posts, news articles, and scientific papers to fill this gap. We\nshow tokenizers and models tailored to COVID-19 data do not provide a\nsignificant advantage over general-purpose ones. Our study provides a realistic\nassessment of models for detecting COVID-19 misinformation. We expect that\nevaluating a broad spectrum of datasets and models will benefit future research\nin developing misinformation detection systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wahle_J/0/1/0/all/0/1\">Jan Philip Wahle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ashok_N/0/1/0/all/0/1\">Nischal Ashok</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruas_T/0/1/0/all/0/1\">Terry Ruas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meuschke_N/0/1/0/all/0/1\">Norman Meuschke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosal_T/0/1/0/all/0/1\">Tirthankar Ghosal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gipp_B/0/1/0/all/0/1\">Bela Gipp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ITA: Image-Text Alignments for Multi-Modal Named Entity Recognition. (arXiv:2112.06482v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.06482","description":"<p>Recently, Multi-modal Named Entity Recognition (MNER) has attracted a lot of\nattention. Most of the work utilizes image information through region-level\nvisual representations obtained from a pretrained object detector and relies on\nan attention mechanism to model the interactions between image and text\nrepresentations. However, it is difficult to model such interactions as image\nand text representations are trained separately on the data of their respective\nmodality and are not aligned in the same space. As text representations take\nthe most important role in MNER, in this paper, we propose {\\bf I}mage-{\\bf\nt}ext {\\bf A}lignments (ITA) to align image features into the textual space, so\nthat the attention mechanism in transformer-based pretrained textual embeddings\ncan be better utilized. ITA first aligns the image into regional object tags,\nimage-level captions and optical characters as visual contexts, concatenates\nthem with the input texts as a new cross-modal input, and then feeds it into a\npretrained textual embedding model. This makes it easier for the attention\nmodule of a pretrained textual embedding model to model the interaction between\nthe two modalities since they are both represented in the textual space. ITA\nfurther aligns the output distributions predicted from the cross-modal input\nand textual input views so that the MNER model can be more practical in dealing\nwith text-only inputs and robust to noises from images. In our experiments, we\nshow that ITA models can achieve state-of-the-art accuracy on multi-modal Named\nEntity Recognition datasets, even without image information.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gui_M/0/1/0/all/0/1\">Min Gui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yong Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_Z/0/1/0/all/0/1\">Zixia Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bach_N/0/1/0/all/0/1\">Nguyen Bach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhongqiang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_K/0/1/0/all/0/1\">Kewei Tu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Two Contrasting Data Annotation Paradigms for Subjective NLP Tasks. (arXiv:2112.07475v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.07475","description":"<p>Labelled data is the foundation of most natural language processing tasks.\nHowever, labelling data is difficult and there often are diverse valid beliefs\nabout what the correct data labels should be. So far, dataset creators have\nacknowledged annotator subjectivity, but rarely actively managed it in the\nannotation process. This has led to partly-subjective datasets that fail to\nserve a clear downstream use. To address this issue, we propose two contrasting\nparadigms for data annotation. The descriptive paradigm encourages annotator\nsubjectivity, whereas the prescriptive paradigm discourages it. Descriptive\nannotation allows for the surveying and modelling of different beliefs, whereas\nprescriptive annotation enables the training of models that consistently apply\none belief. We discuss benefits and challenges in implementing both paradigms,\nand argue that dataset creators should explicitly aim for one or the other to\nfacilitate the intended use of their dataset. Lastly, we conduct an annotation\nexperiment using hate speech data that illustrates the contrast between the two\nparadigms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rottger_P/0/1/0/all/0/1\">Paul R&#xf6;ttger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vidgen_B/0/1/0/all/0/1\">Bertie Vidgen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hovy_D/0/1/0/all/0/1\">Dirk Hovy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pierrehumbert_J/0/1/0/all/0/1\">Janet B. Pierrehumbert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"QAFactEval: Improved QA-Based Factual Consistency Evaluation for Summarization. (arXiv:2112.08542v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.08542","description":"<p>Factual consistency is an essential quality of text summarization models in\npractical settings. Existing work in evaluating this dimension can be broadly\ncategorized into two lines of research, entailment-based and question answering\n(QA)-based metrics, and different experimental setups often lead to contrasting\nconclusions as to which paradigm performs the best. In this work, we conduct an\nextensive comparison of entailment and QA-based metrics, demonstrating that\ncarefully choosing the components of a QA-based metric, especially question\ngeneration and answerability classification, is critical to performance.\nBuilding on those insights, we propose an optimized metric, which we call\nQAFactEval, that leads to a 14% average improvement over previous QA-based\nmetrics on the SummaC factual consistency benchmark, and also outperforms the\nbest-performing entailment-based metric. Moreover, we find that QA-based and\nentailment-based metrics can offer complementary signals and be combined into a\nsingle metric for a further performance boost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fabbri_A/0/1/0/all/0/1\">Alexander R. Fabbri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chien-Sheng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenhao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analyzing the Limits of Self-Supervision in Handling Bias in Language. (arXiv:2112.08637v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.08637","description":"<p>Prompting inputs with natural language task descriptions has emerged as a\npopular mechanism to elicit reasonably accurate outputs from large-scale\ngenerative language models with little to no in-context supervision. This also\nhelps gain insight into how well language models capture the semantics of a\nwide range of downstream tasks purely from self-supervised pre-training on\nmassive corpora of unlabeled text. Such models have naturally also been exposed\nto a lot of undesirable content like racist and sexist language and there is\nlimited work on awareness of models along these dimensions. In this paper, we\ndefine and comprehensively evaluate how well such language models capture the\nsemantics of four tasks for bias: diagnosis, identification, extraction and\nrephrasing. We define three broad classes of task descriptions for these tasks:\nstatement, question, and completion, with numerous lexical variants within each\nclass. We study the efficacy of prompting for each task using these classes and\nthe null task description across several decoding methods and few-shot\nexamples. Our analyses indicate that language models are capable of performing\nthese tasks to widely varying degrees across different bias dimensions, such as\ngender and political affiliation. We believe our work is an important step\ntowards unbiased language models by quantifying the limits of current\nself-supervision objectives at accomplishing such sociologically challenging\ntasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bauer_L/0/1/0/all/0/1\">Lisa Bauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gopalakrishnan_K/0/1/0/all/0/1\">Karthik Gopalakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gella_S/0/1/0/all/0/1\">Spandana Gella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hakkani_Tur_D/0/1/0/all/0/1\">Dilek Hakkani-Tur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluation of HTR models without Ground Truth Material. (arXiv:2201.06170v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.06170","description":"<p>The evaluation of Handwritten Text Recognition (HTR) models during their\ndevelopment is straightforward: because HTR is a supervised problem, the usual\ndata split into training, validation, and test data sets allows the evaluation\nof models in terms of accuracy or error rates. However, the evaluation process\nbecomes tricky as soon as we switch from development to application. A\ncompilation of a new (and forcibly smaller) ground truth (GT) from a sample of\nthe data that we want to apply the model on and the subsequent evaluation of\nmodels thereon only provides hints about the quality of the recognised text, as\ndo confidence scores (if available) the models return. Moreover, if we have\nseveral models at hand, we face a model selection problem since we want to\nobtain the best possible result during the application phase. This calls for\nGT-free metrics to select the best model, which is why we (re-)introduce and\ncompare different metrics, from simple, lexicon-based to more elaborate ones\nusing standard language models and masked language models (MLM). We show that\nMLM-based evaluation can compete with lexicon-based methods, with the advantage\nthat large and multilingual transformers are readily available, thus making\ncompiling lexical resources for other metrics superfluous.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Strobel_P/0/1/0/all/0/1\">Phillip Benjamin Str&#xf6;bel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clematide_S/0/1/0/all/0/1\">Simon Clematide</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Volk_M/0/1/0/all/0/1\">Martin Volk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwitter_R/0/1/0/all/0/1\">Raphael Schwitter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hodel_T/0/1/0/all/0/1\">Tobias Hodel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schoch_D/0/1/0/all/0/1\">David Schoch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Table Pre-training: A Survey on Model Architectures, Pre-training Objectives, and Downstream Tasks. (arXiv:2201.09745v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.09745","description":"<p>Since a vast number of tables can be easily collected from web pages,\nspreadsheets, PDFs, and various other document types, a flurry of table\npre-training frameworks have been proposed following the success of text and\nimages, and they have achieved new state-of-the-arts on various tasks such as\ntable question answering, table type recognition, column relation\nclassification, table search, formula prediction, etc. To fully use the\nsupervision signals in unlabeled tables, a variety of pre-training objectives\nhave been designed and evaluated, for example, denoising cell values,\npredicting numerical relationships, and implicitly executing SQLs. And to best\nleverage the characteristics of (semi-)structured tables, various tabular\nlanguage models, particularly with specially-designed attention mechanisms,\nhave been explored. Since tables usually appear and interact with free-form\ntext, table pre-training usually takes the form of table-text joint\npre-training, which attracts significant research interests from multiple\ndomains. This survey aims to provide a comprehensive review of different model\ndesigns, pre-training objectives, and downstream tasks for table pre-training,\nand we further share our thoughts and vision on existing challenges and future\nopportunities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Haoyu Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zhoujun Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xinyi He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Mengyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1\">Anda Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_F/0/1/0/all/0/1\">Fan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Ao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Shi Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongmei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards a Broad Coverage Named Entity Resource: A Data-Efficient Approach for Many Diverse Languages. (arXiv:2201.12219v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.12219","description":"<p>Parallel corpora are ideal for extracting a multilingual named entity (MNE)\nresource, i.e., a dataset of names translated into multiple languages. Prior\nwork on extracting MNE datasets from parallel corpora required resources such\nas large monolingual corpora or word aligners that are unavailable or perform\npoorly for underresourced languages. We present CLC-BN, a new method for\ncreating an MNE resource, and apply it to the Parallel Bible Corpus, a corpus\nof more than 1000 languages. CLC-BN learns a neural transliteration model from\nparallel-corpus statistics, without requiring any other bilingual resources,\nword aligners, or seed data. Experimental results show that CLC-BN clearly\noutperforms prior work. We release an MNE resource for 1340 languages and\ndemonstrate its effectiveness in two downstream tasks: knowledge graph\naugmentation and bilingual lexicon induction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Severini_S/0/1/0/all/0/1\">Silvia Severini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Imani_A/0/1/0/all/0/1\">Ayyoob Imani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dufter_P/0/1/0/all/0/1\">Philipp Dufter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Benchmark Corpus for the Detection of Automatically Generated Text in Academic Publications. (arXiv:2202.02013v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.02013","description":"<p>Automatic text generation based on neural language models has achieved\nperformance levels that make the generated text almost indistinguishable from\nthose written by humans. Despite the value that text generation can have in\nvarious applications, it can also be employed for malicious tasks. The\ndiffusion of such practices represent a threat to the quality of academic\npublishing. To address these problems, we propose in this paper two datasets\ncomprised of artificially generated research content: a completely synthetic\ndataset and a partial text substitution dataset. In the first case, the content\nis completely generated by the GPT-2 model after a short prompt extracted from\noriginal papers. The partial or hybrid dataset is created by replacing several\nsentences of abstracts with sentences that are generated by the Arxiv-NLP\nmodel. We evaluate the quality of the datasets comparing the generated texts to\naligned original texts using fluency metrics such as BLEU and ROUGE. The more\nnatural the artificial texts seem, the more difficult they are to detect and\nthe better is the benchmark. We also evaluate the difficulty of the task of\ndistinguishing original from generated text by using state-of-the-art\nclassification models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liyanage_V/0/1/0/all/0/1\">Vijini Liyanage</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buscaldi_D/0/1/0/all/0/1\">Davide Buscaldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nazarenko_A/0/1/0/all/0/1\">Adeline Nazarenko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Generalizable Semantic Product Search by Text Similarity Pre-training on Search Click Logs. (arXiv:2204.05231v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2204.05231","description":"<p>Recently, semantic search has been successfully applied to e-commerce product\nsearch and the learned semantic space(s) for query and product encoding are\nexpected to generalize to unseen queries or products. Yet, whether\ngeneralization can conveniently emerge has not been thoroughly studied in the\ndomain thus far. In this paper, we examine several general-domain and\ndomain-specific pre-trained Roberta variants and discover that general-domain\nfine-tuning does not help generalization, which aligns with the discovery of\nprior art. Proper domain-specific fine-tuning with clickstream data can lead to\nbetter model generalization, based on a bucketed analysis of a publicly\navailable manual annotated query-product pair da\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Weiyi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_T/0/1/0/all/0/1\">Tianchuan Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schroeder_B/0/1/0/all/0/1\">Benjamin Schroeder</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beam Decoding with Controlled Patience. (arXiv:2204.05424v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.05424","description":"<p>Text generation with beam search has proven successful in a wide range of\napplications. The commonly-used implementation of beam decoding follows a first\ncome, first served heuristic: it keeps a set of already completed sequences\nover time steps and stops when the size of this set reaches the beam size. We\nintroduce a patience factor, a simple modification to this decoding algorithm,\nthat generalizes the stopping criterion and provides flexibility to the depth\nof search. Extensive empirical results demonstrate that the patience factor\nimproves decoding performance of strong pretrained models on news text\nsummarization and machine translation over diverse language pairs, with a\nnegligible inference slowdown. Our approach only modifies one line of code and\ncan be thus readily incorporated in any implementation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kasai_J/0/1/0/all/0/1\">Jungo Kasai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sakaguchi_K/0/1/0/all/0/1\">Keisuke Sakaguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bras_R/0/1/0/all/0/1\">Ronan Le Bras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1\">Dragomir Radev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"XDBERT: Distilling Visual Information to BERT from Cross-Modal Systems to Improve Language Understanding. (arXiv:2204.07316v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.07316","description":"<p>Transformer-based models are widely used in natural language understanding\n(NLU) tasks, and multimodal transformers have been effective in visual-language\ntasks. This study explores distilling visual information from pretrained\nmultimodal transformers to pretrained language encoders. Our framework is\ninspired by cross-modal encoders' success in visual-language tasks while we\nalter the learning objective to cater to the language-heavy characteristics of\nNLU. After training with a small number of extra adapting steps and finetuned,\nthe proposed XDBERT (cross-modal distilled BERT) outperforms pretrained-BERT in\ngeneral language understanding evaluation (GLUE), situations with adversarial\ngenerations (SWAG) benchmarks, and readability benchmarks. We analyze the\nperformance of XDBERT on GLUE to show that the improvement is likely visually\ngrounded.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hsu_C/0/1/0/all/0/1\">Chan-Jan Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsao_Y/0/1/0/all/0/1\">Yu Tsao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MoEBERT: from BERT to Mixture-of-Experts via Importance-Guided Adaptation. (arXiv:2204.07675v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.07675","description":"<p>Pre-trained language models have demonstrated superior performance in various\nnatural language processing tasks. However, these models usually contain\nhundreds of millions of parameters, which limits their practicality because of\nlatency requirements in real-world applications. Existing methods train small\ncompressed models via knowledge distillation. However, performance of these\nsmall models drops significantly compared with the pre-trained models due to\ntheir reduced model capacity. We propose MoEBERT, which uses a\nMixture-of-Experts structure to increase model capacity and inference speed. We\ninitialize MoEBERT by adapting the feed-forward neural networks in a\npre-trained model into multiple experts. As such, representation power of the\npre-trained model is largely retained. During inference, only one of the\nexperts is activated, such that speed can be improved. We also propose a\nlayer-wise distillation method to train MoEBERT. We validate the efficiency and\neffectiveness of MoEBERT on natural language understanding and question\nanswering tasks. Results show that the proposed method outperforms existing\ntask-specific distillation algorithms. For example, our method outperforms\nprevious approaches by over 2% on the MNLI (mismatched) dataset. Our code is\npublicly available at https://github.com/SimiaoZuo/MoEBERT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zuo_S/0/1/0/all/0/1\">Simiao Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qingru Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1\">Chen Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1\">Pengcheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tuo Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WikiOmnia: generative QA corpus on the whole Russian Wikipedia. (arXiv:2204.08009v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.08009","description":"<p>The General QA field has been developing the methodology referencing the\nStanford Question answering dataset (SQuAD) as the significant benchmark.\nHowever, compiling factual questions is accompanied by time- and\nlabour-consuming annotation, limiting the training data's potential size. We\npresent the WikiOmnia dataset, a new publicly available set of QA-pairs and\ncorresponding Russian Wikipedia article summary sections, composed with a fully\nautomated generative pipeline. The dataset includes every available article\nfrom Wikipedia for the Russian language. The WikiOmnia pipeline is available\nopen-source and is also tested for creating SQuAD-formatted QA on other\ndomains, like news texts, fiction, and social media. The resulting dataset\nincludes two parts: raw data on the whole Russian Wikipedia (7,930,873 QA pairs\nwith paragraphs for ruGPT-3 XL and 7,991,040 QA pairs with paragraphs for\nruT5-large) and cleaned data with strict automatic verification (over 160,000\nQA pairs with paragraphs for ruGPT-3 XL and over 3,400,000 QA pairs with\nparagraphs for ruT5-large).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pisarevskaya_D/0/1/0/all/0/1\">Dina Pisarevskaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shavrina_T/0/1/0/all/0/1\">Tatiana Shavrina</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TranS: Transition-based Knowledge Graph Embedding with Synthetic Relation Representation. (arXiv:2204.08401v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.08401","description":"<p>Knowledge graph embedding (KGE) aims to learn continuous vectors of relations\nand entities in knowledge graph. Recently, transition-based KGE methods have\nachieved promising performance, where the single relation vector learns to\ntranslate head entity to tail entity. However, this scoring pattern is not\nsuitable for complex scenarios where the same entity pair has different\nrelations. Previous models usually focus on the improvement of entity\nrepresentation for 1-to-N, N-to-1 and N-to-N relations, but ignore the single\nrelation vector. In this paper, we propose a novel transition-based method,\nTranS, for knowledge graph embedding. The single relation vector in traditional\nscoring patterns is replaced with synthetic relation representation, which can\nsolve these issues effectively and efficiently. Experiments on a large\nknowledge graph dataset, ogbl-wikikg2, show that our model achieves\nstate-of-the-art results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xuanyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1\">Qing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Dongliang Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Retrieval Enhanced Data Augmentation for Question Answering on Privacy Policies. (arXiv:2204.08952v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.08952","description":"<p>Prior studies in privacy policies frame the question answering (QA) tasks as\nidentifying the most relevant text segment or a list of sentences from the\npolicy document for a user query. However, annotating such a dataset is\nchallenging as it requires specific domain expertise (e.g., law academics).\nEven if we manage a small-scale one, a bottleneck that remains is that the\nlabeled data are heavily imbalanced (only a few segments are relevant)\n--limiting the gain in this domain. Therefore, in this paper, we develop a\nnovel data augmentation framework based on ensembling retriever models that\ncaptures the relevant text segments from unlabeled policy documents and expand\nthe positive examples in the training set. In addition, to improve the\ndiversity and quality of the augmented data, we leverage multiple pre-trained\nlanguage models (LMs) and cascaded them with noise reduction oracles. Using our\naugmented data on the PrivacyQA benchmark, we elevate the existing baseline by\na large margin (10% F1) and achieve a new state-of-the-art F1 score of 50%. Our\nablation studies provide further insights into the effectiveness of our\napproach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Parvez_M/0/1/0/all/0/1\">Md Rizwan Parvez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chi_J/0/1/0/all/0/1\">Jianfeng Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_W/0/1/0/all/0/1\">Wasi Uddin Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yuan Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating User Radicalization: A Novel Dataset for Identifying Fine-Grained Temporal Shifts in Opinion. (arXiv:2204.10190v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.10190","description":"<p>There is an increasing need for the ability to model fine-grained opinion\nshifts of social media users, as concerns about the potential polarizing social\neffects increase. However, the lack of publicly available datasets that are\nsuitable for the task presents a major challenge. In this paper, we introduce\nan innovative annotated dataset for modeling subtle opinion fluctuations and\ndetecting fine-grained stances. The dataset includes a sufficient amount of\nstance polarity and intensity labels per user over time and within entire\nconversational threads, thus making subtle opinion fluctuations detectable both\nin long term and in short term. All posts are annotated by non-experts and a\nsignificant portion of the data is also annotated by experts. We provide a\nstrategy for recruiting suitable non-experts. Our analysis of the\ninter-annotator agreements shows that the resulting annotations obtained from\nthe majority vote of the non-experts are of comparable quality to the\nannotations of the experts. We provide analyses of the stance evolution in\nshort term and long term levels, a comparison of language usage between users\nwith vacillating and resolute attitudes, and fine-grained stance detection\nbaselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sakketou_F/0/1/0/all/0/1\">Flora Sakketou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lahnala_A/0/1/0/all/0/1\">Allison Lahnala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vogel_L/0/1/0/all/0/1\">Liane Vogel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flek_L/0/1/0/all/0/1\">Lucie Flek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MuCGEC: a Multi-Reference Multi-Source Evaluation Dataset for Chinese Grammatical Error Correction. (arXiv:2204.10994v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.10994","description":"<p>This paper presents MuCGEC, a multi-reference multi-source evaluation dataset\nfor Chinese Grammatical Error Correction (CGEC), consisting of 7,063 sentences\ncollected from three different Chinese-as-a-Second-Language (CSL) learner\nsources. Each sentence has been corrected by three annotators, and their\ncorrections are meticulously reviewed by an expert, resulting in 2.3 references\nper sentence. We conduct experiments with two mainstream CGEC models, i.e., the\nsequence-to-sequence (Seq2Seq) model and the sequence-to-edit (Seq2Edit) model,\nboth enhanced with large pretrained language models (PLMs), achieving\ncompetitive benchmark performance on previous and our datasets. We also discuss\nCGEC evaluation methodologies, including the effect of multiple references and\nusing a char-based metric. Our annotation guidelines, data, and code are\navailable at \\url{https://github.com/HillZhang1999/MuCGEC}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenghua Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_Z/0/1/0/all/0/1\">Zuyi Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiacheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PLOD: An Abbreviation Detection Dataset for Scientific Documents. (arXiv:2204.12061v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.12061","description":"<p>The detection and extraction of abbreviations from unstructured texts can\nhelp to improve the performance of Natural Language Processing tasks, such as\nmachine translation and information retrieval. However, in terms of publicly\navailable datasets, there is not enough data for training\ndeep-neural-networks-based models to the point of generalising well over data.\nThis paper presents PLOD, a large-scale dataset for abbreviation detection and\nextraction that contains 160k+ segments automatically annotated with\nabbreviations and their long forms. We performed manual validation over a set\nof instances and a complete automatic validation for this dataset. We then used\nit to generate several baseline models for detecting abbreviations and long\nforms. The best models achieved an F1-score of 0.92 for abbreviations and 0.89\nfor detecting their corresponding long forms. We release this dataset along\nwith our code and all the models publicly in\nhttps://github.com/surrey-nlp/PLOD-AbbreviationDetection\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zilio_L/0/1/0/all/0/1\">Leonardo Zilio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saadany_H/0/1/0/all/0/1\">Hadeel Saadany</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_P/0/1/0/all/0/1\">Prashant Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanojia_D/0/1/0/all/0/1\">Diptesh Kanojia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orasan_C/0/1/0/all/0/1\">Constantin Or&#x103;san</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Science Checker: Extractive-Boolean Question Answering For Scientific Fact Checking. (arXiv:2204.12263v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.12263","description":"<p>With the explosive growth of scientific publications, making the synthesis of\nscientific knowledge and fact checking becomes an increasingly complex task. In\nthis paper, we propose a multi-task approach for verifying the scientific\nquestions based on a joint reasoning from facts and evidence in research\narticles. We propose an intelligent combination of (1) an automatic information\nsummarization and (2) a Boolean Question Answering which allows to generate an\nanswer to a scientific question from only extracts obtained after\nsummarization. Thus on a given topic, our proposed approach conducts structured\ncontent modeling based on paper abstracts to answer a scientific question while\nhighlighting texts from paper that discuss the topic. We based our final system\non an end-to-end Extractive Question Answering (EQA) combined with a three\noutputs classification model to perform in-depth semantic understanding of a\nquestion to illustrate the aggregation of multiple responses. With our light\nand fast proposed architecture, we achieved an average error rate of 4% and a\nF1-score of 95.6%. Our results are supported via experiments with two QA models\n(BERT, RoBERTa) over 3 Million Open Access (OA) articles in the medical and\nhealth domains on Europe PMC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rakotoson_L/0/1/0/all/0/1\">Lo&#xef;c Rakotoson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Letaillieur_C/0/1/0/all/0/1\">Charles Letaillieur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Massip_S/0/1/0/all/0/1\">Sylvain Massip</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laleye_F/0/1/0/all/0/1\">Fr&#xe9;jus Laleye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Label Search for Zero-Shot Multi-Lingual Extractive Summarization. (arXiv:2204.13512v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.13512","description":"<p>In zero-shot multilingual extractive text summarization, a model is typically\ntrained on English summarization dataset and then applied on summarization\ndatasets of other languages. Given English gold summaries and documents,\nsentence-level labels for extractive summarization are usually generated using\nheuristics. However, these monolingual labels created on English datasets may\nnot be optimal on datasets of other languages, for that there is the syntactic\nor semantic discrepancy between different languages. In this way, it is\npossible to translate the English dataset to other languages and obtain\ndifferent sets of labels again using heuristics. To fully leverage the\ninformation of these different sets of labels, we propose NLSSum (Neural Label\nSearch for Summarization), which jointly learns hierarchical weights for these\ndifferent sets of labels together with our summarization model. We conduct\nmultilingual zero-shot summarization experiments on MLSUM and WikiLingua\ndatasets, and we achieve state-of-the-art results using both human and\nautomatic evaluations across these two datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1\">Ruipeng Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xingxing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yanan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-05-01T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","dc":"http://purl.org/dc/elements/1.1/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Federated Learning: Balancing the Thin Line Between Data Intelligence and Privacy. (arXiv:2204.13697v1 [cs.LG])","link":"http://arxiv.org/abs/2204.13697","description":"<p>Federated learning holds great promise in learning from fragmented sensitive\ndata and has revolutionized how machine learning models are trained. This\narticle provides a systematic overview and detailed taxonomy of federated\nlearning. We investigate the existing security challenges in federated learning\nand provide a comprehensive overview of established defense techniques for data\npoisoning, inference attacks, and model poisoning attacks. The work also\npresents an overview of current training challenges for federated learning,\nfocusing on handling non-i.i.d. data, high dimensionality issues, and\nheterogeneous architecture, and discusses several solutions for the associated\nchallenges. Finally, we discuss the remaining challenges in managing federated\nlearning training and suggest focused research directions to address the open\nquestions. Potential candidate areas for federated learning, including IoT\necosystem, healthcare applications, are discussed with a particular focus on\nbanking and financial domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mathews_S/0/1/0/all/0/1\">Sherin Mary Mathews</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Assefa_S/0/1/0/all/0/1\">Samuel A. Assefa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Channel Pruned YOLOv5-based Deep Learning Approach for Rapid and Accurate Outdoor Obstacles Detection. (arXiv:2204.13699v1 [cs.CV])","link":"http://arxiv.org/abs/2204.13699","description":"<p>One-stage algorithm have been widely used in target detection systems that\nneed to be trained with massive data. Most of them perform well both in\nreal-time and accuracy. However, due to their convolutional structure, they\nneed more computing power and greater memory consumption. Hence, we applied\npruning strategy to target detection networks to reduce the number of\nparameters and the size of model. To demonstrate the practicality of the\npruning method, we select the YOLOv5 model for experiments and provide a data\nset of outdoor obstacles to show the effect of model. In this specific data\nset, in the best circumstances, the volume of the network model is reduced by\n49.7% compared with the original model, and the reasoning time is reduced by\n52.5%. Meanwhile, it also uses data processing methods to compensate for the\ndrop in accuracy caused by pruning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zeqian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_K/0/1/0/all/0/1\">Keyu Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhibin Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Coupling Deep Imputation with Multitask Learning for Downstream Tasks on Genomics Data. (arXiv:2204.13705v1 [q-bio.GN])","link":"http://arxiv.org/abs/2204.13705","description":"<p>Genomics data such as RNA gene expression, methylation and micro RNA\nexpression are valuable sources of information for various clinical predictive\ntasks. For example, predicting survival outcomes, cancer histology type and\nother patients' related information is possible using not only clinical data\nbut molecular data as well. Moreover, using these data sources together, for\nexample in multitask learning, can boost the performance. However, in practice,\nthere are many missing data points which leads to significantly lower patient\nnumbers when analysing full cases, which in our setting refers to all\nmodalities being present.\n</p>\n<p>In this paper we investigate how imputing data with missing values using deep\nlearning coupled with multitask learning can help to reach state-of-the-art\nperformance results using combined genomics modalities, RNA, micro RNA and\nmethylation. We propose a generalised deep imputation method to impute values\nwhere a patient has all modalities present except one. Interestingly enough,\ndeep imputation alone outperforms multitask learning alone for the\nclassification and regression tasks across most combinations of modalities. In\ncontrast, when using all modalities for survival prediction we observe that\nmultitask learning alone outperforms deep imputation alone with statistical\nsignificance (adjusted p-value 0.03). Thus, both approaches are complementary\nwhen optimising performance for downstream predictive tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Peacock_S/0/1/0/all/0/1\">Sophie Peacock</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Jacob_E/0/1/0/all/0/1\">Etai Jacob</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Burlutskiy_N/0/1/0/all/0/1\">Nikolay Burlutskiy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning cosmology and clustering with cosmic graphs. (arXiv:2204.13713v1 [astro-ph.CO])","link":"http://arxiv.org/abs/2204.13713","description":"<p>We train deep learning models on thousands of galaxy catalogues from the\nstate-of-the-art hydrodynamic simulations of the CAMELS project to perform\nregression and inference. We employ Graph Neural Networks (GNNs), architectures\ndesigned to work with irregular and sparse data, like the distribution of\ngalaxies in the Universe. We first show that GNNs can learn to compute the\npower spectrum of galaxy catalogues with a few percent accuracy. We then train\nGNNs to perform likelihood-free inference at the galaxy-field level. Our models\nare able to infer the value of $\\Omega_{\\rm m}$ with a $\\sim12\\%-13\\%$ accuracy\njust from the positions of $\\sim1000$ galaxies in a volume of $(25~h^{-1}{\\rm\nMpc})^3$ at $z=0$ while accounting for astrophysical uncertainties as modelled\nin CAMELS. Incorporating information from galaxy properties, such as stellar\nmass, stellar metallicity, and stellar radius, increases the accuracy to\n$4\\%-8\\%$. Our models are built to be translational and rotational invariant,\nand they can extract information from any scale larger than the minimum\ndistance between two galaxies. However, our models are not completely robust:\ntesting on simulations run with a different subgrid physics than the ones used\nfor training does not yield as accurate results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/astro-ph/1/au:+Villanueva_Domingo_P/0/1/0/all/0/1\">Pablo Villanueva-Domingo</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Villaescusa_Navarro_F/0/1/0/all/0/1\">Francisco Villaescusa-Navarro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"One Model to Synthesize Them All: Multi-contrast Multi-scale Transformer for Missing Data Imputation. (arXiv:2204.13738v1 [eess.IV])","link":"http://arxiv.org/abs/2204.13738","description":"<p>Multi-contrast magnetic resonance imaging (MRI) is widely used in clinical\npractice as each contrast provides complementary information. However, the\navailability of each contrast may vary amongst patients in reality. This poses\nchallenges to both radiologists and automated image analysis algorithms. A\ngeneral approach for tackling this problem is missing data imputation, which\naims to synthesize the missing contrasts from existing ones. While several\nconvolutional neural network (CNN) based algorithms have been proposed, they\nsuffer from the fundamental limitations of CNN models, such as requirement for\nfixed numbers of input and output channels, inability to capture long-range\ndependencies, and lack of interpretability. In this paper, we formulate missing\ndata imputation as a sequence-to-sequence learning problem and propose a\nmulti-contrast multi-scale Transformer (MMT), which can take any subset of\ninput contrasts and synthesize those that are missing. MMT consists of a\nmulti-scale Transformer encoder that builds hierarchical representations of\ninputs combined with a multi-scale Transformer decoder that generates the\noutputs in a coarse-to-fine fashion. Thanks to the proposed multi-contrast Swin\nTransformer blocks, it can efficiently capture intra- and inter-contrast\ndependencies for accurate image synthesis. Moreover, MMT is inherently\ninterpretable. It allows us to understand the importance of each input contrast\nin different regions by analyzing the in-built attention maps of Transformer\nblocks in the decoder. Extensive experiments on two large-scale multi-contrast\nMRI datasets demonstrate that MMT outperforms the state-of-the-art methods\nquantitatively and qualitatively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jiang Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pasumarthi_S/0/1/0/all/0/1\">Srivathsa Pasumarthi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Duffy_B/0/1/0/all/0/1\">Ben Duffy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gong_E/0/1/0/all/0/1\">Enhao Gong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zaharchuk_G/0/1/0/all/0/1\">Greg Zaharchuk</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Datta_K/0/1/0/all/0/1\">Keshav Datta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Split for Automatic Bias Detection. (arXiv:2204.13749v1 [cs.LG])","link":"http://arxiv.org/abs/2204.13749","description":"<p>Classifiers are biased when trained on biased datasets. As a remedy, we\npropose Learning to Split (ls), an algorithm for automatic bias detection.\nGiven a dataset with input-label pairs, ls learns to split this dataset so that\npredictors trained on the training split generalize poorly to the testing\nsplit. This performance gap provides a proxy for measuring the degree of bias\nin the learned features and can therefore be used to reduce biases. Identifying\nnon-generalizable splits is challenging as we don't have any explicit\nannotations about how to split. In this work, we show that the prediction\ncorrectness of the testing example can be used as a source of weak supervision:\ngeneralization performance will drop if we move examples that are predicted\ncorrectly away from the testing split, leaving only those that are\nmispredicted. We evaluate our approach on Beer Review, Waterbirds, CelebA and\nMNLI. Empirical results show that ls is able to generate astonishingly\nchallenging splits that correlate with human-identified biases. Moreover, we\ndemonstrate that combining robust learning algorithms (such as group DRO) with\nsplits identified by ls enables automatic de-biasing. Compared with previous\nstate-of-the-arts, we substantially improves the worst-group performance (23.4%\non average) when the source of biases is unknown during training and\nvalidation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bao_Y/0/1/0/all/0/1\">Yujia Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barzilay_R/0/1/0/all/0/1\">Regina Barzilay</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Depth Estimation with Simplified Transformer. (arXiv:2204.13791v1 [cs.CV])","link":"http://arxiv.org/abs/2204.13791","description":"<p>Transformer and its variants have shown state-of-the-art results in many\nvision tasks recently, ranging from image classification to dense prediction.\nDespite of their success, limited work has been reported on improving the model\nefficiency for deployment in latency-critical applications, such as autonomous\ndriving and robotic navigation. In this paper, we aim at improving upon the\nexisting transformers in vision, and propose a method for self-supervised\nmonocular Depth Estimation with Simplified Transformer (DEST), which is\nefficient and particularly suitable for deployment on GPU-based platforms.\nThrough strategic design choices, our model leads to significant reduction in\nmodel size, complexity, as well as inference latency, while achieving superior\naccuracy as compared to state-of-the-art. We also show that our design\ngeneralize well to other dense prediction task without bells and whistles.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">John Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_L/0/1/0/all/0/1\">Le An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dixit_A/0/1/0/all/0/1\">Anurag Dixit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koo_J/0/1/0/all/0/1\">Jinkyu Koo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Su Inn Park</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A very preliminary analysis of DALL-E 2. (arXiv:2204.13807v1 [cs.CV])","link":"http://arxiv.org/abs/2204.13807","description":"<p>The DALL-E 2 system generates original synthetic images corresponding to an\ninput text as caption. We report here on the outcome of fourteen tests of this\nsystem designed to assess its common sense, reasoning and ability to understand\ncomplex texts. All of our prompts were intentionally much more challenging than\nthe typical ones that have been showcased in recent weeks. Nevertheless, for 5\nout of the 14 prompts, at least one of the ten images fully satisfied our\nrequests. On the other hand, on no prompt did all of the ten images satisfy our\nrequests.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Marcus_G/0/1/0/all/0/1\">Gary Marcus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davis_E/0/1/0/all/0/1\">Ernest Davis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aaronson_S/0/1/0/all/0/1\">Scott Aaronson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analysing the Influence of Attack Configurations on the Reconstruction of Medical Images in Federated Learning. (arXiv:2204.13808v1 [eess.IV])","link":"http://arxiv.org/abs/2204.13808","description":"<p>The idea of federated learning is to train deep neural network models\ncollaboratively and share them with multiple participants without exposing\ntheir private training data to each other. This is highly attractive in the\nmedical domain due to patients' privacy records. However, a recently proposed\nmethod called Deep Leakage from Gradients enables attackers to reconstruct data\nfrom shared gradients. This study shows how easy it is to reconstruct images\nfor different data initialization schemes and distance measures. We show how\ndata and model architecture influence the optimal choice of initialization\nscheme and distance measure configurations when working with single images. We\ndemonstrate that the choice of initialization scheme and distance measure can\nsignificantly increase convergence speed and quality. Furthermore, we find that\nthe optimal attack configuration depends largely on the nature of the target\nimage distribution and the complexity of the model architecture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Dahlgaard_M/0/1/0/all/0/1\">Mads Emil Dahlgaard</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jorgensen_M/0/1/0/all/0/1\">Morten Wehlast J&#xf8;rgensen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fuglsang_N/0/1/0/all/0/1\">Niels Asp Fuglsang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nassar_H/0/1/0/all/0/1\">Hiba Nassar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning-based Automatic Player Identification and Logging in American Football Videos. (arXiv:2204.13809v1 [cs.CV])","link":"http://arxiv.org/abs/2204.13809","description":"<p>American football games attract significant worldwide attention every year.\nGame analysis systems generate crucial information that can help analyze the\ngames by providing fans and coaches with a convenient means to track and\nevaluate player performance. Identifying participating players in each play is\nalso important for the video indexing of player participation per play.\nProcessing football game video presents challenges such as crowded setting,\ndistorted objects, and imbalanced data for identifying players, especially\njersey numbers. In this work, we propose a deep learning-based football video\nanalysis system to automatically track players and index their participation\nper play. It is a multi-stage network design to highlight area of interest and\nidentify jersey number information with high accuracy. First, we utilize an\nobject detection network, a detection transformer, to tackle the player\ndetection problem in crowded context. Second, we identify players using jersey\nnumber recognition with a secondary convolutional neural network, then\nsynchronize it with a game clock subsystem. Finally, the system outputs a\ncomplete log in a database for play indexing. We demonstrate the effectiveness\nand reliability of player identification and the logging system by analyzing\nthe qualitative and quantitative results on football videos. The proposed\nsystem shows great potential for implementation in and analysis of football\nbroadcast video.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hongshan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aderon_C/0/1/0/all/0/1\">Colin Aderon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wagon_N/0/1/0/all/0/1\">Noah Wagon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Huapu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+MacCall_S/0/1/0/all/0/1\">Steven MacCall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_Y/0/1/0/all/0/1\">Yu Gan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding the impact of image and input resolution on deep digital pathology patch classifiers. (arXiv:2204.13829v1 [cs.CV])","link":"http://arxiv.org/abs/2204.13829","description":"<p>We consider annotation efficient learning in Digital Pathology (DP), where\nexpert annotations are expensive and thus scarce. We explore the impact of\nimage and input resolution on DP patch classification performance. We use two\ncancer patch classification datasets PCam and CRC, to validate the results of\nour study. Our experiments show that patch classification performance can be\nimproved by manipulating both the image and input resolution in\nannotation-scarce and annotation-rich environments. We show a positive\ncorrelation between the image and input resolution and the patch classification\naccuracy on both datasets. By exploiting the image and input resolution, our\nfinal model trained on &lt; 1% of data performs equally well compared to the model\ntrained on 100% of data in the original image resolution on the PCam dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Teh_E/0/1/0/all/0/1\">Eu Wern Teh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taylor_G/0/1/0/all/0/1\">Graham W. Taylor</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Noise-reducing attention cross fusion learning transformer for histological image classification of osteosarcoma. (arXiv:2204.13838v1 [eess.IV])","link":"http://arxiv.org/abs/2204.13838","description":"<p>The degree of malignancy of osteosarcoma and its tendency to\nmetastasize/spread mainly depend on the pathological grade (determined by\nobserving the morphology of the tumor under a microscope). The purpose of this\nstudy is to use artificial intelligence to classify osteosarcoma histological\nimages and to assess tumor survival and necrosis, which will help doctors\nreduce their workload, improve the accuracy of osteosarcoma cancer detection,\nand make a better prognosis for patients. The study proposes a typical\ntransformer image classification framework by integrating noise reduction\nconvolutional autoencoder and feature cross fusion learning (NRCA-FCFL) to\nclassify osteosarcoma histological images. Noise reduction convolutional\nautoencoder could well denoise histological images of osteosarcoma, resulting\nin more pure images for osteosarcoma classification. Moreover, we introduce\nfeature cross fusion learning, which integrates two scale image patches, to\nsufficiently explore their interactions by using additional classification\ntokens. As a result, a refined fusion feature is generated, which is fed to the\nresidual neural network for label predictions. We conduct extensive experiments\nto evaluate the performance of the proposed approach. The experimental results\ndemonstrate that our method outperforms the traditional and deep learning\napproaches on various evaluation metrics, with an accuracy of 99.17% to support\nosteosarcoma diagnosis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Pan_L/0/1/0/all/0/1\">Liangrui Pan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1\">Hetian Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_L/0/1/0/all/0/1\">Lian Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ji_B/0/1/0/all/0/1\">Boya Ji</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_M/0/1/0/all/0/1\">Mingting Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chongcheawchamnan_M/0/1/0/all/0/1\">Mitchai Chongcheawchamnan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yuan_J/0/1/0/all/0/1\">Jin Yuan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Peng_S/0/1/0/all/0/1\">Shaoliang Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GenDR: A Generalized Differentiable Renderer. (arXiv:2204.13845v1 [cs.CV])","link":"http://arxiv.org/abs/2204.13845","description":"<p>In this work, we present and study a generalized family of differentiable\nrenderers. We discuss from scratch which components are necessary for\ndifferentiable rendering and formalize the requirements for each component. We\ninstantiate our general differentiable renderer, which generalizes existing\ndifferentiable renderers like SoftRas and DIB-R, with an array of different\nsmoothing distributions to cover a large spectrum of reasonable settings. We\nevaluate an array of differentiable renderer instantiations on the popular\nShapeNet 3D reconstruction benchmark and analyze the implications of our\nresults. Surprisingly, the simple uniform distribution yields the best overall\nresults when averaged over 13 classes; in general, however, the optimal choice\nof distribution heavily depends on the task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Petersen_F/0/1/0/all/0/1\">Felix Petersen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldluecke_B/0/1/0/all/0/1\">Bastian Goldluecke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borgelt_C/0/1/0/all/0/1\">Christian Borgelt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deussen_O/0/1/0/all/0/1\">Oliver Deussen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Goldilocks-curriculum Domain Randomization and Fractal Perlin Noise with Application to Sim2Real Pneumonia Lesion Detection. (arXiv:2204.13849v1 [cs.CV])","link":"http://arxiv.org/abs/2204.13849","description":"<p>A computer-aided detection (CAD) system based on machine learning is expected\nto assist radiologists in making a diagnosis. It is desirable to build CAD\nsystems for the various types of diseases accumulating daily in a hospital. An\nobstacle in developing a CAD system for a disease is that the number of medical\nimages is typically too small to improve the performance of the machine\nlearning model. In this paper, we aim to explore ways to address this problem\nthrough a sim2real transfer approach in medical image fields. To build a\nplatform to evaluate the performance of sim2real transfer methods in the field\nof medical imaging, we construct a benchmark dataset that consists of $101$\nchest X-images with difficult-to-identify pneumonia lesions judged by an\nexperienced radiologist and a simulator based on fractal Perlin noise and the\nX-ray principle for generating pseudo pneumonia lesions. We then develop a\nnovel domain randomization method, called Goldilocks-curriculum domain\nrandomization (GDR) and evaluate our method in this platform.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Suzuki_T/0/1/0/all/0/1\">Takahiro Suzuki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanaoka_S/0/1/0/all/0/1\">Shouhei Hanaoka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sato_I/0/1/0/all/0/1\">Issei Sato</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COVID-Net US-X: Enhanced Deep Neural Network for Detection of COVID-19 Patient Cases from Convex Ultrasound Imaging Through Extended Linear-Convex Ultrasound Augmentation Learning. (arXiv:2204.13851v1 [eess.IV])","link":"http://arxiv.org/abs/2204.13851","description":"<p>As the global population continues to face significant negative impact by the\non-going COVID-19 pandemic, there has been an increasing usage of point-of-care\nultrasound (POCUS) imaging as a low-cost and effective imaging modality of\nchoice in the COVID-19 clinical workflow. A major barrier with widespread\nadoption of POCUS in the COVID-19 clinical workflow is the scarcity of expert\nclinicians that can interpret POCUS examinations, leading to considerable\ninterest in deep learning-driven clinical decision support systems to tackle\nthis challenge. A major challenge to building deep neural networks for COVID-19\nscreening using POCUS is the heterogeneity in the types of probes used to\ncapture ultrasound images (e.g., convex vs. linear probes), which can lead to\nvery different visual appearances. In this study, we explore the impact of\nleveraging extended linear-convex ultrasound augmentation learning on producing\nenhanced deep neural networks for COVID-19 assessment, where we conduct data\naugmentation on convex probe data alongside linear probe data that have been\ntransformed to better resemble convex probe data. Experimental results using an\nefficient deep columnar anti-aliased convolutional neural network designed via\na machined-driven design exploration strategy (which we name COVID-Net US-X)\nshow that the proposed extended linear-convex ultrasound augmentation learning\nsignificantly increases performance, with a gain of 5.1% in test accuracy and\n13.6% in AUC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zeng_E/0/1/0/all/0/1\">E. Zhixuan Zeng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Florea_A/0/1/0/all/0/1\">Adrian Florea</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wong_A/0/1/0/all/0/1\">Alexander Wong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Equine radiograph classification using deep convolutional neural networks. (arXiv:2204.13857v1 [cs.CV])","link":"http://arxiv.org/abs/2204.13857","description":"<p>Purpose: To assess the capability of deep convolutional neural networks to\nclassify anatomical location and projection from a series of 48 standard views\nof racehorse limbs.\n</p>\n<p>Materials and Methods: 9504 equine pre-import radiographs were used to train,\nvalidate, and test six deep learning architectures available as part of the\nopen source machine learning framework PyTorch.\n</p>\n<p>Results: ResNet-34 achieved a top-1 accuracy of 0.8408 and the majority (88%)\nof misclassification was because of wrong laterality. Class activation maps\nindicated that joint morphology drove the model decision.\n</p>\n<p>Conclusion: Deep convolutional neural networks are capable of classifying\nequine pre-import radiographs into the 48 standard views including moderate\ndiscrimination of laterality independent of side marker presence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Silva_R/0/1/0/all/0/1\">Raniere Gaia Costa da Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_A/0/1/0/all/0/1\">Ambika Prasad Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riggs_C/0/1/0/all/0/1\">Christopher Riggs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doube_M/0/1/0/all/0/1\">Michael Doube</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Where in the World is this Image? Transformer-based Geo-localization in the Wild. (arXiv:2204.13861v1 [cs.CV])","link":"http://arxiv.org/abs/2204.13861","description":"<p>Predicting the geographic location (geo-localization) from a single\nground-level RGB image taken anywhere in the world is a very challenging\nproblem. The challenges include huge diversity of images due to different\nenvironmental scenarios, drastic variation in the appearance of the same\nlocation depending on the time of the day, weather, season, and more\nimportantly, the prediction is made from a single image possibly having only a\nfew geo-locating cues. For these reasons, most existing works are restricted to\nspecific cities, imagery, or worldwide landmarks. In this work, we focus on\ndeveloping an efficient solution to planet-scale single-image geo-localization.\nTo this end, we propose TransLocator, a unified dual-branch transformer network\nthat attends to tiny details over the entire image and produces robust feature\nrepresentation under extreme appearance variations. TransLocator takes an RGB\nimage and its semantic segmentation map as inputs, interacts between its two\nparallel branches after each transformer layer, and simultaneously performs\ngeo-localization and scene recognition in a multi-task fashion. We evaluate\nTransLocator on four benchmark datasets - Im2GPS, Im2GPS3k, YFCC4k, YFCC26k and\nobtain 5.5%, 14.1%, 4.9%, 9.9% continent-level accuracy improvement over the\nstate-of-the-art. TransLocator is also validated on real-world test images and\nfound to be more effective than previous methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pramanick_S/0/1/0/all/0/1\">Shraman Pramanick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nowara_E/0/1/0/all/0/1\">Ewa M. Nowara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gleason_J/0/1/0/all/0/1\">Joshua Gleason</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castillo_C/0/1/0/all/0/1\">Carlos D. Castillo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chellappa_R/0/1/0/all/0/1\">Rama Chellappa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision-Language Pre-Training for Boosting Scene Text Detectors. (arXiv:2204.13867v1 [cs.CV])","link":"http://arxiv.org/abs/2204.13867","description":"<p>Recently, vision-language joint representation learning has proven to be\nhighly effective in various scenarios. In this paper, we specifically adapt\nvision-language joint learning for scene text detection, a task that\nintrinsically involves cross-modal interaction between the two modalities:\nvision and language, since text is the written form of language. Concretely, we\npropose to learn contextualized, joint representations through vision-language\npre-training, for the sake of enhancing the performance of scene text\ndetectors. Towards this end, we devise a pre-training architecture with an\nimage encoder, a text encoder and a cross-modal encoder, as well as three\npretext tasks: image-text contrastive learning (ITC), masked language modeling\n(MLM) and word-in-image prediction (WIP). The pre-trained model is able to\nproduce more informative representations with richer semantics, which could\nreadily benefit existing scene text detectors (such as EAST and PSENet) in the\ndown-stream text detection task. Extensive experiments on standard benchmarks\ndemonstrate that the proposed paradigm can significantly improve the\nperformance of various representative text detectors, outperforming previous\npre-training approaches. The code and pre-trained models will be publicly\nreleased.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Sibo Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_J/0/1/0/all/0/1\">Jianqiang Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhibo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jun Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_W/0/1/0/all/0/1\">Wenqing Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xiang Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_C/0/1/0/all/0/1\">Cong Yao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multiple Degradation and Reconstruction Network for Single Image Denoising via Knowledge Distillation. (arXiv:2204.13873v1 [cs.CV])","link":"http://arxiv.org/abs/2204.13873","description":"<p>Single image denoising (SID) has achieved significant breakthroughs with the\ndevelopment of deep learning. However, the proposed methods are often\naccompanied by plenty of parameters, which greatly limits their application\nscenarios. Different from previous works that blindly increase the depth of the\nnetwork, we explore the degradation mechanism of the noisy image and propose a\nlightweight Multiple Degradation and Reconstruction Network (MDRN) to\nprogressively remove noise. Meanwhile, we propose two novel Heterogeneous\nKnowledge Distillation Strategies (HMDS) to enable MDRN to learn richer and\nmore accurate features from heterogeneous models, which make it possible to\nreconstruct higher-quality denoised images under extreme conditions. Extensive\nexperiments show that our MDRN achieves favorable performance against other SID\nmodels with fewer parameters. Meanwhile, plenty of ablation studies demonstrate\nthat the introduced HMDS can improve the performance of tiny models or the\nmodel under high noise levels, which is extremely useful for related\napplications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juncheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hanhui Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_Q/0/1/0/all/0/1\">Qiaosi Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_F/0/1/0/all/0/1\">Faming Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_G/0/1/0/all/0/1\">Guangwei Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_T/0/1/0/all/0/1\">Tieyong Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guixu Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Struct-MDC: Mesh-Refined Unsupervised Depth Completion Leveraging Structural Regularities from Visual SLAM. (arXiv:2204.13877v1 [cs.CV])","link":"http://arxiv.org/abs/2204.13877","description":"<p>Feature-based visual simultaneous localization and mapping (SLAM) methods\nonly estimate the depth of extracted features, generating a sparse depth map.\nTo solve this sparsity problem, depth completion tasks that estimate a dense\ndepth from a sparse depth have gained significant importance in robotic\napplications like exploration. Existing methodologies that use sparse depth\nfrom visual SLAM mainly employ point features. However, point features have\nlimitations in preserving structural regularities owing to texture-less\nenvironments and sparsity problems. To deal with these issues, we perform depth\ncompletion with visual SLAM using line features, which can better contain\nstructural regularities than point features. The proposed methodology creates a\nconvex hull region by performing constrained Delaunay triangulation with depth\ninterpolation using line features. However, the generated depth includes\nlow-frequency information and is discontinuous at the convex hull boundary.\nTherefore, we propose a mesh depth refinement (MDR) module to address this\nproblem. The MDR module effectively transfers the high-frequency details of an\ninput image to the interpolated depth and plays a vital role in bridging the\nconventional and deep learning-based approaches. The Struct-MDC outperforms\nother state-of-the-art algorithms on public and our custom datasets, and even\noutperforms supervised methodologies for some metrics. In addition, the\neffectiveness of the proposed MDR module is verified by a rigorous ablation\nstudy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jeon_J/0/1/0/all/0/1\">Jinwoo Jeon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_H/0/1/0/all/0/1\">Hyunjun Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_D/0/1/0/all/0/1\">Dong-Uk Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Myung_H/0/1/0/all/0/1\">Hyun Myung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Adaptive Warping for Real-World Rolling Shutter Correction. (arXiv:2204.13886v1 [cs.CV])","link":"http://arxiv.org/abs/2204.13886","description":"<p>This paper proposes the first real-world rolling shutter (RS) correction\ndataset, BS-RSC, and a corresponding model to correct the RS frames in a\ndistorted video. Mobile devices in the consumer market with CMOS-based sensors\nfor video capture often result in rolling shutter effects when relative\nmovements occur during the video acquisition process, calling for RS effect\nremoval techniques. However, current state-of-the-art RS correction methods\noften fail to remove RS effects in real scenarios since the motions are various\nand hard to model. To address this issue, we propose a real-world RS correction\ndataset BS-RSC. Real distorted videos with corresponding ground truth are\nrecorded simultaneously via a well-designed beam-splitter-based acquisition\nsystem. BS-RSC contains various motions of both camera and objects in dynamic\nscenes. Further, an RS correction model with adaptive warping is proposed. Our\nmodel can warp the learned RS features into global shutter counterparts\nadaptively with predicted multiple displacement fields. These warped features\nare aggregated and then reconstructed into high-quality global shutter frames\nin a coarse-to-fine strategy. Experimental results demonstrate the\neffectiveness of the proposed method, and our dataset can improve the model's\nability to remove the RS effects in the real world.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_M/0/1/0/all/0/1\">Mingdeng Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1\">Zhihang Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiahao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yinqiang Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yujiu Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SideRT: A Real-time Pure Transformer Architecture for Single Image Depth Estimation. (arXiv:2204.13892v1 [cs.CV])","link":"http://arxiv.org/abs/2204.13892","description":"<p>Since context modeling is critical for estimating depth from a single image,\nresearchers put tremendous effort into obtaining global context. Many global\nmanipulations are designed for traditional CNN-based architectures to overcome\nthe locality of convolutions. Attention mechanisms or transformers originally\ndesigned for capturing long-range dependencies might be a better choice, but\nusually complicates architectures and could lead to a decrease in inference\nspeed. In this work, we propose a pure transformer architecture called SideRT\nthat can attain excellent predictions in real-time. In order to capture better\nglobal context, Cross-Scale Attention (CSA) and Multi-Scale Refinement (MSR)\nmodules are designed to work collaboratively to fuse features of different\nscales efficiently. CSA modules focus on fusing features of high semantic\nsimilarities, while MSR modules aim to fuse features at corresponding\npositions. These two modules contain a few learnable parameters without\nconvolutions, based on which a lightweight yet effective model is built. This\narchitecture achieves state-of-the-art performances in real-time (51.3 FPS) and\nbecomes much faster with a reasonable performance drop on a smaller backbone\nSwin-T (83.1 FPS). Furthermore, its performance surpasses the previous\nstate-of-the-art by a large margin, improving AbsRel metric 6.9% on KITTI and\n9.7% on NYU. To the best of our knowledge, this is the first work to show that\ntransformer-based networks can attain state-of-the-art performance in real-time\nin the single image depth estimation field. Code will be made available soon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shu_C/0/1/0/all/0/1\">Chang Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Ziming Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Kuan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Minghui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1\">Haibing Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leaner and Faster: Two-Stage Model Compression for Lightweight Text-Image Retrieval. (arXiv:2204.13913v1 [cs.CV])","link":"http://arxiv.org/abs/2204.13913","description":"<p>Current text-image approaches (e.g., CLIP) typically adopt dual-encoder\narchitecture using pre-trained vision-language representation. However, these\nmodels still pose non-trivial memory requirements and substantial incremental\nindexing time, which makes them less practical on mobile devices. In this\npaper, we present an effective two-stage framework to compress large\npre-trained dual-encoder for lightweight text-image retrieval. The resulting\nmodel is smaller (39% of the original), faster (1.6x/2.9x for processing\nimage/text respectively), yet performs on par with or better than the original\nfull model on Flickr30K and MSCOCO benchmarks. We also open-source an\naccompanying realistic mobile image search application.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1\">Siyu Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1\">Kenny Q. Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Privacy-Preserving Model Upgrades with Bidirectional Compatible Training in Image Retrieval. (arXiv:2204.13919v1 [cs.CV])","link":"http://arxiv.org/abs/2204.13919","description":"<p>The task of privacy-preserving model upgrades in image retrieval desires to\nreap the benefits of rapidly evolving new models without accessing the raw\ngallery images. A pioneering work introduced backward-compatible training,\nwhere the new model can be directly deployed in a backfill-free manner, i.e.,\nthe new query can be directly compared to the old gallery features. Despite a\npossible solution, its improvement in sequential model upgrades is gradually\nlimited by the fixed and under-quality old gallery embeddings. To this end, we\npropose a new model upgrade paradigm, termed Bidirectional Compatible Training\n(BiCT), which will upgrade the old gallery embeddings by forward-compatible\ntraining towards the embedding space of the backward-compatible new model. We\nconduct comprehensive experiments to verify the prominent improvement by BiCT\nand interestingly observe that the inconspicuous loss weight of backward\ncompatibility actually plays an essential role for both backward and forward\nretrieval performance. To summarize, we introduce a new and valuable problem\nnamed privacy-preserving model upgrades, with a proper solution BiCT. Several\nintriguing insights are further proposed to get the most out of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_S/0/1/0/all/0/1\">Shupeng Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Binjie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yixiao Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xuyuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yexin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1\">Chun Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1\">Ying Shan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learned Gradient of a Regularizer for Plug-and-Play Gradient Descent. (arXiv:2204.13940v1 [eess.IV])","link":"http://arxiv.org/abs/2204.13940","description":"<p>The Plug-and-Play (PnP) framework allows integrating advanced image denoising\npriors into optimization algorithms, to efficiently solve a variety of image\nrestoration tasks. The Plug-and-Play alternating direction method of\nmultipliers (ADMM) and the Regularization by Denoising (RED) algorithms are two\nexamples of such methods that made a breakthrough in image restoration.\nHowever, while the former method only applies to proximal algorithms, it has\nrecently been shown that there exists no regularization that explains the RED\nalgorithm when the denoisers lack Jacobian symmetry, which happen to be the\ncase of most practical denoisers. To the best of our knowledge, there exists no\nmethod for training a network that directly represents the gradient of a\nregularizer, which can be directly used in Plug-and-Play gradient-based\nalgorithms. We show that it is possible to train a denoiser along with a\nnetwork that corresponds to the gradient of its regularizer. We use this\ngradient of the regularizer in gradient-based optimization methods and obtain\nbetter results comparing to other generic Plug-and-Play approaches. We also\nshow that the regularizer can be used as a pre-trained network for unrolled\ngradient descent. Lastly, we show that the resulting denoiser allows for a\nquick convergence of the Plug-and-Play ADMM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Fermanian_R/0/1/0/all/0/1\">Rita Fermanian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pendu_M/0/1/0/all/0/1\">Mikael Le Pendu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guillemot_C/0/1/0/all/0/1\">Christine Guillemot</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Geometry Post-Processing for Decompressed Point Clouds. (arXiv:2204.13952v1 [cs.CV])","link":"http://arxiv.org/abs/2204.13952","description":"<p>Point cloud compression plays a crucial role in reducing the huge cost of\ndata storage and transmission. However, distortions can be introduced into the\ndecompressed point clouds due to quantization. In this paper, we propose a\nnovel learning-based post-processing method to enhance the decompressed point\nclouds. Specifically, a voxelized point cloud is first divided into small\ncubes. Then, a 3D convolutional network is proposed to predict the occupancy\nprobability for each location of a cube. We leverage both local and global\ncontexts by generating multi-scale probabilities. These probabilities are\nprogressively summed to predict the results in a coarse-to-fine manner.\nFinally, we obtain the geometry-refined point clouds based on the predicted\nprobabilities. Different from previous methods, we deal with decompressed point\nclouds with huge variety of distortions using a single model. Experimental\nresults show that the proposed method can significantly improve the quality of\nthe decompressed point clouds, achieving 9.30dB BDPSNR gain on three\nrepresentative datasets on average.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1\">Xiaoqing Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Ge Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dingquan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Yurui Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1\">Wei Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Thomas H. Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SCS-Co: Self-Consistent Style Contrastive Learning for Image Harmonization. (arXiv:2204.13962v1 [cs.CV])","link":"http://arxiv.org/abs/2204.13962","description":"<p>Image harmonization aims to achieve visual consistency in composite images by\nadapting a foreground to make it compatible with a background. However,\nexisting methods always only use the real image as the positive sample to guide\nthe training, and at most introduce the corresponding composite image as a\nsingle negative sample for an auxiliary constraint, which leads to limited\ndistortion knowledge, and further causes a too large solution space, making the\ngenerated harmonized image distorted. Besides, none of them jointly constrain\nfrom the foreground self-style and foreground-background style consistency,\nwhich exacerbates this problem. Moreover, recent region-aware adaptive instance\nnormalization achieves great success but only considers the global background\nfeature distribution, making the aligned foreground feature distribution\nbiased. To address these issues, we propose a self-consistent style contrastive\nlearning scheme (SCS-Co). By dynamically generating multiple negative samples,\nour SCS-Co can learn more distortion knowledge and well regularize the\ngenerated harmonized image in the style representation space from two aspects\nof the foreground self-style and foreground-background style consistency,\nleading to a more photorealistic visual result. In addition, we propose a\nbackground-attentional adaptive instance normalization (BAIN) to achieve an\nattention-weighted background feature distribution according to the\nforeground-background feature similarity. Experiments demonstrate the\nsuperiority of our method over other state-of-the-art methods in both\nquantitative comparison and visual analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hang_Y/0/1/0/all/0/1\">Yucheng Hang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_B/0/1/0/all/0/1\">Bin Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wenming Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Q/0/1/0/all/0/1\">Qingmin Liao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using 3D Shadows to Detect Object Hiding Attacks on Autonomous Vehicle Perception. (arXiv:2204.13973v1 [cs.CV])","link":"http://arxiv.org/abs/2204.13973","description":"<p>Autonomous Vehicles (AVs) are mostly reliant on LiDAR sensors which enable\nspatial perception of their surroundings and help make driving decisions.\nRecent works demonstrated attacks that aim to hide objects from AV perception,\nwhich can result in severe consequences. 3D shadows, are regions void of\nmeasurements in 3D point clouds which arise from occlusions of objects in a\nscene. 3D shadows were proposed as a physical invariant valuable for detecting\nspoofed or fake objects. In this work, we leverage 3D shadows to locate\nobstacles that are hidden from object detectors. We achieve this by searching\nfor void regions and locating the obstacles that cause these shadows. Our\nproposed methodology can be used to detect an object that has been hidden by an\nadversary as these objects, while hidden from 3D object detectors, still induce\nshadow artifacts in 3D point clouds, which we use for obstacle detection. We\nshow that using 3D shadows for obstacle detection can achieve high accuracy in\nmatching shadows to their object and provide precise prediction of an\nobstacle's distance from the ego-vehicle.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hau_Z/0/1/0/all/0/1\">Zhongyuan Hau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demetriou_S/0/1/0/all/0/1\">Soteris Demetriou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lupu_E/0/1/0/all/0/1\">Emil C. Lupu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AdaInt: Learning Adaptive Intervals for 3D Lookup Tables on Real-time Image Enhancement. (arXiv:2204.13983v1 [cs.CV])","link":"http://arxiv.org/abs/2204.13983","description":"<p>The 3D Lookup Table (3D LUT) is a highly-efficient tool for real-time image\nenhancement tasks, which models a non-linear 3D color transform by sparsely\nsampling it into a discretized 3D lattice. Previous works have made efforts to\nlearn image-adaptive output color values of LUTs for flexible enhancement but\nneglect the importance of sampling strategy. They adopt a sub-optimal uniform\nsampling point allocation, limiting the expressiveness of the learned LUTs\nsince the (tri-)linear interpolation between uniform sampling points in the LUT\ntransform might fail to model local non-linearities of the color transform.\nFocusing on this problem, we present AdaInt (Adaptive Intervals Learning), a\nnovel mechanism to achieve a more flexible sampling point allocation by\nadaptively learning the non-uniform sampling intervals in the 3D color space.\nIn this way, a 3D LUT can increase its capability by conducting dense sampling\nin color ranges requiring highly non-linear transforms and sparse sampling for\nnear-linear transforms. The proposed AdaInt could be implemented as a compact\nand efficient plug-and-play module for a 3D LUT-based method. To enable the\nend-to-end learning of AdaInt, we design a novel differentiable operator called\nAiLUT-Transform (Adaptive Interval LUT Transform) to locate input colors in the\nnon-uniform 3D LUT and provide gradients to the sampling intervals. Experiments\ndemonstrate that methods equipped with AdaInt can achieve state-of-the-art\nperformance on two public benchmark datasets with a negligible overhead\nincrease. Our source code is available at https://github.com/ImCharlesY/AdaInt.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Canqian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_M/0/1/0/all/0/1\">Meiguang Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1\">Xu Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Ying Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning High-DOF Reaching-and-Grasping via Dynamic Representation of Gripper-Object Interaction. (arXiv:2204.13998v1 [cs.RO])","link":"http://arxiv.org/abs/2204.13998","description":"<p>We approach the problem of high-DOF reaching-and-grasping via learning joint\nplanning of grasp and motion with deep reinforcement learning. To resolve the\nsample efficiency issue in learning the high-dimensional and complex control of\ndexterous grasping, we propose an effective representation of grasping state\ncharacterizing the spatial interaction between the gripper and the target\nobject. To represent gripper-object interaction, we adopt Interaction Bisector\nSurface (IBS) which is the Voronoi diagram between two close by 3D geometric\nobjects and has been successfully applied in characterizing spatial relations\nbetween 3D objects. We found that IBS is surprisingly effective as a state\nrepresentation since it well informs the fine-grained control of each finger\nwith spatial relation against the target object. This novel grasp\nrepresentation, together with several technical contributions including a fast\nIBS approximation, a novel vector-based reward and an effective training\nstrategy, facilitate learning a strong control model of high-DOF grasping with\ngood sample efficiency, dynamic adaptability, and cross-category generality.\nExperiments show that it generates high-quality dexterous grasp for complex\nshapes with smooth grasping motions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+She_Q/0/1/0/all/0/1\">Qijin She</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1\">Ruizhen Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Juzhan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Min Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kai Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hui Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Searching for Efficient Neural Architectures for On-Device ML on Edge TPUs. (arXiv:2204.14007v1 [cs.DC])","link":"http://arxiv.org/abs/2204.14007","description":"<p>On-device ML accelerators are becoming a standard in modern mobile\nsystem-on-chips (SoC). Neural architecture search (NAS) comes to the rescue for\nefficiently utilizing the high compute throughput offered by these\naccelerators. However, existing NAS frameworks have several practical\nlimitations in scaling to multiple tasks and different target platforms. In\nthis work, we provide a two-pronged approach to this challenge: (i) a\nNAS-enabling infrastructure that decouples model cost evaluation, search space\ndesign, and the NAS algorithm to rapidly target various on-device ML tasks, and\n(ii) search spaces crafted from group convolution based inverted bottleneck\n(IBN) variants that provide flexible quality/performance trade-offs on ML\naccelerators, complementing the existing full and depthwise convolution based\nIBNs. Using this approach we target a state-of-the-art mobile platform, Google\nTensor SoC, and demonstrate neural architectures that improve the\nquality-performance pareto frontier for various computer vision\n(classification, detection, segmentation) as well as natural language\nprocessing tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Akin_B/0/1/0/all/0/1\">Berkin Akin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1\">Suyog Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_Y/0/1/0/all/0/1\">Yun Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spiridonov_A/0/1/0/all/0/1\">Anton Spiridonov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhuo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+White_M/0/1/0/all/0/1\">Marie White</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Ping Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yanqi Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Implicit Representations for Physical Parameter Inference from a Single Video. (arXiv:2204.14030v1 [cs.CV])","link":"http://arxiv.org/abs/2204.14030","description":"<p>Neural networks have recently been used to analyze diverse physical systems\nand to identify the underlying dynamics. While existing methods achieve\nimpressive results, they are limited by their strong demand for training data\nand their weak generalization abilities to out-of-distribution data. To\novercome these limitations, in this work we propose to combine neural implicit\nrepresentations for appearance modeling with neural ordinary differential\nequations (ODEs) for modelling physical phenomena to obtain a dynamic scene\nrepresentation that can be identified directly from visual observations. Our\nproposed model combines several unique advantages: (i) Contrary to existing\napproaches that require large training datasets, we are able to identify\nphysical parameters from only a single video. (ii) The use of neural implicit\nrepresentations enables the processing of high-resolution videos and the\nsynthesis of photo-realistic images. (iii) The embedded neural ODE has a known\nparametric form that allows for the identification of interpretable physical\nparameters, and (iv) long-term prediction in state space. (v) Furthermore, the\nphoto-realistic rendering of novel scenes with modified physical parameters\nbecomes possible.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hofherr_F/0/1/0/all/0/1\">Florian Hofherr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koestler_L/0/1/0/all/0/1\">Lukas Koestler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bernard_F/0/1/0/all/0/1\">Florian Bernard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cremers_D/0/1/0/all/0/1\">Daniel Cremers</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Challenging Benchmark of Anime Style Recognition. (arXiv:2204.14034v1 [cs.CV])","link":"http://arxiv.org/abs/2204.14034","description":"<p>Given two images of different anime roles, anime style recognition (ASR) aims\nto learn abstract painting style to determine whether the two images are from\nthe same work, which is an interesting but challenging problem. Unlike\nbiometric recognition, such as face recognition, iris recognition, and person\nre-identification, ASR suffers from a much larger semantic gap but receives\nless attention. In this paper, we propose a challenging ASR benchmark. Firstly,\nwe collect a large-scale ASR dataset (LSASRD), which contains 20,937 images of\n190 anime works and each work at least has ten different roles. In addition to\nthe large-scale, LSASRD contains a list of challenging factors, such as complex\nilluminations, various poses, theatrical colors and exaggerated compositions.\nSecondly, we design a cross-role protocol to evaluate ASR performance, in which\nquery and gallery images must come from different roles to validate an ASR\nmodel is to learn abstract painting style rather than learn discriminative\nfeatures of roles. Finally, we apply two powerful person re-identification\nmethods, namely, AGW and TransReID, to construct the baseline performance on\nLSASRD. Surprisingly, the recent transformer model (i.e., TransReID) only\nacquires a 42.24% mAP on LSASRD. Therefore, we believe that the ASR task of a\nhuge semantic gap deserves deep and long-term research. We will open our\ndataset and code at https://github.com/nkjcqvcpi/ASR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haotang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Shengtao Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_K/0/1/0/all/0/1\">Kailin Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianchen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jianqing Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_H/0/1/0/all/0/1\">Huanqiang Zeng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"C3-STISR: Scene Text Image Super-resolution with Triple Clues. (arXiv:2204.14044v1 [cs.CV])","link":"http://arxiv.org/abs/2204.14044","description":"<p>Scene text image super-resolution (STISR) has been regarded as an important\npre-processing task for text recognition from low-resolution scene text images.\nMost recent approaches use the recognizer's feedback as clues to guide\nsuper-resolution. However, directly using recognition clue has two problems: 1)\nCompatibility. It is in the form of probability distribution, has an obvious\nmodal gap with STISR - a pixel-level task; 2) Inaccuracy. it usually contains\nwrong information, thus will mislead the main task and degrade super-resolution\nperformance. In this paper, we present a novel method C3-STISR that jointly\nexploits the recognizer's feedback, visual and linguistical information as\nclues to guide super-resolution. Here, visual clue is from the images of texts\npredicted by the recognizer, which is informative and more compatible with the\nSTISR task; while linguistical clue is generated by a pre-trained\ncharacter-level language model, which is able to correct the predicted texts.\nWe design effective extraction and fusion mechanisms for the triple cross-modal\nclues to generate a comprehensive and unified guidance for super-resolution.\nExtensive experiments on TextZoom show that C3-STISR outperforms the SOTA\nmethods in fidelity and recognition performance. Code is available in\nhttps://github.com/zhaominyiz/C3-STISR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1\">Minyi Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Miao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_F/0/1/0/all/0/1\">Fan Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bingjia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shuigeng Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Deep Learning based No-reference Quality Assessment Model for UGC Videos. (arXiv:2204.14047v1 [cs.CV])","link":"http://arxiv.org/abs/2204.14047","description":"<p>Quality assessment for User Generated Content (UGC) videos plays an important\nrole in ensuring the viewing experience of end-users. Previous UGC video\nquality assessment (VQA) studies either use the image recognition model or the\nimage quality assessment (IQA) models to extract frame-level features of UGC\nvideos for quality regression, which are regarded as the sub-optimal solutions\nbecause of the domain shifts between these tasks and the UGC VQA task. In this\npaper, we propose a very simple but effective UGC VQA model, which tries to\naddress this problem by training an end-to-end spatial feature extraction\nnetwork to directly learn the quality-aware spatial feature representation from\nraw pixels of the video frames. We also extract the motion features to measure\nthe temporal-related distortions that the spatial features cannot model. The\nproposed model utilizes very sparse frames to extract spatial features and\ndense frames (i.e. the video chunk) with a very low spatial resolution to\nextract motion features, which thereby has low computational complexity. With\nthe better quality-aware features, we only use the simple multilayer perception\nlayer (MLP) network to regress them into the chunk-level quality scores, and\nthen the temporal average pooling strategy is adopted to obtain the video-level\nquality score. We further introduce a multi-scale quality fusion strategy to\nsolve the problem of VQA across different spatial resolutions, where the\nmulti-scale weights are obtained from the contrast sensitivity function of the\nhuman visual system. The experimental results show that the proposed model\nachieves the best performance on five popular UGC VQA databases, which\ndemonstrates the effectiveness of the proposed model. The code will be publicly\navailable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Wei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_X/0/1/0/all/0/1\">Xiongkuo Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Wei Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_G/0/1/0/all/0/1\">Guangtao Zhai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Voice-Face Representation Learning by Cross-Modal Prototype Contrast. (arXiv:2204.14057v1 [cs.SD])","link":"http://arxiv.org/abs/2204.14057","description":"<p>We present an approach to learn voice-face representations from the talking\nface videos, without any identity labels. Previous works employ cross-modal\ninstance discrimination tasks to establish the correlation of voice and face.\nThese methods neglect the semantic content of different videos, introducing\nfalse-negative pairs as training noise. Furthermore, the positive pairs are\nconstructed based on the natural correlation between audio clips and visual\nframes. However, this correlation might be weak or inaccurate in a large amount\nof real-world data, which leads to deviating positives into the contrastive\nparadigm. To address these issues, we propose the cross-modal prototype\ncontrastive learning (CMPC), which takes advantage of contrastive methods and\nresists adverse effects of false negatives and deviate positives. On one hand,\nCMPC could learn the intra-class invariance by constructing semantic-wise\npositives via unsupervised clustering in different modalities. On the other\nhand, by comparing the similarities of cross-modal instances from that of\ncross-modal prototypes, we dynamically recalibrate the unlearnable instances'\ncontribution to overall loss. Experiments show that the proposed approach\noutperforms state-of-the-art unsupervised methods on various voice-face\nassociation evaluation protocols. Additionally, in the low-shot supervision\nsetting, our method also has a significant improvement compared to previous\ninstance-wise contrastive learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1\">Boqing Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kele Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Changjian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1\">Zheng Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1\">Tao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huaimin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yuxing Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fix the Noise: Disentangling Source Feature for Transfer Learning of StyleGAN. (arXiv:2204.14079v1 [cs.CV])","link":"http://arxiv.org/abs/2204.14079","description":"<p>Transfer learning of StyleGAN has recently shown great potential to solve\ndiverse tasks, especially in domain translation. Previous methods utilized a\nsource model by swapping or freezing weights during transfer learning, however,\nthey have limitations on visual quality and controlling source features. In\nother words, they require additional models that are computationally demanding\nand have restricted control steps that prevent a smooth transition. In this\npaper, we propose a new approach to overcome these limitations. Instead of\nswapping or freezing, we introduce a simple feature matching loss to improve\ngeneration quality. In addition, to control the degree of source features, we\ntrain a target model with the proposed strategy, FixNoise, to preserve the\nsource features only in a disentangled subspace of a target feature space.\nOwing to the disentangled feature space, our method can smoothly control the\ndegree of the source features in a single model. Extensive experiments\ndemonstrate that the proposed method can generate more consistent and realistic\nimages than previous works.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dongyeun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jae Young Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Doyeon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jaehyun Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Junmo Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Localization-aware Target Confidence for Siamese Visual Tracking. (arXiv:2204.14093v1 [cs.CV])","link":"http://arxiv.org/abs/2204.14093","description":"<p>Siamese tracking paradigm has achieved great success, providing effective\nappearance discrimination and size estimation by the classification and\nregression. While such a paradigm typically optimizes the classification and\nregression independently, leading to task misalignment (accurate prediction\nboxes have no high target confidence scores). In this paper, to alleviate this\nmisalignment, we propose a novel tracking paradigm, called SiamLA. Within this\nparadigm, a series of simple, yet effective localization-aware components are\nintroduced, to generate localization-aware target confidence scores.\nSpecifically, with the proposed localization-aware dynamic label (LADL) loss\nand localization-aware label smoothing (LALS) strategy, collaborative\noptimization between the classification and regression is achieved, enabling\nclassification scores to be aware of location state, not just appearance\nsimilarity. Besides, we propose a separate localization branch, centered on a\nlocalization-aware feature aggregation (LAFA) module, to produce location\nquality scores to further modify the classification scores. Consequently, the\nresulting target confidence scores, are more discriminative for the location\nstate, allowing accurate prediction boxes tend to be predicted as high scores.\nExtensive experiments are conducted on six challenging benchmarks, including\nGOT-10k, TrackingNet, LaSOT, TNL2K, OTB100 and VOT2018. Our SiamLA achieves\nstate-of-the-art performance in terms of both accuracy and efficiency.\nFurthermore, a stability analysis reveals that our tracking paradigm is\nrelatively stable, implying the paradigm is potential to real-world\napplications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nie_J/0/1/0/all/0/1\">Jiahao Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Han Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhiwei He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yuxiang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1\">Mingyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1\">Zhekang Dong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PyramidCLIP: Hierarchical Feature Alignment for Vision-language Model Pretraining. (arXiv:2204.14095v1 [cs.CV])","link":"http://arxiv.org/abs/2204.14095","description":"<p>Large-scale vision-language pre-training has achieved promising results on\ndownstream tasks. Existing methods highly rely on the assumption that the\nimage-text pairs crawled from the Internet are in perfect one-to-one\ncorrespondence. However, in real scenarios, this assumption can be difficult to\nhold: the text description, obtained by crawling the affiliated metadata of the\nimage, often suffer from semantic mismatch and mutual compatibility. To address\nthese issues, here we introduce PyramidCLIP, which constructs an input pyramid\nwith different semantic levels, and aligns visual elements and linguistic\nelements in the form of hierarchy via intra-level semantics alignment and\ncross-level relation alignment. Furthermore, we adjust the objective function\nby softening the loss of negative samples (unpaired samples) so as to weaken\nthe strict constraint during the pre-training stage, thus mitigating the risk\nof the model being over-confident. Experiments on three downstream tasks,\nincluding zero-shot image classification, zero-shot image-text retrieval and\nimage object detection, verify the effectiveness of the proposed PyramidCLIP.\nIn particular, with the same amount of pre-training data of 15 millions\nimage-text pairs, PyramidCLIP exceeds CLIP by 19.2%/18.5%/19.6% respectively,\nwith the image encoder being ResNet-50/ViT-B32/ViT-B16 on ImageNet zero-shot\nclassification top-1 accuracy. When scaling to larger datasets, the results of\nPyramidCLIP only trained for 8 epochs using 128M image-text pairs are very\nclose to that of CLIP trained for 32 epochs using 400M training data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yuting Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jinfeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zihan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Ke Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Distortion Learning for Medical Image Denoising. (arXiv:2204.14100v1 [eess.IV])","link":"http://arxiv.org/abs/2204.14100","description":"<p>We present a novel adversarial distortion learning (ADL) for denoising two-\nand three-dimensional (2D/3D) biomedical image data. The proposed ADL consists\nof two auto-encoders: a denoiser and a discriminator. The denoiser removes\nnoise from input data and the discriminator compares the denoised result to its\nnoise-free counterpart. This process is repeated until the discriminator cannot\ndifferentiate the denoised data from the reference. Both the denoiser and the\ndiscriminator are built upon a proposed auto-encoder called Efficient-Unet.\nEfficient-Unet has a light architecture that uses the residual blocks and a\nnovel pyramidal approach in the backbone to efficiently extract and re-use\nfeature maps. During training, the textural information and contrast are\ncontrolled by two novel loss functions. The architecture of Efficient-Unet\nallows generalizing the proposed method to any sort of biomedical data. The 2D\nversion of our network was trained on ImageNet and tested on biomedical\ndatasets whose distribution is completely different from ImageNet; so, there is\nno need for re-training. Experimental results carried out on magnetic resonance\nimaging (MRI), dermatoscopy, electron microscopy and X-ray datasets show that\nthe proposed method achieved the best on each benchmark. Our implementation and\npre-trained models are available at https://github.com/mogvision/ADL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ghahremani_M/0/1/0/all/0/1\">Morteza Ghahremani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khateri_M/0/1/0/all/0/1\">Mohammad Khateri</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sierra_A/0/1/0/all/0/1\">Alejandra Sierra</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tohka_J/0/1/0/all/0/1\">Jussi Tohka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TEMOS: Generating diverse human motions from textual descriptions. (arXiv:2204.14109v1 [cs.CV])","link":"http://arxiv.org/abs/2204.14109","description":"<p>We address the problem of generating diverse 3D human motions from textual\ndescriptions. This challenging task requires joint modeling of both modalities:\nunderstanding and extracting useful human-centric information from the text,\nand then generating plausible and realistic sequences of human poses. In\ncontrast to most previous work which focuses on generating a single,\ndeterministic, motion from a textual description, we design a variational\napproach that can produce multiple diverse human motions. We propose TEMOS, a\ntext-conditioned generative model leveraging variational autoencoder (VAE)\ntraining with human motion data, in combination with a text encoder that\nproduces distribution parameters compatible with the VAE latent space. We show\nthat TEMOS framework can produce both skeleton-based animations as in prior\nwork, as well more expressive SMPL body motions. We evaluate our approach on\nthe KIT Motion-Language benchmark and, despite being relatively\nstraightforward, demonstrate significant improvements over the state of the\nart. Code and models are available on our project page.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Petrovich_M/0/1/0/all/0/1\">Mathis Petrovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Black_M/0/1/0/all/0/1\">Michael J. Black</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varol_G/0/1/0/all/0/1\">G&#xfc;l Varol</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Seeing without Looking: Analysis Pipeline for Child Sexual Abuse Datasets. (arXiv:2204.14110v1 [cs.CV])","link":"http://arxiv.org/abs/2204.14110","description":"<p>The online sharing and viewing of Child Sexual Abuse Material (CSAM) are\ngrowing fast, such that human experts can no longer handle the manual\ninspection. However, the automatic classification of CSAM is a challenging\nfield of research, largely due to the inaccessibility of target data that is -\nand should forever be - private and in sole possession of law enforcement\nagencies. To aid researchers in drawing insights from unseen data and safely\nproviding further understanding of CSAM images, we propose an analysis template\nthat goes beyond the statistics of the dataset and respective labels. It\nfocuses on the extraction of automatic signals, provided both by pre-trained\nmachine learning models, e.g., object categories and pornography detection, as\nwell as image metrics such as luminance and sharpness. Only aggregated\nstatistics of sparse signals are provided to guarantee the anonymity of\nchildren and adolescents victimized. The pipeline allows filtering the data by\napplying thresholds to each specified signal and provides the distribution of\nsuch signals within the subset, correlations between signals, as well as a bias\nevaluation. We demonstrated our proposal on the Region-based annotated Child\nPornography Dataset (RCPD), one of the few CSAM benchmarks in the literature,\ncomposed of over 2000 samples among regular and CSAM images, produced in\npartnership with Brazil's Federal Police. Although noisy and limited in several\nsenses, we argue that automatic signals can highlight important aspects of the\noverall distribution of data, which is valuable for databases that can not be\ndisclosed. Our goal is to safely publicize the characteristics of CSAM\ndatasets, encouraging researchers to join the field and perhaps other\ninstitutions to provide similar reports on their benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Laranjeira_C/0/1/0/all/0/1\">Camila Laranjeira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Macedo_J/0/1/0/all/0/1\">Jo&#xe3;o Macedo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avila_S/0/1/0/all/0/1\">Sandra Avila</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santos_J/0/1/0/all/0/1\">Jefersson A. dos Santos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comparative Study of Meter Detection Methods for Automated Infrastructure Inspection. (arXiv:2204.14117v1 [cs.CV])","link":"http://arxiv.org/abs/2204.14117","description":"<p>In order to read meter values from a camera on an autonomous inspection robot\nwith positional errors, it is necessary to detect meter regions from the image.\nIn this study, we developed shape-based, texture-based, and background\ninformation-based methods as meter area detection techniques and compared their\neffectiveness for meters of different shapes and sizes. As a result, we\nconfirmed that the background information-based method can detect the farthest\nmeters regardless of the shape and number of meters, and can stably detect\nmeters with a diameter of 40px.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ohtsubo_Y/0/1/0/all/0/1\">Yusuke Ohtsubo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sato_T/0/1/0/all/0/1\">Takuto Sato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sagawa_H/0/1/0/all/0/1\">Hirohiko Sagawa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Automatic Parsing of Structured Visual Content through the Use of Synthetic Data. (arXiv:2204.14136v1 [cs.CV])","link":"http://arxiv.org/abs/2204.14136","description":"<p>Structured Visual Content (SVC) such as graphs, flow charts, or the like are\nused by authors to illustrate various concepts. While such depictions allow the\naverage reader to better understand the contents, images containing SVCs are\ntypically not machine-readable. This, in turn, not only hinders automated\nknowledge aggregation, but also the perception of displayed in-formation for\nvisually impaired people. In this work, we propose a synthetic dataset,\ncontaining SVCs in the form of images as well as ground truths. We show the\nusage of this dataset by an application that automatically extracts a graph\nrepresentation from an SVC image. This is done by training a model via common\nsupervised learning methods. As there currently exist no large-scale public\ndatasets for the detailed analysis of SVC, we propose the Synthetic SVC (SSVC)\ndataset comprising 12,000 images with respective bounding box annotations and\ndetailed graph representations. Our dataset enables the development of strong\nmodels for the interpretation of SVCs while skipping the time-consuming dense\ndata annotation. We evaluate our model on both synthetic and manually annotated\ndata and show the transferability of synthetic to real via various metrics,\ngiven the presented application. Here, we evaluate that this proof of concept\nis possible to some extend and lay down a solid baseline for this task. We\ndiscuss the limitations of our approach for further improvements. Our utilized\nmetrics can be used as a tool for future comparisons in this domain. To enable\nfurther research on this task, the dataset is publicly available at\nhttps://bit.ly/3jN1pJJ\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Scholch_L/0/1/0/all/0/1\">Lukas Scholch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steinhauser_J/0/1/0/all/0/1\">Jonas Steinhauser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beichter_M/0/1/0/all/0/1\">Maximilian Beichter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seibold_C/0/1/0/all/0/1\">Constantin Seibold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kailun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knable_M/0/1/0/all/0/1\">Merlin Kn&#xe4;ble</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwarz_T/0/1/0/all/0/1\">Thorsten Schwarz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madche_A/0/1/0/all/0/1\">Alexander M&#xe4;dche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stiefelhagen_R/0/1/0/all/0/1\">Rainer Stiefelhagen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Segmentation of kidney stones in endoscopic video feeds. (arXiv:2204.14175v1 [eess.IV])","link":"http://arxiv.org/abs/2204.14175","description":"<p>Image segmentation has been increasingly applied in medical settings as\nrecent developments have skyrocketed the potential applications of deep\nlearning. Urology, specifically, is one field of medicine that is primed for\nthe adoption of a real-time image segmentation system with the long-term aim of\nautomating endoscopic stone treatment. In this project, we explored supervised\ndeep learning models to annotate kidney stones in surgical endoscopic video\nfeeds. In this paper, we describe how we built a dataset from the raw videos\nand how we developed a pipeline to automate as much of the process as possible.\nFor the segmentation task, we adapted and analyzed three baseline deep learning\nmodels -- U-Net, U-Net++, and DenseNet -- to predict annotations on the frames\nof the endoscopic videos with the highest accuracy above 90\\%. To show clinical\npotential for real-time use, we also confirmed that our best trained model can\naccurately annotate new videos at 30 frames per second. Our results demonstrate\nthat the proposed method justifies continued development and study of image\nsegmentation to annotate ureteroscopic video feeds.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Stoebner_Z/0/1/0/all/0/1\">Zachary A Stoebner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_D/0/1/0/all/0/1\">Daiwei Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hong_S/0/1/0/all/0/1\">Seok Hee Hong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kavoussi_N/0/1/0/all/0/1\">Nicholas L Kavoussi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Oguz_I/0/1/0/all/0/1\">Ipek Oguz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Oracle Guided Image Synthesis with Relative Queries. (arXiv:2204.14189v1 [cs.CV])","link":"http://arxiv.org/abs/2204.14189","description":"<p>Isolating and controlling specific features in the outputs of generative\nmodels in a user-friendly way is a difficult and open-ended problem. We develop\ntechniques that allow an oracle user to generate an image they are envisioning\nin their head by answering a sequence of relative queries of the form\n\\textit{\"do you prefer image $a$ or image $b$?\"} Our framework consists of a\nConditional VAE that uses the collected relative queries to partition the\nlatent space into preference-relevant features and non-preference-relevant\nfeatures. We then use the user's responses to relative queries to determine the\npreference-relevant features that correspond to their envisioned output image.\nAdditionally, we develop techniques for modeling the uncertainty in images'\npredicted preference-relevant features, allowing our framework to generalize to\nscenarios in which the relative query training set contains noise.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Helbling_A/0/1/0/all/0/1\">Alec Helbling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rozell_C/0/1/0/all/0/1\">Christopher John Rozell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OShaughnessy_M/0/1/0/all/0/1\">Matthew O&#x27;Shaughnessy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fallah_K/0/1/0/all/0/1\">Kion Fallah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Transferability for Domain Adaptive Detection Transformers. (arXiv:2204.14195v1 [cs.CV])","link":"http://arxiv.org/abs/2204.14195","description":"<p>DETR-style detectors stand out amongst in-domain scenarios, but their\nproperties in domain shift settings are under-explored. This paper aims to\nbuild a simple but effective baseline with a DETR-style detector on domain\nshift settings based on two findings. For one, mitigating the domain shift on\nthe backbone and the decoder output features excels in getting favorable\nresults. For another, advanced domain alignment methods in both parts further\nenhance the performance. Thus, we propose the Object-Aware Alignment (OAA)\nmodule and the Optimal Transport based Alignment (OTA) module to achieve\ncomprehensive domain alignment on the outputs of the backbone and the detector.\nThe OAA module aligns the foreground regions identified by pseudo-labels in the\nbackbone outputs, leading to domain-invariant based features. The OTA module\nutilizes sliced Wasserstein distance to maximize the retention of location\ninformation while minimizing the domain gap in the decoder outputs. We\nimplement the findings and the alignment modules into our adaptation method,\nand it benchmarks the DETR-style detector on the domain shift settings.\nExperiments on various domain adaptive scenarios validate the effectiveness of\nour method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gong_K/0/1/0/all/0/1\">Kaixiong Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shugang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chi Harold Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qiang Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Flamingo: a Visual Language Model for Few-Shot Learning. (arXiv:2204.14198v1 [cs.CV])","link":"http://arxiv.org/abs/2204.14198","description":"<p>Building models that can be rapidly adapted to numerous tasks using only a\nhandful of annotated examples is an open challenge for multimodal machine\nlearning research. We introduce Flamingo, a family of Visual Language Models\n(VLM) with this ability. Flamingo models include key architectural innovations\nto: (i) bridge powerful pretrained vision-only and language-only models, (ii)\nhandle sequences of arbitrarily interleaved visual and textual data, and (iii)\nseamlessly ingest images or videos as inputs. Thanks to their flexibility,\nFlamingo models can be trained on large-scale multimodal web corpora containing\narbitrarily interleaved text and images, which is key to endow them with\nin-context few-shot learning capabilities. We perform a thorough evaluation of\nthe proposed Flamingo models, exploring and measuring their ability to rapidly\nadapt to a variety of image and video understanding benchmarks. These include\nopen-ended tasks such as visual question-answering, where the model is prompted\nwith a question which it has to answer, captioning tasks, which evaluate the\nability to describe a scene or an event, and close-ended tasks such as multiple\nchoice visual question-answering. For tasks lying anywhere on this spectrum, we\ndemonstrate that a single Flamingo model can achieve a new state of the art for\nfew-shot learning, simply by prompting the model with task-specific examples.\nOn many of these benchmarks, Flamingo actually surpasses the performance of\nmodels that are fine-tuned on thousands of times more task-specific data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alayrac_J/0/1/0/all/0/1\">Jean-Baptiste Alayrac</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Donahue_J/0/1/0/all/0/1\">Jeff Donahue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luc_P/0/1/0/all/0/1\">Pauline Luc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miech_A/0/1/0/all/0/1\">Antoine Miech</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barr_I/0/1/0/all/0/1\">Iain Barr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasson_Y/0/1/0/all/0/1\">Yana Hasson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lenc_K/0/1/0/all/0/1\">Karel Lenc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mensch_A/0/1/0/all/0/1\">Arthur Mensch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Millican_K/0/1/0/all/0/1\">Katie Millican</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reynolds_M/0/1/0/all/0/1\">Malcolm Reynolds</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ring_R/0/1/0/all/0/1\">Roman Ring</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rutherford_E/0/1/0/all/0/1\">Eliza Rutherford</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cabi_S/0/1/0/all/0/1\">Serkan Cabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_T/0/1/0/all/0/1\">Tengda Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1\">Zhitao Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samangooei_S/0/1/0/all/0/1\">Sina Samangooei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Monteiro_M/0/1/0/all/0/1\">Marianne Monteiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menick_J/0/1/0/all/0/1\">Jacob Menick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borgeaud_S/0/1/0/all/0/1\">Sebastian Borgeaud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brock_A/0/1/0/all/0/1\">Andrew Brock</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nematzadeh_A/0/1/0/all/0/1\">Aida Nematzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharifzadeh_S/0/1/0/all/0/1\">Sahand Sharifzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Binkowski_M/0/1/0/all/0/1\">Mikolaj Binkowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barreira_R/0/1/0/all/0/1\">Ricardo Barreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinyals_O/0/1/0/all/0/1\">Oriol Vinyals</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1\">Andrew Zisserman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simonyan_K/0/1/0/all/0/1\">Karen Simonyan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Preoperative brain tumor imaging: models and software for segmentation and standardized reporting. (arXiv:2204.14199v1 [eess.IV])","link":"http://arxiv.org/abs/2204.14199","description":"<p>For patients suffering from brain tumor, prognosis estimation and treatment\ndecisions are made by a multidisciplinary team based on a set of preoperative\nMR scans. Currently, the lack of standardized and automatic methods for tumor\ndetection and generation of clinical reports represents a major hurdle. In this\nstudy, we investigate glioblastomas, lower grade gliomas, meningiomas, and\nmetastases, through four cohorts of up to 4000 patients. Tumor segmentation\nmodels were trained using the AGU-Net architecture with different preprocessing\nsteps and protocols. Segmentation performances were assessed in-depth using a\nwide-range of voxel and patient-wise metrics covering volume, distance, and\nprobabilistic aspects. Finally, two software solutions have been developed,\nenabling an easy use of the trained models and standardized generation of\nclinical reports: Raidionics and Raidionics-Slicer. Segmentation performances\nwere quite homogeneous across the four different brain tumor types, with an\naverage true positive Dice ranging between 80% and 90%, patient-wise recall\nbetween 88% and 98%, and patient-wise precision around 95%. With our Raidionics\nsoftware, running on a desktop computer with CPU support, tumor segmentation\ncan be performed in 16 to 54 seconds depending on the dimensions of the MRI\nvolume. For the generation of a standardized clinical report, including the\ntumor segmentation and features computation, 5 to 15 minutes are necessary. All\ntrained models have been made open-access together with the source code for\nboth software solutions and validation metrics computation. In the future, an\nautomatic classification of the brain tumor type would be necessary to replace\nmanual user input. Finally, the inclusion of post-operative segmentation in\nboth software solutions will be key for generating complete post-operative\nstandardized clinical reports.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bouget_D/0/1/0/all/0/1\">D. Bouget</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pedersen_A/0/1/0/all/0/1\">A. Pedersen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jakola_A/0/1/0/all/0/1\">A. S. Jakola</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kavouridis_V/0/1/0/all/0/1\">V. Kavouridis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Emblem_K/0/1/0/all/0/1\">K. E. Emblem</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Eijgelaar_R/0/1/0/all/0/1\">R. S. Eijgelaar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kommers_I/0/1/0/all/0/1\">I. Kommers</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ardon_H/0/1/0/all/0/1\">H. Ardon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Barkhof_F/0/1/0/all/0/1\">F. Barkhof</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bello_L/0/1/0/all/0/1\">L. Bello</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Berger_M/0/1/0/all/0/1\">M. S. Berger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nibali_M/0/1/0/all/0/1\">M. C. Nibali</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Furtner_J/0/1/0/all/0/1\">J. Furtner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hervey_Jumper_S/0/1/0/all/0/1\">S. Hervey-Jumper</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Idema_A/0/1/0/all/0/1\">A. J. S. Idema</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kiesel_B/0/1/0/all/0/1\">B. Kiesel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kloet_A/0/1/0/all/0/1\">A. Kloet</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mandonnet_E/0/1/0/all/0/1\">E. Mandonnet</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Muller_D/0/1/0/all/0/1\">D. M. J. M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Robe_P/0/1/0/all/0/1\">P. A. Robe</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rossi_M/0/1/0/all/0/1\">M. Rossi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sciortino_T/0/1/0/all/0/1\">T. Sciortino</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Brink_W/0/1/0/all/0/1\">W. Van den Brink</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wagemakers_M/0/1/0/all/0/1\">M. Wagemakers</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Widhalm_G/0/1/0/all/0/1\">G. Widhalm</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Witte_M/0/1/0/all/0/1\">M. G. Witte</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zwinderman_A/0/1/0/all/0/1\">A. H. Zwinderman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hamer_P/0/1/0/all/0/1\">P. C. De Witt Hamer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Solheim_O/0/1/0/all/0/1\">O. Solheim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Reinertsen_I/0/1/0/all/0/1\">I. Reinertsen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CogView2: Faster and Better Text-to-Image Generation via Hierarchical Transformers. (arXiv:2204.14217v1 [cs.CV])","link":"http://arxiv.org/abs/2204.14217","description":"<p>The development of the transformer-based text-to-image models are impeded by\nits slow generation and complexity for high-resolution images. In this work, we\nput forward a solution based on hierarchical transformers and local parallel\nauto-regressive generation. We pretrain a 6B-parameter transformer with a\nsimple and flexible self-supervised task, Cross-modal general language model\n(CogLM), and finetune it for fast super-resolution. The new text-to-image\nsystem, CogView2, shows very competitive generation compared to concurrent\nstate-of-the-art DALL-E-2, and naturally supports interactive text-guided\nediting on images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1\">Ming Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1\">Wendi Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_W/0/1/0/all/0/1\">Wenyi Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jie Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Application of machine learning methods to detect and classify Core images using GAN and texture recognition. (arXiv:2204.14224v1 [cs.CV])","link":"http://arxiv.org/abs/2204.14224","description":"<p>During exploration campaigns, oil companies rely heavily on drill core\nsamples as they provide valuable geological information that helps them find\nimportant oil deposits. Traditional core logging techniques are laborious and\nsubjective. Core imaging, a new technique in the oil industry, is used to\nsupplement analysis by rapidly characterising large quantities of drill cores\nin a nondestructive and noninvasive manner. In this paper, we will present the\nproblem of core detection and classification. The first problem is detecting\nthe cores and segmenting the holes in images by using Faster RCNN and Mask RCNN\nmodels respectively. The second problem is filling the hole in the core image\nby applying the Generative adversarial network(GAN) technique and using\nContextual Residual Aggregation(CRA) which creates high frequency residual for\nmissing contents in images. And finally applying Texture recognition models for\nthe classification of core images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nurseitov_D/0/1/0/all/0/1\">Daniyar Nurseitov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bostanbekov_K/0/1/0/all/0/1\">Kairat Bostanbekov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdimanap_G/0/1/0/all/0/1\">Galymzhan Abdimanap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdallah_A/0/1/0/all/0/1\">Abdelrahman Abdallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alimova_A/0/1/0/all/0/1\">Anel Alimova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurmangaliyev_D/0/1/0/all/0/1\">Darkhan Kurmangaliyev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recommendations on test datasets for evaluating AI solutions in pathology. (arXiv:2204.14226v1 [eess.IV])","link":"http://arxiv.org/abs/2204.14226","description":"<p>Artificial intelligence (AI) solutions that automatically extract information\nfrom digital histology images have shown great promise for improving\npathological diagnosis. Prior to routine use, it is important to evaluate their\npredictive performance and obtain regulatory approval. This assessment requires\nappropriate test datasets. However, compiling such datasets is challenging and\nspecific recommendations are missing.\n</p>\n<p>A committee of various stakeholders, including commercial AI developers,\npathologists, and researchers, discussed key aspects and conducted extensive\nliterature reviews on test datasets in pathology. Here, we summarize the\nresults and derive general recommendations for the collection of test datasets.\n</p>\n<p>We address several questions: Which and how many images are needed? How to\ndeal with low-prevalence subsets? How can potential bias be detected? How\nshould datasets be reported? What are the regulatory requirements in different\ncountries?\n</p>\n<p>The recommendations are intended to help AI developers demonstrate the\nutility of their products and to help regulatory agencies and end users verify\nreported performance measures. Further research is needed to formulate criteria\nfor sufficiently representative test datasets so that AI solutions can operate\nwith less user intervention and better support diagnostic workflows in the\nfuture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Homeyer_A/0/1/0/all/0/1\">Andr&#xe9; Homeyer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Geissler_C/0/1/0/all/0/1\">Christian Gei&#xdf;ler</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schwen_L/0/1/0/all/0/1\">Lars Ole Schwen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zakrzewski_F/0/1/0/all/0/1\">Falk Zakrzewski</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Evans_T/0/1/0/all/0/1\">Theodore Evans</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Strohmenger_K/0/1/0/all/0/1\">Klaus Strohmenger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Westphal_M/0/1/0/all/0/1\">Max Westphal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bulow_R/0/1/0/all/0/1\">Roman David B&#xfc;low</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kargl_M/0/1/0/all/0/1\">Michaela Kargl</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Karjauv_A/0/1/0/all/0/1\">Aray Karjauv</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Munne_Bertran_I/0/1/0/all/0/1\">Isidre Munn&#xe9;-Bertran</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Retzlaff_C/0/1/0/all/0/1\">Carl Orge Retzlaff</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Romero_Lopez_A/0/1/0/all/0/1\">Adri&#xe0; Romero-L&#xf3;pez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Soltysinski_T/0/1/0/all/0/1\">Tomasz So&#x142;tysi&#x144;ski</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Plass_M/0/1/0/all/0/1\">Markus Plass</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Carvalho_R/0/1/0/all/0/1\">Rita Carvalho</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Steinbach_P/0/1/0/all/0/1\">Peter Steinbach</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lan_Y/0/1/0/all/0/1\">Yu-Chia Lan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bouteldja_N/0/1/0/all/0/1\">Nassim Bouteldja</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Haber_D/0/1/0/all/0/1\">David Haber</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rojas_Carulla_M/0/1/0/all/0/1\">Mateo Rojas-Carulla</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sadr_A/0/1/0/all/0/1\">Alireza Vafaei Sadr</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kraft_M/0/1/0/all/0/1\">Matthias Kraft</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kruger_D/0/1/0/all/0/1\">Daniel Kr&#xfc;ger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fick_R/0/1/0/all/0/1\">Rutger Fick</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lang_T/0/1/0/all/0/1\">Tobias Lang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Boor_P/0/1/0/all/0/1\">Peter Boor</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Muller_H/0/1/0/all/0/1\">Heimo M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hufnagl_P/0/1/0/all/0/1\">Peter Hufnagl</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zerbe_N/0/1/0/all/0/1\">Norman Zerbe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hardware Trojan Detection Using Unsupervised Deep Learning on Quantum Diamond Microscope Magnetic Field Images. (arXiv:2204.14228v1 [cs.CV])","link":"http://arxiv.org/abs/2204.14228","description":"<p>This paper presents a method for hardware trojan detection in integrated\ncircuits. Unsupervised deep learning is used to classify wide field-of-view\n(4x4 mm$^2$), high spatial resolution magnetic field images taken using a\nQuantum Diamond Microscope (QDM). QDM magnetic imaging is enhanced using\nquantum control techniques and improved diamond material to increase magnetic\nfield sensitivity by a factor of 4 and measurement speed by a factor of 16 over\nprevious demonstrations. These upgrades facilitate the first demonstration of\nQDM magnetic field measurement for hardware trojan detection. Unsupervised\nconvolutional neural networks and clustering are used to infer trojan presence\nfrom unlabeled data sets of 600x600 pixel magnetic field images without human\nbias. This analysis is shown to be more accurate than principal component\nanalysis for distinguishing between field programmable gate arrays configured\nwith trojan free and trojan inserted logic. This framework is tested on a set\nof scalable trojans that we developed and measured with the QDM. Scalable and\nTrustHub trojans are detectable down to a minimum trojan trigger size of 0.5%\nof the total logic. The trojan detection framework can be used for golden-chip\nfree detection, since knowledge of the chips' identities is only used to\nevaluate detection accuracy\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ashok_M/0/1/0/all/0/1\">Maitreyi Ashok</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turner_M/0/1/0/all/0/1\">Matthew J. Turner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walsworth_R/0/1/0/all/0/1\">Ronald L. Walsworth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_E/0/1/0/all/0/1\">Edlyn V. Levine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandrakasan_A/0/1/0/all/0/1\">Anantha P. Chandrakasan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EndoMapper dataset of complete calibrated endoscopy procedures. (arXiv:2204.14240v1 [cs.CV])","link":"http://arxiv.org/abs/2204.14240","description":"<p>Computer-assisted systems are becoming broadly used in medicine. In\nendoscopy, most research focuses on automatic detection of polyps or other\npathologies, but localization and navigation of the endoscope is completely\nperformed manually by physicians. To broaden this research and bring spatial\nArtificial Intelligence to endoscopies, data from complete procedures are\nneeded. This data will be used to build a 3D mapping and localization systems\nthat can perform special task like, for example, detect blind zones during\nexploration, provide automatic polyp measurements, guide doctors to a polyp\nfound in a previous exploration and retrieve previous images of the same area\naligning them for easy comparison. These systems will provide an improvement in\nthe quality and precision of the procedures while lowering the burden on the\nphysicians. This paper introduces the Endomapper dataset, the first collection\nof complete endoscopy sequences acquired during regular medical practice,\nincluding slow and careful screening explorations, making secondary use of\nmedical data. Its original purpose is to facilitate the development and\nevaluation of VSLAM (Visual Simultaneous Localization and Mapping) methods in\nreal endoscopy data. The first release of the dataset is composed of 59\nsequences with more than 15 hours of video. It is also the first endoscopic\ndataset that includes both the computed geometric and photometric endoscope\ncalibration with the original calibration videos. Meta-data and annotations\nassociated to the dataset varies from anatomical landmark and description of\nthe procedure labeling, tools segmentation masks, COLMAP 3D reconstructions,\nsimulated sequences with groundtruth and meta-data related to special cases,\nsuch as sequences from the same patient. This information will improve the\nresearch in endoscopic VSLAM, as well as other research lines, and create new\nresearch lines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Azagra_P/0/1/0/all/0/1\">Pablo Azagra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sostres_C/0/1/0/all/0/1\">Carlos Sostres</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrandez_A/0/1/0/all/0/1\">&#xc1;ngel Ferrandez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riazuelo_L/0/1/0/all/0/1\">Luis Riazuelo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tomasini_C/0/1/0/all/0/1\">Clara Tomasini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barbed_O/0/1/0/all/0/1\">Oscar Le&#xf3;n Barbed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morlana_J/0/1/0/all/0/1\">Javier Morlana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Recasens_D/0/1/0/all/0/1\">David Recasens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batlle_V/0/1/0/all/0/1\">Victor M. Batlle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomez_Rodriguez_J/0/1/0/all/0/1\">Juan J. G&#xf3;mez-Rodr&#xed;guez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elvira_R/0/1/0/all/0/1\">Richard Elvira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lopez_J/0/1/0/all/0/1\">Julia L&#xf3;pez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oriol_C/0/1/0/all/0/1\">Cristina Oriol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Civera_J/0/1/0/all/0/1\">Javier Civera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tardos_J/0/1/0/all/0/1\">Juan D. Tard&#xf3;s</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murillo_A/0/1/0/all/0/1\">Ana Cristina Murillo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lanas_A/0/1/0/all/0/1\">Angel Lanas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Montiel_J/0/1/0/all/0/1\">Jos&#xe9; M.M. Montiel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLIP-Art: Contrastive Pre-training for Fine-Grained Art Classification. (arXiv:2204.14244v1 [cs.CV])","link":"http://arxiv.org/abs/2204.14244","description":"<p>Existing computer vision research in artwork struggles with artwork's\nfine-grained attributes recognition and lack of curated annotated datasets due\nto their costly creation. To the best of our knowledge, we are one of the first\nmethods to use CLIP (Contrastive Language-Image Pre-Training) to train a neural\nnetwork on a variety of artwork images and text descriptions pairs. CLIP is\nable to learn directly from free-form art descriptions, or, if available,\ncurated fine-grained labels. Model's zero-shot capability allows predicting\naccurate natural language description for a given image, without directly\noptimizing for the task. Our approach aims to solve 2 challenges: instance\nretrieval and fine-grained artwork attribute recognition. We use the iMet\nDataset, which we consider the largest annotated artwork dataset. In this\nbenchmark we achieved competitive results using only self-supervision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Conde_M/0/1/0/all/0/1\">Marcos V. Conde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turgutlu_K/0/1/0/all/0/1\">Kerem Turgutlu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OSSGAN: Open-Set Semi-Supervised Image Generation. (arXiv:2204.14249v1 [cs.CV])","link":"http://arxiv.org/abs/2204.14249","description":"<p>We introduce a challenging training scheme of conditional GANs, called\nopen-set semi-supervised image generation, where the training dataset consists\nof two parts: (i) labeled data and (ii) unlabeled data with samples belonging\nto one of the labeled data classes, namely, a closed-set, and samples not\nbelonging to any of the labeled data classes, namely, an open-set. Unlike the\nexisting semi-supervised image generation task, where unlabeled data only\ncontain closed-set samples, our task is more general and lowers the data\ncollection cost in practice by allowing open-set samples to appear. Thanks to\nentropy regularization, the classifier that is trained on labeled data is able\nto quantify sample-wise importance to the training of cGAN as confidence,\nallowing us to use all samples in unlabeled data. We design OSSGAN, which\nprovides decision clues to the discriminator on the basis of whether an\nunlabeled image belongs to one or none of the classes of interest, smoothly\nintegrating labeled and unlabeled data during training. The results of\nexperiments on Tiny ImageNet and ImageNet show notable improvements over\nsupervised BigGAN and semi-supervised methods. Our code is available at\nhttps://github.com/raven38/OSSGAN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Katsumata_K/0/1/0/all/0/1\">Kai Katsumata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vo_D/0/1/0/all/0/1\">Duc Minh Vo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakayama_H/0/1/0/all/0/1\">Hideki Nakayama</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VehicleNet: Learning Robust Visual Representation for Vehicle Re-identification. (arXiv:2004.06305v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2004.06305","description":"<p>One fundamental challenge of vehicle re-identification (re-id) is to learn\nrobust and discriminative visual representation, given the significant\nintra-class vehicle variations across different camera views. As the existing\nvehicle datasets are limited in terms of training images and viewpoints, we\npropose to build a unique large-scale vehicle dataset (called VehicleNet) by\nharnessing four public vehicle datasets, and design a simple yet effective\ntwo-stage progressive approach to learning more robust visual representation\nfrom VehicleNet. The first stage of our approach is to learn the generic\nrepresentation for all domains (i.e., source vehicle datasets) by training with\nthe conventional classification loss. This stage relaxes the full alignment\nbetween the training and testing domains, as it is agnostic to the target\nvehicle domain. The second stage is to fine-tune the trained model purely based\non the target vehicle set, by minimizing the distribution discrepancy between\nour VehicleNet and any target domain. We discuss our proposed multi-source\ndataset VehicleNet and evaluate the effectiveness of the two-stage progressive\nrepresentation learning through extensive experiments. We achieve the\nstate-of-art accuracy of 86.07% mAP on the private test set of AICity\nChallenge, and competitive results on two other public vehicle re-id datasets,\ni.e., VeRi-776 and VehicleID. We hope this new VehicleNet dataset and the\nlearned robust representations can pave the way for vehicle re-id in the\nreal-world environments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zhedong Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruan_T/0/1/0/all/0/1\">Tao Ruan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yunchao Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_T/0/1/0/all/0/1\">Tao Mei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Training Data Generating Networks: Shape Reconstruction via Bi-level Optimization. (arXiv:2010.08276v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2010.08276","description":"<p>We propose a novel 3d shape representation for 3d shape reconstruction from a\nsingle image. Rather than predicting a shape directly, we train a network to\ngenerate a training set which will be fed into another learning algorithm to\ndefine the shape. The nested optimization problem can be modeled by bi-level\noptimization. Specifically, the algorithms for bi-level optimization are also\nbeing used in meta learning approaches for few-shot learning. Our framework\nestablishes a link between 3D shape analysis and few-shot learning. We combine\ntraining data generating networks with bi-level optimization algorithms to\nobtain a complete framework for which all components can be jointly trained. We\nimprove upon recent work on standard benchmarks for 3d shape reconstruction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Biao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wonka_P/0/1/0/all/0/1\">Peter Wonka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Duality-Gated Mutual Condition Network for RGBT Tracking. (arXiv:2011.07188v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.07188","description":"<p>Low-quality modalities contain not only a lot of noisy information but also\nsome discriminative features in RGBT tracking. However, the potentials of\nlow-quality modalities are not well explored in existing RGBT tracking\nalgorithms. In this work, we propose a novel duality-gated mutual condition\nnetwork to fully exploit the discriminative information of all modalities while\nsuppressing the effects of data noise. In specific, we design a mutual\ncondition module, which takes the discriminative information of a modality as\nthe condition to guide feature learning of target appearance in another\nmodality. Such module can effectively enhance target representations of all\nmodalities even in the presence of low-quality modalities. To improve the\nquality of conditions and further reduce data noise, we propose a duality-gated\nmechanism and integrate it into the mutual condition module. To deal with the\ntracking failure caused by sudden camera motion, which often occurs in RGBT\ntracking, we design a resampling strategy based on optical flow algorithms. It\ndoes not increase much computational cost since we perform optical flow\ncalculation only when the model prediction is unreliable and then execute\nresampling when the sudden camera motion is detected. Extensive experiments on\nfour RGBT tracking benchmark datasets show that our method performs favorably\nagainst the state-of-the-art tracking algorithms\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_A/0/1/0/all/0/1\">Andong Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1\">Cun Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chenglong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jin Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RAMP-CNN: A Novel Neural Network for Enhanced Automotive Radar Object Recognition. (arXiv:2011.08981v2 [eess.SP] UPDATED)","link":"http://arxiv.org/abs/2011.08981","description":"<p>Millimeter-wave radars are being increasingly integrated into commercial\nvehicles to support new advanced driver-assistance systems by enabling robust\nand high-performance object detection, localization, as well as recognition - a\nkey component of new environmental perception. In this paper, we propose a\nnovel radar multiple-perspectives convolutional neural network (RAMP-CNN) that\nextracts the location and class of objects based on further processing of the\nrange-velocity-angle (RVA) heatmap sequences. To bypass the complexity of 4D\nconvolutional neural networks (NN), we propose to combine several\nlower-dimension NN models within our RAMP-CNN model that nonetheless approaches\nthe performance upper-bound with lower complexity. The extensive experiments\nshow that the proposed RAMP-CNN model achieves better average recall and\naverage precision than prior works in all testing scenarios. Besides, the\nRAMP-CNN model is validated to work robustly under nighttime, which enables\nlow-cost radars as a potential substitute for pure optical sensing under severe\nconditions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Gao_X/0/1/0/all/0/1\">Xiangyu Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xing_G/0/1/0/all/0/1\">Guanbin Xing</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Roy_S/0/1/0/all/0/1\">Sumit Roy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_H/0/1/0/all/0/1\">Hui Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TrackFormer: Multi-Object Tracking with Transformers. (arXiv:2101.02702v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.02702","description":"<p>The challenging task of multi-object tracking (MOT) requires simultaneous\nreasoning about track initialization, identity, and spatio-temporal\ntrajectories. We formulate this task as a frame-to-frame set prediction problem\nand introduce TrackFormer, an end-to-end trainable MOT approach based on an\nencoder-decoder Transformer architecture. Our model achieves data association\nbetween frames via attention by evolving a set of track predictions through a\nvideo sequence. The Transformer decoder initializes new tracks from static\nobject queries and autoregressively follows existing tracks in space and time\nwith the conceptually new and identity preserving track queries. Both query\ntypes benefit from self- and encoder-decoder attention on global frame-level\nfeatures, thereby omitting any additional graph optimization or modeling of\nmotion and/or appearance. TrackFormer introduces a new tracking-by-attention\nparadigm and while simple in its design is able to achieve state-of-the-art\nperformance on the task of multi-object tracking (MOT17 and MOT20) and\nsegmentation (MOTS20). The code is available at\nhttps://github.com/timmeinhardt/trackformer .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meinhardt_T/0/1/0/all/0/1\">Tim Meinhardt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirillov_A/0/1/0/all/0/1\">Alexander Kirillov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leal_Taixe_L/0/1/0/all/0/1\">Laura Leal-Taixe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feichtenhofer_C/0/1/0/all/0/1\">Christoph Feichtenhofer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Efficient Multitask Neural Network for Face Alignment, Head Pose Estimation and Face Tracking. (arXiv:2103.07615v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.07615","description":"<p>While Convolutional Neural Networks (CNNs) have significantly boosted the\nperformance of face related algorithms, maintaining accuracy and efficiency\nsimultaneously in practical use remains challenging. The state-of-the-art\nmethods employ deeper networks for better performance, which makes it less\npractical for mobile applications because of more parameters and higher\ncomputational complexity. Therefore, we propose an efficient multitask neural\nnetwork, Alignment &amp; Tracking &amp; Pose Network (ATPN) for face alignment, face\ntracking and head pose estimation. Specifically, to achieve better performance\nwith fewer layers for face alignment, we introduce a shortcut connection\nbetween shallow-layer and deep-layer features. We find the shallow-layer\nfeatures are highly correspond to facial boundaries that can provide the\nstructural information of face and it is crucial for face alignment. Moreover,\nwe generate a cheap heatmap based on the face alignment result and fuse it with\nfeatures to improve the performance of the other two tasks. Based on the\nheatmap, the network can utilize both geometric information of landmarks and\nappearance information for head pose estimation. The heatmap also provides\nattention clues for face tracking. The face tracking task also saves us the\nface detection procedure for each frame, which also significantly boost the\nreal-time capability for video-based tasks. We experimentally validate ATPN on\nfour benchmark datasets, WFLW, 300VW, WIDER Face and 300W-LP. The experimental\nresults demonstrate that it achieves better performance with much less\nparameters and lower computational complexity compared to other light models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xia_J/0/1/0/all/0/1\">Jiahao Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haimin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_S/0/1/0/all/0/1\">Shiping Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shuo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Min Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The GIST and RIST of Iterative Self-Training for Semi-Supervised Segmentation. (arXiv:2103.17105v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.17105","description":"<p>We consider the task of semi-supervised semantic segmentation, where we aim\nto produce pixel-wise semantic object masks given only a small number of\nhuman-labeled training examples. We focus on iterative self-training methods in\nwhich we explore the behavior of self-training over multiple refinement stages.\nWe show that iterative self-training leads to performance degradation if done\nna\\\"ively with a fixed ratio of human-labeled to pseudo-labeled training\nexamples. We propose Greedy Iterative Self-Training (GIST) and Random Iterative\nSelf-Training (RIST) strategies that alternate between training on either\nhuman-labeled data or pseudo-labeled data at each refinement stage, resulting\nin a performance boost rather than degradation. We further show that GIST and\nRIST can be combined with existing semi-supervised learning methods to boost\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Teh_E/0/1/0/all/0/1\">Eu Wern Teh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DeVries_T/0/1/0/all/0/1\">Terrance DeVries</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duke_B/0/1/0/all/0/1\">Brendan Duke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_R/0/1/0/all/0/1\">Ruowei Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aarabi_P/0/1/0/all/0/1\">Parham Aarabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taylor_G/0/1/0/all/0/1\">Graham W. Taylor</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pyramid Medical Transformer for Medical Image Segmentation. (arXiv:2104.14702v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.14702","description":"<p>Deep neural networks have been a prevailing technique in the field of medical\nimage processing. However, the most popular convolutional neural networks\n(CNNs) based methods for medical image segmentation are imperfect because they\nmodel long-range dependencies by stacking layers or enlarging filters.\nTransformers and the self-attention mechanism are recently proposed to\neffectively learn long-range dependencies by modeling all pairs of word-to-word\nattention regardless of their positions. The idea has also been extended to the\ncomputer vision field by creating and treating image patches as embeddings.\nConsidering the computation complexity for whole image self-attention, current\ntransformer-based models settle for a rigid partitioning scheme that\npotentially loses informative relations. Besides, current medical transformers\nmodel global context on full resolution images, leading to unnecessary\ncomputation costs. To address these issues, we developed a novel method to\nintegrate multi-scale attention and CNN feature extraction using a pyramidal\nnetwork architecture, namely Pyramid Medical Transformer (PMTrans). The PMTrans\ncaptured multi-range relations by working on multi-resolution images. An\nadaptive partitioning scheme was implemented to retain informative relations\nand to access different receptive fields efficiently. Experimental results on\nthree medical image datasets (gland segmentation, MoNuSeg, and HECKTOR\ndatasets) showed that PMTrans outperformed the latest CNN-based and\ntransformer-based models for medical image segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhuangzhuang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weixiong Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Smaller Is Better: An Analysis of Instance Quantity/Quality Trade-off in Rehearsal-based Continual Learning. (arXiv:2105.14106v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.14106","description":"<p>The design of machines and algorithms capable of learning in a dynamically\nchanging environment has become an increasingly topical problem with the\nincrease of the size and heterogeneity of data available to learning systems.\nAs a consequence, the key issue of Continual Learning has become that of\naddressing the stability-plasticity dilemma of connectionist systems, as they\nneed to adapt their model without forgetting previously acquired knowledge.\nWithin this context, rehearsal-based methods i.e., solutions in where the\nlearner exploits memory to revisit past data, has proven to be very effective,\nleading to performance at the state-of-the-art. In our study, we propose an\nanalysis of the memory quantity/quality trade-off adopting various data\nreduction approaches to increase the number of instances storable in memory. In\nparticular, we investigate complex instance compression techniques such as deep\nencoders, but also trivial approaches such as image resizing and linear\ndimensionality reduction. Our findings suggest that the optimal trade-off is\nseverely skewed toward instance quantity, where rehearsal approaches with\nseveral heavily compressed instances easily outperform state-of-the-art\napproaches with the same amount of memory at their disposal. Further, in high\nmemory configurations, deep approaches extracting spatial structure combined\nwith extreme resizing (of the order of $8\\times8$ images) yield the best\nresults, while in memory-constrained configurations where deep approaches\ncannot be used due to their memory requirement in training, Extreme Learning\nMachines (ELM) offer a clear advantage.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pelosin_F/0/1/0/all/0/1\">Francesco Pelosin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torsello_A/0/1/0/all/0/1\">Andrea Torsello</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DETReg: Unsupervised Pretraining with Region Priors for Object Detection. (arXiv:2106.04550v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.04550","description":"<p>Recent self-supervised pretraining methods for object detection largely focus\non pretraining the backbone of the object detector, neglecting key parts of\ndetection architecture. Instead, we introduce DETReg, a new self-supervised\nmethod that pretrains the entire object detection network, including the object\nlocalization and embedding components. During pretraining, DETReg predicts\nobject localizations to match the localizations from an unsupervised region\nproposal generator and simultaneously aligns the corresponding feature\nembeddings with embeddings from a self-supervised image encoder. We implement\nDETReg using the DETR family of detectors and show that it improves over\ncompetitive baselines when finetuned on COCO, PASCAL VOC, and Airbus Ship\nbenchmarks. In low-data regimes, including semi-supervised and few-shot\nlearning settings, DETReg establishes many state-of-the-art results, e.g., on\nCOCO we see a +6.0 AP improvement for 10-shot detection and over 2 AP\nimprovements when training with only 1\\% of the labels. For code and pretrained\nmodels, visit the project page at https://amirbar.net/detreg\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bar_A/0/1/0/all/0/1\">Amir Bar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kantorov_V/0/1/0/all/0/1\">Vadim Kantorov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reed_C/0/1/0/all/0/1\">Colorado J Reed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herzig_R/0/1/0/all/0/1\">Roei Herzig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chechik_G/0/1/0/all/0/1\">Gal Chechik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohrbach_A/0/1/0/all/0/1\">Anna Rohrbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1\">Trevor Darrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Globerson_A/0/1/0/all/0/1\">Amir Globerson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to See by Looking at Noise. (arXiv:2106.05963v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.05963","description":"<p>Current vision systems are trained on huge datasets, and these datasets come\nwith costs: curation is expensive, they inherit human biases, and there are\nconcerns over privacy and usage rights. To counter these costs, interest has\nsurged in learning from cheaper data sources, such as unlabeled images. In this\npaper we go a step further and ask if we can do away with real image datasets\nentirely, instead learning from noise processes. We investigate a suite of\nimage generation models that produce images from simple random processes. These\nare then used as training data for a visual representation learner with a\ncontrastive loss. We study two types of noise processes, statistical image\nmodels and deep generative models under different random initializations. Our\nfindings show that it is important for the noise to capture certain structural\nproperties of real data but that good performance can be achieved even with\nprocesses that are far from realistic. We also find that diversity is a key\nproperty to learn good representations. Datasets, models, and code are\navailable at https://mbaradad.github.io/learning_with_noise.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baradad_M/0/1/0/all/0/1\">Manel Baradad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wulff_J/0/1/0/all/0/1\">Jonas Wulff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tongzhou Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Isola_P/0/1/0/all/0/1\">Phillip Isola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1\">Antonio Torralba</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GLASS: Geometric Latent Augmentation for Shape Spaces. (arXiv:2108.03225v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.03225","description":"<p>We investigate the problem of training generative models on a very sparse\ncollection of 3D models. We use geometrically motivated energies to augment and\nthus boost a sparse collection of example (training) models. We analyze the\nHessian of the as-rigid-as-possible (ARAP) energy to sample from and project to\nthe underlying (local) shape space, and use the augmented dataset to train a\nvariational autoencoder (VAE). We iterate the process of building latent spaces\nof VAE and augmenting the associated dataset, to progressively reveal a richer\nand more expressive generative space for creating geometrically and\nsemantically valid samples. Our framework allows us to train generative 3D\nmodels even with a small set of good quality 3D models, which are typically\nhard to curate. We extensively evaluate our method against a set of strong\nbaselines, provide ablation studies and demonstrate application towards\nestablishing shape correspondences. We present multiple examples of interesting\nand meaningful shape variations even when starting from as few as 3-10 training\nshapes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Muralikrishnan_S/0/1/0/all/0/1\">Sanjeev Muralikrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhuri_S/0/1/0/all/0/1\">Siddhartha Chaudhuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aigerman_N/0/1/0/all/0/1\">Noam Aigerman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_V/0/1/0/all/0/1\">Vladimir Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fisher_M/0/1/0/all/0/1\">Matthew Fisher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitra_N/0/1/0/all/0/1\">Niloy Mitra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Re-using Adversarial Mask Discriminators for Test-time Training under Distribution Shifts. (arXiv:2108.11926v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.11926","description":"<p>Thanks to their ability to learn flexible data-driven losses, Generative\nAdversarial Networks (GANs) are an integral part of many semi- and\nweakly-supervised methods for medical image segmentation. GANs jointly optimise\na generator and an adversarial discriminator on a set of training data. After\ntraining is complete, the discriminator is usually discarded, and only the\ngenerator is used for inference. But should we discard discriminators? In this\nwork, we argue that training stable discriminators produces expressive loss\nfunctions that we can re-use at inference to detect and \\textit{correct}\nsegmentation mistakes. First, we identify key challenges and suggest possible\nsolutions to make discriminators re-usable at inference. Then, we show that we\ncan combine discriminators with image reconstruction costs (via decoders) to\nendow a causal perspective to test-time training and further improve the model.\nOur method is simple and improves the test-time performance of pre-trained\nGANs. Moreover, we show that it is compatible with standard post-processing\ntechniques and it has the potential to be used for Online Continual Learning.\nWith our work, we open new research avenues for re-using adversarial\ndiscriminators at inference. Our code is available at\nhttps://vios-s.github.io/adversarial-test-time-training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Valvano_G/0/1/0/all/0/1\">Gabriele Valvano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leo_A/0/1/0/all/0/1\">Andrea Leo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsaftaris_S/0/1/0/all/0/1\">Sotirios A. Tsaftaris</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Supervised Contrastive Learning for Detecting Anomalous Driving Behaviours from Multimodal Videos. (arXiv:2109.04021v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.04021","description":"<p>Distracted driving is one of the major reasons for vehicle accidents.\nTherefore, detecting distracted driving behaviors is of paramount importance to\nreduce the millions of deaths and injuries occurring worldwide. Distracted or\nanomalous driving behaviors are deviations from 'normal' driving that need to\nbe identified correctly to alert the driver. However, these driving behaviors\ndo not comprise one specific type of driving style and their distribution can\nbe different during the training and test phases of a classifier. We formulate\nthis problem as a supervised contrastive learning approach to learn a visual\nrepresentation to detect normal, and seen and unseen anomalous driving\nbehaviors. We made a change to the standard contrastive loss function to adjust\nthe similarity of negative pairs to aid the optimization. Normally, in a (self)\nsupervised contrastive framework, the projection head layers are omitted during\nthe test phase as the encoding layers are considered to contain general visual\nrepresentative information. However, we assert that for a video-based\nsupervised contrastive learning task, including a projection head can be\nbeneficial. We showed our results on a driver anomaly detection dataset that\ncontains 783 minutes of video recordings of normal and anomalous driving\nbehaviors of 31 drivers from the various top and front cameras (both depth and\ninfrared). Out of 9 video modalities combinations, our proposed contrastive\napproach improved the ROC AUC on 6 in comparison to the baseline models (from\n4.23% to 8.91% for different modalities). We performed statistical tests that\nshowed evidence that our proposed method performs better than the baseline\ncontrastive learning setup. Finally, the results showed that the fusion of\ndepth and infrared modalities from the top and front views achieved the best\nAUC ROC of 0.9738 and AUC PR of 0.9772.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1\">Shehroz S. Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Ziting Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Haoying Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_A/0/1/0/all/0/1\">Ax Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abedi_A/0/1/0/all/0/1\">Ali Abedi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ROS-X-Habitat: Bridging the ROS Ecosystem with Embodied AI. (arXiv:2109.07703v3 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2109.07703","description":"<p>We introduce ROS-X-Habitat, a software interface that bridges the AI Habitat\nplatform for embodied learning-based agents with other robotics resources via\nROS. This interface not only offers standardized communication protocols\nbetween embodied agents and simulators, but also enables physically and\nphotorealistic simulation that benefits the training and/or testing of\nvision-based embodied agents. With this interface, roboticists can evaluate\ntheir own Habitat RL agents in another ROS-based simulator or use Habitat Sim\nv2 as the test bed for their own robotic algorithms. Through in silico\nexperiments, we demonstrate that ROS-X-Habitat has minimal impact on the\nnavigation performance and simulation speed of a Habitat RGBD agent; that a\nstandard set of ROS mapping, planning and navigation tools can run in Habitat\nSim v2; and that a Habitat agent can run in the standard ROS simulator Gazebo.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guanxiong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haoyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitchell_I/0/1/0/all/0/1\">Ian M. Mitchell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsolved Problems in ML Safety. (arXiv:2109.13916v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2109.13916","description":"<p>Machine learning (ML) systems are rapidly increasing in size, are acquiring\nnew capabilities, and are increasingly deployed in high-stakes settings. As\nwith other powerful technologies, safety for ML should be a leading research\npriority. In response to emerging safety challenges in ML, such as those\nintroduced by recent large-scale models, we provide a new roadmap for ML Safety\nand refine the technical problems that the field needs to address. We present\nfour problems ready for research, namely withstanding hazards (\"Robustness\"),\nidentifying hazards (\"Monitoring\"), reducing inherent model hazards\n(\"Alignment\"), and reducing systemic hazards (\"Systemic Safety\"). Throughout,\nwe clarify each problem's motivation and provide concrete research directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hendrycks_D/0/1/0/all/0/1\">Dan Hendrycks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carlini_N/0/1/0/all/0/1\">Nicholas Carlini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schulman_J/0/1/0/all/0/1\">John Schulman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steinhardt_J/0/1/0/all/0/1\">Jacob Steinhardt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLIP-Forge: Towards Zero-Shot Text-to-Shape Generation. (arXiv:2110.02624v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.02624","description":"<p>Generating shapes using natural language can enable new ways of imagining and\ncreating the things around us. While significant recent progress has been made\nin text-to-image generation, text-to-shape generation remains a challenging\nproblem due to the unavailability of paired text and shape data at a large\nscale. We present a simple yet effective method for zero-shot text-to-shape\ngeneration that circumvents such data scarcity. Our proposed method, named\nCLIP-Forge, is based on a two-stage training process, which only depends on an\nunlabelled shape dataset and a pre-trained image-text network such as CLIP. Our\nmethod has the benefits of avoiding expensive inference time optimization, as\nwell as the ability to generate multiple shapes for a given text. We not only\ndemonstrate promising zero-shot generalization of the CLIP-Forge model\nqualitatively and quantitatively, but also provide extensive comparative\nevaluations to better understand its behavior.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sanghi_A/0/1/0/all/0/1\">Aditya Sanghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_H/0/1/0/all/0/1\">Hang Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lambourne_J/0/1/0/all/0/1\">Joseph G. Lambourne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Ye Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1\">Chin-Yi Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fumero_M/0/1/0/all/0/1\">Marco Fumero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malekshan_K/0/1/0/all/0/1\">Kamal Rahimi Malekshan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"To Trust or Not To Trust Prediction Scores for Membership Inference Attacks. (arXiv:2111.09076v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2111.09076","description":"<p>Membership inference attacks (MIAs) aim to determine whether a specific\nsample was used to train a predictive model. Knowing this may indeed lead to a\nprivacy breach. Most MIAs, however, make use of the model's prediction scores -\nthe probability of each output given some input - following the intuition that\nthe trained model tends to behave differently on its training data. We argue\nthat this is a fallacy for many modern deep network architectures.\nConsequently, MIAs will miserably fail since overconfidence leads to high\nfalse-positive rates not only on known domains but also on out-of-distribution\ndata and implicitly acts as a defense against MIAs. Specifically, using\ngenerative adversarial networks, we are able to produce a potentially infinite\nnumber of samples falsely classified as part of the training data. In other\nwords, the threat of MIAs is overestimated, and less information is leaked than\npreviously assumed. Moreover, there is actually a trade-off between the\noverconfidence of models and their susceptibility to MIAs: the more classifiers\nknow when they do not know, making low confidence predictions, the more they\nreveal the training data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hintersdorf_D/0/1/0/all/0/1\">Dominik Hintersdorf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Struppek_L/0/1/0/all/0/1\">Lukas Struppek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1\">Kristian Kersting</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ice hockey player identification via transformers and weakly supervised learning. (arXiv:2111.11535v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.11535","description":"<p>Identifying players in video is a foundational step in computer vision-based\nsports analytics. Obtaining player identities is essential for analyzing the\ngame and is used in downstream tasks such as game event recognition.\nTransformers are the existing standard in Natural Language Processing (NLP) and\nare swiftly gaining traction in computer vision. Motivated by the increasing\nsuccess of transformers in computer vision, in this paper, we introduce a\ntransformer network for recognizing players through their jersey numbers in\nbroadcast National Hockey League (NHL) videos. The transformer takes temporal\nsequences of player frames (also called player tracklets) as input and outputs\nthe probabilities of jersey numbers present in the frames. The proposed network\nperforms better than the previous benchmark on the dataset used. We implement a\nweakly-supervised training approach by generating approximate frame-level\nlabels for jersey number presence and use the frame-level labels for faster\ntraining. We also utilize player shifts available in the NHL play-by-play data\nby reading the game time using optical character recognition (OCR) to get the\nplayers on the ice rink at a certain game time. Using player shifts improved\nthe player identification accuracy by 6%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vats_K/0/1/0/all/0/1\">Kanav Vats</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McNally_W/0/1/0/all/0/1\">William McNally</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walters_P/0/1/0/all/0/1\">Pascale Walters</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clausi_D/0/1/0/all/0/1\">David A. Clausi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zelek_J/0/1/0/all/0/1\">John S. Zelek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Intuitive Shape Editing in Latent Space. (arXiv:2111.12488v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.12488","description":"<p>The use of autoencoders for shape editing or generation through latent space\nmanipulation suffers from unpredictable changes in the output shape. Our\nautoencoder-based method enables intuitive shape editing in latent space by\ndisentangling latent sub-spaces into style variables and control points on the\nsurface that can be manipulated independently. The key idea is adding a\nLipschitz-type constraint to the loss function, i.e. bounding the change of the\noutput shape proportionally to the change in latent space, leading to\ninterpretable latent space representations. The control points on the surface\nthat are part of the latent code of an object can then be freely moved,\nallowing for intuitive shape editing directly in latent space. We evaluate our\nmethod by comparing to state-of-the-art data-driven shape editing methods. We\nfurther demonstrate the expressiveness of our learned latent space by\nleveraging it for unsupervised part segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Elsner_T/0/1/0/all/0/1\">Tim Elsner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ibing_M/0/1/0/all/0/1\">Moritz Ibing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Czech_V/0/1/0/all/0/1\">Victor Czech</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nehring_Wirxel_J/0/1/0/all/0/1\">Julius Nehring-Wirxel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kobbelt_L/0/1/0/all/0/1\">Leif Kobbelt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MHFormer: Multi-Hypothesis Transformer for 3D Human Pose Estimation. (arXiv:2111.12707v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.12707","description":"<p>Estimating 3D human poses from monocular videos is a challenging task due to\ndepth ambiguity and self-occlusion. Most existing works attempt to solve both\nissues by exploiting spatial and temporal relationships. However, those works\nignore the fact that it is an inverse problem where multiple feasible solutions\n(i.e., hypotheses) exist. To relieve this limitation, we propose a\nMulti-Hypothesis Transformer (MHFormer) that learns spatio-temporal\nrepresentations of multiple plausible pose hypotheses. In order to effectively\nmodel multi-hypothesis dependencies and build strong relationships across\nhypothesis features, the task is decomposed into three stages: (i) Generate\nmultiple initial hypothesis representations; (ii) Model self-hypothesis\ncommunication, merge multiple hypotheses into a single converged representation\nand then partition it into several diverged hypotheses; (iii) Learn\ncross-hypothesis communication and aggregate the multi-hypothesis features to\nsynthesize the final 3D pose. Through the above processes, the final\nrepresentation is enhanced and the synthesized pose is much more accurate.\nExtensive experiments show that MHFormer achieves state-of-the-art results on\ntwo challenging datasets: Human3.6M and MPI-INF-3DHP. Without bells and\nwhistles, its performance surpasses the previous best result by a large margin\nof 3% on Human3.6M. Code and models are available at\n\\url{https://github.com/Vegetebird/MHFormer}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenhao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pichao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PokeBNN: A Binary Pursuit of Lightweight Accuracy. (arXiv:2112.00133v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2112.00133","description":"<p>Optimization of Top-1 ImageNet promotes enormous networks that may be\nimpractical in inference settings. Binary neural networks (BNNs) have the\npotential to significantly lower the compute intensity but existing models\nsuffer from low quality. To overcome this deficiency, we propose PokeConv, a\nbinary convolution block which improves quality of BNNs by techniques such as\nadding multiple residual paths, and tuning the activation function. We apply it\nto ResNet-50 and optimize ResNet's initial convolutional layer which is hard to\nbinarize. We name the resulting network family PokeBNN. These techniques are\nchosen to yield favorable improvements in both top-1 accuracy and the network's\ncost. In order to enable joint optimization of the cost together with accuracy,\nwe define arithmetic computation effort (ACE), a hardware- and energy-inspired\ncost metric for quantized and binarized networks. We also identify a need to\noptimize an under-explored hyper-parameter controlling the binarization\ngradient approximation.\n</p>\n<p>We establish a new, strong state-of-the-art (SOTA) on top-1 accuracy together\nwith commonly-used CPU64 cost, ACE cost and network size metrics.\nReActNet-Adam, the previous SOTA in BNNs, achieved a 70.5% top-1 accuracy with\n7.9 ACE. A small variant of PokeBNN achieves 70.5% top-1 with 2.6 ACE, more\nthan 3x reduction in cost; a larger PokeBNN achieves 75.6% top-1 with 7.8 ACE,\nmore than 5% improvement in accuracy without increasing the cost. PokeBNN\nimplementation in JAX/Flax and reproduction instructions are available in AQT\nrepository: https://github.com/google/aqt\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yichi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhiru Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lew_L/0/1/0/all/0/1\">Lukasz Lew</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GANORCON: Are Generative Models Useful for Few-shot Segmentation?. (arXiv:2112.00854v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.00854","description":"<p>Advances in generative modeling based on GANs has motivated the community to\nfind their use beyond image generation and editing tasks. In particular,\nseveral recent works have shown that GAN representations can be re-purposed for\ndiscriminative tasks such as part segmentation, especially when training data\nis limited. But how do these improvements stack-up against recent advances in\nself-supervised learning? Motivated by this we present an alternative approach\nbased on contrastive learning and compare their performance on standard\nfew-shot part segmentation benchmarks. Our experiments reveal that not only do\nthe GAN-based approach offer no significant performance advantage, their\nmulti-step training is complex, nearly an order-of-magnitude slower, and can\nintroduce additional bias. These experiments suggest that the inductive biases\nof generative models, such as their ability to disentangle shape and texture,\nare well captured by standard feed-forward networks trained using contrastive\nlearning. These experiments suggest that the inductive biases present in\ncurrent generative models, such as their ability to disentangle shape and\ntexture, are well captured by standard feed-forward networks trained using\ncontrastive learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saha_O/0/1/0/all/0/1\">Oindrila Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zezhou Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maji_S/0/1/0/all/0/1\">Subhransu Maji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CAVER: Cross-Modal View-Mixed Transformer for Bi-Modal Salient Object Detection. (arXiv:2112.02363v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.02363","description":"<p>Most of the existing bi-modal (RGB-D and RGB-T) salient object detection\nmethods utilize the convolution operation and construct complex interweave\nfusion structures to achieve cross-modal information integration. The inherent\nlocal connectivity of the convolution operation constrains the performance of\nthe convolution-based methods to a ceiling. In this work, we rethink these\ntasks from the perspective of global information alignment and transformation.\nSpecifically, the proposed \\underline{c}ross-mod\\underline{a}l\n\\underline{v}iew-mixed transform\\underline{er} (CAVER) cascades several\ncross-modal integration units to construct a top-down transformer-based\ninformation propagation path. CAVER treats the multi-scale and multi-modal\nfeature integration as a sequence-to-sequence context propagation and update\nprocess built on a novel view-mixed attention mechanism. Besides, considering\nthe quadratic complexity w.r.t. the number of input tokens, we design a\nparameter-free patch-wise token re-embedding strategy to simplify operations.\nExtensive experimental results on RGB-D and RGB-T SOD datasets demonstrate that\nsuch a simple two-stream encoder-decoder framework can surpass recent\nstate-of-the-art methods when it is equipped with the proposed components.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pang_Y/0/1/0/all/0/1\">Youwei Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xiaoqi Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lihe Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Huchuan Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GUNNEL: Guided Mixup Augmentation and Multi-View Fusion for Aquatic Animal Segmentation. (arXiv:2112.06193v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.06193","description":"<p>Recent years have witnessed great advances in object segmentation research.\nIn addition to generic objects, aquatic animals have attracted research\nattention. Deep learning-based methods are widely used for aquatic animal\nsegmentation and have achieved promising performance. However, there is a lack\nof challenging datasets for benchmarking. In this work, we build a new dataset\ndubbed \"Aquatic Animal Species.\" We also devise a novel GUided mixup\naugmeNtatioN and multi-viEw fusion for aquatic animaL segmentation (GUNNEL)\nthat leverages the advantages of multiple view segmentation models to\neffectively segment aquatic animals and improves the training performance by\nsynthesizing hard samples. Extensive experiments demonstrated the superiority\nof our proposed framework over existing state-of-the-art instance segmentation\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Le_M/0/1/0/all/0/1\">Minh-Quan Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1\">Trung-Nghia Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tam V. Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Echizen_I/0/1/0/all/0/1\">Isao Echizen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_M/0/1/0/all/0/1\">Minh-Triet Tran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepAdversaries: Examining the Robustness of Deep Learning Models for Galaxy Morphology Classification. (arXiv:2112.14299v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2112.14299","description":"<p>Data processing and analysis pipelines in cosmological survey experiments\nintroduce data perturbations that can significantly degrade the performance of\ndeep learning-based models. Given the increased adoption of supervised deep\nlearning methods for processing and analysis of cosmological survey data, the\nassessment of data perturbation effects and the development of methods that\nincrease model robustness are increasingly important. In the context of\nmorphological classification of galaxies, we study the effects of perturbations\nin imaging data. In particular, we examine the consequences of using neural\nnetworks when training on baseline data and testing on perturbed data. We\nconsider perturbations associated with two primary sources: 1) increased\nobservational noise as represented by higher levels of Poisson noise and 2)\ndata processing noise incurred by steps such as image compression or telescope\nerrors as represented by one-pixel adversarial attacks. We also test the\nefficacy of domain adaptation techniques in mitigating the perturbation-driven\nerrors. We use classification accuracy, latent space visualizations, and latent\nspace distance to assess model robustness. Without domain adaptation, we find\nthat processing pixel-level errors easily flip the classification into an\nincorrect class and that higher observational noise makes the model trained on\nlow-noise data unable to classify galaxy morphologies. On the other hand, we\nshow that training with domain adaptation improves model robustness and\nmitigates the effects of these perturbations, improving the classification\naccuracy by 23% on data with higher observational noise. Domain adaptation also\nincreases by a factor of ~2.3 the latent space distance between the baseline\nand the incorrectly classified one-pixel perturbed image, making the model more\nrobust to inadvertent perturbations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ciprijanovic_A/0/1/0/all/0/1\">Aleksandra &#x106;iprijanovi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kafkes_D/0/1/0/all/0/1\">Diana Kafkes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snyder_G/0/1/0/all/0/1\">Gregory Snyder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_F/0/1/0/all/0/1\">F. Javier S&#xe1;nchez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perdue_G/0/1/0/all/0/1\">Gabriel Nathan Perdue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pedro_K/0/1/0/all/0/1\">Kevin Pedro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nord_B/0/1/0/all/0/1\">Brian Nord</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madireddy_S/0/1/0/all/0/1\">Sandeep Madireddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wild_S/0/1/0/all/0/1\">Stefan M. Wild</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StyleGAN-V: A Continuous Video Generator with the Price, Image Quality and Perks of StyleGAN2. (arXiv:2112.14683v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.14683","description":"<p>Videos show continuous events, yet most $-$ if not all $-$ video synthesis\nframeworks treat them discretely in time. In this work, we think of videos of\nwhat they should be $-$ time-continuous signals, and extend the paradigm of\nneural representations to build a continuous-time video generator. For this, we\nfirst design continuous motion representations through the lens of positional\nembeddings. Then, we explore the question of training on very sparse videos and\ndemonstrate that a good generator can be learned by using as few as 2 frames\nper clip. After that, we rethink the traditional image + video discriminators\npair and design a holistic discriminator that aggregates temporal information\nby simply concatenating frames' features. This decreases the training cost and\nprovides richer learning signal to the generator, making it possible to train\ndirectly on 1024$^2$ videos for the first time. We build our model on top of\nStyleGAN2 and it is just ${\\approx}5\\%$ more expensive to train at the same\nresolution while achieving almost the same image quality. Moreover, our latent\nspace features similar properties, enabling spatial manipulations that our\nmethod can propagate in time. We can generate arbitrarily long videos at\narbitrary high frame rate, while prior work struggles to generate even 64\nframes at a fixed rate. Our model is tested on four modern 256$^2$ and one\n1024$^2$-resolution video synthesis benchmarks. In terms of sheer metrics, it\nperforms on average ${\\approx}30\\%$ better than the closest runner-up. Project\nwebsite: https://universome.github.io.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Skorokhodov_I/0/1/0/all/0/1\">Ivan Skorokhodov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tulyakov_S/0/1/0/all/0/1\">Sergey Tulyakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elhoseiny_M/0/1/0/all/0/1\">Mohamed Elhoseiny</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conditional Generative Data-free Knowledge Distillation. (arXiv:2112.15358v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.15358","description":"<p>Knowledge distillation has made remarkable achievements in model compression.\nHowever, most existing methods require the original training data, which is\nusually unavailable due to privacy and security issues. In this paper, we\npropose a conditional generative data-free knowledge distillation (CGDD)\nframework for training lightweight networks without any training data. This\nmethod realizes efficient knowledge distillation based on conditional image\ngeneration. Specifically, we treat the preset labels as ground truth to train a\nconditional generator in a semi-supervised manner. The trained generator can\nproduce specified classes of training images. For training the student network,\nwe force it to extract the knowledge hidden in teacher feature maps, which\nprovide crucial cues for the learning process. Moreover, an adversarial\ntraining framework for promoting distillation performance is constructed by\ndesigning several loss functions. This framework helps the student model to\nexplore larger data space. To demonstrate the effectiveness of the proposed\nmethod, we conduct extensive experiments on different datasets. Compared with\nother data-free works, our work obtains state-of-the-art results on CIFAR100,\nCaltech101, and different versions of ImageNet datasets. The codes will be\nreleased.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xinyi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_L/0/1/0/all/0/1\">Ling Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Libo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ou_L/0/1/0/all/0/1\">Linlin Ou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLIP-Event: Connecting Text and Images with Event Structures. (arXiv:2201.05078v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.05078","description":"<p>Vision-language (V+L) pretraining models have achieved great success in\nsupporting multimedia applications by understanding the alignments between\nimages and text. While existing vision-language pretraining models primarily\nfocus on understanding objects in images or entities in text, they often ignore\nthe alignment at the level of events and their argument structures. In this\nwork, we propose a contrastive learning framework to enforce vision-language\npretraining models to comprehend events and associated argument (participant)\nroles. To achieve this, we take advantage of text information extraction\ntechnologies to obtain event structural knowledge, and utilize multiple prompt\nfunctions to contrast difficult negative descriptions by manipulating event\nstructures. We also design an event graph alignment loss based on optimal\ntransport to capture event argument structures. In addition, we collect a large\nevent-rich dataset (106,875 images) for pretraining, which provides a more\nchallenging image retrieval benchmark to assess the understanding of\ncomplicated lengthy sentences. Experiments show that our zero-shot CLIP-Event\noutperforms the state-of-the-art supervised model in argument extraction on\nMultimedia Event Extraction, achieving more than 5% absolute F-score gain in\nevent extraction, as well as significant improvements on a variety of\ndownstream tasks under zero-shot settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Manling Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ruochen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuohang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Luowei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xudong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1\">Michael Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shih-Fu Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive and Selective Hidden Embeddings for Medical Image Segmentation. (arXiv:2201.08779v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.08779","description":"<p>Medical image segmentation has been widely recognized as a pivot procedure\nfor clinical diagnosis, analysis, and treatment planning. However, the\nlaborious and expensive annotation process lags down the speed of further\nadvances. Contrastive learning-based weight pre-training provides an\nalternative by leveraging unlabeled data to learn a good representation. In\nthis paper, we investigate how contrastive learning benefits the general\nsupervised medical segmentation tasks. To this end, patch-dragsaw contrastive\nregularization (PDCR) is proposed to perform patch-level tugging and repulsing\nwith the extent controlled by a continuous affinity score. And a new structure\ndubbed uncertainty-aware feature selection block (UAFS) is designed to perform\nthe feature selection process, which can handle the learning target shift\ncaused by minority features with high uncertainty. By plugging the proposed 2\nmodules into the existing segmentation architecture, we achieve\nstate-of-the-art results across 8 public datasets from 6 domains. Newly\ndesigned modules further decrease the amount of training data to a quarter\nwhile achieving comparable, if not better, performances. From this perspective,\nwe take the opposite direction of the original self/un-supervised contrastive\nlearning by further excavating information contained within the label.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhuowei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zihao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhiqiang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_Q/0/1/0/all/0/1\">Qing Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_R/0/1/0/all/0/1\">Ruiqin Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shaoting Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metaxas_D/0/1/0/all/0/1\">Dimitris Metaxas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1\">Tingting Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero Experience Required: Plug & Play Modular Transfer Learning for Semantic Visual Navigation. (arXiv:2202.02440v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.02440","description":"<p>In reinforcement learning for visual navigation, it is common to develop a\nmodel for each new task, and train that model from scratch with task-specific\ninteractions in 3D environments. However, this process is expensive; massive\namounts of interactions are needed for the model to generalize well. Moreover,\nthis process is repeated whenever there is a change in the task type or the\ngoal modality. We present a unified approach to visual navigation using a novel\nmodular transfer learning model. Our model can effectively leverage its\nexperience from one source task and apply it to multiple target tasks (e.g.,\nObjectNav, RoomNav, ViewNav) with various goal modalities (e.g., image, sketch,\naudio, label). Furthermore, our model enables zero-shot experience learning,\nwhereby it can solve the target tasks without receiving any task-specific\ninteractive training. Our experiments on multiple photorealistic datasets and\nchallenging tasks show that our approach learns faster, generalizes better, and\noutperforms SoTA models by a significant margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Al_Halah_Z/0/1/0/all/0/1\">Ziad Al-Halah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramakrishnan_S/0/1/0/all/0/1\">Santhosh K. Ramakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grauman_K/0/1/0/all/0/1\">Kristen Grauman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Motion Sickness Modeling with Visual Vertical Estimation and Its Application to Autonomous Personal Mobility Vehicles. (arXiv:2202.06299v3 [cs.HC] UPDATED)","link":"http://arxiv.org/abs/2202.06299","description":"<p>Passengers (drivers) of level 3-5 autonomous personal mobility vehicles\n(APMV) and cars can perform non-driving tasks, such as reading books and\nsmartphones, while driving. It has been pointed out that such activities may\nincrease motion sickness. Many studies have been conducted to build\ncountermeasures, of which various computational motion sickness models have\nbeen developed. Many of these are based on subjective vertical conflict (SVC)\ntheory, which describes vertical changes in direction sensed by human sensory\norgans vs. those expected by the central nervous system. Such models are\nexpected to be applied to autonomous driving scenarios. However, no current\ncomputational model can integrate visual vertical information with vestibular\nsensations.\n</p>\n<p>We proposed a 6 DoF SVC-VV model which add a visually perceived vertical\nblock into a conventional six-degrees-of-freedom SVC model to predict VV\ndirections from image data simulating the visual input of a human. Hence, a\nsimple image-based VV estimation method is proposed.\n</p>\n<p>As the validation of the proposed model, this paper focuses on describing the\nfact that the motion sickness increases as a passenger reads a book while using\nan AMPV, assuming that visual vertical (VV) plays an important role. In the\nstatic experiment, it is demonstrated that the estimated VV by the proposed\nmethod accurately described the gravitational acceleration direction with a low\nmean absolute deviation. In addition, the results of the driving experiment\nusing an APMV demonstrated that the proposed 6 DoF SVC-VV model could describe\nthat the increased motion sickness experienced when the VV and gravitational\nacceleration directions were different.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hailong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inoue_S/0/1/0/all/0/1\">Shota Inoue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wada_T/0/1/0/all/0/1\">Takahiro Wada</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D Common Corruptions and Data Augmentation. (arXiv:2203.01441v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.01441","description":"<p>We introduce a set of image transformations that can be used as corruptions\nto evaluate the robustness of models as well as data augmentation mechanisms\nfor training neural networks. The primary distinction of the proposed\ntransformations is that, unlike existing approaches such as Common Corruptions,\nthe geometry of the scene is incorporated in the transformations -- thus\nleading to corruptions that are more likely to occur in the real world. We also\nintroduce a set of semantic corruptions (e.g. natural object occlusions). We\nshow these transformations are `efficient' (can be computed on-the-fly),\n`extendable' (can be applied on most image datasets), expose vulnerability of\nexisting models, and can effectively make models more robust when employed as\n`3D data augmentation' mechanisms. The evaluations on several tasks and\ndatasets suggest incorporating 3D information into benchmarking and training\nopens up a promising direction for robustness research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kar_O/0/1/0/all/0/1\">O&#x11f;uzhan Fatih Kar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeo_T/0/1/0/all/0/1\">Teresa Yeo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atanov_A/0/1/0/all/0/1\">Andrei Atanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zamir_A/0/1/0/all/0/1\">Amir Zamir</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stochastic Video Prediction with Structure and Motion. (arXiv:2203.10528v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.10528","description":"<p>While stochastic video prediction models enable future prediction under\nuncertainty, they mostly fail to model the complex dynamics of real-world\nscenes. For example, they cannot provide reliable predictions for scenes with a\nmoving camera and independently moving foreground objects in driving scenarios.\nThe existing methods fail to fully capture the dynamics of the structured world\nby only focusing on changes in pixels. In this paper, we assume that there is\nan underlying process creating observations in a video and propose to factorize\nit into static and dynamic components. We model the static part based on the\nscene structure and the ego-motion of the vehicle, and the dynamic part based\non the remaining motion of the dynamic objects. By learning separate\ndistributions of changes in foreground and background, we can decompose the\nscene into static and dynamic parts and separately model the change in each.\nOur experiments demonstrate that disentangling structure and motion helps\nstochastic video prediction, leading to better future predictions in complex\ndriving scenarios on two real-world driving datasets, KITTI and Cityscapes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Akan_A/0/1/0/all/0/1\">Adil Kaan Akan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Safadoust_S/0/1/0/all/0/1\">Sadra Safadoust</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guney_F/0/1/0/all/0/1\">Fatma G&#xfc;ney</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Road Layout Parsing with Graph Auto-Encoding. (arXiv:2203.11000v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.11000","description":"<p>Aiming for higher-level scene understanding, this work presents a neural\nnetwork approach that takes a road-layout map in bird's-eye-view as input, and\npredicts a human-interpretable graph that represents the road's topological\nlayout. Our approach elevates the understanding of road layouts from pixel\nlevel to the level of graphs. To achieve this goal, an image-graph-image\nauto-encoder is utilized. The network is designed to learn to regress the graph\nrepresentation at its auto-encoder bottleneck. This learning is self-supervised\nby an image reconstruction loss, without needing any external manual\nannotations. We create a synthetic dataset containing common road layout\npatterns and use it for training of the auto-encoder in addition to the\nreal-world Argoverse dataset. By using this additional synthetic dataset, which\nconceptually captures human knowledge of road layouts and makes this available\nto the network for training, we are able to stabilize and further improve the\nperformance of topological road layout understanding on the real-world\nArgoverse dataset. The evaluation shows that our approach exhibits comparable\nperformance to a strong fully-supervised baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Chenyang Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dubbelman_G/0/1/0/all/0/1\">Gijs Dubbelman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Exemplar-Free Continual Learning in Vision Transformers: an Account of Attention, Functional and Weight Regularization. (arXiv:2203.13167v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.13167","description":"<p>In this paper, we investigate the continual learning of Vision Transformers\n(ViT) for the challenging exemplar-free scenario, with special focus on how to\nefficiently distill the knowledge of its crucial self-attention mechanism\n(SAM). Our work takes an initial step towards a surgical investigation of SAM\nfor designing coherent continual learning methods in ViTs. We first carry out\nan evaluation of established continual learning regularization techniques. We\nthen examine the effect of regularization when applied to two key enablers of\nSAM: (a) the contextualized embedding layers, for their ability to capture\nwell-scaled representations with respect to the values, and (b) the prescaled\nattention maps, for carrying value-independent global contextual information.\nWe depict the perks of each distilling strategy on two image recognition\nbenchmarks (CIFAR100 and ImageNet-32) -- while (a) leads to a better overall\naccuracy, (b) helps enhance the rigidity by maintaining competitive\nperformances. Furthermore, we identify the limitation imposed by the symmetric\nnature of regularization losses. To alleviate this, we propose an asymmetric\nvariant and apply it to the pooled output distillation (POD) loss adapted for\nViTs. Our experiments confirm that introducing asymmetry to POD boosts its\nplasticity while retaining stability across (a) and (b). Moreover, we\nacknowledge low forgetting measures for all the compared methods, indicating\nthat ViTs might be naturally inclined continual learner\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pelosin_F/0/1/0/all/0/1\">Francesco Pelosin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1\">Saurav Jha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torsello_A/0/1/0/all/0/1\">Andrea Torsello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raducanu_B/0/1/0/all/0/1\">Bogdan Raducanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weijer_J/0/1/0/all/0/1\">Joost van de Weijer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"In-N-Out Generative Learning for Dense Unsupervised Video Segmentation. (arXiv:2203.15312v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.15312","description":"<p>In this paper, we focus on unsupervised learning for Video Object\nSegmentation (VOS) which learns visual correspondence (i.e., the similarity\nbetween pixel-level features) from unlabeled videos. Previous methods are\nmainly based on the contrastive learning paradigm, which optimize either in\nimage level or pixel level. Image-level optimization (e.g., the spatially\npooled feature of ResNet) learns robust high-level semantics but is sub-optimal\nsince the pixel-level features are optimized implicitly. By contrast,\npixel-level optimization is more explicit, however, it is sensitive to the\nvisual quality of training data and is not robust to object deformation. To\ncomplementarily perform these two levels of optimization in a unified\nframework, we propose the In-aNd-Out (INO) generative learning from a purely\ngenerative perspective with the help of naturally designed class tokens and\npatch tokens in Vision Transformer (ViT). Specifically, for image-level\noptimization, we force the out-view imagination from local to global views on\nclass tokens, which helps capture high-level semantics, and we name it as\nout-generative learning. As to pixel-level optimization, we perform in-view\nmasked image modeling on patch tokens, which recovers the corrupted parts of an\nimage via inferring its fine-grained structure, and we term it as in-generative\nlearning. To discover the temporal information better, we additionally force\nthe inter-frame consistency from both feature and affinity matrix levels.\nExtensive experiments on DAVIS-2017 val and YouTube-VOS 2018 val show that our\nINO outperforms previous state-of-the-art methods by significant margins.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1\">Xiao Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peike Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zongxin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Huiling Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hongxia Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jingren Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Monitoring social distancing with single image depth estimation. (arXiv:2204.01693v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.01693","description":"<p>The recent pandemic emergency raised many challenges regarding the\ncountermeasures aimed at containing the virus spread, and constraining the\nminimum distance between people resulted in one of the most effective\nstrategies. Thus, the implementation of autonomous systems capable of\nmonitoring the so-called social distance gained much interest. In this paper,\nwe aim to address this task leveraging a single RGB frame without additional\ndepth sensors. In contrast to existing single-image alternatives failing when\nground localization is not available, we rely on single image depth estimation\nto perceive the 3D structure of the observed scene and estimate the distance\nbetween people. During the setup phase, a straightforward calibration\nprocedure, leveraging a scale-aware SLAM algorithm available even on consumer\nsmartphones, allows us to address the scale ambiguity affecting single image\ndepth estimation. We validate our approach through indoor and outdoor images\nemploying a calibrated LiDAR + RGB camera asset. Experimental results highlight\nthat our proposal enables sufficiently reliable estimation of the\ninter-personal distance to monitor social distancing effectively. This fact\nconfirms that despite its intrinsic ambiguity, if appropriately driven single\nimage depth estimation can be a viable alternative to other depth perception\ntechniques, more expensive and not always feasible in practical applications.\nOur evaluation also highlights that our framework can run reasonably fast and\ncomparably to competitors, even on pure CPU systems. Moreover, its practical\ndeployment on low-power systems is around the corner.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mingozzi_A/0/1/0/all/0/1\">Alessio Mingozzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Conti_A/0/1/0/all/0/1\">Andrea Conti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aleotti_F/0/1/0/all/0/1\">Filippo Aleotti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poggi_M/0/1/0/all/0/1\">Matteo Poggi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mattoccia_S/0/1/0/all/0/1\">Stefano Mattoccia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End Audio Strikes Back: Boosting Augmentations Towards An Efficient Audio Classification Network. (arXiv:2204.11479v4 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2204.11479","description":"<p>While efficient architectures and a plethora of augmentations for end-to-end\nimage classification tasks have been suggested and heavily investigated,\nstate-of-the-art techniques for audio classifications still rely on numerous\nrepresentations of the audio signal together with large architectures,\nfine-tuned from large datasets. By utilizing the inherited lightweight nature\nof audio and novel audio augmentations, we were able to present an efficient\nend-to-end network with strong generalization ability. Experiments on a variety\nof sound classification sets demonstrate the effectiveness and robustness of\nour approach, by achieving state-of-the-art results in various settings. Public\ncode will be available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gazneli_A/0/1/0/all/0/1\">Avi Gazneli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zimerman_G/0/1/0/all/0/1\">Gadi Zimerman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ridnik_T/0/1/0/all/0/1\">Tal Ridnik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharir_G/0/1/0/all/0/1\">Gilad Sharir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noy_A/0/1/0/all/0/1\">Asaf Noy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deeper Insights into ViTs Robustness towards Common Corruptions. (arXiv:2204.12143v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.12143","description":"<p>Recent literature have shown design strategies from Convolutions Neural\nNetworks (CNNs) benefit Vision Transformers (ViTs) in various vision tasks.\nHowever, it remains unclear how these design choices impact on robustness when\ntransferred to ViTs. In this paper, we make the first attempt to investigate\nhow CNN-like architectural designs and CNN-based data augmentation strategies\nimpact on ViTs' robustness towards common corruptions through an extensive and\nrigorous benchmarking. We demonstrate that overlapping patch embedding and\nconvolutional Feed-Forward Network (FFN) boost performance on robustness.\nFurthermore, adversarial noise training is powerful on ViTs while\nfourier-domain augmentation fails. Moreover, we introduce a novel conditional\nmethod enabling input-varied augmentations from two angles: (1) Generating\ndynamic augmentation parameters conditioned on input images. It conduces to\nstate-of-the-art performance on robustness through conditional convolutions;\n(2) Selecting most suitable augmentation strategy by an extra predictor helps\nto achieve the best trade-off between clean accuracy and robustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_R/0/1/0/all/0/1\">Rui Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zuxuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Q/0/1/0/all/0/1\">Qi Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Han Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yu-Gang Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-05-01T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}