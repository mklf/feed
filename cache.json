{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-01-20T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Annotating the Tweebank Corpus on Named Entity Recognition and Building NLP Models for Social Media Analysis. (arXiv:2201.07281v1 [cs.CL])","link":"http://arxiv.org/abs/2201.07281","description":"<p>Social media data such as Twitter messages (\"tweets\") pose a particular\nchallenge to NLP systems because of their short, noisy, and colloquial nature.\nTasks such as Named Entity Recognition (NER) and syntactic parsing require\nhighly domain-matched training data for good performance. While there are some\npublicly available annotated datasets of tweets, they are all purpose-built for\nsolving one task at a time. As yet there is no complete training corpus for\nboth syntactic analysis (e.g., part of speech tagging, dependency parsing) and\nNER of tweets. In this study, we aim to create Tweebank-NER, an NER corpus\nbased on Tweebank V2 (TB2), and we use these datasets to train state-of-the-art\nNLP models. We first annotate named entities in TB2 using Amazon Mechanical\nTurk and measure the quality of our annotations. We train a Stanza NER model on\nthe new benchmark, achieving competitive performance against other\nnon-transformer NER systems. Finally, we train other Twitter NLP models (a\ntokenizer, lemmatizer, part of speech tagger, and dependency parser) on TB2\nbased on Stanza, and achieve state-of-the-art or competitive performance on\nthese tasks. We release the dataset and make the models available to use in an\n\"off-the-shelf\" manner for future Tweet NLP research. Our source code, data,\nand pre-trained models are available at:\n\\url{https://github.com/social-machines/TweebankNLP}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Hang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_Y/0/1/0/all/0/1\">Yining Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beeferman_D/0/1/0/all/0/1\">Doug Beeferman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_D/0/1/0/all/0/1\">Deb Roy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extending the Vocabulary of Fictional Languages using Neural Networks. (arXiv:2201.07288v1 [cs.CL])","link":"http://arxiv.org/abs/2201.07288","description":"<p>Fictional languages have become increasingly popular over the recent years\nappearing in novels, movies, TV shows, comics, and video games. While some of\nthese fictional languages have a complete vocabulary, most do not. We propose a\ndeep learning solution to the problem. Using style transfer and machine\ntranslation tools, we generate new words for a given target fictional language,\nwhile maintaining the style of its creator, hence extending this language\nvocabulary.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zacharias_T/0/1/0/all/0/1\">Thomas Zacharias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taklikar_A/0/1/0/all/0/1\">Ashutosh Taklikar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giryes_R/0/1/0/all/0/1\">Raja Giryes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Datasheet for the Pile. (arXiv:2201.07311v1 [cs.CL])","link":"http://arxiv.org/abs/2201.07311","description":"<p>This datasheet describes the Pile, a 825 GiB dataset of human-authored text\ncompiled by EleutherAI for use in large-scale language modeling. The Pile is\ncomprised of 22 different text sources, ranging from original scrapes done for\nthis project, to text data made available by the data owners, to third-party\nscrapes available online.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Biderman_S/0/1/0/all/0/1\">Stella Biderman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bicheno_K/0/1/0/all/0/1\">Kieran Bicheno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Leo Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Privacy-Preserving Unsupervised Domain Adaptation Framework for Clinical Text Analysis. (arXiv:2201.07317v1 [cs.CL])","link":"http://arxiv.org/abs/2201.07317","description":"<p>Unsupervised domain adaptation (UDA) generally aligns the unlabeled target\ndomain data to the distribution of the source domain to mitigate the\ndistribution shift problem. The standard UDA requires sharing the source data\nwith the target, having potential data privacy leaking risks. To protect the\nsource data's privacy, we first propose to share the source feature\ndistribution instead of the source data. However, sharing only the source\nfeature distribution may still suffer from the membership inference attack who\ncan infer an individual's membership by the black-box access to the source\nmodel. To resolve this privacy issue, we further study the under-explored\nproblem of privacy-preserving domain adaptation and propose a method with a\nnovel differential privacy training strategy to protect the source data\nprivacy. We model the source feature distribution by Gaussian Mixture Models\n(GMMs) under the differential privacy setting and send it to the target client\nfor adaptation. The target client resamples differentially private source\nfeatures from GMMs and adapts on target data with several state-of-art UDA\nbackbones. With our proposed method, the source data provider could avoid\nleaking source data privacy during domain adaptation as well as reserve the\nutility. To evaluate our proposed method's utility and privacy loss, we apply\nour model on a medical report disease label classification task using two noisy\nchallenging clinical text datasets. The results show that our proposed method\ncan preserve source data's privacy with a minor performance influence on the\ntext classification task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+An_Q/0/1/0/all/0/1\">Qiyuan An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruijiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_L/0/1/0/all/0/1\">Lin Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qingyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhiyong Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yingying Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning grammar with a divide-and-concur neural network. (arXiv:2201.07341v1 [cs.CL])","link":"http://arxiv.org/abs/2201.07341","description":"<p>We implement a divide-and-concur iterative projection approach to\ncontext-free grammar inference. Unlike most state-of-the-art models of natural\nlanguage processing, our method requires a relatively small number of discrete\nparameters, making the inferred grammar directly interpretable -- one can read\noff from a solution how to construct grammatically valid sentences. Another\nadvantage of our approach is the ability to infer meaningful grammatical rules\nfrom just a few sentences, compared to the hundreds of gigabytes of training\ndata many other models employ. We demonstrate several ways of applying our\napproach: classifying words and inferring a grammar from scratch, taking an\nexisting grammar and refining its categories and rules, and taking an existing\ngrammar and expanding its lexicon as it encounters new words in new data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deyo_S/0/1/0/all/0/1\">Sean Deyo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elser_V/0/1/0/all/0/1\">Veit Elser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Neural Machine Translation by Denoising Training. (arXiv:2201.07365v1 [cs.CL])","link":"http://arxiv.org/abs/2201.07365","description":"<p>We present a simple and effective pretraining strategy {D}en{o}ising\n{T}raining DoT for neural machine translation. Specifically, we update the\nmodel parameters with source- and target-side denoising tasks at the early\nstage and then tune the model normally. Notably, our approach does not increase\nany parameters or training steps, requiring the parallel data merely.\nExperiments show that DoT consistently improves the neural machine translation\nperformance across 12 bilingual and 16 multilingual directions (data size\nranges from 80K to 20M). In addition, we show that DoT can complement existing\ndata manipulation strategies, i.e. curriculum learning, knowledge distillation,\ndata diversification, bidirectional training, and back-translation.\nEncouragingly, we found that DoT outperforms costly pretrained model mBART in\nhigh-resource settings. Analyses show DoT is a novel in-domain cross-lingual\npretraining strategy and could offer further improvements with task-relevant\nself-supervisions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1\">Liang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_K/0/1/0/all/0/1\">Keqin Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Language Models are Effective Plagiarists. (arXiv:2201.07406v1 [cs.CL])","link":"http://arxiv.org/abs/2201.07406","description":"<p>As artificial intelligence (AI) technologies become increasingly powerful and\nprominent in society, their misuse is a growing concern. In educational\nsettings, AI technologies could be used by students to cheat on assignments and\nexams. In this paper we explore whether transformers can be used to solve\nintroductory level programming assignments while bypassing commonly used AI\ntools to detect plagiarism. We find that a student using GPT-J [Wang and\nKomatsuzaki, 2021] can complete introductory level programming assignments\nwithout triggering suspicion from MOSS [Aiken, 2000], a widely used plagiarism\ndetection tool. This holds despite the fact that GPT-J was not trained on the\nproblems in question and is not provided with any examples to work from. We\nfurther find that the code written by GPT-J is diverse in structure, lacking\nany particular tells that future plagiarism detection techniques may use to try\nto identify algorithmically generated code. We conclude with a discussion of\nthe ethical and educational implications of large language models and\ndirections for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Biderman_S/0/1/0/all/0/1\">Stella Biderman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raff_E/0/1/0/all/0/1\">Edward Raff</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Many Ways to be Lonely: Fine-grained Characterization of Loneliness and its Potential Changes in COVID-19. (arXiv:2201.07423v1 [cs.CL])","link":"http://arxiv.org/abs/2201.07423","description":"<p>Loneliness has been associated with negative outcomes for physical and mental\nhealth. Understanding how people express and cope with various forms of\nloneliness is critical for early screening and targeted interventions to reduce\nloneliness, particularly among vulnerable groups such as young adults. To\nexamine how different forms of loneliness and coping strategies manifest in\nloneliness self-disclosure, we built a dataset, FIG-Loneliness (FIne-Grained\nLoneliness) by using Reddit posts in two young adult-focused forums and two\nloneliness related forums consisting of a diverse age group. We provide\nannotations by trained human annotators for binary and fine-grained loneliness\nclassifications of the posts. Trained on FIG-Loneliness, two BERT-based models\nwere used to understand loneliness forms and authors' coping strategies in\nthese forums. Our binary loneliness classification archived an accuracy above\n97%, and fine-grained loneliness category classification reached an average\naccuracy of 77% across all labeled categories. With FIG-Loneliness and model\npredictions, we found that loneliness expressions in the young adult related\nforums are distinct from other forums. Those in young adult-focused forums are\nmore likely to express concerns pertaining to peer relationship, and are\npotentially more sensitive to geographical isolation impacted by the COVID-19\npandemic lockdown. Also, we show that different forms of loneliness have\ndifferential use in coping strategies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yueyi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yunfan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Leqi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winkielman_P/0/1/0/all/0/1\">Piotr Winkielman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interpreting Arabic Transformer Models. (arXiv:2201.07434v1 [cs.CL])","link":"http://arxiv.org/abs/2201.07434","description":"<p>Arabic is a Semitic language which is widely spoken with many dialects. Given\nthe success of pre-trained language models, many transformer models trained on\nArabic and its dialects have surfaced. While these models have been compared\nwith respect to downstream NLP tasks, no evaluation has been carried out to\ndirectly compare the internal representations. We probe how linguistic\ninformation is encoded in Arabic pretrained models, trained on different\nvarieties of Arabic language. We perform a layer and neuron analysis on the\nmodels using three intrinsic tasks: two morphological tagging tasks based on\nMSA (modern standard Arabic) and dialectal POS-tagging and a dialectal\nidentification task. Our analysis enlightens interesting findings such as: i)\nword morphology is learned at the lower and middle layers ii) dialectal\nidentification necessitate more knowledge and hence preserved even in the final\nlayers, iii) despite a large overlap in their vocabulary, the MSA-based models\nfail to capture the nuances of Arabic dialects, iv) we found that neurons in\nembedding layers are polysemous in nature, while the neurons in middle layers\nare exclusive to specific properties.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abdelali_A/0/1/0/all/0/1\">Ahmed Abdelali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrani_N/0/1/0/all/0/1\">Nadir Durrani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dalvi_F/0/1/0/all/0/1\">Fahim Dalvi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sajjad_H/0/1/0/all/0/1\">Hassan Sajjad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TourBERT: A pretrained language model for the tourism industry. (arXiv:2201.07449v1 [cs.CL])","link":"http://arxiv.org/abs/2201.07449","description":"<p>The Bidirectional Encoder Representations from Transformers (BERT) is\ncurrently one of the most important and state-of-the-art models for natural\nlanguage. However, it has also been shown that for domain-specific tasks it is\nhelpful to pretrain BERT on a domain-specific corpus. In this paper, we present\nTourBERT, a pretrained language model for tourism. We describe how TourBERT was\ndeveloped and evaluated. The evaluations show that TourBERT is outperforming\nBERT in all tourism-specific tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arefieva_V/0/1/0/all/0/1\">Veronika Arefieva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Egger_R/0/1/0/all/0/1\">Roman Egger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Development of Fake News Model using Machine Learning through Natural Language Processing. (arXiv:2201.07489v1 [cs.CL])","link":"http://arxiv.org/abs/2201.07489","description":"<p>Fake news detection research is still in the early stage as this is a\nrelatively new phenomenon in the interest raised by society. Machine learning\nhelps to solve complex problems and to build AI systems nowadays and especially\nin those cases where we have tacit knowledge or the knowledge that is not\nknown. We used machine learning algorithms and for identification of fake news;\nwe applied three classifiers; Passive Aggressive, Na\\\"ive Bayes, and Support\nVector Machine. Simple classification is not completely correct in fake news\ndetection because classification methods are not specialized for fake news.\nWith the integration of machine learning and text-based processing, we can\ndetect fake news and build classifiers that can classify the news data. Text\nclassification mainly focuses on extracting various features of text and after\nthat incorporating those features into classification. The big challenge in\nthis area is the lack of an efficient way to differentiate between fake and\nnon-fake due to the unavailability of corpora. We applied three different\nmachine learning classifiers on two publicly available datasets. Experimental\nanalysis based on the existing dataset indicates a very encouraging and\nimproved performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_S/0/1/0/all/0/1\">Sajjad Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hinkelmann_K/0/1/0/all/0/1\">Knut Hinkelmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corradini_F/0/1/0/all/0/1\">Flavio Corradini</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CM3: A Causal Masked Multimodal Model of the Internet. (arXiv:2201.07520v1 [cs.CL])","link":"http://arxiv.org/abs/2201.07520","description":"<p>We introduce CM3, a family of causally masked generative models trained over\na large corpus of structured multi-modal documents that can contain both text\nand image tokens. Our new causally masked approach generates tokens left to\nright while also masking out a small number of long token spans that are\ngenerated at the end of the string, instead of their original positions. The\ncasual masking object provides a type of hybrid of the more common causal and\nmasked language models, by enabling full generative modeling while also\nproviding bidirectional context when generating the masked spans. We train\ncausally masked language-image models on large-scale web and Wikipedia\narticles, where each document contains all of the text, hypertext markup,\nhyperlinks, and image tokens (from a VQVAE-GAN), provided in the order they\nappear in the original HTML source (before masking). The resulting CM3 models\ncan generate rich structured, multi-modal outputs while conditioning on\narbitrary masked document contexts, and thereby implicitly learn a wide range\nof text, image, and cross modal tasks. They can be prompted to recover, in a\nzero-shot fashion, the functionality of models such as DALL-E, GENRE, and HTLM.\nWe set the new state-of-the-art in zero-shot summarization, entity linking, and\nentity disambiguation while maintaining competitive performance in the\nfine-tuning setting. We can generate images unconditionally, conditioned on\ntext (like DALL-E) and do captioning all in a zero-shot setting with a single\nmodel.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aghajanyan_A/0/1/0/all/0/1\">Armen Aghajanyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1\">Bernie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ross_C/0/1/0/all/0/1\">Candace Ross</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karpukhin_V/0/1/0/all/0/1\">Vladimir Karpukhin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_N/0/1/0/all/0/1\">Naman Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okhonko_D/0/1/0/all/0/1\">Dmytro Okhonko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_M/0/1/0/all/0/1\">Mandar Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_G/0/1/0/all/0/1\">Gargi Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1\">Mike Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Writing about COVID-19 vaccines: Emotional profiling unravels how mainstream and alternative press framed AstraZeneca, Pfizer and vaccination campaigns. (arXiv:2201.07538v1 [cs.CY])","link":"http://arxiv.org/abs/2201.07538","description":"<p>Since their announcement in November 2020, COVID-19 vaccines were largely\ndebated by the press and social media. With most studies focusing on COVID-19\ndisinformation in social media, little attention has been paid to how\nmainstream news outlets framed COVID-19 narratives compared to alternative\nsources. To fill this gap, we use cognitive network science and natural\nlanguage processing to reconstruct time-evolving semantic and emotional frames\nof 5745 Italian news, that were massively re-shared on Facebook and Twitter,\nabout COVID-19 vaccines. We found consistently high levels of\ntrust/anticipation and less disgust in the way mainstream sources framed the\ngeneral idea of \"vaccine/vaccino\". These emotions were crucially missing in the\nways alternative sources framed COVID-19 vaccines. More differences were found\nwithin specific instances of vaccines. Alternative news included titles framing\nthe AstraZeneca vaccine with strong levels of sadness, absent in mainstream\ntitles. Mainstream news initially framed \"Pfizer\" along more negative\nassociations with side effects than \"AstraZeneca\". With the temporary\nsuspension of the latter, on March 15th 2021, we identified a\nsemantic/emotional shift: Even mainstream article titles framed \"AstraZeneca\"\nas semantically richer in negative associations with side effects, while\n\"Pfizer\" underwent a positive shift in valence, mostly related to its higher\nefficacy. \"Thrombosis\" entered the frame of vaccines together with fearful\nconceptual associations, while \"death\" underwent an emotional shift, steering\ntowards fear in alternative titles and losing its hopeful connotation in\nmainstream titles. Our findings expose crucial aspects of the emotional\nnarratives around COVID-19 vaccines adopted by the press, highlighting the need\nto understand how alternative and mainstream media report vaccination news.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Semeraro_A/0/1/0/all/0/1\">Alfonso Semeraro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vilella_S/0/1/0/all/0/1\">Salvatore Vilella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruffo_G/0/1/0/all/0/1\">Giancarlo Ruffo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stella_M/0/1/0/all/0/1\">Massimo Stella</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-Supervised Clustering with Contrastive Learning for Discovering New Intents. (arXiv:2201.07604v1 [cs.LG])","link":"http://arxiv.org/abs/2201.07604","description":"<p>Most dialogue systems in real world rely on predefined intents and answers\nfor QA service, so discovering potential intents from large corpus previously\nis really important for building such dialogue services. Considering that most\nscenarios have few intents known already and most intents waiting to be\ndiscovered, we focus on semi-supervised text clustering and try to make the\nproposed method benefit from labeled samples for better overall clustering\nperformance. In this paper, we propose Deep Contrastive Semi-supervised\nClustering (DCSC), which aims to cluster text samples in a semi-supervised way\nand provide grouped intents to operation staff. To make DCSC fully utilize the\nlimited known intents, we propose a two-stage training procedure for DCSC, in\nwhich DCSC will be trained on both labeled samples and unlabeled samples, and\nachieve better text representation and clustering performance. We conduct\nexperiments on two public datasets to compare our model with several popular\nmethods, and the results show DCSC achieve best performance across all datasets\nand circumstances, indicating the effect of the improvements in our work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Feng Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhenbo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_Z/0/1/0/all/0/1\">Zhenghong Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fengxin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_H/0/1/0/all/0/1\">Hua Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1\">Bing Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Sheng Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncovering More Shallow Heuristics: Probing the Natural Language Inference Capacities of Transformer-Based Pre-Trained Language Models Using Syllogistic Patterns. (arXiv:2201.07614v1 [cs.CL])","link":"http://arxiv.org/abs/2201.07614","description":"<p>In this article, we explore the shallow heuristics used by transformer-based\npre-trained language models (PLMs) that are fine-tuned for natural language\ninference (NLI). To do so, we construct or own dataset based on syllogistic,\nand we evaluate a number of models' performance on our dataset. We find\nevidence that the models rely heavily on certain shallow heuristics, picking up\non symmetries and asymmetries between premise and hypothesis. We suggest that\nthe lack of generalization observable in our study, which is becoming a topic\nof lively debate in the field, means that the PLMs are currently not learning\nNLI, but rather spurious heuristics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gubelmann_R/0/1/0/all/0/1\">Reto Gubelmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Handschuh_S/0/1/0/all/0/1\">Siegfried Handschuh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Top-Down Influence? Predicting CEO Personality and Risk Impact from Speech Transcripts. (arXiv:2201.07670v1 [cs.CL])","link":"http://arxiv.org/abs/2201.07670","description":"<p>How much does a CEO's personality impact the performance of their company?\nManagement theory posits a great influence, but it is difficult to show\nempirically -- there is a lack of publicly available self-reported personality\ndata of top managers. Instead, we propose a text-based personality regressor\nusing crowd-sourced Myers--Briggs Type Indicator (MBTI) assessments. The\nratings have a high internal and external validity and can be predicted with\nmoderate to strong correlations for three out of four dimensions. Providing\nevidence for the upper echelons theory, we demonstrate that the predicted CEO\npersonalities have explanatory power of financial risk.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Theil_K/0/1/0/all/0/1\">Kilian Theil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hovy_D/0/1/0/all/0/1\">Dirk Hovy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stuckenschmidt_H/0/1/0/all/0/1\">Heiner Stuckenschmidt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data-to-Value: An Evaluation-First Methodology for Natural Language Projects. (arXiv:2201.07725v1 [cs.CL])","link":"http://arxiv.org/abs/2201.07725","description":"<p>Big data, i.e. collecting, storing and processing of data at scale, has\nrecently been possible due to the arrival of clusters of commodity computers\npowered by application-level distributed parallel operating systems like\nHDFS/Hadoop/Spark, and such infrastructures have revolutionized data mining at\nscale. For data mining project to succeed more consistently, some methodologies\nwere developed (e.g. CRISP-DM, SEMMA, KDD), but these do not account for (1)\nvery large scales of processing, (2) dealing with textual (unstructured) data\n(i.e. Natural Language Processing (NLP, \"text analytics\"), and (3)\nnon-technical considerations (e.g. legal, ethical, project managerial aspects).\n</p>\n<p>To address these shortcomings, a new methodology, called \"Data to Value\"\n(D2V), is introduced, which is guided by a detailed catalog of questions in\norder to avoid a disconnect of big data text analytics project team with the\ntopic when facing rather abstract box-and-arrow diagrams commonly associated\nwith methodologies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Leidner_J/0/1/0/all/0/1\">Jochen L. Leidner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Biomedical Information Retrieval with Neural Retrievers. (arXiv:2201.07745v1 [cs.IR])","link":"http://arxiv.org/abs/2201.07745","description":"<p>Information retrieval (IR) is essential in search engines and dialogue\nsystems as well as natural language processing tasks such as open-domain\nquestion answering. IR serve an important function in the biomedical domain,\nwhere content and sources of scientific knowledge may evolve rapidly. Although\nneural retrievers have surpassed traditional IR approaches such as TF-IDF and\nBM25 in standard open-domain question answering tasks, they are still found\nlacking in the biomedical domain. In this paper, we seek to improve information\nretrieval (IR) using neural retrievers (NR) in the biomedical domain, and\nachieve this goal using a three-pronged approach. First, to tackle the relative\nlack of data in the biomedical domain, we propose a template-based question\ngeneration method that can be leveraged to train neural retriever models.\nSecond, we develop two novel pre-training tasks that are closely aligned to the\ndownstream task of information retrieval. Third, we introduce the ``Poly-DPR''\nmodel which encodes each context into multiple context vectors. Extensive\nexperiments and analysis on the BioASQ challenge suggest that our proposed\nmethod leads to large gains over existing neural approaches and beats BM25 in\nthe small-corpus setting. We show that BM25 and our method can complement each\nother, and a simple hybrid model leads to further gains in the large corpus\nsetting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_M/0/1/0/all/0/1\">Man Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitra_A/0/1/0/all/0/1\">Arindam Mitra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gokhale_T/0/1/0/all/0/1\">Tejas Gokhale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attacking Neural Text Detectors. (arXiv:2002.11768v4 [cs.CR] UPDATED)","link":"http://arxiv.org/abs/2002.11768","description":"<p>Machine learning based language models have recently made significant\nprogress, which introduces a danger to spread misinformation. To combat this\npotential danger, several methods have been proposed for detecting text written\nby these language models. This paper presents two classes of black-box attacks\non these detectors, one which randomly replaces characters with homoglyphs, and\nthe other a simple scheme to purposefully misspell words. The homoglyph and\nmisspelling attacks decrease a popular neural text detector's recall on neural\ntext from 97.44% to 0.26% and 22.68%, respectively. Results also indicate that\nthe attacks are transferable to other neural text detectors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wolff_M/0/1/0/all/0/1\">Max Wolff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolff_S/0/1/0/all/0/1\">Stuart Wolff</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PADA: Example-based Prompt Learning for on-the-fly Adaptation to Unseen Domains. (arXiv:2102.12206v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2102.12206","description":"<p>Natural Language Processing algorithms have made incredible progress, but\nthey still struggle when applied to out-of-distribution examples. We address a\nchallenging and underexplored version of this domain adaptation problem, where\nan algorithm is trained on several source domains, and then applied to examples\nfrom unseen domains that are unknown at training time. Particularly, no\nexamples, labeled or unlabeled, or any other knowledge about the target domain\nare available to the algorithm at training time. We present PADA: An\nexample-based autoregressive Prompt learning algorithm for on-the-fly\nAny-Domain Adaptation, based on the T5 language model. Given a test example,\nPADA first generates a unique prompt for it and then, conditioned on this\nprompt, labels the example with respect to the NLP prediction task. PADA is\ntrained to generate a prompt which is a token sequence of unrestricted length,\nconsisting of Domain Related Features (DRFs) that characterize each of the\nsource domains. Intuitively, the generated prompt is a unique signature that\nmaps the test example to a semantic space spanned by the source domains. In\nexperiments with 3 tasks (text classification and sequence tagging), for a\ntotal of 14 multi-source adaptation scenarios, PADA substantially outperforms\nstrong baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ben_David_E/0/1/0/all/0/1\">Eyal Ben-David</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oved_N/0/1/0/all/0/1\">Nadav Oved</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reichart_R/0/1/0/all/0/1\">Roi Reichart</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Search Engine for Discovery of Scientific Challenges and Directions. (arXiv:2108.13751v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.13751","description":"<p>Keeping track of scientific challenges, advances and emerging directions is a\nfundamental part of research. However, researchers face a flood of papers that\nhinders discovery of important knowledge. In biomedicine, this directly impacts\nhuman lives. To address this problem, we present a novel task of extraction and\nsearch of scientific challenges and directions, to facilitate rapid knowledge\ndiscovery. We construct and release an expert-annotated corpus of texts sampled\nfrom full-length papers, labeled with novel semantic categories that generalize\nacross many types of challenges and directions. We focus on a large corpus of\ninterdisciplinary work relating to the COVID-19 pandemic, ranging from\nbiomedicine to areas such as AI and economics. We apply a model trained on our\ndata to identify challenges and directions across the corpus and build a\ndedicated search engine. In experiments with 19 researchers and clinicians\nusing our system, we outperform a popular scientific search engine in assisting\nknowledge discovery. Finally, we show that models trained on our resource\ngeneralize to the wider biomedical domain and to AI papers, highlighting its\nbroad utility. We make our data, model and search engine publicly available.\nhttps://challenges.apps.allenai.org/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lahav_D/0/1/0/all/0/1\">Dan Lahav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Falcon_J/0/1/0/all/0/1\">Jon Saad Falcon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuehl_B/0/1/0/all/0/1\">Bailey Kuehl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_S/0/1/0/all/0/1\">Sophie Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parasa_S/0/1/0/all/0/1\">Sravanthi Parasa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shomron_N/0/1/0/all/0/1\">Noam Shomron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chau_D/0/1/0/all/0/1\">Duen Horng Chau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Diyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horvitz_E/0/1/0/all/0/1\">Eric Horvitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weld_D/0/1/0/all/0/1\">Daniel S. Weld</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hope_T/0/1/0/all/0/1\">Tom Hope</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Multilingual Translation by Representation and Gradient Regularization. (arXiv:2109.04778v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.04778","description":"<p>Multilingual Neural Machine Translation (NMT) enables one model to serve all\ntranslation directions, including ones that are unseen during training, i.e.\nzero-shot translation. Despite being theoretically attractive, current models\noften produce low quality translations -- commonly failing to even produce\noutputs in the right target language. In this work, we observe that off-target\ntranslation is dominant even in strong multilingual systems, trained on massive\nmultilingual corpora. To address this issue, we propose a joint approach to\nregularize NMT models at both representation-level and gradient-level. At the\nrepresentation level, we leverage an auxiliary target language prediction task\nto regularize decoder outputs to retain information about the target language.\nAt the gradient level, we leverage a small amount of direct data (in thousands\nof sentence pairs) to regularize model gradients. Our results demonstrate that\nour approach is highly effective in both reducing off-target translation\noccurrences and improving zero-shot translation performance by +5.59 and +10.38\nBLEU on WMT and OPUS datasets respectively. Moreover, experiments show that our\nmethod also works well when the small amount of direct data is not available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yilin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eriguchi_A/0/1/0/all/0/1\">Akiko Eriguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muzio_A/0/1/0/all/0/1\">Alexandre Muzio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tadepalli_P/0/1/0/all/0/1\">Prasad Tadepalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Stefan Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassan_H/0/1/0/all/0/1\">Hany Hassan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RAFT: A Real-World Few-Shot Text Classification Benchmark. (arXiv:2109.14076v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.14076","description":"<p>Large pre-trained language models have shown promise for few-shot learning,\ncompleting text-based tasks given only a few task-specific examples. Will\nmodels soon solve classification tasks that have so far been reserved for human\nresearch assistants? Existing benchmarks are not designed to measure progress\nin applied settings, and so don't directly answer this question. The RAFT\nbenchmark (Real-world Annotated Few-shot Tasks) focuses on naturally occurring\ntasks and uses an evaluation setup that mirrors deployment. Baseline\nevaluations on RAFT reveal areas current techniques struggle with: reasoning\nover long texts and tasks with many classes. Human baselines show that some\nclassification tasks are difficult for non-expert humans, reflecting that\nreal-world value sometimes depends on domain expertise. Yet even non-expert\nhuman baseline F1 scores exceed GPT-3 by an average of 0.11. The RAFT datasets\nand leaderboard will track which model improvements translate into real-world\nbenefits at https://raft.elicit.org .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alex_N/0/1/0/all/0/1\">Neel Alex</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lifland_E/0/1/0/all/0/1\">Eli Lifland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tunstall_L/0/1/0/all/0/1\">Lewis Tunstall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thakur_A/0/1/0/all/0/1\">Abhishek Thakur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maham_P/0/1/0/all/0/1\">Pegah Maham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedel_C/0/1/0/all/0/1\">C. Jess Riedel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hine_E/0/1/0/all/0/1\">Emmie Hine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ashurst_C/0/1/0/all/0/1\">Carolyn Ashurst</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sedille_P/0/1/0/all/0/1\">Paul Sedille</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carlier_A/0/1/0/all/0/1\">Alexis Carlier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noetel_M/0/1/0/all/0/1\">Michael Noetel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stuhlmuller_A/0/1/0/all/0/1\">Andreas Stuhlm&#xfc;ller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SNCSE: Contrastive Learning for Unsupervised Sentence Embedding with Soft Negative Samples. (arXiv:2201.05979v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.05979","description":"<p>Unsupervised sentence embedding aims to obtain the most appropriate embedding\nfor a sentence to reflect its semantic. Contrastive learning has been\nattracting developing attention. For a sentence, current models utilize diverse\ndata augmentation methods to generate positive samples, while consider other\nindependent sentences as negative samples. Then they adopt InfoNCE loss to pull\nthe embeddings of positive pairs gathered, and push those of negative pairs\nscattered. Although these models have made great progress on sentence\nembedding, we argue that they may suffer from feature suppression. The models\nfail to distinguish and decouple textual similarity and semantic similarity.\nAnd they may overestimate the semantic similarity of any pairs with similar\ntextual regardless of the actual semantic difference between them. This is\nbecause positive pairs in unsupervised contrastive learning come with similar\nand even the same textual through data augmentation. To alleviate feature\nsuppression, we propose contrastive learning for unsupervised sentence\nembedding with soft negative samples (SNCSE). Soft negative samples share\nhighly similar textual but have surely and apparently different semantic with\nthe original samples. Specifically, we take the negation of original sentences\nas soft negative samples, and propose Bidirectional Margin Loss (BML) to\nintroduce them into traditional contrastive learning framework, which merely\ninvolves positive and negative samples. Our experimental results show that\nSNCSE can obtain state-of-the-art performance on semantic textual similarity\n(STS) task with average Spearman's correlation coefficient of 78.97% on\nBERTbase and 79.23% on RoBERTabase. Besides, we adopt rank-based error analysis\nmethod to detect the weakness of SNCSE for future study.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yangguang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_Y/0/1/0/all/0/1\">Yong Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1\">Lingpeng Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1\">Jing Shao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unintended Bias in Language Model-driven Conversational Recommendation. (arXiv:2201.06224v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2201.06224","description":"<p>Conversational Recommendation Systems (CRSs) have recently started to\nleverage pretrained language models (LM) such as BERT for their ability to\nsemantically interpret a wide range of preference statement variations.\nHowever, pretrained LMs are well-known to be prone to intrinsic biases in their\ntraining data, which may be exacerbated by biases embedded in domain-specific\nlanguage data(e.g., user reviews) used to fine-tune LMs for CRSs. We study a\nrecently introduced LM-driven recommendation backbone (termed LMRec) of a CRS\nto investigate how unintended bias i.e., language variations such as name\nreferences or indirect indicators of sexual orientation or location that should\nnot affect recommendations manifests in significantly shifted price and\ncategory distributions of restaurant recommendations. The alarming results we\nobserve strongly indicate that LMRec has learned to reinforce harmful\nstereotypes through its recommendations. For example, offhand mention of names\nassociated with the black community significantly lowers the price distribution\nof recommended restaurants, while offhand mentions of common male-associated\nnames lead to an increase in recommended alcohol-serving establishments. These\nand many related results presented in this work raise a red flag that advances\nin the language handling capability of LM-drivenCRSs do not come without\nsignificant challenges related to mitigating unintended bias in future deployed\nCRS assistants with a potential reach of hundreds of millions of end-users.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_T/0/1/0/all/0/1\">Tianshu Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiaru Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouadjenek_M/0/1/0/all/0/1\">Mohamed Reda Bouadjenek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mai_Z/0/1/0/all/0/1\">Zheda Mai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanner_S/0/1/0/all/0/1\">Scott Sanner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COPA-SSE: Semi-structured Explanations for Commonsense Reasoning. (arXiv:2201.06777v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.06777","description":"<p>We present Semi-Structured Explanations for COPA (COPA-SSE), a new\ncrowdsourced dataset of 9,747 semi-structured, English common sense\nexplanations for COPA questions. The explanations are formatted as a set of\ntriple-like common sense statements with ConceptNet relations but freely\nwritten concepts. This semi-structured format strikes a balance between the\nhigh quality but low coverage of structured data and the lower quality but high\ncoverage of free-form crowdsourcing. Each explanation also includes a set of\nhuman-given quality ratings. With their familiar format, the explanations are\ngeared towards commonsense reasoners operating on knowledge graphs and serve as\na starting point for ongoing work on improving such systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Brassard_A/0/1/0/all/0/1\">Ana Brassard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heinzerling_B/0/1/0/all/0/1\">Benjamin Heinzerling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kavumba_P/0/1/0/all/0/1\">Pride Kavumba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inui_K/0/1/0/all/0/1\">Kentaro Inui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-01-19T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Is Contrastive Learning Suitable for Left Ventricular Segmentation in Echocardiographic Images?. (arXiv:2201.07219v1 [eess.IV])","link":"http://arxiv.org/abs/2201.07219","description":"<p>Contrastive learning has proven useful in many applications where access to\nlabelled data is limited. The lack of annotated data is particularly\nproblematic in medical image segmentation as it is difficult to have clinical\nexperts manually annotate large volumes of data. One such task is the\nsegmentation of cardiac structures in ultrasound images of the heart. In this\npaper, we argue whether or not contrastive pretraining is helpful for the\nsegmentation of the left ventricle in echocardiography images. Furthermore, we\nstudy the effect of this on two segmentation networks, DeepLabV3, as well as\nthe commonly used segmentation network, UNet. Our results show that contrastive\npretraining helps improve the performance on left ventricle segmentation,\nparticularly when annotated data is scarce. We show how to achieve comparable\nresults to state-of-the-art fully supervised algorithms when we train our\nmodels in a self-supervised fashion followed by fine-tuning on just 5% of the\ndata. We also show that our solution achieves better results than what is\ncurrently published on a large public dataset (EchoNet-Dynamic) and we compare\nthe performance of our solution on another smaller dataset (CAMUS) as well.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Saeed_M/0/1/0/all/0/1\">Mohamed Saeed</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Muhtaseb_R/0/1/0/all/0/1\">Rand Muhtaseb</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yaqub_M/0/1/0/all/0/1\">Mohammad Yaqub</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explainable Ensemble Machine Learning for Breast Cancer Diagnosis based on Ultrasound Image Texture Features. (arXiv:2201.07227v1 [eess.IV])","link":"http://arxiv.org/abs/2201.07227","description":"<p>Image classification is widely used to build predictive models for breast\ncancer diagnosis. Most existing approaches overwhelmingly rely on deep\nconvolutional networks to build such diagnosis pipelines. These model\narchitectures, although remarkable in performance, are black-box systems that\nprovide minimal insight into the inner logic behind their predictions. This is\na major drawback as the explainability of prediction is vital for applications\nsuch as cancer diagnosis. In this paper, we address this issue by proposing an\nexplainable machine learning pipeline for breast cancer diagnosis based on\nultrasound images. We extract first- and second-order texture features of the\nultrasound images and use them to build a probabilistic ensemble of decision\ntree classifiers. Each decision tree learns to classify the input ultrasound\nimage by learning a set of robust decision thresholds for texture features of\nthe image. The decision path of the model predictions can then be interpreted\nby decomposing the learned decision trees. Our results show that our proposed\nframework achieves high predictive performance while being explainable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Rezazadeh_A/0/1/0/all/0/1\">Alireza Rezazadeh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jafarian_Y/0/1/0/all/0/1\">Yasamin Jafarian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kord_A/0/1/0/all/0/1\">Ali Kord</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AI-based Carcinoma Detection and Classification Using Histopathological Images: A Systematic Review. (arXiv:2201.07231v1 [eess.IV])","link":"http://arxiv.org/abs/2201.07231","description":"<p>Histopathological image analysis is the gold standard to diagnose cancer.\nCarcinoma is a subtype of cancer that constitutes more than 80% of all cancer\ncases. Squamous cell carcinoma and adenocarcinoma are two major subtypes of\ncarcinoma, diagnosed by microscopic study of biopsy slides. However, manual\nmicroscopic evaluation is a subjective and time-consuming process. Many\nresearchers have reported methods to automate carcinoma detection and\nclassification. The increasing use of artificial intelligence (AI) in the\nautomation of carcinoma diagnosis also reveals a significant rise in the use of\ndeep network models. In this systematic literature review, we present a\ncomprehensive review of the state-of-the-art approaches reported in carcinoma\ndiagnosis using histopathological images. Studies are selected from well-known\ndatabases with strict inclusion/exclusion criteria. We have categorized the\narticles and recapitulated their methods based on specific organs of carcinoma\norigin. Further, we have summarized pertinent literature on AI methods,\nhighlighted critical challenges and limitations, and provided insights on\nfuture research direction in automated carcinoma diagnosis. Out of 101 articles\nselected, most of the studies experimented on private datasets with varied\nimage sizes, obtaining accuracy between 63% and 100%. Overall, this review\nhighlights the need for a generalized AI-based carcinoma diagnostic system.\nAdditionally, it is desirable to have accountable approaches to extract\nmicroscopic features from images of multiple magnifications that should mimic\npathologists' evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Prabhua_S/0/1/0/all/0/1\">Swathi Prabhua</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Prasada_K/0/1/0/all/0/1\">Keerthana Prasada</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Robels_Kelly_A/0/1/0/all/0/1\">Antonio Robels-Kelly</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_X/0/1/0/all/0/1\">Xuequan Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Kervolutional Neural Networks. (arXiv:2201.07264v1 [cs.CV])","link":"http://arxiv.org/abs/2201.07264","description":"<p>A paper published in the CVPR 2019 conference outlines a new technique called\n'kervolution' used in a new type of augmented convolutional neural network\n(CNN) called a 'kervolutional neural network' (KNN). The paper asserts that\nKNNs achieve faster convergence and higher accuracies than CNNs. This \"mini\npaper\" will further examine the findings in the original paper and perform a\nmore in depth analysis of the KNN architecture. This will be done by analyzing\nthe impact of hyper parameters (specifically the learning rate) on KNNs versus\nCNNs, experimenting with other types of kervolution operations not tested in\nthe original paper, a more rigourous statistical analysis of accuracies and\nconvergence times and additional theoretical analysis. The accompanying code is\npublicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Perez_N/0/1/0/all/0/1\">Nicolas Perez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OSSID: Online Self-Supervised Instance Detection by (and for) Pose Estimation. (arXiv:2201.07309v1 [cs.CV])","link":"http://arxiv.org/abs/2201.07309","description":"<p>Real-time object pose estimation is necessary for many robot manipulation\nalgorithms. However, state-of-the-art methods for object pose estimation are\ntrained for a specific set of objects; these methods thus need to be retrained\nto estimate the pose of each new object, often requiring tens of GPU-days of\ntraining for optimal performance. \\revisef{In this paper, we propose the OSSID\nframework,} leveraging a slow zero-shot pose estimator to self-supervise the\ntraining of a fast detection algorithm. This fast detector can then be used to\nfilter the input to the pose estimator, drastically improving its inference\nspeed. We show that this self-supervised training exceeds the performance of\nexisting zero-shot detection methods on two widely used object pose estimation\nand detection datasets, without requiring any human annotations. Further, we\nshow that the resulting method for pose estimation has a significantly faster\ninference speed, due to the ability to filter out large parts of the image.\nThus, our method for self-supervised online learning of a detector (trained\nusing pseudo-labels from a slow pose estimator) leads to accurate pose\nestimation at real-time speeds, without requiring human annotations.\nSupplementary materials and code can be found at\nhttps://georgegu1997.github.io/OSSID/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1\">Qiao Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okorn_B/0/1/0/all/0/1\">Brian Okorn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Held_D/0/1/0/all/0/1\">David Held</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lung Swapping Autoencoder: Learning a Disentangled Structure-texture Representation of Chest Radiographs. (arXiv:2201.07344v1 [eess.IV])","link":"http://arxiv.org/abs/2201.07344","description":"<p>Well-labeled datasets of chest radiographs (CXRs) are difficult to acquire\ndue to the high cost of annotation. Thus, it is desirable to learn a robust and\ntransferable representation in an unsupervised manner to benefit tasks that\nlack labeled data. Unlike natural images, medical images have their own domain\nprior; e.g., we observe that many pulmonary diseases, such as the COVID-19,\nmanifest as changes in the lung tissue texture rather than the anatomical\nstructure. Therefore, we hypothesize that studying only the texture without the\ninfluence of structure variations would be advantageous for downstream\nprognostic and predictive modeling tasks. In this paper, we propose a\ngenerative framework, the Lung Swapping Autoencoder (LSAE), that learns\nfactorized representations of a CXR to disentangle the texture factor from the\nstructure factor. Specifically, by adversarial training, the LSAE is optimized\nto generate a hybrid image that preserves the lung shape in one image but\ninherits the lung texture of another. To demonstrate the effectiveness of the\ndisentangled texture representation, we evaluate the texture encoder $Enc^t$ in\nLSAE on ChestX-ray14 (N=112,120), and our own multi-institutional COVID-19\noutcome prediction dataset, COVOC (N=340 (Subset-1) + 53 (Subset-2)). On both\ndatasets, we reach or surpass the state-of-the-art by finetuning $Enc^t$ in\nLSAE that is 77% smaller than a baseline Inception v3. Additionally, in\nsemi-and-self supervised settings with a similar model budget, $Enc^t$ in LSAE\nis also competitive with the state-of-the-art MoCo. By \"re-mixing\" the texture\nand shape factors, we generate meaningful hybrid images that can augment the\ntraining set. This data augmentation method can further improve COVOC\nprediction performance. The improvement is consistent even when we directly\nevaluate the Subset-1 trained model on Subset-2 without any fine-tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhou_L/0/1/0/all/0/1\">Lei Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bae_J/0/1/0/all/0/1\">Joseph Bae</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_H/0/1/0/all/0/1\">Huidong Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Singh_G/0/1/0/all/0/1\">Gagandeep Singh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Green_J/0/1/0/all/0/1\">Jeremy Green</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gupta_A/0/1/0/all/0/1\">Amit Gupta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Samaras_D/0/1/0/all/0/1\">Dimitris Samaras</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Prasanna_P/0/1/0/all/0/1\">Prateek Prasanna</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly Supervised Contrastive Learning for Better Severity Scoring of Lung Ultrasound. (arXiv:2201.07357v1 [eess.IV])","link":"http://arxiv.org/abs/2201.07357","description":"<p>With the onset of the COVID-19 pandemic, ultrasound has emerged as an\neffective tool for bedside monitoring of patients. Due to this, a large amount\nof lung ultrasound scans have been made available which can be used for AI\nbased diagnosis and analysis. Several AI-based patient severity scoring models\nhave been proposed that rely on scoring the appearance of the ultrasound scans.\nAI models are trained using ultrasound-appearance severity scores that are\nmanually labeled based on standardized visual features. We address the\nchallenge of labeling every ultrasound frame in the video clips. Our\ncontrastive learning method treats the video clip severity labels as noisy weak\nseverity labels for individual frames, thus requiring only video-level labels.\nWe show that it performs better than the conventional cross-entropy loss based\ntraining. We combine frame severity predictions to come up with video severity\npredictions and show that the frame based model achieves comparable performance\nto a video based TSM model, on a large dataset combining public and private\nsources.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Gare_G/0/1/0/all/0/1\">Gautam Rajendrakumar Gare</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tran_H/0/1/0/all/0/1\">Hai V. Tran</a>, <a href=\"http://arxiv.org/find/eess/1/au:+deBoisblanc_B/0/1/0/all/0/1\">Bennett P deBoisblanc</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rodriguez_R/0/1/0/all/0/1\">Ricardo Luis Rodriguez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Galeotti_J/0/1/0/all/0/1\">John Michael Galeotti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TriCoLo: Trimodal Contrastive Loss for Fine-grained Text to Shape Retrieval. (arXiv:2201.07366v1 [cs.CV])","link":"http://arxiv.org/abs/2201.07366","description":"<p>Recent work on contrastive losses for learning joint embeddings over\nmultimodal data has been successful at downstream tasks such as retrieval and\nclassification. On the other hand, work on joint representation learning for 3D\nshapes and text has thus far mostly focused on improving embeddings through\nmodeling of complex attention between representations , or multi-task learning\n. We show that with large batch contrastive learning we achieve SoTA on\ntext-shape retrieval without complex attention mechanisms or losses. Prior work\nin 3D and text representations has also focused on bimodal representation\nlearning using either voxels or multi-view images with text. To this end, we\npropose a trimodal learning scheme to achieve even higher performance and\nbetter representations for all modalities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ruan_Y/0/1/0/all/0/1\">Yue Ruan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Han-Hung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Ke Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_A/0/1/0/all/0/1\">Angel X. Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Role of Pleura and Adipose in Lung Ultrasound AI. (arXiv:2201.07368v1 [eess.IV])","link":"http://arxiv.org/abs/2201.07368","description":"<p>In this paper, we study the significance of the pleura and adipose tissue in\nlung ultrasound AI analysis. We highlight their more prominent appearance when\nusing high-frequency linear (HFL) instead of curvilinear ultrasound probes,\nshowing HFL reveals better pleura detail. We compare the diagnostic utility of\nthe pleura and adipose tissue using an HFL ultrasound probe. Masking the\nadipose tissue during training and inference (while retaining the pleural line\nand Merlin's space artifacts such as A-lines and B-lines) improved the AI\nmodel's diagnostic accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Gare_G/0/1/0/all/0/1\">Gautam Rajendrakumar Gare</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_W/0/1/0/all/0/1\">Wanwen Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hung_A/0/1/0/all/0/1\">Alex Ling Yu Hung</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_E/0/1/0/all/0/1\">Edward Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tran_H/0/1/0/all/0/1\">Hai V. Tran</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fox_T/0/1/0/all/0/1\">Tom Fox</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lowery_P/0/1/0/all/0/1\">Pete Lowery</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zamora_K/0/1/0/all/0/1\">Kevin Zamora</a>, <a href=\"http://arxiv.org/find/eess/1/au:+deBoisblanc_B/0/1/0/all/0/1\">Bennett P deBoisblanc</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rodriguez_R/0/1/0/all/0/1\">Ricardo Luis Rodriguez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Galeotti_J/0/1/0/all/0/1\">John Michael Galeotti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Online Deep Learning based on Auto-Encoder. (arXiv:2201.07383v1 [cs.LG])","link":"http://arxiv.org/abs/2201.07383","description":"<p>Online learning is an important technical means for sketching massive\nreal-time and high-speed data. Although this direction has attracted intensive\nattention, most of the literature in this area ignore the following three\nissues: (1) they think little of the underlying abstract hierarchical latent\ninformation existing in examples, even if extracting these abstract\nhierarchical latent representations is useful to better predict the class\nlabels of examples; (2) the idea of preassigned model on unseen datapoints is\nnot suitable for modeling streaming data with evolving probability\ndistribution. This challenge is referred as model flexibility. And so, with\nthis in minds, the online deep learning model we need to design should have a\nvariable underlying structure; (3) moreover, it is of utmost importance to\nfusion these abstract hierarchical latent representations to achieve better\nclassification performance, and we should give different weights to different\nlevels of implicit representation information when dealing with the data\nstreaming where the data distribution changes. To address these issues, we\npropose a two-phase Online Deep Learning based on Auto-Encoder (ODLAE). Based\non auto-encoder, considering reconstruction loss, we extract abstract\nhierarchical latent representations of instances; Based on predictive loss, we\ndevise two fusion strategies: the output-level fusion strategy, which is\nobtained by fusing the classification results of encoder each hidden layer; and\nfeature-level fusion strategy, which is leveraged self-attention mechanism to\nfusion every hidden layer output. Finally, in order to improve the robustness\nof the algorithm, we also try to utilize the denoising auto-encoder to yield\nhierarchical latent representations. Experimental results on different datasets\nare presented to verify the validity of our proposed algorithm (ODLAE)\noutperforms several baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Si-si Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jian-wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_X/0/1/0/all/0/1\">Xin Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_R/0/1/0/all/0/1\">Run-kun Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_S/0/1/0/all/0/1\">Si-ming Lian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Swin-Pose: Swin Transformer Based Human Pose Estimation. (arXiv:2201.07384v1 [cs.CV])","link":"http://arxiv.org/abs/2201.07384","description":"<p>Convolutional neural networks (CNNs) have been widely utilized in many\ncomputer vision tasks. However, CNNs have a fixed reception field and lack the\nability of long-range perception, which is crucial to human pose estimation.\nDue to its capability to capture long-range dependencies between pixels,\ntransformer architecture has been adopted to computer vision applications\nrecently and is proven to be a highly effective architecture. We are interested\nin exploring its capability in human pose estimation, and thus propose a novel\nmodel based on transformer architecture, enhanced with a feature pyramid fusion\nstructure. More specifically, we use pre-trained Swin Transformer as our\nbackbone and extract features from input images, we leverage a feature pyramid\nstructure to extract feature maps from different stages. By fusing the features\ntogether, our model predicts the keypoint heatmap. The experiment results of\nour study have demonstrated that the proposed transformer-based model can\nachieve better performance compared to the state-of-the-art CNN-based models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Z/0/1/0/all/0/1\">Zinan Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chenxi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Ying Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yu Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KappaFace: Adaptive Additive Angular Margin Loss for Deep Face Recognition. (arXiv:2201.07394v1 [cs.CV])","link":"http://arxiv.org/abs/2201.07394","description":"<p>Feature learning is a widely used method employed for large-scale face\nrecognition. Recently, large-margin softmax loss methods have demonstrated\nsignificant enhancements on deep face recognition. These methods propose fixed\npositive margins in order to enforce intra-class compactness and inter-class\ndiversity. However, the majority of the proposed methods do not consider the\nclass imbalance issue, which is a major challenge in practice for developing\ndeep face recognition models. We hypothesize that it significantly affects the\ngeneralization ability of the deep face models. Inspired by this observation,\nwe introduce a novel adaptive strategy, called KappaFace, to modulate the\nrelative importance based on class difficultness and imbalance. With the\nsupport of the von Mises-Fisher distribution, our proposed KappaFace loss can\nintensify the margin's magnitude for hard learning or low concentration classes\nwhile relaxing it for counter classes. Experiments conducted on popular facial\nbenchmarks demonstrate that our proposed method achieves superior performance\nto the state-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Oinar_C/0/1/0/all/0/1\">Chingis Oinar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_B/0/1/0/all/0/1\">Binh M. Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woo_S/0/1/0/all/0/1\">Simon S. Woo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Poseur: Direct Human Pose Regression with Transformers. (arXiv:2201.07412v1 [cs.CV])","link":"http://arxiv.org/abs/2201.07412","description":"<p>We propose a direct, regression-based approach to 2D human pose estimation\nfrom single images. We formulate the problem as a sequence prediction task,\nwhich we solve using a Transformer network. This network directly learns a\nregression mapping from images to the keypoint coordinates, without resorting\nto intermediate representations such as heatmaps. This approach avoids much of\nthe complexity associated with heatmap-based approaches. To overcome the\nfeature misalignment issues of previous regression-based methods, we propose an\nattention mechanism that adaptively attends to the features that are most\nrelevant to the target keypoints, considerably improving the accuracy.\nImportantly, our framework is end-to-end differentiable, and naturally learns\nto exploit the dependencies between keypoints. Experiments on MS-COCO and MPII,\ntwo predominant pose-estimation datasets, demonstrate that our method\nsignificantly improves upon the state-of-the-art in regression-based pose\nestimation. More notably, ours is the first regression-based approach to\nperform favorably compared to the best heatmap-based pose estimation methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mao_W/0/1/0/all/0/1\">Weian Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yongtao Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1\">Zhi Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinlong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhibin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hengel_A/0/1/0/all/0/1\">Anton van den Hengel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Deep Blind Video Super-Resolution. (arXiv:2201.07422v1 [cs.CV])","link":"http://arxiv.org/abs/2201.07422","description":"<p>Existing deep learning-based video super-resolution (SR) methods usually\ndepend on the supervised learning approach, where the training data is usually\ngenerated by the blurring operation with known or predefined kernels (e.g.,\nBicubic kernel) followed by a decimation operation. However, this does not hold\nfor real applications as the degradation process is complex and cannot be\napproximated by these idea cases well. Moreover, obtaining high-resolution (HR)\nvideos and the corresponding low-resolution (LR) ones in real-world scenarios\nis difficult. To overcome these problems, we propose a self-supervised learning\nmethod to solve the blind video SR problem, which simultaneously estimates blur\nkernels and HR videos from the LR videos. As directly using LR videos as\nsupervision usually leads to trivial solutions, we develop a simple and\neffective method to generate auxiliary paired data from original LR videos\naccording to the image formation of video SR, so that the networks can be\nbetter constrained by the generated paired data for both blur kernel estimation\nand latent HR video restoration. In addition, we introduce an optical flow\nestimation module to exploit the information from adjacent frames for HR video\nrestoration. Experiments show that our method performs favorably against\nstate-of-the-art ones on benchmarks and real-world videos.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_H/0/1/0/all/0/1\">Haoran Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Jinshan Pan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WebUAV-3M: A Benchmark Unveiling the Power of Million-Scale Deep UAV Tracking. (arXiv:2201.07425v1 [cs.CV])","link":"http://arxiv.org/abs/2201.07425","description":"<p>In this work, we contribute a new million-scale Unmanned Aerial Vehicle (UAV)\ntracking benchmark, called WebUAV-3M. Firstly, we collect 4,485 videos with\nmore than 3M frames from the Internet. Then, an efficient and scalable\nSemi-Automatic Target Annotation (SATA) pipeline is devised to label the\ntremendous WebUAV-3M in every frame. To the best of our knowledge, the densely\nbounding box annotated WebUAV-3M is by far the largest public UAV tracking\nbenchmark. We expect to pave the way for the follow-up study in the UAV\ntracking by establishing a million-scale annotated benchmark covering a wide\nrange of target categories. Moreover, considering the close connections among\nvisual appearance, natural language and audio, we enrich WebUAV-3M by providing\nnatural language specification and audio description, encouraging the\nexploration of natural language features and audio cues for UAV tracking.\nEquipped with this benchmark, we delve into million-scale deep UAV tracking\nproblems, aiming to provide the community with a dedicated large-scale\nbenchmark for training deep UAV trackers and evaluating UAV tracking\napproaches. Extensive experiments on WebUAV-3M demonstrate that there is still\na big room for robust deep UAV tracking improvements. The dataset, toolkits and\nbaseline results will be available at\n\\url{https://github.<a href=\"/abs/com/9836328\">com/9836328</a>47/WebUAV-3M}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chunhui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Guanjie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Li Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yinan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuxuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1\">Xiang Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1\">Shiming Ge</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Variable Augmented Network for Invertible MR Coil Compression. (arXiv:2201.07428v1 [cs.CV])","link":"http://arxiv.org/abs/2201.07428","description":"<p>A large number of coils are able to provide enhanced signal-to-noise ratio\nand improve imaging performance in parallel imaging. As the increasingly grow\nof coil number, however, it simultaneously aggravates the drawbacks of data\nstorage and reconstruction speed, especially in some iterative reconstructions.\nCoil compression addresses these issues by generating fewer virtual coils. In\nthis work, a novel variable augmented network for invertible coil compression\n(VAN-ICC) is presented, which utilizes inherent reversibility of\nnormalizing-flow-based models, for better compression and invertible recovery.\nVAN-ICC trains invertible network by finding an invertible and bijective\nfunction, which can map the original image to the compression image. In the\nexperiments, both fully-sampled images and under-sampled images were used to\nverify the effectiveness of the model. Extensive quantitative and qualitative\nevaluations demonstrated that, in comparison with SCC and GCC, VAN-ICC can\ncarry through better compression effect with equal number of virtual coils.\nAdditionally, its performance is not susceptible to different num-ber of\nvirtual coils.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liao_X/0/1/0/all/0/1\">Xianghao Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shanshan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_L/0/1/0/all/0/1\">Lanlan Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuhao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1\">Dong Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qiegen Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth. (arXiv:2201.07436v1 [cs.CV])","link":"http://arxiv.org/abs/2201.07436","description":"<p>Depth estimation from a single image is an important task that can be applied\nto various fields in computer vision, and has grown rapidly with the\ndevelopment of convolutional neural networks. In this paper, we propose a novel\nstructure and training strategy for monocular depth estimation to further\nimprove the prediction accuracy of the network. We deploy a hierarchical\ntransformer encoder to capture and convey the global context, and design a\nlightweight yet powerful decoder to generate an estimated depth map while\nconsidering local connectivity. By constructing connected paths between\nmulti-scale local features and the global decoding stream with our proposed\nselective feature fusion module, the network can integrate both representations\nand recover fine details. In addition, the proposed decoder shows better\nperformance than the previously proposed decoders, with considerably less\ncomputational complexity. Furthermore, we improve the depth-specific\naugmentation method by utilizing an important observation in depth estimation\nto enhance the model. Our network achieves state-of-the-art performance over\nthe challenging depth dataset NYU Depth V2. Extensive experiments have been\nconducted to validate and show the effectiveness of the proposed approach.\nFinally, our model shows better generalisation ability and robustness than\nother comparative models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Doyeon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ga_W/0/1/0/all/0/1\">Woonghyun Ga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahn_P/0/1/0/all/0/1\">Pyungwhan Ahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joo_D/0/1/0/all/0/1\">Donggyu Joo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chun_S/0/1/0/all/0/1\">Sehwan Chun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Junmo Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TransFuse: A Unified Transformer-based Image Fusion Framework using Self-supervised Learning. (arXiv:2201.07451v1 [cs.CV])","link":"http://arxiv.org/abs/2201.07451","description":"<p>Image fusion is a technique to integrate information from multiple source\nimages with complementary information to improve the richness of a single\nimage. Due to insufficient task-specific training data and corresponding ground\ntruth, most existing end-to-end image fusion methods easily fall into\noverfitting or tedious parameter optimization processes. Two-stage methods\navoid the need of large amount of task-specific training data by training\nencoder-decoder network on large natural image datasets and utilizing the\nextracted features for fusion, but the domain gap between natural images and\ndifferent fusion tasks results in limited performance. In this study, we design\na novel encoder-decoder based image fusion framework and propose a\ndestruction-reconstruction based self-supervised training scheme to encourage\nthe network to learn task-specific features. Specifically, we propose three\ndestruction-reconstruction self-supervised auxiliary tasks for multi-modal\nimage fusion, multi-exposure image fusion and multi-focus image fusion based on\npixel intensity non-linear transformation, brightness transformation and noise\ntransformation, respectively. In order to encourage different fusion tasks to\npromote each other and increase the generalizability of the trained network, we\nintegrate the three self-supervised auxiliary tasks by randomly choosing one of\nthem to destroy a natural image in model training. In addition, we design a new\nencoder that combines CNN and Transformer for feature extraction, so that the\ntrained model can exploit both local and global information. Extensive\nexperiments on multi-modal image fusion, multi-exposure image fusion and\nmulti-focus image fusion tasks demonstrate that our proposed method achieves\nthe state-of-the-art performance in both subjective and objective evaluations.\nThe code will be publicly available soon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1\">Linhao Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shaolei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Manning Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shiman Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_S/0/1/0/all/0/1\">Siqi Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Q/0/1/0/all/0/1\">Qin Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1\">Zhijian Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using Self-Supervised Pretext Tasks for Active Learning. (arXiv:2201.07459v1 [cs.CV])","link":"http://arxiv.org/abs/2201.07459","description":"<p>Labeling a large set of data is expensive. Active learning aims to tackle\nthis problem by asking to annotate only the most informative data from the\nunlabeled set. We propose a novel active learning approach that utilizes\nself-supervised pretext tasks and a unique data sampler to select data that are\nboth difficult and representative. We discover that the loss of a simple\nself-supervised pretext task, such as rotation prediction, is closely\ncorrelated to the downstream task loss. The pretext task learner is trained on\nthe unlabeled set, and the unlabeled data are sorted and grouped into batches\nby their pretext task losses. In each iteration, the main task model is used to\nsample the most uncertain data in a batch to be annotated. We evaluate our\nmethod on various image classification and segmentation benchmarks and achieve\ncompelling performances on CIFAR10, Caltech-101, ImageNet, and CityScapes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1\">John Seon Keun Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_M/0/1/0/all/0/1\">Minseok Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jongchan Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_D/0/1/0/all/0/1\">Dong-Geol Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"High-fidelity 3D Model Compression based on Key Spheres. (arXiv:2201.07486v1 [cs.CV])","link":"http://arxiv.org/abs/2201.07486","description":"<p>In recent years, neural signed distance function (SDF) has become one of the\nmost effectiverepresentation methods for 3D models. By learning continuous SDFs\nin 3D space, neuralnetworks can predict the distance from a given query space\npoint to its closest object surface,whose positive and negative signs denote\ninside and outside of the object, respectively.Training a specific network for\neach 3D model, which individually embeds its shape, canrealize compressed\nrepresentation of objects by storing fewer network (and possibly\nlatent)parameters. Consequently, reconstruction through network inference and\nsurface recoverycan be achieved. In this paper, we propose an SDF prediction\nnetwork using explicit keyspheres as input. Key spheres are extracted from the\ninternal space of objects, whosecenters either have relatively larger SDF\nvalues (sphere radii), or are located at essentialpositions. By inputting the\nspatial information of multiple spheres which imply differentlocal shapes, the\nproposed method can significantly improve the reconstruction accuracywith a\nnegligible storage cost. Compared to previous works, our method achieves the\nhigh-fidelity and high-compression 3D object coding and reconstruction.\nExperiments conductedon three datasets verify the superior performance of our\nmethod.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuanzhan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuqi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yujie Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Siyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_S/0/1/0/all/0/1\">Shen Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yanting Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly Supervised Semantic Segmentation of Remote Sensing Images for Tree Species Classification Based on Explanation Methods. (arXiv:2201.07495v1 [cs.CV])","link":"http://arxiv.org/abs/2201.07495","description":"<p>The collection of a high number of pixel-based labeled training samples for\ntree species identification is time consuming and costly in operational\nforestry applications. To address this problem, in this paper we investigate\nthe effectiveness of explanation methods for deep neural networks in performing\nweakly supervised semantic segmentation using only image-level labels.\nSpecifically, we consider four methods:i) class activation maps (CAM); ii)\ngradient-based CAM; iii) pixel correlation module; and iv) self-enhancing maps\n(SEM). We compare these methods with each other using both quantitative and\nqualitative measures of their segmentation accuracy, as well as their\ncomputational requirements. Experimental results obtained on an aerial image\narchive show that:i) considered explanation techniques are highly relevant for\nthe identification of tree species with weak supervision; and ii) the SEM\noutperforms the other considered methods. The code for this paper is publicly\navailable at https://git.tu-berlin.de/rsim/rs_wsss.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ahlswede_S/0/1/0/all/0/1\">Steve Ahlswede</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thekke_Madam_N/0/1/0/all/0/1\">Nimisha Thekke-Madam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schulz_C/0/1/0/all/0/1\">Christian Schulz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kleinschmit_B/0/1/0/all/0/1\">Birgit Kleinschmit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demir_B/0/1/0/all/0/1\">Beg&#xfc;m Demir</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Virtual Coil Augmentation Technology for MRI via Deep Learning. (arXiv:2201.07540v1 [cs.CV])","link":"http://arxiv.org/abs/2201.07540","description":"<p>Magnetic resonance imaging (MRI) is a widely used medical imaging modality.\nHowever, due to the limitations in hardware, scan time, and throughput, it is\noften clinically challenging to obtain high-quality MR images. In this article,\nwe propose a method of using artificial intelligence to expand the channel to\nachieve the effect of increasing the virtual coil. The main feature of our work\nis utilizing dummy variable technology to expand the channel in both the image\nand k-space domains. The high-dimensional information formed by channel\nexpansion is used as the prior information of parallel imaging to improve the\nreconstruction effect of parallel imaging. Two features are introduced, namely\nvariable enhancement and sum of squares (SOS) objective function. Variable\nargumentation provides the network with more high-dimensional prior\ninformation, which is helpful for the network to extract the deep feature\nin-formation of the image. The SOS objective function is employed to solve the\nproblem that k-space data is difficult to train while speeding up the\nconvergence speed. Ablation studies and experimental results demonstrate that\nour method achieves significantly higher image reconstruction performance than\ncurrent state-of-the-art techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Cailian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_X/0/1/0/all/0/1\">Xianghao Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuhao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Minghui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qiegen Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simpler is better: spectral regularization and up-sampling techniques for variational autoencoders. (arXiv:2201.07544v1 [cs.LG])","link":"http://arxiv.org/abs/2201.07544","description":"<p>Full characterization of the spectral behavior of generative models based on\nneural networks remains an open issue. Recent research has focused heavily on\ngenerative adversarial networks and the high-frequency discrepancies between\nreal and generated images. The current solution to avoid this is to either\nreplace transposed convolutions with bilinear up-sampling or add a spectral\nregularization term in the generator. It is well known that Variational\nAutoencoders (VAEs) also suffer from these issues. In this work, we propose a\nsimple 2D Fourier transform-based spectral regularization loss for the VAE and\nshow that it can achieve results equal to, or better than, the current\nstate-of-the-art in frequency-aware losses for generative models. In addition,\nwe experiment with altering the up-sampling procedure in the generator network\nand investigate how it influences the spectral performance of the model. We\ninclude experiments on synthetic and real data sets to demonstrate our results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bjork_S/0/1/0/all/0/1\">Sara Bj&#xf6;rk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Myhre_J/0/1/0/all/0/1\">Jonas Nordhaug Myhre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johansen_T/0/1/0/all/0/1\">Thomas Haugland Johansen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learned Cone-Beam CT Reconstruction Using Neural Ordinary Differential Equations. (arXiv:2201.07562v1 [eess.IV])","link":"http://arxiv.org/abs/2201.07562","description":"<p>Learned iterative reconstruction algorithms for inverse problems offer the\nflexibility to combine analytical knowledge about the problem with modules\nlearned from data. This way, they achieve high reconstruction performance while\nensuring consistency with the measured data. In computed tomography, extending\nsuch approaches from 2D fan-beam to 3D cone-beam data is challenging due to the\nprohibitively high GPU memory that would be needed to train such models. This\npaper proposes to use neural ordinary differential equations to solve the\nreconstruction problem in a residual formulation via numerical integration. For\ntraining, there is no need to backpropagate through several unrolled network\nblocks nor through the internals of the solver. Instead, the gradients are\nobtained very memory-efficiently in the neural ODE setting allowing for\ntraining on a single consumer graphics card. The method is able to reduce the\nroot mean squared error by over 30% compared to the best performing classical\niterative reconstruction algorithm and produces high quality cone-beam\nreconstructions even in a sparse view scenario.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Thies_M/0/1/0/all/0/1\">Mareike Thies</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wagner_F/0/1/0/all/0/1\">Fabian Wagner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gu_M/0/1/0/all/0/1\">Mingxuan Gu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Folle_L/0/1/0/all/0/1\">Lukas Folle</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Felsner_L/0/1/0/all/0/1\">Lina Felsner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maier_A/0/1/0/all/0/1\">Andreas Maier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Superpixel Pre-Segmentation of HER2 Slides for Efficient Annotation. (arXiv:2201.07572v1 [cs.CV])","link":"http://arxiv.org/abs/2201.07572","description":"<p>Supervised deep learning has shown state-of-the-art performance for medical\nimage segmentation across different applications, including histopathology and\ncancer research; however, the manual annotation of such data is extremely\nlaborious. In this work, we explore the use of superpixel approaches to compute\na pre-segmentation of HER2 stained images for breast cancer diagnosis that\nfacilitates faster manual annotation and correction in a second step. Four\nmethods are compared: Standard Simple Linear Iterative Clustering (SLIC) as a\nbaseline, a domain adapted SLIC, and superpixels based on feature embeddings of\na pretrained ResNet-50 and a denoising autoencoder. To tackle oversegmentation,\nwe propose to hierarchically merge superpixels, based on their content in the\nrespective feature space. When evaluating the approaches on fully manually\nannotated images, we observe that the autoencoder-based superpixels achieve a\n23% increase in boundary F1 score compared to the baseline SLIC superpixels.\nFurthermore, the boundary F1 score increases by 73% when hierarchical\nclustering is applied on the adapted SLIC and the autoencoder-based\nsuperpixels. These evaluations show encouraging first results for a\npre-segmentation for efficient manual refinement without the need for an\ninitial set of annotated training data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ottl_M/0/1/0/all/0/1\">Mathias &#xd6;ttl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Monius_J/0/1/0/all/0/1\">Jana M&#xf6;nius</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marzahl_C/0/1/0/all/0/1\">Christian Marzahl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rubner_M/0/1/0/all/0/1\">Matthias R&#xfc;bner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geppert_C/0/1/0/all/0/1\">Carol I. Geppert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hartmann_A/0/1/0/all/0/1\">Arndt Hartmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beckmann_M/0/1/0/all/0/1\">Matthias W. Beckmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fasching_P/0/1/0/all/0/1\">Peter Fasching</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maier_A/0/1/0/all/0/1\">Andreas Maier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erber_R/0/1/0/all/0/1\">Ramona Erber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Breininger_K/0/1/0/all/0/1\">Katharina Breininger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DMF-Net: Dual-Branch Multi-Scale Feature Fusion Network for copy forgery identification of anti-counterfeiting QR code. (arXiv:2201.07583v1 [cs.CV])","link":"http://arxiv.org/abs/2201.07583","description":"<p>Anti-counterfeiting QR codes are widely used in people's work and life,\nespecially in product packaging. However, the anti-counterfeiting QR code has\nthe risk of being copied and forged in the circulation process. In reality,\ncopying is usually based on genuine anti-counterfeiting QR codes, but the\nbrands and models of copiers are diverse, and it is extremely difficult to\ndetermine which individual copier the forged anti-counterfeiting code come\nfrom. In response to the above problems, this paper proposes a method for copy\nforgery identification of anti-counterfeiting QR code based on deep learning.\nWe first analyze the production principle of anti-counterfeiting QR code, and\nconvert the identification of copy forgery to device category forensics, and\nthen a Dual-Branch Multi-Scale Feature Fusion network is proposed. During the\ndesign of the network, we conducted a detailed analysis of the data\npreprocessing layer, single-branch design, etc., combined with experiments, the\nspecific structure of the dual-branch multi-scale feature fusion network is\ndetermined. The experimental results show that the proposed method has achieved\na high accuracy of copy forgery identification, which exceeds the current\nseries of methods in the field of image forensics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zhongyuan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Hong Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1\">Changhui You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Real-time Recognition of Yoga Poses using computer Vision for Smart Health Care. (arXiv:2201.07594v1 [cs.CV])","link":"http://arxiv.org/abs/2201.07594","description":"<p>Nowadays, yoga has become a part of life for many people. Exercises and\nsports technological assistance is implemented in yoga pose identification. In\nthis work, a self-assistance based yoga posture identification technique is\ndeveloped, which helps users to perform Yoga with the correction feature in\nReal-time. The work also presents Yoga-hand mudra (hand gestures)\nidentification. The YOGI dataset has been developed which include 10 Yoga\npostures with around 400-900 images of each pose and also contain 5 mudras for\nidentification of mudras postures. It contains around 500 images of each mudra.\nThe feature has been extracted by making a skeleton on the body for yoga poses\nand hand for mudra poses. Two different algorithms have been used for creating\na skeleton one for yoga poses and the second for hand mudras. Angles of the\njoints have been extracted as a features for different machine learning and\ndeep learning models. among all the models XGBoost with RandomSearch CV is most\naccurate and gives 99.2\\% accuracy. The complete design framework is described\nin the present paper.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Abhishek Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_Y/0/1/0/all/0/1\">Yash Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_Y/0/1/0/all/0/1\">Yash Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_P/0/1/0/all/0/1\">Prateek Jain</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Confidence-based Iterative Solver of Depths and Surface Normals for Deep Multi-view Stereo. (arXiv:2201.07609v1 [cs.CV])","link":"http://arxiv.org/abs/2201.07609","description":"<p>In this paper, we introduce a deep multi-view stereo (MVS) system that\njointly predicts depths, surface normals and per-view confidence maps. The key\nto our approach is a novel solver that iteratively solves for per-view depth\nmap and normal map by optimizing an energy potential based on the locally\nplanar assumption. Specifically, the algorithm updates depth map by propagating\nfrom neighboring pixels with slanted planes, and updates normal map with local\nprobabilistic plane fitting. Both two steps are monitored by a customized\nconfidence map. This solver is not only effective as a post-processing tool for\nplane-based depth refinement and completion, but also differentiable such that\nit can be efficiently integrated into deep learning pipelines. Our multi-view\nstereo system employs multiple optimization steps of the solver over the\ninitial prediction of depths and surface normals. The whole system can be\ntrained end-to-end, decoupling the challenging problem of matching pixels\nwithin poorly textured regions from the cost-volume based neural network.\nExperimental results on ScanNet and RGB-D Scenes V2 demonstrate\nstate-of-the-art performance of the proposed deep MVS system on multi-view\ndepth estimation, with our proposed solver consistently improving the depth\nquality over both conventional and deep learning based MVS pipelines. Code is\navailable at https://github.com/thuzhaowang/idn-solver.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shaohui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yi Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Hengkai Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong-Jin Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Nonlinear Unknown Input Observability and Unknown Input Reconstruction: The General Analytical Solution. (arXiv:2201.07610v1 [math.OC])","link":"http://arxiv.org/abs/2201.07610","description":"<p>Observability is a fundamental structural property of any dynamic system and\ndescribes the possibility of reconstructing the state that characterizes the\nsystem from observing its inputs and outputs. Despite the huge effort made to\nstudy this property and to introduce analytical criteria able to check whether\na dynamic system satisfies this property or not, there is no general analytical\ncriterion to automatically check the state observability when the dynamics are\nalso driven by unknown inputs. Here, we introduce the general analytical\nsolution of this fundamental problem, often called the unknown input\nobservability problem. This paper provides the general analytical solution of\nthis problem, namely, it provides the systematic procedure, based on automatic\ncomputation (differentiation and matrix rank determination), that allows us to\nautomatically check the state observability even in the presence of unknown\ninputs. A first solution of this problem was presented in the second part of\nthe book: \"Observability: A New Theory Based on the Group of Invariance\" [45].\nThe solution presented by this paper completes the previous solution in [45].\nIn particular, the new solution exhaustively accounts for the systems that do\nnot belong to the category of the systems that are canonic with respect to\ntheir unknown inputs. The new solution is also provided in the form of a new\nalgorithm. A further novelty with respect to the algorithm provided in [45]\nconsists of a new convergence criterion that holds in all the cases (the\nconvergence criterion of the algorithm provided in [45] can fail in some\ncases). Finally, we also provide the answer to the problem of unknown input\nreconstruction which is intimately related to the problem of state\nobservability. We illustrate the implementation of the new algorithm by\nstudying a nonlinear system in the framework of visual-inertial sensor fusion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/math/1/au:+Martinelli_A/0/1/0/all/0/1\">Agostino Martinelli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CAST: Character labeling in Animation using Self-supervision by Tracking. (arXiv:2201.07619v1 [cs.CV])","link":"http://arxiv.org/abs/2201.07619","description":"<p>Cartoons and animation domain videos have very different characteristics\ncompared to real-life images and videos. In addition, this domain carries a\nlarge variability in styles. Current computer vision and deep-learning\nsolutions often fail on animated content because they were trained on natural\nimages. In this paper we present a method to refine a semantic representation\nsuitable for specific animated content. We first train a neural network on a\nlarge-scale set of animation videos and use the mapping to deep features as an\nembedding space. Next, we use self-supervision to refine the representation for\nany specific animation style by gathering many examples of animated characters\nin this style, using a multi-object tracking. These examples are used to define\ntriplets for contrastive loss training. The refined semantic space allows\nbetter clustering of animated characters even when they have diverse\nmanifestations. Using this space we can build dictionaries of characters in an\nanimation videos, and define specialized classifiers for specific stylistic\ncontent (e.g., characters in a specific animation series) with very little user\neffort. These classifiers are the basis for automatically labeling characters\nin animation videos. We present results on a collection of characters in a\nvariety of animation styles.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nir_O/0/1/0/all/0/1\">Oron Nir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rapoport_G/0/1/0/all/0/1\">Gal Rapoport</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shamir_A/0/1/0/all/0/1\">Ariel Shamir</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Training Challenges in Generative Adversarial Networks for Biomedical Image Analysis. (arXiv:2201.07646v1 [cs.LG])","link":"http://arxiv.org/abs/2201.07646","description":"<p>In biomedical image analysis, the applicability of deep learning methods is\ndirectly impacted by the quantity of image data available. This is due to deep\nlearning models requiring large image datasets to provide high-level\nperformance. Generative Adversarial Networks (GANs) have been widely utilized\nto address data limitations through the generation of synthetic biomedical\nimages. GANs consist of two models. The generator, a model that learns how to\nproduce synthetic images based on the feedback it receives. The discriminator,\na model that classifies an image as synthetic or real and provides feedback to\nthe generator. Throughout the training process, a GAN can experience several\ntechnical challenges that impede the generation of suitable synthetic imagery.\nFirst, the mode collapse problem whereby the generator either produces an\nidentical image or produces a uniform image from distinct input features.\nSecond, the non-convergence problem whereby the gradient descent optimizer\nfails to reach a Nash equilibrium. Thirdly, the vanishing gradient problem\nwhereby unstable training behavior occurs due to the discriminator achieving\noptimal classification performance resulting in no meaningful feedback being\nprovided to the generator. These problems result in the production of synthetic\nimagery that is blurry, unrealistic, and less diverse. To date, there has been\nno survey article outlining the impact of these technical challenges in the\ncontext of the biomedical imagery domain. This work presents a review and\ntaxonomy based on solutions to the training problems of GANs in the biomedical\nimaging domain. This survey highlights important challenges and outlines future\nresearch directions about the training of GANs in the domain of biomedical\nimagery.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saad_M/0/1/0/all/0/1\">Muhammad Muneeb Saad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OReilly_R/0/1/0/all/0/1\">Ruairi O&#x27;Reilly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rehmani_M/0/1/0/all/0/1\">Mubashir Husain Rehmani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Open Source Handwritten Text Recognition on Medieval Manuscripts using Mixed Models and Document-Specific Finetuning. (arXiv:2201.07661v1 [cs.CV])","link":"http://arxiv.org/abs/2201.07661","description":"<p>This paper deals with the task of practical and open source Handwritten Text\nRecognition (HTR) on German medieval manuscripts. We report on our efforts to\nconstruct mixed recognition models which can be applied out-of-the-box without\nany further document-specific training but also serve as a starting point for\nfinetuning by training a new model on a few pages of transcribed text (ground\ntruth). To train the mixed models we collected a corpus of 35 manuscripts and\nca. 12.5k text lines for two widely used handwriting styles, Gothic and\nBastarda cursives. Evaluating the mixed models out-of-the-box on four unseen\nmanuscripts resulted in an average Character Error Rate (CER) of 6.22%. After\ntraining on 2, 4 and eventually 32 pages the CER dropped to 3.27%, 2.58%, and\n1.65%, respectively. While the in-domain recognition and training of models\n(Bastarda model to Bastarda material, Gothic to Gothic) unsurprisingly yielded\nthe best results, finetuning out-of-domain models to unseen scripts was still\nshown to be superior to training from scratch.\n</p>\n<p>Our new mixed models have been made openly available to the community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Reul_C/0/1/0/all/0/1\">Christian Reul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tomasek_S/0/1/0/all/0/1\">Stefan Tomasek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Langhanki_F/0/1/0/all/0/1\">Florian Langhanki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Springmann_U/0/1/0/all/0/1\">Uwe Springmann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-automatic 3D Object Keypoint Annotation and Detection for the Masses. (arXiv:2201.07665v1 [cs.CV])","link":"http://arxiv.org/abs/2201.07665","description":"<p>Creating computer vision datasets requires careful planning and lots of time\nand effort. In robotics research, we often have to use standardized objects,\nsuch as the YCB object set, for tasks such as object tracking, pose estimation,\ngrasping and manipulation, as there are datasets and pre-learned methods\navailable for these objects. This limits the impact of our research since\nlearning-based computer vision methods can only be used in scenarios that are\nsupported by existing datasets.\n</p>\n<p>In this work, we present a full object keypoint tracking toolkit,\nencompassing the entire process from data collection, labeling, model learning\nand evaluation. We present a semi-automatic way of collecting and labeling\ndatasets using a wrist mounted camera on a standard robotic arm. Using our\ntoolkit and method, we are able to obtain a working 3D object keypoint detector\nand go through the whole process of data collection, annotation and learning in\njust a couple hours of active time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Blomqvist_K/0/1/0/all/0/1\">Kenneth Blomqvist</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_J/0/1/0/all/0/1\">Jen Jen Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ott_L/0/1/0/all/0/1\">Lionel Ott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siegwart_R/0/1/0/all/0/1\">Roland Siegwart</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neighborhood Spatial Aggregation MC Dropout for Efficient Uncertainty-aware Semantic Segmentation in Point Clouds. (arXiv:2201.07676v1 [cs.CV])","link":"http://arxiv.org/abs/2201.07676","description":"<p>Uncertainty-aware semantic segmentation of the point clouds includes the\npredictive uncertainty estimation and the uncertainty-guided model\noptimization. One key challenge in the task is the efficiency of point-wise\npredictive distribution establishment. The widely-used MC dropout establishes\nthe distribution by computing the standard deviation of samples using multiple\nstochastic forward propagations, which is time-consuming for tasks based on\npoint clouds containing massive points. Hence, a framework embedded with NSA-MC\ndropout, a variant of MC dropout, is proposed to establish distributions in\njust one forward pass. Specifically, the NSA-MC dropout samples the model many\ntimes through a space-dependent way, outputting point-wise distribution by\naggregating stochastic inference results of neighbors. Based on this, aleatoric\nand predictive uncertainties acquire from the predictive distribution. The\naleatoric uncertainty is integrated into the loss function to penalize noisy\npoints, avoiding the over-fitting of the model to some degree. Besides, the\npredictive uncertainty quantifies the confidence degree of predictions.\nExperimental results show that our framework obtains better segmentation\nresults of real-world point clouds and efficiently quantifies the credibility\nof results. Our NSA-MC dropout is several times faster than MC dropout, and the\ninference time does not establish a coupling relation with the sampling times.\nThe code will be available if the paper is accepted.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qi_C/0/1/0/all/0/1\">Chao Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1\">Jianqin Yin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GroupGazer: A Tool to Compute the Gaze per Participant in Groups with integrated Calibration to Map the Gaze Online to a Screen or Beamer Projection. (arXiv:2201.07692v1 [cs.CV])","link":"http://arxiv.org/abs/2201.07692","description":"<p>In this paper we present GroupGaze. It is a tool that can be used to\ncalculate the gaze direction and the gaze position of whole groups. GroupGazer\ncalculates the gaze direction of every single person in the image and allows to\nmap these gaze vectors to a projection like a projector. In addition to the\nperson-specific gaze direction, the person affiliation of each gaze vector is\nstored based on the position in the image. Also, it is possible to save the\ngroup attention after a calibration. The software is free to use and requires a\nsimple webcam as well as an NVIDIA GPU and the operating system Windows or\nLinux.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fuhl_W/0/1/0/all/0/1\">Wolfgang Fuhl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visualization and Analysis of Wearable Health Data From COVID-19 Patients. (arXiv:2201.07698v1 [cs.HC])","link":"http://arxiv.org/abs/2201.07698","description":"<p>Effective visualizations were evaluated to reveal relevant health patterns\nfrom multi-sensor real-time wearable devices that recorded vital signs from\npatients admitted to hospital with COVID-19. Furthermore, specific challenges\nassociated with wearable health data visualizations, such as fluctuating data\nquality resulting from compliance problems, time needed to charge the device\nand technical problems are described. As a primary use case, we examined the\ndetection and communication of relevant health patterns visible in the vital\nsigns acquired by the technology. Customized heat maps and bar charts were used\nto specifically highlight medically relevant patterns in vital signs. A survey\nof two medical doctors, one clinical project manager and seven health data\nscience researchers was conducted to evaluate the visualization methods. From a\ndataset of 84 hospitalized COVID-19 patients, we extracted one typical COVID-19\npatient history and based on the visualizations showcased the health history of\ntwo noteworthy patients. The visualizations were shown to be effective, simple\nand intuitive in deducing the health status of patients. For clinical staff who\nare time-constrained and responsible for numerous patients, such visualization\nmethods can be an effective tool to enable continuous acquisition and\nmonitoring of patients' health statuses even remotely.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Suter_S/0/1/0/all/0/1\">Susanne K. Suter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spinner_G/0/1/0/all/0/1\">Georg R. Spinner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoelz_B/0/1/0/all/0/1\">Bianca Hoelz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rey_S/0/1/0/all/0/1\">Sofia Rey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thanabalasingam_S/0/1/0/all/0/1\">Sujeanthraa Thanabalasingam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eckstein_J/0/1/0/all/0/1\">Jens Eckstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hirsch_S/0/1/0/all/0/1\">Sven Hirsch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Q-ViT: Fully Differentiable Quantization for Vision Transformer. (arXiv:2201.07703v1 [cs.CV])","link":"http://arxiv.org/abs/2201.07703","description":"<p>In this paper, we propose a fully differentiable quantization method for\nvision transformer (ViT) named as Q-ViT, in which both of the quantization\nscales and bit-widths are learnable parameters. Specifically, based on our\nobservation that heads in ViT display different quantization robustness, we\nleverage head-wise bit-width to squeeze the size of Q-ViT while preserving\nperformance. In addition, we propose a novel technique named switchable scale\nto resolve the convergence problem in the joint training of quantization scales\nand bit-widths. In this way, Q-ViT pushes the limits of ViT quantization to\n3-bit without heavy performance drop. Moreover, we analyze the quantization\nrobustness of every architecture component of ViT and show that the Multi-head\nSelf-Attention (MSA) and the Gaussian Error Linear Units (GELU) are the key\naspects for ViT quantization. This study provides some insights for further\nresearch about ViT quantization. Extensive experiments on different ViT models,\nsuch as DeiT and Swin Transformer show the effectiveness of our quantization\nmethod. In particular, our method outperforms the state-of-the-art uniform\nquantization method by 1.5% on DeiT-Tiny.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhexin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Tong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peisong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1\">Jian Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Object Detection in Autonomous Vehicles: Status and Open Challenges. (arXiv:2201.07706v1 [cs.CV])","link":"http://arxiv.org/abs/2201.07706","description":"<p>Object detection is a computer vision task that has become an integral part\nof many consumer applications today such as surveillance and security systems,\nmobile text recognition, and diagnosing diseases from MRI/CT scans. Object\ndetection is also one of the critical components to support autonomous driving.\nAutonomous vehicles rely on the perception of their surroundings to ensure safe\nand robust driving performance. This perception system uses object detection\nalgorithms to accurately determine objects such as pedestrians, vehicles,\ntraffic signs, and barriers in the vehicle's vicinity. Deep learning-based\nobject detectors play a vital role in finding and localizing these objects in\nreal-time. This article discusses the state-of-the-art in object detectors and\nopen challenges for their integration into autonomous vehicles.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Balasubramaniam_A/0/1/0/all/0/1\">Abhishek Balasubramaniam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasricha_S/0/1/0/all/0/1\">Sudeep Pasricha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards holistic scene understanding: Semantic segmentation and beyond. (arXiv:2201.07734v1 [cs.CV])","link":"http://arxiv.org/abs/2201.07734","description":"<p>This dissertation addresses visual scene understanding and enhances\nsegmentation performance and generalization, training efficiency of networks,\nand holistic understanding. First, we investigate semantic segmentation in the\ncontext of street scenes and train semantic segmentation networks on\ncombinations of various datasets. In Chapter 2 we design a framework of\nhierarchical classifiers over a single convolutional backbone, and train it\nend-to-end on a combination of pixel-labeled datasets, improving\ngeneralizability and the number of recognizable semantic concepts. Chapter 3\nfocuses on enriching semantic segmentation with weak supervision and proposes a\nweakly-supervised algorithm for training with bounding box-level and\nimage-level supervision instead of only with per-pixel supervision. The memory\nand computational load challenges that arise from simultaneous training on\nmultiple datasets are addressed in Chapter 4. We propose two methodologies for\nselecting informative and diverse samples from datasets with weak supervision\nto reduce our networks' ecological footprint without sacrificing performance.\nMotivated by memory and computation efficiency requirements, in Chapter 5, we\nrethink simultaneous training on heterogeneous datasets and propose a universal\nsemantic segmentation framework. This framework achieves consistent increases\nin performance metrics and semantic knowledgeability by exploiting various\nscene understanding datasets. Chapter 6 introduces the novel task of part-aware\npanoptic segmentation, which extends our reasoning towards holistic scene\nunderstanding. This task combines scene and parts-level semantics with\ninstance-level object detection. In conclusion, our contributions span over\nconvolutional network architectures, weakly-supervised learning, part and\npanoptic segmentation, paving the way towards a holistic, rich, and sustainable\nvisual scene understanding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meletis_P/0/1/0/all/0/1\">Panagiotis Meletis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A pipeline for automated processing of Corona KH-4 (1962-1972) stereo imagery. (arXiv:2201.07756v1 [cs.CV])","link":"http://arxiv.org/abs/2201.07756","description":"<p>The Corona KH-4 reconnaissance satellite missions from 1962-1972 acquired\npanoramic stereo imagery with high spatial resolution of 1.8-7.5 m. The\npotential of 800,000+ declassified Corona images has not been leveraged due to\nthe complexities arising from handling of panoramic imaging geometry, film\ndistortions and limited availability of the metadata required for\ngeoreferencing of the Corona imagery. This paper presents Corona Stereo\nPipeline (CoSP): A pipeline for processing of Corona KH-4 stereo panoramic\nimagery. CoSP utlizes a deep learning based feature matcher SuperGlue to\nautomatically match features point between Corona KH-4 images and recent\nsatellite imagery to generate Ground Control Points (GCPs). To model the\nimaging geometry and the scanning motion of the panoramic KH-4 cameras, a\nrigorous camera model consisting of modified collinearity equations with time\ndependent exterior orientation parameters is employed. The results show that\nusing the entire frame of the Corona image, bundle adjustment using\nwell-distributed GCPs results in an average standard deviation (SD) of less\nthan 2 pixels. The distortion pattern of image residuals of GCPs and y-parallax\nin epipolar resampled images suggest that film distortions due to long term\nstorage as likely cause of systematic deviations. Compared to the SRTM DEM, the\nCorona DEM computed using CoSP achieved a Normalized Median Absolute Deviation\n(NMAD) of elevation differences of ~4 m over an area of approx. 4000 $km^2$. We\nshow that the proposed pipeline can be applied to sequence of complex scenes\ninvolving high relief and glacierized terrain and that the resulting DEMs can\nbe used to compute long term glacier elevation changes over large areas.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghuffar_S/0/1/0/all/0/1\">Sajid Ghuffar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bolch_T/0/1/0/all/0/1\">Tobias Bolch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rupnik_E/0/1/0/all/0/1\">Ewelina Rupnik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_A/0/1/0/all/0/1\">Atanu Bhattacharya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Look Closer: Bridging Egocentric and Third-Person Views with Transformers for Robotic Manipulation. (arXiv:2201.07779v1 [cs.RO])","link":"http://arxiv.org/abs/2201.07779","description":"<p>Learning to solve precision-based manipulation tasks from visual feedback\nusing Reinforcement Learning (RL) could drastically reduce the engineering\nefforts required by traditional robot systems. However, performing fine-grained\nmotor control from visual inputs alone is challenging, especially with a static\nthird-person camera as often used in previous work. We propose a setting for\nrobotic manipulation in which the agent receives visual feedback from both a\nthird-person camera and an egocentric camera mounted on the robot's wrist.\nWhile the third-person camera is static, the egocentric camera enables the\nrobot to actively control its vision to aid in precise manipulation. To fuse\nvisual information from both cameras effectively, we additionally propose to\nuse Transformers with a cross-view attention mechanism that models spatial\nattention from one view to another (and vice-versa), and use the learned\nfeatures as input to an RL policy. Our method improves learning over strong\nsingle-view and multi-view baselines, and successfully transfers to a set of\nchallenging manipulation tasks on a real robot with uncalibrated cameras, no\naccess to state information, and a high degree of task variability. In a hammer\nmanipulation task, our method succeeds in 75% of trials versus 38% and 13% for\nmulti-view and single-view baselines, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jangir_R/0/1/0/all/0/1\">Rishabh Jangir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hansen_N/0/1/0/all/0/1\">Nicklas Hansen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosal_S/0/1/0/all/0/1\">Sambaral Ghosal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_M/0/1/0/all/0/1\">Mohit Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaolong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards a General Deep Feature Extractor for Facial Expression Recognition. (arXiv:2201.07781v1 [cs.CV])","link":"http://arxiv.org/abs/2201.07781","description":"<p>The human face conveys a significant amount of information. Through facial\nexpressions, the face is able to communicate numerous sentiments without the\nneed for verbalisation. Visual emotion recognition has been extensively\nstudied. Recently several end-to-end trained deep neural networks have been\nproposed for this task. However, such models often lack generalisation ability\nacross datasets. In this paper, we propose the Deep Facial Expression Vector\nExtractoR (DeepFEVER), a new deep learning-based approach that learns a visual\nfeature extractor general enough to be applied to any other facial emotion\nrecognition task or dataset. DeepFEVER outperforms state-of-the-art results on\nthe AffectNet and Google Facial Expression Comparison datasets. DeepFEVER's\nextracted features also generalise extremely well to other datasets -- even\nthose unseen during training -- namely, the Real-World Affective Faces (RAF)\ndataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schoneveld_L/0/1/0/all/0/1\">Liam Schoneveld</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Othmani_A/0/1/0/all/0/1\">Alice Othmani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ConDor: Self-Supervised Canonicalization of 3D Pose for Partial Shapes. (arXiv:2201.07788v1 [cs.CV])","link":"http://arxiv.org/abs/2201.07788","description":"<p>Progress in 3D object understanding has relied on manually canonicalized\nshape datasets that contain instances with consistent position and orientation\n(3D pose). This has made it hard to generalize these methods to in-the-wild\nshapes, eg., from internet model collections or depth sensors. ConDor is a\nself-supervised method that learns to Canonicalize the 3D orientation and\nposition for full and partial 3D point clouds. We build on top of Tensor Field\nNetworks (TFNs), a class of permutation- and rotation-equivariant, and\ntranslation-invariant 3D networks. During inference, our method takes an unseen\nfull or partial 3D point cloud at an arbitrary pose and outputs an equivariant\ncanonical pose. During training, this network uses self-supervision losses to\nlearn the canonical pose from an un-canonicalized collection of full and\npartial 3D point clouds. ConDor can also learn to consistently co-segment\nobject parts without any supervision. Extensive quantitative results on four\nnew metrics show that our approach outperforms existing methods while enabling\nnew applications such as operation on depth images and annotation transfer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sajnani_R/0/1/0/all/0/1\">Rahul Sajnani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poulenard_A/0/1/0/all/0/1\">Adrien Poulenard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_J/0/1/0/all/0/1\">Jivitesh Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dua_R/0/1/0/all/0/1\">Radhika Dua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1\">Leonidas J. Guibas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sridhar_S/0/1/0/all/0/1\">Srinath Sridhar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GAMMA: A General Agent Motion Model for Autonomous Driving. (arXiv:1906.01566v5 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/1906.01566","description":"<p>This paper presents GAMMA, a general motion prediction model that enables\nlarge-scale real-time simulation and planning for autonomous driving. GAMMA\nmodels heterogeneous, interactive traffic agents. They operate under diverse\nroad conditions, with various geometric and kinematic constraints. GAMMA treats\nthe prediction task as constrained optimization in traffic agents' velocity\nspace. The objective is to optimize an agent's driving performance, while\nobeying all the constraints resulting from the agent's kinematics, collision\navoidance with other agents, and the environmental context. Further, GAMMA\nexplicitly conditions the prediction on human behavioral states as parameters\nof the optimization model, in order to account for versatile human behaviors.\nWe evaluated GAMMA on a set of real-world benchmark datasets. The results show\nthat GAMMA achieves high prediction accuracy on both homogeneous and\nheterogeneous traffic datasets, with sub-millisecond execution time. Further,\nthe computational efficiency and the flexibility of GAMMA enable (i) simulation\nof mixed urban traffic at many locations worldwide and (ii) planning for\nautonomous driving in dense traffic with uncertain driver behaviors, both in\nreal-time. The open-source code of GAMMA is available online.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yuanfu Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_P/0/1/0/all/0/1\">Panpan Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yiyuan Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_D/0/1/0/all/0/1\">David Hsu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dilated Convolutions with Lateral Inhibitions for Semantic Image Segmentation. (arXiv:2006.03708v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2006.03708","description":"<p>Dilated convolutions are widely used in deep semantic segmentation models as\nthey can enlarge the filters' receptive field without adding additional weights\nnor sacrificing spatial resolution. However, as dilated convolutional filters\ndo not possess positional knowledge about the pixels on semantically meaningful\ncontours, they could lead to ambiguous predictions on object boundaries. In\naddition, although dilating the filter can expand its receptive field, the\ntotal number of sampled pixels remains unchanged, which usually comprises a\nsmall fraction of the receptive field's total area. Inspired by the Lateral\nInhibition (LI) mechanisms in human visual systems, we propose the dilated\nconvolution with lateral inhibitions (LI-Convs) to overcome these limitations.\nIntroducing LI mechanisms improves the convolutional filter's sensitivity to\nsemantic object boundaries. Moreover, since LI-Convs also implicitly take the\npixels from the laterally inhibited zones into consideration, they can also\nextract features at a denser scale. By integrating LI-Convs into the Deeplabv3+\narchitecture, we propose the Lateral Inhibited Atrous Spatial Pyramid Pooling\n(LI-ASPP), the Lateral Inhibited MobileNet-V2 (LI-MNV2) and the Lateral\nInhibited ResNet (LI-ResNet). Experimental results on three benchmark datasets\n(PASCAL VOC 2012, CelebAMask-HQ and ADE20K) show that our LI-based segmentation\nmodels outperform the baseline on all of them, thus verify the effectiveness\nand generality of the proposed LI-Convs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yujiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_M/0/1/0/all/0/1\">Mingzhi Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jie Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yiming Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pantic_M/0/1/0/all/0/1\">Maja Pantic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D Shape Reconstruction from Free-Hand Sketches. (arXiv:2006.09694v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2006.09694","description":"<p>Sketches are the most abstract 2D representations of real-world objects.\nAlthough a sketch usually has geometrical distortion and lacks visual cues,\nhumans can effortlessly envision a 3D object from it. This suggests that\nsketches encode the information necessary for reconstructing 3D shapes. Despite\ngreat progress achieved in 3D reconstruction from distortion-free line\ndrawings, such as CAD and edge maps, little effort has been made to reconstruct\n3D shapes from free-hand sketches. We study this task and aim to enhance the\npower of sketches in 3D-related applications such as interactive design and\nVR/AR games.\n</p>\n<p>Unlike previous works, which mostly study distortion-free line drawings, our\n3D shape reconstruction is based on free-hand sketches. A major challenge for\nfree-hand sketch 3D reconstruction comes from the insufficient training data\nand free-hand sketch diversity, e.g. individualized sketching styles. We thus\npropose data generation and standardization mechanisms. Instead of\ndistortion-free line drawings, synthesized sketches are adopted as input\ntraining data. Additionally, we propose a sketch standardization module to\nhandle different sketch distortions and styles. Extensive experiments\ndemonstrate the effectiveness of our model and its strong generalizability to\nvarious free-hand sketches. Our code is publicly available at\nhttps://github.com/samaonline/3D-Shape-Reconstruction-from-Free-Hand-Sketches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiayun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jierui Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1\">Qian Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Runtao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yubei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Stella X. Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards a Plug-and-Play Fully AutomatedUnsupervised 360-Degree Deep Learning VisualDefect Detection System. (arXiv:2012.06737v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.06737","description":"<p>Visual defect detection is critical to ensure the quality of most products.\nHowever, majority of small medium manufactures still rely on tedious and\nerror-prune human manual inspection. The main reasons include: 1) the existing\nautomated visual defect detection systems require altering production assembly\nlines, which is time consuming and expensive 2) the existing systems require\nmanually collecting defective samples and labeling them for a comparison-based\nalgorithm or training a machine learning model. This introduces heavy burden\nfor Small and Medium-sized Enterprise (SME) manufactures as defects do not\nhappen often and are difficult and time-consuming to collect. Furthermore, we\ncannot exhaustively collect or define all defect types as any new deviation\nfrom acceptable products are defects. In this paper, we overcome these\nchallenges and design a three-stage plug-and-play fully automated unsupervised\n360-degree defect detection system. In our system, products are freely placed\non an unaltered assembly line and receive 360 degree visual inspection with\nmultiple cameras from different angles. As such, the images collected from\nreal-world product assembly lines contain lots of background noise. The\nproducts face different angles. The product sizes vary due to the distance to\ncameras. All these make defect detection much more difficult. Our system use\nobject detection, background subtraction and unsupervised normalizing\nflow-based defect detection techniques to tackle these difficulty. Experiments\nshow our system can achieve 0.90 AUROC in a real-world non-altered drink ware\nproduction assembly line.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kuang_Z/0/1/0/all/0/1\">Zijian Kuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tie_X/0/1/0/all/0/1\">Xinran Tie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ying_L/0/1/0/all/0/1\">Lihang Ying</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_S/0/1/0/all/0/1\">Shi Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformers in Vision: A Survey. (arXiv:2101.01169v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.01169","description":"<p>Astounding results from Transformer models on natural language tasks have\nintrigued the vision community to study their application to computer vision\nproblems. Among their salient benefits, Transformers enable modeling long\ndependencies between input sequence elements and support parallel processing of\nsequence as compared to recurrent networks e.g., Long short-term memory (LSTM).\nDifferent from convolutional networks, Transformers require minimal inductive\nbiases for their design and are naturally suited as set-functions. Furthermore,\nthe straightforward design of Transformers allows processing multiple\nmodalities (e.g., images, videos, text and speech) using similar processing\nblocks and demonstrates excellent scalability to very large capacity networks\nand huge datasets. These strengths have led to exciting progress on a number of\nvision tasks using Transformer networks. This survey aims to provide a\ncomprehensive overview of the Transformer models in the computer vision\ndiscipline. We start with an introduction to fundamental concepts behind the\nsuccess of Transformers i.e., self-attention, large-scale pre-training, and\nbidirectional encoding. We then cover extensive applications of transformers in\nvision including popular recognition tasks (e.g., image classification, object\ndetection, action recognition, and segmentation), generative modeling,\nmulti-modal tasks (e.g., visual-question answering, visual reasoning, and\nvisual grounding), video processing (e.g., activity recognition, video\nforecasting), low-level vision (e.g., image super-resolution, image\nenhancement, and colorization) and 3D analysis (e.g., point cloud\nclassification and segmentation). We compare the respective advantages and\nlimitations of popular techniques both in terms of architectural design and\ntheir experimental value. Finally, we provide an analysis on open research\ndirections and possible future works.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1\">Salman Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naseer_M/0/1/0/all/0/1\">Muzammal Naseer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayat_M/0/1/0/all/0/1\">Munawar Hayat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zamir_S/0/1/0/all/0/1\">Syed Waqas Zamir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1\">Fahad Shahbaz Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1\">Mubarak Shah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image Splicing Detection, Localization and Attribution via JPEG Primary Quantization Matrix Estimation and Clustering. (arXiv:2102.01439v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2102.01439","description":"<p>Detection of inconsistencies of double JPEG artefacts across different image\nregions is often used to detect local image manipulations, like image splicing,\nand to localize them. In this paper, we move one step further, proposing an\nend-to-end system that, in addition to detecting and localizing spliced\nregions, can also distinguish regions coming from different donor images. We\nassume that both the spliced regions and the background image have undergone a\ndouble JPEG compression, and use a local estimate of the primary quantization\nmatrix to distinguish between spliced regions taken from different sources. To\ndo so, we cluster the image blocks according to the estimated primary\nquantization matrix and refine the result by means of morphological\nreconstruction. The proposed method can work in a wide variety of settings\nincluding aligned and non-aligned double JPEG compression, and regardless of\nwhether the second compression is stronger or weaker than the first one. We\nvalidated the proposed approach by means of extensive experiments showing its\nsuperior performance with respect to baseline methods working in similar\nconditions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Niu_Y/0/1/0/all/0/1\">Yakun Niu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tondi_B/0/1/0/all/0/1\">Benedetta Tondi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_Y/0/1/0/all/0/1\">Yao Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ni_R/0/1/0/all/0/1\">Rongrong Ni</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Barni_M/0/1/0/all/0/1\">Mauro Barni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Wavelength-based Attributed Deep Neural Network for Underwater Image Restoration. (arXiv:2106.07910v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2106.07910","description":"<p>Background: Underwater images, in general, suffer from low contrast and high\ncolor distortions due to the non-uniform attenuation of the light as it\npropagates through the water. In addition, the degree of attenuation varies\nwith the wavelength resulting in the asymmetric traversing of colors. Despite\nthe prolific works for underwater image restoration (UIR) using deep learning,\nthe above asymmetricity has not been addressed in the respective network\nengineering.\n</p>\n<p>Contributions: As the first novelty, this paper shows that attributing the\nright receptive field size (context) based on the traversing range of the color\nchannel may lead to a substantial performance gain for the task of UIR.\nFurther, it is important to suppress the irrelevant multi-contextual features\nand increase the representational power of the model. Therefore, as a second\nnovelty, we have incorporated an attentive skip mechanism to adaptively refine\nthe learned multi-contextual features. The proposed framework, called Deep\nWaveNet, is optimized using the traditional pixel-wise and feature-based cost\nfunctions. An extensive set of experiments have been carried out to show the\nefficacy of the proposed scheme over existing best-published literature on\nbenchmark datasets. More importantly, we have demonstrated a comprehensive\nvalidation of enhanced images across various high-level vision tasks, e.g.,\nunderwater image semantic segmentation, and diver's 2D pose estimation. A\nsample video to exhibit our real-world performance is available at\n\\url{https://tinyurl.com/yzcrup9n}. Also, we have open-sourced our framework at\n\\url{https://github.com/pksvision/Deep-WaveNet-UnderwaterImage-Restoration}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Sharma_P/0/1/0/all/0/1\">Prasen Kumar Sharma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bisht_I/0/1/0/all/0/1\">Ira Bisht</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sur_A/0/1/0/all/0/1\">Arijit Sur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Rule to Multi-Adapt: Generalized Multi-source Feature Learning Using Unsupervised Domain Adaptation for Colorectal Cancer Tissue Detection. (arXiv:2108.09178v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.09178","description":"<p>Supervised learning is constrained by the availability of labeled data, which\nare especially expensive to acquire in the field of digital pathology. Making\nuse of open-source data for pre-training or using domain adaptation can be a\nway to overcome this issue. However, pre-trained networks often fail to\ngeneralize to new test domains that are not distributed identically due to\ntissue stainings, types, and textures variations. Additionally, current domain\nadaptation methods mainly rely on fully-labeled source datasets. In this work,\nwe propose Self-Rule to Multi-Adapt (SRMA), which takes advantage of\nself-supervised learning to perform domain adaptation, and removes the\nnecessity of fully-labeled source datasets. SRMA can effectively transfer the\ndiscriminative knowledge obtained from a few labeled source domain's data to a\nnew target domain without requiring additional tissue annotations. Our method\nharnesses both domains' structures by capturing visual similarity with\nintra-domain and cross-domain self-supervision. Moreover, we present a\ngeneralized formulation of our approach that allows the framework to learn from\nmultiple source domains. We show that our proposed method outperforms baselines\nfor domain adaptation of colorectal tissue type classification \\new{in single\nand multi-source settings}, and further validate our approach on an in-house\nclinical cohort. The code and trained models are available open-source:\nhttps://github.com/christianabbet/SRA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abbet_C/0/1/0/all/0/1\">Christian Abbet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Studer_L/0/1/0/all/0/1\">Linda Studer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fischer_A/0/1/0/all/0/1\">Andreas Fischer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dawson_H/0/1/0/all/0/1\">Heather Dawson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zlobec_I/0/1/0/all/0/1\">Inti Zlobec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bozorgtabar_B/0/1/0/all/0/1\">Behzad Bozorgtabar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thiran_J/0/1/0/all/0/1\">Jean-Philippe Thiran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reconstructing Cosmic Polarization Rotation with ResUNet-CMB. (arXiv:2109.09715v2 [astro-ph.CO] UPDATED)","link":"http://arxiv.org/abs/2109.09715","description":"<p>Cosmic polarization rotation, which may result from parity-violating new\nphysics or the presence of primordial magnetic fields, converts $E$-mode\npolarization of the cosmic microwave background (CMB) into $B$-mode\npolarization. Anisotropic cosmic polarization rotation leads to statistical\nanisotropy in CMB polarization and can be reconstructed with quadratic\nestimator techniques similar to those designed for gravitational lensing of the\nCMB. At the sensitivity of upcoming CMB surveys, lensing-induced $B$-mode\npolarization will act as a limiting factor in the search for anisotropic cosmic\npolarization rotation, meaning that an analysis which incorporates some form of\ndelensing will be required to improve constraints on the effect with future\nsurveys. In this paper we extend the ResUNet-CMB convolutional neural network\nto reconstruct anisotropic cosmic polarization rotation in the presence of\ngravitational lensing and patchy reionization, and we show that the network\nsimultaneously reconstructs all three effects with variance that is lower than\nthat from the standard quadratic estimator nearly matching the performance of\nan iterative reconstruction method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/astro-ph/1/au:+Guzman_E/0/1/0/all/0/1\">Eric Guzman</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Meyers_J/0/1/0/all/0/1\">Joel Meyers</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Map Update Using Dashcam Videos. (arXiv:2109.12131v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.12131","description":"<p>Autonomous driving requires 3D maps that provide accurate and up-to-date\ninformation about semantic landmarks. Due to the wider availability and lower\ncost of cameras compared with laser scanners, vision-based mapping solutions,\nespecially the ones using crowdsourced visual data, have attracted much\nattention from academia and industry. However, previous works have mainly\nfocused on creating 3D point clouds, leaving automatic change detection as open\nissues. We propose in this paper a pipeline for initiating and updating 3D maps\nwith dashcam videos, with a focus on automatic change detection based on\ncomparison of metadata (e.g., the types and locations of traffic signs). To\nimprove the performance of metadata generation, which depends on the accuracy\nof 3D object detection and localization, we introduce a novel deep\nlearning-based pixel-wise 3D localization algorithm. The algorithm, trained\ndirectly with SfM point cloud data, can locate objects detected from 2D images\nin a 3D space with high accuracy by estimating not only depth from monocular\nimages but also lateral and height distances. In addition, we also propose a\npoint clustering and thresholding algorithm to improve the robustness of the\nsystem to errors. We have performed experiments on two distinct areas - a\ncampus and a residential area - with different types of cameras, lighting, and\nweather conditions. The changes were detected with 85% and 100% accuracy in the\ncampus and residential areas, respectively. The errors in the campus area were\nmainly due to traffic signs seen from a far distance to the vehicle and\nintended for pedestrians and cyclists only. We also conducted cause analysis of\nthe detection and localization errors to measure the impact from the\nperformance of the background technology in use.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhanabatyrova_A/0/1/0/all/0/1\">Aziza Zhanabatyrova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leite_C/0/1/0/all/0/1\">Clayton Souza Leite</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yu Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generation of microbial colonies dataset with deep learning style transfer. (arXiv:2111.03789v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.03789","description":"<p>We introduce an effective strategy to generate an annotated synthetic dataset\nof microbiological images of Petri dishes that can be used to train deep\nlearning models in a fully supervised fashion. The developed generator employs\ntraditional computer vision algorithms together with a neural style transfer\nmethod for data augmentation. We show that the method is able to synthesize a\ndataset of realistic looking images that can be used to train a neural network\nmodel capable of localising, segmenting, and classifying five different\nmicrobial species. Our method requires significantly fewer resources to obtain\na useful dataset than collecting and labeling a whole large set of real images\nwith annotations. We show that starting with only 100 real images, we can\ngenerate data to train a detector that achieves comparable results (detection\nmAP = 0.416, and counting MAE = 4.49) to the same detector but trained on a\nreal, several dozen times bigger dataset (mAP = 0.520, MAE = 4.31), containing\nover 7k images. We prove the usefulness of the method in microbe detection and\nsegmentation, but we expect that it is general and flexible and can also be\napplicable in other domains of science and industry to detect various objects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pawlowski_J/0/1/0/all/0/1\">Jaros&#x142;aw Paw&#x142;owski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majchrowska_S/0/1/0/all/0/1\">Sylwia Majchrowska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golan_T/0/1/0/all/0/1\">Tomasz Golan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Medical Visual Question Answering: A Survey. (arXiv:2111.10056v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.10056","description":"<p>Medical Visual Question Answering~(VQA) is a combination of medical\nartificial intelligence and popular VQA challenges. Given a medical image and a\nclinically relevant question in natural language, the medical VQA system is\nexpected to predict a plausible and convincing answer. Although the\ngeneral-domain VQA has been extensively studied, the medical VQA still needs\nspecific investigation and exploration due to its task features. In the first\npart of this survey, we collect and discuss the publicly available medical VQA\ndatasets up to date about the data source, data quantity, and task feature. In\nthe second part, we review the approaches used in medical VQA tasks. We\nsummarize and discuss their techniques, innovation, and potential improvement.\nIn the last part, we analyze some medical-specific challenges for the field and\ndiscuss future research directions. Our goal is to provide comprehensive\ninformation for researchers interested in medical artificial intelligence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhihong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Donghao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tac_Q/0/1/0/all/0/1\">Qingyi Tac</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_D/0/1/0/all/0/1\">Danli Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haffari_G/0/1/0/all/0/1\">Gholamreza Haffari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_M/0/1/0/all/0/1\">Mingguang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Z/0/1/0/all/0/1\">Zongyuan Ge</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extrapolating from a Single Image to a Thousand Classes using Distillation. (arXiv:2112.00725v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.00725","description":"<p>What can neural networks learn about the visual world from a single image?\nWhile it obviously cannot contain the multitudes of possible objects, scenes\nand lighting conditions that exist - within the space of all possible\n256^(3x224x224) 224-sized square images, it might still provide a strong prior\nfor natural images. To analyze this hypothesis, we develop a framework for\ntraining neural networks from scratch using a single image by means of\nknowledge distillation from a supervisedly pretrained teacher. With this, we\nfind that the answer to the above question is: 'surprisingly, a lot'. In\nquantitative terms, we find top-1 accuracies of 94%/74% on CIFAR-10/100, 59% on\nImageNet, and by extending this method to video and audio, 77% on UCF-101 and\n84% on SpeechCommands. In extensive analyses we disentangle the effect of\naugmentations, choice of source image and network architectures and also\ndiscover \"panda neurons\" in networks that have never seen a panda. This work\nshows that one image can be used to extrapolate to thousands of object classes\nand motivates a renewed research agenda on the fundamental interplay of\naugmentations and image.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Asano_Y/0/1/0/all/0/1\">Yuki M. Asano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saeed_A/0/1/0/all/0/1\">Aaqib Saeed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Overcoming the Domain Gap in Neural Action Representations. (arXiv:2112.01176v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.01176","description":"<p>Relating animal behaviors to brain activity is a fundamental goal in\nneuroscience, with practical applications in building robust brain-machine\ninterfaces. However, the domain gap between individuals is a major issue that\nprevents the training of general models that work on unlabeled subjects.\n</p>\n<p>Since 3D pose data can now be reliably extracted from multi-view video\nsequences without manual intervention, we propose to use it to guide the\nencoding of neural action representations together with a set of neural and\nbehavioral augmentations exploiting the properties of microscopy imaging. To\nreduce the domain gap, during training, we swap neural and behavioral data\nacross animals that seem to be performing similar actions.\n</p>\n<p>To demonstrate this, we test our methods on three very different multimodal\ndatasets; one that features flies and their neural activity, one that contains\nhuman neural Electrocorticography (ECoG) data, and lastly the RGB video data of\nhuman activities from different viewpoints.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gunel_S/0/1/0/all/0/1\">Semih G&#xfc;nel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aymanns_F/0/1/0/all/0/1\">Florian Aymanns</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Honari_S/0/1/0/all/0/1\">Sina Honari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramdya_P/0/1/0/all/0/1\">Pavan Ramdya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fua_P/0/1/0/all/0/1\">Pascal Fua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Active learning with MaskAL reduces annotation effort for training Mask R-CNN. (arXiv:2112.06586v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.06586","description":"<p>The generalisation performance of a convolutional neural network (CNN) is\ninfluenced by the quantity, quality, and variety of the training images.\nTraining images must be annotated, and this is time consuming and expensive.\nThe goal of our work was to reduce the number of annotated images needed to\ntrain a CNN while maintaining its performance. We hypothesised that the\nperformance of a CNN can be improved faster by ensuring that the set of\ntraining images contains a large fraction of hard-to-classify images. The\nobjective of our study was to test this hypothesis with an active learning\nmethod that can automatically select the hard-to-classify images. We developed\nan active learning method for Mask Region-based CNN (Mask R-CNN) and named this\nmethod MaskAL. MaskAL involved the iterative training of Mask R-CNN, after\nwhich the trained model was used to select a set of unlabelled images about\nwhich the model was uncertain. The selected images were then annotated and used\nto retrain Mask R-CNN, and this was repeated for a number of sampling\niterations. In our study, Mask R-CNN was trained on 2500 broccoli images that\nwere selected through 12 sampling iterations by either MaskAL or a random\nsampling method from a training set of 14,000 broccoli images. For all sampling\niterations, MaskAL performed significantly better than the random sampling.\nFurthermore, MaskAL had the same performance after sampling 900 images as the\nrandom sampling had after 2300 images. Compared to a Mask R-CNN model that was\ntrained on the entire training set (14,000 images), MaskAL achieved 93.9% of\nits performance with 17.9% of its training data. The random sampling achieved\n81.9% of its performance with 16.4% of its training data. We conclude that by\nusing MaskAL, the annotation effort can be reduced for training Mask R-CNN on a\nbroccoli dataset. Our software is available on\nhttps://github.com/pieterblok/maskal.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Blok_P/0/1/0/all/0/1\">Pieter M. Blok</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kootstra_G/0/1/0/all/0/1\">Gert Kootstra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elghor_H/0/1/0/all/0/1\">Hakim Elchaoui Elghor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diallo_B/0/1/0/all/0/1\">Boubacar Diallo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Evert_F/0/1/0/all/0/1\">Frits K. van Evert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henten_E/0/1/0/all/0/1\">Eldert J. van Henten</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"When less is more: Simplifying inputs aids neural network understanding. (arXiv:2201.05610v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2201.05610","description":"<p>How do neural network image classifiers respond to simpler and simpler\ninputs? And what do such responses reveal about the learning process? To answer\nthese questions, we need a clear measure of input simplicity (or inversely,\ncomplexity), an optimization objective that correlates with simplification, and\na framework to incorporate such objective into training and inference. Lastly\nwe need a variety of testbeds to experiment and evaluate the impact of such\nsimplification on learning. In this work, we measure simplicity with the\nencoding bit size given by a pretrained generative model, and minimize the bit\nsize to simplify inputs in training and inference. We investigate the effect of\nsuch simplification in several scenarios: conventional training, dataset\ncondensation and post-hoc explanations. In all settings, inputs are simplified\nalong with the original classification task, and we investigate the trade-off\nbetween input simplicity and task performance. For images with injected\ndistractors, such simplification naturally removes superfluous information. For\ndataset condensation, we find that inputs can be simplified with almost no\naccuracy degradation. When used in post-hoc explanation, our learning-based\nsimplification approach offers a valuable new tool to explore the basis of\nnetwork decisions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schirrmeister_R/0/1/0/all/0/1\">Robin Tibor Schirrmeister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Rosanne Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hooker_S/0/1/0/all/0/1\">Sara Hooker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ball_T/0/1/0/all/0/1\">Tonio Ball</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GradTail: Learning Long-Tailed Data Using Gradient-based Sample Weighting. (arXiv:2201.05938v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2201.05938","description":"<p>We propose GradTail, an algorithm that uses gradients to improve model\nperformance on the fly in the face of long-tailed training data distributions.\nUnlike conventional long-tail classifiers which operate on converged - and\npossibly overfit - models, we demonstrate that an approach based on gradient\ndot product agreement can isolate long-tailed data early on during model\ntraining and improve performance by dynamically picking higher sample weights\nfor that data. We show that such upweighting leads to model improvements for\nboth classification and regression models, the latter of which are relatively\nunexplored in the long-tail literature, and that the long-tail examples found\nby gradient alignment are consistent with our semantic expectations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Casser_V/0/1/0/all/0/1\">Vincent Casser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kretzschmar_H/0/1/0/all/0/1\">Henrik Kretzschmar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anguelov_D/0/1/0/all/0/1\">Dragomir Anguelov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"H&E-adversarial network: a convolutional neural network to learn stain-invariant features through Hematoxylin & Eosin regression. (arXiv:2201.06329v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2201.06329","description":"<p>Computational pathology is a domain that aims to develop algorithms to\nautomatically analyze large digitized histopathology images, called whole slide\nimages (WSI). WSIs are produced scanning thin tissue samples that are stained\nto make specific structures visible. They show stain colour heterogeneity due\nto different preparation and scanning settings applied across medical centers.\nStain colour heterogeneity is a problem to train convolutional neural networks\n(CNN), the state-of-the-art algorithms for most computational pathology tasks,\nsince CNNs usually underperform when tested on images including different stain\nvariations than those within data used to train the CNN. Despite several\nmethods that were developed, stain colour heterogeneity is still an unsolved\nchallenge that limits the development of CNNs that can generalize on data from\nseveral medical centers. This paper aims to present a novel method to train\nCNNs that better generalize on data including several colour variations. The\nmethod, called H&amp;E-adversarial CNN, exploits H&amp;E matrix information to learn\nstain-invariant features during the training. The method is evaluated on the\nclassification of colon and prostate histopathology images, involving eleven\nheterogeneous datasets, and compared with five other techniques used to handle\nstain colour heterogeneity. H&amp;E-adversarial CNNs show an improvement in\nperformance compared to the other algorithms, demonstrating that it can help to\nbetter deal with stain colour heterogeneous images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Marini_N/0/1/0/all/0/1\">Niccol&#xf3; Marini</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Atzori_M/0/1/0/all/0/1\">Manfredo Atzori</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Otalora_S/0/1/0/all/0/1\">Sebastian Ot&#xe1;lora</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Marchand_Maillet_S/0/1/0/all/0/1\">Stephane Marchand-Maillet</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Muller_H/0/1/0/all/0/1\">Henning M&#xfc;ller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"STURE: Spatial-Temporal Mutual Representation Learning for Robust Data Association in Online Multi-Object Tracking. (arXiv:2201.06824v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.06824","description":"<p>Online multi-object tracking (MOT) is a longstanding task for computer vision\nand intelligent vehicle platform. At present, the main paradigm is\ntracking-by-detection, and the main difficulty of this paradigm is how to\nassociate the current candidate detection with the historical tracklets.\nHowever, in the MOT scenarios, each historical tracklet is composed of an\nobject sequence, while each candidate detection is just a flat image, which\nlacks the temporal features of the object sequence. The feature difference\nbetween current candidate detection and historical tracklets makes the object\nassociation much harder. Therefore, we propose a Spatial-Temporal Mutual\n{Representation} Learning (STURE) approach which learns spatial-temporal\nrepresentations between current candidate detection and historical sequence in\na mutual representation space. For the historical trackelets, the detection\nlearning network is forced to match the representations of sequence learning\nnetwork in a mutual representation space. The proposed approach is capable of\nextracting more distinguishing detection and sequence representations by using\nvarious designed losses in object association. As a result, spatial-temporal\nfeature is learned mutually to reinforce the current detection features, and\nthe feature difference can be relieved. To prove the robustness of the STURE,\nit is applied to the public MOT challenge benchmarks and performs well compared\nwith various state-of-the-art online MOT trackers based on identity-preserving\nmetrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haidong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhiyong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yaping Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nai_K/0/1/0/all/0/1\">Ke Nai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_M/0/1/0/all/0/1\">Ming Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RePre: Improving Self-Supervised Vision Transformer with Reconstructive Pre-training. (arXiv:2201.06857v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.06857","description":"<p>Recently, self-supervised vision transformers have attracted unprecedented\nattention for their impressive representation learning ability. However, the\ndominant method, contrastive learning, mainly relies on an instance\ndiscrimination pretext task, which learns a global understanding of the image.\nThis paper incorporates local feature learning into self-supervised vision\ntransformers via Reconstructive Pre-training (RePre). Our RePre extends\ncontrastive frameworks by adding a branch for reconstructing raw image pixels\nin parallel with the existing contrastive objective. RePre is equipped with a\nlightweight convolution-based decoder that fuses the multi-hierarchy features\nfrom the transformer encoder. The multi-hierarchy features provide rich\nsupervisions from low to high semantic information, which are crucial for our\nRePre. Our RePre brings decent improvements on various contrastive frameworks\nwith different vision transformer architectures. Transfer performance in\ndownstream tasks outperforms supervised pre-training and state-of-the-art\n(SOTA) self-supervised counterparts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Luya Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_F/0/1/0/all/0/1\">Feng Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yangguang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Honggang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1\">Wanli Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1\">Jing Shao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attentional Feature Refinement and Alignment Network for Aircraft Detection in SAR Imagery. (arXiv:2201.07124v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.07124","description":"<p>Aircraft detection in Synthetic Aperture Radar (SAR) imagery is a challenging\ntask in SAR Automatic Target Recognition (SAR ATR) areas due to aircraft's\nextremely discrete appearance, obvious intraclass variation, small size and\nserious background's interference. In this paper, a single-shot detector namely\nAttentional Feature Refinement and Alignment Network (AFRAN) is proposed for\ndetecting aircraft in SAR images with competitive accuracy and speed.\nSpecifically, three significant components including Attention Feature Fusion\nModule (AFFM), Deformable Lateral Connection Module (DLCM) and Anchor-guided\nDetection Module (ADM), are carefully designed in our method for refining and\naligning informative characteristics of aircraft. To represent characteristics\nof aircraft with less interference, low-level textural and high-level semantic\nfeatures of aircraft are fused and refined in AFFM throughly. The alignment\nbetween aircraft's discrete back-scatting points and convolutional sampling\nspots is promoted in DLCM. Eventually, the locations of aircraft are predicted\nprecisely in ADM based on aligned features revised by refined anchors. To\nevaluate the performance of our method, a self-built SAR aircraft sliced\ndataset and a large scene SAR image are collected. Extensive quantitative and\nqualitative experiments with detailed analysis illustrate the effectiveness of\nthe three proposed components. Furthermore, the topmost detection accuracy and\ncompetitive speed are achieved by our method compared with other\ndomain-specific,e.g., DAPN, PADN, and general CNN-based methods,e.g., FPN,\nCascade R-CNN, SSD, RefineDet and RPDet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Lingjun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1\">Dewen Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuang_G/0/1/0/all/0/1\">Gangyao Kuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Li Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-01-19T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}}]}]}