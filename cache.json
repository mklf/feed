{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-12-30T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Can Social Ontological Knowledge Representations be Measured Using Machine Learning?. (arXiv:2112.13870v1 [cs.CL])","link":"http://arxiv.org/abs/2112.13870","description":"<p>Personal Social Ontology (PSO), it is proposed, is how an individual\nperceives the ontological properties of terms. For example, an absolute\nfatalist would arguably use terms that remove any form of agency from a person.\nSuch fatalism has the impact of ontologically defining acts such as winning,\nvictory and success, for example, in a manner that is contrary to how a\nnon-fatalist would ontologically define them. While both a fatalist and\nnon-fatalist would agree on the dictionary definition of these terms, they\nwould differ on what and how they can be caused. This difference between the\ntwo individuals, it is argued, can be induced from the co-occurrence of terms\nused by each individual. That such co-occurrence carries an implied social\nontology, one that is specific to that person. The use of principal social\nperceptions -as evidenced by the social psychology and social neuroscience\nliterature, is put forward as a viable method to feature engineer such texts.\nWith the natural language characterisation of these features, they are then\nusable in machine learning pipelines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Izzidien_A/0/1/0/all/0/1\">Ahmed Izzidien</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Does CLIP Benefit Visual Question Answering in the Medical Domain as Much as it Does in the General Domain?. (arXiv:2112.13906v1 [cs.CV])","link":"http://arxiv.org/abs/2112.13906","description":"<p>Contrastive Language--Image Pre-training (CLIP) has shown remarkable success\nin learning with cross-modal supervision from extensive amounts of image--text\npairs collected online. Thus far, the effectiveness of CLIP has been\ninvestigated primarily in general-domain multimodal problems. This work\nevaluates the effectiveness of CLIP for the task of Medical Visual Question\nAnswering (MedVQA). To this end, we present PubMedCLIP, a fine-tuned version of\nCLIP for the medical domain based on PubMed articles. Our experiments are\nconducted on two MedVQA benchmark datasets and investigate two MedVQA methods,\nMEVF (Mixture of Enhanced Visual Features) and QCR (Question answering via\nConditional Reasoning). For each of these, we assess the merits of visual\nrepresentation learning using PubMedCLIP, the original CLIP, and\nstate-of-the-art MAML (Model-Agnostic Meta-Learning) networks pre-trained only\non visual data. We open source the code for our MedVQA pipeline and\npre-training PubMedCLIP. CLIP and PubMedCLIP achieve improvements in comparison\nto MAML's visual encoder. PubMedCLIP achieves the best results with gains in\nthe overall accuracy of up to 3%. Individual examples illustrate the strengths\nof PubMedCLIP in comparison to the previously widely used MAML networks. Visual\nrepresentation learning with language supervision in PubMedCLIP leads to\nnoticeable improvements for MedVQA. Our experiments reveal distributional\ndifferences in the two MedVQA benchmark datasets that have not been imparted in\nprevious work and cause different back-end visual encoders in PubMedCLIP to\nexhibit different behavior on these datasets. Moreover, we witness fundamental\nperformance differences of VQA in general versus medical domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Eslami_S/0/1/0/all/0/1\">Sedigheh Eslami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melo_G/0/1/0/all/0/1\">Gerard de Melo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meinel_C/0/1/0/all/0/1\">Christoph Meinel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Persuasion in COVID-19 Social Media Content: A Multi-Modal Characterization. (arXiv:2112.13910v1 [cs.CL])","link":"http://arxiv.org/abs/2112.13910","description":"<p>Social media content routinely incorporates multi-modal design to covey\ninformation and shape meanings, and sway interpretations toward desirable\nimplications, but the choices and outcomes of using both texts and visual\nimages have not been sufficiently studied. This work proposes a computational\napproach to analyze the outcome of persuasive information in multi-modal\ncontent, focusing on two aspects, popularity and reliability, in\nCOVID-19-related news articles shared on Twitter. The two aspects are\nintertwined in the spread of misinformation: for example, an unreliable article\nthat aims to misinform has to attain some popularity. This work has several\ncontributions. First, we propose a multi-modal (image and text) approach to\neffectively identify popularity and reliability of information sources\nsimultaneously. Second, we identify textual and visual elements that are\npredictive to information popularity and reliability. Third, by modeling\ncross-modal relations and similarity, we are able to uncover how unreliable\narticles construct multi-modal meaning in a distorted, biased fashion. Our work\ndemonstrates how to use multi-modal analysis for understanding influential\ncontent and has implications to social media literacy and engagement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Unal_M/0/1/0/all/0/1\">Mesut Erhan Unal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kovashka_A/0/1/0/all/0/1\">Adriana Kovashka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_W/0/1/0/all/0/1\">Wen-Ting Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yu-Ru Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The University of Texas at Dallas HLTRI's Participation in EPIC-QA: Searching for Entailed Questions Revealing Novel Answer Nuggets. (arXiv:2112.13946v1 [cs.CL])","link":"http://arxiv.org/abs/2112.13946","description":"<p>The Epidemic Question Answering (EPIC-QA) track at the Text Analysis\nConference (TAC) is an evaluation of methodologies for answering ad-hoc\nquestions about the COVID-19 disease. This paper describes our participation in\nboth tasks of EPIC-QA, targeting: (1) Expert QA and (2) Consumer QA. Our\nmethods used a multi-phase neural Information Retrieval (IR) system based on\ncombining BM25, BERT, and T5 as well as the idea of considering entailment\nrelations between the original question and questions automatically generated\nfrom answer candidate sentences. Moreover, because entailment relations were\nalso considered between all generated questions, we were able to re-rank the\nanswer sentences based on the number of novel answer nuggets they contained, as\nindicated by the processing of a question entailment graph. Our system, called\nSEaRching for Entailed QUestions revealing NOVel nuggets of Answers\n(SER4EQUNOVA), produced promising results in both EPIC-QA tasks, excelling in\nthe Expert QA task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Weinzierl_M/0/1/0/all/0/1\">Maxwell Weinzierl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harabagiu_S/0/1/0/all/0/1\">Sanda M. Harabagiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Preordered RNN Layer Boosts Neural Machine Translation in Low Resource Settings. (arXiv:2112.13960v1 [cs.CL])","link":"http://arxiv.org/abs/2112.13960","description":"<p>Neural Machine Translation (NMT) models are strong enough to convey semantic\nand syntactic information from the source language to the target language.\nHowever, these models are suffering from the need for a large amount of data to\nlearn the parameters. As a result, for languages with scarce data, these models\nare at risk of underperforming. We propose to augment attention based neural\nnetwork with reordering information to alleviate the lack of data. This\naugmentation improves the translation quality for both English to Persian and\nPersian to English by up to 6% BLEU absolute over the baseline models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bastan_M/0/1/0/all/0/1\">Mohaddeseh Bastan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khadivi_S/0/1/0/all/0/1\">Shahram Khadivi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LINDA: Unsupervised Learning to Interpolate in Natural Language Processing. (arXiv:2112.13969v1 [cs.CL])","link":"http://arxiv.org/abs/2112.13969","description":"<p>Despite the success of mixup in data augmentation, its applicability to\nnatural language processing (NLP) tasks has been limited due to the discrete\nand variable-length nature of natural languages. Recent studies have thus\nrelied on domain-specific heuristics and manually crafted resources, such as\ndictionaries, in order to apply mixup in NLP. In this paper, we instead propose\nan unsupervised learning approach to text interpolation for the purpose of data\naugmentation, to which we refer as \"Learning to INterpolate for Data\nAugmentation\" (LINDA), that does not require any heuristics nor manually\ncrafted resources but learns to interpolate between any pair of natural\nlanguage sentences over a natural language manifold. After empirically\ndemonstrating the LINDA's interpolation capability, we show that LINDA indeed\nallows us to seamlessly apply mixup in NLP and leads to better generalization\nin text classification both in-domain and out-of-domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yekyung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_S/0/1/0/all/0/1\">Seohyeong Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1\">Kyunghyun Cho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Processing M.A. Castr\\'en's Materials: Multilingual Typed and Handwritten Manuscripts. (arXiv:2112.14153v1 [cs.CL])","link":"http://arxiv.org/abs/2112.14153","description":"<p>The study forms a technical report of various tasks that have been performed\non the materials collected and published by Finnish ethnographer and linguist,\nMatthias Alexander Castr\\'en (1813-1852). The Finno-Ugrian Society is\npublishing Castr\\'en's manuscripts as new critical and digital editions, and at\nthe same time different research groups have also paid attention to these\nmaterials. We discuss the workflows and technical infrastructure used, and\nconsider how datasets that benefit different computational tasks could be\ncreated to further improve the usability of these materials, and also to aid\nthe further processing of similar archived collections. We specifically focus\non the parts of the collections that are processed in a way that improves their\nusability in more technical applications, complementing the earlier work on the\ncultural and linguistic aspects of these materials. Most of these datasets are\nopenly available in Zenodo. The study points to specific areas where further\nresearch is needed, and provides benchmarks for text recognition tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Partanen_N/0/1/0/all/0/1\">Niko Partanen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rueter_J/0/1/0/all/0/1\">Jack Rueter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamalainen_M/0/1/0/all/0/1\">Mika H&#xe4;m&#xe4;l&#xe4;inen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alnajjar_K/0/1/0/all/0/1\">Khalid Alnajjar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Gender Bias in Natural Language Processing. (arXiv:2112.14168v1 [cs.CL])","link":"http://arxiv.org/abs/2112.14168","description":"<p>Language can be used as a means of reproducing and enforcing harmful\nstereotypes and biases and has been analysed as such in numerous research. In\nthis paper, we present a survey of 304 papers on gender bias in natural\nlanguage processing. We analyse definitions of gender and its categories within\nsocial sciences and connect them to formal definitions of gender bias in NLP\nresearch. We survey lexica and datasets applied in research on gender bias and\nthen compare and contrast approaches to detecting and mitigating gender bias.\nWe find that research on gender bias suffers from four core limitations. 1)\nMost research treats gender as a binary variable neglecting its fluidity and\ncontinuity. 2) Most of the work has been conducted in monolingual setups for\nEnglish or other high-resource languages. 3) Despite a myriad of papers on\ngender bias in NLP methods, we find that most of the newly developed algorithms\ndo not test their models for bias and disregard possible ethical considerations\nof their work. 4) Finally, methodologies developed in this line of research are\nfundamentally flawed covering very limited definitions of gender bias and\nlacking evaluation baselines and pipelines. We suggest recommendations towards\novercoming these limitations as a guide for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stanczak_K/0/1/0/all/0/1\">Karolina Stanczak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Augenstein_I/0/1/0/all/0/1\">Isabelle Augenstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Security Analysis Based on Random Geometry Theory for Satellite-Terrestrial-Vehicle Network. (arXiv:2112.14192v1 [cs.IT])","link":"http://arxiv.org/abs/2112.14192","description":"<p>Driven by B5G and 6G technologies, multi-network fusion is an indispensable\ntendency for future communications. In this paper, we focus on and analyze the\n\\emph{security performance} (SP) of the \\emph{satellite-terrestrial downlink\ntransmission} (STDT). Here, the STDT is composed of a satellite network and a\nvehicular network with a legitimate mobile receiver and an mobile eavesdropper\ndistributing. To theoretically analyze the SP of this system from the\nperspective of mobile terminals better, the random geometry theory is adopted,\nwhich assumes that both terrestrial vehicles are distributed stochastically in\none beam of the satellite. Furthermore, based on this theory, the closed-form\nanalytical expressions for two crucial and specific indicators in the STDT are\nderived, respectively, the secrecy outage probability and the ergodic secrecy\ncapacity. Additionally, several related variables restricting the SP of the\nSTDT are discussed, and specific schemes are presented to enhance the SP. Then,\nthe asymptotic property is investigated in the high signal-to-noise ratio\nscenario, and accurate and asymptotic closed-form expressions are given.\nFinally, simulation results show that, under the precondition of guaranteeing\nthe reliability of the STDT, the asymptotic solutions outperform the\ncorresponding accurate results significantly in the effectiveness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xudong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1\">Ye Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_R/0/1/0/all/0/1\">Rugui Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_N/0/1/0/all/0/1\">Nan Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_X/0/1/0/all/0/1\">Xiaoya Zuo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mirror Matching: Document Matching Approach in Seed-driven Document Ranking for Medical Systematic Reviews. (arXiv:2112.14318v1 [cs.IR])","link":"http://arxiv.org/abs/2112.14318","description":"<p>When medical researchers conduct a systematic review (SR), screening studies\nis the most time-consuming process: researchers read several thousands of\nmedical literature and manually label them relevant or irrelevant. Screening\nprioritization (ie., document ranking) is an approach for assisting researchers\nby providing document rankings where relevant documents are ranked higher than\nirrelevant ones. Seed-driven document ranking (SDR) uses a known relevant\ndocument (ie., seed) as a query and generates such rankings. Previous work on\nSDR seeks ways to identify different term weights in a query document and\nutilizes them in a retrieval model to compute ranking scores. Alternatively, we\nformulate the SDR task as finding similar documents to a query document and\nproduce rankings based on similarity scores. We propose a document matching\nmeasure named Mirror Matching, which calculates matching scores between medical\nabstract texts by incorporating common writing patterns, such as background,\nmethod, result, and conclusion in order. We conduct experiments on CLEF 2019\neHealth Task 2 TAR dataset, and the empirical results show this simple approach\nachieves the higher performance than traditional and neural retrieval models on\nAverage Precision and Precision-focused metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Grace E. Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_A/0/1/0/all/0/1\">Aixin Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simple, Interpretable and Stable Method for Detecting Words with Usage Change across Corpora. (arXiv:2112.14330v1 [cs.CL])","link":"http://arxiv.org/abs/2112.14330","description":"<p>The problem of comparing two bodies of text and searching for words that\ndiffer in their usage between them arises often in digital humanities and\ncomputational social science. This is commonly approached by training word\nembeddings on each corpus, aligning the vector spaces, and looking for words\nwhose cosine distance in the aligned space is large. However, these methods\noften require extensive filtering of the vocabulary to perform well, and - as\nwe show in this work - result in unstable, and hence less reliable, results. We\npropose an alternative approach that does not use vector space alignment, and\ninstead considers the neighbors of each word. The method is simple,\ninterpretable and stable. We demonstrate its effectiveness in 9 different\nsetups, considering different corpus splitting criteria (age, gender and\nprofession of tweet authors, time of tweet) and different languages (English,\nFrench and Hebrew).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gonen_H/0/1/0/all/0/1\">Hila Gonen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jawahar_G/0/1/0/all/0/1\">Ganesh Jawahar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seddah_D/0/1/0/all/0/1\">Djam&#xe9; Seddah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fake or Genuine? Contextualised Text Representation for Fake Review Detection. (arXiv:2112.14343v1 [cs.CL])","link":"http://arxiv.org/abs/2112.14343","description":"<p>Online reviews have a significant influence on customers' purchasing\ndecisions for any products or services. However, fake reviews can mislead both\nconsumers and companies. Several models have been developed to detect fake\nreviews using machine learning approaches. Many of these models have some\nlimitations resulting in low accuracy in distinguishing between fake and\ngenuine reviews. These models focused only on linguistic features to detect\nfake reviews and failed to capture the semantic meaning of the reviews. To deal\nwith this, this paper proposes a new ensemble model that employs transformer\narchitecture to discover the hidden patterns in a sequence of fake reviews and\ndetect them precisely. The proposed approach combines three transformer models\nto improve the robustness of fake and genuine behaviour profiling and modelling\nto detect fake reviews. The experimental results using semi-real benchmark\ndatasets showed the superiority of the proposed model over state-of-the-art\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohawesh_R/0/1/0/all/0/1\">Rami Mohawesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shuxiang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Springer_M/0/1/0/all/0/1\">Matthew Springer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Hawawreh_M/0/1/0/all/0/1\">Muna Al-Hawawreh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maqsood_S/0/1/0/all/0/1\">Sumbal Maqsood</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Variational Learning for the Inverted Beta-Liouville Mixture Model and Its Application to Text Categorization. (arXiv:2112.14375v1 [cs.LG])","link":"http://arxiv.org/abs/2112.14375","description":"<p>The finite invert Beta-Liouville mixture model (IBLMM) has recently gained\nsome attention due to its positive data modeling capability. Under the\nconventional variational inference (VI) framework, the analytically tractable\nsolution to the optimization of the variational posterior distribution cannot\nbe obtained, since the variational object function involves evaluation of\nintractable moments. With the recently proposed extended variational inference\n(EVI) framework, a new function is proposed to replace the original variational\nobject function in order to avoid intractable moment computation, so that the\nanalytically tractable solution of the IBLMM can be derived in an elegant way.\nThe good performance of the proposed approach is demonstrated by experiments\nwith both synthesized data and a real-world application namely text\ncategorization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ling_Y/0/1/0/all/0/1\">Yongfa Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_W/0/1/0/all/0/1\">Wenbo Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruan_Q/0/1/0/all/0/1\">Qiang Ruan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1\">Heping Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_Y/0/1/0/all/0/1\">Yuping Lai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Frequency-Aware Contrastive Learning for Neural Machine Translation. (arXiv:2112.14484v1 [cs.CL])","link":"http://arxiv.org/abs/2112.14484","description":"<p>Low-frequency word prediction remains a challenge in modern neural machine\ntranslation (NMT) systems. Recent adaptive training methods promote the output\nof infrequent words by emphasizing their weights in the overall training\nobjectives. Despite the improved recall of low-frequency words, their\nprediction precision is unexpectedly hindered by the adaptive objectives.\nInspired by the observation that low-frequency words form a more compact\nembedding space, we tackle this challenge from a representation learning\nperspective. Specifically, we propose a frequency-aware token-level contrastive\nlearning method, in which the hidden state of each decoding step is pushed away\nfrom the counterparts of other target words, in a soft contrastive way based on\nthe corresponding word frequencies. We conduct experiments on widely used NIST\nChinese-English and WMT14 English-German translation tasks. Empirical results\nshow that our proposed methods can not only significantly improve the\ntranslation quality but also enhance lexical diversity and optimize word\nrepresentation space. Further investigation reveals that, comparing with\nrelated adaptive training strategies, the superiority of our method on\nlow-frequency word prediction lies in the robustness of token-level recall\nacross different frequencies without sacrificing precision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_W/0/1/0/all/0/1\">Wei Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Baosong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Long Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xingzhang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Dayiheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jinan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shikun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haibo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wen Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-Tuning Transformers: Vocabulary Transfer. (arXiv:2112.14569v1 [cs.CL])","link":"http://arxiv.org/abs/2112.14569","description":"<p>Transformers are responsible for the vast majority of recent advances in\nnatural language processing. The majority of practical natural language\nprocessing applications of these models is typically enabled through transfer\nlearning. This paper studies if corpus-specific tokenization used for\nfine-tuning improves the resulting performance of the model. Through a series\nof experiments, we demonstrate that such tokenization combined with the\ninitialization and fine-tuning strategy for the vocabulary tokens speeds up the\ntransfer and boosts the performance of the fine-tuned model. We call this\naspect of transfer facilitation vocabulary transfer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Samenko_I/0/1/0/all/0/1\">Igor Samenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tikhonov_A/0/1/0/all/0/1\">Alexey Tikhonov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kozlovskii_B/0/1/0/all/0/1\">Borislav Kozlovskii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamshchikov_I/0/1/0/all/0/1\">Ivan P. Yamshchikov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LeSICiN: A Heterogeneous Graph-based Approach for Automatic Legal Statute Identification from Indian Legal Documents. (arXiv:2112.14731v1 [cs.CL])","link":"http://arxiv.org/abs/2112.14731","description":"<p>The task of Legal Statute Identification (LSI) aims to identify the legal\nstatutes that are relevant to a given description of Facts or evidence of a\nlegal case. Existing methods only utilize the textual content of Facts and\nlegal articles to guide such a task. However, the citation network among case\ndocuments and legal statutes is a rich source of additional information, which\nis not considered by existing models. In this work, we take the first step\ntowards utilising both the text and the legal citation network for the LSI\ntask. We curate a large novel dataset for this task, including Facts of cases\nfrom several major Indian Courts of Law, and statutes from the Indian Penal\nCode (IPC). Modeling the statutes and training documents as a heterogeneous\ngraph, our proposed model LeSICiN can learn rich textual and graphical\nfeatures, and can also tune itself to correlate these features. Thereafter, the\nmodel can be used to inductively predict links between test documents (new\nnodes whose graphical features are not available to the model) and statutes\n(existing nodes). Extensive experiments on the dataset show that our model\ncomfortably outperforms several state-of-the-art baselines, by exploiting the\ngraphical structure along with textual features. The dataset and our codes are\navailable at https://github.com/Law-AI/LeSICiN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Paul_S/0/1/0/all/0/1\">Shounak Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_P/0/1/0/all/0/1\">Pawan Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Saptarshi Ghosh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On learning an interpreted language with recurrent models. (arXiv:1809.04128v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/1809.04128","description":"<p>Can recurrent neural nets, inspired by human sequential data processing,\nlearn to understand language? We construct simplified datasets reflecting core\nproperties of natural language as modeled in formal syntax and semantics:\nrecursive syntactic structure and compositionality. We find LSTM and GRU\nnetworks to generalise to compositional interpretation well, but only in the\nmost favorable learning settings, with a well-paced curriculum, extensive\ntraining data, and left-to-right (but not right-to-left) composition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Paperno_D/0/1/0/all/0/1\">Denis Paperno</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating Empathetic Responses by Looking Ahead the User's Sentiment. (arXiv:1906.08487v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/1906.08487","description":"<p>An important aspect of human conversation difficult for machines is\nconversing with empathy, which is to understand the user's emotion and respond\nappropriately. Recent neural conversation models that attempted to generate\nempathetic responses either focused on conditioning the output to a given\nemotion, or incorporating the current user emotional state. However, these\napproaches do not factor in how the user would feel towards the generated\nresponse. Hence, in this paper, we propose Sentiment Look-ahead, which is a\nnovel perspective for empathy that models the future user emotional state. In\nshort, Sentiment Look-ahead is a reward function under a reinforcement learning\nframework that provides a higher reward to the generative model when the\ngenerated utterance improves the user's sentiment. We implement and evaluate\nthree different possible implementations of sentiment look-ahead and\nempirically show that our proposed approach can generate significantly more\nempathetic, relevant, and fluent responses than other competitive baselines\nsuch as multitask learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1\">Jamin Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1\">Peng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madotto_A/0/1/0/all/0/1\">Andrea Madotto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1\">Pascale Fung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Bridging for Empathetic Dialogue Generation. (arXiv:2009.09708v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2009.09708","description":"<p>Lack of external knowledge makes empathetic dialogue systems difficult to\nperceive implicit emotions and learn emotional interactions from limited\ndialogue history. To address the above problems, we propose to leverage\nexternal knowledge, including commonsense knowledge and emotional lexical\nknowledge, to explicitly understand and express emotions in empathetic dialogue\ngeneration. We first enrich the dialogue history by jointly interacting with\nexternal knowledge and construct an emotional context graph. Then we learn\nemotional context representations from the knowledge-enriched emotional context\ngraph and distill emotional signals, which are the prerequisites to predicate\nemotions expressed in responses. Finally, to generate the empathetic response,\nwe propose an emotional cross-attention mechanism to learn the emotional\ndependencies from the emotional context graph. Extensive experiments conducted\non a benchmark dataset verify the effectiveness of the proposed method. In\naddition, we find the performance of our method can be further improved by\nintegrating with a pre-trained model that works orthogonally.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qintong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Piji Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1\">Zhaochun Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_P/0/1/0/all/0/1\">Pengjie Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhumin Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unified Streaming and Non-streaming Two-pass End-to-end Model for Speech Recognition. (arXiv:2012.05481v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2012.05481","description":"<p>In this paper, we present a novel two-pass approach to unify streaming and\nnon-streaming end-to-end (E2E) speech recognition in a single model. Our model\nadopts the hybrid CTC/attention architecture, in which the conformer layers in\nthe encoder are modified. We propose a dynamic chunk-based attention strategy\nto allow arbitrary right context length. At inference time, the CTC decoder\ngenerates n-best hypotheses in a streaming way. The inference latency could be\neasily controlled by only changing the chunk size. The CTC hypotheses are then\nrescored by the attention decoder to get the final result. This efficient\nrescoring process causes very little sentence-level latency. Our experiments on\nthe open 170-hour AISHELL-1 dataset show that, the proposed method can unify\nthe streaming and non-streaming model simply and efficiently. On the AISHELL-1\ntest set, our unified model achieves 5.60% relative character error rate (CER)\nreduction in non-streaming ASR compared to a standard non-streaming\ntransformer. The same model achieves 5.42% CER with 640ms latency in a\nstreaming ASR system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Binbin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Di Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1\">Zhuoyuan Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_L/0/1/0/all/0/1\">Liyong Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yaguang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1\">Lei Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_X/0/1/0/all/0/1\">Xin Lei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WeNet: Production oriented Streaming and Non-streaming End-to-End Speech Recognition Toolkit. (arXiv:2102.01547v5 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2102.01547","description":"<p>In this paper, we propose an open source, production first, and production\nready speech recognition toolkit called WeNet in which a new two-pass approach\nis implemented to unify streaming and non-streaming end-to-end (E2E) speech\nrecognition in a single model. The main motivation of WeNet is to close the gap\nbetween the research and the production of E2E speechrecognition models. WeNet\nprovides an efficient way to ship ASR applications in several real-world\nscenarios, which is the main difference and advantage to other open source E2E\nspeech recognition toolkits. In our toolkit, a new two-pass method is\nimplemented. Our method propose a dynamic chunk-based attention strategy of the\nthe transformer layers to allow arbitrary right context length modifies in\nhybrid CTC/attention architecture. The inference latency could be easily\ncontrolled by only changing the chunk size. The CTC hypotheses are then\nrescored by the attention decoder to get the final result. Our experiments on\nthe AISHELL-1 dataset using WeNet show that, our model achieves 5.03\\% relative\ncharacter error rate (CER) reduction in non-streaming ASR compared to a\nstandard non-streaming transformer. After model quantification, our model\nperform reasonable RTF and latency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1\">Zhuoyuan Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Di Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Binbin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1\">Zhendong Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaoyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1\">Lei Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_X/0/1/0/all/0/1\">Xin Lei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extending Multi-Sense Word Embedding to Phrases and Sentences for Unsupervised Semantic Applications. (arXiv:2103.15330v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.15330","description":"<p>Most unsupervised NLP models represent each word with a single point or\nsingle region in semantic space, while the existing multi-sense word embeddings\ncannot represent longer word sequences like phrases or sentences. We propose a\nnovel embedding method for a text sequence (a phrase or a sentence) where each\nsequence is represented by a distinct set of multi-mode codebook embeddings to\ncapture different semantic facets of its meaning. The codebook embeddings can\nbe viewed as the cluster centers which summarize the distribution of possibly\nco-occurring words in a pre-trained word embedding space. We introduce an\nend-to-end trainable neural model that directly predicts the set of cluster\ncenters from the input text sequence during test time. Our experiments show\nthat the per-sentence codebook embeddings significantly improve the\nperformances in unsupervised sentence similarity and extractive summarization\nbenchmarks. In phrase similarity experiments, we discover that the multi-facet\nembeddings provide an interpretable semantic representation but do not\noutperform the single-facet baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Haw-Shiuan Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_A/0/1/0/all/0/1\">Amol Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCallum_A/0/1/0/all/0/1\">Andrew McCallum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Competency Problems: On Finding and Removing Artifacts in Language Data. (arXiv:2104.08646v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08646","description":"<p>Much recent work in NLP has documented dataset artifacts, bias, and spurious\ncorrelations between input features and output labels. However, how to tell\nwhich features have \"spurious\" instead of legitimate correlations is typically\nleft unspecified. In this work we argue that for complex language understanding\ntasks, all simple feature correlations are spurious, and we formalize this\nnotion into a class of problems which we call competency problems. For example,\nthe word \"amazing\" on its own should not give information about a sentiment\nlabel independent of the context in which it appears, which could include\nnegation, metaphor, sarcasm, etc. We theoretically analyze the difficulty of\ncreating data for competency problems when human bias is taken into account,\nshowing that realistic datasets will increasingly deviate from competency\nproblems as dataset size increases. This analysis gives us a simple statistical\ntest for dataset artifacts, which we use to show more subtle biases than were\ndescribed in prior work, including demonstrating that models are\ninappropriately affected by these less extreme biases. Our theoretical\ntreatment of this problem also allows us to analyze proposed solutions, such as\nmaking local edits to dataset instances, and to give recommendations for future\ndata collection and model design efforts that target competency problems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gardner_M/0/1/0/all/0/1\">Matt Gardner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merrill_W/0/1/0/all/0/1\">William Merrill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dodge_J/0/1/0/all/0/1\">Jesse Dodge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peters_M/0/1/0/all/0/1\">Matthew E. Peters</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ross_A/0/1/0/all/0/1\">Alexis Ross</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sameer Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Boosting Search Engines with Interactive Agents. (arXiv:2109.00527v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.00527","description":"<p>This paper presents first successful steps in designing agents that learn\nmeta-strategies for iterative query refinement. Our approach uses machine\nreading to guide the selection of refinement terms from aggregated search\nresults. Agents are then empowered with simple but effective search operators\nto exert fine-grained and transparent control over queries and search results.\nWe develop a novel way of generating synthetic search sessions, which leverages\nthe power of transformer-based language models through (self-)supervised\nlearning. We also present a reinforcement learning agent with dynamically\nconstrained actions that learns interactive search strategies from scratch. We\nobtain retrieval and answer quality performance comparable to recent neural\nmethods using a traditional term-based BM25 ranking function. We provide an\nin-depth analysis of the search policies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Adolphs_L/0/1/0/all/0/1\">Leonard Adolphs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boerschinger_B/0/1/0/all/0/1\">Benjamin Boerschinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buck_C/0/1/0/all/0/1\">Christian Buck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huebscher_M/0/1/0/all/0/1\">Michelle Chen Huebscher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ciaramita_M/0/1/0/all/0/1\">Massimiliano Ciaramita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Espeholt_L/0/1/0/all/0/1\">Lasse Espeholt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hofmann_T/0/1/0/all/0/1\">Thomas Hofmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kilcher_Y/0/1/0/all/0/1\">Yannic Kilcher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rothe_S/0/1/0/all/0/1\">Sascha Rothe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sessa_P/0/1/0/all/0/1\">Pier Giuseppe Sessa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saralegui_L/0/1/0/all/0/1\">Lierni Sestorain Saralegui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hocalarim: Mining Turkish Student Reviews. (arXiv:2109.02325v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.02325","description":"<p>We introduce Hocalarim (MyProfessors), the largest student review dataset\navailable for the Turkish language. It consists of over 5000 professor reviews\nleft online by students, with different aspects of education rated on a scale\nof 1 to 5 stars. We investigate the properties of the dataset and present its\nstatistics. We examine the impact of students' institution type on their\nratings and the correlation of students' bias to give positive or negative\nfeedback.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ceylan_I/0/1/0/all/0/1\">Ibrahim Faruk Ceylan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calik_N/0/1/0/all/0/1\">Necmettin Bera Calik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yapucuoglu_M/0/1/0/all/0/1\">Mert Yapucuoglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uluslu_A/0/1/0/all/0/1\">Ahmet Yavuz Uluslu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Categorical Semantics of Reversible Pattern-Matching. (arXiv:2109.05837v3 [cs.LO] UPDATED)","link":"http://arxiv.org/abs/2109.05837","description":"<p>This paper is concerned with categorical structures for reversible\ncomputation. In particular, we focus on a typed, functional reversible language\nbased on Theseus. We discuss how join inverse rig categories do not in general\ncapture pattern-matching, the core construct Theseus uses to enforce\nreversibility. We then derive a categorical structure to add to join inverse\nrig categories in order to capture pattern-matching. We show how such a\nstructure makes an adequate model for reversible pattern-matching.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chardonnet_K/0/1/0/all/0/1\">Kostia Chardonnet</a> (LMF, Universit&#xe9; Paris Saclay. IRIF, Universit&#xe9; de Paris.), <a href=\"http://arxiv.org/find/cs/1/au:+Lemonnier_L/0/1/0/all/0/1\">Louis Lemonnier</a> (LMF, Universit&#xe9; Paris Saclay.), <a href=\"http://arxiv.org/find/cs/1/au:+Valiron_B/0/1/0/all/0/1\">Beno&#xee;t Valiron</a> (LMF, CentraleSup&#xe9;lec, Universit&#xe9; Paris Saclay)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Variance of Twitter Embeddings and Temporal Trends of COVID-19 cases. (arXiv:2110.00031v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.00031","description":"<p>The severity of the coronavirus pandemic necessitates the need of effective\nadministrative decisions. Over 4 lakh people in India succumbed to COVID-19,\nwith over 3 crore confirmed cases, and still counting. The threat of a\nplausible third wave continues to haunt millions. In this ever changing dynamic\nof the virus, predictive modeling methods can serve as an integral tool. The\npandemic has further triggered an unprecedented usage of social media. This\npaper aims to propose a method for harnessing social media, specifically\nTwitter, to predict the upcoming scenarios related to COVID-19 cases. In this\nstudy, we seek to understand how the surges in COVID-19 related tweets can\nindicate rise in the cases. This prospective analysis can be utilised to aid\nadministrators about timely resource allocation to lessen the severity of the\ndamage. Using word embeddings to capture the semantic meaning of tweets, we\nidentify Significant Dimensions (SDs).Our methodology predicts the rise in\ncases with a lead time of 15 days and 30 days with R2 scores of 0.80 and 0.62\nrespectively. Finally, we explain the thematic utility of the SDs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sethi_M/0/1/0/all/0/1\">Mayank Sethi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadhu_A/0/1/0/all/0/1\">Ambika Sadhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pahwa_K/0/1/0/all/0/1\">Khushbu Pahwa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagpal_S/0/1/0/all/0/1\">Sargun Nagpal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sethi_T/0/1/0/all/0/1\">Tavpritesh Sethi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TEACh: Task-driven Embodied Agents that Chat. (arXiv:2110.00534v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.00534","description":"<p>Robots operating in human spaces must be able to engage in natural language\ninteraction with people, both understanding and executing instructions, and\nusing conversation to resolve ambiguity and recover from mistakes. To study\nthis, we introduce TEACh, a dataset of over 3,000 human--human, interactive\ndialogues to complete household tasks in simulation. A Commander with access to\noracle information about a task communicates in natural language with a\nFollower. The Follower navigates through and interacts with the environment to\ncomplete tasks varying in complexity from \"Make Coffee\" to \"Prepare Breakfast\",\nasking questions and getting additional information from the Commander. We\npropose three benchmarks using TEACh to study embodied intelligence challenges,\nand we evaluate initial models' abilities in dialogue understanding, language\ngrounding, and task execution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Padmakumar_A/0/1/0/all/0/1\">Aishwarya Padmakumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomason_J/0/1/0/all/0/1\">Jesse Thomason</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1\">Ayush Shrivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lange_P/0/1/0/all/0/1\">Patrick Lange</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayan_Chen_A/0/1/0/all/0/1\">Anjali Narayan-Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gella_S/0/1/0/all/0/1\">Spandana Gella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piramuthu_R/0/1/0/all/0/1\">Robinson Piramuthu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tur_G/0/1/0/all/0/1\">Gokhan Tur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hakkani_Tur_D/0/1/0/all/0/1\">Dilek Hakkani-Tur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Proposed Conceptual Framework for a Representational Approach to Information Retrieval. (arXiv:2110.01529v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2110.01529","description":"<p>This paper outlines a conceptual framework for understanding recent\ndevelopments in information retrieval and natural language processing that\nattempts to integrate dense and sparse retrieval methods. I propose a\nrepresentational approach that breaks the core text retrieval problem into a\nlogical scoring model and a physical retrieval model. The scoring model is\ndefined in terms of encoders, which map queries and documents into a\nrepresentational space, and a comparison function that computes query-document\nscores. The physical retrieval model defines how a system produces the top-$k$\nscoring documents from an arbitrarily large corpus with respect to a query. The\nscoring model can be further analyzed along two dimensions: dense vs. sparse\nrepresentations and supervised (learned) vs. unsupervised approaches. I show\nthat many recently proposed retrieval methods, including multi-stage ranking\ndesigns, can be seen as different parameterizations in this framework, and that\na unified view suggests a number of open research questions, providing a\nroadmap for future work. As a bonus, this conceptual framework establishes\nconnections to sentence similarity tasks in natural language processing and\ninformation access \"technologies\" prior to the dawn of computing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jimmy Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WenetSpeech: A 10000+ Hours Multi-domain Mandarin Corpus for Speech Recognition. (arXiv:2110.03370v4 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2110.03370","description":"<p>In this paper, we present WenetSpeech, a multi-domain Mandarin corpus\nconsisting of 10000+ hours high-quality labeled speech, 2400+ hours weakly\nlabeled speech, and about 10000 hours unlabeled speech, with 22400+ hours in\ntotal. We collect the data from YouTube and Podcast, which covers a variety of\nspeaking styles, scenarios, domains, topics, and noisy conditions. An optical\ncharacter recognition (OCR) based method is introduced to generate the\naudio/text segmentation candidates for the YouTube data on its corresponding\nvideo captions, while a high-quality ASR transcription system is used to\ngenerate audio/text pair candidates for the Podcast data. Then we propose a\nnovel end-to-end label error detection approach to further validate and filter\nthe candidates. We also provide three manually labelled high-quality test sets\nalong with WenetSpeech for evaluation -- Dev for cross-validation purpose in\ntraining, Test_Net, collected from Internet for matched test, and\nTest\\_Meeting, recorded from real meetings for more challenging mismatched\ntest. Baseline systems trained with WenetSpeech are provided for three popular\nspeech recognition toolkits, namely Kaldi, ESPnet, and WeNet, and recognition\nresults on the three test sets are also provided as benchmarks. To the best of\nour knowledge, WenetSpeech is the current largest open-sourced Mandarin speech\ncorpus with transcriptions, which benefits research on production-level speech\nrecognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Binbin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_H/0/1/0/all/0/1\">Hang Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_P/0/1/0/all/0/1\">Pengcheng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Q/0/1/0/all/0/1\">Qijie Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1\">Lei Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bu_H/0/1/0/all/0/1\">Hui Bu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaoyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_C/0/1/0/all/0/1\">Chenchen Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Di Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1\">Zhendong Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MotifClass: Weakly Supervised Text Classification with Higher-order Metadata Information. (arXiv:2111.04022v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.04022","description":"<p>We study the problem of weakly supervised text classification, which aims to\nclassify text documents into a set of pre-defined categories with category\nsurface names only and without any annotated training document provided. Most\nexisting classifiers leverage textual information in each document. However, in\nmany domains, documents are accompanied by various types of metadata (e.g.,\nauthors, venue, and year of a research paper). These metadata and their\ncombinations may serve as strong category indicators in addition to textual\ncontents. In this paper, we explore the potential of using metadata to help\nweakly supervised text classification. To be specific, we model the\nrelationships between documents and metadata via a heterogeneous information\nnetwork. To effectively capture higher-order structures in the network, we use\nmotifs to describe metadata combinations. We propose a novel framework, named\nMotifClass, which (1) selects category-indicative motif instances, (2)\nretrieves and generates pseudo-labeled training samples based on category names\nand indicative motif instances, and (3) trains a text classifier using the\npseudo training data. Extensive experiments on real-world datasets demonstrate\nthe superior performance of MotifClass to existing weakly supervised text\nclassification approaches. Further analysis shows the benefit of considering\nhigher-order metadata information in our framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1\">Shweta Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1\">Yu Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiusi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pointer over Attention: An Improved Bangla Text Summarization Approach Using Hybrid Pointer Generator Network. (arXiv:2111.10269v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.10269","description":"<p>Despite the success of the neural sequence-to-sequence model for abstractive\ntext summarization, it has a few shortcomings, such as repeating inaccurate\nfactual details and tending to repeat themselves. We propose a hybrid pointer\ngenerator network to solve the shortcomings of reproducing factual details\ninadequately and phrase repetition. We augment the attention-based\nsequence-to-sequence using a hybrid pointer generator network that can generate\nOut-of-Vocabulary words and enhance accuracy in reproducing authentic details\nand a coverage mechanism that discourages repetition. It produces a\nreasonable-sized output text that preserves the conceptual integrity and\nfactual information of the input article. For evaluation, we primarily employed\n\"BANSData\" - a highly adopted publicly available Bengali dataset. Additionally,\nwe prepared a large-scale dataset called \"BANS-133\" which consists of 133k\nBangla news articles associated with human-generated summaries. Experimenting\nwith the proposed model, we achieved ROUGE-1 and ROUGE-2 scores of 0.66, 0.41\nfor the \"BANSData\" dataset and 0.67, 0.42 for the BANS-133k\" dataset,\nrespectively. We demonstrated that the proposed system surpasses previous\nstate-of-the-art Bengali abstractive summarization techniques and its stability\non a larger dataset. \"BANS-133\" datasets and code-base will be publicly\navailable for research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dhar_N/0/1/0/all/0/1\">Nobel Dhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saha_G/0/1/0/all/0/1\">Gaurob Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharjee_P/0/1/0/all/0/1\">Prithwiraj Bhattacharjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mallick_A/0/1/0/all/0/1\">Avi Mallick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1\">Md Saiful Islam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Polite Emotional Dialogue Acts for Conversational Analysis in Daily Dialog Data. (arXiv:2112.13572v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.13572","description":"<p>Many socio-linguistic cues are used in the conversational analysis, such as\nemotion, sentiment, and dialogue acts. One of the fundamental social cues is\npoliteness, which linguistically possesses properties useful in conversational\nanalysis. This short article presents some of the brief findings of polite\nemotional dialogue acts, where we can correlate the relational bonds between\nthese socio-linguistics cues. We found that the utterances with emotion classes\nAnger and Disgust are more likely to be impolite while Happiness and Sadness to\nbe polite. Similar phenomenon occurs with dialogue acts, Inform and Commissive\ncontain many polite utterances than Question and Directive. Finally, we will\nconclude on the future work of these findings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bothe_C/0/1/0/all/0/1\">Chandrakant Bothe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pedagogical Word Recommendation: A novel task and dataset on personalized vocabulary acquisition for L2 learners. (arXiv:2112.13808v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.13808","description":"<p>When learning a second language (L2), one of the most important but tedious\ncomponents that often demoralizes students with its ineffectiveness and\ninefficiency is vocabulary acquisition, or more simply put, memorizing words.\nIn light of such, a personalized and educational vocabulary recommendation\nsystem that traces a learner's vocabulary knowledge state would have an immense\nlearning impact as it could resolve both issues. Therefore, in this paper, we\npropose and release data for a novel task called Pedagogical Word\nRecommendation (PWR). The main goal of PWR is to predict whether a given\nlearner knows a given word based on other words the learner has already seen.\nTo elaborate, we collect this data via an Intelligent Tutoring System (ITS)\nthat is serviced to ~1M L2 learners who study for the standardized English\nexam, TOEIC. As a feature of this ITS, students can directly indicate words\nthey do not know from the questions they solved to create wordbooks. Finally,\nwe report the evaluation results of a Neural Collaborative Filtering approach\nalong with an exploratory data analysis and discuss the impact and efficacy of\nthis dataset as a baseline for future studies on this task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1\">Jamin Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Juneyoung Park</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-12-29T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","admin":"http://webns.net/mvcb/","syn":"http://purl.org/rss/1.0/modules/syndication/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"BMPQ: Bit-Gradient Sensitivity Driven Mixed-Precision Quantization of DNNs from Scratch. (arXiv:2112.13843v1 [cs.CV])","link":"http://arxiv.org/abs/2112.13843","description":"<p>Large DNNs with mixed-precision quantization can achieve ultra-high\ncompression while retaining high classification performance. However, because\nof the challenges in finding an accurate metric that can guide the optimization\nprocess, these methods either sacrifice significant performance compared to the\n32-bit floating-point (FP-32) baseline or rely on a compute-expensive,\niterative training policy that requires the availability of a pre-trained\nbaseline. To address this issue, this paper presents BMPQ, a training method\nthat uses bit gradients to analyze layer sensitivities and yield\nmixed-precision quantized models. BMPQ requires a single training iteration but\ndoes not need a pre-trained baseline. It uses an integer linear program (ILP)\nto dynamically adjust the precision of layers during training, subject to a\nfixed hardware budget. To evaluate the efficacy of BMPQ, we conduct extensive\nexperiments with VGG16 and ResNet18 on CIFAR-10, CIFAR-100, and Tiny-ImageNet\ndatasets. Compared to the baseline FP-32 models, BMPQ can yield models that\nhave 15.4x fewer parameter bits with a negligible drop in accuracy. Compared to\nthe SOTA \"during training\", mixed-precision training scheme, our models are\n2.1x, 2.2x, and 2.9x smaller, on CIFAR-10, CIFAR-100, and Tiny-ImageNet,\nrespectively, with an improved accuracy of up to 14.54%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kundu_S/0/1/0/all/0/1\">Souvik Kundu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shikai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1\">Qirui Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beerel_P/0/1/0/all/0/1\">Peter A. Beerel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pedram_M/0/1/0/all/0/1\">Massoud Pedram</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Raw Produce Quality Detection with Shifted Window Self-Attention. (arXiv:2112.13845v1 [cs.CV])","link":"http://arxiv.org/abs/2112.13845","description":"<p>Global food insecurity is expected to worsen in the coming decades with the\naccelerated rate of climate change and the rapidly increasing population. In\nthis vein, it is important to remove inefficiencies at every level of food\nproduction. The recent advances in deep learning can help reduce such\ninefficiencies, yet their application has not yet become mainstream throughout\nthe industry, inducing economic costs at a massive scale. To this point, modern\ntechniques such as CNNs (Convolutional Neural Networks) have been applied to\nRPQD (Raw Produce Quality Detection) tasks. On the other hand, Transformer's\nsuccessful debut in the vision among other modalities led us to expect a better\nperformance with these Transformer-based models in RPQD. In this work, we\nexclusively investigate the recent state-of-the-art Swin (Shifted Windows)\nTransformer which computes self-attention in both intra- and inter-window\nfashion. We compare Swin Transformer against CNN models on four RPQD image\ndatasets, each containing different kinds of raw produce: fruits and\nvegetables, fish, pork, and beef. We observe that Swin Transformer not only\nachieves better or competitive performance but also is data- and\ncompute-efficient, making it ideal for actual deployment in real-world setting.\nTo the best of our knowledge, this is the first large-scale empirical study on\nRPQD task, which we hope will gain more attention in future works.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kwon_O/0/1/0/all/0/1\">Oh Joon Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1\">Byungsoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Youngduck Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Algorithm for recognizing the contour of a honeycomb block. (arXiv:2112.13846v1 [cs.CV])","link":"http://arxiv.org/abs/2112.13846","description":"<p>The article discusses an algorithm for recognizing the contour of fragments\nof a honeycomb block. The inapplicability of ready-made functions of the OpenCV\nlibrary is shown. Two proposed algorithms are considered. The direct scanning\nalgorithm finds the extreme white pixels in the binarized image, it works\nadequately on convex shapes of products, but does not find a contour on concave\nareas and in cavities of products. To solve this problem, a scanning algorithm\nusing a sliding matrix is proposed, which works correctly on products of any\nshape.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kubrikov_M/0/1/0/all/0/1\">Maksim Viktorovich Kubrikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saramud_M/0/1/0/all/0/1\">Mikhail Vladimirovich Saramud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paulin_I/0/1/0/all/0/1\">Ivan Alekseevich Paulin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talay_E/0/1/0/all/0/1\">Evgeniy Petrovich Talay</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using maps to predict economic activity. (arXiv:2112.13850v1 [econ.GN])","link":"http://arxiv.org/abs/2112.13850","description":"<p>We introduce a novel machine learning approach to leverage historical and\ncontemporary maps to systematically predict economic statistics. Remote sensing\ndata have been used as reliable proxies for local economic activity. However,\nthey have only become available in recent years, thus limiting their\napplicability for long-term analysis. Historical maps, on the other hand, date\nback several decades. Our simple algorithm extracts meaningful features from\nthe maps based on their color compositions. The grid-level population\npredictions by our approach outperform the conventional CNN-based predictions\nusing raw map images. It also predicts population better than other approaches\nusing night light satellite images or land cover classifications as the input\nfor predictions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/econ/1/au:+Jeong_I/0/1/0/all/0/1\">Imryoung Jeong</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Yang_H/0/1/0/all/0/1\">Hyunjoo Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Astronomical Image Colorization and upscaling with Generative Adversarial Networks. (arXiv:2112.13865v1 [eess.IV])","link":"http://arxiv.org/abs/2112.13865","description":"<p>Automatic colorization of images without human intervention has been a\nsubject of interest in the machine learning community for a brief period of\ntime. Assigning color to an image is a highly ill-posed problem because of its\ninnate nature of possessing very high degrees of freedom; given an image, there\nis often no single color-combination that is correct. Besides colorization,\nanother problem in reconstruction of images is Single Image Super Resolution,\nwhich aims at transforming low resolution images to a higher resolution. This\nresearch aims to provide an automated approach for the problem by focusing on a\nvery specific domain of images, namely astronomical images, and process them\nusing Generative Adversarial Networks (GANs). We explore the usage of various\nmodels in two different color spaces, RGB and L*a*b. We use transferred\nlearning owing to a small data set, using pre-trained ResNet-18 as a backbone,\ni.e. encoder for the U-net and fine-tune it further. The model produces\nvisually appealing images which hallucinate high resolution, colorized data in\nthese results which does not exist in the original image. We present our\nresults by evaluating the GANs quantitatively using distance metrics such as L1\ndistance and L2 distance in each of the color spaces across all channels to\nprovide a comparative analysis. We use Frechet inception distance (FID) to\ncompare the distribution of the generated images with the distribution of the\nreal image to assess the model's performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kalvankar_S/0/1/0/all/0/1\">Shreyas Kalvankar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pandit_H/0/1/0/all/0/1\">Hrushikesh Pandit</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Parwate_P/0/1/0/all/0/1\">Pranav Parwate</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Patil_A/0/1/0/all/0/1\">Atharva Patil</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kamalapur_S/0/1/0/all/0/1\">Snehal Kamalapur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Fistful of Words: Learning Transferable Visual Models from Bag-of-Words Supervision. (arXiv:2112.13884v1 [cs.CV])","link":"http://arxiv.org/abs/2112.13884","description":"<p>Using natural language as a supervision for training visual recognition\nmodels holds great promise. Recent works have shown that if such supervision is\nused in the form of alignment between images and captions in large training\ndatasets, then the resulting aligned models perform well on zero-shot\nclassification as downstream tasks2. In this paper, we focus on teasing out\nwhat parts of the language supervision are essential for training zero-shot\nimage classification models. Through extensive and careful experiments, we show\nthat: 1) A simple Bag-of-Words (BoW) caption could be used as a replacement for\nmost of the image captions in the dataset. Surprisingly, we observe that this\napproach improves the zero-shot classification performance when combined with\nword balancing. 2) Using a BoW pretrained model, we can obtain more training\ndata by generating pseudo-BoW captions on images that do not have a caption.\nModels trained on images with real and pseudo-BoW captions achieve stronger\nzero-shot performance. On ImageNet-1k zero-shot evaluation, our best model,\nthat uses only 3M image-caption pairs, performs on-par with a CLIP model\ntrained on 15M image-caption pairs (31.5% vs 31.3%).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tejankar_A/0/1/0/all/0/1\">Ajinkya Tejankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tejankar_A/0/1/0/all/0/1\">Ajinkya Tejankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Bichen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1\">Saining Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khabsa_M/0/1/0/all/0/1\">Madian Khabsa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pirsiavash_H/0/1/0/all/0/1\">Hamed Pirsiavash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Firooz_H/0/1/0/all/0/1\">Hamed Firooz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MedShift: identifying shift data for medical dataset curation. (arXiv:2112.13885v1 [eess.IV])","link":"http://arxiv.org/abs/2112.13885","description":"<p>To curate a high-quality dataset, identifying data variance between the\ninternal and external sources is a fundamental and crucial step. However,\nmethods to detect shift or variance in data have not been significantly\nresearched. Challenges to this are the lack of effective approaches to learn\ndense representation of a dataset and difficulties of sharing private data\nacross medical institutions. To overcome the problems, we propose a unified\npipeline called MedShift to detect the top-level shift samples and thus\nfacilitate the medical curation. Given an internal dataset A as the base\nsource, we first train anomaly detectors for each class of dataset A to learn\ninternal distributions in an unsupervised way. Second, without exchanging data\nacross sources, we run the trained anomaly detectors on an external dataset B\nfor each class. The data samples with high anomaly scores are identified as\nshift data. To quantify the shiftness of the external dataset, we cluster B's\ndata into groups class-wise based on the obtained scores. We then train a\nmulti-class classifier on A and measure the shiftness with the classifier's\nperformance variance on B by gradually dropping the group with the largest\nanomaly score for each class. Additionally, we adapt a dataset quality metric\nto help inspect the distribution differences for multiple medical sources. We\nverify the efficacy of MedShift with musculoskeletal radiographs (MURA) and\nchest X-rays datasets from more than one external source. Experiments show our\nproposed shift data detection pipeline can be beneficial for medical centers to\ncurate high-quality datasets more efficiently. An interface introduction video\nto visualize our results is available at https://youtu.be/V3BF0P1sxQE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Guo_X/0/1/0/all/0/1\">Xiaoyuan Guo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gichoya_J/0/1/0/all/0/1\">Judy Wawira Gichoya</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Trivedi_H/0/1/0/all/0/1\">Hari Trivedi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Purkayastha_S/0/1/0/all/0/1\">Saptarshi Purkayastha</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Banerjee_I/0/1/0/all/0/1\">Imon Banerjee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Human View Synthesis using a Single Sparse RGB-D Input. (arXiv:2112.13889v1 [cs.CV])","link":"http://arxiv.org/abs/2112.13889","description":"<p>Novel view synthesis for humans in motion is a challenging computer vision\nproblem that enables applications such as free-viewpoint video. Existing\nmethods typically use complex setups with multiple input views, 3D supervision,\nor pre-trained models that do not generalize well to new identities. Aiming to\naddress these limitations, we present a novel view synthesis framework to\ngenerate realistic renders from unseen views of any human captured from a\nsingle-view sensor with sparse RGB-D, similar to a low-cost depth camera, and\nwithout actor-specific models. We propose an architecture to learn dense\nfeatures in novel views obtained by sphere-based neural rendering, and create\ncomplete renders using a global context inpainting model. Additionally, an\nenhancer network leverages the overall fidelity, even in occluded areas from\nthe original view, producing crisp renders with fine details. We show our\nmethod generates high-quality novel views of synthetic and real human actors\ngiven a single sparse RGB-D input. It generalizes to unseen identities, new\nposes and faithfully reconstructs facial expressions. Our approach outperforms\nprior human view synthesis methods and is robust to different levels of input\nsparsity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_P/0/1/0/all/0/1\">Phong Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarafianos_N/0/1/0/all/0/1\">Nikolaos Sarafianos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lassner_C/0/1/0/all/0/1\">Christoph Lassner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heikkila_J/0/1/0/all/0/1\">Janne Heikkila</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tung_T/0/1/0/all/0/1\">Tony Tung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SPViT: Enabling Faster Vision Transformers via Soft Token Pruning. (arXiv:2112.13890v1 [cs.CV])","link":"http://arxiv.org/abs/2112.13890","description":"<p>Recently, Vision Transformer (ViT) has continuously established new\nmilestones in the computer vision field, while the high computation and memory\ncost makes its propagation in industrial production difficult. Pruning, a\ntraditional model compression paradigm for hardware efficiency, has been widely\napplied in various DNN structures. Nevertheless, it stays ambiguous on how to\nperform exclusive pruning on the ViT structure. Considering three key points:\nthe structural characteristics, the internal data pattern of ViTs, and the\nrelated edge device deployment, we leverage the input token sparsity and\npropose a computation-aware soft pruning framework, which can be set up on\nvanilla Transformers of both flatten and CNN-type structures, such as\nPooling-based ViT (PiT). More concretely, we design a dynamic attention-based\nmulti-head token selector, which is a lightweight module for adaptive\ninstance-wise token selection. We further introduce a soft pruning technique,\nwhich integrates the less informative tokens generated by the selector module\ninto a package token that will participate in subsequent calculations rather\nthan being completely discarded. Our framework is bound to the trade-off\nbetween accuracy and computation constraints of specific edge devices through\nour proposed computation-aware training strategy. Experimental results show\nthat our framework significantly reduces the computation cost of ViTs while\nmaintaining comparable performance on image classification. Moreover, our\nframework can guarantee the identified model to meet resource specifications of\nmobile devices and FPGA, and even achieve the real-time execution of DeiT-T on\nmobile platforms. For example, our method reduces the latency of DeiT-T to 26\nms (26%$\\sim $41% superior to existing works) on the mobile device with\n0.25%$\\sim $4% higher top-1 accuracy on ImageNet. Our code will be released\nsoon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kong_Z/0/1/0/all/0/1\">Zhenglun Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_P/0/1/0/all/0/1\">Peiyan Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaolong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1\">Xin Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_W/0/1/0/all/0/1\">Wei Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Mengshu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_B/0/1/0/all/0/1\">Bin Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_M/0/1/0/all/0/1\">Minghai Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanzhi Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GPU-accelerated Faster Mean Shift with euclidean distance metrics. (arXiv:2112.13891v1 [cs.CV])","link":"http://arxiv.org/abs/2112.13891","description":"<p>Handling clustering problems are important in data statistics, pattern\nrecognition and image processing. The mean-shift algorithm, a common\nunsupervised algorithms, is widely used to solve clustering problems. However,\nthe mean-shift algorithm is restricted by its huge computational resource cost.\nIn previous research[10], we proposed a novel GPU-accelerated Faster Mean-shift\nalgorithm, which greatly speed up the cosine-embedding clustering problem. In\nthis study, we extend and improve the previous algorithm to handle Euclidean\ndistance metrics. Different from conventional GPU-based mean-shift algorithms,\nour algorithm adopts novel Seed Selection &amp; Early Stopping approaches, which\ngreatly increase computing speed and reduce GPU memory consumption. In the\nsimulation testing, when processing a 200K points clustering problem, our\nalgorithm achieved around 3 times speedup compared to the state-of-the-art\nGPU-based mean-shift algorithms with optimized GPU memory consumption.\nMoreover, in this study, we implemented a plug-and-play model for faster\nmean-shift algorithm, which can be easily deployed. (Plug-and-play model is\navailable: https://github.com/masqm/Faster-Mean-Shift-Euc)\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+You_L/0/1/0/all/0/1\">Le You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Han Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jinyong Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1\">Chorng Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lingxi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_X/0/1/0/all/0/1\">Xintong Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1\">Mengyang Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Non-Reference Quality Monitoring of Digital Images using Gradient Statistics and Feedforward Neural Networks. (arXiv:2112.13893v1 [eess.IV])","link":"http://arxiv.org/abs/2112.13893","description":"<p>Digital images contain a lot of redundancies, therefore, compressions are\napplied to reduce the image size without the loss of reasonable image quality.\nThe same become more prominent in the case of videos that contains image\nsequences and higher compression ratios are achieved in low throughput\nnetworks. Assessment of the quality of images in such scenarios becomes of\nparticular interest. Subjective evaluation in most of the scenarios becomes\ninfeasible so objective evaluation is preferred. Among the three objective\nquality measures, full-reference and reduced-reference methods require an\noriginal image in some form to calculate the quality score which is not\nfeasible in scenarios such as broadcasting or IP video. Therefore, a\nnon-reference quality metric is proposed to assess the quality of digital\nimages which calculates luminance and multiscale gradient statistics along with\nmean subtracted contrast normalized products as features to train a Feedforward\nNeural Network with Scaled Conjugate Gradient. The trained network has provided\ngood regression and R2 measures and further testing on LIVE Image Quality\nAssessment database release-2 has shown promising results. Pearson, Kendall,\nand Spearman's correlation are calculated between predicted and actual quality\nscores and their results are comparable to the state-of-the-art systems.\nMoreover, the proposed metric is computationally faster than its counterparts\nand can be used for the quality assessment of image sequences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ahmed_N/0/1/0/all/0/1\">Nisar Ahmed</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Asif_H/0/1/0/all/0/1\">Hafiz Muhammad Shahzad Asif</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khalid_H/0/1/0/all/0/1\">Hassan Khalid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Does CLIP Benefit Visual Question Answering in the Medical Domain as Much as it Does in the General Domain?. (arXiv:2112.13906v1 [cs.CV])","link":"http://arxiv.org/abs/2112.13906","description":"<p>Contrastive Language--Image Pre-training (CLIP) has shown remarkable success\nin learning with cross-modal supervision from extensive amounts of image--text\npairs collected online. Thus far, the effectiveness of CLIP has been\ninvestigated primarily in general-domain multimodal problems. This work\nevaluates the effectiveness of CLIP for the task of Medical Visual Question\nAnswering (MedVQA). To this end, we present PubMedCLIP, a fine-tuned version of\nCLIP for the medical domain based on PubMed articles. Our experiments are\nconducted on two MedVQA benchmark datasets and investigate two MedVQA methods,\nMEVF (Mixture of Enhanced Visual Features) and QCR (Question answering via\nConditional Reasoning). For each of these, we assess the merits of visual\nrepresentation learning using PubMedCLIP, the original CLIP, and\nstate-of-the-art MAML (Model-Agnostic Meta-Learning) networks pre-trained only\non visual data. We open source the code for our MedVQA pipeline and\npre-training PubMedCLIP. CLIP and PubMedCLIP achieve improvements in comparison\nto MAML's visual encoder. PubMedCLIP achieves the best results with gains in\nthe overall accuracy of up to 3%. Individual examples illustrate the strengths\nof PubMedCLIP in comparison to the previously widely used MAML networks. Visual\nrepresentation learning with language supervision in PubMedCLIP leads to\nnoticeable improvements for MedVQA. Our experiments reveal distributional\ndifferences in the two MedVQA benchmark datasets that have not been imparted in\nprevious work and cause different back-end visual encoders in PubMedCLIP to\nexhibit different behavior on these datasets. Moreover, we witness fundamental\nperformance differences of VQA in general versus medical domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Eslami_S/0/1/0/all/0/1\">Sedigheh Eslami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melo_G/0/1/0/all/0/1\">Gerard de Melo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meinel_C/0/1/0/all/0/1\">Christoph Meinel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Persuasion in COVID-19 Social Media Content: A Multi-Modal Characterization. (arXiv:2112.13910v1 [cs.CL])","link":"http://arxiv.org/abs/2112.13910","description":"<p>Social media content routinely incorporates multi-modal design to covey\ninformation and shape meanings, and sway interpretations toward desirable\nimplications, but the choices and outcomes of using both texts and visual\nimages have not been sufficiently studied. This work proposes a computational\napproach to analyze the outcome of persuasive information in multi-modal\ncontent, focusing on two aspects, popularity and reliability, in\nCOVID-19-related news articles shared on Twitter. The two aspects are\nintertwined in the spread of misinformation: for example, an unreliable article\nthat aims to misinform has to attain some popularity. This work has several\ncontributions. First, we propose a multi-modal (image and text) approach to\neffectively identify popularity and reliability of information sources\nsimultaneously. Second, we identify textual and visual elements that are\npredictive to information popularity and reliability. Third, by modeling\ncross-modal relations and similarity, we are able to uncover how unreliable\narticles construct multi-modal meaning in a distorted, biased fashion. Our work\ndemonstrates how to use multi-modal analysis for understanding influential\ncontent and has implications to social media literacy and engagement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Unal_M/0/1/0/all/0/1\">Mesut Erhan Unal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kovashka_A/0/1/0/all/0/1\">Adriana Kovashka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_W/0/1/0/all/0/1\">Wen-Ting Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yu-Ru Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Depth Estimation using Location Information. (arXiv:2112.13925v1 [cs.CV])","link":"http://arxiv.org/abs/2112.13925","description":"<p>The ability to accurately estimate depth information is crucial for many\nautonomous applications to recognize the surrounded environment and predict the\ndepth of important objects. One of the most recently used techniques is\nmonocular depth estimation where the depth map is inferred from a single image.\nThis paper improves the self-supervised deep learning techniques to perform\naccurate generalized monocular depth estimation. The main idea is to train the\ndeep model to take into account a sequence of the different frames, each frame\nis geotagged with its location information. This makes the model able to\nenhance depth estimation given area semantics. We demonstrate the effectiveness\nof our model to improve depth estimation results. The model is trained in a\nrealistic environment and the results show improvements in the depth map after\nadding the location data to the model training phase.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zaitoon_A/0/1/0/all/0/1\">Ahmed Zaitoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Munim_H/0/1/0/all/0/1\">Hossam El Din Abd El Munim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbas_H/0/1/0/all/0/1\">Hazem Abbas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SPIDER: Searching Personalized Neural Architecture for Federated Learning. (arXiv:2112.13939v1 [cs.LG])","link":"http://arxiv.org/abs/2112.13939","description":"<p>Federated learning (FL) is an efficient learning framework that assists\ndistributed machine learning when data cannot be shared with a centralized\nserver due to privacy and regulatory restrictions. Recent advancements in FL\nuse predefined architecture-based learning for all the clients. However, given\nthat clients' data are invisible to the server and data distributions are\nnon-identical across clients, a predefined architecture discovered in a\ncentralized setting may not be an optimal solution for all the clients in FL.\nMotivated by this challenge, in this work, we introduce SPIDER, an algorithmic\nframework that aims to Search Personalized neural architecture for federated\nlearning. SPIDER is designed based on two unique features: (1) alternately\noptimizing one architecture-homogeneous global model (Supernet) in a generic FL\nmanner and one architecture-heterogeneous local model that is connected to the\nglobal model by weight sharing-based regularization (2) achieving\narchitecture-heterogeneous local model by a novel neural architecture search\n(NAS) method that can select optimal subnet progressively using operation-level\nperturbation on the accuracy value as the criterion. Experimental results\ndemonstrate that SPIDER outperforms other state-of-the-art personalization\nmethods, and the searched personalized architectures are more inference\nefficient.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mushtaq_E/0/1/0/all/0/1\">Erum Mushtaq</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_C/0/1/0/all/0/1\">Chaoyang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_J/0/1/0/all/0/1\">Jie Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avestimehr_S/0/1/0/all/0/1\">Salman Avestimehr</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SurFit: Learning to Fit Surfaces Improves Few Shot Learning on Point Clouds. (arXiv:2112.13942v1 [cs.CV])","link":"http://arxiv.org/abs/2112.13942","description":"<p>We present SurFit, a simple approach for label efficient learning of 3D shape\nsegmentation networks. SurFit is based on a self-supervised task of decomposing\nthe surface of a 3D shape into geometric primitives. It can be readily applied\nto existing network architectures for 3D shape segmentation and improves their\nperformance in the few-shot setting, as we demonstrate in the widely used\nShapeNet and PartNet benchmarks. SurFit outperforms the prior state-of-the-art\nin this setting, suggesting that decomposability into primitives is a useful\nprior for learning representations predictive of semantic parts. We present a\nnumber of experiments varying the choice of geometric primitives and downstream\ntasks to demonstrate the effectiveness of the method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_G/0/1/0/all/0/1\">Gopal Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dash_B/0/1/0/all/0/1\">Bidya Dash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gadelha_M/0/1/0/all/0/1\">Matheus Gadelha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+RoyChowdhury_A/0/1/0/all/0/1\">Aruni RoyChowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loizou_M/0/1/0/all/0/1\">Marios Loizou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalogerakis_E/0/1/0/all/0/1\">Evangelos Kalogerakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1\">Liangliang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Learned_Miller_E/0/1/0/all/0/1\">Erik Learned-Miller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maji_R/0/1/0/all/0/1\">Rui Wang andSubhransu Maji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Source Feature Compression for Object Classification in Vision-Based Underwater Robotics. (arXiv:2112.13953v1 [cs.CV])","link":"http://arxiv.org/abs/2112.13953","description":"<p>New efficient source feature compression solutions are proposed based on a\ntwo-stage Walsh-Hadamard Transform (WHT) for Convolutional Neural Network\n(CNN)-based object classification in underwater robotics. The object images are\nfirstly transformed by WHT following a two-stage process. The transform-domain\ntensors have large values concentrated in the upper left corner of the matrices\nin the RGB channels. By observing this property, the transform-domain matrix is\npartitioned into inner and outer regions. Consequently, two novel partitioning\nmethods are proposed in this work: (i) fixing the size of inner and outer\nregions; and (ii) adjusting the size of inner and outer regions adaptively per\nimage. The proposals are evaluated with an underwater object dataset captured\nfrom the Raritan River in New Jersey, USA. It is demonstrated and verified that\nthe proposals reduce the training time effectively for learning-based\nunderwater object classification task and increase the accuracy compared with\nthe competing methods. The object classification is an essential part of a\nvision-based underwater robot that can sense the environment and navigate\nautonomously. Therefore, the proposed method is well-suited for efficient\ncomputer vision-based tasks in underwater robotics applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xueyuan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahmati_M/0/1/0/all/0/1\">Mehdi Rahmati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pompili_D/0/1/0/all/0/1\">Dario Pompili</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Moment in the Sun: Solar Nowcasting from Multispectral Satellite Data using Self-Supervised Learning. (arXiv:2112.13974v1 [cs.LG])","link":"http://arxiv.org/abs/2112.13974","description":"<p>Solar energy is now the cheapest form of electricity in history.\nUnfortunately, significantly increasing the grid's fraction of solar energy\nremains challenging due to its variability, which makes balancing electricity's\nsupply and demand more difficult. While thermal generators' ramp rate -- the\nmaximum rate that they can change their output -- is finite, solar's ramp rate\nis essentially infinite. Thus, accurate near-term solar forecasting, or\nnowcasting, is important to provide advance warning to adjust thermal generator\noutput in response to solar variations to ensure a balanced supply and demand.\nTo address the problem, this paper develops a general model for solar\nnowcasting from abundant and readily available multispectral satellite data\nusing self-supervised learning. Specifically, we develop deep auto-regressive\nmodels using convolutional neural networks (CNN) and long short-term memory\nnetworks (LSTM) that are globally trained across multiple locations to predict\nraw future observations of the spatio-temporal data collected by the recently\nlaunched GOES-R series of satellites. Our model estimates a location's future\nsolar irradiance based on satellite observations, which we feed to a regression\nmodel trained on smaller site-specific solar data to provide near-term solar\nphotovoltaic (PV) forecasts that account for site-specific characteristics. We\nevaluate our approach for different coverage areas and forecast horizons across\n25 solar sites and show that our approach yields errors close to that of a\nmodel using ground-truth observations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bansal_A/0/1/0/all/0/1\">Akansha Singh Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_T/0/1/0/all/0/1\">Trapit Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Irwin_D/0/1/0/all/0/1\">David Irwin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploiting Fine-grained Face Forgery Clues via Progressive Enhancement Learning. (arXiv:2112.13977v1 [cs.CV])","link":"http://arxiv.org/abs/2112.13977","description":"<p>With the rapid development of facial forgery techniques, forgery detection\nhas attracted more and more attention due to security concerns. Existing\napproaches attempt to use frequency information to mine subtle artifacts under\nhigh-quality forged faces. However, the exploitation of frequency information\nis coarse-grained, and more importantly, their vanilla learning process\nstruggles to extract fine-grained forgery traces. To address this issue, we\npropose a progressive enhancement learning framework to exploit both the RGB\nand fine-grained frequency clues. Specifically, we perform a fine-grained\ndecomposition of RGB images to completely decouple the real and fake traces in\nthe frequency space. Subsequently, we propose a progressive enhancement\nlearning framework based on a two-branch network, combined with\nself-enhancement and mutual-enhancement modules. The self-enhancement module\ncaptures the traces in different input spaces based on spatial noise\nenhancement and channel attention. The Mutual-enhancement module concurrently\nenhances RGB and frequency features by communicating in the shared spatial\ndimension. The progressive enhancement process facilitates the learning of\ndiscriminative features with fine-grained face forgery clues. Extensive\nexperiments on several datasets show that our method outperforms the\nstate-of-the-art face forgery detection methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1\">Qiqi Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_T/0/1/0/all/0/1\">Taiping Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1\">Shouhong Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_R/0/1/0/all/0/1\">Ran Yi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quaternion-based dynamic mode decomposition for background modeling in color videos. (arXiv:2112.13982v1 [cs.CV])","link":"http://arxiv.org/abs/2112.13982","description":"<p>Scene Background Initialization (SBI) is one of the challenging problems in\ncomputer vision. Dynamic mode decomposition (DMD) is a recently proposed method\nto robustly decompose a video sequence into the background model and the\ncorresponding foreground part. However, this method needs to convert the color\nimage into the grayscale image for processing, which leads to the neglect of\nthe coupling information between the three channels of the color image. In this\nstudy, we propose a quaternion-based DMD (Q-DMD), which extends the DMD by\nquaternion matrix analysis, so as to completely preserve the inherent color\nstructure of the color image and the color video. We exploit the standard\neigenvalues of the quaternion matrix to compute its spectral decomposition and\ncalculate the corresponding Q-DMD modes and eigenvalues. The results on the\npublicly available benchmark datasets prove that our Q-DMD outperforms the\nexact DMD method, and experiment results also demonstrate that the performance\nof our approach is comparable to that of the state-of-the-art ones.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Juan Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kou_K/0/1/0/all/0/1\">Kit Ian Kou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_J/0/1/0/all/0/1\">Jifei Miao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Siamese Network with Interactive Transformer for Video Object Segmentation. (arXiv:2112.13983v1 [cs.CV])","link":"http://arxiv.org/abs/2112.13983","description":"<p>Semi-supervised video object segmentation (VOS) refers to segmenting the\ntarget object in remaining frames given its annotation in the first frame,\nwhich has been actively studied in recent years. The key challenge lies in\nfinding effective ways to exploit the spatio-temporal context of past frames to\nhelp learn discriminative target representation of current frame. In this\npaper, we propose a novel Siamese network with a specifically designed\ninteractive transformer, called SITVOS, to enable effective context propagation\nfrom historical to current frames. Technically, we use the transformer encoder\nand decoder to handle the past frames and current frame separately, i.e., the\nencoder encodes robust spatio-temporal context of target object from the past\nframes, while the decoder takes the feature embedding of current frame as the\nquery to retrieve the target from the encoder output. To further enhance the\ntarget representation, a feature interaction module (FIM) is devised to promote\nthe information flow between the encoder and decoder. Moreover, we employ the\nSiamese architecture to extract backbone features of both past and current\nframes, which enables feature reuse and is more efficient than existing\nmethods. Experimental results on three challenging benchmarks validate the\nsuperiority of SITVOS over state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lan_M/0/1/0/all/0/1\">Meng Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_F/0/1/0/all/0/1\">Fengxiang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lefei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LatteGAN: Visually Guided Language Attention for Multi-Turn Text-Conditioned Image Manipulation. (arXiv:2112.13985v1 [cs.CV])","link":"http://arxiv.org/abs/2112.13985","description":"<p>Text-guided image manipulation tasks have recently gained attention in the\nvision-and-language community. While most of the prior studies focused on\nsingle-turn manipulation, our goal in this paper is to address the more\nchallenging multi-turn image manipulation (MTIM) task. Previous models for this\ntask successfully generate images iteratively, given a sequence of instructions\nand a previously generated image. However, this approach suffers from\nunder-generation and a lack of generated quality of the objects that are\ndescribed in the instructions, which consequently degrades the overall\nperformance. To overcome these problems, we present a novel architecture called\na Visually Guided Language Attention GAN (LatteGAN). Here, we address the\nlimitations of the previous approaches by introducing a Visually Guided\nLanguage Attention (Latte) module, which extracts fine-grained text\nrepresentations for the generator, and a Text-Conditioned U-Net discriminator\narchitecture, which discriminates both the global and local representations of\nfake or real images. Extensive experiments on two distinct MTIM datasets,\nCoDraw and i-CLEVR, demonstrate the state-of-the-art performance of the\nproposed model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Matsumori_S/0/1/0/all/0/1\">Shoya Matsumori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abe_Y/0/1/0/all/0/1\">Yuki Abe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shingyouchi_K/0/1/0/all/0/1\">Kosuke Shingyouchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugiura_K/0/1/0/all/0/1\">Komei Sugiura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Imai_M/0/1/0/all/0/1\">Michita Imai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep-CNN based Robotic Multi-Class Under-Canopy Weed Control in Precision Farming. (arXiv:2112.13986v1 [cs.CV])","link":"http://arxiv.org/abs/2112.13986","description":"<p>Smart weeding systems to perform plant-specific operations can contribute to\nthe sustainability of agriculture and the environment. Despite monumental\nadvances in autonomous robotic technologies for precision weed management in\nrecent years, work on under-canopy weeding in fields is yet to be realized. A\nprerequisite of such systems is reliable detection and classification of weeds\nto avoid mistakenly spraying and, thus, damaging the surrounding plants.\nReal-time multi-class weed identification enables species-specific treatment of\nweeds and significantly reduces the amount of herbicide use. Here, our first\ncontribution is the first adequately large realistic image dataset\n\\textit{AIWeeds} (one/multiple kinds of weeds in one image), a library of about\n10,000 annotated images of flax, and the 14 most common weeds in fields and\ngardens taken from 20 different locations in North Dakota, California, and\nCentral China. Second, we provide a full pipeline from model training with\nmaximum efficiency to deploying the TensorRT-optimized model onto a single\nboard computer. Based on \\textit{AIWeeds} and the pipeline, we present a\nbaseline for classification performance using five benchmark CNN models. Among\nthem, MobileNetV2, with both the shortest inference time and lowest memory\nconsumption, is the qualified candidate for real-time applications. Finally, we\ndeploy MobileNetV2 onto our own compact autonomous robot \\textit{SAMBot} for\nreal-time weed detection. The 90\\% test accuracy realized in previously unseen\nscenes in flax fields (with a row spacing of 0.2-0.3 m), with crops and weeds,\ndistortion, blur, and shadows, is a milestone towards precision weed control in\nthe real world. We have publicly released the dataset and code to generate the\nresults at\n\\url{https://github.com/StructuresComp/Multi-class-Weed-Classification}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yayun Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guofeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsang_D/0/1/0/all/0/1\">Darren Tsang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jawed_M/0/1/0/all/0/1\">M. Khalid Jawed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Associative Adversarial Learning Based on Selective Attack. (arXiv:2112.13989v1 [cs.CV])","link":"http://arxiv.org/abs/2112.13989","description":"<p>A human's attention can intuitively adapt to corrupted areas of an image by\nrecalling a similar uncorrupted image they have previously seen. This\nobservation motivates us to improve the attention of adversarial images by\nconsidering their clean counterparts. To accomplish this, we introduce\nAssociative Adversarial Learning (AAL) into adversarial learning to guide a\nselective attack. We formulate the intrinsic relationship between attention and\nattack (perturbation) as a coupling optimization problem to improve their\ninteraction. This leads to an attention backtracking algorithm that can\neffectively enhance the attention's adversarial robustness. Our method is\ngeneric and can be used to address a variety of tasks by simply choosing\ndifferent kernels for the associative attention that select other regions for a\nspecific attack. Experimental results show that the selective attack improves\nthe model's performance. We show that our method improves the recognition\naccuracy of adversarial training on ImageNet by 8.32% compared with the\nbaseline. It also increases object detection mAP on PascalVOC by 2.02% and\nrecognition accuracy of few-shot learning on miniImageNet by 1.63%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Runqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_X/0/1/0/all/0/1\">Xiaoyue Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Baochang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_S/0/1/0/all/0/1\">Song Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wentao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doermann_D/0/1/0/all/0/1\">David Doermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_G/0/1/0/all/0/1\">Guodong Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pale Transformer: A General Vision Transformer Backbone with Pale-Shaped Attention. (arXiv:2112.14000v1 [cs.CV])","link":"http://arxiv.org/abs/2112.14000","description":"<p>Recently, Transformers have shown promising performance in various vision\ntasks. To reduce the quadratic computation complexity caused by the global\nself-attention, various methods constrain the range of attention within a local\nregion to improve its efficiency. Consequently, their receptive fields in a\nsingle attention layer are not large enough, resulting in insufficient context\nmodeling. To address this issue, we propose a Pale-Shaped self-Attention\n(PS-Attention), which performs self-attention within a pale-shaped region.\nCompared to the global self-attention, PS-Attention can reduce the computation\nand memory costs significantly. Meanwhile, it can capture richer contextual\ninformation under the similar computation complexity with previous local\nself-attention mechanisms. Based on the PS-Attention, we develop a general\nVision Transformer backbone with a hierarchical architecture, named Pale\nTransformer, which achieves 83.4%, 84.3%, and 84.9% Top-1 accuracy with the\nmodel size of 22M, 48M, and 85M respectively for 224 ImageNet-1K\nclassification, outperforming the previous Vision Transformer backbones. For\ndownstream tasks, our Pale Transformer backbone performs better than the recent\nstate-of-the-art CSWin Transformer by a large margin on ADE20K semantic\nsegmentation and COCO object detection &amp; instance segmentation. The code will\nbe released on https://github.com/BR-IDL/PaddleViT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Sitong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tianyi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1\">Haoru Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_G/0/1/0/all/0/1\">Guodong Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Band Wi-Fi Sensing with Matched Feature Granularity. (arXiv:2112.14006v1 [cs.NI])","link":"http://arxiv.org/abs/2112.14006","description":"<p>Complementary to the fine-grained channel state information (CSI) from the\nphysical layer and coarse-grained received signal strength indicator (RSSI)\nmeasurements, the mid-grained spatial beam attributes (e.g., beam SNR) that are\navailable at millimeter-wave (mmWave) bands during the mandatory beam training\nphase can be repurposed for Wi-Fi sensing applications. In this paper, we\npropose a multi-band Wi-Fi fusion method for Wi-Fi sensing that hierarchically\nfuses the features from both the fine-grained CSI at sub-6 GHz and the\nmid-grained beam SNR at 60 GHz in a granularity matching framework. The\ngranularity matching is realized by pairing two feature maps from the CSI and\nbeam SNR at different granularity levels and linearly combining all paired\nfeature maps into a fused feature map with learnable weights.\n</p>\n<p>To further address the issue of limited labeled training data, we propose an\nautoencoder-based multi-band Wi-Fi fusion network that can be pre-trained in an\nunsupervised fashion. Once the autoencoder-based fusion network is pre-trained,\nwe detach the decoders and append multi-task sensing heads to the fused feature\nmap by fine-tuning the fusion block and re-training the multi-task heads from\nthe scratch. The multi-band Wi-Fi fusion framework is thoroughly validated by\nin-house experimental Wi-Fi sensing datasets spanning three tasks: 1) pose\nrecognition; 2) occupancy sensing; and 3) indoor localization. Comparison to\nfour baseline methods (i.e., CSI-only, beam SNR-only, input fusion, and feature\nfusion) demonstrates the granularity matching improves the multi-task sensing\nperformance. Quantitative performance is evaluated as a function of the number\nof labeled training data, latent space dimension, and fine-tuning learning\nrates.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jianyuan Yu</a>, Pu (Perry) <a href=\"http://arxiv.org/find/cs/1/au:+Wang/0/1/0/all/0/1\">Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koike_Akino_T/0/1/0/all/0/1\">Toshiaki Koike-Akino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Ye Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orlik_P/0/1/0/all/0/1\">Philip V. Orlik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buehrer_R/0/1/0/all/0/1\">R. Michael Buehrer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GuidedMix-Net: Semi-supervised Semantic Segmentation by Using Labeled Images as Reference. (arXiv:2112.14015v1 [cs.CV])","link":"http://arxiv.org/abs/2112.14015","description":"<p>Semi-supervised learning is a challenging problem which aims to construct a\nmodel by learning from limited labeled examples. Numerous methods for this task\nfocus on utilizing the predictions of unlabeled instances consistency alone to\nregularize networks. However, treating labeled and unlabeled data separately\noften leads to the discarding of mass prior knowledge learned from the labeled\nexamples. %, and failure to mine the feature interaction between the labeled\nand unlabeled image pairs. In this paper, we propose a novel method for\nsemi-supervised semantic segmentation named GuidedMix-Net, by leveraging\nlabeled information to guide the learning of unlabeled instances. Specifically,\nGuidedMix-Net employs three operations: 1) interpolation of similar\nlabeled-unlabeled image pairs; 2) transfer of mutual information; 3)\ngeneralization of pseudo masks. It enables segmentation models can learning the\nhigher-quality pseudo masks of unlabeled data by transfer the knowledge from\nlabeled samples to unlabeled data. Along with supervised learning for labeled\ndata, the prediction of unlabeled data is jointly learned with the generated\npseudo masks from the mixed data. Extensive experiments on PASCAL VOC 2012, and\nCityscapes demonstrate the effectiveness of our GuidedMix-Net, which achieves\ncompetitive segmentation accuracy and significantly improves the mIoU by +7$\\%$\ncompared to previous approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tu_P/0/1/0/all/0/1\">Peng Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yawen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_F/0/1/0/all/0/1\">Feng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhenyu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1\">Liujun Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recursive Least-Squares Estimator-Aided Online Learning for Visual Tracking. (arXiv:2112.14016v1 [cs.CV])","link":"http://arxiv.org/abs/2112.14016","description":"<p>Tracking visual objects from a single initial exemplar in the testing phase\nhas been broadly cast as a one-/few-shot problem, i.e., one-shot learning for\ninitial adaptation and few-shot learning for online adaptation. The recent\nfew-shot online adaptation methods incorporate the prior knowledge from large\namounts of annotated training data via complex meta-learning optimization in\nthe offline phase. This helps the online deep trackers to achieve fast\nadaptation and reduce overfitting risk in tracking. In this paper, we propose a\nsimple yet effective recursive least-squares estimator-aided online learning\napproach for few-shot online adaptation without requiring offline training. It\nallows an in-built memory retention mechanism for the model to remember the\nknowledge about the object seen before, and thus the seen data can be safely\nremoved from training. This also bears certain similarities to the emerging\ncontinual learning field in preventing catastrophic forgetting. This mechanism\nenables us to unveil the power of modern online deep trackers without incurring\ntoo much extra computational cost. We evaluate our approach based on two\nnetworks in the online learning families for tracking, i.e., multi-layer\nperceptrons in RT-MDNet and convolutional neural networks in DiMP. The\nconsistent improvements on several challenging tracking benchmarks demonstrate\nits effectiveness and efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1\">Xiaojuan Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kou_Y/0/1/0/all/0/1\">Yutong Kou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Shan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Weiming Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-supervised Salient Object Detection with Effective Confidence Estimation. (arXiv:2112.14019v1 [cs.CV])","link":"http://arxiv.org/abs/2112.14019","description":"<p>The success of existing salient object detection models relies on a large\npixel-wise labeled training dataset. How-ever, collecting such a dataset is not\nonly time-consuming but also very expensive. To reduce the labeling burden, we\nstudy semi-supervised salient object detection, and formulate it as an\nunlabeled dataset pixel-level confidence estimation problem by identifying\npixels with less confident predictions. Specifically, we introduce a new latent\nvariable model with an energy-based prior for effective latent space\nexploration, leading to more reliable confidence maps. With the proposed\nstrategy, the unlabelled images can effectively participate in model training.\nExperimental results show that the proposed solution, using only 1/16 of the\nannotations from the original training dataset, achieves competitive\nperformance compared with state-of-the-art fully supervised models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiawei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barnes_N/0/1/0/all/0/1\">Nick Barnes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilayer Graph Contrastive Clustering Network. (arXiv:2112.14021v1 [cs.SI])","link":"http://arxiv.org/abs/2112.14021","description":"<p>Multilayer graph has garnered plenty of research attention in many areas due\nto their high utility in modeling interdependent systems. However, clustering\nof multilayer graph, which aims at dividing the graph nodes into categories or\ncommunities, is still at a nascent stage. Existing methods are often limited to\nexploiting the multiview attributes or multiple networks and ignoring more\ncomplex and richer network frameworks. To this end, we propose a generic and\neffective autoencoder framework for multilayer graph clustering named\nMultilayer Graph Contrastive Clustering Network (MGCCN). MGCCN consists of\nthree modules: (1)Attention mechanism is applied to better capture the\nrelevance between nodes and neighbors for better node embeddings. (2)To better\nexplore the consistent information in different networks, a contrastive fusion\nstrategy is introduced. (3)MGCCN employs a self-supervised component that\niteratively strengthens the node embedding and clustering. Extensive\nexperiments on different types of real-world graph data indicate that our\nproposed method outperforms state-of-the-art techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Liang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_Z/0/1/0/all/0/1\">Zhao Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_L/0/1/0/all/0/1\">Ling Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wenbo Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xixu He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Low Light Enhancement with RAW Images. (arXiv:2112.14022v1 [eess.IV])","link":"http://arxiv.org/abs/2112.14022","description":"<p>In this paper, we make the first benchmark effort to elaborate on the\nsuperiority of using RAW images in the low light enhancement and develop a\nnovel alternative route to utilize RAW images in a more flexible and practical\nway. Inspired by a full consideration on the typical image processing pipeline,\nwe are inspired to develop a new evaluation framework, Factorized Enhancement\nModel (FEM), which decomposes the properties of RAW images into measurable\nfactors and provides a tool for exploring how properties of RAW images affect\nthe enhancement performance empirically. The empirical benchmark results show\nthat the Linearity of data and Exposure Time recorded in meta-data play the\nmost critical role, which brings distinct performance gains in various measures\nover the approaches taking the sRGB images as input. With the insights obtained\nfrom the benchmark results in mind, a RAW-guiding Exposure Enhancement Network\n(REENet) is developed, which makes trade-offs between the advantages and\ninaccessibility of RAW images in real applications in a way of using RAW images\nonly in the training phase. REENet projects sRGB images into linear RAW domains\nto apply constraints with corresponding RAW images to reduce the difficulty of\nmodeling training. After that, in the testing phase, our REENet does not rely\non RAW images. Experimental results demonstrate not only the superiority of\nREENet to state-of-the-art sRGB-based methods and but also the effectiveness of\nthe RAW guidance and all components.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Huang_H/0/1/0/all/0/1\">Haofeng Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_W/0/1/0/all/0/1\">Wenhan Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_Y/0/1/0/all/0/1\">Yueyu Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jiaying Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Duan_L/0/1/0/all/0/1\">Ling-Yu Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Devil is in the Task: Exploiting Reciprocal Appearance-Localization Features for Monocular 3D Object Detection. (arXiv:2112.14023v1 [cs.CV])","link":"http://arxiv.org/abs/2112.14023","description":"<p>Low-cost monocular 3D object detection plays a fundamental role in autonomous\ndriving, whereas its accuracy is still far from satisfactory. In this paper, we\ndig into the 3D object detection task and reformulate it as the sub-tasks of\nobject localization and appearance perception, which benefits to a deep\nexcavation of reciprocal information underlying the entire task. We introduce a\nDynamic Feature Reflecting Network, named DFR-Net, which contains two novel\nstandalone modules: (i) the Appearance-Localization Feature Reflecting module\n(ALFR) that first separates taskspecific features and then self-mutually\nreflects the reciprocal features; (ii) the Dynamic Intra-Trading module (DIT)\nthat adaptively realigns the training processes of various sub-tasks via a\nself-learning manner. Extensive experiments on the challenging KITTI dataset\ndemonstrate the effectiveness and generalization of DFR-Net. We rank 1st among\nall the monocular 3D object detectors in the KITTI test set (till March 16th,\n2021). The proposed method is also easy to be plug-and-play in many\ncutting-edge 3D detection frameworks at negligible cost to boost performance.\nThe code will be made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zou_Z/0/1/0/all/0/1\">Zhikang Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1\">Xiaoqing Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_L/0/1/0/all/0/1\">Liang Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xianhui Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xiao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jianfeng Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_X/0/1/0/all/0/1\">Xiangyang Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_E/0/1/0/all/0/1\">Errui Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Delving into Probabilistic Uncertainty for Unsupervised Domain Adaptive Person Re-Identification. (arXiv:2112.14025v1 [cs.CV])","link":"http://arxiv.org/abs/2112.14025","description":"<p>Clustering-based unsupervised domain adaptive (UDA) person re-identification\n(ReID) reduces exhaustive annotations. However, owing to unsatisfactory feature\nembedding and imperfect clustering, pseudo labels for target domain data\ninherently contain an unknown proportion of wrong ones, which would mislead\nfeature learning. In this paper, we propose an approach named probabilistic\nuncertainty guided progressive label refinery (P$^2$LR) for domain adaptive\nperson re-identification. First, we propose to model the labeling uncertainty\nwith the probabilistic distance along with ideal single-peak distributions. A\nquantitative criterion is established to measure the uncertainty of pseudo\nlabels and facilitate the network training. Second, we explore a progressive\nstrategy for refining pseudo labels. With the uncertainty-guided alternative\noptimization, we balance between the exploration of target domain data and the\nnegative effects of noisy labeling. On top of a strong baseline, we obtain\nsignificant improvements and achieve the state-of-the-art performance on four\nUDA ReID benchmarks. Specifically, our method outperforms the baseline by 6.5%\nmAP on the Duke2Market task, while surpassing the state-of-the-art method by\n2.5% mAP on the Market2MSMT task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jian Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+li_Y/0/1/0/all/0/1\">Yali li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shengjin Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SECP-Net: SE-Connection Pyramid Network of Organ At Risk Segmentation for Nasopharyngeal Carcinoma. (arXiv:2112.14026v1 [eess.IV])","link":"http://arxiv.org/abs/2112.14026","description":"<p>Nasopharyngeal carcinoma (NPC) is a kind of malignant tumor. Accurate and\nautomatic segmentation of organs at risk (OAR) of computed tomography (CT)\nimages is clinically significant. In recent years, deep learning models\nrepresented by U-Net have been widely applied in medical image segmentation\ntasks, which can help doctors with reduction of workload and get accurate\nresults more quickly. In OAR segmentation of NPC, the sizes of OAR are\nvariable, especially, some of them are small. Traditional deep neural networks\nunderperform during segmentation due to the lack use of global and multi-size\ninformation. This paper proposes a new SE-Connection Pyramid Network\n(SECP-Net). SECP-Net extracts global and multi-size information flow with se\nconnection (SEC) modules and a pyramid structure of network for improving the\nsegmentation performance, especially that of small organs. SECP-Net also\ndesigns an auto-context cascaded network to further improve the segmentation\nperformance. Comparative experiments are conducted between SECP-Net and other\nrecently methods on a dataset with CT images of head and neck. Five-fold cross\nvalidation is used to evaluate the performance based on two metrics, i.e., Dice\nand Jaccard similarity. Experimental results show that SECP-Net can achieve\nSOTA performance in this challenging task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Huang_Z/0/1/0/all/0/1\">Zexi Huang</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Guo_L/0/1/0/all/0/1\">Lihua Guo</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Yang_X/0/1/0/all/0/1\">Xin Yang</a> (2), <a href=\"http://arxiv.org/find/eess/1/au:+Huang_S/0/1/0/all/0/1\">Sijuan Huang</a> (2) ((1) School of Electronic and Information Engineering, South China University of Technology, (2) Sun Yat-sen University Cancer Center)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DetarNet: Decoupling Translation and Rotation by Siamese Network for Point Cloud Registration. (arXiv:2112.14059v1 [cs.CV])","link":"http://arxiv.org/abs/2112.14059","description":"<p>Point cloud registration is a fundamental step for many tasks. In this paper,\nwe propose a neural network named DetarNet to decouple the translation $t$ and\nrotation $R$, so as to overcome the performance degradation due to their mutual\ninterference in point cloud registration. First, a Siamese Network based\nProgressive and Coherent Feature Drift (PCFD) module is proposed to align the\nsource and target points in high-dimensional feature space, and accurately\nrecover translation from the alignment process. Then we propose a Consensus\nEncoding Unit (CEU) to construct more distinguishable features for a set of\nputative correspondences. After that, a Spatial and Channel Attention (SCA)\nblock is adopted to build a classification network for finding good\ncorrespondences. Finally, the rotation is obtained by Singular Value\nDecomposition (SVD). In this way, the proposed network decouples the estimation\nof translation and rotation, resulting in better performance for both of them.\nExperimental results demonstrate that the proposed DetarNet improves\nregistration performance on both indoor and outdoor scenes. Our code will be\navailable in \\url{https://github.com/ZhiChen902/DetarNet}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_W/0/1/0/all/0/1\">Wenbing Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating Shifts in GAN Output-Distributions. (arXiv:2112.14061v1 [cs.LG])","link":"http://arxiv.org/abs/2112.14061","description":"<p>A fundamental and still largely unsolved question in the context of\nGenerative Adversarial Networks is whether they are truly able to capture the\nreal data distribution and, consequently, to sample from it. In particular, the\nmultidimensional nature of image distributions leads to a complex evaluation of\nthe diversity of GAN distributions. Existing approaches provide only a partial\nunderstanding of this issue, leaving the question unanswered. In this work, we\nintroduce a loop-training scheme for the systematic investigation of observable\nshifts between the distributions of real training data and GAN generated data.\nAdditionally, we introduce several bounded measures for distribution shifts,\nwhich are both easy to compute and to interpret. Overall, the combination of\nthese methods allows an explorative investigation of innate limitations of\ncurrent GAN algorithms. Our experiments on different data-sets and multiple\nstate-of-the-art GAN architectures show large shifts between input and output\ndistributions, showing that existing theoretical guarantees towards the\nconvergence of output distributions appear not to be holding in practice.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Durall_R/0/1/0/all/0/1\">Ricard Durall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keuper_J/0/1/0/all/0/1\">Janis Keuper</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Embodied Learning for Lifelong Visual Perception. (arXiv:2112.14084v1 [cs.CV])","link":"http://arxiv.org/abs/2112.14084","description":"<p>We study lifelong visual perception in an embodied setup, where we develop\nnew models and compare various agents that navigate in buildings and\noccasionally request annotations which, in turn, are used to refine their\nvisual perception capabilities. The purpose of the agents is to recognize\nobjects and other semantic classes in the whole building at the end of a\nprocess that combines exploration and active visual learning. As we study this\ntask in a lifelong learning context, the agents should use knowledge gained in\nearlier visited environments in order to guide their exploration and active\nlearning strategy in successively visited buildings. We use the semantic\nsegmentation performance as a proxy for general visual perception and study\nthis novel task for several exploration and annotation methods, ranging from\nfrontier exploration baselines which use heuristic active learning, to a fully\nlearnable approach. For the latter, we introduce a deep reinforcement learning\n(RL) based agent which jointly learns both navigation and active learning. A\npoint goal navigation formulation, coupled with a global planner which supplies\ngoals, is integrated into the RL model in order to provide further incentives\nfor systematic exploration of novel scenes. By performing extensive experiments\non the Matterport3D dataset, we show how the proposed agents can utilize\nknowledge from previously explored scenes when exploring new ones, e.g. through\nless granular exploration and less frequent requests for annotations. The\nresults also suggest that a learning-based agent is able to use its prior\nvisual knowledge more effectively than heuristic alternatives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nilsson_D/0/1/0/all/0/1\">David Nilsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pirinen_A/0/1/0/all/0/1\">Aleksis Pirinen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gartner_E/0/1/0/all/0/1\">Erik G&#xe4;rtner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sminchisescu_C/0/1/0/all/0/1\">Cristian Sminchisescu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"APRIL: Finding the Achilles' Heel on Privacy for Vision Transformers. (arXiv:2112.14087v1 [cs.CV])","link":"http://arxiv.org/abs/2112.14087","description":"<p>Federated learning frameworks typically require collaborators to share their\nlocal gradient updates of a common model instead of sharing training data to\npreserve privacy. However, prior works on Gradient Leakage Attacks showed that\nprivate training data can be revealed from gradients. So far almost all\nrelevant works base their attacks on fully-connected or convolutional neural\nnetworks. Given the recent overwhelmingly rising trend of adapting Transformers\nto solve multifarious vision tasks, it is highly valuable to investigate the\nprivacy risk of vision transformers. In this paper, we analyse the gradient\nleakage risk of self-attention based mechanism in both theoretical and\npractical manners. Particularly, we propose APRIL - Attention PRIvacy Leakage,\nwhich poses a strong threat to self-attention inspired models such as ViT.\nShowing how vision Transformers are at the risk of privacy leakage via\ngradients, we urge the significance of designing privacy-safer Transformer\nmodels and defending schemes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiahao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xi Sheryl Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tianli Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiangyu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1\">Jian Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Synchronized Audio-Visual Frames with Fractional Positional Encoding for Transformers in Video-to-Text Translation. (arXiv:2112.14088v1 [cs.CV])","link":"http://arxiv.org/abs/2112.14088","description":"<p>Video-to-Text (VTT) is the task of automatically generating descriptions for\nshort audio-visual video clips, which can support visually impaired people to\nunderstand scenes of a YouTube video for instance. Transformer architectures\nhave shown great performance in both machine translation and image captioning,\nlacking a straightforward and reproducible application for VTT. However, there\nis no comprehensive study on different strategies and advice for video\ndescription generation including exploiting the accompanying audio with fully\nself-attentive networks. Thus, we explore promising approaches from image\ncaptioning and video processing and apply them to VTT by developing a\nstraightforward Transformer architecture. Additionally, we present a novel way\nof synchronizing audio and video features in Transformers which we call\nFractional Positional Encoding (FPE). We run multiple experiments on the VATEX\ndataset to determine a configuration applicable to unseen datasets that helps\ndescribe short video clips in natural language and improved the CIDEr and\nBLEU-4 scores by 37.13 and 12.83 points compared to a vanilla Transformer\nnetwork and achieve state-of-the-art results on the MSR-VTT and MSVD datasets.\nAlso, FPE helps increase the CIDEr score by a relative factor of 8.6%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Harzig_P/0/1/0/all/0/1\">Philipp Harzig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Einfalt_M/0/1/0/all/0/1\">Moritz Einfalt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lienhart_R/0/1/0/all/0/1\">Rainer Lienhart</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extended Self-Critical Pipeline for Transforming Videos to Text (TRECVID-VTT Task 2021) -- Team: MMCUniAugsburg. (arXiv:2112.14100v1 [cs.CV])","link":"http://arxiv.org/abs/2112.14100","description":"<p>The Multimedia and Computer Vision Lab of the University of Augsburg\nparticipated in the VTT task only. We use the VATEX and TRECVID-VTT datasets\nfor training our VTT models. We base our model on the Transformer approach for\nboth of our submitted runs. For our second model, we adapt the X-Linear\nAttention Networks for Image Captioning which does not yield the desired bump\nin scores. For both models, we train on the complete VATEX dataset and 90% of\nthe TRECVID-VTT dataset for pretraining while using the remaining 10% for\nvalidation. We finetune both models with self-critical sequence training, which\nboosts the validation performance significantly. Overall, we find that training\na Video-to-Text system on traditional Image Captioning pipelines delivers very\npoor performance. When switching to a Transformer-based architecture our\nresults greatly improve and the generated captions match better with the\ncorresponding video.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Harzig_P/0/1/0/all/0/1\">Philipp Harzig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Einfalt_M/0/1/0/all/0/1\">Moritz Einfalt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ludwig_K/0/1/0/all/0/1\">Katja Ludwig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lienhart_R/0/1/0/all/0/1\">Rainer Lienhart</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Skin feature point tracking using deep feature encodings. (arXiv:2112.14159v1 [cs.CV])","link":"http://arxiv.org/abs/2112.14159","description":"<p>Facial feature tracking is a key component of imaging ballistocardiography\n(BCG) where accurate quantification of the displacement of facial keypoints is\nneeded for good heart rate estimation. Skin feature tracking enables\nvideo-based quantification of motor degradation in Parkinson's disease.\nTraditional computer vision algorithms include Scale Invariant Feature\nTransform (SIFT), Speeded-Up Robust Features (SURF), and Lucas-Kanade method\n(LK). These have long represented the state-of-the-art in efficiency and\naccuracy but fail when common deformations, like affine local transformations\nor illumination changes, are present.\n</p>\n<p>Over the past five years, deep convolutional neural networks have\noutperformed traditional methods for most computer vision tasks. We propose a\npipeline for feature tracking, that applies a convolutional stacked autoencoder\nto identify the most similar crop in an image to a reference crop containing\nthe feature of interest. The autoencoder learns to represent image crops into\ndeep feature encodings specific to the object category it is trained on.\n</p>\n<p>We train the autoencoder on facial images and validate its ability to track\nskin features in general using manually labeled face and hand videos. The\ntracking errors of distinctive skin features (moles) are so small that we\ncannot exclude that they stem from the manual labelling based on a\n$\\chi^2$-test. With a mean error of 0.6-4.2 pixels, our method outperformed the\nother methods in all but one scenario. More importantly, our method was the\nonly one to not diverge.\n</p>\n<p>We conclude that our method creates better feature descriptors for feature\ntracking, feature matching, and image registration than the traditional\nalgorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1\">Jose Ramon Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nordling_T/0/1/0/all/0/1\">Torbj&#xf6;rn E.M. Nordling</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Constrained Gradient Descent: A Powerful and Principled Evasion Attack Against Neural Networks. (arXiv:2112.14232v1 [cs.LG])","link":"http://arxiv.org/abs/2112.14232","description":"<p>Minimal adversarial perturbations added to inputs have been shown to be\neffective at fooling deep neural networks. In this paper, we introduce several\ninnovations that make white-box targeted attacks follow the intuition of the\nattacker's goal: to trick the model to assign a higher probability to the\ntarget class than to any other, while staying within a specified distance from\nthe original input. First, we propose a new loss function that explicitly\ncaptures the goal of targeted attacks, in particular, by using the logits of\nall classes instead of just a subset, as is common. We show that Auto-PGD with\nthis loss function finds more adversarial examples than it does with other\ncommonly used loss functions. Second, we propose a new attack method that uses\na further developed version of our loss function capturing both the\nmisclassification objective and the $L_{\\infty}$ distance limit $\\epsilon$.\nThis new attack method is relatively 1.5--4.2% more successful on the CIFAR10\ndataset and relatively 8.2--14.9% more successful on the ImageNet dataset, than\nthe next best state-of-the-art attack. We confirm using statistical tests that\nour attack outperforms state-of-the-art attacks on different datasets and\nvalues of $\\epsilon$ and against different defenses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Weiran Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucas_K/0/1/0/all/0/1\">Keane Lucas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bauer_L/0/1/0/all/0/1\">Lujo Bauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reiter_M/0/1/0/all/0/1\">Michael K. Reiter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharif_M/0/1/0/all/0/1\">Mahmood Sharif</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AdaFocus V2: End-to-End Training of Spatial Dynamic Networks for Video Recognition. (arXiv:2112.14238v1 [cs.CV])","link":"http://arxiv.org/abs/2112.14238","description":"<p>Recent works have shown that the computational efficiency of video\nrecognition can be significantly improved by reducing the spatial redundancy.\nAs a representative work, the adaptive focus method (AdaFocus) has achieved a\nfavorable trade-off between accuracy and inference speed by dynamically\nidentifying and attending to the informative regions in each video frame.\nHowever, AdaFocus requires a complicated three-stage training pipeline\n(involving reinforcement learning), leading to slow convergence and is\nunfriendly to practitioners. This work reformulates the training of AdaFocus as\na simple one-stage algorithm by introducing a differentiable\ninterpolation-based patch selection operation, enabling efficient end-to-end\noptimization. We further present an improved training scheme to address the\nissues introduced by the one-stage formulation, including the lack of\nsupervision, input diversity and training stability. Moreover, a\nconditional-exit technique is proposed to perform temporal adaptive computation\non top of AdaFocus without additional training. Extensive experiments on six\nbenchmark datasets (i.e., ActivityNet, FCVID, Mini-Kinetics,\nSomething-Something V1&amp;V2, and Jester) demonstrate that our model significantly\noutperforms the original AdaFocus and other competitive baselines, while being\nconsiderably more simple and efficient to train. Code is available at\nhttps://github.com/LeapLabTHU/AdaFocusV2.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yulin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_Y/0/1/0/all/0/1\">Yang Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yuanze Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Haojun Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_Z/0/1/0/all/0/1\">Zihang Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulikov_V/0/1/0/all/0/1\">Victor Kulikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orlov_N/0/1/0/all/0/1\">Nikita Orlov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Humphrey Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Gao Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TAGPerson: A Target-Aware Generation Pipeline for Person Re-identification. (arXiv:2112.14239v1 [cs.CV])","link":"http://arxiv.org/abs/2112.14239","description":"<p>Nowadays, real data in person re-identification (ReID) task is facing privacy\nissues, e.g., the banned dataset DukeMTMC-ReID. Thus it becomes much harder to\ncollect real data for ReID task. Meanwhile, the labor cost of labeling ReID\ndata is still very high and further hinders the development of the ReID\nresearch. Therefore, many methods turn to generate synthetic images for ReID\nalgorithms as alternatives instead of real images. However, there is an\ninevitable domain gap between synthetic and real images. In previous methods,\nthe generation process is based on virtual scenes, and their synthetic training\ndata can not be changed according to different target real scenes\nautomatically. To handle this problem, we propose a novel Target-Aware\nGeneration pipeline to produce synthetic person images, called TAGPerson.\nSpecifically, it involves a parameterized rendering method, where the\nparameters are controllable and can be adjusted according to target scenes. In\nTAGPerson, we extract information from target scenes and use them to control\nour parameterized rendering process to generate target-aware synthetic images,\nwhich would hold a smaller gap to the real images in the target domain. In our\nexperiments, our target-aware synthetic images can achieve a much higher\nperformance than the generalized synthetic images on MSMT17, i.e. 47.5% vs.\n40.9% for rank-1 accuracy. We will release this toolkit\\footnote{\\noindent Code\nis available at\n\\href{https://github.com/tagperson/tagperson-blender}{https://github.com/tagperson/tagperson-blender}}\nfor the ReID community to generate synthetic images at any desired taste.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weihua Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1\">Tao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_R/0/1/0/all/0/1\">Rong Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiuyu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yuchen Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_G/0/1/0/all/0/1\">Guiguang Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal perception for dexterous manipulation. (arXiv:2112.14298v1 [cs.CV])","link":"http://arxiv.org/abs/2112.14298","description":"<p>Humans usually perceive the world in a multimodal way that vision, touch,\nsound are utilised to understand surroundings from various dimensions. These\nsenses are combined together to achieve a synergistic effect where the learning\nis more effectively than using each sense separately. For robotics, vision and\ntouch are two key senses for the dexterous manipulation. Vision usually gives\nus apparent features like shape, color, and the touch provides local\ninformation such as friction, texture, etc. Due to the complementary properties\nbetween visual and tactile senses, it is desirable for us to combine vision and\ntouch for a synergistic perception and manipulation. Many researches have been\ninvestigated about multimodal perception such as cross-modal learning, 3D\nreconstruction, multimodal translation with vision and touch. Specifically, we\npropose a cross-modal sensory data generation framework for the translation\nbetween vision and touch, which is able to generate realistic pseudo data. By\nusing this cross-modal translation method, it is desirable for us to make up\ninaccessible data, helping us to learn the object's properties from different\nviews. Recently, the attention mechanism becomes a popular method either in\nvisual perception or in tactile perception. We propose a spatio-temporal\nattention model for tactile texture recognition, which takes both spatial\nfeatures and time dimension into consideration. Our proposed method not only\npays attention to the salient features in each spatial feature, but also models\nthe temporal correlation in the through the time. The obvious improvement\nproves the efficiency of our selective attention mechanism. The spatio-temporal\nattention method has potential in many applications such as grasping,\nrecognition, and multimodal perception.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_G/0/1/0/all/0/1\">Guanqun Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1\">Shan Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepAdversaries: Examining the Robustness of Deep Learning Models for Galaxy Morphology Classification. (arXiv:2112.14299v1 [cs.LG])","link":"http://arxiv.org/abs/2112.14299","description":"<p>Data processing and analysis pipelines in cosmological survey experiments\nintroduce data perturbations that can significantly degrade the performance of\ndeep learning-based models. Given the increased adoption of supervised deep\nlearning methods for processing and analysis of cosmological survey data, the\nassessment of data perturbation effects and the development of methods that\nincrease model robustness are increasingly important. In the context of\nmorphological classification of galaxies, we study the effects of perturbations\nin imaging data. In particular, we examine the consequences of using neural\nnetworks when training on baseline data and testing on perturbed data. We\nconsider perturbations associated with two primary sources: 1) increased\nobservational noise as represented by higher levels of Poisson noise and 2)\ndata processing noise incurred by steps such as image compression or telescope\nerrors as represented by one-pixel adversarial attacks. We also test the\nefficacy of domain adaptation techniques in mitigating the perturbation-driven\nerrors. We use classification accuracy, latent space visualizations, and latent\nspace distance to assess model robustness. Without domain adaptation, we find\nthat processing pixel-level errors easily flip the classification into an\nincorrect class and that higher observational noise makes the model trained on\nlow-noise data unable to classify galaxy morphologies. On the other hand, we\nshow that training with domain adaptation improves model robustness and\nmitigates the effects of these perturbations, improving the classification\naccuracy by 23% on data with higher observational noise. Domain adaptation also\nincreases by a factor of ~2.3 the latent space distance between the baseline\nand the incorrectly classified one-pixel perturbed image, making the model more\nrobust to inadvertent perturbations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ciprijanovic_A/0/1/0/all/0/1\">Aleksandra &#x106;iprijanovi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kafkes_D/0/1/0/all/0/1\">Diana Kafkes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snyder_G/0/1/0/all/0/1\">Gregory Snyder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_F/0/1/0/all/0/1\">F. Javier S&#xe1;nchez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perdue_G/0/1/0/all/0/1\">Gabriel Nathan Perdue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pedro_K/0/1/0/all/0/1\">Kevin Pedro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nord_B/0/1/0/all/0/1\">Brian Nord</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madireddy_S/0/1/0/all/0/1\">Sandeep Madireddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wild_S/0/1/0/all/0/1\">Stefan M. Wild</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FRIDA -- Generative Feature Replay for Incremental Domain Adaptation. (arXiv:2112.14316v1 [cs.CV])","link":"http://arxiv.org/abs/2112.14316","description":"<p>We tackle the novel problem of incremental unsupervised domain adaptation\n(IDA) in this paper. We assume that a labeled source domain and different\nunlabeled target domains are incrementally observed with the constraint that\ndata corresponding to the current domain is only available at a time. The goal\nis to preserve the accuracies for all the past domains while generalizing well\nfor the current domain. The IDA setup suffers due to the abrupt differences\namong the domains and the unavailability of past data including the source\ndomain. Inspired by the notion of generative feature replay, we propose a novel\nframework called Feature Replay based Incremental Domain Adaptation (FRIDA)\nwhich leverages a new incremental generative adversarial network (GAN) called\ndomain-generic auxiliary classification GAN (DGAC-GAN) for producing\ndomain-specific feature representations seamlessly. For domain alignment, we\npropose a simple extension of the popular domain adversarial neural network\n(DANN) called DANN-IB which encourages discriminative domain-invariant and\ntask-relevant feature learning. Experimental results on Office-Home,\nOffice-CalTech, and DomainNet datasets confirm that FRIDA maintains superior\nstability-plasticity trade-off than the literature.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rakshit_S/0/1/0/all/0/1\">Sayan Rakshit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohanty_A/0/1/0/all/0/1\">Anwesh Mohanty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chavhan_R/0/1/0/all/0/1\">Ruchika Chavhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_B/0/1/0/all/0/1\">Biplab Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roig_G/0/1/0/all/0/1\">Gemma Roig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhuri_S/0/1/0/all/0/1\">Subhasis Chaudhuri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Brain Tumor Classification by Cascaded Multiscale Multitask Learning Framework Based on Feature Aggregation. (arXiv:2112.14320v1 [eess.IV])","link":"http://arxiv.org/abs/2112.14320","description":"<p>Brain tumor analysis in MRI images is a significant and challenging issue\nbecause misdiagnosis can lead to death. Diagnosis and evaluation of brain\ntumors in the early stages increase the probability of successful treatment.\nHowever, the complexity and variety of tumors, shapes, and locations make their\nsegmentation and classification complex. In this regard, numerous researchers\nhave proposed brain tumor segmentation and classification methods. This paper\npresents an approach that simultaneously segments and classifies brain tumors\nin MRI images using a framework that contains MRI image enhancement and tumor\nregion detection. Eventually, a network based on a multitask learning approach\nis proposed. Subjective and objective results indicate that the segmentation\nand classification results based on evaluation metrics are better or comparable\nto the state-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Sobhaninia_Z/0/1/0/all/0/1\">Zahra Sobhaninia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Karimi_N/0/1/0/all/0/1\">Nader Karimi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khadivi_P/0/1/0/all/0/1\">Pejman Khadivi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Samavi_S/0/1/0/all/0/1\">Shadrokh Samavi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Head Deep Metric Learning Using Global and Local Representations. (arXiv:2112.14327v1 [cs.CV])","link":"http://arxiv.org/abs/2112.14327","description":"<p>Deep Metric Learning (DML) models often require strong local and global\nrepresentations, however, effective integration of local and global features in\nDML model training is a challenge. DML models are often trained with specific\nloss functions, including pairwise-based and proxy-based losses. The\npairwise-based loss functions leverage rich semantic relations among data\npoints, however, they often suffer from slow convergence during DML model\ntraining. On the other hand, the proxy-based loss functions often lead to\nsignificant speedups in convergence during training, while the rich relations\namong data points are often not fully explored by the proxy-based losses. In\nthis paper, we propose a novel DML approach to address these challenges. The\nproposed DML approach makes use of a hybrid loss by integrating the\npairwise-based and the proxy-based loss functions to leverage rich data-to-data\nrelations as well as fast convergence. Furthermore, the proposed DML approach\nutilizes both global and local features to obtain rich representations in DML\nmodel training. Finally, we also use the second-order attention for feature\nenhancement to improve accurate and efficient retrieval. In our experiments, we\nextensively evaluated the proposed DML approach on four public benchmarks, and\nthe experimental results demonstrate that the proposed method achieved\nstate-of-the-art performance on all benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ebrahimpour_M/0/1/0/all/0/1\">Mohammad K. Ebrahimpour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_G/0/1/0/all/0/1\">Gang Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beach_A/0/1/0/all/0/1\">Allison Beach</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"360{\\deg} Optical Flow using Tangent Images. (arXiv:2112.14331v1 [cs.CV])","link":"http://arxiv.org/abs/2112.14331","description":"<p>Omnidirectional 360{\\deg} images have found many promising and exciting\napplications in computer vision, robotics and other fields, thanks to their\nincreasing affordability, portability and their 360{\\deg} field of view. The\nmost common format for storing, processing and visualising 360{\\deg} images is\nequirectangular projection (ERP). However, the distortion introduced by the\nnonlinear mapping from 360{\\deg} image to ERP image is still a barrier that\nholds back ERP images from being used as easily as conventional perspective\nimages. This is especially relevant when estimating 360{\\deg} optical flow, as\nthe distortions need to be mitigated appropriately. In this paper, we propose a\n360{\\deg} optical flow method based on tangent images. Our method leverages\ngnomonic projection to locally convert ERP images to perspective images, and\nuniformly samples the ERP image by projection to a cubemap and regular\nicosahedron vertices, to incrementally refine the estimated 360{\\deg} flow\nfields even in the presence of large rotations. Our experiments demonstrate the\nbenefits of our proposed method both quantitatively and qualitatively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_M/0/1/0/all/0/1\">Mingze Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Richardt_C/0/1/0/all/0/1\">Christian Richardt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Closer Look at the Transferability of Adversarial Examples: How They Fool Different Models Differently. (arXiv:2112.14337v1 [cs.LG])","link":"http://arxiv.org/abs/2112.14337","description":"<p>Deep neural networks are vulnerable to adversarial examples (AEs), which have\nadversarial transferability: AEs generated for the source model can mislead\nanother (target) model's predictions. However, the transferability has not been\nunderstood from the perspective of to which class target model's predictions\nwere misled (i.e., class-aware transferability). In this paper, we\ndifferentiate the cases in which a target model predicts the same wrong class\nas the source model (\"same mistake\") or a different wrong class (\"different\nmistake\") to analyze and provide an explanation of the mechanism. First, our\nanalysis shows (1) that same mistakes correlate with \"non-targeted\ntransferability\" and (2) that different mistakes occur between similar models\nregardless of the perturbation size. Second, we present evidence that the\ndifference in same and different mistakes can be explained by non-robust\nfeatures, predictive but human-uninterpretable patterns: different mistakes\noccur when non-robust features in AEs are used differently by models.\nNon-robust features can thus provide consistent explanations for the\nclass-aware transferability of AEs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Waseda_F/0/1/0/all/0/1\">Futa Waseda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nishikawa_S/0/1/0/all/0/1\">Sosuke Nishikawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1\">Trung-Nghia Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Huy H. Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Echizen_I/0/1/0/all/0/1\">Isao Echizen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Super-Efficient Super Resolution for Fast Adversarial Defense at the Edge. (arXiv:2112.14340v1 [eess.IV])","link":"http://arxiv.org/abs/2112.14340","description":"<p>Autonomous systems are highly vulnerable to a variety of adversarial attacks\non Deep Neural Networks (DNNs). Training-free model-agnostic defenses have\nrecently gained popularity due to their speed, ease of deployment, and ability\nto work across many DNNs. To this end, a new technique has emerged for\nmitigating attacks on image classification DNNs, namely, preprocessing\nadversarial images using super resolution -- upscaling low-quality inputs into\nhigh-resolution images. This defense requires running both image classifiers\nand super resolution models on constrained autonomous systems. However, super\nresolution incurs a heavy computational cost. Therefore, in this paper, we\ninvestigate the following question: Does the robustness of image classifiers\nsuffer if we use tiny super resolution models? To answer this, we first review\na recent work called Super-Efficient Super Resolution (SESR) that achieves\nsimilar or better image quality than prior art while requiring 2x to 330x fewer\nMultiply-Accumulate (MAC) operations. We demonstrate that despite being orders\nof magnitude smaller than existing models, SESR achieves the same level of\nrobustness as significantly larger networks. Finally, we estimate end-to-end\nperformance of super resolution-based defenses on a commercial Arm Ethos-U55\nmicro-NPU. Our findings show that SESR achieves nearly 3x higher FPS than a\nbaseline while achieving similar robustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bhardwaj_K/0/1/0/all/0/1\">Kartikeya Bhardwaj</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gope_D/0/1/0/all/0/1\">Dibakar Gope</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ward_J/0/1/0/all/0/1\">James Ward</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Whatmough_P/0/1/0/all/0/1\">Paul Whatmough</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Loh_D/0/1/0/all/0/1\">Danny Loh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Background-aware Classification Activation Map for Weakly Supervised Object Localization. (arXiv:2112.14379v1 [cs.CV])","link":"http://arxiv.org/abs/2112.14379","description":"<p>Weakly supervised object localization (WSOL) relaxes the requirement of dense\nannotations for object localization by using image-level classification masks\nto supervise its learning process. However, current WSOL methods suffer from\nexcessive activation of background locations and need post-processing to obtain\nthe localization mask. This paper attributes these issues to the unawareness of\nbackground cues, and propose the background-aware classification activation map\n(B-CAM) to simultaneously learn localization scores of both object and\nbackground with only image-level labels. In our B-CAM, two image-level\nfeatures, aggregated by pixel-level features of potential background and object\nlocations, are used to purify the object feature from the object-related\nbackground and to represent the feature of the pure-background sample,\nrespectively. Then based on these two features, both the object classifier and\nthe background classifier are learned to determine the binary object\nlocalization mask. Our B-CAM can be trained in end-to-end manner based on a\nproposed stagger classification loss, which not only improves the objects\nlocalization but also suppresses the background activation. Experiments show\nthat our B-CAM outperforms one-stage WSOL methods on the CUB-200, OpenImages\nand VOC2012 datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Lei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+She_Q/0/1/0/all/0/1\">Qi She</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1\">Xiangxi Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_M/0/1/0/all/0/1\">Mufeng Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1\">Lujia Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zhe Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_B/0/1/0/all/0/1\">Bin Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1\">Yunfei You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yibao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Q/0/1/0/all/0/1\">Qiushi Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yanye Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Domain Empirical Risk Minimization for Unbiased Long-tailed Classification. (arXiv:2112.14380v1 [cs.CV])","link":"http://arxiv.org/abs/2112.14380","description":"<p>We address the overlooked unbiasedness in existing long-tailed classification\nmethods: we find that their overall improvement is mostly attributed to the\nbiased preference of tail over head, as the test distribution is assumed to be\nbalanced; however, when the test is as imbalanced as the long-tailed training\ndata -- let the test respect Zipf's law of nature -- the tail bias is no longer\nbeneficial overall because it hurts the head majorities. In this paper, we\npropose Cross-Domain Empirical Risk Minimization (xERM) for training an\nunbiased model to achieve strong performances on both test distributions, which\nempirically demonstrates that xERM fundamentally improves the classification by\nlearning better feature representation rather than the head vs. tail game.\nBased on causality, we further theoretically explain why xERM achieves\nunbiasedness: the bias caused by the domain selection is removed by adjusting\nthe empirical risks on the imbalanced domain and the balanced but unseen\ndomain. Codes are available at https://github.com/BeierZhu/xERM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1\">Beier Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_Y/0/1/0/all/0/1\">Yulei Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_X/0/1/0/all/0/1\">Xian-Sheng Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hanwang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COTReg:Coupled Optimal Transport based Point Cloud Registration. (arXiv:2112.14381v1 [cs.CV])","link":"http://arxiv.org/abs/2112.14381","description":"<p>Generating a set of high-quality correspondences or matches is one of the\nmost critical steps in point cloud registration. This paper proposes a learning\nframework COTReg by jointly considering the pointwise and structural matchings\nto predict correspondences of 3D point cloud registration. Specifically, we\ntransform the two matchings into a Wasserstein distance-based and a\nGromov-Wasserstein distance-based optimizations, respectively. Thus the task of\nestablishing the correspondences can be naturally reshaped to a coupled optimal\ntransport problem. Furthermore, we design a network to predict the confidence\nscore of being an inlier for each point of the point clouds, which provides the\noverlap region information to generate correspondences. Our correspondence\nprediction pipeline can be easily integrated into either learning-based\nfeatures like FCGF or traditional descriptors like FPFH. We conducted\ncomprehensive experiments on 3DMatch, KITTI, 3DCSR, and ModelNet40 benchmarks,\nshowing the state-of-art performance of the proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mei_G/0/1/0/all/0/1\">Guofeng Mei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaoshui Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Litao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bennamoun_M/0/1/0/all/0/1\">Mohammed Bennamoun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Robustifying Guidance for Monocular 3D Face Reconstruction. (arXiv:2112.14382v1 [cs.CV])","link":"http://arxiv.org/abs/2112.14382","description":"<p>Despite the recent developments in 3D Face Reconstruction from occluded and\nnoisy face images, the performance is still unsatisfactory. One of the main\nchallenges is to handle moderate to heavy occlusions in the face images. In\naddition, the noise in the face images inhibits the correct capture of facial\nattributes, thus needing to be reliably addressed. Moreover, most existing\nmethods rely on additional dependencies, posing numerous constraints over the\ntraining procedure. Therefore, we propose a Self-Supervised RObustifying\nGUidancE (ROGUE) framework to obtain robustness against occlusions and noise in\nthe face images. The proposed network contains 1) the Guidance Pipeline to\nobtain the 3D face coefficients for the clean faces, and 2) the Robustification\nPipeline to acquire the consistency between the estimated coefficients for\noccluded or noisy images and the clean counterpart. The proposed image- and\nfeature-level loss functions aid the ROGUE learning process without posing\nadditional dependencies. On the three variations of the test dataset of CelebA:\nrational occlusions, delusional occlusions, and noisy face images, our method\noutperforms the current state-of-the-art method by large margins (e.g., for the\nshape-based 3D vertex errors, a reduction from 0.146 to 0.048 for rational\nocclusions, from 0.292 to 0.061 for delusional occlusions and from 0.269 to\n0.053 for the noise in the face images), demonstrating the effectiveness of the\nproposed approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tiwari_H/0/1/0/all/0/1\">Hitika Tiwari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Min-Hung Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsai_Y/0/1/0/all/0/1\">Yi-Min Tsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuo_H/0/1/0/all/0/1\">Hsien-Kai Kuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hung-Jen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jou_K/0/1/0/all/0/1\">Kevin Jou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkatesh_K/0/1/0/all/0/1\">K. S. Venkatesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yong-Sheng Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Overcoming Mode Collapse with Adaptive Multi Adversarial Training. (arXiv:2112.14406v1 [cs.CV])","link":"http://arxiv.org/abs/2112.14406","description":"<p>Generative Adversarial Networks (GANs) are a class of generative models used\nfor various applications, but they have been known to suffer from the mode\ncollapse problem, in which some modes of the target distribution are ignored by\nthe generator. Investigative study using a new data generation procedure\nindicates that the mode collapse of the generator is driven by the\ndiscriminator's inability to maintain classification accuracy on previously\nseen samples, a phenomenon called Catastrophic Forgetting in continual\nlearning. Motivated by this observation, we introduce a novel training\nprocedure that adaptively spawns additional discriminators to remember previous\nmodes of generation. On several datasets, we show that our training scheme can\nbe plugged-in to existing GAN frameworks to mitigate mode collapse and improve\nstandard metrics for GAN evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mangalam_K/0/1/0/all/0/1\">Karttikeya Mangalam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_R/0/1/0/all/0/1\">Rohin Garg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Invertible Image Dataset Protection. (arXiv:2112.14420v1 [cs.CV])","link":"http://arxiv.org/abs/2112.14420","description":"<p>Deep learning has achieved enormous success in various industrial\napplications. Companies do not want their valuable data to be stolen by\nmalicious employees to train pirated models. Nor do they wish the data analyzed\nby the competitors after using them online. We propose a novel solution for\ndataset protection in this scenario by robustly and reversibly transform the\nimages into adversarial images. We develop a reversible adversarial example\ngenerator (RAEG) that introduces slight changes to the images to fool\ntraditional classification models. Even though malicious attacks train pirated\nmodels based on the defensed versions of the protected images, RAEG can\nsignificantly weaken the functionality of these models. Meanwhile, the\nreversibility of RAEG ensures the performance of authorized models. Extensive\nexperiments demonstrate that RAEG can better protect the data with slight\ndistortion against adversarial defense than previous methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kejiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1\">Xianhan Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ying_Q/0/1/0/all/0/1\">Qichao Ying</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Z/0/1/0/all/0/1\">Zhenxing Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinpeng Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Color Image Steganography Based on Frequency Sub-band Selection. (arXiv:2112.14437v1 [cs.CR])","link":"http://arxiv.org/abs/2112.14437","description":"<p>Color image steganography based on deep learning is the art of hiding\ninformation in the color image. Among them, image hiding steganography(hiding\nimage with image) has attracted much attention in recent years because of its\ngreat steganographic capacity. However, images generated by image hiding\nsteganography may show some obvious color distortion or artificial texture\ntraces. We propose a color image steganographic model based on frequency\nsub-band selection to solve the above problems. Firstly, we discuss the\nrelationship between the characteristics of different color spaces/frequency\nsub-bands and the generated image quality. Then, we select the B channel of the\nRGB image as the embedding channel and the high-frequency sub-band as the\nembedding domain. DWT(discrete wavelet transformation) transforms B channel\ninformation and secret gray image into frequency domain information, and then\nthe secret image is embedded and extracted in the frequency domain.\nComprehensive experiments demonstrate that images generated by our model have\nbetter image quality, and the imperceptibility is significantly increased.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hai Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shuqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Songsen Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ACDNet: Adaptively Combined Dilated Convolution for Monocular Panorama Depth Estimation. (arXiv:2112.14440v1 [cs.CV])","link":"http://arxiv.org/abs/2112.14440","description":"<p>Depth estimation is a crucial step for 3D reconstruction with panorama images\nin recent years. Panorama images maintain the complete spatial information but\nintroduce distortion with equirectangular projection. In this paper, we propose\nan ACDNet based on the adaptively combined dilated convolution to predict the\ndense depth map for a monocular panoramic image. Specifically, we combine the\nconvolution kernels with different dilations to extend the receptive field in\nthe equirectangular projection. Meanwhile, we introduce an adaptive\nchannel-wise fusion module to summarize the feature maps and get diverse\nattention areas in the receptive field along the channels. Due to the\nutilization of channel-wise attention in constructing the adaptive channel-wise\nfusion module, the network can capture and leverage the cross-channel\ncontextual information efficiently. Finally, we conduct depth estimation\nexperiments on three datasets (both virtual and real-world) and the\nexperimental results demonstrate that our proposed ACDNet substantially\noutperforms the current state-of-the-art (SOTA) methods. Our codes and model\nparameters are accessed in https://github.com/zcq15/ACDNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_C/0/1/0/all/0/1\">Chuanqing Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhengda Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yiqun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jun Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Ying Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Feature Extraction for Generalized Zero-shot Learning. (arXiv:2112.14478v1 [cs.CV])","link":"http://arxiv.org/abs/2112.14478","description":"<p>Generalized zero-shot learning (GZSL) is a technique to train a deep learning\nmodel to identify unseen classes using the attribute. In this paper, we put\nforth a new GZSL technique that improves the GZSL classification performance\ngreatly. Key idea of the proposed approach, henceforth referred to as semantic\nfeature extraction-based GZSL (SE-GZSL), is to use the semantic feature\ncontaining only attribute-related information in learning the relationship\nbetween the image and the attribute. In doing so, we can remove the\ninterference, if any, caused by the attribute-irrelevant information contained\nin the image feature. To train a network extracting the semantic feature, we\npresent two novel loss functions, 1) mutual information-based loss to capture\nall the attribute-related information in the image feature and 2)\nsimilarity-based loss to remove unwanted attribute-irrelevant information. From\nextensive experiments using various datasets, we show that the proposed SE-GZSL\ntechnique outperforms conventional GZSL approaches by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Junhan Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shim_K/0/1/0/all/0/1\">Kyuhong Shim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shim_B/0/1/0/all/0/1\">Byonghyo Shim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Two-phase training mitigates class imbalance for camera trap image classification with CNNs. (arXiv:2112.14491v1 [cs.CV])","link":"http://arxiv.org/abs/2112.14491","description":"<p>By leveraging deep learning to automatically classify camera trap images,\necologists can monitor biodiversity conservation efforts and the effects of\nclimate change on ecosystems more efficiently. Due to the imbalanced\nclass-distribution of camera trap datasets, current models are biased towards\nthe majority classes. As a result, they obtain good performance for a few\nmajority classes but poor performance for many minority classes. We used\ntwo-phase training to increase the performance for these minority classes. We\ntrained, next to a baseline model, four models that implemented a different\nversions of two-phase training on a subset of the highly imbalanced Snapshot\nSerengeti dataset. Our results suggest that two-phase training can improve\nperformance for many minority classes, with limited loss in performance for the\nother classes. We find that two-phase training based on majority undersampling\nincreases class-specific F1-scores up to 3.0%. We also find that two-phase\ntraining outperforms using only oversampling or undersampling by 6.1% in\nF1-score on average. Finally, we find that a combination of over- and\nundersampling leads to a better performance than using them individually.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Malik_F/0/1/0/all/0/1\">Farjad Malik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wouters_S/0/1/0/all/0/1\">Simon Wouters</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cartuyvels_R/0/1/0/all/0/1\">Ruben Cartuyvels</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghadery_E/0/1/0/all/0/1\">Erfan Ghadery</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moens_M/0/1/0/all/0/1\">Marie-Francine Moens</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spatial Distribution Patterns of Clownfish in Recirculating Aquaculture Systems. (arXiv:2112.14513v1 [cs.CV])","link":"http://arxiv.org/abs/2112.14513","description":"<p>Monitoring and detecting fish behaviors provide essential information on fish\nwelfare and contribute to achieving intelligent production in global\naquaculture. This work proposes an efficient approach to analyze the spatial\ndistribution status and motion patterns of juvenile clownfish (Amphiprion\nbicinctus) maintained in aquaria at three stocking densities (1, 5, and 10\nindividuals/aquarium). The estimated displacement is the key factor in\nassessing the dispersion and velocity to express the clownfish's spatial\ndistribution and movement behavior in a recirculating aquaculture system.\nIndeed, we aim at computing the velocity, magnitude, and turning angle using an\noptical flow method to assist aquaculturists in efficiently monitoring and\nidentifying fish behavior. We test the system design on a database containing\ntwo days of video streams of juvenile clownfish maintained in aquaria. The\nproposed displacement estimation reveals good performance in measuring\nclownfish's motion and dispersion characteristics. Furthermore, we demonstrate\nthe effectiveness of the proposed technique for quantifying variation in\nclownfish activity levels between recordings taken in the morning and\nafternoon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aljehani_F/0/1/0/all/0/1\">Fahad Aljehani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+NDoye_I/0/1/0/all/0/1\">Ibrahima N&#x27;Doye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Justo_M/0/1/0/all/0/1\">Micaela S. Justo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majoris_J/0/1/0/all/0/1\">John E. Majoris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berumen_M/0/1/0/all/0/1\">Michael L. Berumen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laleg_Kirati_T/0/1/0/all/0/1\">Taous-Meriem Laleg-Kirati</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Res2NetFuse: A Fusion Method for Infrared and Visible Images. (arXiv:2112.14540v1 [cs.CV])","link":"http://arxiv.org/abs/2112.14540","description":"<p>This paper presents a novel Res2Net-based fusion framework for infrared and\nvisible images. The proposed fusion model has three parts: an encoder, a fusion\nlayer and a decoder, respectively. The Res2Net-based encoder is used to extract\nmulti-scale features of source images, the paper introducing a new training\nstrategy for training a Res2Net-based encoder that uses only a single image.\nThen, a new fusion strategy is developed based on the attention model. Finally,\nthe fused image is reconstructed by the decoder. The proposed approach is also\nanalyzed in detail. Experiments show that our method achieves state-of-the-art\nfusion performance in objective and subjective assessment by comparing with the\nexisting methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xu Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiao-Jun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jun Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palade_V/0/1/0/all/0/1\">Vasile Palade</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Onsite Non-Line-of-Sight Imaging via Online Calibrations. (arXiv:2112.14555v1 [eess.IV])","link":"http://arxiv.org/abs/2112.14555","description":"<p>There has been an increasing interest in deploying non-line-of-sight (NLOS)\nimaging systems for recovering objects behind an obstacle. Existing solutions\ngenerally pre-calibrate the system before scanning the hidden objects. Onsite\nadjustments of the occluder, object and scanning pattern require\nre-calibration. We present an online calibration technique that directly\ndecouples the acquired transients at onsite scanning into the LOS and hidden\ncomponents. We use the former to directly (re)calibrate the system upon changes\nof scene/obstacle configurations, scanning regions, and scanning patterns\nwhereas the latter for hidden object recovery via spatial, frequency or\nlearning based techniques. Our technique avoids using auxiliary calibration\napparatus such as mirrors or checkerboards and supports both laboratory\nvalidations and real-world deployments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Pan_Z/0/1/0/all/0/1\">Zhengqing Pan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_R/0/1/0/all/0/1\">Ruiqian Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gao_T/0/1/0/all/0/1\">Tian Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1\">Zi Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_P/0/1/0/all/0/1\">Ping Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shen_S/0/1/0/all/0/1\">Siyuan Shen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_T/0/1/0/all/0/1\">Tao Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_J/0/1/0/all/0/1\">Jingyi Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_S/0/1/0/all/0/1\">Shiying Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HPRN: Holistic Prior-embedded Relation Network for Spectral Super-Resolution. (arXiv:2112.14608v1 [eess.IV])","link":"http://arxiv.org/abs/2112.14608","description":"<p>Spectral super-resolution (SSR) refers to the hyperspectral image (HSI)\nrecovery from an RGB counterpart. Due to the one-to-many nature of the SSR\nproblem, a single RGB image can be reprojected to many HSIs. The key to tackle\nthis illposed problem is to plug into multi-source prior information such as\nthe natural RGB spatial context-prior, deep feature-prior or inherent HSI\nstatistical-prior, etc., so as to improve the confidence and fidelity of\nreconstructed spectra. However, most current approaches only consider the\ngeneral and limited priors in their designing the customized convolutional\nneural networks (CNNs), which leads to the inability to effectively alleviate\nthe degree of ill-posedness. To address the problematic issues, we propose a\nnovel holistic prior-embedded relation network (HPRN) for SSR. Basically, the\ncore framework is delicately assembled by several multi-residual relation\nblocks (MRBs) that fully facilitate the transmission and utilization of the\nlow-frequency content prior of RGB signals. Innovatively, the semantic prior of\nRGB input is introduced to identify category attributes and a semantic-driven\nspatial relation module (SSRM) is put forward to perform the feature\naggregation among the clustered similar characteristics using a\nsemantic-embedded relation matrix. Additionally, we develop a transformer-based\nchannel relation module (TCRM), which breaks the habit of employing scalars as\nthe descriptors of channel-wise relations in the previous deep feature-prior\nand replaces them with certain vectors, together with Transformerstyle feature\ninteractions, supporting the representations to be more discriminative. In\norder to maintain the mathematical correlation and spectral consistency between\nhyperspectral bands, the second-order prior constraints (SOPC) are incorporated\ninto the loss function to guide the HSI reconstruction process.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wu_C/0/1/0/all/0/1\">Chaoxiong Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Jiaojiao Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Song_R/0/1/0/all/0/1\">Rui Song</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yunsong Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Du_Q/0/1/0/all/0/1\">Qian Du</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Implementation of Convolutional Neural Network Architecture on 3D Multiparametric Magnetic Resonance Imaging for Prostate Cancer Diagnosis. (arXiv:2112.14644v1 [eess.IV])","link":"http://arxiv.org/abs/2112.14644","description":"<p>Prostate cancer is one of the most common causes of cancer deaths in men.\nThere is a growing demand for noninvasively and accurately diagnostic methods\nthat facilitate the current standard prostate cancer risk assessment in\nclinical practice. Still, developing computer-aided classification tools in\nprostate cancer diagnostics from multiparametric magnetic resonance images\ncontinues to be a challenge. In this work, we propose a novel deep learning\napproach for automatic classification of prostate lesions in the corresponding\nmagnetic resonance images by constructing a two-stage multimodal multi-stream\nconvolutional neural network (CNN)-based architecture framework. Without\nimplementing sophisticated image preprocessing steps or third-party software,\nour framework achieved the classification performance with the area under a\nReceiver Operating Characteristic (ROC) curve value of 0.87. The result\noutperformed most of the submitted methods and shared the highest value\nreported by the PROSTATEx Challenge organizer. Our proposed CNN-based framework\nreflects the potential of assisting medical image interpretation in prostate\ncancer and reducing unnecessary biopsies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lin_P/0/1/0/all/0/1\">Ping-Chang Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Szasz_T/0/1/0/all/0/1\">Teodora Szasz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Runesha_H/0/1/0/all/0/1\">Hakizumwami B. Runesha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Instability of Relative Pose Estimation and RANSAC's Role. (arXiv:2112.14651v1 [cs.CV])","link":"http://arxiv.org/abs/2112.14651","description":"<p>In this paper we study the numerical instabilities of the 5- and 7-point\nproblems for essential and fundamental matrix estimation in multiview geometry.\nIn both cases we characterize the ill-posed world scenes where the condition\nnumber for epipolar estimation is infinite. We also characterize the ill-posed\ninstances in terms of the given image data. To arrive at these results, we\npresent a general framework for analyzing the conditioning of minimal problems\nin multiview geometry, based on Riemannian manifolds. Experiments with\nsynthetic and real-world data then reveal a striking conclusion: that Random\nSample Consensus (RANSAC) in Structure-from-Motion (SfM) does not only serve to\nfilter out outliers, but RANSAC also selects for well-conditioned image data,\nsufficiently separated from the ill-posed locus that our theory predicts. Our\nfindings suggest that, in future work, one could try to accelerate and increase\nthe success of RANSAC by testing only well-conditioned image data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1\">Hongyi Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kileel_J/0/1/0/all/0/1\">Joe Kileel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kimia_B/0/1/0/all/0/1\">Benjamin Kimia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gendered Differences in Face Recognition Accuracy Explained by Hairstyles, Makeup, and Facial Morphology. (arXiv:2112.14656v1 [cs.CV])","link":"http://arxiv.org/abs/2112.14656","description":"<p>Media reports have accused face recognition of being ''biased'', ''sexist''\nand ''racist''. There is consensus in the research literature that face\nrecognition accuracy is lower for females, who often have both a higher false\nmatch rate and a higher false non-match rate. However, there is little\npublished research aimed at identifying the cause of lower accuracy for\nfemales. For instance, the 2019 Face Recognition Vendor Test that documents\nlower female accuracy across a broad range of algorithms and datasets also\nlists ''Analyze cause and effect'' under the heading ''What we did not do''. We\npresent the first experimental analysis to identify major causes of lower face\nrecognition accuracy for females on datasets where previous research has\nobserved this result. Controlling for equal amount of visible face in the test\nimages mitigates the apparent higher false non-match rate for females.\nAdditional analysis shows that makeup-balanced datasets further improves\nfemales to achieve lower false non-match rates. Finally, a clustering\nexperiment suggests that images of two different females are inherently more\nsimilar than of two different males, potentially accounting for a difference in\nfalse match rates.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Albiero_V/0/1/0/all/0/1\">V&#xed;tor Albiero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+King_M/0/1/0/all/0/1\">Michael C. King</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bowyer_K/0/1/0/all/0/1\">Kevin W. Bowyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MetaGraspNet: A Large-Scale Benchmark Dataset forVision-driven Robotic Grasping via Physics-basedMetaverse Synthesis. (arXiv:2112.14663v1 [cs.CV])","link":"http://arxiv.org/abs/2112.14663","description":"<p>There has been increasing interest in smart factories powered by robotics\nsystems to tackle repetitive, laborious tasks. One impactful yet challenging\ntask in robotics-powered smart factory applications is robotic grasping: using\nrobotic arms to grasp objects autonomously in different settings. Robotic\ngrasping requires a variety of computer vision tasks such as object detection,\nsegmentation, grasp prediction, pick planning, etc. While significant progress\nhas been made in leveraging of machine learning for robotic grasping,\nparticularly with deep learning, a big challenge remains in the need for\nlarge-scale, high-quality RGBD datasets that cover a wide diversity of\nscenarios and permutations. To tackle this big, diverse data problem, we are\ninspired by the recent rise in the concept of metaverse, which has greatly\nclosed the gap between virtual worlds and the physical world. Metaverses allow\nus to create digital twins of real-world manufacturing scenarios and to\nvirtually create different scenarios from which large volumes of data can be\ngenerated for training models. In this paper, we present MetaGraspNet: a\nlarge-scale benchmark dataset for vision-driven robotic grasping via\nphysics-based metaverse synthesis. The proposed dataset contains 100,000 images\nand 25 different object types and is split into 5 difficulties to evaluate\nobject detection and segmentation model performance in different grasping\nscenarios. We also propose a new layout-weighted performance metric alongside\nthe dataset for evaluating object detection and segmentation performance in a\nmanner that is more appropriate for robotic grasp applications compared to\nexisting general-purpose performance metrics. Our benchmark dataset is\navailable open-source on Kaggle, with the first phase consisting of detailed\nobject detection, segmentation, layout annotations, and a layout-weighted\nperformance metric script.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuhao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_E/0/1/0/all/0/1\">E. Zhixuan Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gilles_M/0/1/0/all/0/1\">Maximilian Gilles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1\">Alexander Wong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StyleGAN-V: A Continuous Video Generator with the Price, Image Quality and Perks of StyleGAN2. (arXiv:2112.14683v1 [cs.CV])","link":"http://arxiv.org/abs/2112.14683","description":"<p>Videos show continuous events, yet most - if not all - video synthesis\nframeworks treat them discretely in time. In this work, we think of videos of\nwhat they should be - time-continuous signals, and extend the paradigm of\nneural representations to build a continuous-time video generator. For this, we\nfirst design continuous motion representations through the lens of positional\nembeddings. Then, we explore the question of training on very sparse videos and\ndemonstrate that a good generator can be learned by using as few as 2 frames\nper clip. After that, we rethink the traditional image and video discriminators\npair and propose to use a single hypernetwork-based one. This decreases the\ntraining cost and provides richer learning signal to the generator, making it\npossible to train directly on 1024$^2$ videos for the first time. We build our\nmodel on top of StyleGAN2 and it is just 5% more expensive to train at the same\nresolution while achieving almost the same image quality. Moreover, our latent\nspace features similar properties, enabling spatial manipulations that our\nmethod can propagate in time. We can generate arbitrarily long videos at\narbitrary high frame rate, while prior work struggles to generate even 64\nframes at a fixed rate. Our model achieves state-of-the-art results on four\nmodern 256$^2$ video synthesis benchmarks and one 1024$^2$ resolution one.\nVideos and the source code are available at the project website:\nhttps://universome.github.io/stylegan-v.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Skorokhodov_I/0/1/0/all/0/1\">Ivan Skorokhodov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tulyakov_S/0/1/0/all/0/1\">Sergey Tulyakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elhoseiny_M/0/1/0/all/0/1\">Mohamed Elhoseiny</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Disentanglement and Generalization Under Correlation Shifts. (arXiv:2112.14754v1 [cs.LG])","link":"http://arxiv.org/abs/2112.14754","description":"<p>Correlations between factors of variation are prevalent in real-world data.\nMachine learning algorithms may benefit from exploiting such correlations, as\nthey can increase predictive performance on noisy data. However, often such\ncorrelations are not robust (e.g., they may change between domains, datasets,\nor applications) and we wish to avoid exploiting them. Disentanglement methods\naim to learn representations which capture different factors of variation in\nlatent subspaces. A common approach involves minimizing the mutual information\nbetween latent subspaces, such that each encodes a single underlying attribute.\nHowever, this fails when attributes are correlated. We solve this problem by\nenforcing independence between subspaces conditioned on the available\nattributes, which allows us to remove only dependencies that are not due to the\ncorrelation structure present in the training data. We achieve this via an\nadversarial approach to minimize the conditional mutual information (CMI)\nbetween subspaces with respect to categorical variables. We first show\ntheoretically that CMI minimization is a good objective for robust\ndisentanglement on linear problems with Gaussian data. We then apply our method\non real-world datasets based on MNIST and CelebA, and show that it yields\nmodels that are disentangled and robust under correlation shift, including in\nweakly supervised settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Funke_C/0/1/0/all/0/1\">Christina M. Funke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vicol_P/0/1/0/all/0/1\">Paul Vicol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kuan-Chieh Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kummerer_M/0/1/0/all/0/1\">Matthias K&#xfc;mmerer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zemel_R/0/1/0/all/0/1\">Richard Zemel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bethge_M/0/1/0/all/0/1\">Matthias Bethge</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Simple Baseline for Zero-shot Semantic Segmentation with Pre-trained Vision-language Model. (arXiv:2112.14757v1 [cs.CV])","link":"http://arxiv.org/abs/2112.14757","description":"<p>Recently, zero-shot image classification by vision-language pre-training has\ndemonstrated incredible achievements, that the model can classify arbitrary\ncategory without seeing additional annotated images of that category. However,\nit is still unclear how to make the zero-shot recognition working well on\nbroader vision problems, such as object detection and semantic segmentation. In\nthis paper, we target for zero-shot semantic segmentation, by building it on an\noff-the-shelf pre-trained vision-language model, i.e., CLIP. It is difficult\nbecause semantic segmentation and the CLIP model perform on different visual\ngranularity, that semantic segmentation processes on pixels while CLIP performs\non images. To remedy the discrepancy on processing granularity, we refuse the\nuse of the prevalent one-stage FCN based framework, and advocate a two-stage\nsemantic segmentation framework, with the first stage extracting generalizable\nmask proposals and the second stage leveraging an image based CLIP model to\nperform zero-shot classification on the masked image crops which are generated\nin the first stage. Our experimental results show that this simple framework\nsurpasses previous state-of-the-arts by a large margin: +29.5 hIoU on the\nPascal VOC 2012 dataset, and +8.9 hIoU on the COCO Stuff dataset. With its\nsimplicity and strong performance, we hope this framework to serve as a\nbaseline to facilitate the future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mengde Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Fangyun Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yutong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yue Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Han Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xiang Bai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Task-Oriented Convex Bilevel Optimization with Latent Feasibility. (arXiv:1907.03083v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1907.03083","description":"<p>This paper firstly proposes a convex bilevel optimization paradigm to\nformulate and optimize popular learning and vision problems in real-world\nscenarios. Different from conventional approaches, which directly design their\niteration schemes based on given problem formulation, we introduce a\ntask-oriented energy as our latent constraint which integrates richer task\ninformation. By explicitly re-characterizing the feasibility, we establish an\nefficient and flexible algorithmic framework to tackle convex models with both\nshrunken solution space and powerful auxiliary (based on domain knowledge and\ndata distribution of the task). In theory, we present the convergence analysis\nof our latent feasibility re-characterization based numerical strategy. We also\nanalyze the stability of the theoretical convergence under computational error\nperturbation. Extensive numerical experiments are conducted to verify our\ntheoretical findings and evaluate the practical performance of our method on\ndifferent applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Risheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Long Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xiaoming Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_S/0/1/0/all/0/1\">Shangzhi Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jin Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Histogram Layers for Texture Analysis. (arXiv:2001.00215v12 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2001.00215","description":"<p>An essential aspect of texture analysis is the extraction of features that\ndescribe the distribution of values in local, spatial regions. We present a\nlocalized histogram layer for artificial neural networks. Instead of computing\nglobal histograms as done previously, the proposed histogram layer directly\ncomputes the local, spatial distribution of features for texture analysis and\nparameters for the layer are estimated during backpropagation. We compare our\nmethod with state-of-the-art texture encoding methods such as the Deep Encoding\nNetwork Pooling, Deep Texture Encoding Network, Fisher Vector convolutional\nneural network, and Multi-level Texture Encoding and Representation on three\nmaterial/texture datasets: (1) the Describable Texture Dataset; (2) an\nextension of the ground terrain in outdoor scenes; (3) and a subset of the\nMaterials in Context dataset. Results indicate that the inclusion of the\nproposed histogram layer improves performance. The source code for the\nhistogram layer is publicly available:\nhttps://github.com/GatorSense/Histogram_Layer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peeples_J/0/1/0/all/0/1\">Joshua Peeples</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Weihuang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zare_A/0/1/0/all/0/1\">Alina Zare</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TPPO: A Novel Trajectory Predictor with Pseudo Oracle. (arXiv:2002.01852v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2002.01852","description":"<p>Forecasting pedestrian trajectories in dynamic scenes remains a critical\nproblem in various applications, such as autonomous driving and socially aware\nrobots. Such forecasting is challenging due to human-human and human-object\ninteractions and future uncertainties caused by human randomness. Generative\nmodel-based methods handle future uncertainties by sampling a latent variable.\nHowever, few studies explored the generation of the latent variable. In this\nwork, we propose the Trajectory Predictor with Pseudo Oracle (TPPO), which is a\ngenerative model-based trajectory predictor. The first pseudo oracle is\npedestrians' moving directions, and the second one is the latent variable\nestimated from ground truth trajectories. A social attention module is used to\naggregate neighbors' interactions based on the correlation between pedestrians'\nmoving directions and future trajectories. This correlation is inspired by the\nfact that pedestrians' future trajectories are often influenced by pedestrians\nin front. A latent variable predictor is proposed to estimate latent variable\ndistributions from observed and ground-truth trajectories. Moreover, the gap\nbetween these two distributions is minimized during training. Therefore, the\nlatent variable predictor can estimate the latent variable from observed\ntrajectories to approximate that estimated from ground-truth trajectories. We\ncompare the performance of TPPO with related methods on several public\ndatasets. Results demonstrate that TPPO outperforms state-of-the-art methods\nwith low average and final displacement errors. The ablation study shows that\nthe prediction performance will not dramatically decrease as sampling times\ndecline during tests.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Biao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_C/0/1/0/all/0/1\">Caizhen He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_C/0/1/0/all/0/1\">Ching-yao Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaofeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yang Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improved Gradient based Adversarial Attacks for Quantized Networks. (arXiv:2003.13511v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2003.13511","description":"<p>Neural network quantization has become increasingly popular due to efficient\nmemory consumption and faster computation resulting from bitwise operations on\nthe quantized networks. Even though they exhibit excellent generalization\ncapabilities, their robustness properties are not well-understood. In this\nwork, we systematically study the robustness of quantized networks against\ngradient based adversarial attacks and demonstrate that these quantized models\nsuffer from gradient vanishing issues and show a fake sense of robustness. By\nattributing gradient vanishing to poor forward-backward signal propagation in\nthe trained network, we introduce a simple temperature scaling approach to\nmitigate this issue while preserving the decision boundary. Despite being a\nsimple modification to existing gradient based adversarial attacks, experiments\non multiple image classification datasets with multiple network architectures\ndemonstrate that our temperature scaled attacks obtain near-perfect success\nrate on quantized networks while outperforming original attacks on\nadversarially trained models as well as floating-point networks. Code is\navailable at https://github.com/kartikgupta-at-anu/attack-bnn.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_K/0/1/0/all/0/1\">Kartik Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ajanthan_T/0/1/0/all/0/1\">Thalaiyasingam Ajanthan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepSSM: Deep State-Space Model for 3D Human Motion Prediction. (arXiv:2005.12155v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2005.12155","description":"<p>Predicting future human motion plays a significant role in human-machine\ninteractions for various real-life applications. A unified formulation and\nmulti-order modeling are two critical perspectives for analyzing and\nrepresenting human motion. In contrast to prior works, we improve the\nmulti-order modeling ability of human motion systems for more accurate\npredictions by building a deep state-space model (DeepSSM). The DeepSSM\nutilizes the advantages of both the state-space theory and the deep network.\nSpecifically, we formulate the human motion system as the state-space model of\na dynamic system and model the motion system by the state-space theory,\noffering a unified formulation for diverse human motion systems. Moreover, a\nnovel deep network is designed to parameterize this system, which jointly\nmodels the state-state transition and state-observation transition processes.\nIn this way, the state of a system is updated by the multi-order information of\na time-varying human motion sequence. Multiple future poses are recursively\npredicted via the state-observation transition. To further improve the model\nability of the system, a novel loss, WT-MPJPE (Weighted Temporal Mean Per Joint\nPosition Error), is introduced to optimize the model. The proposed loss\nencourages the system to achieve more accurate predictions by increasing\nweights to the early time steps. The experiments on two benchmark datasets\n(i.e., Human3.6M and 3DPW) confirm that our method achieves state-of-the-art\nperformance with improved accuracy of at least 2.2mm per joint. The code will\nbe available at: \\url{https://github.com/lily2lab/DeepSSM.git}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaoli Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1\">Jianqin Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Huaping Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jun Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Calibration of Neural Networks using Splines. (arXiv:2006.12800v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2006.12800","description":"<p>Calibrating neural networks is of utmost importance when employing them in\nsafety-critical applications where the downstream decision making depends on\nthe predicted probabilities. Measuring calibration error amounts to comparing\ntwo empirical distributions. In this work, we introduce a binning-free\ncalibration measure inspired by the classical Kolmogorov-Smirnov (KS)\nstatistical test in which the main idea is to compare the respective cumulative\nprobability distributions. From this, by approximating the empirical cumulative\ndistribution using a differentiable function via splines, we obtain a\nrecalibration function, which maps the network outputs to actual (calibrated)\nclass assignment probabilities. The spine-fitting is performed using a held-out\ncalibration set and the obtained recalibration function is evaluated on an\nunseen test set. We tested our method against existing calibration approaches\non various image classification datasets and our spline-based recalibration\napproach consistently outperforms existing methods on KS error as well as other\ncommonly used calibration measures. Our Code is available at\nhttps://github.com/kartikgupta-at-anu/spline-calibration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_K/0/1/0/all/0/1\">Kartik Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahimi_A/0/1/0/all/0/1\">Amir Rahimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ajanthan_T/0/1/0/all/0/1\">Thalaiyasingam Ajanthan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mensink_T/0/1/0/all/0/1\">Thomas Mensink</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sminchisescu_C/0/1/0/all/0/1\">Cristian Sminchisescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hartley_R/0/1/0/all/0/1\">Richard Hartley</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ThreeDWorld: A Platform for Interactive Multi-Modal Physical Simulation. (arXiv:2007.04954v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2007.04954","description":"<p>We introduce ThreeDWorld (TDW), a platform for interactive multi-modal\nphysical simulation. TDW enables simulation of high-fidelity sensory data and\nphysical interactions between mobile agents and objects in rich 3D\nenvironments. Unique properties include: real-time near-photo-realistic image\nrendering; a library of objects and environments, and routines for their\ncustomization; generative procedures for efficiently building classes of new\nenvironments; high-fidelity audio rendering; realistic physical interactions\nfor a variety of material types, including cloths, liquid, and deformable\nobjects; customizable agents that embody AI agents; and support for human\ninteractions with VR devices. TDW's API enables multiple agents to interact\nwithin a simulation and returns a range of sensor and physics data representing\nthe state of the world. We present initial experiments enabled by TDW in\nemerging research directions in computer vision, machine learning, and\ncognitive science, including multi-modal physical scene understanding, physical\ndynamics predictions, multi-agent interactions, models that learn like a child,\nand attention studies in humans and neural networks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1\">Chuang Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_J/0/1/0/all/0/1\">Jeremy Schwartz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alter_S/0/1/0/all/0/1\">Seth Alter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mrowca_D/0/1/0/all/0/1\">Damian Mrowca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schrimpf_M/0/1/0/all/0/1\">Martin Schrimpf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Traer_J/0/1/0/all/0/1\">James Traer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freitas_J/0/1/0/all/0/1\">Julian De Freitas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kubilius_J/0/1/0/all/0/1\">Jonas Kubilius</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhandwaldar_A/0/1/0/all/0/1\">Abhishek Bhandwaldar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haber_N/0/1/0/all/0/1\">Nick Haber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sano_M/0/1/0/all/0/1\">Megumi Sano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kuno Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_E/0/1/0/all/0/1\">Elias Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lingelbach_M/0/1/0/all/0/1\">Michael Lingelbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Curtis_A/0/1/0/all/0/1\">Aidan Curtis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feigelis_K/0/1/0/all/0/1\">Kevin Feigelis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bear_D/0/1/0/all/0/1\">Daniel M. Bear</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gutfreund_D/0/1/0/all/0/1\">Dan Gutfreund</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cox_D/0/1/0/all/0/1\">David Cox</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1\">Antonio Torralba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DiCarlo_J/0/1/0/all/0/1\">James J. DiCarlo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1\">Joshua B. Tenenbaum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McDermott_J/0/1/0/all/0/1\">Josh H. McDermott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamins_D/0/1/0/all/0/1\">Daniel L.K. Yamins</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Robust and Reliable Point Cloud Recognition Network Under Rigid Transformation. (arXiv:2009.06903v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2009.06903","description":"<p>Point cloud recognition is an essential task in industrial robotics and\nautonomous driving. Recently, several point cloud processing models have\nachieved state-of-the-art performances. However, these methods lack rotation\nrobustness, and their performances degrade severely under random rotations,\nfailing to extend to real-world scenarios with varying orientations. To this\nend, we propose a method named Self Contour-based Transformation (SCT), which\ncan be flexibly integrated into various existing point cloud recognition models\nagainst arbitrary rotations. SCT provides efficient rotation and translation\ninvariance by introducing Contour-Aware Transformation (CAT), which linearly\ntransforms Cartesian coordinates of points to translation and\nrotation-invariant representations. We prove that CAT is a rotation and\ntranslation-invariant transformation based on the theoretical analysis.\nFurthermore, the Frame Alignment module is proposed to enhance discriminative\nfeature extraction by capturing contours and transforming self contour-based\nframes into intra-class frames. Extensive experimental results show that SCT\noutperforms the state-of-the-art approaches under arbitrary rotations in\neffectiveness and efficiency on synthetic and real-world benchmarks.\nFurthermore, the robustness and generality evaluations indicate that SCT is\nrobust and is applicable to various point cloud processing models, which\nhighlights the superiority of SCT in industrial applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Dongrui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chuanchuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Changqing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Q/0/1/0/all/0/1\">Qi Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_L/0/1/0/all/0/1\">Lei Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_F/0/1/0/all/0/1\">Fei Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_R/0/1/0/all/0/1\">Robert Caiming Qiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CHS-Net: A Deep learning approach for hierarchical segmentation of COVID-19 infected CT images. (arXiv:2012.07079v7 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2012.07079","description":"<p>The pandemic of novel SARS-CoV-2 also known as COVID-19 has been spreading\nworldwide, causing rampant loss of lives. Medical imaging such as CT, X-ray,\netc., plays a significant role in diagnosing the patients by presenting the\nvisual representation of the functioning of the organs. However, for any\nradiologist analyzing such scans is a tedious and time-consuming task. The\nemerging deep learning technologies have displayed its strength in analyzing\nsuch scans to aid in the faster diagnosis of the diseases and viruses such as\nCOVID-19. In the present article, an automated deep learning based model,\nCOVID-19 hierarchical segmentation network (CHS-Net) is proposed that functions\nas a semantic hierarchical segmenter to identify the COVID-19 infected regions\nfrom lungs contour via CT medical imaging using two cascaded residual attention\ninception U-Net (RAIU-Net) models. RAIU-Net comprises of a residual inception\nU-Net model with spectral spatial and depth attention network (SSD) that is\ndeveloped with the contraction and expansion phases of depthwise separable\nconvolutions and hybrid pooling (max and spectral pooling) to efficiently\nencode and decode the semantic and varying resolution information. The CHS-Net\nis trained with the segmentation loss function that is the defined as the\naverage of binary cross entropy loss and dice loss to penalize false negative\nand false positive predictions. The approach is compared with the recently\nproposed approaches and evaluated using the standard metrics like accuracy,\nprecision, specificity, recall, dice coefficient and Jaccard similarity along\nwith the visualized interpretation of the model prediction with GradCam++ and\nuncertainty maps. With extensive trials, it is observed that the proposed\napproach outperformed the recently proposed approaches and effectively segments\nthe COVID-19 infected regions in the lungs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Punn_N/0/1/0/all/0/1\">Narinder Singh Punn</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Agarwal_S/0/1/0/all/0/1\">Sonali Agarwal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Adversarial Inconsistent Cognitive Sampling for Multi-view Progressive Subspace Clustering. (arXiv:2101.03783v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.03783","description":"<p>Deep multi-view clustering methods have achieved remarkable performance.\nHowever, all of them failed to consider the difficulty labels (uncertainty of\nground-truth for training samples) over multi-view samples, which may result\ninto a nonideal clustering network for getting stuck into poor local optima\nduring training process; worse still, the difficulty labels from multi-view\nsamples are always inconsistent, such fact makes it even more challenging to\nhandle. In this paper, we propose a novel Deep Adversarial Inconsistent\nCognitive Sampling (DAICS) method for multi-view progressive subspace\nclustering. A multiview binary classification (easy or difficult) loss and a\nfeature similarity loss are proposed to jointly learn a binary classifier and a\ndeep consistent feature embedding network, throughout an adversarial minimax\ngame over difficulty labels of multiview consistent samples. We develop a\nmulti-view cognitive sampling strategy to select the input samples from easy to\ndifficult for multi-view clustering network training. However, the\ndistributions of easy and difficult samples are mixed together, hence not\ntrivial to achieve the goal. To resolve it, we define a sampling probability\nwith theoretical guarantee. Based on that, a golden section mechanism is\nfurther designed to generate a sample set boundary to progressively select the\nsamples with varied difficulty labels via a gate unit, which is utilized to\njointly learn a multi-view common progressive subspace and clustering network\nfor more efficient clustering. Experimental results on four real-world datasets\ndemonstrate the superiority of DAICS over the state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_R/0/1/0/all/0/1\">Renhao Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Balanced Softmax Cross-Entropy for Incremental Learning. (arXiv:2103.12532v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2103.12532","description":"<p>Deep neural networks are prone to catastrophic forgetting when incrementally\ntrained on new classes or new tasks as adaptation to the new data leads to a\ndrastic decrease of the performance on the old classes and tasks. By using a\nsmall memory for rehearsal and knowledge distillation, recent methods have\nproven to be effective to mitigate catastrophic forgetting. However due to the\nlimited size of the memory, large imbalance between the amount of data\navailable for the old and new classes still remains which results in a\ndeterioration of the overall accuracy of the model. To address this problem, we\npropose the use of the Balanced Softmax Cross-Entropy loss and show that it can\nbe combined with exiting methods for incremental learning to improve their\nperformances while also decreasing the computational cost of the training\nprocedure in some cases. Experiments on the competitive ImageNet, subImageNet\nand CIFAR100 datasets show states-of-the-art results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jodelet_Q/0/1/0/all/0/1\">Quentin Jodelet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murata_T/0/1/0/all/0/1\">Tsuyoshi Murata</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Best-Buddy GANs for Highly Detailed Image Super-Resolution. (arXiv:2103.15295v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2103.15295","description":"<p>We consider the single image super-resolution (SISR) problem, where a\nhigh-resolution (HR) image is generated based on a low-resolution (LR) input.\nRecently, generative adversarial networks (GANs) become popular to hallucinate\ndetails. Most methods along this line rely on a predefined single-LR-single-HR\nmapping, which is not flexible enough for the SISR task. Also, GAN-generated\nfake details may often undermine the realism of the whole image. We address\nthese issues by proposing best-buddy GANs (Beby-GAN) for rich-detail SISR.\nRelaxing the immutable one-to-one constraint, we allow the estimated patches to\ndynamically seek the best supervision during training, which is beneficial to\nproducing more reasonable details. Besides, we propose a region-aware\nadversarial learning strategy that directs our model to focus on generating\ndetails for textured areas adaptively. Extensive experiments justify the\neffectiveness of our method. An ultra-high-resolution 4K dataset is also\nconstructed to facilitate future super-resolution research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Li_W/0/1/0/all/0/1\">Wenbo Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_K/0/1/0/all/0/1\">Kun Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qi_L/0/1/0/all/0/1\">Lu Qi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_L/0/1/0/all/0/1\">Liying Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jiang_N/0/1/0/all/0/1\">Nianjuan Jiang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_J/0/1/0/all/0/1\">Jiangbo Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jia_J/0/1/0/all/0/1\">Jiaya Jia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Breaking Shortcut: Exploring Fully Convolutional Cycle-Consistency for Video Correspondence Learning. (arXiv:2105.05838v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.05838","description":"<p>Previous cycle-consistency correspondence learning methods usually leverage\nimage patches for training. In this paper, we present a fully convolutional\nmethod, which is simpler and more coherent to the inference process. While\ndirectly applying fully convolutional training results in model collapse, we\nstudy the underline reason behind this collapse phenomenon, indicating that the\nabsolute positions of pixels provide a shortcut to easily accomplish\ncycle-consistence, which hinders the learning of meaningful visual\nrepresentations. To break this absolute position shortcut, we propose to apply\ndifferent crops for forward and backward frames, and adopt feature warping to\nestablish correspondence between two crops of a same frame. The former\ntechnique enforces the corresponding pixels at forward and back tracks to have\ndifferent absolute positions, and the latter effectively blocks the shortcuts\ngoing between forward and back tracks. In three label propagation benchmarks\nfor pose tracking, face landmark tracking and video object segmentation, our\nmethod largely improves the results of vanilla fully convolutional\ncycle-consistency method, achieving very competitive performance compared with\nthe self-supervised state-of-the-art approaches. Our trained model and code are\navailable at \\url{https://github.com/Steve-Tod/STFC3}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yansong Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zhenyu Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zhenda Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yue Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip H. S. Torr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Han Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Class-Incremental Few-Shot Object Detection. (arXiv:2105.07637v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.07637","description":"<p>Conventional detection networks usually need abundant labeled training\nsamples, while humans can learn new concepts incrementally with just a few\nexamples. This paper focuses on a more challenging but realistic\nclass-incremental few-shot object detection problem (iFSD). It aims to\nincrementally transfer the model for novel objects from only a few annotated\nsamples without catastrophically forgetting the previously learned ones. To\ntackle this problem, we propose a novel method LEAST, which can transfer with\nLess forgetting, fEwer training resources, And Stronger Transfer capability.\nSpecifically, we first present the transfer strategy to reduce unnecessary\nweight adaptation and improve the transfer capability for iFSD. On this basis,\nwe then integrate the knowledge distillation technique using a less\nresource-consuming approach to alleviate forgetting and propose a novel\nclustering-based exemplar selection process to preserve more discriminative\nfeatures previously learned. Being a generic and effective method, LEAST can\nlargely improve the iFSD performance on various benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Pengyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_H/0/1/0/all/0/1\">Han Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Donghui Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Subdivision-Based Mesh Convolution Networks. (arXiv:2106.02285v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.02285","description":"<p>Convolutional neural networks (CNNs) have made great breakthroughs in 2D\ncomputer vision. However, their irregular structure makes it hard to harness\nthe potential of CNNs directly on meshes. A subdivision surface provides a\nhierarchical multi-resolution structure, in which each face in a closed\n2-manifold triangle mesh is exactly adjacent to three faces. Motivated by these\ntwo observations, this paper presents SubdivNet, an innovative and versatile\nCNN framework for 3D triangle meshes with Loop subdivision sequence\nconnectivity. Making an analogy between mesh faces and pixels in a 2D image\nallows us to present a mesh convolution operator to aggregate local features\nfrom nearby faces. By exploiting face neighborhoods, this convolution can\nsupport standard 2D convolutional network concepts, e.g. variable kernel size,\nstride, and dilation. Based on the multi-resolution hierarchy, we make use of\npooling layers which uniformly merge four faces into one and an upsampling\nmethod which splits one face into four. Thereby, many popular 2D CNN\narchitectures can be easily adapted to process 3D meshes. Meshes with arbitrary\nconnectivity can be remeshed to have Loop subdivision sequence connectivity via\nself-parameterization, making SubdivNet a general approach. Extensive\nevaluation and various applications demonstrate SubdivNet's effectiveness and\nefficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shi-Min Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zheng-Ning Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1\">Meng-Hao Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jun-Xiong Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiahui Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mu_T/0/1/0/all/0/1\">Tai-Jiang Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_R/0/1/0/all/0/1\">Ralph R. Martin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fully Transformer Networks for Semantic Image Segmentation. (arXiv:2106.04108v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.04108","description":"<p>Transformers have shown impressive performance in various natural language\nprocessing and computer vision tasks, due to the capability of modeling\nlong-range dependencies. Recent progress has demonstrated that combining such\nTransformers with CNN-based semantic image segmentation models is very\npromising. However, it is not well studied yet on how well a pure Transformer\nbased approach can achieve for image segmentation. In this work, we explore a\nnovel framework for semantic image segmentation, which is encoder-decoder based\nFully Transformer Networks (FTN). Specifically, we first propose a Pyramid\nGroup Transformer (PGT) as the encoder for progressively learning hierarchical\nfeatures, meanwhile reducing the computation complexity of the standard Visual\nTransformer (ViT). Then, we propose a Feature Pyramid Transformer (FPT) to fuse\nsemantic-level and spatial-level information from multiple levels of the PGT\nencoder for semantic image segmentation. Surprisingly, this simple baseline can\nachieve better results on multiple challenging semantic segmentation and face\nparsing benchmarks, including PASCAL Context, ADE20K, COCOStuff, and\nCelebAMask-HQ. The source code will be released on\nhttps://github.com/BR-IDL/PaddleViT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Sitong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tianyi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_F/0/1/0/all/0/1\">Fangjian Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_S/0/1/0/all/0/1\">Shengwei Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_G/0/1/0/all/0/1\">Guodong Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Topological Semantic Mapping by Consolidation of Deep Visual Features. (arXiv:2106.12709v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.12709","description":"<p>Many works in the recent literature introduce semantic mapping methods that\nuse CNNs (Convolutional Neural Networks) to recognize semantic properties in\nimages. The types of properties (eg.: room size, place category, and objects)\nand their classes (eg.: kitchen and bathroom, for place category) are usually\npredefined and restricted to a specific task. Thus, all the visual data\nacquired and processed during the construction of the maps are lost and only\nthe recognized semantic properties remain on the maps. In contrast, this work\nintroduces a topological semantic mapping method that uses deep visual features\nextracted by a CNN (GoogLeNet), from 2D images captured in multiple views of\nthe environment as the robot operates, to create, through averages,\nconsolidated representations of the visual features acquired in the regions\ncovered by each topological node. These representations allow flexible\nrecognition of semantic properties of the regions and use in other visual\ntasks. Experiments with a real-world indoor dataset showed that the method is\nable to consolidate the visual features of regions and use them to recognize\nobjects and place categories as semantic properties, and to indicate the\ntopological location of images, with very promising results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sousa_Y/0/1/0/all/0/1\">Ygor C. N. Sousa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bassani_H/0/1/0/all/0/1\">Hansenclever F. Bassani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Panoptic Segmentation of Satellite Image Time Series with Convolutional Temporal Attention Networks. (arXiv:2107.07933v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.07933","description":"<p>Unprecedented access to multi-temporal satellite imagery has opened new\nperspectives for a variety of Earth observation tasks. Among them,\npixel-precise panoptic segmentation of agricultural parcels has major economic\nand environmental implications. While researchers have explored this problem\nfor single images, we argue that the complex temporal patterns of crop\nphenology are better addressed with temporal sequences of images. In this\npaper, we present the first end-to-end, single-stage method for panoptic\nsegmentation of Satellite Image Time Series (SITS). This module can be combined\nwith our novel image sequence encoding network which relies on temporal\nself-attention to extract rich and adaptive multi-scale spatio-temporal\nfeatures. We also introduce PASTIS, the first open-access SITS dataset with\npanoptic annotations. We demonstrate the superiority of our encoder for\nsemantic segmentation against multiple competing architectures, and set up the\nfirst state-of-the-art of panoptic segmentation of SITS. Our implementation and\nPASTIS are publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Garnot_V/0/1/0/all/0/1\">Vivien Sainte Fare Garnot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Landrieu_L/0/1/0/all/0/1\">Loic Landrieu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Explainability: A Tutorial on Gradient-Based Attribution Methods for Deep Neural Networks. (arXiv:2107.11400v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2107.11400","description":"<p>With the rise of deep neural networks, the challenge of explaining the\npredictions of these networks has become increasingly recognized. While many\nmethods for explaining the decisions of deep neural networks exist, there is\ncurrently no consensus on how to evaluate them. On the other hand, robustness\nis a popular topic for deep learning research; however, it is hardly talked\nabout in explainability until very recently. In this tutorial paper, we start\nby presenting gradient-based interpretability methods. These techniques use\ngradient signals to assign the burden of the decision on the input features.\nLater, we discuss how gradient-based methods can be evaluated for their\nrobustness and the role that adversarial robustness plays in having meaningful\nexplanations. We also discuss the limitations of gradient-based methods.\nFinally, we present the best practices and attributes that should be examined\nbefore choosing an explainability method. We conclude with the future\ndirections for research in the area at the convergence of robustness and\nexplainability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nielsen_I/0/1/0/all/0/1\">Ian E. Nielsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dera_D/0/1/0/all/0/1\">Dimah Dera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rasool_G/0/1/0/all/0/1\">Ghulam Rasool</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouaynaya_N/0/1/0/all/0/1\">Nidhal Bouaynaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramachandran_R/0/1/0/all/0/1\">Ravi P. Ramachandran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RCA-IUnet: A residual cross-spatial attention guided inception U-Net model for tumor segmentation in breast ultrasound imaging. (arXiv:2108.02508v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2108.02508","description":"<p>The advancements in deep learning technologies have produced immense\ncontributions to biomedical image analysis applications. With breast cancer\nbeing the common deadliest disease among women, early detection is the key\nmeans to improve survivability. Medical imaging like ultrasound presents an\nexcellent visual representation of the functioning of the organs; however, for\nany radiologist analysing such scans is challenging and time consuming which\ndelays the diagnosis process. Although various deep learning based approaches\nare proposed that achieved promising results, the present article introduces an\nefficient residual cross-spatial attention guided inception U-Net (RCA-IUnet)\nmodel with minimal training parameters for tumor segmentation using breast\nultrasound imaging to further improve the segmentation performance of varying\ntumor sizes. The RCA-IUnet model follows U-Net topology with residual inception\ndepth-wise separable convolution and hybrid pooling (max pooling and spectral\npooling) layers. In addition, cross-spatial attention filters are added to\nsuppress the irrelevant features and focus on the target structure. The\nsegmentation performance of the proposed model is validated on two publicly\navailable datasets using standard segmentation evaluation metrics, where it\noutperformed the other state-of-the-art segmentation models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Punn_N/0/1/0/all/0/1\">Narinder Singh Punn</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Agarwal_S/0/1/0/all/0/1\">Sonali Agarwal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Real-Time Online Learning Framework for Joint 3D Reconstruction and Semantic Segmentation of Indoor Scenes. (arXiv:2108.05246v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.05246","description":"<p>This paper presents a real-time online vision framework to jointly recover an\nindoor scene's 3D structure and semantic label. Given noisy depth maps, a\ncamera trajectory, and 2D semantic labels at train time, the proposed deep\nneural network based approach learns to fuse the depth over frames with\nsuitable semantic labels in the scene space. Our approach exploits the joint\nvolumetric representation of the depth and semantics in the scene feature space\nto solve this task. For a compelling online fusion of the semantic labels and\ngeometry in real-time, we introduce an efficient vortex pooling block while\ndropping the use of routing network in online depth fusion to preserve\nhigh-frequency surface details. We show that the context information provided\nby the semantics of the scene helps the depth fusion network learn\nnoise-resistant features. Not only that, it helps overcome the shortcomings of\nthe current online depth fusion method in dealing with thin object structures,\nthickening artifacts, and false surfaces. Experimental evaluation on the\nReplica dataset shows that our approach can perform depth fusion at 37 and 10\nframes per second with an average reconstruction F-score of 88% and 91%,\nrespectively, depending on the depth map resolution. Moreover, our model shows\nan average IoU score of 0.515 on the ScanNet 3D semantic benchmark leaderboard.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Menini_D/0/1/0/all/0/1\">Davide Menini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Suryansh Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oswald_M/0/1/0/all/0/1\">Martin R. Oswald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sandstrom_E/0/1/0/all/0/1\">Erik Sandstrom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sminchisescu_C/0/1/0/all/0/1\">Cristian Sminchisescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"New Pruning Method Based on DenseNet Network for Image Classification. (arXiv:2108.12604v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.12604","description":"<p>Deep neural networks have made significant progress in the field of computer\nvision. Recent studies have shown that depth, width and shortcut connections of\nneural network architectures play a crucial role in their performance. One of\nthe most advanced neural network architectures, DenseNet, has achieved\nexcellent convergence rates through dense connections. However, it still has\nobvious shortcomings in the usage of amount of memory. In this paper, we\nintroduce a new type of pruning tool, threshold, which refers to the principle\nof the threshold voltage in MOSFET. This work employs this method to connect\nblocks of different depths in different ways to reduce the usage of memory. It\nis denoted as ThresholdNet. We evaluate ThresholdNet and other different\nnetworks on datasets of CIFAR10. Experiments show that HarDNet is twice as fast\nas DenseNet, and on this basis, ThresholdNet is 10% faster and 10% lower error\nrate than HarDNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ju_R/0/1/0/all/0/1\">Rui-Yang Ju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Ting-Yu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiang_J/0/1/0/all/0/1\">Jen-Shiun Chiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is Attention Better Than Matrix Decomposition?. (arXiv:2109.04553v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.04553","description":"<p>As an essential ingredient of modern deep learning, attention mechanism,\nespecially self-attention, plays a vital role in the global correlation\ndiscovery. However, is hand-crafted attention irreplaceable when modeling the\nglobal context? Our intriguing finding is that self-attention is not better\nthan the matrix decomposition (MD) model developed 20 years ago regarding the\nperformance and computational cost for encoding the long-distance dependencies.\nWe model the global context issue as a low-rank recovery problem and show that\nits optimization algorithms can help design global information blocks. This\npaper then proposes a series of Hamburgers, in which we employ the optimization\nalgorithms for solving MDs to factorize the input representations into\nsub-matrices and reconstruct a low-rank embedding. Hamburgers with different\nMDs can perform favorably against the popular global context module\nself-attention when carefully coping with gradients back-propagated through\nMDs. Comprehensive experiments are conducted in the vision tasks where it is\ncrucial to learn the global context, including semantic segmentation and image\ngeneration, demonstrating significant improvements over self-attention and its\nvariants.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Geng_Z/0/1/0/all/0/1\">Zhengyang Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1\">Meng-Hao Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hongxu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_K/0/1/0/all/0/1\">Ke Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhouchen Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TEACh: Task-driven Embodied Agents that Chat. (arXiv:2110.00534v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.00534","description":"<p>Robots operating in human spaces must be able to engage in natural language\ninteraction with people, both understanding and executing instructions, and\nusing conversation to resolve ambiguity and recover from mistakes. To study\nthis, we introduce TEACh, a dataset of over 3,000 human--human, interactive\ndialogues to complete household tasks in simulation. A Commander with access to\noracle information about a task communicates in natural language with a\nFollower. The Follower navigates through and interacts with the environment to\ncomplete tasks varying in complexity from \"Make Coffee\" to \"Prepare Breakfast\",\nasking questions and getting additional information from the Commander. We\npropose three benchmarks using TEACh to study embodied intelligence challenges,\nand we evaluate initial models' abilities in dialogue understanding, language\ngrounding, and task execution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Padmakumar_A/0/1/0/all/0/1\">Aishwarya Padmakumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomason_J/0/1/0/all/0/1\">Jesse Thomason</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1\">Ayush Shrivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lange_P/0/1/0/all/0/1\">Patrick Lange</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayan_Chen_A/0/1/0/all/0/1\">Anjali Narayan-Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gella_S/0/1/0/all/0/1\">Spandana Gella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piramuthu_R/0/1/0/all/0/1\">Robinson Piramuthu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tur_G/0/1/0/all/0/1\">Gokhan Tur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hakkani_Tur_D/0/1/0/all/0/1\">Dilek Hakkani-Tur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CertainNet: Sampling-free Uncertainty Estimation for Object Detection. (arXiv:2110.01604v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.01604","description":"<p>Estimating the uncertainty of a neural network plays a fundamental role in\nsafety-critical settings. In perception for autonomous driving, measuring the\nuncertainty means providing additional calibrated information to downstream\ntasks, such as path planning, that can use it towards safe navigation. In this\nwork, we propose a novel sampling-free uncertainty estimation method for object\ndetection. We call it CertainNet, and it is the first to provide separate\nuncertainties for each output signal: objectness, class, location and size. To\nachieve this, we propose an uncertainty-aware heatmap, and exploit the\nneighboring bounding boxes provided by the detector at inference time. We\nevaluate the detection performance and the quality of the different uncertainty\nestimates separately, also with challenging out-of-domain samples: BDD100K and\nnuImages with models trained on KITTI. Additionally, we propose a new metric to\nevaluate location and size uncertainties. When transferring to unseen datasets,\nCertainNet generalizes substantially better than previous methods and an\nensemble, while being real-time and providing high quality and comprehensive\nuncertainty estimates.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gasperini_S/0/1/0/all/0/1\">Stefano Gasperini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haug_J/0/1/0/all/0/1\">Jan Haug</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahani_M/0/1/0/all/0/1\">Mohammad-Ali Nikouei Mahani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marcos_Ramiro_A/0/1/0/all/0/1\">Alvaro Marcos-Ramiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1\">Nassir Navab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Busam_B/0/1/0/all/0/1\">Benjamin Busam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tombari_F/0/1/0/all/0/1\">Federico Tombari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Pedestrian Attribute Recognition Using Group Sparsity for Occlusion Videos. (arXiv:2110.08708v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.08708","description":"<p>Occlusion processing is a key issue in pedestrian attribute recognition\n(PAR). Nevertheless, several existing video-based PAR methods have not yet\nconsidered occlusion handling in depth. In this paper, we formulate finding\nnon-occluded frames as sparsity-based temporal attention of a crowded video. In\nthis manner, a model is guided not to pay attention to the occluded frame.\nHowever, temporal sparsity cannot include a correlation between attributes when\nocclusion occurs. For example, \"boots\" and \"shoe color\" cannot be recognized\nwhen the foot is invisible. To solve the uncorrelated attention issue, we also\npropose a novel group sparsity-based temporal attention module. Group sparsity\nis applied across attention weights in correlated attributes. Thus, attention\nweights in a group are forced to pay attention to the same frames. Experimental\nresults showed that the proposed method achieved a higher F1-score than the\nstate-of-the-art methods on two video-based PAR datasets and five occlusion\nscenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Geonu Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yun_K/0/1/0/all/0/1\">Kimin Yun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1\">Jungchan Cho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LoveDA: A Remote Sensing Land-Cover Dataset for Domain Adaptive Semantic Segmentation. (arXiv:2110.08733v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.08733","description":"<p>Deep learning approaches have shown promising results in remote sensing high\nspatial resolution (HSR) land-cover mapping. However, urban and rural scenes\ncan show completely different geographical landscapes, and the inadequate\ngeneralizability of these algorithms hinders city-level or national-level\nmapping. Most of the existing HSR land-cover datasets mainly promote the\nresearch of learning semantic representation, thereby ignoring the model\ntransferability. In this paper, we introduce the Land-cOVEr Domain Adaptive\nsemantic segmentation (LoveDA) dataset to advance semantic and transferable\nlearning. The LoveDA dataset contains 5987 HSR images with 166768 annotated\nobjects from three different cities. Compared to the existing datasets, the\nLoveDA dataset encompasses two domains (urban and rural), which brings\nconsiderable challenges due to the: 1) multi-scale objects; 2) complex\nbackground samples; and 3) inconsistent class distributions. The LoveDA dataset\nis suitable for both land-cover semantic segmentation and unsupervised domain\nadaptation (UDA) tasks. Accordingly, we benchmarked the LoveDA dataset on\neleven semantic segmentation methods and eight UDA methods. Some exploratory\nstudies including multi-scale architectures and strategies, additional\nbackground supervision, and pseudo-label analysis were also carried out to\naddress these challenges. The code and data are available at\nhttps://github.com/Junjue-Wang/LoveDA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Junjue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zhuo Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_A/0/1/0/all/0/1\">Ailong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiaoyan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1\">Yanfei Zhong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Boosting the Transferability of Video Adversarial Examples via Temporal Translation. (arXiv:2110.09075v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.09075","description":"<p>Although deep-learning based video recognition models have achieved\nremarkable success, they are vulnerable to adversarial examples that are\ngenerated by adding human-imperceptible perturbations on clean video samples.\nAs indicated in recent studies, adversarial examples are transferable, which\nmakes it feasible for black-box attacks in real-world applications.\nNevertheless, most existing adversarial attack methods have poor\ntransferability when attacking other video models and transfer-based attacks on\nvideo models are still unexplored. To this end, we propose to boost the\ntransferability of video adversarial examples for black-box attacks on video\nrecognition models. Through extensive analysis, we discover that different\nvideo recognition models rely on different discriminative temporal patterns,\nleading to the poor transferability of video adversarial examples. This\nmotivates us to introduce a temporal translation attack method, which optimizes\nthe adversarial perturbations over a set of temporal translated video clips. By\ngenerating adversarial examples over translated videos, the resulting\nadversarial examples are less sensitive to temporal patterns existed in the\nwhite-box model being attacked and thus can be better transferred. Extensive\nexperiments on the Kinetics-400 dataset and the UCF-101 dataset demonstrate\nthat our method can significantly boost the transferability of video\nadversarial examples. For transfer-based attack against video recognition\nmodels, it achieves a 61.56% average attack success rate on the Kinetics-400\nand 48.60% on the UCF-101. Code is available at\nhttps://github.com/zhipeng-wei/TT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhipeng Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jingjing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zuxuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yu-Gang Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HM-Net: A Regression Network for Object Center Detection and Tracking on Wide Area Motion Imagery. (arXiv:2110.09881v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.09881","description":"<p>Wide Area Motion Imagery (WAMI) yields high-resolution images with a large\nnumber of extremely small objects. Target objects have large spatial\ndisplacements throughout consecutive frames. This nature of WAMI images makes\nobject tracking and detection challenging. In this paper, we present our deep\nneural network-based combined object detection and tracking model, namely, Heat\nMap Network (HM-Net). HM-Net is significantly faster than state-of-the-art\nframe differencing and background subtraction-based methods, without\ncompromising detection and tracking performances. HM-Net follows the object\ncenter-based joint detection and tracking paradigm. Simple heat map-based\npredictions support an unlimited number of simultaneous detections. The\nproposed method uses two consecutive frames and the object detection heat map\nobtained from the previous frame as input, which helps HM-Net monitor\nspatio-temporal changes between frames and keeps track of previously predicted\nobjects. Although reuse of prior object detection heat map acts as a vital\nfeedback-based memory element, it can lead to an unintended surge of\nfalse-positive detections. To increase the robustness of the method against\nfalse positives and to eliminate low confidence detections, HM-Net employs\nnovel feedback filters and advanced data augmentations. HM-Net outperforms\nstate-of-the-art WAMI moving object detection and tracking methods on the WPAFB\ndataset with its 96.2% F1 and 94.4% mAP detection scores while achieving a\n61.8% mAP tracking score on the same dataset. This performance corresponds to\nan improvement of 2.1% for F1, 6.1% for mAP scores on detection, and 9.5% for\nmAP score on tracking over the state-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Motorcu_H/0/1/0/all/0/1\">Hakki Motorcu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ates_H/0/1/0/all/0/1\">Hasan F. Ates</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ugurdag_H/0/1/0/all/0/1\">H. Fatih Ugurdag</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunturk_B/0/1/0/all/0/1\">Bahadir Gunturk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"QU-net++: Image Quality Detection Framework for Segmentation of 3D Medical Image Stacks. (arXiv:2110.14181v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2110.14181","description":"<p>Automated segmentation of pathological regions of interest aids medical image\ndiagnostics and follow-up care. However, accurate pathological segmentations\nrequire high quality of annotated data that can be both cost and time intensive\nto generate. In this work, we propose an automated two-step method that detects\na minimal image subset required to train segmentation models by evaluating the\nquality of medical images from 3D image stacks using a U-net++ model. These\nimages that represent a lack of quality training can then be annotated and used\nto fully train a U-net-based segmentation model. The proposed QU-net++ model\ndetects lack of quality training based on the disagreement in segmentations\nproduced from the final two output layers. The proposed model isolates around\n10% of images per 3D stack and can scale across imaging modalities to segment\ncysts in OCT images and ground glass opacity in Lung CT images with Dice scores\nin the range 0.56-0.72. Thus, the proposed method can be applied for cost\neffective multi-modal pathology segmentation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Roychowdhury_S/0/1/0/all/0/1\">Sohini Roychowdhury</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video Text Tracking With a Spatio-Temporal Complementary Model. (arXiv:2111.04987v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.04987","description":"<p>Text tracking is to track multiple texts in a video,and construct a\ntrajectory for each text. Existing methodstackle this task by utilizing the\ntracking-by-detection frame-work, i.e., detecting the text instances in each\nframe andassociating the corresponding text instances in consecutiveframes. We\nargue that the tracking accuracy of this paradigmis severely limited in more\ncomplex scenarios, e.g., owing tomotion blur, etc., the missed detection of\ntext instances causesthe break of the text trajectory. In addition, different\ntextinstances with similar appearance are easily confused, leadingto the\nincorrect association of the text instances. To this end,a novel\nspatio-temporal complementary text tracking model isproposed in this paper. We\nleverage a Siamese ComplementaryModule to fully exploit the continuity\ncharacteristic of the textinstances in the temporal dimension, which\neffectively alleviatesthe missed detection of the text instances, and hence\nensuresthe completeness of each text trajectory. We further integratethe\nsemantic cues and the visual cues of the text instance intoa unified\nrepresentation via a text similarity learning network,which supplies a high\ndiscriminative power in the presence oftext instances with similar appearance,\nand thus avoids the mis-association between them. Our method achieves\nstate-of-the-art performance on several public benchmarks. The source codeis\navailable at https://github.com/lsabrinax/VideoTextSCM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yuzhe Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiajian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_D/0/1/0/all/0/1\">Dian Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Shenggao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xiang Bai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DSPoint: Dual-scale Point Cloud Recognition with High-frequency Fusion. (arXiv:2111.10332v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.10332","description":"<p>Point cloud processing is a challenging task due to its sparsity and\nirregularity. Prior works introduce delicate designs on either local feature\naggregator or global geometric architecture, but few combine both advantages.\nWe propose Dual-Scale Point Cloud Recognition with High-frequency Fusion\n(DSPoint) to extract local-global features by concurrently operating on voxels\nand points. We reverse the conventional design of applying convolution on\nvoxels and attention to points. Specifically, we disentangle point features\nthrough channel dimension for dual-scale processing: one by point-wise\nconvolution for fine-grained geometry parsing, the other by voxel-wise global\nattention for long-range structural exploration. We design a co-attention\nfusion module for feature alignment to blend local-global modalities, which\nconducts inter-scale cross-modality interaction by communicating high-frequency\ncoordinates information. Experiments and ablations on widely-adopted\nModelNet40, ShapeNet, and S3DIS demonstrate the state-of-the-art performance of\nour DSPoint.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Renrui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Ziyao Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Ziyu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xinben Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_K/0/1/0/all/0/1\">Kexue Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jianbo Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TransMorph: Transformer for unsupervised medical image registration. (arXiv:2111.10480v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2111.10480","description":"<p>In the last decade, convolutional neural networks (ConvNets) have been a\nmajor focus of research in medical image analysis. However, the performances of\nConvNets may be limited by a lack of explicit consideration of the long-range\nspatial relationships in an image. Recently Vision Transformer architectures\nhave been proposed to address the shortcomings of ConvNets and have produced\nstate-of-the-art performances in many medical imaging applications.\nTransformers may be a strong candidate for image registration because their\nunlimited receptive field enables a more precise comprehension of the spatial\ncorrespondence between moving and fixed images. Here, we present TransMorph, a\nhybrid Transformer-ConvNet model for volumetric medical image registration.\nThis paper also presents diffeomorphic and Bayesian variants of TransMorph: the\ndiffeomorphic variants ensure the topology-preserving deformations, and the\nBayesian variant produces a well-calibrated registration uncertainty estimate.\nWe extensively validated the proposed models using 3D medical images from three\napplications: inter-patient and atlas-to-patient brain MRI registration and\nphantom-to-CT registration. The proposed models are evaluated in comparison to\na variety of existing registration methods and Transformer architectures.\nQualitative and quantitative results demonstrate that the proposed\nTransformer-based model leads to a substantial performance improvement over the\nbaseline methods, confirming the effectiveness of Transformers for medical\nimage registration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chen_J/0/1/0/all/0/1\">Junyu Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Frey_E/0/1/0/all/0/1\">Eric C. Frey</a>, <a href=\"http://arxiv.org/find/eess/1/au:+He_Y/0/1/0/all/0/1\">Yufan He</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Segars_W/0/1/0/all/0/1\">William P. Segars</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Ye Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Du_Y/0/1/0/all/0/1\">Yong Du</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Path Guiding Using Spatio-Directional Mixture Models. (arXiv:2111.13094v2 [cs.GR] UPDATED)","link":"http://arxiv.org/abs/2111.13094","description":"<p>We propose a learning-based method for light-path construction in path\ntracing algorithms, which iteratively optimizes and samples from what we refer\nto as spatio-directional Gaussian mixture models (SDMMs). In particular, we\napproximate incident radiance as an online-trained $5$D mixture that is\naccelerated by a $k$D-tree. Using the same framework, we approximate BSDFs as\npre-trained $n$D mixtures, where $n$ is the number of BSDF parameters. Such an\napproach addresses two major challenges in path-guiding models. First, the $5$D\nradiance representation naturally captures correlation between the spatial and\ndirectional dimensions. Such correlations are present in e.g. parallax and\ncaustics. Second, by using a tangent-space parameterization of Gaussians, our\nspatio-directional mixtures can perform approximate product sampling with\narbitrarily oriented BSDFs. Existing models are only able to do this by either\nforegoing anisotropy of the mixture components or by representing the radiance\nfield in local (normal aligned) coordinates, which both make the radiance field\nmore difficult to learn. An additional benefit of the tangent-space\nparameterization is that each individual Gaussian is mapped to the solid sphere\nwith low distortion near its center of mass. Our method performs especially\nwell on scenes with small, localized luminaires that induce high\nspatio-directional correlation in the incident radiance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dodik_A/0/1/0/all/0/1\">Ana Dodik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papas_M/0/1/0/all/0/1\">Marios Papas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oztireli_C/0/1/0/all/0/1\">Cengiz &#xd6;ztireli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_T/0/1/0/all/0/1\">Thomas M&#xfc;ller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ExCon: Explanation-driven Supervised Contrastive Learning for Image Classification. (arXiv:2111.14271v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.14271","description":"<p>Contrastive learning has led to substantial improvements in the quality of\nlearned embedding representations for tasks such as image classification.\nHowever, a key drawback of existing contrastive augmentation methods is that\nthey may lead to the modification of the image content which can yield\nundesired alterations of its semantics. This can affect the performance of the\nmodel on downstream tasks. Hence, in this paper, we ask whether we can augment\nimage data in contrastive learning such that the task-relevant semantic content\nof an image is preserved. For this purpose, we propose to leverage\nsaliency-based explanation methods to create content-preserving masked\naugmentations for contrastive learning. Our novel explanation-driven supervised\ncontrastive learning (ExCon) methodology critically serves the dual goals of\nencouraging nearby image embeddings to have similar content and explanation. To\nquantify the impact of ExCon, we conduct experiments on the CIFAR-100 and the\nTiny ImageNet datasets. We demonstrate that ExCon outperforms vanilla\nsupervised contrastive learning in terms of classification, explanation\nquality, adversarial robustness as well as calibration of probabilistic\npredictions of the model in the context of distributional shift.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhibo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_J/0/1/0/all/0/1\">Jongseong Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trabelsi_C/0/1/0/all/0/1\">Chiheb Trabelsi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruiwen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanner_S/0/1/0/all/0/1\">Scott Sanner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_Y/0/1/0/all/0/1\">Yeonjeong Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shim_D/0/1/0/all/0/1\">Dongsub Shim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3rd Place: A Global and Local Dual Retrieval Solution to Facebook AI Image Similarity Challenge. (arXiv:2112.02373v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.02373","description":"<p>As a basic task of computer vision, image similarity retrieval is facing the\nchallenge of large-scale data and image copy attacks. This paper presents our\n3rd place solution to the matching track of Image Similarity Challenge (ISC)\n2021 organized by Facebook AI. We propose a multi-branch retrieval method of\ncombining global descriptors and local descriptors to cover all attack cases.\nSpecifically, we attempt many strategies to optimize global descriptors,\nincluding abundant data augmentations, self-supervised learning with a single\nTransformer model, overlay detection preprocessing. Moreover, we introduce the\nrobust SIFT feature and GPU Faiss for local retrieval which makes up for the\nshortcomings of the global retrieval. Finally, KNN-matching algorithm is used\nto judge the match and merge scores. We show some ablation experiments of our\nmethod, which reveals the complementary advantages of global and local\nfeatures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xinlong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yangyang Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xuyuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_G/0/1/0/all/0/1\">Guoping Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yang Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yexin Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deblurring via Stochastic Refinement. (arXiv:2112.02475v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.02475","description":"<p>Image deblurring is an ill-posed problem with multiple plausible solutions\nfor a given input image. However, most existing methods produce a deterministic\nestimate of the clean image and are trained to minimize pixel-level distortion.\nThese metrics are known to be poorly correlated with human perception, and\noften lead to unrealistic reconstructions. We present an alternative framework\nfor blind deblurring based on conditional diffusion models. Unlike existing\ntechniques, we train a stochastic sampler that refines the output of a\ndeterministic predictor and is capable of producing a diverse set of plausible\nreconstructions for a given input. This leads to a significant improvement in\nperceptual quality over existing state-of-the-art methods across multiple\nstandard benchmarks. Our predict-and-refine approach also enables much more\nefficient sampling compared to typical diffusion models. Combined with a\ncarefully tuned network architecture and inference procedure, our method is\ncompetitive in terms of distortion metrics such as PSNR. These results show\nclear benefits of our diffusion-based method for deblurring and challenge the\nwidely used strategy of producing a single, deterministic reconstruction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Whang_J/0/1/0/all/0/1\">Jay Whang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Delbracio_M/0/1/0/all/0/1\">Mauricio Delbracio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talebi_H/0/1/0/all/0/1\">Hossein Talebi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saharia_C/0/1/0/all/0/1\">Chitwan Saharia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dimakis_A/0/1/0/all/0/1\">Alexandros G. Dimakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milanfar_P/0/1/0/all/0/1\">Peyman Milanfar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FaceFormer: Speech-Driven 3D Facial Animation with Transformers. (arXiv:2112.05329v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.05329","description":"<p>Speech-driven 3D facial animation is challenging due to the complex geometry\nof human faces and the limited availability of 3D audio-visual data. Prior\nworks typically focus on learning phoneme-level features of short audio windows\nwith limited context, occasionally resulting in inaccurate lip movements. To\ntackle this limitation, we propose a Transformer-based autoregressive model,\nFaceFormer, which encodes the long-term audio context and autoregressively\npredicts a sequence of animated 3D face meshes. To cope with the data scarcity\nissue, we integrate the self-supervised pre-trained speech representations.\nAlso, we devise two biased attention mechanisms well suited to this specific\ntask, including the biased cross-modal multi-head (MH) attention and the biased\ncausal MH self-attention with a periodic positional encoding strategy. The\nformer effectively aligns the audio-motion modalities, whereas the latter\noffers abilities to generalize to longer audio sequences. Extensive experiments\nand a perceptual user study show that our approach outperforms the existing\nstate-of-the-arts. We encourage watching the video. The code will be made\navailable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1\">Yingruo Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhaojiang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saito_J/0/1/0/all/0/1\">Jun Saito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Komura_T/0/1/0/all/0/1\">Taku Komura</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Paced Deep Regression Forests with Consideration on Ranking Fairness. (arXiv:2112.06455v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.06455","description":"<p>Deep discriminative models (DDMs), such as deep regression forests, deep\nneural decision forests, have been extensively studied recently to solve\nproblems like facial age estimation, head pose estimation, gaze estimation and\nso forth. Such problems are challenging in part because a large amount of\neffective training data without noise and bias is often not available. While\nsome progress has been achieved through learning more discriminative features,\nor reweighting samples, we argue what is more desirable is to learn gradually\nto discriminate like human beings. Then, we resort to self-paced learning\n(SPL). But a natural question arises: can self-paced regime lead DDMs to\nachieve more robust and less biased solutions? A serious problem with SPL,\nwhich is firstly discussed by this work, is it tends to aggravate the bias of\nsolutions, especially for obvious imbalanced data. To this end, this paper\nproposes a new self-paced paradigm for deep discriminative model, which\ndistinguishes noisy and underrepresented examples according to the output\nlikelihood and entropy associated with each example, and tackle the fundamental\nranking problem in SPL from a new perspective: fairness. This paradigm is\nfundamental, and could be easily combined with a variety of DDMs. Extensive\nexperiments on three computer vision tasks, such as facial age estimation, head\npose estimation and gaze estimation, demonstrate the efficacy of our paradigm.\nTo the best of our knowledge, our work is the first paper in the literature of\nSPL that considers ranking fairness for self-paced regime construction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1\">Lili Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_M/0/1/0/all/0/1\">Mingming Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Yazhou Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yali Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zenglin Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VirtualCube: An Immersive 3D Video Communication System. (arXiv:2112.06730v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.06730","description":"<p>The VirtualCube system is a 3D video conference system that attempts to\novercome some limitations of conventional technologies. The key ingredient is\nVirtualCube, an abstract representation of a real-world cubicle instrumented\nwith RGBD cameras for capturing the 3D geometry and texture of a user. We\ndesign VirtualCube so that the task of data capturing is standardized and\nsignificantly simplified, and everything can be built using off-the-shelf\nhardware. We use VirtualCubes as the basic building blocks of a virtual\nconferencing environment, and we provide each VirtualCube user with a\nsurrounding display showing life-size videos of remote participants. To achieve\nreal-time rendering of remote participants, we develop the V-Cube View\nalgorithm, which uses multi-view stereo for more accurate depth estimation and\nLumi-Net rendering for better rendering quality. The VirtualCube system\ncorrectly preserves the mutual eye gaze between participants, allowing them to\nestablish eye contact and be aware of who is visually paying attention to them.\nThe system also allows a participant to have side discussions with remote\nparticipants as if they were in the same room. Finally, the system sheds lights\non how to support the shared space of work items (e.g., documents and\napplications) and track the visual attention of participants to work items.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yizhong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jiaolong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ruicheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guojun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_X/0/1/0/all/0/1\">Xin Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_B/0/1/0/all/0/1\">Baining Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Depth Completion with Uncertainty-Driven Loss Functions. (arXiv:2112.07895v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.07895","description":"<p>Recovering a dense depth image from sparse LiDAR scans is a challenging task.\nDespite the popularity of color-guided methods for sparse-to-dense depth\ncompletion, they treated pixels equally during optimization, ignoring the\nuneven distribution characteristics in the sparse depth map and the accumulated\noutliers in the synthesized ground truth. In this work, we introduce\nuncertainty-driven loss functions to improve the robustness of depth completion\nand handle the uncertainty in depth completion. Specifically, we propose an\nexplicit uncertainty formulation for robust depth completion with Jeffrey's\nprior. A parametric uncertain-driven loss is introduced and translated to new\nloss functions that are robust to noisy or missing data. Meanwhile, we propose\na multiscale joint prediction model that can simultaneously predict depth and\nuncertainty maps. The estimated uncertainty map is also used to perform\nadaptive prediction on the pixels with high uncertainty, leading to a residual\nmap for refining the completion results. Our method has been tested on KITTI\nDepth Completion Benchmark and achieved the state-of-the-art robustness\nperformance in terms of MAE, IMAE, and IRMSE metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yufan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_W/0/1/0/all/0/1\">Weisheng Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Leida Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jinjian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_G/0/1/0/all/0/1\">Guangming Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Object Recognition as Classification via Visual Properties. (arXiv:2112.10531v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.10531","description":"<p>We base our work on the teleosemantic modelling of concepts as abilities\nimplementing the distinct functions of recognition and classification.\nAccordingly, we model two types of concepts - substance concepts suited for\nobject recognition exploiting visual properties, and classification concepts\nsuited for classification of substance concepts exploiting linguistically\ngrounded properties. The goal in this paper is to demonstrate that object\nrecognition can be construed as classification via visual properties, as\ndistinct from work in mainstream computer vision. Towards that, we present an\nobject recognition process based on Ranganathan's four-phased faceted knowledge\norganization process, grounded in the teleosemantic distinctions of substance\nconcept and classification concept. We also briefly introduce the ongoing\nproject MultiMedia UKC, whose aim is to build an object recognition resource\nfollowing our proposed process\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Giunchiglia_F/0/1/0/all/0/1\">Fausto Giunchiglia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bagchi_M/0/1/0/all/0/1\">Mayukh Bagchi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EyePAD++: A Distillation-based approach for joint Eye Authentication and Presentation Attack Detection using Periocular Images. (arXiv:2112.11610v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.11610","description":"<p>A practical eye authentication (EA) system targeted for edge devices needs to\nperform authentication and be robust to presentation attacks, all while\nremaining compute and latency efficient. However, existing eye-based frameworks\na) perform authentication and Presentation Attack Detection (PAD) independently\nand b) involve significant pre-processing steps to extract the iris region.\nHere, we introduce a joint framework for EA and PAD using periocular images.\nWhile a deep Multitask Learning (MTL) network can perform both the tasks, MTL\nsuffers from the forgetting effect since the training datasets for EA and PAD\nare disjoint. To overcome this, we propose Eye Authentication with PAD\n(EyePAD), a distillation-based method that trains a single network for EA and\nPAD while reducing the effect of forgetting. To further improve the EA\nperformance, we introduce a novel approach called EyePAD++ that includes\ntraining an MTL network on both EA and PAD data, while distilling the\n`versatility' of the EyePAD network through an additional distillation step.\nOur proposed methods outperform the SOTA in PAD and obtain near-SOTA\nperformance in eye-to-eye verification, without any pre-processing. We also\ndemonstrate the efficacy of EyePAD and EyePAD++ in user-to-user verification\nwith PAD across network backbones and image quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dhar_P/0/1/0/all/0/1\">Prithviraj Dhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Amit Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaplan_K/0/1/0/all/0/1\">Kirsten Kaplan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_K/0/1/0/all/0/1\">Khushi Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranjan_R/0/1/0/all/0/1\">Rakesh Ranjan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chellappa_R/0/1/0/all/0/1\">Rama Chellappa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep learning for brain metastasis detection and segmentation in longitudinal MRI data. (arXiv:2112.11833v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2112.11833","description":"<p>Brain metastases occur frequently in patients with metastatic cancer. Early\nand accurate detection of brain metastases is very essential for treatment\nplanning and prognosis in radiation therapy. To improve brain metastasis\ndetection performance with deep learning, a custom detection loss called\nvolume-level sensitivity-specificity (VSS) is proposed, which rates individual\nmetastasis detection sensitivity and specificity in (sub-)volume levels. As\nsensitivity and precision are always a trade-off in a metastasis level, either\na high sensitivity or a high precision can be achieved by adjusting the weights\nin the VSS loss without decline in dice score coefficient for segmented\nmetastases. To reduce metastasis-like structures being detected as false\npositive metastases, a temporal prior volume is proposed as an additional input\nof the neural network. Our proposed VSS loss improves the sensitivity of brain\nmetastasis detection, increasing the sensitivity from 86.7% to 95.5%.\nAlternatively, it improves the precision from 68.8% to 97.8%. With the\nadditional temporal prior volume, about 45% of the false positive metastases\nare reduced in the high sensitivity model and the precision reaches 99.6% for\nthe high specificity model. The mean dice coefficient for all metastases is\nabout 0.81. With the ensemble of the high sensitivity and high specificity\nmodels, on average only 1.5 false positive metastases per patient needs further\ncheck, while the majority of true positive metastases are confirmed. The\nensemble learning is able to distinguish high confidence true positive\nmetastases from metastases candidates that require special expert review or\nfurther follow-up, being particularly well-fit to the requirements of expert\nsupport in real clinical practice.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1\">Yixing Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bert_C/0/1/0/all/0/1\">Christoph Bert</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sommer_P/0/1/0/all/0/1\">Philipp Sommer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Frey_B/0/1/0/all/0/1\">Benjamin Frey</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gaipl_U/0/1/0/all/0/1\">Udo Gaipl</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Distel_L/0/1/0/all/0/1\">Luitpold V. Distel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Weissmann_T/0/1/0/all/0/1\">Thomas Weissmann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Uder_M/0/1/0/all/0/1\">Michael Uder</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schmidt_M/0/1/0/all/0/1\">Manuel A. Schmidt</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dorfler_A/0/1/0/all/0/1\">Arnd D&#xf6;rfler</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maier_A/0/1/0/all/0/1\">Andreas Maier</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fietkau_R/0/1/0/all/0/1\">Rainer Fietkau</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Putz_F/0/1/0/all/0/1\">Florian Putz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A formal approach to good practices in Pseudo-Labeling for Unsupervised Domain Adaptive Re-Identification. (arXiv:2112.12887v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.12887","description":"<p>The use of pseudo-labels prevails in order to tackle Unsupervised Domain\nAdaptive (UDA) Re-Identification (re-ID) with the best performance. Indeed,\nthis family of approaches has given rise to several UDA re-ID specific\nframeworks, which are effective. In these works, research directions to improve\nPseudo-Labeling UDA re-ID performance are varied and mostly based on intuition\nand experiments: refining pseudo-labels, reducing the impact of errors in\npseudo-labels... It can be hard to deduce from them general good practices,\nwhich can be implemented in any Pseudo-Labeling method, to consistently improve\nits performance. To address this key question, a new theoretical view on\nPseudo-Labeling UDA re-ID is proposed. The contributions are threefold: (i) A\nnovel theoretical framework for Pseudo-Labeling UDA re-ID, formalized through a\nnew general learning upper-bound on the UDA re-ID performance. (ii) General\ngood practices for Pseudo-Labeling, directly deduced from the interpretation of\nthe proposed theoretical framework, in order to improve the target re-ID\nperformance. (iii) Extensive experiments on challenging person and vehicle\ncross-dataset re-ID tasks, showing consistent performance improvements for\nvarious state-of-the-art methods and various proposed implementations of good\npractices.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dubourvieux_F/0/1/0/all/0/1\">Fabian Dubourvieux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Audigier_R/0/1/0/all/0/1\">Romaric Audigier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loesch_A/0/1/0/all/0/1\">Ang&#xe9;lique Loesch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ainouz_S/0/1/0/all/0/1\">Samia Ainouz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Canu_S/0/1/0/all/0/1\">St&#xe9;phane Canu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ViR:the Vision Reservoir. (arXiv:2112.13545v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.13545","description":"<p>The most recent year has witnessed the success of applying the Vision\nTransformer (ViT) for image classification. However, there are still evidences\nindicating that ViT often suffers following two aspects, i) the high\ncomputation and the memory burden from applying the multiple Transformer layers\nfor pre-training on a large-scale dataset, ii) the over-fitting when training\non small datasets from scratch. To address these problems, a novel method,\nnamely, Vision Reservoir computing (ViR), is proposed here for image\nclassification, as a parallel to ViT. By splitting each image into a sequence\nof tokens with fixed length, the ViR constructs a pure reservoir with a nearly\nfully connected topology to replace the Transformer module in ViT. Two kinds of\ndeep ViR models are subsequently proposed to enhance the network performance.\nComparative experiments between the ViR and the ViT are carried out on several\nimage classification benchmarks. Without any pre-training process, the ViR\noutperforms the ViT in terms of both model and computational complexity.\nSpecifically, the number of parameters of the ViR is about 15% even 5% of the\nViT, and the memory footprint is about 20% to 40% of the ViT. The superiority\nof the ViR performance is explained by Small-World characteristics, Lyapunov\nexponents, and memory capacity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xian Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mingsong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Ji Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_H/0/1/0/all/0/1\">Hai Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jiehuang Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xuan Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_B/0/1/0/all/0/1\">Bo Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guozhang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Dongping Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Robust and Lightweight Model through Separable Structured Transformations. (arXiv:2112.13551v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.13551","description":"<p>With the proliferation of mobile devices and the Internet of Things, deep\nlearning models are increasingly deployed on devices with limited computing\nresources and memory, and are exposed to the threat of adversarial noise.\nLearning deep models with both lightweight and robustness is necessary for\nthese equipments. However, current deep learning solutions are difficult to\nlearn a model that possesses these two properties without degrading one or the\nother. As is well known, the fully-connected layers contribute most of the\nparameters of convolutional neural networks. We perform a separable structural\ntransformation of the fully-connected layer to reduce the parameters, where the\nlarge-scale weight matrix of the fully-connected layer is decoupled by the\ntensor product of several separable small-sized matrices. Note that data, such\nas images, no longer need to be flattened before being fed to the\nfully-connected layer, retaining the valuable spatial geometric information of\nthe data. Moreover, in order to further enhance both lightweight and\nrobustness, we propose a joint constraint of sparsity and differentiable\ncondition number, which is imposed on these separable matrices. We evaluate the\nproposed approach on MLP, VGG-16 and Vision Transformer. The experimental\nresults on datasets such as ImageNet, SVHN, CIFAR-100 and CIFAR10 show that we\nsuccessfully reduce the amount of network parameters by 90%, while the robust\naccuracy loss is less than 1.5%, which is better than the SOTA methods based on\nthe original fully-connected layer. Interestingly, it can achieve an\noverwhelming advantage even at a high compression rate, e.g., 200 times.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xian Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yanhui Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yangyu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mingsong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_H/0/1/0/all/0/1\">Hai Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuanxiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhongfeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xuan Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Domain Balanced Sampling Improves Out-of-Distribution Generalization of Chest X-ray Pathology Prediction Models. (arXiv:2112.13734v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.13734","description":"<p>Learning models that generalize under different distribution shifts in\nmedical imaging has been a long-standing research challenge. There have been\nseveral proposals for efficient and robust visual representation learning among\nvision research practitioners, especially in the sensitive and critical\nbiomedical domain. In this paper, we propose an idea for out-of-distribution\ngeneralization of chest X-ray pathologies that uses a simple balanced batch\nsampling technique. We observed that balanced sampling between the multiple\ntraining datasets improves the performance over baseline models trained without\nbalancing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tetteh_E/0/1/0/all/0/1\">Enoch Tetteh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Viviano_J/0/1/0/all/0/1\">Joseph Viviano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1\">Yoshua Bengio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krueger_D/0/1/0/all/0/1\">David Krueger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_J/0/1/0/all/0/1\">Joseph Paul Cohen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-12-29T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}}]}]}