{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.6","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-10-29T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Emoji-aware Co-attention Network with EmoGraph2vec Model for Sentiment Anaylsis. (arXiv:2110.14636v1 [cs.CL])","link":"http://arxiv.org/abs/2110.14636","description":"<p>In social media platforms, emojis have an extremely high occurrence in\ncomputer-mediated communications. Many emojis are used to strengthen the\nemotional expressions and the emojis that co-occurs in a sentence also have a\nstrong sentiment connection. However, when it comes to emoji representation\nlearning, most studies have only utilized the fixed descriptions provided by\nthe Unicode Consortium, without consideration of actual usage scenario. As for\nthe sentiment analysis task, many researchers ignore the emotional impact of\nthe interaction between text and emojis. It results that the emotional\nsemantics of emojis cannot be fully explored. In this work, we propose a method\nto learn emoji representations called EmoGraph2vec and design an emoji-aware\nco-attention network that learns the mutual emotional semantics between text\nand emojis on short texts of social media. In EmoGraph2vec, we form an emoji\nco-occurrence network on real social data and enrich the semantic information\nbased on an external knowledge base EmojiNet to obtain emoji node embeddings.\nOur model designs a co-attention mechanism to incorporate the text and emojis,\nand integrates a squeeze-and-excitation (SE) block into a convolutional neural\nnetwork as a classifier. Finally, we use the transfer learning method to\nincrease converge speed and achieve higher accuracy. Experimental results show\nthat the proposed model can outperform several baselines for sentiment analysis\non benchmark datasets. Additionally, we conduct a series of ablation and\ncomparison experiments to investigate the effectiveness of our model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xiaowei Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jingyuan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaodan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_H/0/1/0/all/0/1\">Honglei Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hao Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Realistic Single-Task Continuous Learning Research for NER. (arXiv:2110.14694v1 [cs.CL])","link":"http://arxiv.org/abs/2110.14694","description":"<p>There is an increasing interest in continuous learning (CL), as data privacy\nis becoming a priority for real-world machine learning applications. Meanwhile,\nthere is still a lack of academic NLP benchmarks that are applicable for\nrealistic CL settings, which is a major challenge for the advancement of the\nfield. In this paper we discuss some of the unrealistic data characteristics of\npublic datasets, study the challenges of realistic single-task continuous\nlearning as well as the effectiveness of data rehearsal as a way to mitigate\naccuracy loss. We construct a CL NER dataset from an existing publicly\navailable dataset and release it along with the code to the research community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Payan_J/0/1/0/all/0/1\">Justin Payan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merhav_Y/0/1/0/all/0/1\">Yuval Merhav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1\">He Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishna_S/0/1/0/all/0/1\">Satyapriya Krishna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramakrishna_A/0/1/0/all/0/1\">Anil Ramakrishna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sridhar_M/0/1/0/all/0/1\">Mukund Sridhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1\">Rahul Gupta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Anomaly-Injected Deep Support Vector Data Description for Text Outlier Detection. (arXiv:2110.14729v1 [cs.CL])","link":"http://arxiv.org/abs/2110.14729","description":"<p>Anomaly detection or outlier detection is a common task in various domains,\nwhich has attracted significant research efforts in recent years. Existing\nworks mainly focus on structured data such as numerical or categorical data;\nhowever, anomaly detection on unstructured textual data is less attended. In\nthis work, we target the textual anomaly detection problem and propose a deep\nanomaly-injected support vector data description (AI-SVDD) framework. AI-SVDD\nnot only learns a more compact representation of the data hypersphere but also\nadopts a small number of known anomalies to increase the discriminative power.\nTo tackle text input, we employ a multilayer perceptron (MLP) network in\nconjunction with BERT to obtain enriched text representations. We conduct\nexperiments on three text anomaly detection applications with multiple\ndatasets. Experimental results show that the proposed AI-SVDD is promising and\noutperforms existing works.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+You_Z/0/1/0/all/0/1\">Zeyu You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yichu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Tao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_W/0/1/0/all/0/1\">Wei Fan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Review-based Recommenders. (arXiv:2110.14747v1 [cs.IR])","link":"http://arxiv.org/abs/2110.14747","description":"<p>Just as user preferences change with time, item reviews also reflect those\nsame preference changes. In a nutshell, if one is to sequentially incorporate\nreview content knowledge into recommender systems, one is naturally led to\ndynamical models of text. In the present work we leverage the known power of\nreviews to enhance rating predictions in a way that (i) respects the causality\nof review generation and (ii) includes, in a bidirectional fashion, the ability\nof ratings to inform language review models and vice-versa, language\nrepresentations that help predict ratings end-to-end. Moreover, our\nrepresentations are time-interval aware and thus yield a continuous-time\nrepresentation of the dynamics. We provide experiments on real-world datasets\nand show that our methodology is able to outperform several state-of-the-art\nmodels. Source code for all models can be found at [1].\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cvejoski_K/0/1/0/all/0/1\">Kostadin Cvejoski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_R/0/1/0/all/0/1\">Ramses J. Sanchez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bauckhage_C/0/1/0/all/0/1\">Christian Bauckhage</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ojeda_C/0/1/0/all/0/1\">Cesar Ojeda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalized Funnelling: Ensemble Learning and Heterogeneous Document Embeddings for Cross-Lingual Text Classification. (arXiv:2110.14764v1 [cs.CL])","link":"http://arxiv.org/abs/2110.14764","description":"<p>\\emph{Funnelling} (Fun) is a recently proposed method for cross-lingual text\nclassification (CLTC) based on a two-tier learning ensemble for heterogeneous\ntransfer learning (HTL). In this ensemble method, 1st-tier classifiers, each\nworking on a different and language-dependent feature space, return a vector of\ncalibrated posterior probabilities (with one dimension for each class) for each\ndocument, and the final classification decision is taken by a metaclassifier\nthat uses this vector as its input. The metaclassifier can thus exploit\nclass-class correlations, and this (among other things) gives Fun an edge over\nCLTC systems in which these correlations cannot be brought to bear. In this\npaper we describe \\emph{Generalized Funnelling} (gFun), a generalization of Fun\nconsisting of an HTL architecture in which 1st-tier components can be arbitrary\n\\emph{view-generating functions}, i.e., language-dependent functions that each\nproduce a language-independent representation (\"view\") of the document. We\ndescribe an instance of gFun in which the metaclassifier receives as input a\nvector of calibrated posterior probabilities (as in Fun) aggregated to other\nembedded representations that embody other types of correlations, such as\nword-class correlations (as encoded by \\emph{Word-Class Embeddings}), word-word\ncorrelations (as encoded by \\emph{Multilingual Unsupervised or Supervised\nEmbeddings}), and word-context correlations (as encoded by \\emph{multilingual\nBERT}). We show that this instance of \\textsc{gFun} substantially improves over\nFun and over state-of-the-art baselines, by reporting experimental results\nobtained on two large, standard datasets for multilingual multilabel text\nclassification. Our code that implements gFun is publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moreo_A/0/1/0/all/0/1\">Alejandro Moreo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pedrotti_A/0/1/0/all/0/1\">Andrea Pedrotti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebastiani_F/0/1/0/all/0/1\">Fabrizio Sebastiani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine Grained Human Evaluation for English-to-Chinese Machine Translation: A Case Study on Scientific Text. (arXiv:2110.14766v1 [cs.CL])","link":"http://arxiv.org/abs/2110.14766","description":"<p>Recent research suggests that neural machine translation (MT) in the news\ndomain has reached human-level performance, but for other professional domains,\nit is far below the level. In this paper, we conduct a fine-grained systematic\nhuman evaluation for four widely used Chinese-English NMT systems on scientific\nabstracts which are collected from published journals and books. Our human\nevaluation results show that all the systems return with more than 10\\% error\nrates on average, which requires much post editing effort for real academic\nuse. Furthermore, we categorize six main error types and and provide some real\nexamples. Our findings emphasise the needs that research attention in the MT\ncommunity should be shifted from short text generic translation to professional\nmachine translation and build large scale bilingual corpus for these specific\ndomains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Ming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">He Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1\">Guanhao Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Dementia from Speech and Transcripts using Transformers. (arXiv:2110.14769v1 [cs.CL])","link":"http://arxiv.org/abs/2110.14769","description":"<p>Alzheimer's disease (AD) constitutes a neurodegenerative disease with serious\nconsequences to peoples' everyday lives, if it is not diagnosed early since\nthere is no available cure. Because of the cost of examinations for diagnosing\ndementia, i.e., Magnetic Resonance Imaging (MRI), electroencephalogram (EEG)\nsignals etc., current work has been focused on diagnosing dementia from\nspontaneous speech. However, little work has been done regarding the conversion\nof speech data to Log-Mel spectrograms and Mel-frequency cepstral coefficients\n(MFCCs) and the usage of pretrained models. Concurrently, little work has been\ndone in terms of both the usage of transformer networks and the way the two\nmodalities, i.e., speech and transcripts, are combined in a single neural\nnetwork. To address these limitations, first we employ several pretrained\nmodels, with Vision Transformer (ViT) achieving the highest evaluation results.\nSecondly, we propose multimodal models. More specifically, our introduced\nmodels include Gated Multimodal Unit in order to control the influence of each\nmodality towards the final classification and crossmodal attention so as to\ncapture in an effective way the relationships between the two modalities.\nExtensive experiments conducted on the ADReSS Challenge dataset demonstrate the\neffectiveness of the proposed models and their superiority over\nstate-of-the-art approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ilias_L/0/1/0/all/0/1\">Loukas Ilias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Askounis_D/0/1/0/all/0/1\">Dimitris Askounis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Psarras_J/0/1/0/all/0/1\">John Psarras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Combining Vagueness Detection with Deep Learning to Identify Fake News. (arXiv:2110.14780v1 [cs.CL])","link":"http://arxiv.org/abs/2110.14780","description":"<p>In this paper, we combine two independent detection methods for identifying\nfake news: the algorithm VAGO uses semantic rules combined with NLP techniques\nto measure vagueness and subjectivity in texts, while the classifier FAKE-CLF\nrelies on Convolutional Neural Network classification and supervised deep\nlearning to classify texts as biased or legitimate. We compare the results of\nthe two methods on four corpora. We find a positive correlation between the\nvagueness and subjectivity measures obtained by VAGO, and the classification of\ntext as biased by FAKE-CLF. The comparison yields mutual benefits: VAGO helps\nexplain the results of FAKE-CLF. Conversely FAKE-CLF helps us corroborate and\nexpand VAGO's database. The use of two complementary techniques (rule-based vs\ndata-driven) proves a fruitful approach for the challenging problem of\nidentifying fake news.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guelorget_P/0/1/0/all/0/1\">Paul Gu&#xe9;lorget</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Icard_B/0/1/0/all/0/1\">Benjamin Icard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gadek_G/0/1/0/all/0/1\">Guillaume Gadek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghabiche_S/0/1/0/all/0/1\">Souhir Ghabiche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gatepaille_S/0/1/0/all/0/1\">Sylvain Gatepaille</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atemezing_G/0/1/0/all/0/1\">Ghislain Atemezing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Egre_P/0/1/0/all/0/1\">Paul &#xc9;gr&#xe9;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"When is BERT Multilingual? Isolating Crucial Ingredients for Cross-lingual Transfer. (arXiv:2110.14782v1 [cs.CL])","link":"http://arxiv.org/abs/2110.14782","description":"<p>While recent work on multilingual language models has demonstrated their\ncapacity for cross-lingual zero-shot transfer on downstream tasks, there is a\nlack of consensus in the community as to what shared properties between\nlanguages enable such transfer. Analyses involving pairs of natural languages\nare often inconclusive and contradictory since languages simultaneously differ\nin many linguistic aspects. In this paper, we perform a large-scale empirical\nstudy to isolate the effects of various linguistic properties by measuring\nzero-shot transfer between four diverse natural languages and their\ncounterparts constructed by modifying aspects such as the script, word order,\nand syntax. Among other things, our experiments show that the absence of\nsub-word overlap significantly affects zero-shot transfer when languages differ\nin their word order, and there is a strong correlation between transfer\nperformance and word embedding alignment between languages (e.g., R=0.94 on the\ntask of NLI). Our results call for focus in multilingual models on explicitly\nimproving word embedding alignment between languages rather than relying on its\nimplicit emergence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deshpande_A/0/1/0/all/0/1\">Ameet Deshpande</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talukdar_P/0/1/0/all/0/1\">Partha Talukdar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narasimhan_K/0/1/0/all/0/1\">Karthik Narasimhan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hate Speech Classifiers Learn Human-Like Social Stereotypes. (arXiv:2110.14839v1 [cs.CL])","link":"http://arxiv.org/abs/2110.14839","description":"<p>Social stereotypes negatively impact individuals' judgements about different\ngroups and may have a critical role in how people understand language directed\ntoward minority social groups. Here, we assess the role of social stereotypes\nin the automated detection of hateful language by examining the relation\nbetween individual annotator biases and erroneous classification of texts by\nhate speech classifiers. Specifically, in Study 1 we investigate the impact of\nnovice annotators' stereotypes on their hate-speech-annotation behavior. In\nStudy 2 we examine the effect of language-embedded stereotypes on expert\nannotators' aggregated judgements in a large annotated corpus. Finally, in\nStudy 3 we demonstrate how language-embedded stereotypes are associated with\nsystematic prediction errors in a neural-network hate speech classifier. Our\nresults demonstrate that hate speech classifiers learn human-like biases which\ncan further perpetuate social inequalities when propagated at scale. This\nframework, combining social psychological and computational linguistic methods,\nprovides insights into additional sources of bias in hate speech moderation,\ninforming ongoing debates regarding fairness in machine learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Davani_A/0/1/0/all/0/1\">Aida Mostafazadeh Davani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atari_M/0/1/0/all/0/1\">Mohammad Atari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kennedy_B/0/1/0/all/0/1\">Brendan Kennedy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1\">Morteza Dehghani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Sequence to Sequence Model for Extracting Multiple Product Name Entities from Dialog. (arXiv:2110.14843v1 [cs.CL])","link":"http://arxiv.org/abs/2110.14843","description":"<p>E-commerce voice ordering systems need to recognize multiple product name\nentities from ordering utterances. Existing voice ordering systems such as\nAmazon Alexa can capture only a single product name entity. This restrains\nusers from ordering multiple items with one utterance. In recent years,\npre-trained language models, e.g., BERT and GPT-2, have shown promising results\non NLP benchmarks like Super-GLUE. However, they can't perfectly generalize to\nthis Multiple Product Name Entity Recognition (MPNER) task due to the ambiguity\nin voice ordering utterances. To fill this research gap, we propose Entity\nTransformer (ET) neural network architectures which recognize up to 10 items in\nan utterance. In our evaluation, the best ET model (conveRT + ngram + ET) has a\nperformance improvement of 12% on our test set compared to the non-neural\nmodel, and outperforms BERT with ET as well. This helps customers finalize\ntheir shopping cart via voice dialog, which improves shopping efficiency and\nexperience.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gubbala_P/0/1/0/all/0/1\">Praneeth Gubbala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xuan Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Colossal-AI: A Unified Deep Learning System For Large-Scale Parallel Training. (arXiv:2110.14883v1 [cs.LG])","link":"http://arxiv.org/abs/2110.14883","description":"<p>The Transformer architecture has improved the performance of deep learning\nmodels in domains such as Computer Vision and Natural Language Processing.\nTogether with better performance come larger model sizes. This imposes\nchallenges to the memory wall of the current accelerator hardware such as GPU.\nIt is never ideal to train large models such as Vision Transformer, BERT, and\nGPT on a single GPU or a single machine. There is an urgent demand to train\nmodels in a distributed environment. However, distributed training, especially\nmodel parallelism, often requires domain expertise in computer systems and\narchitecture. It remains a challenge for AI researchers to implement complex\ndistributed training solutions for their models.\n</p>\n<p>In this paper, we introduce Colossal-AI, which is a unified parallel training\nsystem designed to seamlessly integrate different paradigms of parallelization\ntechniques including data parallelism, pipeline parallelism, multiple tensor\nparallelism, and sequence parallelism. Colossal-AI aims to support the AI\ncommunity to write distributed models in the same way as how they write models\nnormally. This allows them to focus on developing the model architecture and\nseparates the concerns of distributed training from the development process.\nThe documentations can be found at https://www.colossalai.org and the source\ncode can be found at https://github.com/hpcaitech/ColossalAI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bian_Z/0/1/0/all/0/1\">Zhengda Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hongxin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Boxiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Haichen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yongbin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chuanrui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_F/0/1/0/all/0/1\">Fan Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1\">Yang You</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-Siamese Bi-encoder Neural Ranking Model Using Lightweight Fine-Tuning. (arXiv:2110.14943v1 [cs.CL])","link":"http://arxiv.org/abs/2110.14943","description":"<p>A BERT-based Neural Ranking Model (NRM) can be either a cross-encoder or a\nbi-encoder. Between the two, bi-encoder is highly efficient because all the\ndocuments can be pre-processed before the actual query time. Although query and\ndocument are independently encoded, the existing bi-encoder NRMs are Siamese\nmodels where a single language model is used for consistently encoding both of\nquery and document. In this work, we show two approaches for improving the\nperformance of BERT-based bi-encoders. The first approach is to replace the\nfull fine-tuning step with a lightweight fine-tuning. We examine lightweight\nfine-tuning methods that are adapter-based, prompt-based, and hybrid of the\ntwo. The second approach is to develop semi-Siamese models where queries and\ndocuments are handled with a limited amount of difference. The limited\ndifference is realized by learning two lightweight fine-tuning modules, where\nthe main language model of BERT is kept common for both query and document. We\nprovide extensive experiment results for monoBERT, TwinBERT, and ColBERT where\nthree performance metrics are evaluated over Robust04, ClueWeb09b, and MS-MARCO\ndatasets. The results confirm that both lightweight fine-tuning and\nsemi-Siamese are considerably helpful for improving BERT-based bi-encoders. In\nfact, lightweight fine-tuning is helpful for cross-encoder, too.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jung_E/0/1/0/all/0/1\">Euna Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jaekeol Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rhee_W/0/1/0/all/0/1\">Wonjong Rhee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Preventing posterior collapse in variational autoencoders for text generation via decoder regularization. (arXiv:2110.14945v1 [cs.LG])","link":"http://arxiv.org/abs/2110.14945","description":"<p>Variational autoencoders trained to minimize the reconstruction error are\nsensitive to the posterior collapse problem, that is the proposal posterior\ndistribution is always equal to the prior. We propose a novel regularization\nmethod based on fraternal dropout to prevent posterior collapse. We evaluate\nour approach using several metrics and observe improvements in all the tested\nconfigurations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Petit_A/0/1/0/all/0/1\">Alban Petit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corro_C/0/1/0/all/0/1\">Caio Corro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End Speech Emotion Recognition: Challenges of Real-Life Emergency Call Centers Data Recordings. (arXiv:2110.14957v1 [cs.AI])","link":"http://arxiv.org/abs/2110.14957","description":"<p>Recognizing a speaker's emotion from their speech can be a key element in\nemergency call centers. End-to-end deep learning systems for speech emotion\nrecognition now achieve equivalent or even better results than conventional\nmachine learning approaches. In this paper, in order to validate the\nperformance of our neural network architecture for emotion recognition from\nspeech, we first trained and tested it on the widely used corpus accessible by\nthe community, IEMOCAP. We then used the same architecture as the real life\ncorpus, CEMO, composed of 440 dialogs (2h16m) from 485 speakers. The most\nfrequent emotions expressed by callers in these real life emergency dialogues\nare fear, anger and positive emotions such as relief. In the IEMOCAP general\ntopic conversations, the most frequent emotions are sadness, anger and\nhappiness. Using the same end-to-end deep learning architecture, an Unweighted\nAccuracy Recall (UA) of 63% is obtained on IEMOCAP and a UA of 45.6% on CEMO,\neach with 4 classes. Using only 2 classes (Anger, Neutral), the results for\nCEMO are 76.9% UA compared to 81.1% UA for IEMOCAP. We expect that these\nencouraging results with CEMO can be improved by combining the audio channel\nwith the linguistic channel. Real-life emotions are clearly more complex than\nacted ones, mainly due to the large diversity of emotional expressions of\nspeakers. Index Terms-emotion detection, end-to-end deep learning architecture,\ncall center, real-life database, complex emotions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deschamps_Berger_T/0/1/0/all/0/1\">Th&#xe9;o Deschamps-Berger</a> (LISN, CNRS), <a href=\"http://arxiv.org/find/cs/1/au:+Lamel_L/0/1/0/all/0/1\">Lori Lamel</a> (LISN, CNRS), <a href=\"http://arxiv.org/find/cs/1/au:+Devillers_L/0/1/0/all/0/1\">Laurence Devillers</a> (LISN, CNRS, SU)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Empirical Analysis of Korean Public AI Hub Parallel Corpora and in-depth Analysis using LIWC. (arXiv:2110.15023v1 [cs.CL])","link":"http://arxiv.org/abs/2110.15023","description":"<p>Machine translation (MT) system aims to translate source language into target\nlanguage. Recent studies on MT systems mainly focus on neural machine\ntranslation (NMT). One factor that significantly affects the performance of NMT\nis the availability of high-quality parallel corpora. However, high-quality\nparallel corpora concerning Korean are relatively scarce compared to those\nassociated with other high-resource languages, such as German or Italian. To\naddress this problem, AI Hub recently released seven types of parallel corpora\nfor Korean. In this study, we conduct an in-depth verification of the quality\nof corresponding parallel corpora through Linguistic Inquiry and Word Count\n(LIWC) and several relevant experiments. LIWC is a word-counting software\nprogram that can analyze corpora in multiple ways and extract linguistic\nfeatures as a dictionary base. To the best of our knowledge, this study is the\nfirst to use LIWC to analyze parallel corpora in the field of NMT. Our findings\nsuggest the direction of further research toward obtaining the improved quality\nparallel corpora through our correlation analysis in LIWC and NMT performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1\">Chanjun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shim_M/0/1/0/all/0/1\">Midan Shim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eo_S/0/1/0/all/0/1\">Sugyeong Eo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seolhwa Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_J/0/1/0/all/0/1\">Jaehyung Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moon_H/0/1/0/all/0/1\">Hyeonseok Moon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_H/0/1/0/all/0/1\">Heuiseok Lim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Multimodal and Multisensory Empathic Technologies for Enhanced Human Communication. (arXiv:2110.15054v1 [cs.HC])","link":"http://arxiv.org/abs/2110.15054","description":"<p>As digital social platforms and mobile technologies are becoming more\nprevalent and robust, the use of Artificial Intelligence (AI) in facilitating\nhuman communication will grow. This, in turn, will pave the way for the\ndevelopment of intuitive, adaptive, and effective empathic AI interfaces that\nbetter address the needs of socially and culturally diverse communities. I\nbelieve such developments must consider a principled framework that includes\nthe human perceptual senses in the digital design process right from the start,\nfor a more accurate, as well as a more aesthetic, memorable, and soothing\nexperience. In this position paper, I suggest features, identify some\nchallenges that need to be addressed in the process, and propose some future\nresearch directions that I think should be part of the design and\nimplementation. Such an approach will allow various communities of practice to\ninvestigate the areas of intersection between artificial intelligence, on one\nside, and human communication, perceptual needs and social and cultural values,\non the other.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Girju_R/0/1/0/all/0/1\">Roxana Girju</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SenTag: a Web-based Tool for Semantic Annotation of Textual Documents. (arXiv:2110.15062v1 [cs.DL])","link":"http://arxiv.org/abs/2110.15062","description":"<p>In this work, we present SenTag, a lightweight web-based tool focused on\nsemantic annotation of textual documents. The platform allows multiple users to\nwork on a corpus of documents. The tool enables to tag a corpus of documents\nthrough an intuitive and easy-to-use user interface that adopts the Extensible\nMarkup Language (XML) as output format. The main goal of the application is\ntwo-fold: facilitating the tagging process and reducing or avoiding for errors\nin the output documents. Moreover, it allows to identify arguments and other\nentities that are used to build an arguments graph. It is also possible to\nassess the level of agreement of annotators working on a corpus of text.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Loreggia_A/0/1/0/all/0/1\">Andrea Loreggia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mosco_S/0/1/0/all/0/1\">Simone Mosco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zerbinati_A/0/1/0/all/0/1\">Alberto Zerbinati</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TEXTOIR: An Integrated and Visualized Platform for Text Open Intent Recognition. (arXiv:2110.15063v1 [cs.CL])","link":"http://arxiv.org/abs/2110.15063","description":"<p>TEXTOIR is the first integrated and visualized platform for text open intent\nrecognition. It is composed of two main modules: open intent detection and open\nintent discovery. Each module integrates most of the state-of-the-art\nalgorithms and benchmark intent datasets. It also contains an overall framework\nconnecting the two modules in a pipeline scheme. In addition, this platform has\nvisualized tools for data and model management, training, evaluation and\nanalysis of the performance from different aspects. TEXTOIR provides useful\ntoolkits and convenient visualized interfaces for each sub-module (Toolkit\ncode: https://github.com/thuiar/TEXTOIR), and designs a framework to implement\na complete process to both identify known intents and discover open intents\n(Demo code: https://github.com/thuiar/TEXTOIR-DEMO).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hanlei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoteng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hua Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Panpan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_K/0/1/0/all/0/1\">Kang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_K/0/1/0/all/0/1\">Kai Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Fine-Grained Reasoning for Fake News Detection. (arXiv:2110.15064v1 [cs.CL])","link":"http://arxiv.org/abs/2110.15064","description":"<p>The detection of fake news often requires sophisticated reasoning skills,\nsuch as logically combining information by considering word-level subtle clues.\nIn this paper, we move towards fine-grained reasoning for fake news detection\nby better reflecting the logical processes of human thinking and enabling the\nmodeling of subtle clues. In particular, we propose a fine-grained reasoning\nframework by following the human's information-processing model, introduce a\nmutual-reinforcement-based method for incorporating human knowledge about which\nevidence is more important, and design a prior-aware bi-channel kernel graph\nnetwork to model subtle differences between pieces of evidence. Extensive\nexperiments show that our model outperforms the state-of-art methods and\ndemonstrate the explainability of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yiqiao Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiting Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1\">Ruichao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yizhou Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_H/0/1/0/all/0/1\">Hao Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xing Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Abstract, Rationale, Stance: A Joint Model for Scientific Claim Verification. (arXiv:2110.15116v1 [cs.CL])","link":"http://arxiv.org/abs/2110.15116","description":"<p>Scientific claim verification can help the researchers to easily find the\ntarget scientific papers with the sentence evidence from a large corpus for the\ngiven claim. Some existing works propose pipeline models on the three tasks of\nabstract retrieval, rationale selection and stance prediction. Such works have\nthe problems of error propagation among the modules in the pipeline and lack of\nsharing valuable information among modules. We thus propose an approach, named\nas ARSJoint, that jointly learns the modules for the three tasks with a machine\nreading comprehension framework by including claim information. In addition, we\nenhance the information exchanges and constraints among tasks by proposing a\nregularization term between the sentence attention scores of abstract retrieval\nand the estimated outputs of rational selection. The experimental results on\nthe benchmark dataset SciFact show that our approach outperforms the existing\nworks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhiwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fukumoto_F/0/1/0/all/0/1\">Fumiyo Fukumoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1\">Yanming Ye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Confounds and Overestimations in Fake Review Detection: Experimentally Controlling for Product-Ownership and Data-Origin. (arXiv:2110.15130v1 [cs.CL])","link":"http://arxiv.org/abs/2110.15130","description":"<p>The popularity of online shopping is steadily increasing. At the same time,\nfake product reviewsare published widely and have the potential to affect\nconsumer purchasing behavior. In response,previous work has developed automated\nmethods for the detection of deceptive product reviews.However, studies vary\nconsiderably in terms of classification performance, and many use data\nthatcontain potential confounds, which makes it difficult to determine their\nvalidity. Two possibleconfounds are data-origin (i.e., the dataset is composed\nof more than one source) and productownership (i.e., reviews written by\nindividuals who own or do not own the reviewed product). Inthe present study,\nwe investigate the effect of both confounds for fake review detection. Using\nanexperimental design, we manipulate data-origin, product ownership, review\npolarity, and veracity.Supervised learning analysis suggests that review\nveracity (60.26 - 69.87%) is somewhat detectablebut reviews additionally\nconfounded with product-ownership (66.19 - 74.17%), or with data-origin(84.44 -\n86.94%) are easier to classify. Review veracity is most easily classified if\nconfounded withproduct-ownership and data-origin combined (87.78 - 88.12%),\nsuggesting overestimations of thetrue performance in other work. These findings\nare moderated by review polarity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Soldner_F/0/1/0/all/0/1\">Felix Soldner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kleinberg_B/0/1/0/all/0/1\">Bennett Kleinberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_S/0/1/0/all/0/1\">Shane Johnson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating Table Vector Representations. (arXiv:2110.15132v1 [cs.LG])","link":"http://arxiv.org/abs/2110.15132","description":"<p>High-quality Web tables are rich sources of information that can be used to\npopulate Knowledge Graphs (KG). The focus of this paper is an evaluation of\nmethods for table-to-class annotation, which is a sub-task of Table\nInterpretation (TI). We provide a formal definition for table classification as\na machine learning task. We propose an experimental setup and we evaluate 5\nfundamentally different approaches to find the best method for generating\nvector table representations. Our findings indicate that although transfer\nlearning methods achieve high F1 score on the table classification task,\ndedicated table encoding models are a promising direction as they appear to\ncapture richer semantics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Koleva_A/0/1/0/all/0/1\">Aneta Koleva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ringsquandl_M/0/1/0/all/0/1\">Martin Ringsquandl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joblin_M/0/1/0/all/0/1\">Mitchell Joblin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tresp_V/0/1/0/all/0/1\">Volker Tresp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Analysis of Programming Course Evaluations Before and After the Introduction of an Autograder. (arXiv:2110.15134v1 [cs.HC])","link":"http://arxiv.org/abs/2110.15134","description":"<p>Commonly, introductory programming courses in higher education institutions\nhave hundreds of participating students eager to learn to program. The manual\neffort for reviewing the submitted source code and for providing feedback can\nno longer be managed. Manually reviewing the submitted homework can be\nsubjective and unfair, particularly if many tutors are responsible for grading.\nDifferent autograders can help in this situation; however, there is a lack of\nknowledge about how autograders can impact students' overall perception of\nprogramming classes and teaching. This is relevant for course organizers and\ninstitutions to keep their programming courses attractive while coping with\nincreasing students.\n</p>\n<p>This paper studies the answers to the standardized university evaluation\nquestionnaires of multiple large-scale foundational computer science courses\nwhich recently introduced autograding. The differences before and after this\nintervention are analyzed. By incorporating additional observations, we\nhypothesize how the autograder might have contributed to the significant\nchanges in the data, such as, improved interactions between tutors and\nstudents, improved overall course quality, improved learning success, increased\ntime spent, and reduced difficulty. This qualitative study aims to provide\nhypotheses for future research to define and conduct quantitative surveys and\ndata analysis. The autograder technology can be validated as a teaching method\nto improve student satisfaction with programming courses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hagerer_G/0/1/0/all/0/1\">Gerhard Hagerer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lahesoo_L/0/1/0/all/0/1\">Laura Lahesoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anschutz_M/0/1/0/all/0/1\">Miriam Ansch&#xfc;tz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krusche_S/0/1/0/all/0/1\">Stephan Krusche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Groh_G/0/1/0/all/0/1\">Georg Groh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diversity-Driven Combination for Grammatical Error Correction. (arXiv:2110.15149v1 [cs.CL])","link":"http://arxiv.org/abs/2110.15149","description":"<p>Grammatical error correction (GEC) is the task of detecting and correcting\nerrors in a written text. The idea of combining multiple system outputs has\nbeen successfully used in GEC. To achieve successful system combination,\nmultiple component systems need to produce corrected sentences that are both\ndiverse and of comparable quality. However, most existing state-of-the-art GEC\napproaches are based on similar sequence-to-sequence neural networks, so the\ngains are limited from combining the outputs of component systems similar to\none another. In this paper, we present Diversity-Driven Combination (DDC) for\nGEC, a system combination strategy that encourages diversity among component\nsystems. We evaluate our system combination strategy on the CoNLL-2014 shared\ntask and the BEA-2019 shared task. On both benchmarks, DDC achieves significant\nperformance gain with a small number of training examples and outperforms the\ncomponent systems by a large margin. Our source code is available at\nhttps://github.com/nusnlp/gec-ddc.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1\">Wenjuan Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_H/0/1/0/all/0/1\">Hwee Tou Ng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BERTian Poetics: Constrained Composition with Masked LMs. (arXiv:2110.15181v1 [cs.CL])","link":"http://arxiv.org/abs/2110.15181","description":"<p>Masked language models have recently been interpreted as energy-based\nsequence models that can be generated from using a Metropolis--Hastings\nsampler. This short paper demonstrates how this can be instrumentalized for\nconstrained composition and explores the poetics implied by such a usage. Our\nfocus on constraints makes it especially apt to understand the generated text\nthrough the poetics of the OuLiPo movement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Akiki_C/0/1/0/all/0/1\">Christopher Akiki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potthast_M/0/1/0/all/0/1\">Martin Potthast</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Add-On for Empowering Google Forms to be an Automatic Question Generator in Online Assessments. (arXiv:2110.15220v1 [cs.CL])","link":"http://arxiv.org/abs/2110.15220","description":"<p>This research suggests an add-on to empower Google Forms to be an automatic\nmachine for generating multiple-choice questions (MCQs) used in online\nassessments. In this paper, we elaborate an add-on design mainly comprising\nquestion-formulating software and data storage. The algorithm as an\nintellectual mechanism of this software can produce MCQs at an analytical\nlevel. In an experiment, we found the MCQs could assess levels of students'\nknowledge comparably with those generated by human experts. This add-on can be\napplied generally to formulate MCQs for any rational concepts. With no effort\nfrom an instructor at runtime, the add-on can transform a few data instances\ndescribing rational concepts to be variety sets of MCQs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sirithumgul_P/0/1/0/all/0/1\">Pornpat Sirithumgul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prasertsilp_P/0/1/0/all/0/1\">Pimpaka Prasertsilp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olfman_L/0/1/0/all/0/1\">Lorne Olfman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Word-level confidence estimation for RNN transducers. (arXiv:2110.15222v1 [cs.CL])","link":"http://arxiv.org/abs/2110.15222","description":"<p>Confidence estimate is an often requested feature in applications such as\nmedical transcription where errors can impact patient care and the confidence\nestimate could be used to alert medical professionals to verify potential\nerrors in recognition.\n</p>\n<p>In this paper, we present a lightweight neural confidence model tailored for\nAutomatic Speech Recognition (ASR) system with Recurrent Neural Network\nTransducers (RNN-T). Compared to other existing approaches, our model utilizes:\n(a) the time information associated with recognized words, which reduces the\ncomputational complexity, and (b) a simple and elegant trick for mapping\nbetween sub-word and word sequences. The mapping addresses the non-unique\ntokenization and token deletion problems while amplifying differences between\nconfusable words. Through extensive empirical evaluations on two different\nlong-form test sets, we demonstrate that the model achieves a performance of\n0.4 Normalized Cross Entropy (NCE) and 0.05 Expected Calibration Error (ECE).\nIt is robust across different ASR configurations, including target types\n(graphemes vs. morphemes), traffic conditions (streaming vs. non-streaming),\nand encoder types. We further discuss the importance of evaluation metrics to\nreflect practical applications and highlight the need for further work in\nimproving Area Under the Curve (AUC) for Negative Precision Rate (NPV) and True\nNegative Rate (TNR).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mingqiu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soltau_H/0/1/0/all/0/1\">Hagen Soltau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shafey_L/0/1/0/all/0/1\">Laurent El Shafey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shafran_I/0/1/0/all/0/1\">Izhak Shafran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pruning Attention Heads of Transformer Models Using A* Search: A Novel Approach to Compress Big NLP Architectures. (arXiv:2110.15225v1 [cs.CL])","link":"http://arxiv.org/abs/2110.15225","description":"<p>Recent years have seen a growing adoption of Transformer models such as BERT\nin Natural Language Processing and even in Computer Vision. However, due to the\nsize, there has been limited adoption of such models within\nresource-constrained computing environments This paper proposes novel pruning\nalgorithms to compress transformer models by eliminating redundant Attention\nHeads. We apply the A* search algorithm to obtain a pruned model with minimal\naccuracy guarantees. Our results indicate that the method could eliminate as\nmuch as 40% of the attention heads in the BERT transformer model with almost no\nloss in accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Parnami_A/0/1/0/all/0/1\">Archit Parnami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1\">Rahul Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_T/0/1/0/all/0/1\">Tarun Joshi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-stage Clarification in Conversational AI: The case of Question-Answering Dialogue Systems. (arXiv:2110.15235v1 [cs.CL])","link":"http://arxiv.org/abs/2110.15235","description":"<p>Clarification resolution plays an important role in various information\nretrieval tasks such as interactive question answering and conversational\nsearch. In such context, the user often formulates their information needs as\nshort and ambiguous queries, some popular search interfaces then prompt the\nuser to confirm her intent (e.g. \"Did you mean ... ?\") or to rephrase if\nneeded. When it comes to dialogue systems, having fluid user-bot exchanges is\nkey to good user experience. In the absence of such clarification mechanism,\none of the following responses is given to the user: 1) A direct answer, which\ncan potentially be non-relevant if the intent was not clear, 2) a generic\nfallback message informing the user that the retrieval tool is incapable of\nhandling the query. Both scenarios might raise frustration and degrade the user\nexperience. To this end, we propose a multi-stage clarification mechanism for\nprompting clarification and query selection in the context of a question\nanswering dialogue system. We show that our proposed mechanism improves the\noverall user experience and outperforms competitive baselines with two\ndatasets, namely the public in-scope out-of-scope dataset and a commercial\ndataset based on real user logs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lautraite_H/0/1/0/all/0/1\">Hadrien Lautraite</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naji_N/0/1/0/all/0/1\">Nada Naji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marceau_L/0/1/0/all/0/1\">Louis Marceau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Queudot_M/0/1/0/all/0/1\">Marc Queudot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charton_E/0/1/0/all/0/1\">Eric Charton</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"\\'UFAL at MultiLexNorm 2021: Improving Multilingual Lexical Normalization by Fine-tuning ByT5. (arXiv:2110.15248v1 [cs.CL])","link":"http://arxiv.org/abs/2110.15248","description":"<p>We present the winning entry to the Multilingual Lexical Normalization\n(MultiLexNorm) shared task at W-NUT 2021 (van der Goot et al., 2021a), which\nevaluates lexical-normalization systems on 12 social media datasets in 11\nlanguages. We base our solution on a pre-trained byte-level language model,\nByT5 (Xue et al., 2021a), which we further pre-train on synthetic data and then\nfine-tune on authentic normalization data. Our system achieves the best\nperformance by a wide margin in intrinsic evaluation, and also the best\nperformance in extrinsic evaluation through dependency parsing. The source code\nis released at https://github.com/ufal/multilexnorm2021 and the fine-tuned\nmodels at https://huggingface.co/ufal.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Samuel_D/0/1/0/all/0/1\">David Samuel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Straka_M/0/1/0/all/0/1\">Milan Straka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cognitive network science quantifies feelings expressed in suicide letters and Reddit mental health communities. (arXiv:2110.15269v1 [cs.CL])","link":"http://arxiv.org/abs/2110.15269","description":"<p>Writing messages is key to expressing feelings. This study adopts cognitive\nnetwork science to reconstruct how individuals report their feelings in\nclinical narratives like suicide notes or mental health posts. We achieve this\nby reconstructing syntactic/semantic associations between conceptsin texts as\nco-occurrences enriched with affective data. We transform 142 suicide notes and\n77,000 Reddit posts from the r/anxiety, r/depression, r/schizophrenia, and\nr/do-it-your-own (r/DIY) forums into 5 cognitive networks, each one expressing\nmeanings and emotions as reported by authors. These networks reconstruct the\nsemantic frames surrounding \\textit{feel}, enabling a quantification of\nprominent associations and emotions focused around feelings. We find strong\nfeelings of sadness across all clinical Reddit boards, added to fear\nr/depression, and replaced by joy/anticipation in r/DIY. Semantic communities\nand topic modelling both highlight key narrative topics of \\textit{regret},\n\\textit{unhealthy lifestyle} and \\textit{low mental well-being}. Importantly,\nnegative associations and emotions co-existed with trustful/positive language,\nfocused on \\textit{getting better}. This emotional polarisation provides\nquantitative evidence that online clinical boards possess a complex structure,\nwhere users mix both positive and negative outlooks. This dichotomy is absent\nin the r/DIY reference board and in suicide notes, where negative emotional\nassociations about regret and pain persist but are overwhelmed by positive\njargon addressing loved ones. Our quantitative comparisons provide strong\nevidence that suicide notes encapsulate different ways of expressing feelings\ncompared to online Reddit boards, the latter acting more like personal diaries\nand relief valve. Our findings provide an interpretable, quantitative aid for\nsupporting psychological inquiries of human feelings in digital and clinical\nsettings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Joseph_S/0/1/0/all/0/1\">Simmi Marina Joseph</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Citraro_S/0/1/0/all/0/1\">Salvatore Citraro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morini_V/0/1/0/all/0/1\">Virginia Morini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rossetti_G/0/1/0/all/0/1\">Giulio Rossetti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stella_M/0/1/0/all/0/1\">Massimo Stella</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bridge the Gap Between CV and NLP! A Gradient-based Textual Adversarial Attack Framework. (arXiv:2110.15317v1 [cs.CL])","link":"http://arxiv.org/abs/2110.15317","description":"<p>Despite great success on many machine learning tasks, deep neural networks\nare still vulnerable to adversarial samples. While gradient-based adversarial\nattack methods are well-explored in the field of computer vision, it is\nimpractical to directly apply them in natural language processing due to the\ndiscrete nature of text. To bridge this gap, we propose a general framework to\nadapt existing gradient-based methods to craft textual adversarial samples. In\nthis framework, gradient-based continuous perturbations are added to the\nembedding layer and are amplified in the forward propagation process. Then the\nfinal perturbed latent representations are decoded with a mask language model\nhead to obtain potential adversarial samples. In this paper, we instantiate our\nframework with \\textbf{T}extual \\textbf{P}rojected \\textbf{G}radient\n\\textbf{D}escent (\\textbf{TPGD}). We conduct comprehensive experiments to\nevaluate our framework by performing transfer black-box attacks on BERT,\nRoBERTa and ALBERT on three benchmark datasets. Experimental results\ndemonstrate our method achieves an overall better performance and produces more\nfluent and grammatical adversarial samples compared to strong baseline methods.\nAll the code and data will be made public.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lifan Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yichi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yangyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1\">Wei Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Ground Multi-Agent Communication with Autoencoders. (arXiv:2110.15349v1 [cs.LG])","link":"http://arxiv.org/abs/2110.15349","description":"<p>Communication requires having a common language, a lingua franca, between\nagents. This language could emerge via a consensus process, but it may require\nmany generations of trial and error. Alternatively, the lingua franca can be\ngiven by the environment, where agents ground their language in representations\nof the observed world. We demonstrate a simple way to ground language in\nlearned representations, which facilitates decentralized multi-agent\ncommunication and coordination. We find that a standard representation learning\nalgorithm -- autoencoding -- is sufficient for arriving at a grounded common\nlanguage. When agents broadcast these representations, they learn to understand\nand respond to each other's utterances and achieve surprisingly strong task\nperformance across a variety of multi-agent communication environments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Toru Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huh_M/0/1/0/all/0/1\">Minyoung Huh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stauffer_C/0/1/0/all/0/1\">Chris Stauffer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1\">Ser-Nam Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Isola_P/0/1/0/all/0/1\">Phillip Isola</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Geometry matters: Exploring language examples at the decision boundary. (arXiv:2010.07212v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.07212","description":"<p>A growing body of recent evidence has highlighted the limitations of natural\nlanguage processing (NLP) datasets and classifiers. These include the presence\nof annotation artifacts in datasets, classifiers relying on shallow features\nlike a single word (e.g., if a movie review has the word \"romantic\", the review\ntends to be positive), or unnecessary words (e.g., learning a proper noun to\nclassify a movie as positive or negative). The presence of such artifacts has\nsubsequently led to the development of challenging datasets to force the model\nto generalize better. While a variety of heuristic strategies, such as\ncounterfactual examples and contrast sets, have been proposed, the theoretical\njustification about what makes these examples difficult for the classifier is\noften lacking or unclear. In this paper, using tools from information geometry,\nwe propose a theoretical way to quantify the difficulty of an example in NLP.\nUsing our approach, we explore difficult examples for several deep learning\narchitectures. We discover that both BERT, CNN and fasttext are susceptible to\nword substitutions in high difficulty examples. These classifiers tend to\nperform poorly on the FIM test set. (generated by sampling and perturbing\ndifficult examples, with accuracy dropping below 50%). We replicate our\nexperiments on 5 NLP datasets (YelpReviewPolarity, AGNEWS, SogouNews,\nYelpReviewFull and Yahoo Answers). On YelpReviewPolarity we observe a\ncorrelation coefficient of -0.4 between resilience to perturbations and the\ndifficulty score. Similarly we observe a correlation of 0.35 between the\ndifficulty score and the empirical success probability of random substitutions.\nOur approach is simple, architecture agnostic and can be used to study the\nfragilities of text classification models. All the code used will be made\npublicly available, including a tool to explore the difficult examples for\nother datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Datta_D/0/1/0/all/0/1\">Debajyoti Datta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Shashwat Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barnes_L/0/1/0/all/0/1\">Laura Barnes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fletcher_T/0/1/0/all/0/1\">Tom Fletcher</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Review of Speaker Diarization: Recent Advances with Deep Learning. (arXiv:2101.09624v3 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2101.09624","description":"<p>Speaker diarization is a task to label audio or video recordings with classes\nthat correspond to speaker identity, or in short, a task to identify \"who spoke\nwhen\". In the early years, speaker diarization algorithms were developed for\nspeech recognition on multispeaker audio recordings to enable speaker adaptive\nprocessing. These algorithms also gained their own value as a standalone\napplication over time to provide speaker-specific metainformation for\ndownstream tasks such as audio retrieval. More recently, with the emergence of\ndeep learning technology, which has driven revolutionary changes in research\nand practices across speech application domains, rapid advancements have been\nmade for speaker diarization. In this paper, we review not only the historical\ndevelopment of speaker diarization technology but also the recent advancements\nin neural speaker diarization approaches. Furthermore, we discuss how speaker\ndiarization systems have been integrated with speech recognition applications\nand how the recent surge of deep learning is leading the way of jointly\nmodeling these two components to be complementary to each other. By considering\nsuch exciting technical trends, we believe that this paper is a valuable\ncontribution to the community to provide a survey work by consolidating the\nrecent developments with neural methods and thus facilitating further progress\ntoward a more efficient speaker diarization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Park_T/0/1/0/all/0/1\">Tae Jin Park</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kanda_N/0/1/0/all/0/1\">Naoyuki Kanda</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dimitriadis_D/0/1/0/all/0/1\">Dimitrios Dimitriadis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Han_K/0/1/0/all/0/1\">Kyu J. Han</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Narayanan_S/0/1/0/all/0/1\">Shrikanth Narayanan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bias Out-of-the-Box: An Empirical Analysis of Intersectional Occupational Biases in Popular Generative Language Models. (arXiv:2102.04130v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2102.04130","description":"<p>The capabilities of natural language models trained on large-scale data have\nincreased immensely over the past few years. Open source libraries such as\nHuggingFace have made these models easily available and accessible. While prior\nresearch has identified biases in large language models, this paper considers\nbiases contained in the most popular versions of these models when applied\n`out-of-the-box' for downstream tasks. We focus on generative language models\nas they are well-suited for extracting biases inherited from training data.\nSpecifically, we conduct an in-depth analysis of GPT-2, which is the most\ndownloaded text generation model on HuggingFace, with over half a million\ndownloads per month. We assess biases related to occupational associations for\ndifferent protected categories by intersecting gender with religion, sexuality,\nethnicity, political affiliation, and continental name origin. Using a\ntemplate-based data collection pipeline, we collect 396K sentence completions\nmade by GPT-2 and find: (i) The machine-predicted jobs are less diverse and\nmore stereotypical for women than for men, especially for intersections; (ii)\nIntersectional interactions are highly relevant for occupational associations,\nwhich we quantify by fitting 262 logistic models; (iii) For most occupations,\nGPT-2 reflects the skewed gender and ethnicity distribution found in US Labor\nBureau data, and even pulls the societally-skewed distribution towards gender\nparity in cases where its predictions deviate from real labor market\nobservations. This raises the normative question of what language models should\nlearn - whether they should reflect or correct for existing inequalities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kirk_H/0/1/0/all/0/1\">Hannah Kirk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jun_Y/0/1/0/all/0/1\">Yennie Jun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iqbal_H/0/1/0/all/0/1\">Haider Iqbal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benussi_E/0/1/0/all/0/1\">Elias Benussi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Volpin_F/0/1/0/all/0/1\">Filippo Volpin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dreyer_F/0/1/0/all/0/1\">Frederic A. Dreyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shtedritski_A/0/1/0/all/0/1\">Aleksandar Shtedritski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asano_Y/0/1/0/all/0/1\">Yuki M. Asano</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DOBF: A Deobfuscation Pre-Training Objective for Programming Languages. (arXiv:2102.07492v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2102.07492","description":"<p>Recent advances in self-supervised learning have dramatically improved the\nstate of the art on a wide variety of tasks. However, research in language\nmodel pre-training has mostly focused on natural languages, and it is unclear\nwhether models like BERT and its variants provide the best pre-training when\napplied to other modalities, such as source code. In this paper, we introduce a\nnew pre-training objective, DOBF, that leverages the structural aspect of\nprogramming languages and pre-trains a model to recover the original version of\nobfuscated source code. We show that models pre-trained with DOBF significantly\noutperform existing approaches on multiple downstream tasks, providing relative\nimprovements of up to 13% in unsupervised code translation, and 24% in natural\nlanguage code search. Incidentally, we found that our pre-trained model is able\nto de-obfuscate fully obfuscated source files, and to suggest descriptive\nvariable names.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Roziere_B/0/1/0/all/0/1\">Baptiste Roziere</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lachaux_M/0/1/0/all/0/1\">Marie-Anne Lachaux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szafraniec_M/0/1/0/all/0/1\">Marc Szafraniec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lample_G/0/1/0/all/0/1\">Guillaume Lample</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"#PraCegoVer: A Large Dataset for Image Captioning in Portuguese. (arXiv:2103.11474v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.11474","description":"<p>Automatically describing images using natural sentences is an important task\nto support visually impaired people's inclusion onto the Internet. It is still\na big challenge that requires understanding the relation of the objects present\nin the image and their attributes and actions they are involved in. Then,\nvisual interpretation methods are needed, but linguistic models are also\nnecessary to verbally describe the semantic relations. This problem is known as\nImage Captioning. Although many datasets were proposed in the literature, the\nmajority contains only English captions, whereas datasets with captions\ndescribed in other languages are scarce. Recently, a movement called PraCegoVer\narose on the Internet, stimulating users from social media to publish images,\ntag #PraCegoVer and add a short description of their content. Thus, inspired by\nthis movement, we have proposed the #PraCegoVer, a multi-modal dataset with\nPortuguese captions based on posts from Instagram. It is the first large\ndataset for image captioning in Portuguese with freely annotated images.\nFurther, the captions in our dataset bring additional challenges to the\nproblem: first, in contrast to popular datasets such as MS COCO Captions,\n#PraCegoVer has only one reference to each image; also, both mean and variance\nof our reference sentence length are significantly greater than those in the MS\nCOCO Captions. These two characteristics contribute to making our dataset\ninteresting due to the linguistic aspect and the challenges that it introduces\nto the image captioning problem. We publicly-share the dataset at\nhttps://github.com/gabrielsantosrv/PraCegoVer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Santos_G/0/1/0/all/0/1\">Gabriel Oliveira dos Santos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colombini_E/0/1/0/all/0/1\">Esther Luna Colombini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avila_S/0/1/0/all/0/1\">Sandra Avila</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GraphTMT: Unsupervised Graph-based Topic Modeling from Video Transcripts. (arXiv:2105.01466v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.01466","description":"<p>To unfold the tremendous amount of multimedia data uploaded daily to social\nmedia platforms, effective topic modeling techniques are needed. Existing work\ntends to apply topic models on written text datasets. In this paper, we propose\na topic extractor on video transcripts. Exploiting neural word embeddings\nthrough graph-based clustering, we aim to improve usability and semantic\ncoherence. Unlike most topic models, this approach works without knowing the\ntrue number of topics, which is important when no such assumption can or should\nbe made. Experimental results on the real-life multimodal dataset MuSe-CaR\ndemonstrates that our approach GraphTMT extracts coherent and meaningful topics\nand outperforms baseline methods. Furthermore, we successfully demonstrate the\napplicability of our approach on the popular Citysearch corpus.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stappen_L/0/1/0/all/0/1\">Lukas Stappen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thies_J/0/1/0/all/0/1\">Jason Thies</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hagerer_G/0/1/0/all/0/1\">Gerhard Hagerer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuller_B/0/1/0/all/0/1\">Bj&#xf6;rn W. Schuller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Groh_G/0/1/0/all/0/1\">Georg Groh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Out-of-Distribution Problem in Explainability and Search Methods for Feature Importance Explanations. (arXiv:2106.00786v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.00786","description":"<p>Feature importance (FI) estimates are a popular form of explanation, and they\nare commonly created and evaluated by computing the change in model confidence\ncaused by removing certain input features at test time. For example, in the\nstandard Sufficiency metric, only the top-k most important tokens are kept. In\nthis paper, we study several under-explored dimensions of FI explanations,\nproviding conceptual and empirical improvements for this form of explanation.\nFirst, we advance a new argument for why it can be problematic to remove\nfeatures from an input when creating or evaluating explanations: the fact that\nthese counterfactual inputs are out-of-distribution (OOD) to models implies\nthat the resulting explanations are socially misaligned. The crux of the\nproblem is that the model prior and random weight initialization influence the\nexplanations (and explanation metrics) in unintended ways. To resolve this\nissue, we propose a simple alteration to the model training process, which\nresults in more socially aligned explanations and metrics. Second, we compare\namong five approaches for removing features from model inputs. We find that\nsome methods produce more OOD counterfactuals than others, and we make\nrecommendations for selecting a feature-replacement function. Finally, we\nintroduce four search-based methods for identifying FI explanations and compare\nthem to strong baselines, including LIME, Anchors, and Integrated Gradients.\nThrough experiments with six diverse text classification datasets, we find that\nthe only method that consistently outperforms random search is a Parallel Local\nSearch (PLS) that we introduce. Improvements over the second-best method are as\nlarge as 5.4 points for Sufficiency and 17 points for Comprehensiveness. All\nsupporting code for experiments in this paper is publicly available at\nhttps://github.com/peterbhase/ExplanationSearch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hase_P/0/1/0/all/0/1\">Peter Hase</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1\">Harry Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is Automated Topic Model Evaluation Broken?: The Incoherence of Coherence. (arXiv:2107.02173v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.02173","description":"<p>Topic model evaluation, like evaluation of other unsupervised methods, can be\ncontentious. However, the field has coalesced around automated estimates of\ntopic coherence, which rely on the frequency of word co-occurrences in a\nreference corpus. Contemporary neural topic models surpass classical ones\naccording to these metrics. At the same time, topic model evaluation suffers\nfrom a validation gap: automated coherence, developed for classical models, has\nnot been validated using human experimentation for neural models. In addition,\na meta-analysis of topic modeling literature reveals a substantial\nstandardization gap in automated topic modeling benchmarks. To address the\nvalidation gap, we compare automated coherence with the two most widely\naccepted human judgment tasks: topic rating and word intrusion. To address the\nstandardization gap, we systematically evaluate a dominant classical model and\ntwo state-of-the-art neural models on two commonly used datasets. Automated\nevaluations declare a winning model when corresponding human evaluations do\nnot, calling into question the validity of fully automatic evaluations\nindependent of human judgments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hoyle_A/0/1/0/all/0/1\">Alexander Hoyle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goel_P/0/1/0/all/0/1\">Pranav Goel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peskov_D/0/1/0/all/0/1\">Denis Peskov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hian_Cheong_A/0/1/0/all/0/1\">Andrew Hian-Cheong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boyd_Graber_J/0/1/0/all/0/1\">Jordan Boyd-Graber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Resnik_P/0/1/0/all/0/1\">Philip Resnik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Combiner: Full Attention Transformer with Sparse Computation Cost. (arXiv:2107.05768v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2107.05768","description":"<p>Transformers provide a class of expressive architectures that are extremely\neffective for sequence modeling. However, the key limitation of transformers is\ntheir quadratic memory and time complexity $\\mathcal{O}(L^2)$ with respect to\nthe sequence length in attention layers, which restricts application in\nextremely long sequences. Most existing approaches leverage sparsity or\nlow-rank assumptions in the attention matrix to reduce cost, but sacrifice\nexpressiveness. Instead, we propose Combiner, which provides full attention\ncapability in each attention head while maintaining low computation and memory\ncomplexity. The key idea is to treat the self-attention mechanism as a\nconditional expectation over embeddings at each location, and approximate the\nconditional distribution with a structured factorization. Each location can\nattend to all other locations, either via direct attention, or through indirect\nattention to abstractions, which are again conditional expectations of\nembeddings from corresponding local regions. We show that most sparse attention\npatterns used in existing sparse transformers are able to inspire the design of\nsuch factorization for full attention, resulting in the same sub-quadratic cost\n($\\mathcal{O}(L\\log(L))$ or $\\mathcal{O}(L\\sqrt{L})$). Combiner is a drop-in\nreplacement for attention layers in existing transformers and can be easily\nimplemented in common frameworks. An experimental evaluation on both\nautoregressive and bidirectional sequence tasks demonstrates the effectiveness\nof this approach, yielding state-of-the-art results on several image and text\nmodeling tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1\">Hongyu Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1\">Hanjun Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Z/0/1/0/all/0/1\">Zihang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Mengjiao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leskovec_J/0/1/0/all/0/1\">Jure Leskovec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuurmans_D/0/1/0/all/0/1\">Dale Schuurmans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1\">Bo Dai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"One Question Answering Model for Many Languages with Cross-lingual Dense Passage Retrieval. (arXiv:2107.11976v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.11976","description":"<p>We present Cross-lingual Open-Retrieval Answer Generation (CORA), the first\nunified many-to-many question answering (QA) model that can answer questions\nacross many languages, even for ones without language-specific annotated data\nor knowledge sources. We introduce a new dense passage retrieval algorithm that\nis trained to retrieve documents across languages for a question. Combined with\na multilingual autoregressive generation model, CORA answers directly in the\ntarget language without any translation or in-language retrieval modules as\nused in prior work. We propose an iterative training method that automatically\nextends annotated data available only in high-resource languages to\nlow-resource ones. Our results show that CORA substantially outperforms the\nprevious state of the art on multilingual open QA benchmarks across 26\nlanguages, 9 of which are unseen during training. Our analyses show the\nsignificance of cross-lingual retrieval and generation in many languages,\nparticularly under low-resource settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Asai_A/0/1/0/all/0/1\">Akari Asai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xinyan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasai_J/0/1/0/all/0/1\">Jungo Kasai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating Disagreement in the Scientific Literature. (arXiv:2107.14641v2 [cs.DL] UPDATED)","link":"http://arxiv.org/abs/2107.14641","description":"<p>Disagreement is essential to scientific progress. However, the extent of\ndisagreement in science, its evolution over time, and the fields in which it\nhappens, remains poorly understood. Leveraging a massive collection of\nEnglish-language scientific texts, we develop a cue-phrase based approach to\nidentify instances of disagreement citations across more than four million\nscientific articles. Using this method, we construct an indicator of\ndisagreement across scientific fields over the 2000-2015 period. In contrast\nwith black-box text classification methods, our framework is transparent and\neasily interpretable. We reveal a disciplinary spectrum of disagreement, with\nhigher disagreement in the social sciences and lower disagreement in physics\nand mathematics. However, detailed disciplinary analysis demonstrates\nheterogeneity across sub-fields, revealing the importance of local disciplinary\ncultures and epistemic characteristics of disagreement. Paper-level analysis\nreveals notable episodes of disagreement in science, and illustrates how\nmethodological artifacts can confound analyses of scientific texts. These\nfindings contribute to a broader understanding of disagreement and establish a\nfoundation for future research to understanding key processes underlying\nscientific progress.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lamers_W/0/1/0/all/0/1\">Wout S. Lamers</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Boyack_K/0/1/0/all/0/1\">Kevin Boyack</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Lariviere_V/0/1/0/all/0/1\">Vincent Larivi&#xe8;re</a> (3), <a href=\"http://arxiv.org/find/cs/1/au:+Sugimoto_C/0/1/0/all/0/1\">Cassidy R. Sugimoto</a> (4), <a href=\"http://arxiv.org/find/cs/1/au:+Eck_N/0/1/0/all/0/1\">Nees Jan van Eck</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Waltman_L/0/1/0/all/0/1\">Ludo Waltman</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Murray_D/0/1/0/all/0/1\">Dakota Murray</a> (4) ((1) Centre for Science and Technology Studies, Leiden University, Leiden, Netherlands, (2) SciTech Strategies, Inc., Albuquerque, NM, USA, (3) &#xc9;cole de biblioth&#xe9;conomie et des sciences de l&#x27;information, Universit&#xe9; de Montr&#xe9;al, Canada, (4) School of Informatics, Computing, and Engineering, Indiana University Bloomington, IN, USA)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Interplay Between Sparsity, Naturalness, Intelligibility, and Prosody in Speech Synthesis. (arXiv:2110.01147v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2110.01147","description":"<p>Are end-to-end text-to-speech (TTS) models over-parametrized? To what extent\ncan these models be pruned, and what happens to their synthesis capabilities?\nThis work serves as a starting point to explore pruning both spectrogram\nprediction networks and vocoders. We thoroughly investigate the tradeoffs\nbetween sparsity and its subsequent effects on synthetic speech. Additionally,\nwe explored several aspects of TTS pruning: amount of finetuning data versus\nsparsity, TTS-Augmentation to utilize unspoken text, and combining knowledge\ndistillation and pruning. Our findings suggest that not only are end-to-end TTS\nmodels highly prunable, but also, perhaps surprisingly, pruned TTS models can\nproduce synthetic speech with equal or higher naturalness and intelligibility,\nwith similar prosody. All of our experiments are conducted on publicly\navailable models, and findings in this work are backed by large-scale\nsubjective tests and objective measures. Code and 200 pruned models are made\navailable to facilitate future research on efficiency in TTS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lai_C/0/1/0/all/0/1\">Cheng-I Jeff Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cooper_E/0/1/0/all/0/1\">Erica Cooper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shiyu Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_K/0/1/0/all/0/1\">Kaizhi Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1\">Yi-Lun Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chuang_Y/0/1/0/all/0/1\">Yung-Sung Chuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Alexander H. Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamagishi_J/0/1/0/all/0/1\">Junichi Yamagishi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cox_D/0/1/0/all/0/1\">David Cox</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glass_J/0/1/0/all/0/1\">James Glass</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Wav2vec 2.0 fine-tuning for improved speech emotion recognition. (arXiv:2110.06309v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2110.06309","description":"<p>While wav2vec 2.0 has been proposed for speech recognition (ASR), it can also\nbe used for speech emotion recognition (SER); its performance can be\nsignificantly improved using different fine-tuning strategies. Two baseline\nmethods, vanilla fine-tuning (V-FT) and task adaptive pretraining (TAPT) are\nfirst presented. We show that V-FT is able to outperform state-of-the-art\nmodels on the IEMOCAP dataset. TAPT, an existing NLP fine-tuning strategy,\nfurther improves the performance on SER. We also introduce a novel fine-tuning\nmethod termed P-TAPT, which modifies the TAPT objective to learn contextualized\nemotion representations. Experiments show that P-TAPT performs better than TAPT\nespecially under low-resource settings. Compared to prior works in this\nliterature, our top-line system achieved a 7.4% absolute improvement on\nunweighted accuracy (UA) over the state-of-the-art performance on IEMOCAP. Our\ncode is publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chen_L/0/1/0/all/0/1\">Li-Wei Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rudnicky_A/0/1/0/all/0/1\">Alexander Rudnicky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-10-28T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Improving Super-Resolution Performance using Meta-Attention Layers. (arXiv:2110.14638v1 [eess.IV])","link":"http://arxiv.org/abs/2110.14638","description":"<p>Convolutional Neural Networks (CNNs) have achieved impressive results across\nmany super-resolution (SR) and image restoration tasks. While many such\nnetworks can upscale low-resolution (LR) images using just the raw pixel-level\ninformation, the ill-posed nature of SR can make it difficult to accurately\nsuper-resolve an image which has undergone multiple different degradations.\nAdditional information (metadata) describing the degradation process (such as\nthe blur kernel applied, compression level, etc.) can guide networks to\nsuper-resolve LR images with higher fidelity to the original source. Previous\nattempts at informing SR networks with degradation parameters have indeed been\nable to improve performance in a number of scenarios. However, due to the\nfully-convolutional nature of many SR networks, most of these metadata fusion\nmethods either require a complete architectural change, or necessitate the\naddition of significant extra complexity. Thus, these approaches are difficult\nto introduce into arbitrary SR networks without considerable design\nalterations. In this paper, we introduce meta-attention, a simple mechanism\nwhich allows any SR CNN to exploit the information available in relevant\ndegradation parameters. The mechanism functions by translating the metadata\ninto a channel attention vector, which in turn selectively modulates the\nnetwork's feature maps. Incorporating meta-attention into SR networks is\nstraightforward, as it requires no specific type of architecture to function\ncorrectly. Extensive testing has shown that meta-attention can consistently\nimprove the pixel-level accuracy of state-of-the-art (SOTA) networks when\nprovided with relevant degradation metadata. For PSNR, the gain on\nblurred/downsampled (X4) images is of 0.2969 dB (on average) and 0.3320 dB for\nSOTA general and face SR models, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Aquilina_M/0/1/0/all/0/1\">Matthew Aquilina</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Galea_C/0/1/0/all/0/1\">Christian Galea</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Abela_J/0/1/0/all/0/1\">John Abela</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Camilleri_K/0/1/0/all/0/1\">Kenneth P. Camilleri</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Farrugia_R/0/1/0/all/0/1\">Reuben A. Farrugia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sensing Anomalies as Potential Hazards: Datasets and Benchmarks. (arXiv:2110.14706v1 [cs.RO])","link":"http://arxiv.org/abs/2110.14706","description":"<p>We consider the problem of detecting, in the visual sensing data stream of an\nautonomous mobile robot, semantic patterns that are unusual (i.e., anomalous)\nwith respect to the robot's previous experience in similar environments. These\nanomalies might indicate unforeseen hazards and, in scenarios where failure is\ncostly, can be used to trigger an avoidance behavior. We contribute three novel\nimage-based datasets acquired in robot exploration scenarios, comprising a\ntotal of more than 200k labeled frames, spanning various types of anomalies. On\nthese datasets, we study the performance of an anomaly detection approach based\non autoencoders operating at different scales.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mantegazza_D/0/1/0/all/0/1\">Dario Mantegazza</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Redondo_C/0/1/0/all/0/1\">Carlos Redondo</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Espada_F/0/1/0/all/0/1\">Fran Espada</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Gambardella_L/0/1/0/all/0/1\">Luca M. Gambardella</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Giusti_A/0/1/0/all/0/1\">Alessandro Giusti</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Guzzi_J/0/1/0/all/0/1\">J&#xe9;r&#xf4;me Guzzi</a> (1) ((1) Dalle Molle Institute for Artificial Intelligence (IDSIA), USI-SUPSI, Lugano, Switzerland,(2) Hovering Solutions Ltd, Madrid, Spain)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sharp-GAN: Sharpness Loss Regularized GAN for Histopathology Image Synthesis. (arXiv:2110.14709v1 [eess.IV])","link":"http://arxiv.org/abs/2110.14709","description":"<p>Existing deep learning-based approaches for histopathology image analysis\nrequire large annotated training sets to achieve good performance; but\nannotating histopathology images is slow and resource-intensive. Conditional\ngenerative adversarial networks have been applied to generate synthetic\nhistopathology images to alleviate this issue, but current approaches fail to\ngenerate clear contours for overlapped and touching nuclei. In this study, We\npropose a sharpness loss regularized generative adversarial network to\nsynthesize realistic histopathology images. The proposed network uses\nnormalized nucleus distance map rather than the binary mask to encode nuclei\ncontour information. The proposed sharpness loss enhances the contrast of\nnuclei contour pixels. The proposed method is evaluated using four image\nquality metrics and segmentation results on two public datasets. Both\nquantitative and qualitative results demonstrate that the proposed approach can\ngenerate realistic histopathology images with clear nuclei contours.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Butte_S/0/1/0/all/0/1\">Sujata Butte</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1\">Haotian Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xian_M/0/1/0/all/0/1\">Min Xian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vakanski_A/0/1/0/all/0/1\">Aleksandar Vakanski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Self-Supervised and Few-Shot Object Detection. (arXiv:2110.14711v1 [cs.CV])","link":"http://arxiv.org/abs/2110.14711","description":"<p>Labeling data is often expensive and time-consuming, especially for tasks\nsuch as object detection and instance segmentation, which require dense\nlabeling of the image. While few-shot object detection is about training a\nmodel on novel (unseen) object classes with little data, it still requires\nprior training on many labeled examples of base (seen) classes. On the other\nhand, self-supervised methods aim at learning representations from unlabeled\ndata which transfer well to downstream tasks such as object detection.\nCombining few-shot and self-supervised object detection is a promising research\ndirection. In this survey, we review and characterize the most recent\napproaches on few-shot and self-supervised object detection. Then, we give our\nmain takeaways and discuss future research directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Gabriel Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laradji_I/0/1/0/all/0/1\">Issam Laradji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vazquez_D/0/1/0/all/0/1\">David Vazquez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lacoste_Julien_S/0/1/0/all/0/1\">Simon Lacoste-Julien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_P/0/1/0/all/0/1\">Pau Rodriguez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lung Cancer Lesion Detection in Histopathology Images Using Graph-Based Sparse PCA Network. (arXiv:2110.14728v1 [eess.IV])","link":"http://arxiv.org/abs/2110.14728","description":"<p>Early detection of lung cancer is critical for improvement of patient\nsurvival. To address the clinical need for efficacious treatments, genetically\nengineered mouse models (GEMM) have become integral in identifying and\nevaluating the molecular underpinnings of this complex disease that may be\nexploited as therapeutic targets. Assessment of GEMM tumor burden on\nhistopathological sections performed by manual inspection is both time\nconsuming and prone to subjective bias. Therefore, an interplay of needs and\nchallenges exists for computer-aided diagnostic tools, for accurate and\nefficient analysis of these histopathology images. In this paper, we propose a\nsimple machine learning approach called the graph-based sparse principal\ncomponent analysis (GS-PCA) network, for automated detection of cancerous\nlesions on histological lung slides stained by hematoxylin and eosin (H&amp;E). Our\nmethod comprises four steps: 1) cascaded graph-based sparse PCA, 2) PCA binary\nhashing, 3) block-wise histograms, and 4) support vector machine (SVM)\nclassification. In our proposed architecture, graph-based sparse PCA is\nemployed to learn the filter banks of the multiple stages of a convolutional\nnetwork. This is followed by PCA hashing and block histograms for indexing and\npooling. The meaningful features extracted from this GS-PCA are then fed to an\nSVM classifier. We evaluate the performance of the proposed algorithm on H&amp;E\nslides obtained from an inducible K-rasG12D lung cancer mouse model using\nprecision/recall rates, F-score, Tanimoto coefficient, and area under the curve\n(AUC) of the receiver operator characteristic (ROC) and show that our algorithm\nis efficient and provides improved detection accuracy compared to existing\nalgorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ram_S/0/1/0/all/0/1\">Sundaresh Ram</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tang_W/0/1/0/all/0/1\">Wenfei Tang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bell_A/0/1/0/all/0/1\">Alexander J. Bell</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Spencer_C/0/1/0/all/0/1\">Cara Spencer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Buschhaus_A/0/1/0/all/0/1\">Alexander Buschhaus</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hatt_C/0/1/0/all/0/1\">Charles R. Hatt</a>, <a href=\"http://arxiv.org/find/eess/1/au:+diMagliano_M/0/1/0/all/0/1\">Marina Pasca diMagliano</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rodriguez_J/0/1/0/all/0/1\">Jeffrey J. Rodriguez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Galban_S/0/1/0/all/0/1\">Stefanie Galban</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Galban_C/0/1/0/all/0/1\">Craig J. Galban</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision Transformer for Classification of Breast Ultrasound Images. (arXiv:2110.14731v1 [cs.CV])","link":"http://arxiv.org/abs/2110.14731","description":"<p>Medical ultrasound (US) imaging has become a prominent modality for breast\ncancer imaging due to its ease-of-use, low-cost and safety. In the past decade,\nconvolutional neural networks (CNNs) have emerged as the method of choice in\nvision applications and have shown excellent potential in automatic\nclassification of US images. Despite their success, their restricted local\nreceptive field limits their ability to learn global context information.\nRecently, Vision Transformer (ViT) designs that are based on self-attention\nbetween image patches have shown great potential to be an alternative to CNNs.\nIn this study, for the first time, we utilize ViT to classify breast US images\nusing different augmentation strategies. The results are provided as\nclassification accuracy and Area Under the Curve (AUC) metrics, and the\nperformance is compared with the state-of-the-art CNNs. The results indicate\nthat the ViT models have comparable efficiency with or even better than the\nCNNs in classification of US breast images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gheflati_B/0/1/0/all/0/1\">Behnaz Gheflati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rivaz_H/0/1/0/all/0/1\">Hassan Rivaz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Algorithmic encoding of protected characteristics and its implications on disparities across subgroups. (arXiv:2110.14755v1 [cs.LG])","link":"http://arxiv.org/abs/2110.14755","description":"<p>It has been rightfully emphasized that the use of AI for clinical decision\nmaking could amplify health disparities. A machine learning model may pick up\nundesirable correlations, for example, between a patient's racial identity and\nclinical outcome. Such correlations are often present in (historical) data used\nfor model development. There has been an increase in studies reporting biases\nin disease detection models across patient subgroups. Besides the scarcity of\ndata from underserved populations, very little is known about how these biases\nare encoded and how one may reduce or even remove disparate performance. There\nis some speculation whether algorithms may recognize patient characteristics\nsuch as biological sex or racial identity, and then directly or indirectly use\nthis information when making predictions. But it remains unclear how we can\nestablish whether such information is actually used. This article aims to shed\nsome light on these issues by exploring new methodology allowing intuitive\ninspections of the inner working of machine learning models for image-based\ndetection of disease. We also evaluate an effective yet debatable technique for\naddressing disparities leveraging the automatic prediction of patient\ncharacteristics, resulting in models with comparable true and false positive\nrates across subgroups. Our findings may stimulate the discussion about safe\nand ethical use of AI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Glocker_B/0/1/0/all/0/1\">Ben Glocker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winzeck_S/0/1/0/all/0/1\">Stefan Winzeck</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Regularized Frank-Wolfe for Dense CRFs: Generalizing Mean Field and Beyond. (arXiv:2110.14759v1 [cs.LG])","link":"http://arxiv.org/abs/2110.14759","description":"<p>We introduce regularized Frank-Wolfe, a general and effective algorithm for\ninference and learning of dense conditional random fields (CRFs). The algorithm\noptimizes a nonconvex continuous relaxation of the CRF inference problem using\nvanilla Frank-Wolfe with approximate updates, which are equivalent to\nminimizing a regularized energy function. Our proposed method is a\ngeneralization of existing algorithms such as mean field or concave-convex\nprocedure. This perspective not only offers a unified analysis of these\nalgorithms, but also allows an easy way of exploring different variants that\npotentially yield better performance. We illustrate this in our empirical\nresults on standard semantic segmentation datasets, where several\ninstantiations of our regularized Frank-Wolfe outperform mean field inference,\nboth as a standalone component and as an end-to-end trainable layer in a neural\nnetwork. We also show that dense CRFs, coupled with our new algorithms, produce\nsignificant improvements over strong CNN baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Le_Huu_%7B/0/1/0/all/0/1\">&#x110;.Khu&#xea; L&#xea;-Huu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alahari_K/0/1/0/all/0/1\">Karteek Alahari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Dementia from Speech and Transcripts using Transformers. (arXiv:2110.14769v1 [cs.CL])","link":"http://arxiv.org/abs/2110.14769","description":"<p>Alzheimer's disease (AD) constitutes a neurodegenerative disease with serious\nconsequences to peoples' everyday lives, if it is not diagnosed early since\nthere is no available cure. Because of the cost of examinations for diagnosing\ndementia, i.e., Magnetic Resonance Imaging (MRI), electroencephalogram (EEG)\nsignals etc., current work has been focused on diagnosing dementia from\nspontaneous speech. However, little work has been done regarding the conversion\nof speech data to Log-Mel spectrograms and Mel-frequency cepstral coefficients\n(MFCCs) and the usage of pretrained models. Concurrently, little work has been\ndone in terms of both the usage of transformer networks and the way the two\nmodalities, i.e., speech and transcripts, are combined in a single neural\nnetwork. To address these limitations, first we employ several pretrained\nmodels, with Vision Transformer (ViT) achieving the highest evaluation results.\nSecondly, we propose multimodal models. More specifically, our introduced\nmodels include Gated Multimodal Unit in order to control the influence of each\nmodality towards the final classification and crossmodal attention so as to\ncapture in an effective way the relationships between the two modalities.\nExtensive experiments conducted on the ADReSS Challenge dataset demonstrate the\neffectiveness of the proposed models and their superiority over\nstate-of-the-art approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ilias_L/0/1/0/all/0/1\">Loukas Ilias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Askounis_D/0/1/0/all/0/1\">Dimitris Askounis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Psarras_J/0/1/0/all/0/1\">John Psarras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SiamPolar: Semi-supervised Realtime Video Object Segmentation with Polar Representation. (arXiv:2110.14773v1 [cs.CV])","link":"http://arxiv.org/abs/2110.14773","description":"<p>Video object segmentation (VOS) is an essential part of autonomous vehicle\nnavigation. The real-time speed is very important for the autonomous vehicle\nalgorithms along with the accuracy metric. In this paper, we propose a\nsemi-supervised real-time method based on the Siamese network using a new polar\nrepresentation. The input of bounding boxes is initialized rather than the\nobject masks, which are applied to the video object detection tasks. The polar\nrepresentation could reduce the parameters for encoding masks with subtle\naccuracy loss so that the algorithm speed can be improved significantly. An\nasymmetric siamese network is also developed to extract the features from\ndifferent spatial scales. Moreover, the peeling convolution is proposed to\nreduce the antagonism among the branches of the polar head. The repeated\ncross-correlation and semi-FPN are designed based on this idea. The\nexperimental results on the DAVIS-2016 dataset and other public datasets\ndemonstrate the effectiveness of the proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yaochen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1\">Yuhui Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yonghong Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ying Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ruihao Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BI-GCN: Boundary-Aware Input-Dependent Graph Convolution Network for Biomedical Image Segmentation. (arXiv:2110.14775v1 [cs.CV])","link":"http://arxiv.org/abs/2110.14775","description":"<p>Segmentation is an essential operation of image processing. The convolution\noperation suffers from a limited receptive field, while global modelling is\nfundamental to segmentation tasks. In this paper, we apply graph convolution\ninto the segmentation task and propose an improved \\textit{Laplacian}.\nDifferent from existing methods, our \\textit{Laplacian} is data-dependent, and\nwe introduce two attention diagonal matrices to learn a better vertex\nrelationship. In addition, it takes advantage of both region and boundary\ninformation when performing graph-based information propagation. Specifically,\nwe model and reason about the boundary-aware region-wise correlations of\ndifferent classes through learning graph representations, which is capable of\nmanipulating long range semantic reasoning across various regions with the\nspatial enhancement along the object's boundary. Our model is well-suited to\nobtain global semantic region information while also accommodates local spatial\nboundary characteristics simultaneously. Experiments on two types of\nchallenging datasets demonstrate that our method outperforms the\nstate-of-the-art approaches on the segmentation of polyps in colonoscopy images\nand of the optic disc and optic cup in colour fundus images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1\">Yanda Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongrun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_D/0/1/0/all/0/1\">Dongxu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yitian Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaoyun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_X/0/1/0/all/0/1\">Xuesheng Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaowei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yalin Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SCALP -- Supervised Contrastive Learning for Cardiopulmonary Disease Classification and Localization in Chest X-rays using Patient Metadata. (arXiv:2110.14787v1 [eess.IV])","link":"http://arxiv.org/abs/2110.14787","description":"<p>Computer-aided diagnosis plays a salient role in more accessible and accurate\ncardiopulmonary diseases classification and localization on chest radiography.\nMillions of people get affected and die due to these diseases without an\naccurate and timely diagnosis. Recently proposed contrastive learning heavily\nrelies on data augmentation, especially positive data augmentation. However,\ngenerating clinically-accurate data augmentations for medical images is\nextremely difficult because the common data augmentation methods in computer\nvision, such as sharp, blur, and crop operations, can severely alter the\nclinical settings of medical images. In this paper, we proposed a novel and\nsimple data augmentation method based on patient metadata and supervised\nknowledge to create clinically accurate positive and negative augmentations for\nchest X-rays. We introduce an end-to-end framework, SCALP, which extends the\nself-supervised contrastive approach to a supervised setting. Specifically,\nSCALP pulls together chest X-rays from the same patient (positive keys) and\npushes apart chest X-rays from different patients (negative keys). In addition,\nit uses ResNet-50 along with the triplet-attention mechanism to identify\ncardiopulmonary diseases, and Grad-CAM++ to highlight the abnormal regions. Our\nextensive experiments demonstrate that SCALP outperforms existing baselines\nwith significant margins in both classification and localization tasks.\nSpecifically, the average classification AUCs improve from 82.8% (SOTA using\nDenseNet-121) to 83.9% (SCALP using ResNet-50), while the localization results\nimprove on average by 3.7% over different IoU thresholds.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Jaiswal_A/0/1/0/all/0/1\">Ajay Jaiswal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_T/0/1/0/all/0/1\">Tianhao Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zander_C/0/1/0/all/0/1\">Cyprian Zander</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Han_Y/0/1/0/all/0/1\">Yan Han</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rousseau_J/0/1/0/all/0/1\">Justin F. Rousseau</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Peng_Y/0/1/0/all/0/1\">Yifan Peng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ding_Y/0/1/0/all/0/1\">Ying Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MedMNIST v2: A Large-Scale Lightweight Benchmark for 2D and 3D Biomedical Image Classification. (arXiv:2110.14795v1 [cs.CV])","link":"http://arxiv.org/abs/2110.14795","description":"<p>We introduce MedMNIST v2, a large-scale MNIST-like dataset collection of\nstandardized biomedical images, including 12 datasets for 2D and 6 datasets for\n3D. All images are pre-processed into a small size of 28x28 (2D) or 28x28x28\n(3D) with the corresponding classification labels so that no background\nknowledge is required for users. Covering primary data modalities in biomedical\nimages, MedMNIST v2 is designed to perform classification on lightweight 2D and\n3D images with various dataset scales (from 100 to 100,000) and diverse tasks\n(binary/multi-class, ordinal regression, and multi-label). The resulting\ndataset, consisting of 708,069 2D images and 10,214 3D images in total, could\nsupport numerous research / educational purposes in biomedical image analysis,\ncomputer vision, and machine learning. We benchmark several baseline methods on\nMedMNIST v2, including 2D / 3D neural networks and open-source / commercial\nAutoML tools. The data and code are publicly available at\nhttps://medmnist.com/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jiancheng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_R/0/1/0/all/0/1\">Rui Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_D/0/1/0/all/0/1\">Donglai Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zequan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Lin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ke_B/0/1/0/all/0/1\">Bilian Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfister_H/0/1/0/all/0/1\">Hanspeter Pfister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_B/0/1/0/all/0/1\">Bingbing Ni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Intermediate Layers Matter in Momentum Contrastive Self Supervised Learning. (arXiv:2110.14805v1 [cs.CV])","link":"http://arxiv.org/abs/2110.14805","description":"<p>We show that bringing intermediate layers' representations of two augmented\nversions of an image closer together in self-supervised learning helps to\nimprove the momentum contrastive (MoCo) method. To this end, in addition to the\ncontrastive loss, we minimize the mean squared error between the intermediate\nlayer representations or make their cross-correlation matrix closer to an\nidentity matrix. Both loss objectives either outperform standard MoCo, or\nachieve similar performances on three diverse medical imaging datasets:\nNIH-Chest Xrays, Breast Cancer Histopathology, and Diabetic Retinopathy. The\ngains of the improved MoCo are especially large in a low-labeled data regime\n(e.g. 1% labeled data) with an average gain of 5% across three datasets. We\nanalyze the models trained using our novel approach via feature similarity\nanalysis and layer-wise probing. Our analysis reveals that models trained via\nour approach have higher feature reuse compared to a standard MoCo and learn\ninformative features earlier in the network. Finally, by comparing the output\nprobability distribution of models fine-tuned on small versus large labeled\ndata, we conclude that our proposed method of pre-training leads to lower\nKolmogorov-Smirnov distance, as compared to a standard MoCo. This provides\nadditional evidence that our proposed method learns more informative features\nin the pre-training phase which could be leveraged in a low-labeled data\nregime.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kaku_A/0/1/0/all/0/1\">Aakash Kaku</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Upadhya_S/0/1/0/all/0/1\">Sahana Upadhya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Razavian_N/0/1/0/all/0/1\">Narges Razavian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Characterizing and Taming Resolution in Convolutional Neural Networks. (arXiv:2110.14819v1 [cs.CV])","link":"http://arxiv.org/abs/2110.14819","description":"<p>Image resolution has a significant effect on the accuracy and computational,\nstorage, and bandwidth costs of computer vision model inference. These costs\nare exacerbated when scaling out models to large inference serving systems and\nmake image resolution an attractive target for optimization. However, the\nchoice of resolution inherently introduces additional tightly coupled choices,\nsuch as image crop size, image detail, and compute kernel implementation that\nimpact computational, storage, and bandwidth costs. Further complicating this\nsetting, the optimal choices from the perspective of these metrics are highly\ndependent on the dataset and problem scenario. We characterize this tradeoff\nspace, quantitatively studying the accuracy and efficiency tradeoff via\nsystematic and automated tuning of image resolution, image quality and\nconvolutional neural network operators. With the insights from this study, we\npropose a dynamic resolution mechanism that removes the need to statically\nchoose a resolution ahead of time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_E/0/1/0/all/0/1\">Eddie Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_L/0/1/0/all/0/1\">Liang Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ceze_L/0/1/0/all/0/1\">Luis Ceze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ODMTCNet: An Interpretable Multi-view Deep Neural Network Architecture for Image Feature Representation. (arXiv:2110.14830v1 [cs.CV])","link":"http://arxiv.org/abs/2110.14830","description":"<p>This work proposes an interpretable multi-view deep neural network\narchitecture, namely optimal discriminant multi-view tensor convolutional\nnetwork (ODMTCNet), by integrating statistical machine learning (SML)\nprinciples with the deep neural network (DNN) architecture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Lei Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zheng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_L/0/1/0/all/0/1\">Ling Guan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Audio-visual Representation Learning for Anomaly Events Detection in Crowds. (arXiv:2110.14862v1 [cs.CV])","link":"http://arxiv.org/abs/2110.14862","description":"<p>In recent years, anomaly events detection in crowd scenes attracts many\nresearchers' attention, because of its importance to public safety. Existing\nmethods usually exploit visual information to analyze whether any abnormal\nevents have occurred due to only visual sensors are generally equipped in\npublic places. However, when an abnormal event in crowds occurs, sound\ninformation may be discriminative to assist the crowd analysis system to\ndetermine whether there is an abnormality. Compare with vision information that\nis easily occluded, audio signals have a certain degree of penetration. Thus,\nthis paper attempt to exploit multi-modal learning for modeling the audio and\nvisual signals simultaneously. To be specific, we design a two-branch network\nto model different types of information. The first is a typical 3D CNN model to\nextract temporal appearance features from video clips. The second is an audio\nCNN for encoding Log Mel-Spectrogram of audio signals. Finally, by fusing the\nabove features, a more accurate prediction will be produced. We conduct the\nexperiments on SHADE dataset, a synthetic audio-visual dataset in surveillance\nscenes, and find introducing audio signals effectively improves the performance\nof anomaly events detection and outperforms other state-of-the-art methods.\nFurthermore, we will release the code and the pre-trained models as soon as\npossible.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Junyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1\">Maoguo Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuelong Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalized Depthwise-Separable Convolutions for Adversarially Robust and Efficient Neural Networks. (arXiv:2110.14871v1 [cs.LG])","link":"http://arxiv.org/abs/2110.14871","description":"<p>Despite their tremendous successes, convolutional neural networks (CNNs)\nincur high computational/storage costs and are vulnerable to adversarial\nperturbations. Recent works on robust model compression address these\nchallenges by combining model compression techniques with adversarial training.\nBut these methods are unable to improve throughput (frames-per-second) on\nreal-life hardware while simultaneously preserving robustness to adversarial\nperturbations. To overcome this problem, we propose the method of Generalized\nDepthwise-Separable (GDWS) convolution -- an efficient, universal,\npost-training approximation of a standard 2D convolution. GDWS dramatically\nimproves the throughput of a standard pre-trained network on real-life hardware\nwhile preserving its robustness. Lastly, GDWS is scalable to large problem\nsizes since it operates on pre-trained models and doesn't require any\nadditional training. We establish the optimality of GDWS as a 2D convolution\napproximator and present exact algorithms for constructing optimal GDWS\nconvolutions under complexity and error constraints. We demonstrate the\neffectiveness of GDWS via extensive experiments on CIFAR-10, SVHN, and ImageNet\ndatasets. Our code can be found at https://github.com/hsndbk4/GDWS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dbouk_H/0/1/0/all/0/1\">Hassan Dbouk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shanbhag_N/0/1/0/all/0/1\">Naresh R. Shanbhag</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Colossal-AI: A Unified Deep Learning System For Large-Scale Parallel Training. (arXiv:2110.14883v1 [cs.LG])","link":"http://arxiv.org/abs/2110.14883","description":"<p>The Transformer architecture has improved the performance of deep learning\nmodels in domains such as Computer Vision and Natural Language Processing.\nTogether with better performance come larger model sizes. This imposes\nchallenges to the memory wall of the current accelerator hardware such as GPU.\nIt is never ideal to train large models such as Vision Transformer, BERT, and\nGPT on a single GPU or a single machine. There is an urgent demand to train\nmodels in a distributed environment. However, distributed training, especially\nmodel parallelism, often requires domain expertise in computer systems and\narchitecture. It remains a challenge for AI researchers to implement complex\ndistributed training solutions for their models.\n</p>\n<p>In this paper, we introduce Colossal-AI, which is a unified parallel training\nsystem designed to seamlessly integrate different paradigms of parallelization\ntechniques including data parallelism, pipeline parallelism, multiple tensor\nparallelism, and sequence parallelism. Colossal-AI aims to support the AI\ncommunity to write distributed models in the same way as how they write models\nnormally. This allows them to focus on developing the model architecture and\nseparates the concerns of distributed training from the development process.\nThe documentations can be found at https://www.colossalai.org and the source\ncode can be found at https://github.com/hpcaitech/ColossalAI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bian_Z/0/1/0/all/0/1\">Zhengda Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hongxin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Boxiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Haichen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yongbin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chuanrui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_F/0/1/0/all/0/1\">Fan Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1\">Yang You</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Degraded Reference Image Quality Assessment. (arXiv:2110.14899v1 [eess.IV])","link":"http://arxiv.org/abs/2110.14899","description":"<p>In practical media distribution systems, visual content usually undergoes\nmultiple stages of quality degradation along the delivery chain, but the\npristine source content is rarely available at most quality monitoring points\nalong the chain to serve as a reference for quality assessment. As a result,\nfull-reference (FR) and reduced-reference (RR) image quality assessment (IQA)\nmethods are generally infeasible. Although no-reference (NR) methods are\nreadily applicable, their performance is often not reliable. On the other hand,\nintermediate references of degraded quality are often available, e.g., at the\ninput of video transcoders, but how to make the best use of them in proper ways\nhas not been deeply investigated. Here we make one of the first attempts to\nestablish a new paradigm named degraded-reference IQA (DR IQA). Specifically,\nwe lay out the architectures of DR IQA and introduce a 6-bit code to denote the\nchoices of configurations. We construct the first large-scale databases\ndedicated to DR IQA and will make them publicly available. We make novel\nobservations on distortion behavior in multi-stage distortion pipelines by\ncomprehensively analyzing five multiple distortion combinations. Based on these\nobservations, we develop novel DR IQA models and make extensive comparisons\nwith a series of baseline models derived from top-performing FR and NR models.\nThe results suggest that DR IQA may offer significant performance improvement\nin multiple distortion environments, thereby establishing DR IQA as a valid IQA\nparadigm that is worth further exploration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Athar_S/0/1/0/all/0/1\">Shahrukh Athar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1\">Zhou Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches. (arXiv:2110.14908v1 [cs.HC])","link":"http://arxiv.org/abs/2110.14908","description":"<p>What makes speeches effective has long been a subject for debate, and until\ntoday there is broad controversy among public speaking experts about what\nfactors make a speech effective as well as the roles of these factors in\nspeeches. Moreover, there is a lack of quantitative analysis methods to help\nunderstand effective speaking strategies. In this paper, we propose E-ffective,\na visual analytic system allowing speaking experts and novices to analyze both\nthe role of speech factors and their contribution in effective speeches. From\ninterviews with domain experts and investigating existing literature, we\nidentified important factors to consider in inspirational speeches. We obtained\nthe generated factors from multi-modal data that were then related to\neffectiveness data. Our system supports rapid understanding of critical factors\nin inspirational speeches, including the influence of emotions by means of\nnovel visualization methods and interaction. Two novel visualizations include\nE-spiral (that shows the emotional shifts in speeches in a visually compact\nway) and E-script (that connects speech content with key speech delivery\ninformation). In our evaluation we studied the influence of our system on\nexperts' domain knowledge about speech factors. We further studied the\nusability of the system by speaking novices and experts on assisting analysis\nof inspirational speech effectiveness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maher_K/0/1/0/all/0/1\">Kevin Maher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zeyuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jiancheng Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_X/0/1/0/all/0/1\">Xiaoming Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_Y/0/1/0/all/0/1\">Yu-Kun Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Cuixia Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong-Jin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D Object Tracking with Transformer. (arXiv:2110.14921v1 [cs.CV])","link":"http://arxiv.org/abs/2110.14921","description":"<p>Feature fusion and similarity computation are two core problems in 3D object\ntracking, especially for object tracking using sparse and disordered point\nclouds. Feature fusion could make similarity computing more efficient by\nincluding target object information. However, most existing LiDAR-based\napproaches directly use the extracted point cloud feature to compute similarity\nwhile ignoring the attention changes of object regions during tracking. In this\npaper, we propose a feature fusion network based on transformer architecture.\nBenefiting from the self-attention mechanism, the transformer encoder captures\nthe inter- and intra- relations among different regions of the point cloud. By\nusing cross-attention, the transformer decoder fuses features and includes more\ntarget cues into the current point cloud feature to compute the region\nattentions, which makes the similarity computing more efficient. Based on this\nfeature fusion network, we propose an end-to-end point cloud object tracking\nframework, a simple yet effective method for 3D object tracking using point\nclouds. Comprehensive experimental results on the KITTI dataset show that our\nmethod achieves new state-of-the-art performance. Code is available at:\nhttps://github.com/3bobo/lttr.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Yubo Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1\">Zheng Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_J/0/1/0/all/0/1\">Jiayao Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Z/0/1/0/all/0/1\">Zuoxu Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Sifan Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Counterfactual Explanation of Brain Activity Classifiers using Image-to-Image Transfer by Generative Adversarial Network. (arXiv:2110.14927v1 [q-bio.NC])","link":"http://arxiv.org/abs/2110.14927","description":"<p>Deep neural networks (DNNs) can accurately decode task-related information\nfrom brain activations. However, because of the nonlinearity of the DNN, the\ndecisions made by DNNs are hardly interpretable. One of the promising\napproaches for explaining such a black-box system is counterfactual\nexplanation. In this framework, the behavior of a black-box system is explained\nby comparing real data and realistic synthetic data that are specifically\ngenerated such that the black-box system outputs an unreal outcome. Here we\nintroduce a novel generative DNN (counterfactual activation generator, CAG)\nthat can provide counterfactual explanations for DNN-based classifiers of brain\nactivations. Importantly, CAG can simultaneously handle image transformation\namong multiple classes associated with different behavioral tasks. Using CAG,\nwe demonstrated counterfactual explanation of DNN-based classifiers that\nlearned to discriminate brain activations of seven behavioral tasks.\nFurthermore, by iterative applications of CAG, we were able to enhance and\nextract subtle spatial brain activity patterns that affected the classifier's\ndecisions. Together, these results demonstrate that the counterfactual\nexplanation based on image-to-image transformation would be a promising\napproach to understand and extend the current application of DNNs in fMRI\nanalyses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Matsui_T/0/1/0/all/0/1\">Teppei Matsui</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Taki_M/0/1/0/all/0/1\">Masato Taki</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Pham_T/0/1/0/all/0/1\">Trung Quang Pham</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Chikazoe_J/0/1/0/all/0/1\">Junichi Chikazoe</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Jimura_K/0/1/0/all/0/1\">Koji Jimura</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A recursive robust filtering approach for 3D registration. (arXiv:2110.14932v1 [cs.CV])","link":"http://arxiv.org/abs/2110.14932","description":"<p>This work presents a new recursive robust filtering approach for\nfeature-based 3D registration. Unlike the common state-of-the-art alignment\nalgorithms, the proposed method has four advantages that have not yet occurred\naltogether in any previous solution. For instance, it is able to deal with\ninherent noise contaminating sensory data; it is robust to uncertainties caused\nby noisy feature localisation; it also combines the advantages of both (Formula\npresented.) and (Formula presented.) norms for a higher performance and a more\nprospective prevention of local minima. The result is an accurate and stable\nrigid body transformation. The latter enables a thorough control over the\nconvergence regarding the alignment as well as a correct assessment of the\nquality of registration. The mathematical rationale behind the proposed\napproach is explained, and the results are validated on physical and synthetic\ndata.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Amamra_A/0/1/0/all/0/1\">Abdenour Amamra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aouf_N/0/1/0/all/0/1\">Nabil Aouf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stuart_D/0/1/0/all/0/1\">Dowling Stuart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Richardson_M/0/1/0/all/0/1\">Mark Richardson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GPU based GMM segmentation of kinect data. (arXiv:2110.14934v1 [cs.CV])","link":"http://arxiv.org/abs/2110.14934","description":"<p>This paper presents a novel approach for background/foreground segmentation\nof RGBD data with the Gaussian Mixture Models (GMM). We first start by the\nbackground subtraction from the colour and depth images separately. The\nforegrounds resulting from both streams are then fused for a more accurate\ndetection. Our segmentation solution is implemented on the GPU. Thus, it works\nat the full frame rate of the sensor (30fps). Test results show its robustness\nagainst illumination change, shadows and reflections.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Amamra_A/0/1/0/all/0/1\">Abdenour Amamra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mouats_T/0/1/0/all/0/1\">Tarek Mouats</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aouf_N/0/1/0/all/0/1\">Nabil Aouf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FocusFace: Multi-task Contrastive Learning for Masked Face Recognition. (arXiv:2110.14940v1 [cs.CV])","link":"http://arxiv.org/abs/2110.14940","description":"<p>SARS-CoV-2 has presented direct and indirect challenges to the scientific\ncommunity. One of the most prominent indirect challenges advents from the\nmandatory use of face masks in a large number of countries. Face recognition\nmethods struggle to perform identity verification with similar accuracy on\nmasked and unmasked individuals. It has been shown that the performance of\nthese methods drops considerably in the presence of face masks, especially if\nthe reference image is unmasked. We propose FocusFace, a multi-task\narchitecture that uses contrastive learning to be able to accurately perform\nmasked face recognition. The proposed architecture is designed to be trained\nfrom scratch or to work on top of state-of-the-art face recognition methods\nwithout sacrificing the capabilities of a existing models in conventional face\nrecognition tasks. We also explore different approaches to design the\ncontrastive learning module. Results are presented in terms of masked-masked\n(M-M) and unmasked-masked (U-M) face verification performance. For both\nsettings, the results are on par with published methods, but for M-M\nspecifically, the proposed method was able to outperform all the solutions that\nit was compared to. We further show that when using our method on top of\nalready existing methods the training computational costs decrease\nsignificantly while retaining similar performances. The implementation and the\ntrained models are available at GitHub.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Neto_P/0/1/0/all/0/1\">Pedro C. Neto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boutros_F/0/1/0/all/0/1\">Fadi Boutros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pinto_J/0/1/0/all/0/1\">Jo&#xe3;o Ribeiro Pinto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Damer_N/0/1/0/all/0/1\">Naser Damer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sequeira_A/0/1/0/all/0/1\">Ana F. Sequeira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cardoso_J/0/1/0/all/0/1\">Jaime S. Cardoso</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dispensed Transformer Network for Unsupervised Domain Adaptation. (arXiv:2110.14944v1 [cs.CV])","link":"http://arxiv.org/abs/2110.14944","description":"<p>Accurate segmentation is a crucial step in medical image analysis and\napplying supervised machine learning to segment the organs or lesions has been\nsubstantiated effective. However, it is costly to perform data annotation that\nprovides ground truth labels for training the supervised algorithms, and the\nhigh variance of data that comes from different domains tends to severely\ndegrade system performance over cross-site or cross-modality datasets. To\nmitigate this problem, a novel unsupervised domain adaptation (UDA) method\nnamed dispensed Transformer network (DTNet) is introduced in this paper. Our\nnovel DTNet contains three modules. First, a dispensed residual transformer\nblock is designed, which realizes global attention by dispensed interleaving\noperation and deals with the excessive computational cost and GPU memory usage\nof the Transformer. Second, a multi-scale consistency regularization is\nproposed to alleviate the loss of details in the low-resolution output for\nbetter feature alignment. Finally, a feature ranking discriminator is\nintroduced to automatically assign different weights to domain-gap features to\nlessen the feature distribution distance, reducing the performance shift of two\ndomains. The proposed method is evaluated on large fluorescein angiography (FA)\nretinal nonperfusion (RNP) cross-site dataset with 676 images and a wide used\ncross-modality dataset from the MM-WHS challenge. Extensive results demonstrate\nthat our proposed network achieves the best performance in comparison with\nseveral state-of-the-art techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunxiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jingxiong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dan_R/0/1/0/all/0/1\">Ruilong Dan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_K/0/1/0/all/0/1\">Kai Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_G/0/1/0/all/0/1\">Guodong Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1\">Xiangji Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qianni Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Huiyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1\">Qun Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Li Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaqi Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Large-Scale Rendering of Simulated Crops for Synthetic Ground Truth Generation on Modular Supercomputers. (arXiv:2110.14946v1 [cs.CV])","link":"http://arxiv.org/abs/2110.14946","description":"<p>Computer Vision problems deal with the semantic extraction of information\nfrom camera images. Especially for field crop images, the underlying problems\nare hard to label and even harder to learn, and the availability of\nhigh-quality training data is low. Deep neural networks do a good job of\nextracting the necessary models from training examples. However, they rely on\nan abundance of training data that is not feasible to generate or label by\nexpert annotation. To address this challenge, we make use of the Unreal Engine\nto render large and complex virtual scenes. We rely on the performance of\nindividual nodes by distributing plant simulations across nodes and both\ngenerate scenes as well as train neural networks on GPUs, restricting node\ncommunication to parallel learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Helmrich_D/0/1/0/all/0/1\">Dirk Norbert Helmrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gobbert_J/0/1/0/all/0/1\">Jens Henrik G&#xf6;bbert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giraud_M/0/1/0/all/0/1\">Mona Giraud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scharr_H/0/1/0/all/0/1\">Hanno Scharr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schnepf_A/0/1/0/all/0/1\">Andrea Schnepf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedel_M/0/1/0/all/0/1\">Morris Riedel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DocScanner: Robust Document Image Rectification with Progressive Learning. (arXiv:2110.14968v1 [cs.CV])","link":"http://arxiv.org/abs/2110.14968","description":"<p>Compared to flatbed scanners, portable smartphones are much more convenient\nfor physical documents digitizing. However, such digitized documents are often\ndistorted due to uncontrolled physical deformations, camera positions, and\nillumination variations. To this end, this work presents DocScanner, a new deep\nnetwork architecture for document image rectification. Different from existing\nmethods, DocScanner addresses this issue by introducing a progressive learning\nmechanism. Specifically, DocScanner maintains a single estimate of the\nrectified image, which is progressively corrected with a recurrent\narchitecture. The iterative refinements make DocScanner converge to a robust\nand superior performance, and the lightweight recurrent architecture ensures\nthe running efficiency. In addition, before the above rectification process,\nobserving the corrupted rectified boundaries existing in prior works,\nDocScanner exploits a document localization module to explicitly segment the\nforeground document from the cluttered background environments. To further\nimprove the rectification quality, based on the geometric priori between the\ndistorted and the rectified images, a geometric regularization is introduced\nduring training to further facilitate the performance. Extensive experiments\nare conducted on the Doc3D dataset and the DocUNet benchmark dataset, and the\nquantitative and qualitative evaluation results verify the effectiveness of\nDocScanner, which outperforms previous methods on OCR accuracy, image\nsimilarity, and our proposed distortion metric by a considerable margin.\nFurthermore, our DocScanner shows the highest efficiency in inference time and\nparameter count.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_H/0/1/0/all/0/1\">Hao Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wengang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jiajun Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Houqiang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Skeleton-Based Mutually Assisted Interacted Object Localization and Human Action Recognition. (arXiv:2110.14994v1 [cs.CV])","link":"http://arxiv.org/abs/2110.14994","description":"<p>Skeleton data carries valuable motion information and is widely explored in\nhuman action recognition. However, not only the motion information but also the\ninteraction with the environment provides discriminative cues to recognize the\naction of persons. In this paper, we propose a joint learning framework for\nmutually assisted \"interacted object localization\" and \"human action\nrecognition\" based on skeleton data. The two tasks are serialized together and\ncollaborate to promote each other, where preliminary action type derived from\nskeleton alone helps improve interacted object localization, which in turn\nprovides valuable cues for the final human action recognition. Besides, we\nexplore the temporal consistency of interacted object as constraint to better\nlocalize the interacted object with the absence of ground-truth labels.\nExtensive experiments on the datasets of SYSU-3D, NTU60 RGB+D and\nNorthwestern-UCLA show that our method achieves the best or competitive\nperformance with the state-of-the-art methods for human action recognition.\nVisualization results show that our method can also provide reasonable\ninteracted object localization results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Liang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_C/0/1/0/all/0/1\">Cuiling Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1\">Wenjun Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Cewu Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sliding Sequential CVAE with Time Variant Socially-aware Rethinking for Trajectory Prediction. (arXiv:2110.15016v1 [cs.CV])","link":"http://arxiv.org/abs/2110.15016","description":"<p>Pedestrian trajectory prediction is a key technology in many applications\nsuch as video surveillance, social robot navigation, and autonomous driving,\nand significant progress has been made in this research topic. However, there\nremain two limitations of previous studies. First, with the continuation of\ntime, the prediction error at each time step increases significantly, causing\nthe final displacement error to be impossible to ignore. Second, the prediction\nresults of multiple pedestrians might be impractical in the prediction horizon,\ni.e., the predicted trajectories might collide with each other. To overcome\nthese limitations, this work proposes a novel trajectory prediction method\ncalled CSR, which consists of a cascaded conditional variational autoencoder\n(CVAE) module and a socially-aware regression module. The cascaded CVAE module\nfirst estimates the future trajectories in a sequential pattern. Specifically,\neach CVAE concatenates the past trajectories and the predicted points so far as\nthe input and predicts the location at the following time step. Then, the\nsocially-aware regression module generates offsets from the estimated future\ntrajectories to produce the socially compliant final predictions, which are\nmore reasonable and accurate results than the estimated trajectories. Moreover,\nconsidering the large model parameters of the cascaded CVAE module, a slide\nCVAE module is further exploited to improve the model efficiency using one\nshared CVAE, in a slidable manner. Experiments results demonstrate that the\nproposed method exhibits improvements over state-of-the-art method on the\nStanford Drone Dataset (SDD) and ETH/UCY of approximately 38.0% and 22.2%,\nrespectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_D/0/1/0/all/0/1\">Dongchun Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_M/0/1/0/all/0/1\">Mingyu Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hai Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bridging Non Co-occurrence with Unlabeled In-the-wild Data for Incremental Object Detection. (arXiv:2110.15017v1 [cs.CV])","link":"http://arxiv.org/abs/2110.15017","description":"<p>Deep networks have shown remarkable results in the task of object detection.\nHowever, their performance suffers critical drops when they are subsequently\ntrained on novel classes without any sample from the base classes originally\nused to train the model. This phenomenon is known as catastrophic forgetting.\nRecently, several incremental learning methods are proposed to mitigate\ncatastrophic forgetting for object detection. Despite the effectiveness, these\nmethods require co-occurrence of the unlabeled base classes in the training\ndata of the novel classes. This requirement is impractical in many real-world\nsettings since the base classes do not necessarily co-occur with the novel\nclasses. In view of this limitation, we consider a more practical setting of\ncomplete absence of co-occurrence of the base and novel classes for the object\ndetection task. We propose the use of unlabeled in-the-wild data to bridge the\nnon co-occurrence caused by the missing base classes during the training of\nadditional novel classes. To this end, we introduce a blind sampling strategy\nbased on the responses of the base-class model and pre-trained novel-class\nmodel to select a smaller relevant dataset from the large in-the-wild dataset\nfor incremental learning. We then design a dual-teacher distillation framework\nto transfer the knowledge distilled from the base- and novel-class teacher\nmodels to the student model using the sampled in-the-wild data. Experimental\nresults on the PASCAL VOC and MS COCO datasets show that our proposed method\nsignificantly outperforms other state-of-the-art class-incremental object\ndetection methods when there is no co-occurrence between the base and novel\nclasses during training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_N/0/1/0/all/0/1\">Na Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongqiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1\">Mingli Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Gim Hee Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deformable Registration of Brain MR Images via a Hybrid Loss. (arXiv:2110.15027v1 [cs.CV])","link":"http://arxiv.org/abs/2110.15027","description":"<p>We learn a deformable registration model for T1-weighted MR images by\nconsidering multiple image characteristics via a hybrid loss. Our method\nregisters the OASIS dataset with high accuracy while preserving deformation\nsmoothness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1\">Luyi Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_H/0/1/0/all/0/1\">Haoran Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yunzhi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yap_P/0/1/0/all/0/1\">Pew-Thian Yap</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Facial Emotion Recognition: A multi-task approach using deep learning. (arXiv:2110.15028v1 [cs.CV])","link":"http://arxiv.org/abs/2110.15028","description":"<p>Facial Emotion Recognition is an inherently difficult problem, due to vast\ndifferences in facial structures of individuals and ambiguity in the emotion\ndisplayed by a person. Recently, a lot of work is being done in the field of\nFacial Emotion Recognition, and the performance of the CNNs for this task has\nbeen inferior compared to the results achieved by CNNs in other fields like\nObject detection, Facial recognition etc. In this paper, we propose a\nmulti-task learning algorithm, in which a single CNN detects gender, age and\nrace of the subject along with their emotion. We validate this proposed\nmethodology using two datasets containing real-world images. The results show\nthat this approach is significantly better than the current State of the art\nalgorithms for this task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saroop_A/0/1/0/all/0/1\">Aakash Saroop</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghugare_P/0/1/0/all/0/1\">Pathik Ghugare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathamsetty_S/0/1/0/all/0/1\">Sashank Mathamsetty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasani_V/0/1/0/all/0/1\">Vaibhav Vasani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explicitly Modeling the Discriminability for Instance-Aware Visual Object Tracking. (arXiv:2110.15030v1 [cs.CV])","link":"http://arxiv.org/abs/2110.15030","description":"<p>Visual object tracking performance has been dramatically improved in recent\nyears, but some severe challenges remain open, like distractors and occlusions.\nWe suspect the reason is that the feature representations of the tracking\ntargets are only expressively learned but not fully discriminatively modeled.\nIn this paper, we propose a novel Instance-Aware Tracker (IAT) to explicitly\nexcavate the discriminability of feature representations, which improves the\nclassical visual tracking pipeline with an instance-level classifier. First, we\nintroduce a contrastive learning mechanism to formulate the classification\ntask, ensuring that every training sample could be uniquely modeled and be\nhighly distinguishable from plenty of other samples. Besides, we design an\neffective negative sample selection scheme to contain various intra and inter\nclasses in the instance classification branch. Furthermore, we implement two\nvariants of the proposed IAT, including a video-level one and an object-level\none. They realize the concept of \\textbf{instance} in different granularity as\nvideos and target bounding boxes, respectively. The former enhances the ability\nto recognize the target from the background while the latter boosts the\ndiscriminative power for mitigating the target-distractor dilemma. Extensive\nexperimental evaluations on 8 benchmark datasets show that both two versions of\nthe proposed IAT achieve leading results against state-of-the-art methods while\nrunning at 30FPS. Code will be available when it is published.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mengmeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaoqian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Deep Representation with Energy-Based Self-Expressiveness for Subspace Clustering. (arXiv:2110.15037v1 [cs.LG])","link":"http://arxiv.org/abs/2110.15037","description":"<p>Deep subspace clustering has attracted increasing attention in recent years.\nAlmost all the existing works are required to load the whole training data into\none batch for learning the self-expressive coefficients in the framework of\ndeep learning. Although these methods achieve promising results, such a\nlearning fashion severely prevents from the usage of deeper neural network\narchitectures (e.g., ResNet), leading to the limited representation abilities\nof the models. In this paper, we propose a new deep subspace clustering\nframework, motivated by the energy-based models. In contrast to previous\napproaches taking the weights of a fully connected layer as the self-expressive\ncoefficients, we propose to learn an energy-based network to obtain the\nself-expressive coefficients by mini-batch training. By this means, it is no\nlonger necessary to load all data into one batch for learning, and it thus\nbecomes a reality that we can utilize deeper neural network models for subspace\nclustering. Considering the powerful representation ability of the recently\npopular self-supervised learning, we attempt to leverage self-supervised\nrepresentation learning to learn the dictionary. Finally, we propose a joint\nframework to learn both the self-expressive coefficients and dictionary\nsimultaneously, and train the model in an end-to-end manner. The experiments\nare performed on three publicly available datasets, and extensive experimental\nresults demonstrate our method can significantly outperform the other related\napproaches. For instance, on the three datasets, our method can averagely\nachieve $13.8\\%$, $15.4\\%$, $20.8\\%$ improvements in terms of Accuracy, NMI,\nand ARI over SENet which is proposed very recently and obtains the second best\nresults in the experiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Changsheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shiye Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Ye Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guoren Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LF-YOLO: A Lighter and Faster YOLO for Weld Defect Detection of X-ray Image. (arXiv:2110.15045v1 [cs.CV])","link":"http://arxiv.org/abs/2110.15045","description":"<p>X-ray image plays an important role in manufacturing for quality assurance,\nbecause it can reflect the internal condition of weld region. However, the\nshape and scale of different defect types vary greatly, which makes it\nchallenging for model to detect weld defects. In this paper, we propose a weld\ndefect detection method based on convolution neural network (CNN), namely\nLighter and Faster YOLO (LF-YOLO). In particularly, an enhanced multiscale\nfeature (EMF) module is designed to implement both parameter-based and\nparameter-free multi-scale information extracting operation. EMF enables the\nextracted feature map capable to represent more plentiful information, which is\nachieved by superior hierarchical fusion structure. To improve the performance\nof detection network, we propose an efficient feature extraction (EFE) module.\nEFE processes input data with extremely low consumption, and improve the\npracticability of whole network in actual industry. Experimental results show\nthat our weld defect network achieves satisfactory balance between performance\nand consumption, and reaches 92.9 mAP50 with 61.5 FPS. To further prove the\nability of our method, we test it on public dataset MS COCO, and the results\nshow that our LF-YOLO has a outstanding versatility detection performance. The\ncode is available at https://github.com/lmomoy/LF-YOLO.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Moyun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Youping Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Lei He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jingming Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Robustness in Multi-Task Learning: Promises and Illusions. (arXiv:2110.15053v1 [cs.LG])","link":"http://arxiv.org/abs/2110.15053","description":"<p>Vulnerability to adversarial attacks is a well-known weakness of Deep Neural\nnetworks. While most of the studies focus on single-task neural networks with\ncomputer vision datasets, very little research has considered complex\nmulti-task models that are common in real applications. In this paper, we\nevaluate the design choices that impact the robustness of multi-task deep\nlearning networks. We provide evidence that blindly adding auxiliary tasks, or\nweighing the tasks provides a false sense of robustness. Thereby, we tone down\nthe claim made by previous research and study the different factors which may\naffect robustness. In particular, we show that the choice of the task to\nincorporate in the loss function are important factors that can be leveraged to\nyield more robust models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghamizi_S/0/1/0/all/0/1\">Salah Ghamizi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cordy_M/0/1/0/all/0/1\">Maxime Cordy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papadakis_M/0/1/0/all/0/1\">Mike Papadakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Traon_Y/0/1/0/all/0/1\">Yves Le Traon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Meta Guided Metric Learner for Overcoming Class Confusion in Few-Shot Road Object Detection. (arXiv:2110.15074v1 [cs.CV])","link":"http://arxiv.org/abs/2110.15074","description":"<p>Localization and recognition of less-occurring road objects have been a\nchallenge in autonomous driving applications due to the scarcity of data\nsamples. Few-Shot Object Detection techniques extend the knowledge from\nexisting base object classes to learn novel road objects given few training\nexamples. Popular techniques in FSOD adopt either meta or metric learning\ntechniques which are prone to class confusion and base class forgetting. In\nthis work, we introduce a novel Meta Guided Metric Learner (MGML) to overcome\nclass confusion in FSOD. We re-weight the features of the novel classes higher\nthan the base classes through a novel Squeeze and Excite module and encourage\nthe learning of truly discriminative class-specific features by applying an\nOrthogonality Constraint to the meta learner. Our method outperforms\nState-of-the-Art (SoTA) approaches in FSOD on the India Driving Dataset (IDD)\nby upto 11 mAP points while suffering from the least class confusion of 20%\ngiven only 10 examples of each novel road object. We further show similar\nimprovements on the few-shot splits of PASCAL VOC dataset where we outperform\nSoTA approaches by upto 5.8 mAP accross all splits.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Majee_A/0/1/0/all/0/1\">Anay Majee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subramanian_A/0/1/0/all/0/1\">Anbumani Subramanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_K/0/1/0/all/0/1\">Kshitij Agrawal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SpineOne: A One-Stage Detection Framework for Degenerative Discs and Vertebrae. (arXiv:2110.15082v1 [cs.CV])","link":"http://arxiv.org/abs/2110.15082","description":"<p>Spinal degeneration plagues many elders, office workers, and even the younger\ngenerations. Effective pharmic or surgical interventions can help relieve\ndegenerative spine conditions. However, the traditional diagnosis procedure is\noften too laborious. Clinical experts need to detect discs and vertebrae from\nspinal magnetic resonance imaging (MRI) or computed tomography (CT) images as a\npreliminary step to perform pathological diagnosis or preoperative evaluation.\nMachine learning systems have been developed to aid this procedure generally\nfollowing a two-stage methodology: first perform anatomical localization, then\npathological classification. Towards more efficient and accurate diagnosis, we\npropose a one-stage detection framework termed SpineOne to simultaneously\nlocalize and classify degenerative discs and vertebrae from MRI slices.\nSpineOne is built upon the following three key techniques: 1) a new design of\nthe keypoint heatmap to facilitate simultaneous keypoint localization and\nclassification; 2) the use of attention modules to better differentiate the\nrepresentations between discs and vertebrae; and 3) a novel gradient-guided\nobjective association mechanism to associate multiple learning objectives at\nthe later training stage. Empirical results on the Spinal Disease Intelligent\nDiagnosis Tianchi Competition (SDID-TC) dataset of 550 exams demonstrate that\nour approach surpasses existing methods by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jiabo He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xingjun Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_X/0/1/0/all/0/1\">Xian-Sheng Hua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mosaicking to Distill: Knowledge Distillation from Out-of-Domain Data. (arXiv:2110.15094v1 [cs.LG])","link":"http://arxiv.org/abs/2110.15094","description":"<p>Knowledge distillation~(KD) aims to craft a compact student model that\nimitates the behavior of a pre-trained teacher in a target domain. Prior KD\napproaches, despite their gratifying results, have largely relied on the\npremise that \\emph{in-domain} data is available to carry out the knowledge\ntransfer. Such an assumption, unfortunately, in many cases violates the\npractical setting, since the original training data or even the data domain is\noften unreachable due to privacy or copyright reasons. In this paper, we\nattempt to tackle an ambitious task, termed as \\emph{out-of-domain} knowledge\ndistillation~(OOD-KD), which allows us to conduct KD using only OOD data that\ncan be readily obtained at a very low cost. Admittedly, OOD-KD is by nature a\nhighly challenging task due to the agnostic domain gap. To this end, we\nintroduce a handy yet surprisingly efficacious approach, dubbed\nas~\\textit{MosaicKD}. The key insight behind MosaicKD lies in that, samples\nfrom various domains share common local patterns, even though their global\nsemantic may vary significantly; these shared local patterns, in turn, can be\nre-assembled analogous to mosaic tiling, to approximate the in-domain data and\nto further alleviating the domain discrepancy. In MosaicKD, this is achieved\nthrough a four-player min-max game, in which a generator, a discriminator, a\nstudent network, are collectively trained in an adversarial manner, partially\nunder the guidance of a pre-trained teacher. We validate MosaicKD over\n{classification and semantic segmentation tasks} across various benchmarks, and\ndemonstrate that it yields results much superior to the state-of-the-art\ncounterparts on OOD data. Our code is available at\n\\url{https://github.com/zju-vipa/MosaicKD}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fang_G/0/1/0/all/0/1\">Gongfan Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_Y/0/1/0/all/0/1\">Yifan Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jie Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinchao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_D/0/1/0/all/0/1\">Donglin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chengchao Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1\">Mingli Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrast and Mix: Temporal Contrastive Video Domain Adaptation with Background Mixing. (arXiv:2110.15128v1 [cs.CV])","link":"http://arxiv.org/abs/2110.15128","description":"<p>Unsupervised domain adaptation which aims to adapt models trained on a\nlabeled source domain to a completely unlabeled target domain has attracted\nmuch attention in recent years. While many domain adaptation techniques have\nbeen proposed for images, the problem of unsupervised domain adaptation in\nvideos remains largely underexplored. In this paper, we introduce Contrast and\nMix (CoMix), a new contrastive learning framework that aims to learn\ndiscriminative invariant feature representations for unsupervised video domain\nadaptation. First, unlike existing methods that rely on adversarial learning\nfor feature alignment, we utilize temporal contrastive learning to bridge the\ndomain gap by maximizing the similarity between encoded representations of an\nunlabeled video at two different speeds as well as minimizing the similarity\nbetween different videos played at different speeds. Second, we propose a novel\nextension to the temporal contrastive loss by using background mixing that\nallows additional positives per anchor, thus adapting contrastive learning to\nleverage action semantics shared across both domains. Moreover, we also\nintegrate a supervised contrastive learning objective using target\npseudo-labels to enhance discriminability of the latent space for video domain\nadaptation. Extensive experiments on several benchmark datasets demonstrate the\nsuperiority of our proposed approach over state-of-the-art methods. Project\npage: https://cvir.github.io/projects/comix\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sahoo_A/0/1/0/all/0/1\">Aadarsh Sahoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1\">Rutav Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panda_R/0/1/0/all/0/1\">Rameswar Panda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1\">Kate Saenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1\">Abir Das</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Blending Anti-Aliasing into Vision Transformer. (arXiv:2110.15156v1 [cs.CV])","link":"http://arxiv.org/abs/2110.15156","description":"<p>The transformer architectures, based on self-attention mechanism and\nconvolution-free design, recently found superior performance and booming\napplications in computer vision. However, the discontinuous patch-wise\ntokenization process implicitly introduces jagged artifacts into attention\nmaps, arising the traditional problem of aliasing for vision transformers.\nAliasing effect occurs when discrete patterns are used to produce high\nfrequency or continuous information, resulting in the indistinguishable\ndistortions. Recent researches have found that modern convolution networks\nstill suffer from this phenomenon. In this work, we analyze the uncharted\nproblem of aliasing in vision transformer and explore to incorporate\nanti-aliasing properties. Specifically, we propose a plug-and-play\nAliasing-Reduction Module(ARM) to alleviate the aforementioned issue. We\ninvestigate the effectiveness and generalization of the proposed method across\nmultiple tasks and various vision transformer families. This lightweight design\nconsistently attains a clear boost over several famous structures. Furthermore,\nour module also improves data efficiency and robustness of vision transformers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qian_S/0/1/0/all/0/1\">Shengju Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_H/0/1/0/all/0/1\">Hao Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\">Jiaya Jia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Authentication Attacks on Projection-based Cancelable Biometric Schemes. (arXiv:2110.15163v1 [cs.CR])","link":"http://arxiv.org/abs/2110.15163","description":"<p>Cancelable biometric schemes aim at generating secure biometric templates by\ncombining user specific tokens, such as password, stored secret or salt, along\nwith biometric data. This type of transformation is constructed as a\ncomposition of a biometric transformation with a feature extraction algorithm.\nThe security requirements of cancelable biometric schemes concern the\nirreversibility, unlinkability and revocability of templates, without losing in\naccuracy of comparison. While several schemes were recently attacked regarding\nthese requirements, full reversibility of such a composition in order to\nproduce colliding biometric characteristics, and specifically presentation\nattacks, were never demonstrated to the best of our knowledge. In this paper,\nwe formalize these attacks for a traditional cancelable scheme with the help of\ninteger linear programming (ILP) and quadratically constrained quadratic\nprogramming (QCQP). Solving these optimization problems allows an adversary to\nslightly alter its fingerprint image in order to impersonate any individual.\nMoreover, in an even more severe scenario, it is possible to simultaneously\nimpersonate several individuals.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Durbet_A/0/1/0/all/0/1\">Axel Durbet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lafourcade_P/0/1/0/all/0/1\">Pascal Lafourcade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Migdal_D/0/1/0/all/0/1\">Denis Migdal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thiry_Atighehchi_K/0/1/0/all/0/1\">Kevin Thiry-Atighehchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grollemund_P/0/1/0/all/0/1\">Paul-Marie Grollemund</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Privacy Aware Person Detection in Surveillance Data. (arXiv:2110.15171v1 [cs.CV])","link":"http://arxiv.org/abs/2110.15171","description":"<p>Crowd management relies on inspection of surveillance video either by\noperators or by object detection models. These models are large, making it\ndifficult to deploy them on resource constrained edge hardware. Instead, the\ncomputations are often offloaded to a (third party) cloud platform. While crowd\nmanagement may be a legitimate application, transferring video from the camera\nto remote infrastructure may open the door for extracting additional\ninformation that are infringements of privacy, like person tracking or face\nrecognition. In this paper, we use adversarial training to obtain a lightweight\nobfuscator that transforms video frames to only retain the necessary\ninformation for person detection. Importantly, the obfuscated data can be\nprocessed by publicly available object detectors without retraining and without\nsignificant loss of accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Coninck_S/0/1/0/all/0/1\">Sander De Coninck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leroux_S/0/1/0/all/0/1\">Sam Leroux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simoens_P/0/1/0/all/0/1\">Pieter Simoens</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comparative Study of Coarse to Dense 3D Indoor Scene Registration Algorithms. (arXiv:2110.15179v1 [cs.CV])","link":"http://arxiv.org/abs/2110.15179","description":"<p>3D alignment has become a very important part of 3D scanning technology. For\ninstance, we can divide the alignment process into four steps: key point\ndetection, key point description, initial pose estimation, and alignment\nrefinement. Researchers have contributed several approaches to the literature\nfor each step, which suggests a natural need for a comparative study for an\neducated more appropriate choice. In this work, we propose a description and an\nevaluation of the different methods used for 3D registration with special focus\non RGB-D data to find the best combinations that permit a complete and more\naccurate 3D reconstruction of indoor scenes with cheap depth cameras.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Amamra_A/0/1/0/all/0/1\">Abdenour Amamra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boumaza_K/0/1/0/all/0/1\">Khalid Boumaza</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The magnitude vector of images. (arXiv:2110.15188v1 [cs.LG])","link":"http://arxiv.org/abs/2110.15188","description":"<p>The magnitude of a finite metric space is a recently-introduced invariant\nquantity. Despite beneficial theoretical and practical properties, such as a\ngeneral utility for outlier detection, and a close connection to Laplace radial\nbasis kernels, magnitude has received little attention by the machine learning\ncommunity so far. In this work, we investigate the properties of magnitude on\nindividual images, with each image forming its own metric space. We show that\nthe known properties of outlier detection translate to edge detection in images\nand we give supporting theoretical justifications. In addition, we provide a\nproof of concept of its utility by using a novel magnitude layer to defend\nagainst adversarial attacks. Since naive magnitude calculations may be\ncomputationally prohibitive, we introduce an algorithm that leverages the\nregular structure of images to dramatically reduce the computational cost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Adamer_M/0/1/0/all/0/1\">Michael F. Adamer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OBray_L/0/1/0/all/0/1\">Leslie O&#x27;Bray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brouwer_E/0/1/0/all/0/1\">Edward De Brouwer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rieck_B/0/1/0/all/0/1\">Bastian Rieck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borgwardt_K/0/1/0/all/0/1\">Karsten Borgwardt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Covariate and Concept Shift for Detection and Calibration of Out-of-Distribution Data. (arXiv:2110.15231v1 [cs.LG])","link":"http://arxiv.org/abs/2110.15231","description":"<p>Moving beyond testing on in-distribution data works on Out-of-Distribution\n(OOD) detection have recently increased in popularity. A recent attempt to\ncategorize OOD data introduces the concept of near and far OOD detection.\nSpecifically, prior works define characteristics of OOD data in terms of\ndetection difficulty. We propose to characterize the spectrum of OOD data using\ntwo types of distribution shifts: covariate shift and concept shift, where\ncovariate shift corresponds to change in style, e.g., noise, and concept shift\nindicates a change in semantics. This characterization reveals that sensitivity\nto each type of shift is important to the detection and confidence calibration\nof OOD data. Consequently, we investigate score functions that capture\nsensitivity to each type of dataset shift and methods that improve them. To\nthis end, we theoretically derive two score functions for OOD detection, the\ncovariate shift score and concept shift score, based on the decomposition of\nKL-divergence for both scores, and propose a geometrically-inspired method\n(Geometric ODIN) to improve OOD detection under both shifts with only\nin-distribution data. Additionally, the proposed method naturally leads to an\nexpressive post-hoc calibration function which yields state-of-the-art\ncalibration performance on both in-distribution and out-of-distribution data.\nWe are the first to propose a method that works well across both OOD detection\nand calibration and under different types of shifts. Specifically, we improve\nthe previous state-of-the-art OOD detection by relatively 7% AUROC on CIFAR100\nvs. SVHN and achieve the best calibration performance of 0.084 Expected\nCalibration Error on the corrupted CIFAR100C dataset. View project page at\nhttps://sites.google.com/view/geometric-decomposition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1\">Junjiao Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_Y/0/1/0/all/0/1\">Yen-Change Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yilin Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1\">Hongxia Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kira_Z/0/1/0/all/0/1\">Zsolt Kira</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Guided Evolution for Neural Architecture Search. (arXiv:2110.15232v1 [cs.LG])","link":"http://arxiv.org/abs/2110.15232","description":"<p>Neural Architecture Search (NAS) methods have been successfully applied to\nimage tasks with excellent results. However, NAS methods are often complex and\ntend to converge to local minima as soon as generated architectures seem to\nyield good results. In this paper, we propose G-EA, a novel approach for guided\nevolutionary NAS. The rationale behind G-EA, is to explore the search space by\ngenerating and evaluating several architectures in each generation at\ninitialization stage using a zero-proxy estimator, where only the\nhighest-scoring network is trained and kept for the next generation. This\nevaluation at initialization stage allows continuous extraction of knowledge\nfrom the search space without increasing computation, thus allowing the search\nto be efficiently guided. Moreover, G-EA forces exploitation of the most\nperformant networks by descendant generation while at the same time forcing\nexploration by parent mutation and by favouring younger architectures to the\ndetriment of older ones. Experimental results demonstrate the effectiveness of\nthe proposed method, showing that G-EA achieves state-of-the-art results in\nNAS-Bench-201 search space in CIFAR-10, CIFAR-100 and ImageNet16-120, with mean\naccuracies of 93.98%, 72.12% and 45.94% respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lopes_V/0/1/0/all/0/1\">Vasco Lopes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santos_M/0/1/0/all/0/1\">Miguel Santos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Degardin_B/0/1/0/all/0/1\">Bruno Degardin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alexandre_L/0/1/0/all/0/1\">Lu&#xed;s A. Alexandre</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Subpixel object segmentation using wavelets and multi resolution analysis. (arXiv:2110.15233v1 [cs.CV])","link":"http://arxiv.org/abs/2110.15233","description":"<p>We propose a novel deep learning framework for fast prediction of boundaries\nof two-dimensional simply connected domains using wavelets and Multi Resolution\nAnalysis (MRA). The boundaries are modelled as (piecewise) smooth closed curves\nusing wavelets and the so-called Pyramid Algorithm. Our network architecture is\na hybrid analog of the U-Net, where the down-sampling path is a two-dimensional\nencoder with learnable filters, and the upsampling path is a one-dimensional\ndecoder, which builds curves up from low to high resolution levels. Any wavelet\nbasis induced by a MRA can be used. This flexibility allows for incorporation\nof priors on the smoothness of curves. The effectiveness of the proposed method\nis demonstrated by delineating boundaries of simply connected domains (organs)\nin medical images using Debauches wavelets and comparing performance with a\nU-Net baseline. Our model demonstrates up to 5x faster inference speed compared\nto the U-Net, while maintaining similar performance in terms of Dice score and\nHausdorff distance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sheombarsing_R/0/1/0/all/0/1\">Ray Sheombarsing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moriakov_N/0/1/0/all/0/1\">Nikita Moriakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sonke_J/0/1/0/all/0/1\">Jan-Jakob Sonke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teuwen_J/0/1/0/all/0/1\">Jonas Teuwen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-end Learning the Partial Permutation Matrix for Robust 3D Point Cloud Registration. (arXiv:2110.15250v1 [cs.CV])","link":"http://arxiv.org/abs/2110.15250","description":"<p>Even though considerable progress has been made in deep learning-based 3D\npoint cloud processing, how to obtain accurate correspondences for robust\nregistration remains a major challenge because existing hard assignment methods\ncannot deal with outliers naturally. Alternatively, the soft matching-based\nmethods have been proposed to learn the matching probability rather than hard\nassignment. However, in this paper, we prove that these methods have an\ninherent ambiguity causing many deceptive correspondences. To address the above\nchallenges, we propose to learn a partial permutation matching matrix, which\ndoes not assign corresponding points to outliers, and implements hard\nassignment to prevent ambiguity. However, this proposal poses two new problems,\ni.e., existing hard assignment algorithms can only solve a full rank\npermutation matrix rather than a partial permutation matrix, and this desired\nmatrix is defined in the discrete space, which is non-differentiable. In\nresponse, we design a dedicated soft-to-hard (S2H) matching procedure within\nthe registration pipeline consisting of two steps: solving the soft matching\nmatrix (S-step) and projecting this soft matrix to the partial permutation\nmatrix (H-step). Specifically, we augment the profit matrix before the hard\nassignment to solve an augmented permutation matrix, which is cropped to\nachieve the final partial permutation matrix. Moreover, to guarantee end-to-end\nlearning, we supervise the learned partial permutation matrix but propagate the\ngradient to the soft matrix instead. Our S2H matching procedure can be easily\nintegrated with existing registration frameworks, which has been verified in\nrepresentative frameworks including DCP, RPMNet, and DGR. Extensive experiments\nhave validated our method, which creates a new state-of-the-art performance for\nrobust 3D point cloud registration. The code will be made public.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhiyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jiadai Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1\">Yuchao Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Dingfu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xibin Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_M/0/1/0/all/0/1\">Mingyi He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Learning Disentangled Group Representation as Feature. (arXiv:2110.15255v1 [cs.CV])","link":"http://arxiv.org/abs/2110.15255","description":"<p>A good visual representation is an inference map from observations (images)\nto features (vectors) that faithfully reflects the hidden modularized\ngenerative factors (semantics). In this paper, we formulate the notion of\n\"good\" representation from a group-theoretic view using Higgins' definition of\ndisentangled representation, and show that existing Self-Supervised Learning\n(SSL) only disentangles simple augmentation features such as rotation and\ncolorization, thus unable to modularize the remaining semantics. To break the\nlimitation, we propose an iterative SSL algorithm: Iterative Partition-based\nInvariant Risk Minimization (IP-IRM), which successfully grounds the abstract\nsemantics and the group acting on them into concrete contrastive learning. At\neach iteration, IP-IRM first partitions the training samples into two subsets\nthat correspond to an entangled group element. Then, it minimizes a\nsubset-invariant contrastive loss, where the invariance guarantees to\ndisentangle the group element. We prove that IP-IRM converges to a fully\ndisentangled representation and show its effectiveness on various benchmarks.\nCodes are available at https://github.com/Wangt-CN/IP-IRM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_Z/0/1/0/all/0/1\">Zhongqi Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jianqiang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1\">Qianru Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hanwang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UltraPose: Synthesizing Dense Pose with 1 Billion Points by Human-body Decoupling 3D Model. (arXiv:2110.15267v1 [cs.CV])","link":"http://arxiv.org/abs/2110.15267","description":"<p>Recovering dense human poses from images plays a critical role in\nestablishing an image-to-surface correspondence between RGB images and the 3D\nsurface of the human body, serving the foundation of rich real-world\napplications, such as virtual humans, monocular-to-3d reconstruction. However,\nthe popular DensePose-COCO dataset relies on a sophisticated manual annotation\nsystem, leading to severe limitations in acquiring the denser and more accurate\nannotated pose resources. In this work, we introduce a new 3D human-body model\nwith a series of decoupled parameters that could freely control the generation\nof the body. Furthermore, we build a data generation system based on this\ndecoupling 3D model, and construct an ultra dense synthetic benchmark\nUltraPose, containing around 1.3 billion corresponding points. Compared to the\nexisting manually annotated DensePose-COCO dataset, the synthetic UltraPose has\nultra dense image-to-surface correspondences without annotation cost and error.\nOur proposed UltraPose provides the largest benchmark and data resources for\nlifting the model capability in predicting more accurate dense poses. To\npromote future researches in this field, we also propose a transformer-based\nmethod to model the dense correspondence between 2D and 3D worlds. The proposed\nmodel trained on synthetic UltraPose can be applied to real-world scenarios,\nindicating the effectiveness of our benchmark and model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1\">Haonan Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiaqi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xujie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shengkai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_N/0/1/0/all/0/1\">Nianhong Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_T/0/1/0/all/0/1\">Tianxiang Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Continuous Face Representation with Explicit Functions. (arXiv:2110.15268v1 [cs.CV])","link":"http://arxiv.org/abs/2110.15268","description":"<p>How to represent a face pattern? While it is presented in a continuous way in\nour visual system, computers often store and process the face image in a\ndiscrete manner with 2D arrays of pixels. In this study, we attempt to learn a\ncontinuous representation for face images with explicit functions. First, we\npropose an explicit model (EmFace) for human face representation in the form of\na finite sum of mathematical terms, where each term is an analytic function\nelement. Further, to estimate the unknown parameters of EmFace, a novel neural\nnetwork, EmNet, is designed with an encoder-decoder structure and trained using\nthe backpropagation algorithm, where the encoder is defined by a deep\nconvolutional neural network and the decoder is an explicit mathematical\nexpression of EmFace. Experimental results show that EmFace has a higher\nrepresentation performance on faces with various expressions, postures, and\nother factors, compared to that of other methods. Furthermore, EmFace achieves\nreasonable performance on several face image processing tasks, including face\nimage restoration, denoising, and transformation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liping Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Weijun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Linjun Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Lina Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ning_X/0/1/0/all/0/1\">Xin Ning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xiaoli Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jian Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_H/0/1/0/all/0/1\">Hong Qin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MEGAN: Memory Enhanced Graph Attention Network for Space-Time Video Super-Resolution. (arXiv:2110.15327v1 [cs.CV])","link":"http://arxiv.org/abs/2110.15327","description":"<p>Space-time video super-resolution (STVSR) aims to construct a high space-time\nresolution video sequence from the corresponding low-frame-rate, low-resolution\nvideo sequence. Inspired by the recent success to consider spatial-temporal\ninformation for space-time super-resolution, our main goal in this work is to\ntake full considerations of spatial and temporal correlations within the video\nsequences of fast dynamic events. To this end, we propose a novel one-stage\nmemory enhanced graph attention network (MEGAN) for space-time video\nsuper-resolution. Specifically, we build a novel long-range memory graph\naggregation (LMGA) module to dynamically capture correlations along the channel\ndimensions of the feature maps and adaptively aggregate channel features to\nenhance the feature representations. We introduce a non-local residual block,\nwhich enables each channel-wise feature to attend global spatial hierarchical\nfeatures. In addition, we adopt a progressive fusion module to further enhance\nthe representation ability by extensively exploiting spatial-temporal\ncorrelations from multiple frames. Experiment results demonstrate that our\nmethod achieves better results compared with the state-of-the-art methods\nquantitatively and visually.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1\">Chenyu You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1\">Lianyi Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_A/0/1/0/all/0/1\">Aosong Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1\">Ruihan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hui Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_W/0/1/0/all/0/1\">Wei Fan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Residual Relaxation for Multi-view Representation Learning. (arXiv:2110.15348v1 [cs.LG])","link":"http://arxiv.org/abs/2110.15348","description":"<p>Multi-view methods learn representations by aligning multiple views of the\nsame image and their performance largely depends on the choice of data\naugmentation. In this paper, we notice that some other useful augmentations,\nsuch as image rotation, are harmful for multi-view methods because they cause a\nsemantic shift that is too large to be aligned well. This observation motivates\nus to relax the exact alignment objective to better cultivate stronger\naugmentations. Taking image rotation as a case study, we develop a generic\napproach, Pretext-aware Residual Relaxation (Prelax), that relaxes the exact\nalignment by allowing an adaptive residual vector between different views and\nencoding the semantic shift through pretext-aware learning. Extensive\nexperiments on different backbones show that our method can not only improve\nmulti-view methods with existing augmentations, but also benefit from stronger\nimage augmentations like rotation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yifei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_Z/0/1/0/all/0/1\">Zhengyang Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_F/0/1/0/all/0/1\">Feng Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chuming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yisen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jiansheng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhouchen Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"XDEEP-MSI: Explainable Bias-Rejecting Microsatellite Instability Deep Learning System In Colorectal Cancer. (arXiv:2110.15350v1 [cs.CV])","link":"http://arxiv.org/abs/2110.15350","description":"<p>We present a system for the prediction of microsatellite instability (MSI)\nfrom H&amp;E images of colorectal cancer using deep learning (DL) techniques\ncustomized for tissue microarrays (TMAs). The system incorporates an end-to-end\nimage preprocessing module that produces tiles at multiple magnifications in\nthe regions of interest as guided by a tissue classifier module, and a\nmultiple-bias rejecting module. The training and validation TMA samples were\nobtained from the EPICOLON project and further enriched with samples from a\nsingle institution. A systematic study of biases at tile level identified three\nprotected (bias) variables associated with the learned representations of a\nbaseline model: the project of origin of samples, the patient spot and the TMA\nglass where each spot was placed. A multiple bias rejecting technique based on\nadversarial training is implemented at the DL architecture so to directly avoid\nlearning the batch effects of those variables. The learned features from the\nbias-ablated model have maximum discriminative power with respect to the task\nand minimal statistical mean dependence with the biases. The impact of\ndifferent magnifications, types of tissues and the model performance at tile vs\npatient level is analyzed. The AUC at tile level, and including all three\nselected tissues (tumor epithelium, mucine and lymphocytic regions) and 4\nmagnifications, was 0.87 +/- 0.03 and increased to 0.9 +/- 0.03 at patient\nlevel. To the best of our knowledge, this is the first work that incorporates a\nmultiple bias ablation technique at the DL architecture in digital pathology,\nand the first using TMAs for the MSI prediction task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bustos_A/0/1/0/all/0/1\">Aurelia Bustos</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Paya_A/0/1/0/all/0/1\">Artemio Pay&#xe1;</a> (2 and 3), <a href=\"http://arxiv.org/find/cs/1/au:+Torrubia_A/0/1/0/all/0/1\">Andres Torrubia</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Jover_R/0/1/0/all/0/1\">Rodrigo Jover</a> (2 and 3), <a href=\"http://arxiv.org/find/cs/1/au:+Llor_X/0/1/0/all/0/1\">Xavier Llor</a> (4), <a href=\"http://arxiv.org/find/cs/1/au:+Bessa_X/0/1/0/all/0/1\">Xavier Bessa</a> (5), <a href=\"http://arxiv.org/find/cs/1/au:+Castells_A/0/1/0/all/0/1\">Antoni Castells</a> (6), <a href=\"http://arxiv.org/find/cs/1/au:+Alenda_C/0/1/0/all/0/1\">Cristina Alenda</a> (2 and 3) ((1) AI Cancer Research Unit Medbravo, (2) Alicante University General Hospital, Spain, (3) Alicante Institute for Health and Biomedical Research ISABIAL, (4) Department of Medicine and Cancer Center at Yale University, Connecticut, (5) Hospital del Mar Medical Research Institute IMIM, Barcelona, Spain, (6) Hospital Cl&#xed;nic University of Barcelona IDIBAPS CIBERehd, Spain)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MCUNetV2: Memory-Efficient Patch-based Inference for Tiny Deep Learning. (arXiv:2110.15352v1 [cs.CV])","link":"http://arxiv.org/abs/2110.15352","description":"<p>Tiny deep learning on microcontroller units (MCUs) is challenging due to the\nlimited memory size. We find that the memory bottleneck is due to the\nimbalanced memory distribution in convolutional neural network (CNN) designs:\nthe first several blocks have an order of magnitude larger memory usage than\nthe rest of the network. To alleviate this issue, we propose a generic\npatch-by-patch inference scheduling, which operates only on a small spatial\nregion of the feature map and significantly cuts down the peak memory. However,\nnaive implementation brings overlapping patches and computation overhead. We\nfurther propose network redistribution to shift the receptive field and FLOPs\nto the later stage and reduce the computation overhead. Manually redistributing\nthe receptive field is difficult. We automate the process with neural\narchitecture search to jointly optimize the neural architecture and inference\nscheduling, leading to MCUNetV2. Patch-based inference effectively reduces the\npeak memory usage of existing networks by 4-8x. Co-designed with neural\nnetworks, MCUNetV2 sets a record ImageNet accuracy on MCU (71.8%), and achieves\n&gt;90% accuracy on the visual wake words dataset under only 32kB SRAM. MCUNetV2\nalso unblocks object detection on tiny devices, achieving 16.9% higher mAP on\nPascal VOC compared to the state-of-the-art result. Our study largely addressed\nthe memory bottleneck in tinyML and paved the way for various vision\napplications beyond image classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Ji Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wei-Ming Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1\">Han Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1\">Chuang Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Song Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Visual Reasoning by Learning Differentiable Physics Models from Video and Language. (arXiv:2110.15358v1 [cs.CV])","link":"http://arxiv.org/abs/2110.15358","description":"<p>In this work, we propose a unified framework, called Visual Reasoning with\nDiffer-entiable Physics (VRDP), that can jointly learn visual concepts and\ninfer physics models of objects and their interactions from videos and\nlanguage. This is achieved by seamlessly integrating three components: a visual\nperception module, a concept learner, and a differentiable physics engine. The\nvisual perception module parses each video frame into object-centric\ntrajectories and represents them as latent scene representations. The concept\nlearner grounds visual concepts (e.g., color, shape, and material) from these\nobject-centric representations based on the language, thus providing prior\nknowledge for the physics engine. The differentiable physics model, implemented\nas an impulse-based differentiable rigid-body simulator, performs\ndifferentiable physical simulation based on the grounded concepts to infer\nphysical properties, such as mass, restitution, and velocity, by fitting the\nsimulated trajectories into the video observations. Consequently, these learned\nconcepts and physical models can explain what we have seen and imagine what is\nabout to happen in future and counterfactual scenarios. Integrating\ndifferentiable physics into the dynamic reasoning framework offers several\nappealing benefits. More accurate dynamics prediction in learned physics models\nenables state-of-the-art performance on both synthetic and real-world\nbenchmarks while still maintaining high transparency and interpretability; most\nnotably, VRDP improves the accuracy of predictive and counterfactual questions\nby 4.5% and 11.5% compared to its best counterpart. VRDP is also highly\ndata-efficient: physical parameters can be optimized from very few videos, and\neven a single video can be sufficient. Finally, with all physical parameters\ninferred, VRDP can quickly learn new concepts from a few examples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1\">Mingyu Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhenfang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_T/0/1/0/all/0/1\">Tao Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1\">Joshua B. Tenenbaum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1\">Chuang Gan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Accelerating Robotic Reinforcement Learning via Parameterized Action Primitives. (arXiv:2110.15360v1 [cs.LG])","link":"http://arxiv.org/abs/2110.15360","description":"<p>Despite the potential of reinforcement learning (RL) for building\ngeneral-purpose robotic systems, training RL agents to solve robotics tasks\nstill remains challenging due to the difficulty of exploration in purely\ncontinuous action spaces. Addressing this problem is an active area of research\nwith the majority of focus on improving RL methods via better optimization or\nmore efficient exploration. An alternate but important component to consider\nimproving is the interface of the RL algorithm with the robot. In this work, we\nmanually specify a library of robot action primitives (RAPS), parameterized\nwith arguments that are learned by an RL policy. These parameterized primitives\nare expressive, simple to implement, enable efficient exploration and can be\ntransferred across robots, tasks and environments. We perform a thorough\nempirical study across challenging tasks in three distinct domains with image\ninput and a sparse terminal reward. We find that our simple change to the\naction interface substantially improves both the learning efficiency and task\nperformance irrespective of the underlying RL algorithm, significantly\noutperforming prior methods which learn skills from offline expert data. Code\nand videos at https://mihdalal.github.io/raps/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dalal_M/0/1/0/all/0/1\">Murtaza Dalal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pathak_D/0/1/0/all/0/1\">Deepak Pathak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1\">Ruslan Salakhutdinov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain-adaptive Crowd Counting via High-quality Image Translation and Density Reconstruction. (arXiv:1912.03677v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1912.03677","description":"<p>Recently, crowd counting using supervised learning achieves a remarkable\nimprovement. Nevertheless, most counters rely on a large amount of manually\nlabeled data. With the release of synthetic crowd data, a potential alternative\nis transferring knowledge from them to real data without any manual label.\nHowever, there is no method to effectively suppress domain gaps and output\nelaborate density maps during the transferring. To remedy the above problems,\nthis paper proposes a Domain-Adaptive Crowd Counting (DACC) framework, which\nconsists of a high-quality image translation and density map reconstruction. To\nbe specific, the former focuses on translating synthetic data to realistic\nimages, which prompts the translation quality by segregating\ndomain-shared/independent features and designing content-aware consistency\nloss. The latter aims at generating pseudo labels on real scenes to improve the\nprediction quality. Next, we retrain a final counter using these pseudo labels.\nAdaptation experiments on six real-world datasets demonstrate that the proposed\nmethod outperforms the state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Junyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_T/0/1/0/all/0/1\">Tao Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yuan Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GRAPHITE: A Practical Framework for Generating Automatic Physical Adversarial Machine Learning Attacks. (arXiv:2002.07088v5 [cs.CR] UPDATED)","link":"http://arxiv.org/abs/2002.07088","description":"<p>This paper investigates an adversary's ease of attack in generating\nadversarial examples for real-world scenarios. We address three key\nrequirements for practical attacks for the real-world: 1) automatically\nconstraining the size and shape of the attack so it can be applied with\nstickers, 2) transform-robustness, i.e., robustness of a attack to\nenvironmental physical variations such as viewpoint and lighting changes, and\n3) supporting attacks in both white-box and black-box hard-label scenarios, so\nthat the adversary can attack proprietary models. In particular, the art of\nautomatically picking which areas to perturb remains largely unexplored -- an\nefficient solution would remove the need to search over possible locations,\nshapes, and sizes as in current patch attacks. In this work, we propose\nGRAPHITE, an efficient and general framework for generating attacks that\nsatisfy the above three key requirements. GRAPHITE takes advantage of\ntransform-robustness, a metric based on expectation over transforms (EoT), to\nautomatically generate small masks and optimize with gradient-free\noptimization. GRAPHITE is also flexible as it can easily trade-off\ntransform-robustness, perturbation size, and query count in black-box settings.\nOn a GTSRB model in a hard-label black-box setting, we are able to find attacks\non all possible 1,806 victim-target class pairs with averages of 77.8%\ntransform-robustness, perturbation size of 16.63% of the victim images, and\n126K queries per pair. For digital-only attacks where achieving\ntransform-robustness is not a requirement, GRAPHITE is able to find successful\nsmall-patch attacks with an average of only 566 queries for 92.2% of\nvictim-target pairs. GRAPHITE is also able to find successful attacks using\nperturbations that modify small areas of the input image against PatchGuard, a\nrecently proposed defense against patch-based attacks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_R/0/1/0/all/0/1\">Ryan Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mangaokar_N/0/1/0/all/0/1\">Neal Mangaokar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiefeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandes_E/0/1/0/all/0/1\">Earlence Fernandes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1\">Somesh Jha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prakash_A/0/1/0/all/0/1\">Atul Prakash</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do CNNs Encode Data Augmentations?. (arXiv:2003.08773v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2003.08773","description":"<p>Data augmentations are important ingredients in the recipe for training\nrobust neural networks, especially in computer vision. A fundamental question\nis whether neural network features encode data augmentation transformations. To\nanswer this question, we introduce a systematic approach to investigate which\nlayers of neural networks are the most predictive of augmentation\ntransformations. Our approach uses features in pre-trained vision models with\nminimal additional processing to predict common properties transformed by\naugmentation (scale, aspect ratio, hue, saturation, contrast, and brightness).\nSurprisingly, neural network features not only predict data augmentation\ntransformations, but they predict many transformations with high accuracy.\nAfter validating that neural networks encode features corresponding to\naugmentation transformations, we show that these features are encoded in the\nearly layers of modern CNNs, though the augmentation signal fades in deeper\nlayers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_E/0/1/0/all/0/1\">Eddie Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yanping Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Monocular Depth Reconstruction of Non-Rigid Scenes. (arXiv:2012.15680v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.15680","description":"<p>Monocular depth reconstruction of complex and dynamic scenes is a highly\nchallenging problem. While for rigid scenes learning-based methods have been\noffering promising results even in unsupervised cases, there exists little to\nno literature addressing the same for dynamic and deformable scenes. In this\nwork, we present an unsupervised monocular framework for dense depth estimation\nof dynamic scenes, which jointly reconstructs rigid and non-rigid parts without\nexplicitly modelling the camera motion. Using dense correspondences, we derive\na training objective that aims to opportunistically preserve pairwise distances\nbetween reconstructed 3D points. In this process, the dense depth map is\nlearned implicitly using the as-rigid-as-possible hypothesis. Our method\nprovides promising results, demonstrating its capability of reconstructing 3D\nfrom challenging videos of non-rigid scenes. Furthermore, the proposed method\nalso provides unsupervised motion segmentation results as an auxiliary output.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Takmaz_A/0/1/0/all/0/1\">Ay&#xe7;a Takmaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paudel_D/0/1/0/all/0/1\">Danda Pani Paudel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Probst_T/0/1/0/all/0/1\">Thomas Probst</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chhatkuli_A/0/1/0/all/0/1\">Ajad Chhatkuli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oswald_M/0/1/0/all/0/1\">Martin R. Oswald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Instance Pose Networks: Rethinking Top-Down Pose Estimation. (arXiv:2101.11223v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.11223","description":"<p>A key assumption of top-down human pose estimation approaches is their\nexpectation of having a single person/instance present in the input bounding\nbox. This often leads to failures in crowded scenes with occlusions. We propose\na novel solution to overcome the limitations of this fundamental assumption.\nOur Multi-Instance Pose Network (MIPNet) allows for predicting multiple 2D pose\ninstances within a given bounding box. We introduce a Multi-Instance Modulation\nBlock (MIMB) that can adaptively modulate channel-wise feature responses for\neach instance and is parameter efficient. We demonstrate the efficacy of our\napproach by evaluating on COCO, CrowdPose, and OCHuman datasets. Specifically,\nwe achieve 70.0 AP on CrowdPose and 42.5 AP on OCHuman test sets, a significant\nimprovement of 2.4 AP and 6.5 AP over the prior art, respectively. When using\nground truth bounding boxes for inference, MIPNet achieves an improvement of\n0.7 AP on COCO, 0.9 AP on CrowdPose, and 9.1 AP on OCHuman validation sets\ncompared to HRNet. Interestingly, when fewer, high confidence bounding boxes\nare used, HRNet's performance degrades (by 5 AP) on OCHuman, whereas MIPNet\nmaintains a relatively stable performance (drop of 1 AP) for the same inputs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khirodkar_R/0/1/0/all/0/1\">Rawal Khirodkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chari_V/0/1/0/all/0/1\">Visesh Chari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_A/0/1/0/all/0/1\">Amit Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tyagi_A/0/1/0/all/0/1\">Ambrish Tyagi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evolving GAN Formulations for Higher Quality Image Synthesis. (arXiv:2102.08578v2 [cs.NE] UPDATED)","link":"http://arxiv.org/abs/2102.08578","description":"<p>Generative Adversarial Networks (GANs) have extended deep learning to complex\ngeneration and translation tasks across different data modalities. However,\nGANs are notoriously difficult to train: Mode collapse and other instabilities\nin the training process often degrade the quality of the generated results,\nsuch as images. This paper presents a new technique called TaylorGAN for\nimproving GANs by discovering customized loss functions for each of its two\nnetworks. The loss functions are parameterized as Taylor expansions and\noptimized through multiobjective evolution. On an image-to-image translation\nbenchmark task, this approach qualitatively improves generated image quality\nand quantitatively improves two independent GAN performance metrics. It\ntherefore forms a promising approach for applying GANs to more challenging\ntasks in the future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_S/0/1/0/all/0/1\">Santiago Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kant_M/0/1/0/all/0/1\">Mohak Kant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miikkulainen_R/0/1/0/all/0/1\">Risto Miikkulainen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Context Decoupling Augmentation for Weakly Supervised Semantic Segmentation. (arXiv:2103.01795v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.01795","description":"<p>Data augmentation is vital for deep learning neural networks. By providing\nmassive training samples, it helps to improve the generalization ability of the\nmodel. Weakly supervised semantic segmentation (WSSS) is a challenging problem\nthat has been deeply studied in recent years, conventional data augmentation\napproaches for WSSS usually employ geometrical transformations, random cropping\nand color jittering. However, merely increasing the same contextual semantic\ndata does not bring much gain to the networks to distinguish the objects, e.g.,\nthe correct image-level classification of \"aeroplane\" may be not only due to\nthe recognition of the object itself, but also its co-occurrence context like\n\"sky\", which will cause the model to focus less on the object features. To this\nend, we present a Context Decoupling Augmentation (CDA) method, to change the\ninherent context in which the objects appear and thus drive the network to\nremove the dependence between object instances and contextual information. To\nvalidate the effectiveness of the proposed method, extensive experiments on\nPASCAL VOC 2012 dataset with several alternative network architectures\ndemonstrate that CDA can boost various popular WSSS methods to the new\nstate-of-the-art by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yukun Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_R/0/1/0/all/0/1\">Ruizhou Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1\">Guosheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qingyao Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Computational Efficiency in Visual Reinforcement Learning via Stored Embeddings. (arXiv:2103.02886v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2103.02886","description":"<p>Recent advances in off-policy deep reinforcement learning (RL) have led to\nimpressive success in complex tasks from visual observations. Experience replay\nimproves sample-efficiency by reusing experiences from the past, and\nconvolutional neural networks (CNNs) process high-dimensional inputs\neffectively. However, such techniques demand high memory and computational\nbandwidth. In this paper, we present Stored Embeddings for Efficient\nReinforcement Learning (SEER), a simple modification of existing off-policy RL\nmethods, to address these computational and memory requirements. To reduce the\ncomputational overhead of gradient updates in CNNs, we freeze the lower layers\nof CNN encoders early in training due to early convergence of their parameters.\nAdditionally, we reduce memory requirements by storing the low-dimensional\nlatent vectors for experience replay instead of high-dimensional images,\nenabling an adaptive increase in the replay buffer capacity, a useful technique\nin constrained-memory settings. In our experiments, we show that SEER does not\ndegrade the performance of RL agents while significantly saving computation and\nmemory across a diverse set of DeepMind Control environments and Atari games.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lili Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kimin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivas_A/0/1/0/all/0/1\">Aravind Srinivas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1\">Pieter Abbeel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Student-Teacher Feature Pyramid Matching for Anomaly Detection. (arXiv:2103.04257v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.04257","description":"<p>Anomaly detection is a challenging task and usually formulated as an\none-class learning problem for the unexpectedness of anomalies. This paper\nproposes a simple yet powerful approach to this issue, which is implemented in\nthe student-teacher framework for its advantages but substantially extends it\nin terms of both accuracy and efficiency. Given a strong model pre-trained on\nimage classification as the teacher, we distill the knowledge into a single\nstudent network with the identical architecture to learn the distribution of\nanomaly-free images and this one-step transfer preserves the crucial clues as\nmuch as possible. Moreover, we integrate the multi-scale feature matching\nstrategy into the framework, and this hierarchical feature matching enables the\nstudent network to receive a mixture of multi-level knowledge from the feature\npyramid under better supervision, thus allowing to detect anomalies of various\nsizes. The difference between feature pyramids generated by the two networks\nserves as a scoring function indicating the probability of anomaly occurring.\nDue to such operations, our approach achieves accurate and fast pixel-level\nanomaly detection. Very competitive results are delivered on the MVTec anomaly\ndetection dataset, superior to the state of the art ones.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guodong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Shumin Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_E/0/1/0/all/0/1\">Errui Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">Di Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data-driven Cloud Clustering via a Rotationally Invariant Autoencoder. (arXiv:2103.04885v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.04885","description":"<p>Advanced satellite-born remote sensing instruments produce high-resolution\nmulti-spectral data for much of the globe at a daily cadence. These datasets\nopen up the possibility of improved understanding of cloud dynamics and\nfeedback, which remain the biggest source of uncertainty in global climate\nmodel projections. As a step towards answering these questions, we describe an\nautomated rotation-invariant cloud clustering (RICC) method that leverages deep\nlearning autoencoder technology to organize cloud imagery within large datasets\nin an unsupervised fashion, free from assumptions about predefined classes. We\ndescribe both the design and implementation of this method and its evaluation,\nwhich uses a sequence of testing protocols to determine whether the resulting\nclusters: (1) are physically reasonable, (i.e., embody scientifically relevant\ndistinctions); (2) capture information on spatial distributions, such as\ntextures; (3) are cohesive and separable in latent space; and (4) are\nrotationally invariant, (i.e., insensitive to the orientation of an image).\nResults obtained when these evaluation protocols are applied to RICC outputs\nsuggest that the resultant novel cloud clusters capture meaningful aspects of\ncloud physics, are appropriately spatially coherent, and are invariant to\norientations of input images. Our results support the possibility of using an\nunsupervised data-driven approach for automated clustering and pattern\ndiscovery in cloud imagery.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kurihana_T/0/1/0/all/0/1\">Takuya Kurihana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moyer_E/0/1/0/all/0/1\">Elisabeth Moyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Willett_R/0/1/0/all/0/1\">Rebecca Willett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gilton_D/0/1/0/all/0/1\">Davis Gilton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foster_I/0/1/0/all/0/1\">Ian Foster</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Monocular Multi-Layer Layout Estimation for Warehouse Racks. (arXiv:2103.09174v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.09174","description":"<p>Given a monocular colour image of a warehouse rack, we aim to predict the\nbird's-eye view layout for each shelf in the rack, which we term as multi-layer\nlayout prediction. To this end, we present RackLay, a deep neural network for\nreal-time shelf layout estimation from a single image. Unlike previous layout\nestimation methods, which provide a single layout for the dominant ground plane\nalone, RackLay estimates the top-view and front-view layout for each shelf in\nthe considered rack populated with objects. RackLay's architecture and its\nvariants are versatile and estimate accurate layouts for diverse scenes\ncharacterized by varying number of visible shelves in an image, large range in\nshelf occupancy factor and varied background clutter. Given the extreme paucity\nof datasets in this space and the difficulty involved in acquiring real data\nfrom warehouses, we additionally release a flexible synthetic dataset\ngeneration pipeline WareSynth which allows users to control the generation\nprocess and tailor the dataset according to contingent application. The\nablations across architectural variants and comparison with strong prior\nbaselines vindicate the efficacy of RackLay as an apt architecture for the\nnovel problem of multi-layered layout estimation. We also show that fusing the\ntop-view and front-view enables 3D reasoning applications such as metric free\nspace estimation for the considered rack.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nigam_M/0/1/0/all/0/1\">Meher Shashwat Nigam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prabhu_A/0/1/0/all/0/1\">Avinash Prabhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahu_A/0/1/0/all/0/1\">Anurag Sahu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_P/0/1/0/all/0/1\">Puru Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karandikar_T/0/1/0/all/0/1\">Tanvi Karandikar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shankar_N/0/1/0/all/0/1\">N. Sai Shankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarvadevabhatla_R/0/1/0/all/0/1\">Ravi Kiran Sarvadevabhatla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishna_K/0/1/0/all/0/1\">K. Madhava Krishna</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"#PraCegoVer: A Large Dataset for Image Captioning in Portuguese. (arXiv:2103.11474v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.11474","description":"<p>Automatically describing images using natural sentences is an important task\nto support visually impaired people's inclusion onto the Internet. It is still\na big challenge that requires understanding the relation of the objects present\nin the image and their attributes and actions they are involved in. Then,\nvisual interpretation methods are needed, but linguistic models are also\nnecessary to verbally describe the semantic relations. This problem is known as\nImage Captioning. Although many datasets were proposed in the literature, the\nmajority contains only English captions, whereas datasets with captions\ndescribed in other languages are scarce. Recently, a movement called PraCegoVer\narose on the Internet, stimulating users from social media to publish images,\ntag #PraCegoVer and add a short description of their content. Thus, inspired by\nthis movement, we have proposed the #PraCegoVer, a multi-modal dataset with\nPortuguese captions based on posts from Instagram. It is the first large\ndataset for image captioning in Portuguese with freely annotated images.\nFurther, the captions in our dataset bring additional challenges to the\nproblem: first, in contrast to popular datasets such as MS COCO Captions,\n#PraCegoVer has only one reference to each image; also, both mean and variance\nof our reference sentence length are significantly greater than those in the MS\nCOCO Captions. These two characteristics contribute to making our dataset\ninteresting due to the linguistic aspect and the challenges that it introduces\nto the image captioning problem. We publicly-share the dataset at\nhttps://github.com/gabrielsantosrv/PraCegoVer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Santos_G/0/1/0/all/0/1\">Gabriel Oliveira dos Santos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colombini_E/0/1/0/all/0/1\">Esther Luna Colombini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avila_S/0/1/0/all/0/1\">Sandra Avila</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Elastic Lottery Ticket Hypothesis. (arXiv:2103.16547v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.16547","description":"<p>Lottery Ticket Hypothesis (LTH) raises keen attention to identifying sparse\ntrainable subnetworks, or winning tickets, which can be trained in isolation to\nachieve similar or even better performance compared to the full models. Despite\nmany efforts being made, the most effective method to identify such winning\ntickets is still Iterative Magnitude-based Pruning (IMP), which is\ncomputationally expensive and has to be run thoroughly for every different\nnetwork. A natural question that comes in is: can we \"transform\" the winning\nticket found in one network to another with a different architecture, yielding\na winning ticket for the latter at the beginning, without re-doing the\nexpensive IMP? Answering this question is not only practically relevant for\nefficient \"once-for-all\" winning ticket finding, but also theoretically\nappealing for uncovering inherently scalable sparse patterns in networks. We\nconduct extensive experiments on CIFAR-10 and ImageNet, and propose a variety\nof strategies to tweak the winning tickets found from different networks of the\nsame model family (e.g., ResNets). Based on these results, we articulate the\nElastic Lottery Ticket Hypothesis (E-LTH): by mindfully replicating (or\ndropping) and re-ordering layers for one network, its corresponding winning\nticket could be stretched (or squeezed) into a subnetwork for another deeper\n(or shallower) network from the same family, whose performance is nearly the\nsame competitive as the latter's winning ticket directly found by IMP. We have\nalso extensively compared E-LTH with pruning-at-initialization and dynamic\nsparse training methods, as well as discussed the generalizability of E-LTH to\ndifferent model families, layer types, and across datasets. Code is available\nat https://github.com/VITA-Group/ElasticLTH.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaohan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yu Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuohang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1\">Zhe Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jingjing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Enabling Meta-Learning from Target Models. (arXiv:2104.03736v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2104.03736","description":"<p>Meta-learning can extract an inductive bias from previous learning experience\nand assist the training of new tasks. It is often realized through optimizing a\nmeta-model with the evaluation loss of task-specific solvers. Most existing\nalgorithms sample non-overlapping $\\mathit{support}$ sets and $\\mathit{query}$\nsets to train and evaluate the solvers respectively due to simplicity\n($\\mathcal{S}$/$\\mathcal{Q}$ protocol). Different from\n$\\mathcal{S}$/$\\mathcal{Q}$ protocol, we can also evaluate a task-specific\nsolver by comparing it to a target model $\\mathcal{T}$, which is the optimal\nmodel for this task or a model that behaves well enough on this task\n($\\mathcal{S}$/$\\mathcal{T}$ protocol). Although being short of research,\n$\\mathcal{S}$/$\\mathcal{T}$ protocol has unique advantages such as offering\nmore informative supervision, but it is computationally expensive. This paper\nlooks into this special evaluation method and takes a step towards putting it\ninto practice. We find that with a small ratio of tasks armed with target\nmodels, classic meta-learning algorithms can be improved a lot without\nconsuming many resources. We empirically verify the effectiveness of\n$\\mathcal{S}$/$\\mathcal{T}$ protocol in a typical application of meta-learning,\n$\\mathit{i.e.}$, few-shot learning. In detail, after constructing target models\nby fine-tuning the pre-trained network on those hard tasks, we match the\ntask-specific solvers and target models via knowledge distillation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Su Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Han-Jia Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_L/0/1/0/all/0/1\">Le Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_D/0/1/0/all/0/1\">De-Chuan Zhan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CoPE: Conditional image generation using Polynomial Expansions. (arXiv:2104.05077v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2104.05077","description":"<p>Generative modeling has evolved to a notable field of machine learning. Deep\npolynomial neural networks (PNNs) have demonstrated impressive results in\nunsupervised image generation, where the task is to map an input vector (i.e.,\nnoise) to a synthesized image. However, the success of PNNs has not been\nreplicated in conditional generation tasks, such as super-resolution. Existing\nPNNs focus on single-variable polynomial expansions which do not fare well to\ntwo-variable inputs, i.e., the noise variable and the conditional variable. In\nthis work, we introduce a general framework, called CoPE, that enables a\npolynomial expansion of two input variables and captures their auto- and\ncross-correlations. We exhibit how CoPE can be trivially augmented to accept an\narbitrary number of input variables. CoPE is evaluated in five tasks\n(class-conditional generation, inverse problems, edges-to-image translation,\nimage-to-image translation, attribute-guided generation) involving eight\ndatasets. The thorough evaluation suggests that CoPE can be useful for tackling\ndiverse conditional generation tasks. The source code of CoPE is available at\n\\url{https://github.com/grigorisg9gr/polynomial_nets_for_conditional_generation}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chrysos_G/0/1/0/all/0/1\">Grigorios G Chrysos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Georgopoulos_M/0/1/0/all/0/1\">Markos Georgopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panagakis_Y/0/1/0/all/0/1\">Yannis Panagakis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Learning of Global-Local Video Representations. (arXiv:2104.05418v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2104.05418","description":"<p>Contrastive learning has delivered impressive results for various tasks in\nthe self-supervised regime. However, existing approaches optimize for learning\nrepresentations specific to downstream scenarios, i.e., \\textit{global}\nrepresentations suitable for tasks such as classification or \\textit{local}\nrepresentations for tasks such as detection and localization. While they\nproduce satisfactory results in the intended downstream scenarios, they often\nfail to generalize to tasks that they were not originally designed for. In this\nwork, we propose to learn video representations that generalize to both the\ntasks which require global semantic information (e.g., classification) and the\ntasks that require local fine-grained spatio-temporal information (e.g.,\nlocalization). We achieve this by optimizing two contrastive objectives that\ntogether encourage our model to learn global-local visual information given\naudio signals. We show that the two objectives mutually improve the\ngeneralizability of the learned global-local representations, significantly\noutperforming their disjointly learned counterparts. We demonstrate our\napproach on various tasks including action/sound classification, lip reading,\ndeepfake detection, event and sound localization\n(https://github.com/yunyikristy/global\\_local).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shuang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Zhaoyang Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McDuff_D/0/1/0/all/0/1\">Daniel McDuff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yale Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Novel Binocular Eye-Tracking SystemWith Stereo Stimuli for 3D Gaze Estimation. (arXiv:2104.12167v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.12167","description":"<p>Eye-tracking technologies have been widely used in applications like\npsychological studies and human computer interactions (HCI). However, most\ncurrent eye trackers focus on 2D point of gaze (PoG) estimation and cannot\nprovide accurate gaze depth.Concerning future applications such as HCI with 3D\ndisplays, we propose a novel binocular eye tracking device with stereo stimuli\nto provide highly accurate 3D PoG estimation. In our device, the 3D stereo\nimaging system can provide users with a friendly and immersive 3D visual\nexperience without wearing any accessories. The eye capturing system can\ndirectly record the users eye movements under 3D stimuli without disturbance. A\nregression based 3D eye tracking model is built based on collected eye movement\ndata under stereo stimuli. Our model estimates users 2D gaze with features\ndefined by eye region landmarks and further estimates 3D PoG with a multi\nsource feature set constructed by comprehensive eye movement features and\ndisparity features from stereo stimuli. Two test stereo scenes with different\ndepths of field are designed to verify the model effectiveness. Experimental\nresults show that the average error for 2D gaze estimation was 0.66\\degree and\nfor 3D PoG estimation, the average errors are 1.85~cm/0.15~m over the workspace\nvolume 50~cm $\\times$ 30~cm $\\times$ 75~cm/2.4~m $\\times$ 4.0~m $\\times$ 7.9~m\nseparately.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jinglin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhipeng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Han Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jing_P/0/1/0/all/0/1\">Peiguang Jing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yu Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AGMB-Transformer: Anatomy-Guided Multi-Branch Transformer Network for Automated Evaluation of Root Canal Therapy. (arXiv:2105.00381v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.00381","description":"<p>Accurate evaluation of the treatment result on X-ray images is a significant\nand challenging step in root canal therapy since the incorrect interpretation\nof the therapy results will hamper timely follow-up which is crucial to the\npatients' treatment outcome. Nowadays, the evaluation is performed in a manual\nmanner, which is time-consuming, subjective, and error-prone. In this paper, we\naim to automate this process by leveraging the advances in computer vision and\nartificial intelligence, to provide an objective and accurate method for root\ncanal therapy result assessment. A novel anatomy-guided multi-branch\nTransformer (AGMB-Transformer) network is proposed, which first extracts a set\nof anatomy features and then uses them to guide a multi-branch Transformer\nnetwork for evaluation. Specifically, we design a polynomial curve fitting\nsegmentation strategy with the help of landmark detection to extract the\nanatomy features. Moreover, a branch fusion module and a multi-branch structure\nincluding our progressive Transformer and Group Multi-Head Self-Attention\n(GMHSA) are designed to focus on both global and local features for an accurate\ndiagnosis. To facilitate the research, we have collected a large-scale root\ncanal therapy evaluation dataset with 245 root canal therapy X-ray images, and\nthe experiment results show that our AGMB-Transformer can improve the diagnosis\naccuracy from 57.96% to 90.20% compared with the baseline network. The proposed\nAGMB-Transformer can achieve a highly accurate evaluation of root canal\ntherapy. To our best knowledge, our work is the first to perform automatic root\ncanal therapy evaluation and has important clinical value to reduce the\nworkload of endodontists.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunxiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_G/0/1/0/all/0/1\">Guodong Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yifan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qianni Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1\">Qun Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Lingling Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_Q/0/1/0/all/0/1\">Qisi Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_N/0/1/0/all/0/1\">Neng Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_R/0/1/0/all/0/1\">Ruizi Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_K/0/1/0/all/0/1\">Kai Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuai Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Graph Embeddings for Open World Compositional Zero-Shot Learning. (arXiv:2105.01017v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.01017","description":"<p>Compositional Zero-Shot learning (CZSL) aims to recognize unseen compositions\nof state and object visual primitives seen during training. A problem with\nstandard CZSL is the assumption of knowing which unseen compositions will be\navailable at test time. In this work, we overcome this assumption operating on\nthe open world setting, where no limit is imposed on the compositional space at\ntest time, and the search space contains a large number of unseen compositions.\nTo address this problem, we propose a new approach, Compositional Cosine Graph\nEmbeddings (Co-CGE), based on two principles. First, Co-CGE models the\ndependency between states, objects and their compositions through a graph\nconvolutional neural network. The graph propagates information from seen to\nunseen concepts, improving their representations. Second, since not all unseen\ncompositions are equally feasible, and less feasible ones may damage the\nlearned representations, Co-CGE estimates a feasibility score for each unseen\ncomposition, using the scores as margins in a cosine similarity-based loss and\nas weights in the adjacency matrix of the graphs. Experiments show that our\napproach achieves state-of-the-art performances in standard CZSL while\noutperforming previous methods in the open world scenario.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mancini_M/0/1/0/all/0/1\">Massimiliano Mancini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naeem_M/0/1/0/all/0/1\">Muhammad Ferjad Naeem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xian_Y/0/1/0/all/0/1\">Yongqin Xian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akata_Z/0/1/0/all/0/1\">Zeynep Akata</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deepfake Detection by Human Crowds, Machines, and Machine-informed Crowds. (arXiv:2105.06496v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.06496","description":"<p>The recent emergence of machine-manipulated media raises an important\nsocietal question: how can we know if a video that we watch is real or fake? In\ntwo online studies with 15,016 participants, we present authentic videos and\ndeepfakes and ask participants to identify which is which. We compare the\nperformance of ordinary human observers against the leading computer vision\ndeepfake detection model and find them similarly accurate while making\ndifferent kinds of mistakes. Together, participants with access to the model's\nprediction are more accurate than either alone, but inaccurate model\npredictions often decrease participants' accuracy. To probe the relative\nstrengths and weaknesses of humans and machines as detectors of deepfakes, we\nexamine human and machine performance across video-level features, and we\nevaluate the impact of pre-registered randomized interventions on deepfake\ndetection. We find that manipulations designed to disrupt visual processing of\nfaces hinder human participants' performance while mostly not affecting the\nmodel's performance, suggesting a role for specialized cognitive capacities in\nexplaining human deepfake detection performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Groh_M/0/1/0/all/0/1\">Matthew Groh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Epstein_Z/0/1/0/all/0/1\">Ziv Epstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Firestone_C/0/1/0/all/0/1\">Chaz Firestone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Picard_R/0/1/0/all/0/1\">Rosalind Picard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Trees for Learning on Graphs. (arXiv:2105.07264v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2105.07264","description":"<p>Graph Neural Networks (GNNs) have emerged as a flexible and powerful approach\nfor learning over graphs. Despite this success, existing GNNs are constrained\nby their local message-passing architecture and are provably limited in their\nexpressive power. In this work, we propose a new GNN architecture -- the Neural\nTree. The neural tree architecture does not perform message passing on the\ninput graph, but on a tree-structured graph, called the H-tree, that is\nconstructed from the input graph. Nodes in the H-tree correspond to subgraphs\nin the input graph, and they are reorganized in a hierarchical manner such that\nthe parent of a node in the H-tree always corresponds to a larger subgraph in\nthe input graph. We show that the neural tree architecture can approximate any\nsmooth probability distribution function over an undirected graph. We also\nprove that the number of parameters needed to achieve an\n$\\epsilon$-approximation of the distribution function is exponential in the\ntreewidth of the input graph, but linear in its size. We prove that any\ncontinuous $\\mathcal{G}$-invariant/equivariant function can be approximated by\na nonlinear combination of such probability distribution functions over\n$\\mathcal{G}$. We apply the neural tree to semi-supervised node classification\nin 3D scene graphs, and show that these theoretical properties translate into\nsignificant gains in prediction accuracy, over the more traditional GNN\narchitectures. We also show the applicability of the neural tree architecture\nto citation networks with large treewidth, by using a graph sub-sampling\ntechnique.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Talak_R/0/1/0/all/0/1\">Rajat Talak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Siyi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_L/0/1/0/all/0/1\">Lisa Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carlone_L/0/1/0/all/0/1\">Luca Carlone</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Post-Training Sparsity-Aware Quantization. (arXiv:2105.11010v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2105.11010","description":"<p>Quantization is a technique used in deep neural networks (DNNs) to increase\nexecution performance and hardware efficiency. Uniform post-training\nquantization (PTQ) methods are common, since they can be implemented\nefficiently in hardware and do not require extensive hardware resources or a\ntraining set. Mapping FP32 models to INT8 using uniform PTQ yields models with\nnegligible accuracy degradation; however, reducing precision below 8 bits with\nPTQ is challenging, as accuracy degradation becomes noticeable, due to the\nincrease in quantization noise. In this paper, we propose a sparsity-aware\nquantization (SPARQ) method, in which the unstructured and dynamic activation\nsparsity is leveraged in different representation granularities. 4-bit\nquantization, for example, is employed by dynamically examining the bits of\n8-bit values and choosing a window of 4 bits, while first skipping zero-value\nbits. Moreover, instead of quantizing activation-by-activation to 4 bits, we\nfocus on pairs of 8-bit activations and examine whether one of the two is equal\nto zero. If one is equal to zero, the second can opportunistically use the\nother's 4-bit budget; if both do not equal zero, then each is dynamically\nquantized to 4 bits, as described. SPARQ achieves minor accuracy degradation\nand a practical hardware implementation. The code is available at\nhttps://github.com/gilshm/sparq.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shomron_G/0/1/0/all/0/1\">Gil Shomron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gabbay_F/0/1/0/all/0/1\">Freddy Gabbay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurzum_S/0/1/0/all/0/1\">Samer Kurzum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weiser_U/0/1/0/all/0/1\">Uri Weiser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The effectiveness of feature attribution methods and its correlation with automatic evaluation scores. (arXiv:2105.14944v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.14944","description":"<p>Explaining the decisions of an Artificial Intelligence (AI) model is\nincreasingly critical in many real-world, high-stake applications. Hundreds of\npapers have either proposed new feature attribution methods, discussed or\nharnessed these tools in their work. However, despite humans being the target\nend-users, most attribution methods were only evaluated on proxy\nautomatic-evaluation metrics (Zhang et al. 2018; Zhou et al. 2016; Petsiuk et\nal. 2018). In this paper, we conduct the first user study to measure\nattribution map effectiveness in assisting humans in ImageNet classification\nand Stanford Dogs fine-grained classification, and when an image is natural or\nadversarial (i.e., contains adversarial perturbations). Overall, feature\nattribution is surprisingly not more effective than showing humans nearest\ntraining-set examples. On a harder task of fine-grained dog categorization,\npresenting attribution maps to humans does not help, but instead hurts the\nperformance of human-AI teams compared to AI alone. Importantly, we found\nautomatic attribution-map evaluation measures to correlate poorly with the\nactual human-AI team performance. Our findings encourage the community to\nrigorously test their methods on the downstream human-in-the-loop applications\nand to rethink the existing evaluation metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_G/0/1/0/all/0/1\">Giang Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Daeyoung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1\">Anh Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers. (arXiv:2105.15203v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.15203","description":"<p>We present SegFormer, a simple, efficient yet powerful semantic segmentation\nframework which unifies Transformers with lightweight multilayer perception\n(MLP) decoders. SegFormer has two appealing features: 1) SegFormer comprises a\nnovel hierarchically structured Transformer encoder which outputs multiscale\nfeatures. It does not need positional encoding, thereby avoiding the\ninterpolation of positional codes which leads to decreased performance when the\ntesting resolution differs from training. 2) SegFormer avoids complex decoders.\nThe proposed MLP decoder aggregates information from different layers, and thus\ncombining both local attention and global attention to render powerful\nrepresentations. We show that this simple and lightweight design is the key to\nefficient segmentation on Transformers. We scale our approach up to obtain a\nseries of models from SegFormer-B0 to SegFormer-B5, reaching significantly\nbetter performance and efficiency than previous counterparts. For example,\nSegFormer-B4 achieves 50.3% mIoU on ADE20K with 64M parameters, being 5x\nsmaller and 2.2% better than the previous best method. Our best model,\nSegFormer-B5, achieves 84.0% mIoU on Cityscapes validation set and shows\nexcellent zero-shot robustness on Cityscapes-C. Code will be released at:\ngithub.com/NVlabs/SegFormer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1\">Enze Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenhai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhiding Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1\">Anima Anandkumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alvarez_J/0/1/0/all/0/1\">Jose M. Alvarez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spline Positional Encoding for Learning 3D Implicit Signed Distance Fields. (arXiv:2106.01553v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.01553","description":"<p>Multilayer perceptrons (MLPs) have been successfully used to represent 3D\nshapes implicitly and compactly, by mapping 3D coordinates to the corresponding\nsigned distance values or occupancy values. In this paper, we propose a novel\npositional encoding scheme, called Spline Positional Encoding, to map the input\ncoordinates to a high dimensional space before passing them to MLPs, for\nhelping to recover 3D signed distance fields with fine-scale geometric details\nfrom unorganized 3D point clouds. We verified the superiority of our approach\nover other positional encoding schemes on tasks of 3D shape reconstruction from\ninput point clouds and shape space learning. The efficacy of our approach\nextended to image reconstruction is also demonstrated and evaluated.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peng-Shuai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yu-Qi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_X/0/1/0/all/0/1\">Xin Tong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Local Disentanglement in Variational Auto-Encoders Using Jacobian $L_1$ Regularization. (arXiv:2106.02923v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.02923","description":"<p>There have been many recent advances in representation learning; however,\nunsupervised representation learning can still struggle with model\nidentification issues related to rotations of the latent space. Variational\nAuto-Encoders (VAEs) and their extensions such as $\\beta$-VAEs have been shown\nto improve local alignment of latent variables with PCA directions, which can\nhelp to improve model disentanglement under some conditions. Borrowing\ninspiration from Independent Component Analysis (ICA) and sparse coding, we\npropose applying an $L_1$ loss to the VAE's generative Jacobian during training\nto encourage local latent variable alignment with independent factors of\nvariation in images of multiple objects or images with multiple parts. We\ndemonstrate our results on a variety of datasets, giving qualitative and\nquantitative results using information theoretic and modularity measures that\nshow our added $L_1$ cost encourages local axis alignment of the latent\nrepresentation with individual factors of variation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rhodes_T/0/1/0/all/0/1\">Travers Rhodes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Daniel D. Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantically Controllable Generation of Physical Scenes with Explicit Knowledge. (arXiv:2106.04066v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.04066","description":"<p>Deep Generative Models (DGMs) are known for their superior capability in\ngenerating realistic data. Extending purely data-driven approaches, recent\nspecialized DGMs may satisfy additional controllable requirements such as\nembedding a traffic sign in a driving scene, by manipulating patterns\n\\textit{implicitly} in the neuron or feature level. In this paper, we introduce\na novel method to incorporate domain knowledge \\textit{explicitly} in the\ngeneration process to achieve semantically controllable scene generation. We\ncategorize our knowledge into two types to be consistent with the composition\nof natural scenes, where the first type represents the property of objects and\nthe second type represents the relationship among objects. We then propose a\ntree-structured generative model to learn complex scene representation, whose\nnodes and edges are naturally corresponding to the two types of knowledge\nrespectively. Knowledge can be explicitly integrated to enable semantically\ncontrollable scene generation by imposing semantic rules on properties of nodes\nand edges in the tree structure. We construct a synthetic example to illustrate\nthe controllability and explainability of our method in a clean setting. We\nfurther extend the synthetic example to realistic autonomous vehicle driving\nenvironments and conduct extensive experiments to show that our method\nefficiently identifies adversarial traffic scenes against different\nstate-of-the-art 3D point cloud segmentation models satisfying the traffic\nrules specified as the explicit knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1\">Wenhao Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eun_K/0/1/0/all/0/1\">Kim Ji Eun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Ding Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Learning with Data Augmentations Provably Isolates Content from Style. (arXiv:2106.04619v2 [stat.ML] UPDATED)","link":"http://arxiv.org/abs/2106.04619","description":"<p>Self-supervised representation learning has shown remarkable success in a\nnumber of domains. A common practice is to perform data augmentation via\nhand-crafted transformations intended to leave the semantics of the data\ninvariant. We seek to understand the empirical success of this approach from a\ntheoretical perspective. We formulate the augmentation process as a latent\nvariable model by postulating a partition of the latent representation into a\ncontent component, which is assumed invariant to augmentation, and a style\ncomponent, which is allowed to change. Unlike prior work on disentanglement and\nindependent component analysis, we allow for both nontrivial statistical and\ncausal dependencies in the latent space. We study the identifiability of the\nlatent representation based on pairs of views of the observations and prove\nsufficient conditions that allow us to identify the invariant content partition\nup to an invertible mapping in both generative and discriminative settings. We\nfind numerical simulations with dependent latent variables are consistent with\nour theory. Lastly, we introduce Causal3DIdent, a dataset of high-dimensional,\nvisually complex images with rich causal dependencies, which we use to study\nthe effect of data augmentations performed in practice.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Kugelgen_J/0/1/0/all/0/1\">Julius von K&#xfc;gelgen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sharma_Y/0/1/0/all/0/1\">Yash Sharma</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gresele_L/0/1/0/all/0/1\">Luigi Gresele</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Brendel_W/0/1/0/all/0/1\">Wieland Brendel</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Besserve_M/0/1/0/all/0/1\">Michel Besserve</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Locatello_F/0/1/0/all/0/1\">Francesco Locatello</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"No Fear of Heterogeneity: Classifier Calibration for Federated Learning with Non-IID Data. (arXiv:2106.05001v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.05001","description":"<p>A central challenge in training classification models in the real-world\nfederated system is learning with non-IID data. To cope with this, most of the\nexisting works involve enforcing regularization in local optimization or\nimproving the model aggregation scheme at the server. Other works also share\npublic datasets or synthesized samples to supplement the training of\nunder-represented classes or introduce a certain level of personalization.\nThough effective, they lack a deep understanding of how the data heterogeneity\naffects each layer of a deep classification model. In this paper, we bridge\nthis gap by performing an experimental analysis of the representations learned\nby different layers. Our observations are surprising: (1) there exists a\ngreater bias in the classifier than other layers, and (2) the classification\nperformance can be significantly improved by post-calibrating the classifier\nafter federated training. Motivated by the above findings, we propose a novel\nand simple algorithm called Classifier Calibration with Virtual Representations\n(CCVR), which adjusts the classifier using virtual representations sampled from\nan approximated gaussian mixture model. Experimental results demonstrate that\nCCVR achieves state-of-the-art performance on popular federated learning\nbenchmarks including CIFAR-10, CIFAR-100, and CINIC-10. We hope that our simple\nyet effective method can shed some light on the future research of federated\nlearning with non-IID data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_M/0/1/0/all/0/1\">Mi Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Fei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1\">Dapeng Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yifan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jian Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jiashi Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethink Transfer Learning in Medical Image Classification. (arXiv:2106.05152v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2106.05152","description":"<p>Transfer learning (TL) with deep convolutional neural networks (DCNNs) has\nproved successful in medical image classification (MIC). However, the current\npractice is puzzling, as MIC typically relies only on low- and/or mid-level\nfeatures that are learned in the bottom layers of DCNNs. Following this\nintuition, we question the current strategies of TL in MIC. In this paper, we\nperform careful experimental comparisons between shallow and deep networks for\nclassification on two chest x-ray datasets, using different TL strategies. We\nfind that deep models are not always favorable, and finetuning truncated deep\nmodels almost always yields the best performance, especially in data-poor\nregimes.\n</p>\n<p>Project webpage:\nhttps://sun-umn.github.io/Transfer-Learning-in-Medical-Imaging/\n</p>\n<p>Keywords: Transfer learning, Medical image classification, Feature hierarchy,\nMedical imaging, Evaluation metrics, Imbalanced data\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Peng_L/0/1/0/all/0/1\">Le Peng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liang_H/0/1/0/all/0/1\">Hengyue Liang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_T/0/1/0/all/0/1\">Taihui Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_J/0/1/0/all/0/1\">Ju Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Canonical Face Embeddings. (arXiv:2106.07822v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.07822","description":"<p>We present evidence that many common convolutional neural networks (CNNs)\ntrained for face verification learn functions that are nearly equivalent under\nrotation. More specifically, we demonstrate that one face verification model's\nembeddings (i.e. last-layer activations) can be compared directly to another\nmodel's embeddings after only a rotation or linear transformation, with little\nperformance penalty. This finding is demonstrated using IJB-C 1:1 verification\nacross the combinations of ten modern off-the-shelf CNN-based face verification\nmodels which vary in training dataset, CNN architecture, method of angular loss\ncalculation, or some combination of the 3. These networks achieve a mean true\naccept rate of 0.96 at a false accept rate of 0.01. When instead evaluating\nembeddings generated from two CNNs, where one CNN's embeddings are mapped with\na linear transformation, the mean true accept rate drops to 0.95 using the same\nverification paradigm. Restricting these linear maps to only perform rotation\nproduces a mean true accept rate of 0.91. These mappings' existence suggests\nthat a common representation is learned by models despite variation in training\nor structure. We discuss the broad implications a result like this has,\nincluding an example regarding face template security.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+McNeely_White_D/0/1/0/all/0/1\">David McNeely-White</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sattelberg_B/0/1/0/all/0/1\">Ben Sattelberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blanchard_N/0/1/0/all/0/1\">Nathaniel Blanchard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beveridge_R/0/1/0/all/0/1\">Ross Beveridge</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Combiner: Full Attention Transformer with Sparse Computation Cost. (arXiv:2107.05768v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2107.05768","description":"<p>Transformers provide a class of expressive architectures that are extremely\neffective for sequence modeling. However, the key limitation of transformers is\ntheir quadratic memory and time complexity $\\mathcal{O}(L^2)$ with respect to\nthe sequence length in attention layers, which restricts application in\nextremely long sequences. Most existing approaches leverage sparsity or\nlow-rank assumptions in the attention matrix to reduce cost, but sacrifice\nexpressiveness. Instead, we propose Combiner, which provides full attention\ncapability in each attention head while maintaining low computation and memory\ncomplexity. The key idea is to treat the self-attention mechanism as a\nconditional expectation over embeddings at each location, and approximate the\nconditional distribution with a structured factorization. Each location can\nattend to all other locations, either via direct attention, or through indirect\nattention to abstractions, which are again conditional expectations of\nembeddings from corresponding local regions. We show that most sparse attention\npatterns used in existing sparse transformers are able to inspire the design of\nsuch factorization for full attention, resulting in the same sub-quadratic cost\n($\\mathcal{O}(L\\log(L))$ or $\\mathcal{O}(L\\sqrt{L})$). Combiner is a drop-in\nreplacement for attention layers in existing transformers and can be easily\nimplemented in common frameworks. An experimental evaluation on both\nautoregressive and bidirectional sequence tasks demonstrates the effectiveness\nof this approach, yielding state-of-the-art results on several image and text\nmodeling tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1\">Hongyu Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1\">Hanjun Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Z/0/1/0/all/0/1\">Zihang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Mengjiao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leskovec_J/0/1/0/all/0/1\">Jure Leskovec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuurmans_D/0/1/0/all/0/1\">Dale Schuurmans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1\">Bo Dai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fully Automated Machine Learning Pipeline for Echocardiogram Segmentation. (arXiv:2107.08440v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.08440","description":"<p>Nowadays, cardiac diagnosis largely depends on left ventricular function\nassessment. With the help of the segmentation deep learning model, the\nassessment of the left ventricle becomes more accessible and accurate. However,\ndeep learning technique still faces two main obstacles: the difficulty in\nacquiring sufficient training data and time-consuming in developing quality\nmodels. In the ordinary data acquisition process, the dataset was selected\nrandomly from a large pool of unlabeled images for labeling, leading to massive\nlabor time to annotate those images. Besides that, hand-designed model\ndevelopment is strenuous and also costly. This paper introduces a pipeline that\nrelies on Active Learning to ease the labeling work and utilizes Neural\nArchitecture Search's idea to design the adequate deep learning model\nautomatically. We called this Fully automated machine learning pipeline for\nechocardiogram segmentation. The experiment results show that our method\nobtained the same IOU accuracy with only two-fifths of the original training\ndataset, and the searched model got the same accuracy as the hand-designed\nmodel given the same training dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thuy_H/0/1/0/all/0/1\">Hang Duong Thi Thuy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minh_T/0/1/0/all/0/1\">Tuan Nguyen Minh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Van_P/0/1/0/all/0/1\">Phi Nguyen Van</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quoc_L/0/1/0/all/0/1\">Long Tran Quoc</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Temporal Alignment Prediction for Few-Shot Video Classification. (arXiv:2107.11960v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.11960","description":"<p>The goal of few-shot video classification is to learn a classification model\nwith good generalization ability when trained with only a few labeled videos.\nHowever, it is difficult to learn discriminative feature representations for\nvideos in such a setting. In this paper, we propose Temporal Alignment\nPrediction (TAP) based on sequence similarity learning for few-shot video\nclassification. In order to obtain the similarity of a pair of videos, we\npredict the alignment scores between all pairs of temporal positions in the two\nvideos with the temporal alignment prediction function. Besides, the inputs to\nthis function are also equipped with the context information in the temporal\ndomain. We evaluate TAP on two video classification benchmarks including\nKinetics and Something-Something V2. The experimental results verify the\neffectiveness of TAP and show its superiority over state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_F/0/1/0/all/0/1\">Fei Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chunlei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jie Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yanwen Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Transductive Maximum Margin Classifier for Few-Shot Learning. (arXiv:2107.11975v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.11975","description":"<p>Few-shot learning aims to train a classifier that can generalize well when\njust a small number of labeled examples per class are given. We introduce a\ntransductive maximum margin classifier for few-shot learning (FS-TMMC). The\nbasic idea of the classical maximum margin classifier is to solve an optimal\nprediction function so that the training data can be correctly classified by\nthe resulting classifer with the largest geometric margin. In few-shot\nlearning, it is challenging to find such classifiers with good generalization\nability due to the insufficiency of training data in the support set. FS-TMMC\nleverages the unlabeled query examples to adjust the separating hyperplane of\nthe maximum margin classifier such that the prediction function is optimal on\nboth the support and query sets. Furthermore, we use an efficient and effective\nquasi-Newton algorithm, the L-BFGS method for optimization. Experimental\nresults on three standard few-shot learning benchmarks including miniImagenet,\ntieredImagenet and CUB show that our method achieves state-of-the-art\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_F/0/1/0/all/0/1\">Fei Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chunlei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jie Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yanwen Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Adversarially Blur Visual Object Tracking. (arXiv:2107.12085v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.12085","description":"<p>Motion blur caused by the moving of the object or camera during the exposure\ncan be a key challenge for visual object tracking, affecting tracking accuracy\nsignificantly. In this work, we explore the robustness of visual object\ntrackers against motion blur from a new angle, i.e., adversarial blur attack\n(ABA). Our main objective is to online transfer input frames to their natural\nmotion-blurred counterparts while misleading the state-of-the-art trackers\nduring the tracking process. To this end, we first design the motion blur\nsynthesizing method for visual tracking based on the generation principle of\nmotion blur, considering the motion information and the light accumulation\nprocess. With this synthetic method, we propose optimization-based ABA (OP-ABA)\nby iteratively optimizing an adversarial objective function against the\ntracking w.r.t. the motion and light accumulation parameters. The OP-ABA is\nable to produce natural adversarial examples but the iteration can cause heavy\ntime cost, making it unsuitable for attacking real-time trackers. To alleviate\nthis issue, we further propose one-step ABA (OS-ABA) where we design and train\na joint adversarial motion and accumulation predictive network (JAMANet) with\nthe guidance of OP-ABA, which is able to efficiently estimate the adversarial\nmotion and accumulation parameters in a one-step way. The experiments on four\npopular datasets (e.g., OTB100, VOT2018, UAV123, and LaSOT) demonstrate that\nour methods are able to cause significant accuracy drops on four\nstate-of-the-art trackers with high transferability. Please find the source\ncode at \\url{https://github.com/tsingqguo/ABA}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1\">Qing Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Ziyi Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Juefei_Xu_F/0/1/0/all/0/1\">Felix Juefei-Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xiaofei Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jianjun Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Video Object Segmentation by Motion-Aware Mask Propagation. (arXiv:2107.12569v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.12569","description":"<p>We propose a self-supervised spatio-temporal matching method, coined\nMotion-Aware Mask Propagation (MAMP), for video object segmentation. MAMP\nleverages the frame reconstruction task for training without the need for\nannotations. During inference, MAMP extracts high-resolution features from each\nframe to build a memory bank from the features as well as the predicted masks\nof selected past frames. MAMP then propagates the masks from the memory bank to\nsubsequent frames according to our proposed motion-aware spatio-temporal\nmatching module to handle fast motion and long-term matching scenarios.\nEvaluation on DAVIS-2017 and YouTube-VOS datasets show that MAMP achieves\nstate-of-the-art performance with stronger generalization ability compared to\nexisting self-supervised methods, i.e., 4.2% higher mean J&amp;F on DAVIS-2017 and\n4.85% higher mean J&amp;F on the unseen categories of YouTube-VOS than the nearest\ncompetitor. Moreover, MAMP performs at par with many supervised video object\nsegmentation methods. Our code is available at:\nhttps://github.com/bo-miao/MAMP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Miao_B/0/1/0/all/0/1\">Bo Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bennamoun_M/0/1/0/all/0/1\">Mohammed Bennamoun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yongsheng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mian_A/0/1/0/all/0/1\">Ajmal Mian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Transferable Are Self-supervised Features in Medical Image Classification Tasks?. (arXiv:2108.10048v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.10048","description":"<p>Transfer learning has become a standard practice to mitigate the lack of\nlabeled data in medical classification tasks. Whereas finetuning a downstream\ntask using supervised ImageNet pretrained features is straightforward and\nextensively investigated in many works, there is little study on the usefulness\nof self-supervised pretraining. In this paper, we assess the transferability of\nImageNet self-supervisedpretraining by evaluating the performance of models\ninitialized with pretrained features from three self-supervised techniques\n(SimCLR, SwAV, and DINO) on selected medical classification tasks. The chosen\ntasks cover tumor detection in sentinel axillary lymph node images, diabetic\nretinopathy classification in fundus images, and multiple pathological\ncondition classification in chest X-ray images. We demonstrate that\nself-supervised pretrained models yield richer embeddings than their supervised\ncounterpart, which benefits downstream tasks in view of both linear evaluation\nand finetuning. For example, in view of linear evaluation at acritically small\nsubset of the data, we see an improvement up to 14.79% in Kappa score in the\ndiabetic retinopathy classification task, 5.4% in AUC in the tumor\nclassification task, 7.03% AUC in the pneumonia detection, and 9.4% in AUC in\nthe detection of pathological conditions in chest X-ray. In addition, we\nintroduce Dynamic Visual Meta-Embedding (DVME) as an end-to-end transfer\nlearning approach that fuses pretrained embeddings from multiple models. We\nshow that the collective representation obtained by DVME leads to a significant\nimprovement in the performance of selected tasks compared to using a single\npretrained model approach and can be generalized to any combination of\npretrained models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Truong_T/0/1/0/all/0/1\">Tuan Truong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammadi_S/0/1/0/all/0/1\">Sadegh Mohammadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lenga_M/0/1/0/all/0/1\">Matthias Lenga</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Transformer for Single Image Super-Resolution. (arXiv:2108.11084v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.11084","description":"<p>Single image super-resolution task has witnessed great strides with the\ndevelopment of deep learning. However, most existing studies focus on building\na more complex neural network with a massive number of layers, bringing heavy\ncomputational cost and memory storage. Recently, as Transformer yields\nbrilliant results in NLP tasks, more and more researchers start to explore the\napplication of Transformer in computer vision tasks. But with the heavy\ncomputational cost and high GPU memory occupation of the vision Transformer,\nthe network can not be designed too deep. To address this problem, we propose a\nnovel Efficient Super-Resolution Transformer (ESRT) for fast and accurate image\nsuper-resolution. ESRT is a hybrid Transformer where a CNN-based SR network is\nfirst designed in the front to extract deep features. Specifically, there are\ntwo backbones for formatting the ESRT: lightweight CNN backbone (LCB) and\nlightweight Transformer backbone (LTB). Among them, LCB is a lightweight SR\nnetwork to extract deep SR features at a low computational cost by dynamically\nadjusting the size of the feature map. LTB is made up of an efficient\nTransformer (ET) with a small GPU memory occupation, which benefited from the\nnovel efficient multi-head attention (EMHA). In EMHA, a feature split module\n(FSM) is proposed to split the long sequence into sub-segments and then these\nsub-segments are applied by attention operation. This module can significantly\ndecrease the GPU memory occupation. Extensive experiments show that our ESRT\nachieves competitive results. Compared with the original Transformer which\noccupies 16057M GPU memory, the proposed ET only occupies 4191M GPU memory with\nbetter performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhisheng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juncheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Linlin Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Shifted Chunk Transformer for Spatio-Temporal Representational Learning. (arXiv:2108.11575v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.11575","description":"<p>Spatio-temporal representational learning has been widely adopted in various\nfields such as action recognition, video object segmentation, and action\nanticipation. Previous spatio-temporal representational learning approaches\nprimarily employ ConvNets or sequential models,e.g., LSTM, to learn the\nintra-frame and inter-frame features. Recently, Transformer models have\nsuccessfully dominated the study of natural language processing (NLP), image\nclassification, etc. However, the pure-Transformer based spatio-temporal\nlearning can be prohibitively costly on memory and computation to extract\nfine-grained features from a tiny patch. To tackle the training difficulty and\nenhance the spatio-temporal learning, we construct a shifted chunk Transformer\nwith pure self-attention blocks. Leveraging the recent efficient Transformer\ndesign in NLP, this shifted chunk Transformer can learn hierarchical\nspatio-temporal features from a local tiny patch to a global video clip. Our\nshifted self-attention can also effectively model complicated inter-frame\nvariances. Furthermore, we build a clip encoder based on Transformer to model\nlong-term temporal dependencies. We conduct thorough ablation studies to\nvalidate each component and hyper-parameters in our shifted chunk Transformer,\nand it outperforms previous state-of-the-art approaches on Kinetics-400,\nKinetics-600, UCF101, and HMDB51.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zha_X/0/1/0/all/0/1\">Xuefan Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wentao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_T/0/1/0/all/0/1\">Tingxun Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Ji Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Looking at the whole picture: constrained unsupervised anomaly segmentation. (arXiv:2109.00482v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2109.00482","description":"<p>Current unsupervised anomaly localization approaches rely on generative\nmodels to learn the distribution of normal images, which is later used to\nidentify potential anomalous regions derived from errors on the reconstructed\nimages. However, a main limitation of nearly all prior literature is the need\nof employing anomalous images to set a class-specific threshold to locate the\nanomalies. This limits their usability in realistic scenarios, where only\nnormal data is typically accessible. Despite this major drawback, only a\nhandful of works have addressed this limitation, by integrating supervision on\nattention maps during training. In this work, we propose a novel formulation\nthat does not require accessing images with abnormalities to define the\nthreshold. Furthermore, and in contrast to very recent work, the proposed\nconstraint is formulated in a more principled manner, leveraging well-known\nknowledge in constrained optimization. In particular, the equality constraint\non the attention maps in prior work is replaced by an inequality constraint,\nwhich allows more flexibility. In addition, to address the limitations of\npenalty-based functions we employ an extension of the popular log-barrier\nmethods to handle the constraint. Comprehensive experiments on the popular\nBRATS'19 dataset demonstrate that the proposed approach substantially\noutperforms relevant literature, establishing new state-of-the-art results for\nunsupervised lesion segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Silva_Rodriguez_J/0/1/0/all/0/1\">Julio Silva-Rodr&#xed;guez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Naranjo_V/0/1/0/all/0/1\">Valery Naranjo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dolz_J/0/1/0/all/0/1\">Jose Dolz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detection Accuracy for Evaluating Compositional Explanations of Units. (arXiv:2109.07804v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2109.07804","description":"<p>The recent success of deep learning models in solving complex problems and in\ndifferent domains has increased interest in understanding what they learn.\nTherefore, different approaches have been employed to explain these models, one\nof which uses human-understandable concepts as explanations. Two examples of\nmethods that use this approach are Network Dissection and Compositional\nexplanations. The former explains units using atomic concepts, while the latter\nmakes explanations more expressive, replacing atomic concepts with logical\nforms. While intuitively, logical forms are more informative than atomic\nconcepts, it is not clear how to quantify this improvement, and their\nevaluation is often based on the same metric that is optimized during the\nsearch-process and on the usage of hyper-parameters to be tuned. In this paper,\nwe propose to use as evaluation metric the Detection Accuracy, which measures\nunits' consistency of detection of their assigned explanations. We show that\nthis metric (1) evaluates explanations of different lengths effectively, (2)\ncan be used as a stopping criterion for the compositional explanation search,\neliminating the explanation length hyper-parameter, and (3) exposes new\nspecialized units whose length 1 explanations are the perceptual abstractions\nof their longer explanations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Makinwa_S/0/1/0/all/0/1\">Sayo M. Makinwa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosa_B/0/1/0/all/0/1\">Biagio La Rosa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Capobianco_R/0/1/0/all/0/1\">Roberto Capobianco</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BabelCalib: A Universal Approach to Calibrating Central Cameras. (arXiv:2109.09704v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.09704","description":"<p>Existing calibration methods occasionally fail for large field-of-view\ncameras due to the non-linearity of the underlying problem and the lack of good\ninitial values for all parameters of the used camera model. This might occur\nbecause a simpler projection model is assumed in an initial step, or a poor\ninitial guess for the internal parameters is pre-defined. A lot of the\ndifficulties of general camera calibration lie in the use of a forward\nprojection model. We side-step these challenges by first proposing a solver to\ncalibrate the parameters in terms of a back-projection model and then regress\nthe parameters for a target forward model. These steps are incorporated in a\nrobust estimation framework to cope with outlying detections. Extensive\nexperiments demonstrate that our approach is very reliable and returns the most\naccurate calibration parameters as measured on the downstream task of absolute\npose estimation on test sets. The code is released at\nhttps://github.com/ylochman/babelcalib.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lochman_Y/0/1/0/all/0/1\">Yaroslava Lochman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liepieshov_K/0/1/0/all/0/1\">Kostiantyn Liepieshov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jianhui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perdoch_M/0/1/0/all/0/1\">Michal Perdoch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zach_C/0/1/0/all/0/1\">Christopher Zach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pritts_J/0/1/0/all/0/1\">James Pritts</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Object Detection in Thermal Spectrum for Advanced Driver-Assistance Systems (ADAS). (arXiv:2109.09854v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.09854","description":"<p>Object detection in thermal infrared spectrum provides more reliable data\nsource in low-lighting conditions and different weather conditions, as it is\nuseful both in-cabin and outside for pedestrian, animal, and vehicular\ndetection as well as for detecting street-signs &amp; lighting poles. This paper is\nabout exploring and adapting state-of-the-art object detection and classifier\nframework on thermal vision with seven distinct classes for advanced\ndriver-assistance systems (ADAS). The trained network variants on public\ndatasets are validated on test data with three different test approaches which\ninclude test-time with no augmentation, test-time augmentation, and test-time\nwith model ensembling. Additionally, the efficacy of trained networks is tested\non locally gathered novel test-data captured with an uncooled LWIR prototype\nthermal camera in challenging weather and environmental scenarios. The\nperformance analysis of trained models is investigated by computing precision,\nrecall, and mean average precision scores (mAP). Furthermore, the trained model\narchitecture is optimized using TensorRT inference accelerator and deployed on\nresource-constrained edge hardware Nvidia Jetson Nano to explicitly reduce the\ninference time on GPU as well as edge devices for further real-time onboard\ninstallations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Farooq_M/0/1/0/all/0/1\">Muhammad Ali Farooq</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corcoran_P/0/1/0/all/0/1\">Peter Corcoran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rotariu_C/0/1/0/all/0/1\">Cosmin Rotariu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shariff_W/0/1/0/all/0/1\">Waseem Shariff</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Predict Trustworthiness with Steep Slope Loss. (arXiv:2110.00054v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.00054","description":"<p>Understanding the trustworthiness of a prediction yielded by a classifier is\ncritical for the safe and effective use of AI models. Prior efforts have been\nproven to be reliable on small-scale datasets. In this work, we study the\nproblem of predicting trustworthiness on real-world large-scale datasets, where\nthe task is more challenging due to high-dimensional features, diverse visual\nconcepts, and large-scale samples. In such a setting, we observe that the\ntrustworthiness predictors trained with prior-art loss functions, i.e., the\ncross entropy loss, focal loss, and true class probability confidence loss, are\nprone to view both correct predictions and incorrect predictions to be\ntrustworthy. The reasons are two-fold. Firstly, correct predictions are\ngenerally dominant over incorrect predictions. Secondly, due to the data\ncomplexity, it is challenging to differentiate the incorrect predictions from\nthe correct ones on real-world large-scale datasets. To improve the\ngeneralizability of trustworthiness predictors, we propose a novel steep slope\nloss to separate the features w.r.t. correct predictions from the ones w.r.t.\nincorrect predictions by two slide-like curves that oppose each other. The\nproposed loss is evaluated with two representative deep learning models, i.e.,\nVision Transformer and ResNet, as trustworthiness predictors. We conduct\ncomprehensive experiments and analyses on ImageNet, which show that the\nproposed loss effectively improves the generalizability of trustworthiness\npredictors. The code and pre-trained trustworthiness predictors for\nreproducibility are available at\nhttps://github.com/luoyan407/predict_trustworthiness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_Y/0/1/0/all/0/1\">Yongkang Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kankanhalli_M/0/1/0/all/0/1\">Mohan S. Kankanhalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1\">Qi Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Instance Segmentation with High-Resolution Automotive Radar. (arXiv:2110.01775v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.01775","description":"<p>Automotive radar has been widely used in the modern advanced driver\nassistance systems (ADAS) and autonomous driving system as it provides reliable\nenvironmental perception in all-weather conditions with affordable cost.\nHowever, automotive radar usually only plays as an auxiliary sensor since it\nhardly supplies semantic and geometry information due to the sparsity of radar\ndetection points. Nonetheless, as development of high-resolution automotive\nradar in recent years, more advanced perception functionality like instance\nsegmentation which has only been well explored using Lidar point clouds,\nbecomes possible by using automotive radar. Its data comes with rich contexts\nsuch as Radar Cross Section (RCS) and micro-doppler effects which may\npotentially be pertinent, and sometimes can even provide detection when the\nfield of view is completely obscured. Therefore, the effective utilization of\nradar detection points data is an integral part of automotive perception. The\noutcome from instance segmentation could be seen as comparable result of\nclustering, and could be potentially used as the input of tracker for tracking\nthe targets. In this paper, we propose two efficient methods for instance\nsegmentation with radar detection points, one is implemented in an end-to-end\ndeep learning driven fashion using PointNet++ framework, and the other is based\non clustering of the radar detection points with semantic information. Both\napproaches can be further improved by implementing visual multi-layer\nperceptron (MLP). The effectiveness of the proposed methods is verified using\nexperimental results on the recent RadarScenes dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jianan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_W/0/1/0/all/0/1\">Weiyi Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_L/0/1/0/all/0/1\">Liping Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1\">Yuxuan Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1\">Bing Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-center, multi-vendor automated segmentation of left ventricular anatomy in contrast-enhanced MRI. (arXiv:2110.07360v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2110.07360","description":"<p>Accurate delineation of the left ventricular boundaries in late\ngadolinium-enhanced magnetic resonance imaging (LGE-MRI) is an essential step\nfor scar tissue quantification and patient-specific assessment of myocardial\ninfarction. Many deep-learning techniques have been proposed to perform\nautomatic segmentations of the left ventricle (LV) in LGE-MRI showing\nsegmentations as accurate as those obtained by expert cardiologists. Thus far,\nthe existing models have been overwhelmingly developed and evaluated with\nLGE-MRI datasets from single clinical centers. However, in practice, LGE-MRI\nimages vary significantly between clinical centers within and across countries,\nin particular due to differences in the MRI scanners, imaging conditions,\ncontrast injection protocols and local clinical practise. This work\ninvestigates for the first time multi-center and multi-vendor LV segmentation\nin LGE-MRI, by proposing, implementing and evaluating in detail several\nstrategies to enhance model generalizability across clinical cites. These\ninclude data augmentation to artificially augment the image variability in the\ntraining sample, image harmonization to align the distributions of LGE-MRI\nimages across centers, and transfer learning to adjust existing single-center\nmodels to unseen images from new clinical sites. The results obtained based on\na new multi-center LGE-MRI dataset acquired in four clinical centers in Spain,\nFrance and China, show that the combination of data augmentation and transfer\nlearning can lead to single-center models that generalize well to new clinical\ncenters not included in the original training. The proposed framework shows the\npotential for developing clinical tools for automated LV segmentation in\nLGE-MRI that can be deployed in multiple clinical centers across distinct\ngeographical locations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Sendra_Balcells_C/0/1/0/all/0/1\">Carla Sendra-Balcells</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Campello_V/0/1/0/all/0/1\">V&#xed;ctor M. Campello</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Martin_Isla_C/0/1/0/all/0/1\">Carlos Mart&#xed;n-Isla</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Medel_D/0/1/0/all/0/1\">David Vilades Medel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Descalzo_M/0/1/0/all/0/1\">Mart&#xed;n Lu&#xed;s Descalzo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guala_A/0/1/0/all/0/1\">Andrea Guala</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Palomares_J/0/1/0/all/0/1\">Jos&#xe9; F. Rodr&#xed;guez Palomares</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lekadir_K/0/1/0/all/0/1\">Karim Lekadir</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EMDS-7: Environmental Microorganism Image Dataset Seventh Version for Multiple Object Detection Evaluation. (arXiv:2110.07723v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.07723","description":"<p>The Environmental Microorganism Image Dataset Seventh Version (EMDS-7) is a\nmicroscopic image data set, including the original Environmental Microorganism\nimages (EMs) and the corresponding object labeling files in \".XML\" format file.\nThe EMDS-7 data set consists of 41 types of EMs, which has a total of 2365\nimages and 13216 labeled objects. The EMDS-7 database mainly focuses on the\nobject detection. In order to prove the effectiveness of EMDS-7, we select the\nmost commonly used deep learning methods (Faster-RCNN, YOLOv3, YOLOv4, SSD and\nRetinaNet) and evaluation indices for testing and evaluation. EMDS-7 is freely\npublished for non-commercial purpose at:\nhttps://figshare.com/articles/dataset/EMDS-7_DataSet/16869571\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hechen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_B/0/1/0/all/0/1\">Bencheng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiawei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_P/0/1/0/all/0/1\">Pingli Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1\">Peng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1\">Ao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1\">Tao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Hongzan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teng_Y/0/1/0/all/0/1\">Yueyang Teng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_S/0/1/0/all/0/1\">Shouliang Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1\">Tao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grzegorzek_M/0/1/0/all/0/1\">Marcin Grzegorzek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EFENet: Reference-based Video Super-Resolution with Enhanced Flow Estimation. (arXiv:2110.07797v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.07797","description":"<p>In this paper, we consider the problem of reference-based video\nsuper-resolution(RefVSR), i.e., how to utilize a high-resolution (HR) reference\nframe to super-resolve a low-resolution (LR) video sequence. The existing\napproaches to RefVSR essentially attempt to align the reference and the input\nsequence, in the presence of resolution gap and long temporal range. However,\nthey either ignore temporal structure within the input sequence, or suffer\naccumulative alignment errors. To address these issues, we propose EFENet to\nexploit simultaneously the visual cues contained in the HR reference and the\ntemporal information contained in the LR sequence. EFENet first globally\nestimates cross-scale flow between the reference and each LR frame. Then our\nnovel flow refinement module of EFENet refines the flow regarding the furthest\nframe using all the estimated flows, which leverages the global temporal\ninformation within the sequence and therefore effectively reduces the alignment\nerrors. We provide comprehensive evaluations to validate the strengths of our\napproach, and to demonstrate that the proposed framework outperforms the\nstate-of-the-art methods. Code is available at\nhttps://github.com/IndigoPurple/EFENet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yaping Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_M/0/1/0/all/0/1\">Mengqi Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1\">Ruqi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shengjin Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mapping illegal waste dumping sites with neural-network classification of satellite imagery. (arXiv:2110.08599v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.08599","description":"<p>Public health and habitat quality are crucial goals of urban planning. In\nrecent years, the severe social and environmental impact of illegal waste\ndumping sites has made them one of the most serious problems faced by cities in\nthe Global South, in a context of scarce information available for decision\nmaking. To help identify the location of dumping sites and track their\nevolution over time we adopt a data-driven model from the machine learning\ndomain, analyzing satellite images. This allows us to take advantage of the\nincreasing availability of geo-spatial open-data, high-resolution satellite\nimagery, and open source tools to train machine learning algorithms with a\nsmall set of known waste dumping sites in Buenos Aires, and then predict the\nlocation of other sites over vast areas at high speed and low cost. This case\nstudy shows the results of a collaboration between Dymaxion Labs and\nFundaci\\'on Bunge y Born to harness this technique in order to create a\ncomprehensive map of potential locations of illegal waste dumping sites in the\nregion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Devesa_M/0/1/0/all/0/1\">Maria Roberta Devesa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brust_A/0/1/0/all/0/1\">Antonio Vazquez Brust</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AI-Based Detection, Classification and Prediction/Prognosis in Medical Imaging: Towards Radiophenomics. (arXiv:2110.10332v2 [physics.med-ph] UPDATED)","link":"http://arxiv.org/abs/2110.10332","description":"<p>Artificial intelligence (AI) techniques have significant potential to enable\neffective, robust and automated image phenotyping including identification of\nsubtle patterns. AI-based detection searches the image space to find the\nregions of interest based on patterns and features. There is a spectrum of\ntumor histologies from benign to malignant that can be identified by AI-based\nclassification approaches using image features. The extraction of minable\ninformation from images gives way to the field of radiomics and can be explored\nvia explicit (handcrafted/engineered) and deep radiomics frameworks. Radiomics\nanalysis has the potential to be utilized as a noninvasive technique for the\naccurate characterization of tumors to improve diagnosis and treatment\nmonitoring. This work reviews AI-based techniques, with a special focus on\noncological PET and PET/CT imaging, for different detection, classification,\nand prediction/prognosis tasks. We also discuss needed efforts to enable the\ntranslation of AI techniques to routine clinical workflows, and potential\nimprovements and complementary techniques such as the use of natural language\nprocessing on electronic health records and neuro-symbolic AI techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Yousefirizi_F/0/1/0/all/0/1\">Fereshteh Yousefirizi</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Decazes_P/0/1/0/all/0/1\">Pierre Decazes</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Amyar_A/0/1/0/all/0/1\">Amine Amyar</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Ruan_S/0/1/0/all/0/1\">Su Ruan</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Saboury_B/0/1/0/all/0/1\">Babak Saboury</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Rahmim_A/0/1/0/all/0/1\">Arman Rahmim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Illiterate DALL-E Learns to Compose. (arXiv:2110.11405v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.11405","description":"<p>Although DALL-E has shown an impressive ability of composition-based\nsystematic generalization in image generation, it requires the dataset of\ntext-image pairs and the compositionality is provided by the text. In contrast,\nobject-centric representation models like the Slot Attention model learn\ncomposable representations without the text prompt. However, unlike DALL-E its\nability to systematically generalize for zero-shot generation is significantly\nlimited. In this paper, we propose a simple but novel slot-based autoencoding\narchitecture, called SLATE, for combining the best of both worlds: learning\nobject-centric representations that allows systematic generalization in\nzero-shot image generation without text. As such, this model can also be seen\nas an illiterate DALL-E model. Unlike the pixel-mixture decoders of existing\nobject-centric representation models, we propose to use the Image GPT decoder\nconditioned on the slots for capturing complex interactions among the slots and\npixels. In experiments, we show that this simple and easy-to-implement\narchitecture not requiring a text prompt achieves significant improvement in\nin-distribution and out-of-distribution (zero-shot) image generation and\nqualitatively comparable or better slot-attention structure than the models\nbased on mixture decoders.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_G/0/1/0/all/0/1\">Gautam Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_F/0/1/0/all/0/1\">Fei Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahn_S/0/1/0/all/0/1\">Sungjin Ahn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploiting Cross-Modal Prediction and Relation Consistency for Semi-Supervised Image Captioning. (arXiv:2110.11767v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.11767","description":"<p>The task of image captioning aims to generate captions directly from images\nvia the automatically learned cross-modal generator. To build a well-performing\ngenerator, existing approaches usually need a large number of described images,\nwhich requires a huge effects on manual labeling. However, in real-world\napplications, a more general scenario is that we only have limited amount of\ndescribed images and a large number of undescribed images. Therefore, a\nresulting challenge is how to effectively combine the undescribed images into\nthe learning of cross-modal generator. To solve this problem, we propose a\nnovel image captioning method by exploiting the Cross-modal Prediction and\nRelation Consistency (CPRC), which aims to utilize the raw image input to\nconstrain the generated sentence in the commonly semantic space. In detail,\nconsidering that the heterogeneous gap between modalities always leads to the\nsupervision difficulty of using the global embedding directly, CPRC turns to\ntransform both the raw image and corresponding generated sentence into the\nshared semantic space, and measure the generated sentence from two aspects: 1)\nPrediction consistency. CPRC utilizes the prediction of raw image as soft label\nto distill useful supervision for the generated sentence, rather than employing\nthe traditional pseudo labeling; 2) Relation consistency. CPRC develops a novel\nrelation consistency between augmented images and corresponding generated\nsentences to retain the important relational knowledge. In result, CPRC\nsupervises the generated sentence from both the informativeness and\nrepresentativeness perspectives, and can reasonably use the undescribed images\nto learn a more effective generator under the semi-supervised scenario.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_H/0/1/0/all/0/1\">Hongchen Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hengshu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dianhai Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1\">Hui Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jian Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Accelerate 3D Object Processing via Spectral Layout. (arXiv:2110.12621v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.12621","description":"<p>3D image processing is an important problem in computer vision and pattern\nrecognition fields. Compared with 2D image processing, its computation\ndifficulty and cost are much higher due to the extra dimension. To\nfundamentally address this problem, we propose to embed the essential\ninformation in a 3D object into 2D space via spectral layout. Specifically, we\nconstruct a 3D adjacency graph to capture spatial structure of the 3D voxel\ngrid. Then we calculate the eigenvectors corresponding to the second and third\nsmallest eigenvalues of its graph Laplacian and perform spectral layout to map\neach voxel into a pixel in 2D Cartesian coordinate plane. The proposed method\ncan achieve high quality 2D representations for 3D objects, which enables to\nuse 2D-based methods to process 3D objects. The experimental results\ndemonstrate the effectiveness and efficiency of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongyu Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image Comes Dancing with Collaborative Parsing-Flow Video Synthesis. (arXiv:2110.14147v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.14147","description":"<p>Transferring human motion from a source to a target person poses great\npotential in computer vision and graphics applications. A crucial step is to\nmanipulate sequential future motion while retaining the appearance\ncharacteristic.Previous work has either relied on crafted 3D human models or\ntrained a separate model specifically for each target person, which is not\nscalable in practice.This work studies a more general setting, in which we aim\nto learn a single model to parsimoniously transfer motion from a source video\nto any target person given only one image of the person, named as Collaborative\nParsing-Flow Network (CPF-Net). The paucity of information regarding the target\nperson makes the task particularly challenging to faithfully preserve the\nappearance in varying designated poses. To address this issue, CPF-Net\nintegrates the structured human parsing and appearance flow to guide the\nrealistic foreground synthesis which is merged into the background by a\nspatio-temporal fusion module. In particular, CPF-Net decouples the problem\ninto stages of human parsing sequence generation, foreground sequence\ngeneration and final video generation. The human parsing generation stage\ncaptures both the pose and the body structure of the target. The appearance\nflow is beneficial to keep details in synthesized frames. The integration of\nhuman parsing and appearance flow effectively guides the generation of video\nframes with realistic appearance. Finally, the dedicated designed fusion\nnetwork ensure the temporal coherence. We further collect a large set of human\ndancing videos to push forward this research field. Both quantitative and\nqualitative results show our method substantially improves over previous\napproaches and is able to generate appealing and photo-realistic target videos\ngiven any input person image. All source code and dataset will be released at\nhttps://github.com/xiezhy6/CPF-Net.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Bowen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zhenyu Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yubei Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Haoye Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Liang Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Temporal-attentive Covariance Pooling Networks for Video Recognition. (arXiv:2110.14381v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.14381","description":"<p>For video recognition task, a global representation summarizing the whole\ncontents of the video snippets plays an important role for the final\nperformance. However, existing video architectures usually generate it by using\na simple, global average pooling (GAP) method, which has limited ability to\ncapture complex dynamics of videos. For image recognition task, there exist\nevidences showing that covariance pooling has stronger representation ability\nthan GAP. Unfortunately, such plain covariance pooling used in image\nrecognition is an orderless representative, which cannot model spatio-temporal\nstructure inherent in videos. Therefore, this paper proposes a\nTemporal-attentive Covariance Pooling(TCP), inserted at the end of deep\narchitectures, to produce powerful video representations. Specifically, our TCP\nfirst develops a temporal attention module to adaptively calibrate\nspatio-temporal features for the succeeding covariance pooling, approximatively\nproducing attentive covariance representations. Then, a temporal covariance\npooling performs temporal pooling of the attentive covariance representations\nto characterize both intra-frame correlations and inter-frame\ncross-correlations of the calibrated features. As such, the proposed TCP can\ncapture complex temporal dynamics. Finally, a fast matrix power normalization\nis introduced to exploit geometry of covariance representations. Note that our\nTCP is model-agnostic and can be flexibly integrated into any video\narchitectures, resulting in TCPNet for effective video recognition. The\nextensive experiments on six benchmarks (e.g., Kinetics, Something-Something V1\nand Charades) using various video architectures show our TCPNet is clearly\nsuperior to its counterparts, while having strong generalization ability. The\nsource code is publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1\">Zilin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qilong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bingbing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1\">Qinghua Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peihua Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Geometric Perspective towards Neural Calibration via Sensitivity Decomposition. (arXiv:2110.14577v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.14577","description":"<p>It is well known that vision classification models suffer from poor\ncalibration in the face of data distribution shifts. In this paper, we take a\ngeometric approach to this problem. We propose Geometric Sensitivity\nDecomposition (GSD) which decomposes the norm of a sample feature embedding and\nthe angular similarity to a target classifier into an instance-dependent and an\ninstance-independent component. The instance-dependent component captures the\nsensitive information about changes in the input while the instance-independent\ncomponent represents the insensitive information serving solely to minimize the\nloss on the training dataset. Inspired by the decomposition, we analytically\nderive a simple extension to current softmax-linear models, which learns to\ndisentangle the two components during training. On several common vision\nmodels, the disentangled model outperforms other calibration methods on\nstandard calibration metrics in the face of out-of-distribution (OOD) data and\ncorruption with significantly less complexity. Specifically, we surpass the\ncurrent state of the art by 30.8% relative improvement on corrupted CIFAR100 in\nExpected Calibration Error. Code available at\nhttps://github.com/GT-RIPL/Geometric-Sensitivity-Decomposition.git.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1\">Junjiao Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yung_D/0/1/0/all/0/1\">Dylan Yung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_Y/0/1/0/all/0/1\">Yen-Chang Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kira_Z/0/1/0/all/0/1\">Zsolt Kira</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Impact of lung segmentation on the diagnosis and explanation of COVID-19 in chest X-ray images. (arXiv:2009.09780v4 [eess.IV] CROSS LISTED)","link":"http://arxiv.org/abs/2009.09780","description":"<p>COVID-19 frequently provokes pneumonia, which can be diagnosed using imaging\nexams. Chest X-ray (CXR) is often useful because it is cheap, fast, widespread,\nand uses less radiation. Here, we demonstrate the impact of lung segmentation\nin COVID-19 identification using CXR images and evaluate which contents of the\nimage influenced the most. Semantic segmentation was performed using a U-Net\nCNN architecture, and the classification using three CNN architectures (VGG,\nResNet, and Inception). Explainable Artificial Intelligence techniques were\nemployed to estimate the impact of segmentation. A three-classes database was\ncomposed: lung opacity (pneumonia), COVID-19, and normal. We assessed the\nimpact of creating a CXR image database from different sources, and the\nCOVID-19 generalization from one source to another. The segmentation achieved a\nJaccard distance of 0.034 and a Dice coefficient of 0.982. The classification\nusing segmented images achieved an F1-Score of 0.88 for the multi-class setup,\nand 0.83 for COVID-19 identification. In the cross-dataset scenario, we\nobtained an F1-Score of 0.74 and an area under the ROC curve of 0.9 for\nCOVID-19 identification using segmented images. Experiments support the\nconclusion that even after segmentation, there is a strong bias introduced by\nunderlying factors from different sources.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Teixeira_L/0/1/0/all/0/1\">Lucas O. Teixeira</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pereira_R/0/1/0/all/0/1\">Rodolfo M. Pereira</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bertolini_D/0/1/0/all/0/1\">Diego Bertolini</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Oliveira_L/0/1/0/all/0/1\">Luiz S. Oliveira</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nanni_L/0/1/0/all/0/1\">Loris Nanni</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cavalcanti_G/0/1/0/all/0/1\">George D. C. Cavalcanti</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Costa_Y/0/1/0/all/0/1\">Yandre M. G. Costa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-10-28T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/"}}]}]}