{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-03-10T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Which side are you on? Insider-Outsider classification in conspiracy-theoretic social media. (arXiv:2203.04356v1 [cs.CL])","link":"http://arxiv.org/abs/2203.04356","description":"<p>Social media is a breeding ground for threat narratives and related\nconspiracy theories. In these, an outside group threatens the integrity of an\ninside group, leading to the emergence of sharply defined group identities:\nInsiders -- agents with whom the authors identify and Outsiders -- agents who\nthreaten the insiders. Inferring the members of these groups constitutes a\nchallenging new NLP task: (i) Information is distributed over many\npoorly-constructed posts; (ii) Threats and threat agents are highly contextual,\nwith the same post potentially having multiple agents assigned to membership in\neither group; (iii) An agent's identity is often implicit and transitive; and\n(iv) Phrases used to imply Outsider status often do not follow common negative\nsentiment patterns. To address these challenges, we define a novel\nInsider-Outsider classification task. Because we are not aware of any\nappropriate existing datasets or attendant models, we introduce a labeled\ndataset (CT5K) and design a model (NP2IO) to address this task. NP2IO leverages\npretrained language modeling to classify Insiders and Outsiders. NP2IO is shown\nto be robust, generalizing to noun phrases not seen during training, and\nexceeding the performance of non-trivial baseline models by $20\\%$.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Holur_P/0/1/0/all/0/1\">Pavan Holur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahsavari_S/0/1/0/all/0/1\">Shadi Shahsavari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tangherlini_T/0/1/0/all/0/1\">Timothy Tangherlini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roychowdhury_V/0/1/0/all/0/1\">Vwani Roychowdhury</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"It's AI Match: A Two-Step Approach for Schema Matching Using Embeddings. (arXiv:2203.04366v1 [cs.DB])","link":"http://arxiv.org/abs/2203.04366","description":"<p>Since data is often stored in different sources, it needs to be integrated to\ngather a global view that is required in order to create value and derive\nknowledge from it. A critical step in data integration is schema matching which\naims to find semantic correspondences between elements of two schemata. In\norder to reduce the manual effort involved in schema matching, many solutions\nfor the automatic determination of schema correspondences have already been\ndeveloped.\n</p>\n<p>In this paper, we propose a novel end-to-end approach for schema matching\nbased on neural embeddings. The main idea is to use a two-step approach\nconsisting of a table matching step followed by an attribute matching step. In\nboth steps we use embeddings on different levels either representing the whole\ntable or single attributes. Our results show that our approach is able to\ndetermine correspondences in a robust and reliable way and compared to\ntraditional schema matching approaches can find non-trivial correspondences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hattasch_B/0/1/0/all/0/1\">Benjamin H&#xe4;ttasch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Truong_Ngoc_M/0/1/0/all/0/1\">Michael Truong-Ngoc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_A/0/1/0/all/0/1\">Andreas Schmidt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Binnig_C/0/1/0/all/0/1\">Carsten Binnig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"iSEA: An Interactive Pipeline for Semantic Error Analysis of NLP Models. (arXiv:2203.04408v1 [cs.HC])","link":"http://arxiv.org/abs/2203.04408","description":"<p>Error analysis in NLP models is essential to successful model development and\ndeployment. One common approach for diagnosing errors is to identify\nsubpopulations in the dataset where the model produces the most errors.\nHowever, existing approaches typically define subpopulations based on\npre-defined features, which requires users to form hypotheses of errors in\nadvance. To complement these approaches, we propose iSEA, an Interactive\nPipeline for Semantic Error Analysis in NLP Models, which automatically\ndiscovers semantically-grounded subpopulations with high error rates in the\ncontext of a human-in-the-loop interactive system. iSEA enables model\ndevelopers to learn more about their model errors through discovered\nsubpopulations, validate the sources of errors through interactive analysis on\nthe discovered subpopulations, and test hypotheses about model errors by\ndefining custom subpopulations. The tool supports semantic descriptions of\nerror-prone subpopulations at the token and concept level, as well as\npre-defined higher-level features. Through use cases and expert interviews, we\ndemonstrate how iSEA can assist error understanding and analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Jun Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vig_J/0/1/0/all/0/1\">Jesse Vig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajani_N/0/1/0/all/0/1\">Nazneen Rajani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Estimating the Uncertainty in Emotion Class Labels with Utterance-Specific Dirichlet Priors. (arXiv:2203.04443v1 [cs.CL])","link":"http://arxiv.org/abs/2203.04443","description":"<p>Emotion recognition is a key attribute for artificial intelligence systems\nthat need to naturally interact with humans. However, the task definition is\nstill an open problem due to inherent ambiguity of emotions. In this paper, a\nnovel Bayesian training loss based on per-utterance Dirichlet prior\ndistributions is proposed for verbal emotion recognition, which models the\nuncertainty in one-hot labels created when human annotators assign the same\nutterance to different emotion classes. An additional metric is used to\nevaluate the performance by detecting test utterances with high labelling\nuncertainty. This removes a major limitation that emotion classification\nsystems only consider utterances with majority labels.Furthermore, a\nfrequentist approach is studied to leverage the continuous-valued \"soft\" labels\nobtained by averaging the one-hot labels. We propose a two-branch model\nstructure for emotion classification on a per-utterance basis. Experiments with\nthe widely used IEMOCAP dataset demonstrate that the two-branch structure\nachieves state-of-the-art classification results with all common IEMOCAP test\nsetups. Based on this, uncertainty estimation experiments were performed. The\nbest performance in terms of the area under the precision-recall curve when\ndetecting utterances with high uncertainty was achieved by interpolating the\nBayesian training loss with the Kullback-Leibler divergence training loss for\nthe soft labels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xixin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woodland_P/0/1/0/all/0/1\">Philip C. Woodland</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contextual Networks and Unsupervised Ranking of Sentences. (arXiv:2203.04459v1 [cs.CL])","link":"http://arxiv.org/abs/2203.04459","description":"<p>We construct a contextual network to represent a document with syntactic and\nsemantic relations between word-sentence pairs, based on which we devise an\nunsupervised algorithm called CNATAR (Contextual Network And Text Analysis\nRank) to score sentences, and rank them through a bi-objective 0-1 knapsack\nmaximization problem over topic analysis and sentence scores. We show that\nCNATAR outperforms the combined ranking of the three human judges provided on\nthe SummBank dataset under both ROUGE and BLEU metrics, which in term\nsignificantly outperforms each individual judge's ranking. Moreover, CNATAR\nproduces so far the highest ROUGE scores over DUC-02, and outperforms previous\nsupervised algorithms on the CNN/DailyMail and NYT datasets. We also compare\nthe performance of CNATAR and the latest supervised neural-network\nsummarization models and compute oracle results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">You Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jie Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Evaluation of Answer-Agnostic Paragraph-level Multi-Question Generation. (arXiv:2203.04464v1 [cs.CL])","link":"http://arxiv.org/abs/2203.04464","description":"<p>We study the task of predicting a set of salient questions from a given\nparagraph without any prior knowledge of the precise answer. We make two main\ncontributions. First, we propose a new method to evaluate a set of predicted\nquestions against the set of references by using the Hungarian algorithm to\nassign predicted questions to references before scoring the assigned pairs. We\nshow that our proposed evaluation strategy has better theoretical and practical\nproperties compared to prior methods because it can properly account for the\ncoverage of references. Second, we compare different strategies to utilize a\npre-trained seq2seq model to generate and select a set of questions related to\na given paragraph. The code is available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_J/0/1/0/all/0/1\">Jishnu Ray Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahata_D/0/1/0/all/0/1\">Debanjan Mahata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caragea_C/0/1/0/all/0/1\">Cornelia Caragea</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Boilerplate Detection via Semantic Classification of TextBlocks. (arXiv:2203.04467v1 [cs.CL])","link":"http://arxiv.org/abs/2203.04467","description":"<p>We present a hierarchical neural network model called SemText to detect HTML\nboilerplate based on a novel semantic representation of HTML tags, class names,\nand text blocks. We train SemText on three published datasets of news webpages\nand fine-tune it using a small number of development data in CleanEval and\nGoogleTrends-2017. We show that SemText achieves the state-of-the-art accuracy\non these datasets. We then demonstrate the robustness of SemText by showing\nthat it also detects boilerplate effectively on out-of-domain community-based\nquestion-answer webpages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jie Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Onception: Active Learning with Expert Advice for Real World Machine Translation. (arXiv:2203.04507v1 [cs.CL])","link":"http://arxiv.org/abs/2203.04507","description":"<p>Active learning can play an important role in low-resource settings (i.e.,\nwhere annotated data is scarce), by selecting which instances may be more\nworthy to annotate. Most active learning approaches for Machine Translation\nassume the existence of a pool of sentences in a source language, and rely on\nhuman annotators to provide translations or post-edits, which can still be\ncostly. In this paper, we assume a real world human-in-the-loop scenario in\nwhich: (i) the source sentences may not be readily available, but instead\narrive in a stream; (ii) the automatic translations receive feedback in the\nform of a rating, instead of a correct/edited translation, since the\nhuman-in-the-loop might be a user looking for a translation, but not be able to\nprovide one. To tackle the challenge of deciding whether each incoming pair\nsource-translations is worthy to query for human feedback, we resort to a\nnumber of stream-based active learning query strategies. Moreover, since we not\nknow in advance which query strategy will be the most adequate for a certain\nlanguage pair and set of Machine Translation models, we propose to dynamically\ncombine multiple strategies using prediction with expert advice. Our\nexperiments show that using active learning allows to converge to the best\nMachine Translation systems with fewer human interactions. Furthermore,\ncombining multiple strategies using prediction with expert advice often\noutperforms several individual active learning strategies with even fewer\ninteractions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mendonca_V/0/1/0/all/0/1\">V&#xe2;nia Mendon&#xe7;a</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Rei_R/0/1/0/all/0/1\">Ricardo Rei</a> (1 and 2 and 3), <a href=\"http://arxiv.org/find/cs/1/au:+Coheur_L/0/1/0/all/0/1\">Luisa Coheur</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Sardinha_A/0/1/0/all/0/1\">Alberto Sardinha</a> (1 and 2) ((1) INESC-ID Lisboa, (2) Instituto Superior T&#xe9;cnico, (3) Unbabel AI)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mapping global dynamics of benchmark creation and saturation in artificial intelligence. (arXiv:2203.04592v1 [cs.AI])","link":"http://arxiv.org/abs/2203.04592","description":"<p>Benchmarks are crucial to measuring and steering progress in artificial\nintelligence (AI). However, recent studies raised concerns over the state of AI\nbenchmarking, reporting issues such as benchmark overfitting, benchmark\nsaturation and increasing centralization of benchmark dataset creation. To\nfacilitate monitoring of the health of the AI benchmarking ecosystem, we\nintroduce methodologies for creating condensed maps of the global dynamics of\nbenchmark creation and saturation. We curated data for 1688 benchmarks covering\nthe entire domains of computer vision and natural language processing, and show\nthat a large fraction of benchmarks quickly trended towards near-saturation,\nthat many benchmarks fail to find widespread utilization, and that benchmark\nperformance gains for different AI tasks were prone to unforeseen bursts. We\nconclude that future work should focus on large-scale community collaboration\nand on mapping benchmark performance gains to real-world utility and impact of\nAI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Barbosa_Silva_A/0/1/0/all/0/1\">Adriano Barbosa-Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ott_S/0/1/0/all/0/1\">Simon Ott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blagec_K/0/1/0/all/0/1\">Kathrin Blagec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brauner_J/0/1/0/all/0/1\">Jan Brauner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samwald_M/0/1/0/all/0/1\">Matthias Samwald</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PALI-NLP at SemEval-2022 Task 4: Discriminative Fine-tuning of Deep Transformers for Patronizing and Condescending Language Detection. (arXiv:2203.04616v1 [cs.CL])","link":"http://arxiv.org/abs/2203.04616","description":"<p>Patronizing and condescending language (PCL) has a large harmful impact and\nis difficult to detect, both for human judges and existing NLP systems. At\nSemEval-2022 Task 4, we propose a novel Transformer-based model and its\nensembles to accurately understand such language context for PCL detection. To\nfacilitate comprehension of the subtle and subjective nature of PCL, two\nfine-tuning strategies are applied to capture discriminative features from\ndiverse linguistic behaviour and categorical distribution. The system achieves\nremarkable results on the official ranking, namely 1st in Subtask 1 and 5th in\nSubtask 2. Extensive experiments on the task demonstrate the effectiveness of\nour system and its strategies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1\">Dou Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Mengyuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1\">Xiyang Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_M/0/1/0/all/0/1\">Mengfei Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_M/0/1/0/all/0/1\">Meizhi Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Lianxin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mo_Y/0/1/0/all/0/1\">Yang Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xiaofeng Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LEBP -- Language Expectation & Binding Policy: A Two-Stream Framework for Embodied Vision-and-Language Interaction Task Learning Agents. (arXiv:2203.04637v1 [cs.AI])","link":"http://arxiv.org/abs/2203.04637","description":"<p>People always desire an embodied agent that can perform a task by\nunderstanding language instruction. Moreover, they also want to monitor and\nexpect agents to understand commands the way they expected. But, how to build\nsuch an embodied agent is still unclear. Recently, people can explore this\nproblem with the Vision-and-Language Interaction benchmark ALFRED, which\nrequires an agent to perform complicated daily household tasks following\nnatural language instructions in unseen scenes. In this paper, we propose LEBP\n-- Language Expectation and Binding Policy Module to tackle the ALFRED. The\nLEBP contains a two-stream process: 1) It first conducts a language expectation\nmodule to generate an expectation describing how to perform tasks by\nunderstanding the language instruction. The expectation consists of a sequence\nof sub-steps for the task (e.g., Pick an apple). The expectation allows people\nto access and check the understanding results of instructions before the agent\ntakes actual actions, in case the task might go wrong. 2) Then, it uses the\nbinding policy module to bind sub-steps in expectation to actual actions to\nspecific scenarios. Actual actions include navigation and object manipulation.\nExperimental results suggest our approach achieves comparable performance to\ncurrently published SOTA methods and can avoid large decay from seen scenarios\nto unseen scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Haoyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Hongkai He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hangfang Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Memory Efficient Continual Learning for Neural Text Classification. (arXiv:2203.04640v1 [cs.CL])","link":"http://arxiv.org/abs/2203.04640","description":"<p>Learning text classifiers based on pre-trained language models has become the\nstandard practice in natural language processing applications. Unfortunately,\ntraining large neural language models, such as transformers, from scratch is\nvery costly and requires a vast amount of training data, which might not be\navailable in the application domain of interest. Moreover, in many real-world\nscenarios, classes are uncovered as more data is seen, calling for\nclass-incremental modelling approaches. In this work we devise a method to\nperform text classification using pre-trained models on a sequence of\nclassification tasks provided in sequence. We formalize the problem as a\ncontinual learning problem where the algorithm learns new tasks without\nperformance degradation on the previous ones and without re-training the model\nfrom scratch. We empirically demonstrate that our method requires significantly\nless model parameters compared to other state of the art methods and that it is\nsignificantly faster at inference time. The tight control on the number of\nmodel parameters, and so the memory, is not only improving efficiency. It is\nmaking possible the usage of the algorithm in real-world applications where\ndeploying a solution with a constantly increasing memory consumption is just\nunrealistic. While our method suffers little forgetting, it retains a\npredictive performance on-par with state of the art but less memory efficient\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ermis_B/0/1/0/all/0/1\">Beyza Ermis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zappella_G/0/1/0/all/0/1\">Giovanni Zappella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wistuba_M/0/1/0/all/0/1\">Martin Wistuba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Archambeau_C/0/1/0/all/0/1\">Cedric Archambeau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Slangvolution: A Causal Analysis of Semantic Change and Frequency Dynamics in Slang. (arXiv:2203.04651v1 [cs.CL])","link":"http://arxiv.org/abs/2203.04651","description":"<p>Languages are continuously undergoing changes, and the mechanisms that\nunderlie these changes are still a matter of debate. In this work, we approach\nlanguage evolution through the lens of causality in order to model not only how\nvarious distributional factors associate with language change, but how they\ncausally affect it. In particular, we study slang, which is an informal\nlanguage that is typically restricted to a specific group or social setting. We\nanalyze the semantic change and frequency shift of slang words and compare them\nto those of standard, nonslang words. With causal discovery and causal\ninference techniques, we measure the effect that word type (slang/nonslang) has\non both semantic change and frequency shift, as well as its relationship to\nfrequency, polysemy and part of speech. Our analysis provides some new insights\nin the study of semantic change, e.g., we show that slang words undergo less\nsemantic change but tend to have larger frequency shifts over time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Keidar_D/0/1/0/all/0/1\">Daphna Keidar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Opedal_A/0/1/0/all/0/1\">Andreas Opedal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhijing Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ASET: Ad-hoc Structured Exploration of Text Collections [Extended Abstract]. (arXiv:2203.04663v1 [cs.CL])","link":"http://arxiv.org/abs/2203.04663","description":"<p>In this paper, we propose a new system called ASET that allows users to\nperform structured explorations of text collections in an ad-hoc manner. The\nmain idea of ASET is to use a new two-phase approach that first extracts a\nsuperset of information nuggets from the texts using existing extractors such\nas named entity recognizers and then matches the extractions to a structured\ntable definition as requested by the user based on embeddings. In our\nevaluation, we show that ASET is thus able to extract structured data from\nreal-world text collections in high quality without the need to design\nextraction pipelines upfront.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hattasch_B/0/1/0/all/0/1\">Benjamin H&#xe4;ttasch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bodensohn_J/0/1/0/all/0/1\">Jan-Micha Bodensohn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Binnig_C/0/1/0/all/0/1\">Carsten Binnig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Nested Named Entity Recognition as Latent Lexicalized Constituency Parsing. (arXiv:2203.04665v1 [cs.CL])","link":"http://arxiv.org/abs/2203.04665","description":"<p>Nested named entity recognition (NER) has been receiving increasing\nattention. Recently, (Fu et al, 2021) adapt a span-based constituency parser to\ntackle nested NER. They treat nested entities as partially-observed\nconstituency trees and propose the masked inside algorithm for partial\nmarginalization. However, their method cannot leverage entity heads, which have\nbeen shown useful in entity mention detection and entity typing. In this work,\nwe resort to more expressive structures, lexicalized constituency trees in\nwhich constituents are annotated by headwords, to model nested entities. We\nleverage the Eisner-Satta algorithm to perform partial marginalization and\ninference efficiently. In addition, we propose to use (1) a two-stage strategy\n(2) a head regularization loss and (3) a head-aware labeling loss in order to\nenhance the performance. We make a thorough ablation study to investigate the\nfunctionality of each component. Experimentally, our method achieves the\nstate-of-the-art performance on ACE2004, ACE2005 and NNE, and competitive\nperformance on GENIA, and meanwhile has a fast inference speed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lou_C/0/1/0/all/0/1\">Chao Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Songlin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_K/0/1/0/all/0/1\">Kewei Tu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Diversity: Visible to Humans, Exploitable by Machines. (arXiv:2203.04723v1 [cs.CL])","link":"http://arxiv.org/abs/2203.04723","description":"<p>The Universal Knowledge Core (UKC) is a large multilingual lexical database\nwith a focus on language diversity and covering over a thousand languages. The\naim of the database, as well as its tools and data catalogue, is to make the\nsomewhat abstract notion of diversity visually understandable for humans and\nformally exploitable by machines. The UKC website lets users explore millions\nof individual words and their meanings, but also phenomena of cross-lingual\nconvergence and divergence, such as shared interlingual meanings, lexicon\nsimilarities, cognate clusters, or lexical gaps. The UKC LiveLanguage\nCatalogue, in turn, provides access to the underlying lexical data in a\ncomputer-processable form, ready to be reused in cross-lingual applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bella_G/0/1/0/all/0/1\">G&#xe1;bor Bella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Byambadorj_E/0/1/0/all/0/1\">Erdenebileg Byambadorj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandrashekar_Y/0/1/0/all/0/1\">Yamini Chandrashekar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batsuren_K/0/1/0/all/0/1\">Khuyagbaatar Batsuren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheema_D/0/1/0/all/0/1\">Danish Ashgar Cheema</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giunchiglia_F/0/1/0/all/0/1\">Fausto Giunchiglia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pretrained Domain-Specific Language Model for General Information Retrieval Tasks in the AEC Domain. (arXiv:2203.04729v1 [cs.CL])","link":"http://arxiv.org/abs/2203.04729","description":"<p>As an essential task for the architecture, engineering, and construction\n(AEC) industry, information retrieval (IR) from unstructured textual data based\non natural language processing (NLP) is gaining increasing attention. Although\nvarious deep learning (DL) models for IR tasks have been investigated in the\nAEC domain, it is still unclear how domain corpora and domain-specific\npretrained DL models can improve performance in various IR tasks. To this end,\nthis work systematically explores the impacts of domain corpora and various\ntransfer learning techniques on the performance of DL models for IR tasks and\nproposes a pretrained domain-specific language model for the AEC domain. First,\nboth in-domain and close-domain corpora are developed. Then, two types of\npretrained models, including traditional wording embedding models and\nBERT-based models, are pretrained based on various domain corpora and transfer\nlearning strategies. Finally, several widely used DL models for IR tasks are\nfurther trained and tested based on various configurations and pretrained\nmodels. The result shows that domain corpora have opposite effects on\ntraditional word embedding models for text classification and named entity\nrecognition tasks but can further improve the performance of BERT-based models\nin all tasks. Meanwhile, BERT-based models dramatically outperform traditional\nmethods in all IR tasks, with maximum improvements of 5.4% and 10.1% in the F1\nscore, respectively. This research contributes to the body of knowledge in two\nways: 1) demonstrating the advantages of domain corpora and pretrained DL\nmodels and 2) opening the first domain-specific dataset and pretrained language\nmodel for the AEC domain, to the best of our knowledge. Thus, this work sheds\nlight on the adoption and application of pretrained models in the AEC domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zhe Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xin-Zheng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Ke-Yin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yu-Cheng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jia-Rui Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhance Topics Analysis based on Keywords Properties. (arXiv:2203.04786v1 [cs.IR])","link":"http://arxiv.org/abs/2203.04786","description":"<p>Topic Modelling is one of the most prevalent text analysis technique used to\nexplore and retrieve collection of documents. The evaluation of the topic model\nalgorithms is still a very challenging tasks due to the absence of\ngold-standard list of topics to compare against for every corpus. In this work,\nwe present a specificity score based on keywords properties that is able to\nselect the most informative topics. This approach helps the user to focus on\nthe most informative topics. In the experiments, we show that we are able to\ncompress the state-of-the-art topic modelling results of different factors with\nan information loss that is much lower than the solution based on the recent\ncoherence score presented in literature.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Penta_A/0/1/0/all/0/1\">Antonio Penta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"One-Shot Learning from a Demonstration with Hierarchical Latent Language. (arXiv:2203.04806v1 [cs.CL])","link":"http://arxiv.org/abs/2203.04806","description":"<p>Humans have the capability, aided by the expressive compositionality of their\nlanguage, to learn quickly by demonstration. They are able to describe unseen\ntask-performing procedures and generalize their execution to other contexts. In\nthis work, we introduce DescribeWorld, an environment designed to test this\nsort of generalization skill in grounded agents, where tasks are linguistically\nand procedurally composed of elementary concepts. The agent observes a single\ntask demonstration in a Minecraft-like grid world, and is then asked to carry\nout the same task in a new map. To enable such a level of generalization, we\npropose a neural agent infused with hierarchical latent language--both at the\nlevel of task inference and subtask planning. Our agent first generates a\ntextual description of the demonstrated unseen task, then leverages this\ndescription to replicate it. Through multiple evaluation scenarios and a suite\nof generalization tests, we find that agents that perform text-based inference\nare better equipped for the challenge under a random split of tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Weir_N/0/1/0/all/0/1\">Nathaniel Weir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xingdi Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cote_M/0/1/0/all/0/1\">Marc-Alexandre C&#xf4;t&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hausknecht_M/0/1/0/all/0/1\">Matthew Hausknecht</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laroche_R/0/1/0/all/0/1\">Romain Laroche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Momennejad_I/0/1/0/all/0/1\">Ida Momennejad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seijen_H/0/1/0/all/0/1\">Harm Van Seijen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durme_B/0/1/0/all/0/1\">Benjamin Van Durme</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Sub-structured Knowledge Distillation. (arXiv:2203.04825v1 [cs.LG])","link":"http://arxiv.org/abs/2203.04825","description":"<p>Structured prediction models aim at solving a type of problem where the\noutput is a complex structure, rather than a single variable. Performing\nknowledge distillation for such models is not trivial due to their\nexponentially large output space. In this work, we propose an approach that is\nmuch simpler in its formulation and far more efficient for training than\nexisting approaches. Specifically, we transfer the knowledge from a teacher\nmodel to its student model by locally matching their predictions on all\nsub-structures, instead of the whole output space. In this manner, we avoid\nadopting some time-consuming techniques like dynamic programming (DP) for\ndecoding output structures, which permits parallel computation and makes the\ntraining process even faster in practice. Besides, it encourages the student\nmodel to better mimic the internal behavior of the teacher model. Experiments\non two structured prediction tasks demonstrate that our approach outperforms\nprevious methods and halves the time cost for one training epoch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Wenye Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yangming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lemao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shuming Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Hai-tao Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Language Identification for Celtic Texts. (arXiv:2203.04831v1 [cs.CL])","link":"http://arxiv.org/abs/2203.04831","description":"<p>Language identification is an important Natural Language Processing task. It\nhas been thoroughly researched in the literature. However, some issues are\nstill open. This work addresses the identification of the related low-resource\nlanguages on the example of the Celtic language family.\n</p>\n<p>This work's main goals were: (1) to collect the dataset of three Celtic\nlanguages; (2) to prepare a method to identify the languages from the Celtic\nfamily, i.e. to train a successful classification model; (3) to evaluate the\ninfluence of different feature extraction methods, and explore the\napplicability of the unsupervised models as a feature extraction technique; (4)\nto experiment with the unsupervised feature extraction on a reduced annotated\nset.\n</p>\n<p>We collected a new dataset including Irish, Scottish, Welsh and English\nrecords. We tested supervised models such as SVM and neural networks with\ntraditional statistical features alongside the output of clustering,\nautoencoder, and topic modelling methods. The analysis showed that the\nunsupervised features could serve as a valuable extension to the n-gram feature\nvectors. It led to an improvement in performance for more entangled classes.\nThe best model achieved a 98\\% F1 score and 97\\% MCC. The dense neural network\nconsistently outperformed the SVM model.\n</p>\n<p>The low-resource languages are also challenging due to the scarcity of\navailable annotated training data. This work evaluated the performance of the\nclassifiers using the unsupervised feature extraction on the reduced labelled\ndataset to handle this issue. The results uncovered that the unsupervised\nfeature vectors are more robust to the labelled set reduction. Therefore, they\nproved to help achieve comparable classification performance with much less\nlabelled data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dovbnia_O/0/1/0/all/0/1\">Olha Dovbnia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wroblewska_A/0/1/0/all/0/1\">Anna Wr&#xf3;blewska</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"'Beach' to 'Bitch': Inadvertent Unsafe Transcription of Kids' Content on YouTube. (arXiv:2203.04837v1 [eess.AS])","link":"http://arxiv.org/abs/2203.04837","description":"<p>Over the last few years, YouTube Kids has emerged as one of the highly\ncompetitive alternatives to television for children's entertainment.\nConsequently, YouTube Kids' content should receive an additional level of\nscrutiny to ensure children's safety. While research on detecting offensive or\ninappropriate content for kids is gaining momentum, little or no current work\nexists that investigates to what extent AI applications can (accidentally)\nintroduce content that is inappropriate for kids.\n</p>\n<p>In this paper, we present a novel (and troubling) finding that well-known\nautomatic speech recognition (ASR) systems may produce text content highly\ninappropriate for kids while transcribing YouTube Kids' videos. We dub this\nphenomenon as \\emph{inappropriate content hallucination}. Our analyses suggest\nthat such hallucinations are far from occasional, and the ASR systems often\nproduce them with high confidence. We release a first-of-its-kind data set of\naudios for which the existing state-of-the-art ASR systems hallucinate\ninappropriate content for kids. In addition, we demonstrate that some of these\nerrors can be fixed using language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ramesh_K/0/1/0/all/0/1\">Krithika Ramesh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+KhudaBukhsh_A/0/1/0/all/0/1\">Ashiqur R. KhudaBukhsh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kumar_S/0/1/0/all/0/1\">Sumeet Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neuro-symbolic Natural Logic with Introspective Revision for Natural Language Inference. (arXiv:2203.04857v1 [cs.CL])","link":"http://arxiv.org/abs/2203.04857","description":"<p>We introduce a neuro-symbolic natural logic framework based on reinforcement\nlearning with introspective revision. The model samples and rewards specific\nreasoning paths through policy gradient, in which the introspective revision\nalgorithm modifies intermediate symbolic reasoning steps to discover\nreward-earning operations as well as leverages external knowledge to alleviate\nspurious reasoning and training inefficiency. The framework is supported by\nproperly designed local relation models to avoid input entangling, which helps\nensure the interpretability of the proof paths. The proposed model has built-in\ninterpretability and shows superior capability in monotonicity inference,\nsystematic generalization, and interpretability, compared to previous models on\nthe existing datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yufei Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaoyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaodan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Greenspan_M/0/1/0/all/0/1\">Michael Greenspan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PET: A new Dataset for Process Extraction from Natural Language Text. (arXiv:2203.04860v1 [cs.CL])","link":"http://arxiv.org/abs/2203.04860","description":"<p>Although there is a long tradition of work in NLP on extracting entities and\nrelations from text, to date there exists little work on the acquisition of\nbusiness processes from unstructured data such as textual corpora of process\ndescriptions. With this work we aim at filling this gap and establishing the\nfirst steps towards bridging data-driven information extraction methodologies\nfrom Natural Language Processing and the model-based formalization that is\naimed from Business Process Management. For this, we develop the first corpus\nof business process descriptions annotated with activities, gateways, actors\nand flow information. We present our new resource, including a detailed\noverview of the annotation schema and guidelines, as well as a variety of\nbaselines to benchmark the difficulty and challenges of business process\nextraction from text.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bellan_P/0/1/0/all/0/1\">Patrizio Bellan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aa_H/0/1/0/all/0/1\">Han van der Aa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dragoni_M/0/1/0/all/0/1\">Mauro Dragoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghidini_C/0/1/0/all/0/1\">Chiara Ghidini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ponzetto_S/0/1/0/all/0/1\">Simone Paolo Ponzetto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Alignment of Distributional Word Embeddings. (arXiv:2203.04863v1 [cs.CL])","link":"http://arxiv.org/abs/2203.04863","description":"<p>Cross-domain alignment play a key roles in tasks ranging from machine\ntranslation to transfer learning. Recently, purely unsupervised methods\noperating on monolingual embeddings have successfully been used to infer a\nbilingual lexicon without relying on supervision. However, current state-of-the\nart methods only focus on point vectors although distributional embeddings have\nproven to embed richer semantic information when representing words. In this\npaper, we propose stochastic optimization approach for aligning probabilistic\nembeddings. Finally, we evaluate our method on the problem of unsupervised word\ntranslation, by aligning word embeddings trained on monolingual data. We show\nthat the proposed approach achieves good performance on the bilingual lexicon\ninduction task across several language pairs and performs better than the\npoint-vector based approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Diallo_A/0/1/0/all/0/1\">Aissatou Diallo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Model-Agnostic Multitask Fine-tuning for Few-shot Vision-Language Transfer Learning. (arXiv:2203.04904v1 [cs.MM])","link":"http://arxiv.org/abs/2203.04904","description":"<p>Despite achieving state-of-the-art zero-shot performance, existing\nvision-language models, e.g., CLIP, still fall short of domain-specific\nclassification tasks, e.g., Fungi Classification. In the context of few-shot\ntransfer learning, traditional fine-tuning fails to prevent highly expressive\nmodel from exploiting spurious correlations in the training data. On the other\nhand, although model-agnostic meta-learning (MAML) presents as a natural\nalternative for transfer learning, the expensive computation due to implicit\nsecond-order optimization limits its use in large-scale models and datasets. In\nthis work we aim to further improve the generalization of existing\nvision-language models on unseen tasks via a simple yet efficient fine-tuning\nstrategy based on uniform task sampling. We term our method as Model-Agnostic\nMultitask Fine-tuning (MAMF). Compared with MAML, MAMF discards the bi-level\noptimization and uses only first-order gradients, which makes it easily\nscalable and computationally efficient. Due to the uniform task sampling\nprocedure, MAMF consistently outperforms the classical fine-tuning method for\nfew-shot transfer learning on five benchmark datasets. Empirically, we further\ndiscover that the effectiveness of first-order MAML is highly dependent on the\nzero-shot performance of the pretrained model, and our simple algorithm can\noutperform first-order MAML on more challenging datasets with low zero-shot\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhenhailong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Manling Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Han Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pose Guided Multi-person Image Generation From Text. (arXiv:2203.04907v1 [cs.CV])","link":"http://arxiv.org/abs/2203.04907","description":"<p>Transformers have recently been shown to generate high quality images from\ntexts. However, existing methods struggle to create high fidelity full-body\nimages, especially multiple people. A person's pose has a high degree of\nfreedom that is difficult to describe using words only; this creates errors in\nthe generated image, such as incorrect body proportions and pose. We propose a\npose-guided text-to-image model, using pose as an additional input constraint.\nUsing the proposed Keypoint Pose Encoding (KPE) to encode human pose into low\ndimensional representation, our model can generate novel multi-person images\naccurately representing the pose and text descriptions provided, with minimal\nerrors. We demonstrate that KPE is invariant to changes in the target image\ndomain and image resolution; we show results on the Deepfashion dataset and\ncreate a new multi-person Deepfashion dataset to demonstrate the\nmulti-capabilities of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheong_S/0/1/0/all/0/1\">Soon Yau Cheong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mustafa_A/0/1/0/all/0/1\">Armin Mustafa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gilbert_A/0/1/0/all/0/1\">Andrew Gilbert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DUAL: Textless Spoken Question Answering with Speech Discrete Unit Adaptive Learning. (arXiv:2203.04911v1 [cs.CL])","link":"http://arxiv.org/abs/2203.04911","description":"<p>Spoken Question Answering (SQA) has gained research attention and made\nremarkable progress in recent years. However, existing SQA methods rely on\nAutomatic Speech Recognition (ASR) transcripts, which are time and\ncost-prohibitive to collect. This work proposes an ASR transcript-free SQA\nframework named Discrete Unit Adaptive Learning (DUAL), which leverages\nunlabeled data for pre-training and is fine-tuned by the SQA downstream task.\nDAUL can directly predict the time interval of the spoken answer from the\nspoken document. We also release a new SQA benchmark corpus Natural\nMulti-speaker Spoken Question Answering (NMSQA) for testing SQA in realistic\nscenarios. The experimental results show that DUAL performs competitively with\nthe cascade approach (ASR + text QA), and DUAL is robust to real-world speech.\nWe will open-source our code and model to inspire more SQA innovations from the\ncommunity\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1\">Guan-Ting Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chuang_Y/0/1/0/all/0/1\">Yung-Sung Chuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_H/0/1/0/all/0/1\">Ho-Lam Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shu-wen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hsuan-Jui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shang-Wen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1\">Abdelrahman Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_L/0/1/0/all/0/1\">Lin-shan Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Flexible Multi-Task Model for BERT Serving. (arXiv:2107.05377v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.05377","description":"<p>In this demonstration, we present an efficient BERT-based multi-task (MT)\nframework that is particularly suitable for iterative and incremental\ndevelopment of the tasks. The proposed framework is based on the idea of\npartial fine-tuning, i.e. only fine-tune some top layers of BERT while keep the\nother layers frozen. For each task, we train independently a single-task (ST)\nmodel using partial fine-tuning. Then we compress the task-specific layers in\neach ST model using knowledge distillation. Those compressed ST models are\nfinally merged into one MT model so that the frozen layers of the former are\nshared across the tasks. We exemplify our approach on eight GLUE tasks,\ndemonstrating that it is able to achieve both strong performance and\nefficiency. We have implemented our method in the utterance understanding\nsystem of XiaoAI, a commercial AI assistant developed by Xiaomi. We estimate\nthat our model reduces the overall serving cost by 86%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_T/0/1/0/all/0/1\">Tianwen Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_J/0/1/0/all/0/1\">Jianwei Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Shenghuan He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Headed-Span-Based Projective Dependency Parsing. (arXiv:2108.04750v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.04750","description":"<p>We propose a new method for projective dependency parsing based on headed\nspans. In a projective dependency tree, the largest subtree rooted at each word\ncovers a contiguous sequence (i.e., a span) in the surface order. We call such\na span marked by a root word \\textit{headed span}.\n</p>\n<p>A projective dependency tree can be represented as a collection of headed\nspans. We decompose the score of a dependency tree into the scores of the\nheaded spans and design a novel $O(n^3)$ dynamic programming algorithm to\nenable global training and exact inference. Our model achieves state-of-the-art\nor competitive results on PTB, CTB, and UD. Our code is publicly available at\n\\url{https://github.com/sustcsonglin/span-based-dependency-parsing}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Songlin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_K/0/1/0/all/0/1\">Kewei Tu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Combining (second-order) graph-based and headed-span-based projective dependency parsing. (arXiv:2108.05838v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.05838","description":"<p>Graph-based methods, which decompose the score of a dependency tree into\nscores of dependency arcs, are popular in dependency parsing for decades.\nRecently, \\citet{Yang2022Span} propose a headed-span-based method that\ndecomposes the score of a dependency tree into scores of headed spans. They\nshow improvement over first-order graph-based methods. However, their method\ndoes not score dependency arcs at all, and dependency arcs are implicitly\ninduced by their cubic-time algorithm, which is possibly sub-optimal since\nmodeling dependency arcs is intuitively useful. In this work, we aim to combine\ngraph-based and headed-span-based methods, incorporating both arc scores and\nheaded span scores into our model. First, we show a direct way to combine with\n$O(n^4)$ parsing complexity. To decrease complexity, inspired by the classical\nhead-splitting trick, we show two $O(n^3)$ dynamic programming algorithms to\ncombine first- and second-order graph-based and headed-span-based methods. Our\nexperiments on PTB, CTB, and UD show that combining first-order graph-based and\nheaded-span-based methods is effective. We also confirm the effectiveness of\nsecond-order graph-based parsing in the deep learning age, however, we observe\nmarginal or no improvement when combining second-order graph-based and\nheaded-span-based methods. Our code is publicly available at\n\\url{https://github.com/sustcsonglin/span-based-dependency-parsing}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Songlin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_K/0/1/0/all/0/1\">Kewei Tu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bottom-Up Constituency Parsing and Nested Named Entity Recognition with Pointer Networks. (arXiv:2110.05419v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.05419","description":"<p>Constituency parsing and nested named entity recognition (NER) are similar\ntasks since they both aim to predict a collection of nested and non-crossing\nspans. In this work, we cast nested NER to constituency parsing and propose a\nnovel pointing mechanism for bottom-up parsing to tackle both tasks. The key\nidea is based on the observation that if we traverse a constituency tree in\npost-order, i.e., visiting a parent after its children, then two consecutively\nvisited spans would share a boundary. Our model tracks the shared boundaries\nand predicts the next boundary at each step by leveraging a pointer network. As\na result, it needs only linear steps to parse and thus is efficient. It also\nmaintains a parsing configuration for structural consistency, i.e., always\noutputting valid trees. Experimentally, our model achieves the state-of-the-art\nperformance on PTB among all BERT-based models (96.01 F1 score) and competitive\nperformance on CTB7 in constituency parsing; and it also achieves strong\nperformance on three benchmark datasets of nested NER: ACE2004, ACE2005, and\nGENIA. Our code is publicly available at\n\\url{https://github.com/sustcsonglin/pointer-net-for-nested}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Songlin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_K/0/1/0/all/0/1\">Kewei Tu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fusing Heterogeneous Factors with Triaffine Mechanism for Nested Named Entity Recognition. (arXiv:2110.07480v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.07480","description":"<p>Nested entities are observed in many domains due to their compositionality,\nwhich cannot be easily recognized by the widely-used sequence labeling\nframework. A natural solution is to treat the task as a span classification\nproblem. To learn better span representation and increase classification\nperformance, it is crucial to effectively integrate heterogeneous factors\nincluding inside tokens, boundaries, labels, and related spans which could be\ncontributing to nested entities recognition. To fuse these heterogeneous\nfactors, we propose a novel triaffine mechanism including triaffine attention\nand scoring. Triaffine attention uses boundaries and labels as queries and uses\ninside tokens and related spans as keys and values for span representations.\nTriaffine scoring interacts with boundaries and span representations for\nclassification. Experiments show that our proposed method outperforms previous\nspan-based methods, achieves the state-of-the-art $F_1$ scores on nested NER\ndatasets GENIA and KBP2017, and shows comparable results on ACE2004 and\nACE2005.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zheng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Songfang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prix-LM: Pretraining for Multilingual Knowledge Base Construction. (arXiv:2110.08443v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.08443","description":"<p>Knowledge bases (KBs) contain plenty of structured world and commonsense\nknowledge. As such, they often complement distributional text-based information\nand facilitate various downstream tasks. Since their manual construction is\nresource- and time-intensive, recent efforts have tried leveraging large\npretrained language models (PLMs) to generate additional monolingual knowledge\nfacts for KBs. However, such methods have not been attempted for building and\nenriching multilingual KBs. Besides wider application, such multilingual KBs\ncan provide richer combined knowledge than monolingual (e.g., English) KBs.\nKnowledge expressed in different languages may be complementary and unequally\ndistributed: this implies that the knowledge available in high-resource\nlanguages can be transferred to low-resource ones. To achieve this, it is\ncrucial to represent multilingual knowledge in a shared/unified space. To this\nend, we propose a unified representation model, Prix-LM, for multilingual KB\nconstruction and completion. We leverage two types of knowledge, monolingual\ntriples and cross-lingual links, extracted from existing multilingual KBs, and\ntune a multilingual language encoder XLM-R via a causal language modeling\nobjective. Prix-LM integrates useful multilingual and KB-based factual\nknowledge into a single model. Experiments on standard entity-related tasks,\nsuch as link prediction in multiple languages, cross-lingual entity linking and\nbilingual lexicon induction, demonstrate its effectiveness, with gains reported\nover strong task-specialised baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wenxuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fangyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vulic_I/0/1/0/all/0/1\">Ivan Vuli&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collier_N/0/1/0/all/0/1\">Nigel Collier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Muhao Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Trajectory Prediction with Linguistic Representations. (arXiv:2110.09741v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2110.09741","description":"<p>Language allows humans to build mental models that interpret what is\nhappening around them resulting in more accurate long-term predictions. We\npresent a novel trajectory prediction model that uses linguistic intermediate\nrepresentations to forecast trajectories, and is trained using trajectory\nsamples with partially-annotated captions. The model learns the meaning of each\nof the words without direct per-word supervision. At inference time, it\ngenerates a linguistic description of trajectories which captures maneuvers and\ninteractions over an extended time interval. This generated description is used\nto refine predictions of the trajectories of multiple agents. We train and\nvalidate our model on the Argoverse dataset, and demonstrate improved accuracy\nresults in trajectory prediction. In addition, our model is more interpretable:\nit presents part of its reasoning in plain language as captions, which can aid\nmodel development and can aid in building confidence in the model before\ndeploying it.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kuo_Y/0/1/0/all/0/1\">Yen-Ling Kuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barbu_A/0/1/0/all/0/1\">Andrei Barbu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McGill_S/0/1/0/all/0/1\">Stephen G. McGill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katz_B/0/1/0/all/0/1\">Boris Katz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leonard_J/0/1/0/all/0/1\">John J. Leonard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosman_G/0/1/0/all/0/1\">Guy Rosman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"L-Verse: Bidirectional Generation Between Image and Text. (arXiv:2111.11133v7 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.11133","description":"<p>Far beyond learning long-range interactions of natural language, transformers\nare becoming the de-facto standard for many vision tasks with their power and\nscalability. Especially with cross-modal tasks between image and text, vector\nquantized variational autoencoders (VQ-VAEs) are widely used to make a raw RGB\nimage into a sequence of feature vectors. To better leverage the correlation\nbetween image and text, we propose L-Verse, a novel architecture consisting of\nfeature-augmented variational autoencoder (AugVAE) and bidirectional\nauto-regressive transformer (BiART) for text-to-image and image-to-text\ngeneration. Our AugVAE shows the state-of-the-art reconstruction performance on\nImageNet1K validation set, along with the robustness to unseen images in the\nwild. Unlike other models, BiART can distinguish between image (or text) as a\nconditional reference and a generation target. L-Verse can be directly used for\nimage-to-text or text-to-image generation tasks without any finetuning or extra\nobject detection framework. In quantitative and qualitative experiments,\nL-Verse shows impressive results against previous methods in both image-to-text\nand text-to-image generation on MS-COCO Captions. We furthermore assess the\nscalability of L-Verse architecture on Conceptual Captions and present the\ninitial results of bidirectional vision-language representation learning on\ngeneral domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Taehoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_G/0/1/0/all/0/1\">Gwangmo Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sihaeng Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sangyun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_Y/0/1/0/all/0/1\">Yewon Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Soonyoung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seung Hwan Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Honglak Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bae_K/0/1/0/all/0/1\">Kyunghoon Bae</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A novel knowledge graph development for industry design: A case study on indirect coal liquefaction process. (arXiv:2111.13854v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.13854","description":"<p>Hazard and operability analysis (HAZOP) is a remarkable representative in\nindustrial safety engineering. However, a great storehouse of industrial safety\nknowledge (ISK) in HAZOP reports has not been thoroughly exploited. In order to\nreuse and unlock the value of ISK and optimize HAZOP, we have developed a novel\nknowledge graph for industrial safety (ISKG) with HAZOP as the carrier through\nbridging data science and engineering design. Specifically, firstly,\nconsidering that the knowledge contained in HAZOP reports of different\nprocesses in industry is not the same, we creatively develope a general ISK\nstandardization framework, it provides a practical scheme for integrating HAZOP\nreports from various processes and uniformly representing the ISK with diverse\nexpressions. Secondly, we conceive a novel and reliable information extraction\nmodel based on deep learning combined with data science, it can effectively\nmine ISK from HAZOP reports, which alleviates the obstacle of ISK extraction\ncaused by the particularity of HAZOP text. Finally, we build ISK triples and\nstore them in the Neo4j graph database. We take indirect coal liquefaction\nprocess as a case study to develop ISKG, and its oriented applications can\noptimize HAZOP and mine the potential of ISK, which is of great significance to\nimprove the security of the system and enhance prevention awareness for people.\nISKG containing the ISK standardization framework and the information\nextraction model sets an example of the interaction between data science and\nengineering design, which can enlighten other researchers and extend the\nperspectives of industrial safety.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhenhua Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Beike Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_D/0/1/0/all/0/1\">Dong Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Editing for Counterfactual Stories. (arXiv:2112.05417v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.05417","description":"<p>Creating what-if stories requires reasoning about prior statements and\npossible outcomes of the changed conditions. One can easily generate coherent\nendings under new conditions, but it would be challenging for current systems\nto do it with minimal changes to the original story. Therefore, one major\nchallenge is the trade-off between generating a logical story and rewriting\nwith minimal-edits. In this paper, we propose EDUCAT, an editing-based\nunsupervised approach for counterfactual story rewriting. EDUCAT includes a\ntarget position detection strategy based on estimating causal effects of the\nwhat-if conditions, which keeps the causal invariant parts of the story. EDUCAT\nthen generates the stories under fluency, coherence and minimal-edits\nconstraints. We also propose a new metric to alleviate the shortcomings of\ncurrent automatic metrics and better evaluate the trade-off. We evaluate EDUCAT\non a public counterfactual story rewriting benchmark. Experiments show that\nEDUCAT achieves the best trade-off over unsupervised SOTA methods according to\nboth automatic and human evaluation. The resources of EDUCAT are available at:\nhttps://github.com/jiangjiechen/EDUCAT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiangjie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1\">Chun Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1\">Sijie Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yanghua Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Many Ways to be Lonely: Fine-grained Characterization of Loneliness and its Potential Changes in COVID-19. (arXiv:2201.07423v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.07423","description":"<p>Loneliness has been associated with negative outcomes for physical and mental\nhealth. Understanding how people express and cope with various forms of\nloneliness is critical for early screening and targeted interventions to reduce\nloneliness, particularly among vulnerable groups such as young adults. To\nexamine how different forms of loneliness and coping strategies manifest in\nloneliness self-disclosure, we built a dataset, FIG-Loneliness (FIne-Grained\nLoneliness) by using Reddit posts in two young adult-focused forums and two\nloneliness related forums consisting of a diverse age group. We provide\nannotations by trained human annotators for binary and fine-grained loneliness\nclassifications of the posts. Trained on FIG-Loneliness, two BERT-based models\nwere used to understand loneliness forms and authors' coping strategies in\nthese forums. Our binary loneliness classification achieved an accuracy above\n97%, and fine-grained loneliness category classification reached an average\naccuracy of 77% across all labeled categories. With FIG-Loneliness and model\npredictions, we found that loneliness expressions in the young adult related\nforums are distinct from other forums. Those in young adult-focused forums are\nmore likely to express concerns pertaining to peer relationship, and are\npotentially more sensitive to geographical isolation impacted by the COVID-19\npandemic lockdown. Also, we show that different forms of loneliness have\ndifferential use in coping strategies. For example, those who experienced\ntransient and social loneliness had stronger desires to reach out and connect\nwith others, whereas individuals who experienced physical and romantic\nloneliness were more likely to seek advice from others.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yueyi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yunfan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leqi_L/0/1/0/all/0/1\">Liu Leqi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winkielman_P/0/1/0/all/0/1\">Piotr Winkielman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Grounding Answers for Visual Questions Asked by Visually Impaired People. (arXiv:2202.01993v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.01993","description":"<p>Visual question answering is the task of answering questions about images. We\nintroduce the VizWiz-VQA-Grounding dataset, the first dataset that visually\ngrounds answers to visual questions asked by people with visual impairments. We\nanalyze our dataset and compare it with five VQA-Grounding datasets to\ndemonstrate what makes it similar and different. We then evaluate the SOTA VQA\nand VQA-Grounding models and demonstrate that current SOTA algorithms often\nfail to identify the correct visual evidence where the answer is located. These\nmodels regularly struggle when the visual evidence occupies a small fraction of\nthe image, for images that are higher quality, as well as for visual questions\nthat require skills in text recognition. The dataset, evaluation server, and\nleaderboard all can be found at the following link:\nhttps://vizwiz.org/tasks-and-datasets/answer-grounding-for-vqa/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chongyan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anjum_S/0/1/0/all/0/1\">Samreen Anjum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurari_D/0/1/0/all/0/1\">Danna Gurari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ClarET: Pre-training a Correlation-Aware Context-To-Event Transformer for Event-Centric Generation and Classification. (arXiv:2203.02225v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.02225","description":"<p>Generating new events given context with correlated ones plays a crucial role\nin many event-centric reasoning tasks. Existing works either limit their scope\nto specific scenarios or overlook event-level correlations. In this paper, we\npropose to pre-train a general Correlation-aware context-to-Event Transformer\n(ClarET) for event-centric reasoning. To achieve this, we propose three novel\nevent-centric objectives, i.e., whole event recovering, contrastive\nevent-correlation encoding and prompt-based event locating, which highlight\nevent-level correlations with effective training. The proposed ClarET is\napplicable to a wide range of event-centric reasoning scenarios, considering\nits versatility of (i) event-correlation types (e.g., causal, temporal,\ncontrast), (ii) application formulations (i.e., generation and classification),\nand (iii) reasoning types (e.g., abductive, counterfactual and ending\nreasoning). Empirical fine-tuning results, as well as zero- and few-shot\nlearning, on 9 benchmarks (5 generation and 4 classification tasks covering 4\nreasoning types with diverse event correlations), verify its effectiveness and\ngeneralization ability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yucheng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_T/0/1/0/all/0/1\">Tao Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1\">Xiubo Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_G/0/1/0/all/0/1\">Guodong Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ClueGraphSum: Let Key Clues Guide the Cross-Lingual Abstractive Summarization. (arXiv:2203.02797v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.02797","description":"<p>Cross-Lingual Summarization (CLS) is the task to generate a summary in one\nlanguage for an article in a different language. Previous studies on CLS mainly\ntake pipeline methods or train the end-to-end model using the translated\nparallel data. However, the quality of generated cross-lingual summaries needs\nmore further efforts to improve, and the model performance has never been\nevaluated on the hand-written CLS dataset. Therefore, we first propose a\nclue-guided cross-lingual abstractive summarization method to improve the\nquality of cross-lingual summaries, and then construct a novel hand-written CLS\ndataset for evaluation. Specifically, we extract keywords, named entities, etc.\nof the input article as key clues for summarization and then design a\nclue-guided algorithm to transform an article into a graph with less noisy\nsentences. One Graph encoder is built to learn sentence semantics and article\nstructures and one Clue encoder is built to encode and translate key clues,\nensuring the information of important parts are reserved in the generated\nsummary. These two encoders are connected by one decoder to directly learn\ncross-lingual semantics. Experimental results show that our method has stronger\nrobustness for longer inputs and substantially improves the performance over\nthe strong baseline, achieving an improvement of 8.55 ROUGE-1\n(English-to-Chinese summarization) and 2.13 MoverScore (Chinese-to-English\nsummarization) scores over the existing SOTA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Shuyu Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_D/0/1/0/all/0/1\">Dengbiao Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xingshu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_R/0/1/0/all/0/1\">Rui Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenxian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haizhou Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ILDAE: Instance-Level Difficulty Analysis of Evaluation Data. (arXiv:2203.03073v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.03073","description":"<p>Knowledge of questions' difficulty level helps a teacher in several ways,\nsuch as estimating students' potential quickly by asking carefully selected\nquestions and improving quality of examination by modifying trivial and hard\nquestions. Can we extract such benefits of instance difficulty in NLP? To this\nend, we conduct Instance-Level Difficulty Analysis of Evaluation data (ILDAE)\nin a large-scale setup of 23 datasets and demonstrate its five novel\napplications: 1) conducting efficient-yet-accurate evaluations with fewer\ninstances saving computational cost and time, 2) improving quality of existing\nevaluation datasets by repairing erroneous and trivial instances, 3) selecting\nthe best model based on application requirements, 4) analyzing dataset\ncharacteristics for guiding future data creation, 5) estimating Out-of-Domain\nperformance reliably. Comprehensive experiments for these applications result\nin several interesting findings, such as evaluation using just 5% instances\n(selected via ILDAE) achieves as high as 0.93 Kendall correlation with\nevaluation using complete dataset and computing weighted accuracy using\ndifficulty scores leads to 5.2% higher correlation with Out-of-Domain\nperformance. We release the difficulty scores and hope our analyses and\nfindings will bring more attention to this important yet understudied field of\nleveraging instance difficulty in evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Varshney_N/0/1/0/all/0/1\">Neeraj Varshney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Swaroop Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-03-09T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Residual Aligner Network. (arXiv:2203.04290v1 [eess.IV])","link":"http://arxiv.org/abs/2203.04290","description":"<p>Image registration is important for medical imaging, the estimation of the\nspatial transformation between different images. Many previous studies have\nused learning-based methods for coarse-to-fine registration to efficiently\nperform 3D image registration. The coarse-to-fine approach, however, is limited\nwhen dealing with the different motions of nearby objects. Here we propose a\nnovel Motion-Aware (MA) structure that captures the different motions in a\nregion. The MA structure incorporates a novel Residual Aligner (RA) module\nwhich predicts the multi-head displacement field used to disentangle the\ndifferent motions of multiple neighbouring objects. Compared with other deep\nlearning methods, the network based on the MA structure and RA module achieve\none of the most accurate unsupervised inter-subject registration on the 9\norgans of assorted sizes in abdominal CT scans, with the highest-ranked\nregistration of the veins (Dice Similarity Coefficient / Average surface\ndistance: 62\\%/4.9mm for the vena cava and 34\\%/7.9mm for the portal and\nsplenic vein), with a half-sized structure and more efficient computation.\nApplied to the segmentation of lungs in chest CT scans, the new network\nachieves results which were indistinguishable from the best-ranked networks\n(94\\%/3.0mm). Additionally, the theorem on predicted motion pattern and the\ndesign of MA structure are validated by further analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zheng_J/0/1/0/all/0/1\">Jian-Qing Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1\">Ziyang Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_B/0/1/0/all/0/1\">Baoru Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lim_N/0/1/0/all/0/1\">Ngee Han Lim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Papiez_B/0/1/0/all/0/1\">Bartlomiej W. Papiez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning from Few Examples: A Summary of Approaches to Few-Shot Learning. (arXiv:2203.04291v1 [cs.LG])","link":"http://arxiv.org/abs/2203.04291","description":"<p>Few-Shot Learning refers to the problem of learning the underlying pattern in\nthe data just from a few training samples. Requiring a large number of data\nsamples, many deep learning solutions suffer from data hunger and extensively\nhigh computation time and resources. Furthermore, data is often not available\ndue to not only the nature of the problem or privacy concerns but also the cost\nof data preparation. Data collection, preprocessing, and labeling are strenuous\nhuman tasks. Therefore, few-shot learning that could drastically reduce the\nturnaround time of building machine learning applications emerges as a low-cost\nsolution. This survey paper comprises a representative list of recently\nproposed few-shot learning algorithms. Given the learning dynamics and\ncharacteristics, the approaches to few-shot learning problems are discussed in\nthe perspectives of meta-learning, transfer learning, and hybrid approaches\n(i.e., different variations of the few-shot learning problem).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Parnami_A/0/1/0/all/0/1\">Archit Parnami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Minwoo Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards performant and reliable undersampled MR reconstruction via diffusion model sampling. (arXiv:2203.04292v1 [eess.IV])","link":"http://arxiv.org/abs/2203.04292","description":"<p>Magnetic Resonance (MR) image reconstruction from under-sampled acquisition\npromises faster scanning time. To this end, current State-of-The-Art (SoTA)\napproaches leverage deep neural networks and supervised training to learn a\nrecovery model. While these approaches achieve impressive performances, the\nlearned model can be fragile on unseen degradation, e.g. when given a different\nacceleration factor. These methods are also generally deterministic and provide\na single solution to an ill-posed problem; as such, it can be difficult for\npractitioners to understand the reliability of the reconstruction. We introduce\nDiffuseRecon, a novel diffusion model-based MR reconstruction method.\nDiffuseRecon guides the generation process based on the observed signals and a\npre-trained diffusion model, and does not require additional training on\nspecific acceleration factors. DiffuseRecon is stochastic in nature and\ngenerates results from a distribution of fully-sampled MR images; as such, it\nallows us to explicitly visualize different potential reconstruction solutions.\nLastly, DiffuseRecon proposes an accelerated, coarse-to-fine Monte-Carlo\nsampling scheme to approximate the most likely reconstruction candidate. The\nproposed DiffuseRecon achieves SoTA performances reconstructing from raw\nacquisition signals in fastMRI and SKM-TEA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Peng_C/0/1/0/all/0/1\">Cheng Peng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guo_P/0/1/0/all/0/1\">Pengfei Guo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_S/0/1/0/all/0/1\">S. Kevin Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Patel_V/0/1/0/all/0/1\">Vishal Patel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chellappa_R/0/1/0/all/0/1\">Rama Chellappa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NaviAirway: a bronchiole-sensitive deep learning-based airway segmentation pipeline for planning of navigation bronchoscopy. (arXiv:2203.04294v1 [eess.IV])","link":"http://arxiv.org/abs/2203.04294","description":"<p>Navigation bronchoscopy is a minimally invasive procedure in which doctors\npass a bronchoscope into a subject's airways to sample the target pulmonary\nlesion. A three-dimensional (3D) airway roadmap reconstructed from Computer\nTomography (CT) scans is a prerequisite for this procedure, especially when the\ntarget is distally located. Therefore, an accurate and efficient airway\nsegmentation algorithm is essential to reduce bronchoscopists' burden of\npre-procedural airway identification as well as patients' discomfort during the\nprolonged procedure. However, airway segmentation remains a challenging task\nbecause of the intrinsic complex tree-like structure, imbalanced sizes of\nairway branches, potential domain shifts of CT scans, and few available labeled\nimages. To address these problems, we present a deep learning-based pipeline,\ndenoted as NaviAirway, which finds finer bronchioles through four major novel\ncomponents - feature extractor modules in model architecture design, a\nbronchiole-sensitive loss function, a human-vision-inspired iterative training\nstrategy, and a semi-supervised learning framework to utilize unlabeled CT\nimages. Experimental results showed that NaviAirway outperformed existing\nmethods, particularly in identification of higher generation bronchioles and\nrobustness to new CT scans. On average, NaviAirway takes five minutes to\nsegment the CT scans of one patient on a GPU-embedded computer. Moreover, we\npropose two new metrics to complement conventional ones for a more\ncomprehensive and fairer evaluation of deep learning-based airway segmentation\napproaches. The code is publicly available on\nhttps://github.com/AntonotnaWang/NaviAirway.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_A/0/1/0/all/0/1\">Andong Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tam_T/0/1/0/all/0/1\">Terence Chi Chun Tam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Poon_H/0/1/0/all/0/1\">Ho Ming Poon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_K/0/1/0/all/0/1\">Kun-Chang Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_W/0/1/0/all/0/1\">Wei-Ning Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Region Specific Optimization (RSO)-based Deep Interactive Registration. (arXiv:2203.04295v1 [eess.IV])","link":"http://arxiv.org/abs/2203.04295","description":"<p>Medical image registration is a fundamental and vital task which will affect\nthe efficacy of many downstream clinical tasks. Deep learning (DL)-based\ndeformable image registration (DIR) methods have been investigated, showing\nstate-of-the-art performance. A test time optimization (TTO) technique was\nproposed to further improve the DL models' performance. Despite the substantial\naccuracy improvement with this TTO technique, there still remained some regions\nthat exhibited large registration errors even after many TTO iterations. To\nmitigate this challenge, we firstly identified the reason why the TTO technique\nwas slow, or even failed, to improve those regions' registration results. We\nthen proposed a two-levels TTO technique, i.e., image-specific optimization\n(ISO) and region-specific optimization (RSO), where the region can be\ninteractively indicated by the clinician during the registration result\nreviewing process. For both efficiency and accuracy, we further envisioned a\nthree-step DL-based image registration workflow. Experimental results showed\nthat our proposed method outperformed the conventional method qualitatively and\nquantitatively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bai_T/0/1/0/all/0/1\">Ti Bai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_M/0/1/0/all/0/1\">Muhan Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liang_X/0/1/0/all/0/1\">Xiao Liang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_B/0/1/0/all/0/1\">Biling Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dohopolski_M/0/1/0/all/0/1\">Michael Dohopolski</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cai_B/0/1/0/all/0/1\">Bin Cai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_D/0/1/0/all/0/1\">Dan Nguyen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jiang_S/0/1/0/all/0/1\">Steve Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Source-free Domain Adaptation for Multi-site and Lifespan Brain Skull Stripping. (arXiv:2203.04299v1 [eess.IV])","link":"http://arxiv.org/abs/2203.04299","description":"<p>Skull stripping is a crucial prerequisite step in the analysis of brain\nmagnetic resonance (MR) images. Although many excellent works or tools have\nbeen proposed, they suffer from low generalization capability. For instance,\nthe model trained on a dataset with specific imaging parameters (source domain)\ncannot be well applied to other datasets with different imaging parameters\n(target domain). Especially, for the lifespan datasets, the model trained on an\nadult dataset is not applicable to an infant dataset due to the large domain\ndifference. To address this issue, numerous domain adaptation (DA) methods have\nbeen proposed to align the extracted features between the source and target\ndomains, requiring concurrent access to the input images of both domains.\nUnfortunately, it is problematic to share the images due to privacy. In this\npaper, we design a source-free domain adaptation framework (SDAF) for\nmulti-site and lifespan skull stripping that can accomplish domain adaptation\nwithout access to source domain images. Our method only needs to share the\nsource labels as shape dictionaries and the weights trained on the source data,\nwithout disclosing private information from source domain subjects. To deal\nwith the domain shift between multi-site lifespan datasets, we take advantage\nof the brain shape prior which is invariant to imaging parameters and ages.\nExperiments demonstrate that our framework can significantly outperform the\nstate-of-the-art methods on multi-site lifespan datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yunxiang Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dan_R/0/1/0/all/0/1\">Ruilong Dan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1\">Shuai Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cao_Y/0/1/0/all/0/1\">Yifan Cao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Luo_X/0/1/0/all/0/1\">Xiangde Luo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tan_C/0/1/0/all/0/1\">Chenghao Tan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jia_G/0/1/0/all/0/1\">Gangyong Jia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_H/0/1/0/all/0/1\">Huiyu Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yaqi Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_L/0/1/0/all/0/1\">Li Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UENAS: A Unified Evolution-based NAS Framework. (arXiv:2203.04300v1 [cs.LG])","link":"http://arxiv.org/abs/2203.04300","description":"<p>Neural architecture search (NAS) has gained significant attention for\nautomatic network design in recent years. Previous NAS methods suffer from\nlimited search spaces, which may lead to sub-optimal results. In this paper, we\npropose UENAS, an evolution-based NAS framework with a broader search space\nthat supports optimizing network architectures, pruning strategies, and\nhyperparameters simultaneously. To alleviate the huge search cost caused by the\nexpanded search space, three strategies are adopted: First, an adaptive pruning\nstrategy that iteratively trims the average model size in the population\nwithout compromising performance. Second, child networks share weights of\noverlapping layers with pre-trained parent networks to reduce the training\nepochs. Third, an online predictor scores the joint representations of\narchitecture, pruning strategy, and hyperparameters to filter out inferior\ncombos. By the proposed three strategies, the search efficiency is\nsignificantly improved and more well-performed compact networks with tailored\nhyper-parameters are derived. In experiments, UENAS achieves error rates of\n2.81% on CIFAR-10, 20.24% on CIFAR-100, and 33% on Tiny-ImageNet, which shows\nthe effectiveness of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zimian Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_H/0/1/0/all/0/1\">Hengyue Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_X/0/1/0/all/0/1\">Xin Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_P/0/1/0/all/0/1\">Peijie Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dongsheng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Live Laparoscopic Video Retrieval with Compressed Uncertainty. (arXiv:2203.04301v1 [eess.IV])","link":"http://arxiv.org/abs/2203.04301","description":"<p>Searching through large volumes of medical data to retrieve relevant\ninformation is a challenging yet crucial task for clinical care. However the\nprimitive and most common approach to retrieval, involving text in the form of\nkeywords, is severely limited when dealing with complex media formats.\nContent-based retrieval offers a way to overcome this limitation, by using rich\nmedia as the query itself. Surgical video-to-video retrieval in particular is a\nnew and largely unexplored research problem with high clinical value,\nespecially in the real-time case: using real-time video hashing, search can be\nachieved directly inside of the operating room. Indeed, the process of hashing\nconverts large data entries into compact binary arrays or hashes, enabling\nlarge-scale search operations at a very fast rate. However, due to fluctuations\nover the course of a video, not all bits in a given hash are equally reliable.\nIn this work, we propose a method capable of mitigating this uncertainty while\nmaintaining a light computational footprint. We present superior retrieval\nresults (3-4 % top 10 mean average precision) on a multi-task evaluation\nprotocol for surgery, using cholecystectomy phases, bypass phases, and coming\nfrom an entirely new dataset introduced here, critical events across six\ndifferent surgery types. Success on this multi-task benchmark shows the\ngeneralizability of our approach for surgical video retrieval.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yu_T/0/1/0/all/0/1\">Tong Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mascagni_P/0/1/0/all/0/1\">Pietro Mascagni</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Verde_J/0/1/0/all/0/1\">Juan Verde</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Marescaux_J/0/1/0/all/0/1\">Jacques Marescaux</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mutter_D/0/1/0/all/0/1\">Didier Mutter</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Padoy_N/0/1/0/all/0/1\">Nicolas Padoy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SuperPoint features in endoscopy. (arXiv:2203.04302v1 [eess.IV])","link":"http://arxiv.org/abs/2203.04302","description":"<p>There is often a significant gap between research results and applicability\nin routine medical practice. This work studies the performance of well-known\nlocal features on a medical dataset captured during routine colonoscopy\nprocedures. Local feature extraction and matching is a key step for many\ncomputer vision applications, specially regarding 3D modelling. In the medical\ndomain, handcrafted local features such as SIFT, with public pipelines such as\nCOLMAP, are still a predominant tool for this kind of tasks. We explore the\npotential of the well known self-supervised approach SuperPoint, present an\nadapted variation for the endoscopic domain and propose a challenging\nevaluation framework. SuperPoint based models achieve significantly higher\nmatching quality than commonly used local features in this domain. Our adapted\nmodel avoids features within specularity regions, a frequent and problematic\nartifact in endoscopic images, with consequent benefits for matching and\nreconstruction results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Barbed_O/0/1/0/all/0/1\">O. L. Barbed</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chadebecq_F/0/1/0/all/0/1\">F. Chadebecq</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Morlana_J/0/1/0/all/0/1\">J. Morlana</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Martinez_Montiel_J/0/1/0/all/0/1\">J.M. Mart&#xed;nez-Montiel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Murillo_A/0/1/0/all/0/1\">A. C. Murillo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Dual-Output Diffusion Models. (arXiv:2203.04304v1 [eess.IV])","link":"http://arxiv.org/abs/2203.04304","description":"<p>Iterative denoising-based generation, also known as denoising diffusion\nmodels, has recently been shown to be comparable in quality to other classes of\ngenerative models, and even surpass them. Including, in particular, Generative\nAdversarial Networks, which are currently the state of the art in many\nsub-tasks of image generation. However, a major drawback of this method is that\nit requires hundreds of iterations to produce a competitive result. Recent\nworks have proposed solutions that allow for faster generation with fewer\niterations, but the image quality gradually deteriorates with increasingly\nfewer iterations being applied during generation. In this paper, we reveal some\nof the causes that affect the generation quality of diffusion models,\nespecially when sampling with few iterations, and come up with a simple, yet\neffective, solution to mitigate them. We consider two opposite equations for\nthe iterative denoising, the first predicts the applied noise, and the second\npredicts the image directly. Our solution takes the two options and learns to\ndynamically alternate between them through the denoising process. Our proposed\nsolution is general and can be applied to any existing diffusion model. As we\nshow, when applied to various SOTA architectures, our solution immediately\nimproves their generation quality, with negligible added complexity and\nparameters. We experiment on multiple datasets and configurations and run an\nextensive ablation study to support these findings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Benny_Y/0/1/0/all/0/1\">Yaniv Benny</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wolf_L/0/1/0/all/0/1\">Lior Wolf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diffusion Models for Medical Anomaly Detection. (arXiv:2203.04306v1 [eess.IV])","link":"http://arxiv.org/abs/2203.04306","description":"<p>In medical applications, weakly supervised anomaly detection methods are of\ngreat interest, as only image-level annotations are required for training.\nCurrent anomaly detection methods mainly rely on generative adversarial\nnetworks or autoencoder models. Those models are often complicated to train or\nhave difficulties to preserve fine details in the image. We present a novel\nweakly supervised anomaly detection method based on denoising diffusion\nimplicit models. We combine the deterministic iterative noising and denoising\nscheme with classifier guidance for image-to-image translation between diseased\nand healthy subjects. Our method generates very detailed anomaly maps without\nthe need for a complex training procedure. We evaluate our method on the\nBRATS2020 dataset for brain tumor detection and the CheXpert dataset for\ndetecting pleural effusions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wolleb_J/0/1/0/all/0/1\">Julia Wolleb</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bieder_F/0/1/0/all/0/1\">Florentin Bieder</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sandkuhler_R/0/1/0/all/0/1\">Robin Sandk&#xfc;hler</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cattin_P/0/1/0/all/0/1\">Philippe C. Cattin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Breast cancer detection using artificial intelligence techniques: A systematic literature review. (arXiv:2203.04308v1 [eess.IV])","link":"http://arxiv.org/abs/2203.04308","description":"<p>Cancer is one of the most dangerous diseases to humans, and yet no permanent\ncure has been developed for it. Breast cancer is one of the most common cancer\ntypes. According to the National Breast Cancer foundation, in 2020 alone, more\nthan 276,000 new cases of invasive breast cancer and more than 48,000\nnon-invasive cases were diagnosed in the US. To put these figures in\nperspective, 64% of these cases are diagnosed early in the disease's cycle,\ngiving patients a 99% chance of survival. Artificial intelligence and machine\nlearning have been used effectively in detection and treatment of several\ndangerous diseases, helping in early diagnosis and treatment, and thus\nincreasing the patient's chance of survival. Deep learning has been designed to\nanalyze the most important features affecting detection and treatment of\nserious diseases. For example, breast cancer can be detected using genes or\nhistopathological imaging. Analysis at the genetic level is very expensive, so\nhistopathological imaging is the most common approach used to detect breast\ncancer. In this research work, we systematically reviewed previous work done on\ndetection and treatment of breast cancer using genetic sequencing or\nhistopathological imaging with the help of deep learning and machine learning.\nWe also provide recommendations to researchers who will work in this field\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Nassif_A/0/1/0/all/0/1\">Ali Bou Nassif</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Talib_M/0/1/0/all/0/1\">Manar Abu Talib</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nasir_Q/0/1/0/all/0/1\">Qassim Nasir</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Afadar_Y/0/1/0/all/0/1\">Yaman Afadar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Elgendy_O/0/1/0/all/0/1\">Omar Elgendy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Scale Adaptive Network for Single Image Denoising. (arXiv:2203.04313v1 [eess.IV])","link":"http://arxiv.org/abs/2203.04313","description":"<p>Multi-scale architectures have shown effectiveness in a variety of tasks\nincluding single image denoising, thanks to appealing cross-scale\ncomplementarity. However, existing methods treat different scale features\nequally without considering their scale-specific characteristics, i.e., the\nwithin-scale characteristics are ignored. In this paper, we reveal this missing\npiece for multi-scale architecture design and accordingly propose a novel\nMulti-Scale Adaptive Network (MSANet) for single image denoising. To be\nspecific, MSANet simultaneously embraces the within-scale characteristics and\nthe cross-scale complementarity thanks to three novel neural blocks, i.e.,\nadaptive feature block (AFeB), adaptive multi-scale block (AMB), and adaptive\nfusion block (AFuB). In brief, AFeB is designed to adaptively select details\nand filter noises, which is highly expected for fine-grained features. AMB\ncould enlarge the receptive field and aggregate the multi-scale information,\nwhich is designed to satisfy the demands of both fine- and coarse-grained\nfeatures. AFuB devotes to adaptively sampling and transferring the features\nfrom one scale to another scale, which is used to fuse the features with\nvarying characteristics from coarse to fine. Extensive experiments on both\nthree real and six synthetic noisy image datasets show the superiority of\nMSANet compared with 12 methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Gou_Y/0/1/0/all/0/1\">Yuanbiao Gou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_P/0/1/0/all/0/1\">Peng Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lv_J/0/1/0/all/0/1\">Jiancheng Lv</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Peng_X/0/1/0/all/0/1\">Xi Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PyNET-QxQ: A Distilled PyNET for QxQ Bayer Pattern Demosaicing in CMOS Image Sensor. (arXiv:2203.04314v1 [eess.IV])","link":"http://arxiv.org/abs/2203.04314","description":"<p>The deep learning-based ISP models for mobile cameras produce high-quality\nimages comparable to the professional DSLR camera. However, many of them are\ncomputationally expensive, which may not be appropriate for mobile\nenvironments. Also, the recent mobile cameras adopt non-Bayer CFAs (e.g., Quad\nBayer, Nona Bayer, and QxQ Bayer) to improve image quality; however, most deep\nlearning-based ISP models mainly focus on standard Bayer CFA. In this work, we\npropose PyNET-QxQ based on PyNET, a light-weighted ISP explicitly designed for\nthe QxQ CFA pattern. The number of parameters of PyNET-QxQ is less than 2.5% of\nPyNET. We also introduce a novel knowledge distillation technique, progressive\ndistillation, to train the compressed network effectively. Finally, experiments\nwith QxQ images (obtained by an actual QxQ camera sensor, under development)\ndemonstrate the outstanding performance of PyNET-QxQ despite significant\nparameter reductions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Cho_M/0/1/0/all/0/1\">Minhyeok Cho</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_H/0/1/0/all/0/1\">Haechang Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Je_H/0/1/0/all/0/1\">Hyunwoo Je</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_K/0/1/0/all/0/1\">Kijeong Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ryu_D/0/1/0/all/0/1\">Dongil Ryu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_J/0/1/0/all/0/1\">Jinsu Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bae_J/0/1/0/all/0/1\">Jonghyun Bae</a>, <a href=\"http://arxiv.org/find/eess/1/au:+No_A/0/1/0/all/0/1\">Albert No</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MICDIR: Multi-scale Inverse-consistent Deformable Image Registration using UNetMSS with Self-Constructing Graph Latent. (arXiv:2203.04317v1 [eess.IV])","link":"http://arxiv.org/abs/2203.04317","description":"<p>Image registration is the process of bringing different images into a common\ncoordinate system - a technique widely used in various applications of computer\nvision, such as remote sensing, image retrieval, and most commonly in medical\nimaging. Deep Learning based techniques have been applied successfully to\ntackle various complex medical image processing problems, including medical\nimage registration. Over the years, several image registration techniques have\nbeen proposed using deep learning. Deformable image registration techniques\nsuch as Voxelmorph have been successful in capturing finer changes and\nproviding smoother deformations. However, Voxelmorph, as well as ICNet and\nFIRE, do not explicitly encode global dependencies (i.e. the overall anatomical\nview of the supplied image) and therefore can not track large deformations. In\norder to tackle the aforementioned problems, this paper extends the Voxelmorph\napproach in three different ways. To improve the performance in case of small\nas well as large deformations, supervision of the model at different\nresolutions have been integrated using a multi-scale UNet. To support the\nnetwork to learn and encode the minute structural co-relations of the given\nimage-pairs, a self-constructing graph network (SCGNet) has been used as the\nlatent of the multi-scale UNet - which can improve the learning process of the\nmodel and help the model to generalise better. And finally, to make the\ndeformations inverse-consistent, cycle consistency loss has been employed. On\nthe task of registration of brain MRIs, the proposed method achieved\nsignificant improvements over ANTs and VoxelMorph, obtaining a Dice score of\n0.8013$\\pm$0.0243 for intramodal and 0.6211$\\pm$0.0309 for intermodal, while\nVoxelMorph achieved 0.7747$\\pm$0.0260 and 0.6071$\\pm$0.0510, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chatterjee_S/0/1/0/all/0/1\">Soumick Chatterjee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bajaj_H/0/1/0/all/0/1\">Himanshi Bajaj</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Siddiquee_I/0/1/0/all/0/1\">Istiyak H. Siddiquee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Subbarayappa_N/0/1/0/all/0/1\">Nandish Bandi Subbarayappa</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Simon_S/0/1/0/all/0/1\">Steve Simon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shashidhar_S/0/1/0/all/0/1\">Suraj Bangalore Shashidhar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Speck_O/0/1/0/all/0/1\">Oliver Speck</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nurnberge_A/0/1/0/all/0/1\">Andreas N&#xfc;rnberge</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unrolled Primal-Dual Networks for Lensless Cameras. (arXiv:2203.04353v1 [cs.CV])","link":"http://arxiv.org/abs/2203.04353","description":"<p>Conventional image reconstruction models for lensless cameras often assume\nthat each measurement results from convolving a given scene with a single\nexperimentally measured point-spread function. These image reconstruction\nmodels fall short in simulating lensless cameras truthfully as these models are\nnot sophisticated enough to account for optical aberrations or scenes with\ndepth variations. Our work shows that learning a supervised primal-dual\nreconstruction method results in image quality matching state of the art in the\nliterature without demanding a large network capacity. This improvement stems\nfrom our primary finding that embedding learnable forward and adjoint models in\na learned primal-dual optimization framework can even improve the quality of\nreconstructed images (+5dB PSNR) compared to works that do not correct for the\nmodel error. In addition, we built a proof-of-concept lensless camera prototype\nthat uses a pseudo-random phase mask to demonstrate our point. Finally, we\nshare the extensive evaluation of our learned model based on an open dataset\nand a dataset from our proof-of-concept lensless camera prototype.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kingshott_O/0/1/0/all/0/1\">Oliver Kingshott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Antipa_N/0/1/0/all/0/1\">Nick Antipa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bostan_E/0/1/0/all/0/1\">Emrah Bostan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aksit_K/0/1/0/all/0/1\">Kaan Ak&#x15f;it</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ImageNet-Patch: A Dataset for Benchmarking Machine Learning Robustness against Adversarial Patches. (arXiv:2203.04412v1 [cs.CR])","link":"http://arxiv.org/abs/2203.04412","description":"<p>Adversarial patches are optimized contiguous pixel blocks in an input image\nthat cause a machine-learning model to misclassify it. However, their\noptimization is computationally demanding, and requires careful hyperparameter\ntuning, potentially leading to suboptimal robustness evaluations. To overcome\nthese issues, we propose ImageNet-Patch, a dataset to benchmark\nmachine-learning models against adversarial patches. It consists of a set of\npatches, optimized to generalize across different models, and readily\napplicable to ImageNet data after preprocessing them with affine\ntransformations. This process enables an approximate yet faster robustness\nevaluation, leveraging the transferability of adversarial perturbations. We\nshowcase the usefulness of this dataset by testing the effectiveness of the\ncomputed patches against 127 models. We conclude by discussing how our dataset\ncould be used as a benchmark for robustness, and how our methodology can be\ngeneralized to other domains. We open source our dataset and evaluation code at\nhttps://github.com/pralab/ImageNet-Patch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pintor_M/0/1/0/all/0/1\">Maura Pintor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angioni_D/0/1/0/all/0/1\">Daniele Angioni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sotgiu_A/0/1/0/all/0/1\">Angelo Sotgiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demetrio_L/0/1/0/all/0/1\">Luca Demetrio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demontis_A/0/1/0/all/0/1\">Ambra Demontis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biggio_B/0/1/0/all/0/1\">Battista Biggio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roli_F/0/1/0/all/0/1\">Fabio Roli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Flag Median and FlagIRLS. (arXiv:2203.04437v1 [stat.ML])","link":"http://arxiv.org/abs/2203.04437","description":"<p>Finding prototypes (e.g., mean and median) for a dataset is central to a\nnumber of common machine learning algorithms. Subspaces have been shown to\nprovide useful, robust representations for datasets of images, videos and more.\nSince subspaces correspond to points on a Grassmann manifold, one is led to\nconsider the idea of a subspace prototype for a Grassmann-valued dataset. While\na number of different subspace prototypes have been described, the calculation\nof some of these prototypes has proven to be computationally expensive while\nother prototypes are affected by outliers and produce highly imperfect\nclustering on noisy data. This work proposes a new subspace prototype, the flag\nmedian, and introduces the FlagIRLS algorithm for its calculation. We provide\nevidence that the flag median is robust to outliers and can be used effectively\nin algorithms like Linde-Buzo-Grey (LBG) to produce improved clusterings on\nGrassmannians. Numerical experiments include a synthetic dataset, the MNIST\nhandwritten digits dataset, the Mind's Eye video dataset and the UCF YouTube\naction dataset. The flag median is compared the other leading algorithms for\ncomputing prototypes on the Grassmannian, namely, the $\\ell_2$-median and to\nthe flag mean. We find that using FlagIRLS to compute the flag median converges\nin $4$ iterations on a synthetic dataset. We also see that Grassmannian LBG\nwith a codebook size of $20$ and using the flag median produces at least a\n$10\\%$ improvement in cluster purity over Grassmannian LBG using the flag mean\nor $\\ell_2$-median on the Mind's Eye dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Mankovich_N/0/1/0/all/0/1\">Nathan Mankovich</a>, <a href=\"http://arxiv.org/find/stat/1/au:+King_E/0/1/0/all/0/1\">Emily King</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Peterson_C/0/1/0/all/0/1\">Chris Peterson</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kirby_M/0/1/0/all/0/1\">Michael Kirby</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pointillism: Accurate 3D bounding box estimation with multi-radars. (arXiv:2203.04440v1 [cs.CV])","link":"http://arxiv.org/abs/2203.04440","description":"<p>Autonomous perception requires high-quality environment sensing in the form\nof 3D bounding boxes of dynamic objects. The primary sensors used in automotive\nsystems are light-based cameras and LiDARs. However, they are known to fail in\nadverse weather conditions. Radars can potentially solve this problem as they\nare barely affected by adverse weather conditions. However, specular\nreflections of wireless signals cause poor performance of radar point clouds.\nWe introduce Pointillism, a system that combines data from multiple spatially\nseparated radars with an optimal separation to mitigate these problems. We\nintroduce a novel concept of Cross Potential Point Clouds, which uses the\nspatial diversity induced by multiple radars and solves the problem of noise\nand sparsity in radar point clouds. Furthermore, we present the design of\nRP-net, a novel deep learning architecture, designed explicitly for radar's\nsparse data distribution, to enable accurate 3D bounding box estimation. The\nspatial techniques designed and proposed in this paper are fundamental to\nradars point cloud distribution and would benefit other radar sensing\napplications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bansal_K/0/1/0/all/0/1\">Kshitiz Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rungta_K/0/1/0/all/0/1\">Keshav Rungta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Siyuan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bharadia_D/0/1/0/all/0/1\">Dinesh Bharadia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervision, Remote Sensing and Abstraction: Representation Learning Across 3 Million Locations. (arXiv:2203.04445v1 [cs.CV])","link":"http://arxiv.org/abs/2203.04445","description":"<p>Self-supervision based deep learning classification approaches have received\nconsiderable attention in academic literature. However, the performance of such\nmethods on remote sensing imagery domains remains under-explored. In this work,\nwe explore contrastive representation learning methods on the task of\nimagery-based city classification, an important problem in urban computing. We\nuse satellite and map imagery across 2 domains, 3 million locations and more\nthan 1500 cities. We show that self-supervised methods can build a\ngeneralizable representation from as few as 200 cities, with representations\nachieving over 95\\% accuracy in unseen cities with minimal additional training.\nWe also find that the performance discrepancy of such methods, when compared to\nsupervised methods, induced by the domain discrepancy between natural imagery\nand abstract imagery is significant for remote sensing imagery. We compare all\nanalysis against existing supervised models from academic literature and\nopen-source our models for broader usage and further criticism.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Seneviratne_S/0/1/0/all/0/1\">Sachith Seneviratne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nice_K/0/1/0/all/0/1\">Kerry A. Nice</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wijnands_J/0/1/0/all/0/1\">Jasper S. Wijnands</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stevenson_M/0/1/0/all/0/1\">Mark Stevenson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thompson_J/0/1/0/all/0/1\">Jason Thompson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tune your Place Recognition: Self-Supervised Domain Calibration via Robust SLAM. (arXiv:2203.04446v1 [cs.CV])","link":"http://arxiv.org/abs/2203.04446","description":"<p>Visual place recognition techniques based on deep learning, which have\nimposed themselves as the state-of-the-art in recent years, do not always\ngeneralize well to environments that are visually different from the training\nset. Thus, to achieve top performance, it is sometimes necessary to fine-tune\nthe networks to the target environment. To this end, we propose a completely\nself-supervised domain calibration procedure based on robust pose graph\nestimation from Simultaneous Localization and Mapping (SLAM) as the supervision\nsignal without requiring GPS or manual labeling. We first show that the\ntraining samples produced by our technique are sufficient to train a visual\nplace recognition system from a pre-trained classification model. Then, we show\nthat our approach can improve the performance of a state-of-the-art technique\non a target environment dissimilar from the training set. We believe that this\napproach will help practitioners to deploy more robust place recognition\nsolutions in real-world applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lajoie_P/0/1/0/all/0/1\">Pierre-Yves Lajoie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beltrame_G/0/1/0/all/0/1\">Giovanni Beltrame</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CIDER: Exploiting Hyperspherical Embeddings for Out-of-Distribution Detection. (arXiv:2203.04450v1 [cs.CV])","link":"http://arxiv.org/abs/2203.04450","description":"<p>Out-of-distribution (OOD) detection is a critical task for reliable machine\nlearning. Recent advances in representation learning give rise to developments\nin distance-based OOD detection, where testing samples are detected as OOD if\nthey are relatively far away from the centroids or prototypes of\nin-distribution (ID) classes. However, prior methods directly take\noff-the-shelf loss functions that suffice for classifying ID samples, but are\nnot optimally designed for OOD detection. In this paper, we propose CIDER, a\nsimple and effective representation learning framework by exploiting\nhyperspherical embeddings for OOD detection. CIDER jointly optimizes two losses\nto promote strong ID-OOD separability: (1) a dispersion loss that promotes\nlarge angular distances among different class prototypes, and (2) a compactness\nloss that encourages samples to be close to their class prototypes. We show\nthat CIDER is effective under various settings and establishes state-of-the-art\nperformance. On a hard OOD detection task CIFAR-100 vs. CIFAR-10, our method\nsubstantially improves the AUROC by 14.20% compared to the embeddings learned\nby the cross-entropy loss.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ming_Y/0/1/0/all/0/1\">Yifei Ming</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yiyou Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dia_O/0/1/0/all/0/1\">Ousmane Dia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yixuan Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Probabilistic Rotation Representation With an Efficiently Computable Bingham Loss Function and Its Application to Pose Estimation. (arXiv:2203.04456v1 [cs.CV])","link":"http://arxiv.org/abs/2203.04456","description":"<p>In recent years, a deep learning framework has been widely used for object\npose estimation. While quaternion is a common choice for rotation\nrepresentation of 6D pose, it cannot represent an uncertainty of the\nobservation. In order to handle the uncertainty, Bingham distribution is one\npromising solution because this has suitable features, such as a smooth\nrepresentation over SO(3), in addition to the ambiguity representation.\nHowever, it requires the complex computation of the normalizing constants. This\nis the bottleneck of loss computation in training neural networks based on\nBingham representation. As such, we propose a fast-computable and\neasy-to-implement loss function for Bingham distribution. We also show not only\nto examine the parametrization of Bingham distribution but also an application\nbased on our loss function.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sato_H/0/1/0/all/0/1\">Hiroya Sato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ikeda_T/0/1/0/all/0/1\">Takuya Ikeda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nishiwaki_K/0/1/0/all/0/1\">Koichi Nishiwaki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Autonomous Mosquito Habitat Detection Using Satellite Imagery and Convolutional Neural Networks for Disease Risk Mapping. (arXiv:2203.04463v1 [cs.CV])","link":"http://arxiv.org/abs/2203.04463","description":"<p>Mosquitoes are known vectors for disease transmission that cause over one\nmillion deaths globally each year. The majority of natural mosquito habitats\nare areas containing standing water that are challenging to detect using\nconventional ground-based technology on a macro scale. Contemporary approaches,\nsuch as drones, UAVs, and other aerial imaging technology are costly when\nimplemented and are only most accurate on a finer spatial scale whereas the\nproposed convolutional neural network(CNN) approach can be applied for disease\nrisk mapping and further guide preventative efforts on a more global scale. By\nassessing the performance of autonomous mosquito habitat detection technology,\nthe transmission of mosquito-borne diseases can be prevented in a\ncost-effective manner. This approach aims to identify the spatiotemporal\ndistribution of mosquito habitats in extensive areas that are difficult to\nsurvey using ground-based technology by employing computer vision on satellite\nimagery for proof of concept. The research presents an evaluation and the\nresults of 3 different CNN models to determine their accuracy of predicting\nlarge-scale mosquito habitats. For this approach, a dataset was constructed\ncontaining a variety of geographical features. Larger land cover variables such\nas ponds/lakes, inlets, and rivers were utilized to classify mosquito habitats\nwhile minute sites were omitted for higher accuracy on a larger scale. Using\nthe dataset, multiple CNN networks were trained and evaluated for accuracy of\nhabitat prediction. Utilizing a CNN-based approach on readily available\nsatellite imagery is cost-effective and scalable, unlike most aerial imaging\ntechnology. Testing revealed that YOLOv4 obtained greater accuracy in mosquito\nhabitat detection for identifying large-scale mosquito habitats.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Elango_S/0/1/0/all/0/1\">Sriram Elango</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramachandran_N/0/1/0/all/0/1\">Nandini Ramachandran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Low_R/0/1/0/all/0/1\">Russanne Low</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Combinatorial Brain Surgeon: Pruning Weights That Cancel One Another in Neural Networks. (arXiv:2203.04466v1 [cs.LG])","link":"http://arxiv.org/abs/2203.04466","description":"<p>Neural networks tend to achieve better accuracy with training if they are\nlarger -- even if the resulting models are overparameterized. Nevertheless,\ncarefully removing such excess parameters before, during, or after training may\nalso produce models with similar or even improved accuracy. In many cases, that\ncan be curiously achieved by heuristics as simple as removing a percentage of\nthe weights with the smallest absolute value -- even though magnitude is not a\nperfect proxy for weight relevance. With the premise that obtaining\nsignificantly better performance from pruning depends on accounting for the\ncombined effect of removing multiple weights, we revisit one of the classic\napproaches for impact-based pruning: the Optimal Brain Surgeon~(OBS). We\npropose a tractable heuristic for solving the combinatorial extension of OBS,\nin which we select weights for simultaneous removal, as well as a systematic\nupdate of the remaining weights. Our selection method outperforms other methods\nunder high sparsity, and the weight update is advantageous even when combined\nwith the other methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Serra_T/0/1/0/all/0/1\">Thiago Serra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhe_S/0/1/0/all/0/1\">Shandian Zhe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramalingam_S/0/1/0/all/0/1\">Srikumar Ramalingam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Part-level Action Parsing via a Pose-guided Coarse-to-Fine Framework. (arXiv:2203.04476v1 [cs.CV])","link":"http://arxiv.org/abs/2203.04476","description":"<p>Action recognition from videos, i.e., classifying a video into one of the\npre-defined action types, has been a popular topic in the communities of\nartificial intelligence, multimedia, and signal processing. However, existing\nmethods usually consider an input video as a whole and learn models, e.g.,\nConvolutional Neural Networks (CNNs), with coarse video-level class labels.\nThese methods can only output an action class for the video, but cannot provide\nfine-grained and explainable cues to answer why the video shows a specific\naction. Therefore, researchers start to focus on a new task, Part-level Action\nParsing (PAP), which aims to not only predict the video-level action but also\nrecognize the frame-level fine-grained actions or interactions of body parts\nfor each person in the video. To this end, we propose a coarse-to-fine\nframework for this challenging task. In particular, our framework first\npredicts the video-level class of the input video, then localizes the body\nparts and predicts the part-level action. Moreover, to balance the accuracy and\ncomputation in part-level action parsing, we propose to recognize the\npart-level actions by segment-level features. Furthermore, to overcome the\nambiguity of body parts, we propose a pose-guided positional embedding method\nto accurately localize body parts. Through comprehensive experiments on a\nlarge-scale dataset, i.e., Kinetics-TPS, our framework achieves\nstate-of-the-art performance and outperforms existing methods over a 31.10% ROC\nscore.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaodong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xinchen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Dong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongdong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_T/0/1/0/all/0/1\">Tao Mei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3SD: Self-Supervised Saliency Detection With No Labels. (arXiv:2203.04478v1 [cs.CV])","link":"http://arxiv.org/abs/2203.04478","description":"<p>We present a conceptually simple self-supervised method for saliency\ndetection. Our method generates and uses pseudo-ground truth labels for\ntraining. The generated pseudo-GT labels don't require any kind of human\nannotations (e.g., pixel-wise labels or weak labels like scribbles). Recent\nworks show that features extracted from classification tasks provide important\nsaliency cues like structure and semantic information of salient objects in the\nimage. Our method, called 3SD, exploits this idea by adding a branch for a\nself-supervised classification task in parallel with salient object detection,\nto obtain class activation maps (CAM maps). These CAM maps along with the edges\nof the input image are used to generate the pseudo-GT saliency maps to train\nour 3SD network. Specifically, we propose a contrastive learning-based training\non multiple image patches for the classification task. We show the multi-patch\nclassification with contrastive loss improves the quality of the CAM maps\ncompared to naive classification on the entire image. Experiments on six\nbenchmark datasets demonstrate that without any labels, our 3SD method\noutperforms all existing weakly supervised and unsupervised methods, and its\nperformance is on par with the fully-supervised methods. Code is available at\n:https://github.com/rajeevyasarla/3SD\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yasarla_R/0/1/0/all/0/1\">Rajeev Yasarla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weng_R/0/1/0/all/0/1\">Renliang Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_W/0/1/0/all/0/1\">Wongun Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1\">Vishal Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadeghian_A/0/1/0/all/0/1\">Amir Sadeghian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image Steganography based on Style Transfer. (arXiv:2203.04500v1 [cs.CV])","link":"http://arxiv.org/abs/2203.04500","description":"<p>Image steganography is the art and science of using images as cover for\ncovert communications. With the development of neural networks, traditional\nimage steganography is more likely to be detected by deep learning-based\nsteganalysis. To improve upon this, we propose image steganography network\nbased on style transfer, and the embedding of secret messages can be disguised\nas image stylization. We embed secret information while transforming the\ncontent image style. In latent space, the secret information is integrated into\nthe latent representation of the cover image to generate the stego images,\nwhich are indistinguishable from normal stylized images. It is an end-to-end\nunsupervised model without pre-training. Extensive experiments on the benchmark\ndataset demonstrate the reliability, quality and security of stego images\ngenerated by our steganographic network.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1\">Donghui Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1\">Cong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaofei Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Update Compression for Deep Neural Networks on the Edge. (arXiv:2203.04516v1 [cs.CV])","link":"http://arxiv.org/abs/2203.04516","description":"<p>An increasing number of artificial intelligence (AI) applications involve the\nexecution of deep neural networks (DNNs) on edge devices. Many practical\nreasons motivate the need to update the DNN model on the edge device\npost-deployment, such as refining the model, concept drift, or outright change\nin the learning task. In this paper, we consider the scenario where retraining\ncan be done on the server side based on a copy of the DNN model, with only the\nnecessary data transmitted to the edge to update the deployed model. However,\ndue to bandwidth constraints, we want to minimise the transmission required to\nachieve the update. We develop a simple approach based on matrix factorisation\nto compress the model update -- this differs from compressing the model itself.\nThe key idea is to preserve existing knowledge in the current model and\noptimise only small additional parameters for the update which can be used to\nreconstitute the model on the edge. We compared our method to similar\ntechniques used in federated learning; our method usually requires less than\nhalf of the update size of existing methods to achieve the same accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bakhshi_A/0/1/0/all/0/1\">Ali Bakhshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batista_G/0/1/0/all/0/1\">Gustavo Batista</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_B/0/1/0/all/0/1\">Brian Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chin_T/0/1/0/all/0/1\">Tat-Jun Chin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast Road Segmentation via Uncertainty-aware Symmetric Network. (arXiv:2203.04537v1 [cs.CV])","link":"http://arxiv.org/abs/2203.04537","description":"<p>The high performance of RGB-D based road segmentation methods contrasts with\ntheir rare application in commercial autonomous driving, which is owing to two\nreasons: 1) the prior methods cannot achieve high inference speed and high\naccuracy in both ways; 2) the different properties of RGB and depth data are\nnot well-exploited, limiting the reliability of predicted road. In this paper,\nbased on the evidence theory, an uncertainty-aware symmetric network (USNet) is\nproposed to achieve a trade-off between speed and accuracy by fully fusing RGB\nand depth data. Firstly, cross-modal feature fusion operations, which are\nindispensable in the prior RGB-D based methods, are abandoned. We instead\nseparately adopt two light-weight subnetworks to learn road representations\nfrom RGB and depth inputs. The light-weight structure guarantees the real-time\ninference of our method. Moreover, a multiscale evidence collection (MEC)\nmodule is designed to collect evidence in multiple scales for each modality,\nwhich provides sufficient evidence for pixel class determination. Finally, in\nuncertainty-aware fusion (UAF) module, the uncertainty of each modality is\nperceived to guide the fusion of the two subnetworks. Experimental results\ndemonstrate that our method achieves a state-of-the-art accuracy with real-time\ninference speed of 43+ FPS. The source code is available at\nhttps://github.com/morancyc/USNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yicong Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_F/0/1/0/all/0/1\">Feng Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheng_F/0/1/0/all/0/1\">Fei Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_W/0/1/0/all/0/1\">Wenteng Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ming_A/0/1/0/all/0/1\">Anlong Ming</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Monocular Depth Distribution Alignment with Low Computation. (arXiv:2203.04538v1 [cs.CV])","link":"http://arxiv.org/abs/2203.04538","description":"<p>The performance of monocular depth estimation generally depends on the amount\nof parameters and computational cost. It leads to a large accuracy contrast\nbetween light-weight networks and heavy-weight networks, which limits their\napplication in the real world. In this paper, we model the majority of accuracy\ncontrast between them as the difference of depth distribution, which we call\n\"Distribution drift\". To this end, a distribution alignment network (DANet) is\nproposed. We firstly design a pyramid scene transformer (PST) module to capture\ninter-region interaction in multiple scales. By perceiving the difference of\ndepth features between every two regions, DANet tends to predict a reasonable\nscene structure, which fits the shape of distribution to ground truth. Then, we\npropose a local-global optimization (LGO) scheme to realize the supervision of\nglobal range of scene depth. Thanks to the alignment of depth distribution\nshape and scene depth range, DANet sharply alleviates the distribution drift,\nand achieves a comparable performance with prior heavy-weight methods, but uses\nonly 1% floating-point operations per second (FLOPs) of them. The experiments\non two datasets, namely the widely used NYUDv2 dataset and the more challenging\niBims-1 dataset, demonstrate the effectiveness of our method. The source code\nis available at https://github.com/YiLiM1/DANet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sheng_F/0/1/0/all/0/1\">Fei Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_F/0/1/0/all/0/1\">Feng Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yicong Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_W/0/1/0/all/0/1\">Wenteng Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ming_A/0/1/0/all/0/1\">Anlong Ming</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChiTransformer:Towards Reliable Stereo from Cues. (arXiv:2203.04554v1 [cs.CV])","link":"http://arxiv.org/abs/2203.04554","description":"<p>Current stereo matching techniques are challenged by restricted searching\nspace, occluded regions, and sheer size. While single image depth estimation is\nspared from these challenges and can achieve satisfactory results with the\nextracted monocular cues, the lack of stereoscopic relationship renders the\nmonocular prediction less reliable on its own, especially in highly dynamic or\ncluttered environments. To address these issues in both scenarios, we present\nan optic-chiasm-inspired self-supervised binocular depth estimation method,\nwherein vision transformer (ViT) with a gated positional cross-attention (GPCA)\nlayer is designed to enable feature-sensitive pattern retrieval between views\nwhile retaining the extensive context information aggregated through\nself-attentions. Monocular cues from a single view are thereafter conditionally\nrectified by a blending layer with the retrieved pattern pairs. This crossover\ndesign is biologically analogous to the optic-chasma structure in human visual\nsystem and hence the name, ChiTransformer. Our experiments show that this\narchitecture yields substantial improvements over state-of-the-art\nself-supervised stereo approaches by 11%, and can be used on both rectilinear\nand non-rectilinear (e.g., fisheye) images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_Q/0/1/0/all/0/1\">Qing Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shihao Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Temporal Consistency for Source-Free Video Domain Adaptation. (arXiv:2203.04559v1 [cs.CV])","link":"http://arxiv.org/abs/2203.04559","description":"<p>Video-based Unsupervised Domain Adaptation (VUDA) methods improve the\nrobustness of video models, enabling them to be applied to action recognition\ntasks across different environments. However, these methods require constant\naccess to source data during the adaptation process. Yet in many real-world\napplications, subjects and scenes in the source video domain should be\nirrelevant to those in the target video domain. With the increasing emphasis on\ndata privacy, such methods that require source data access would raise serious\nprivacy issues. Therefore, to cope with such concern, a more practical domain\nadaptation scenario is formulated as the Source-Free Video-based Domain\nAdaptation (SFVDA). Though there are a few methods for Source-Free Domain\nAdaptation (SFDA) on image data, these methods yield degenerating performance\nin SFVDA due to the multi-modality nature of videos, with the existence of\nadditional temporal features. In this paper, we propose a novel Attentive\nTemporal Consistent Network (ATCoN) to address SFVDA by learning temporal\nconsistency, guaranteed by two novel consistency objectives, namely feature\nconsistency and source prediction consistency, performed across local temporal\nfeatures. ATCoN further constructs effective overall temporal features by\nattending to local temporal features based on prediction confidence. Empirical\nresults demonstrate the state-of-the-art performance of ATCoN across various\ncross-domain action recognition benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yuecong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianfei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_H/0/1/0/all/0/1\">Haozhi Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1\">Keyu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_W/0/1/0/all/0/1\">Wu Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhenghua Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Active Self-Semi-Supervised Learning for Few Labeled Samples Fast Training. (arXiv:2203.04560v1 [cs.CV])","link":"http://arxiv.org/abs/2203.04560","description":"<p>Faster training and fewer annotations are two key issues for applying deep\nmodels to various practical domains. Now, semi-supervised learning has achieved\ngreat success in training with few annotations. However, low-quality labeled\nsamples produced by random sampling make it difficult to continue to reduce the\nnumber of annotations. In this paper we propose an active self-semi-supervised\ntraining framework that bootstraps semi-supervised models with good prior\npseudo-labels, where the priors are obtained by label propagation over\nself-supervised features. Because the accuracy of the prior is not only\naffected by the quality of features, but also by the selection of the labeled\nsamples. We develop active learning and label propagation strategies to obtain\nbetter prior pseudo-labels. Consequently, our framework can greatly improve the\nperformance of models with few annotations and greatly reduce the training\ntime. Experiments on three semi-supervised learning benchmarks demonstrate\neffectiveness. Our method achieves similar accuracy to standard semi-supervised\napproaches in about 1/3 of the training time, and even outperform them when\nfewer annotations are available (84.10\\% in CIFAR-10 with 10 labels).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wen_Z/0/1/0/all/0/1\">Ziting Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pizarro_O/0/1/0/all/0/1\">Oscar Pizarro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_S/0/1/0/all/0/1\">Stefan Williams</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MLNav: Learning to Safely Navigate on Martian Terrains. (arXiv:2203.04563v1 [cs.RO])","link":"http://arxiv.org/abs/2203.04563","description":"<p>We present MLNav, a learning-enhanced path planning framework for\nsafety-critical and resource-limited systems operating in complex environments,\nsuch as rovers navigating on Mars. MLNav makes judicious use of machine\nlearning to enhance the efficiency of path planning while fully respecting\nsafety constraints. In particular, the dominant computational cost in such\nsafety-critical settings is running a model-based safety checker on the\nproposed paths. Our learned search heuristic can simultaneously predict the\nfeasibility for all path options in a single run, and the model-based safety\nchecker is only invoked on the top-scoring paths. We validate in high-fidelity\nsimulations using both real Martian terrain data collected by the Perseverance\nrover, as well as a suite of challenging synthetic terrains. Our experiments\nshow that: (i) compared to the baseline ENav path planner on board the\nPerserverance rover, MLNav can provide a significant improvement in multiple\nkey metrics, such as a 10x reduction in collision checks when navigating real\nMartian terrains, despite being trained with synthetic terrains; and (ii) MLNav\ncan successfully navigate highly challenging terrains where the baseline ENav\nfails to find a feasible path before timing out.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Daftry_S/0/1/0/all/0/1\">Shreyansh Daftry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abcouwer_N/0/1/0/all/0/1\">Neil Abcouwer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sesto_T/0/1/0/all/0/1\">Tyler Del Sesto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkatraman_S/0/1/0/all/0/1\">Siddarth Venkatraman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jialin Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Igel_L/0/1/0/all/0/1\">Lucas Igel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Byon_A/0/1/0/all/0/1\">Amos Byon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosolia_U/0/1/0/all/0/1\">Ugo Rosolia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_Y/0/1/0/all/0/1\">Yisong Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ono_M/0/1/0/all/0/1\">Masahiro Ono</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Region-Aware Face Swapping. (arXiv:2203.04564v1 [cs.CV])","link":"http://arxiv.org/abs/2203.04564","description":"<p>This paper presents a novel Region-Aware Face Swapping (RAFSwap) network to\nachieve identity-consistent harmonious high-resolution face generation in a\nlocal-global manner: \\textbf{1)} Local Facial Region-Aware (FRA) branch\naugments local identity-relevant features by introducing the Transformer to\neffectively model misaligned cross-scale semantic interaction. \\textbf{2)}\nGlobal Source Feature-Adaptive (SFA) branch further complements global\nidentity-relevant cues for generating identity-consistent swapped faces.\nBesides, we propose a \\textit{Face Mask Predictor} (FMP) module incorporated\nwith StyleGAN2 to predict identity-relevant soft facial masks in an\nunsupervised manner that is more practical for generating harmonious\nhigh-resolution faces. Abundant experiments qualitatively and quantitatively\ndemonstrate the superiority of our method for generating more\nidentity-consistent high-resolution swapped faces over SOTA methods, \\eg,\nobtaining 96.70 ID retrieval that outperforms SOTA MegaFS by 5.87$\\uparrow$.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiangning Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_M/0/1/0/all/0/1\">Miao Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1\">Qian He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_Z/0/1/0/all/0/1\">Zili Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"All You Need is LUV: Unsupervised Collection of Labeled Images using Invisible UV Fluorescent Indicators. (arXiv:2203.04566v1 [cs.CV])","link":"http://arxiv.org/abs/2203.04566","description":"<p>Large-scale semantic image annotation is a significant challenge for\nlearning-based perception systems in robotics. Current approaches often rely on\nhuman labelers, which can be expensive, or simulation data, which can visually\nor physically differ from real data. This paper proposes Labels from\nUltraViolet (LUV), a novel framework that enables rapid, labeled data\ncollection in real manipulation environments without human labeling. LUV uses\ntransparent, ultraviolet-fluorescent paint with programmable ultraviolet LEDs\nto collect paired images of a scene in standard lighting and UV lighting to\nautonomously extract segmentation masks and keypoints via color segmentation.\nWe apply LUV to a suite of diverse robot perception tasks to evaluate its\nlabeling quality, flexibility, and data collection rate. Results suggest that\nLUV is 180-2500 times faster than a human labeler across the tasks. We show\nthat LUV provides labels consistent with human annotations on unpainted test\nimages. The networks trained on these labels are used to smooth and fold\ncrumpled towels with 83% success rate and achieve 1.7mm position error with\nrespect to human labels on a surgical needle pose estimation task. The low cost\nof LUV makes it ideal as a lightweight replacement for human labeling systems,\nwith the one-time setup costs at $300 equivalent to the cost of collecting\naround 200 semantic segmentation labels on Amazon Mechanical Turk. Code,\ndatasets, visualizations, and supplementary material can be found at\nhttps://sites.google.com/berkeley.edu/luv\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thananjeyan_B/0/1/0/all/0/1\">Brijen Thananjeyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kerr_J/0/1/0/all/0/1\">Justin Kerr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Huang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1\">Joseph E. Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_K/0/1/0/all/0/1\">Ken Goldberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PHTrans: Parallelly Aggregating Global and Local Representations for Medical Image Segmentation. (arXiv:2203.04568v1 [eess.IV])","link":"http://arxiv.org/abs/2203.04568","description":"<p>The success of Transformer in computer vision has attracted increasing\nattention in the medical imaging community. Especially for medical image\nsegmentation, many excellent hybrid architectures based on convolutional neural\nnetworks (CNNs) and Transformer have been presented and achieve impressive\nperformance. However, most of these methods, which embed modular Transformer\ninto CNNs, struggle to reach their full potential. In this paper, we propose a\nnovel hybrid architecture for medical image segmentation called PHTrans, which\nparallelly hybridizes Transformer and CNN in main building blocks to produce\nhierarchical representations from global and local features and adaptively\naggregate them, aiming to fully exploit their strengths to obtain better\nsegmentation performance. Specifically, PHTrans follows the U-shaped\nencoder-decoder design and introduces the parallel hybird module in deep\nstages, where convolution blocks and the modified 3D Swin Transformer learn\nlocal features and global dependencies separately, then a sequence-to-volume\noperation unifies the dimensions of the outputs to achieve feature aggregation.\nExtensive experimental results on both Multi-Atlas Labeling Beyond the Cranial\nVault and Automated Cardiac Diagnosis Challeng datasets corroborate its\neffectiveness, consistently outperforming state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Liu_W/0/1/0/all/0/1\">Wentao Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tian_T/0/1/0/all/0/1\">Tong Tian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_W/0/1/0/all/0/1\">Weijin Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_H/0/1/0/all/0/1\">Huihua Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pan_X/0/1/0/all/0/1\">Xipeng Pan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CP-ViT: Cascade Vision Transformer Pruning via Progressive Sparsity Prediction. (arXiv:2203.04570v1 [cs.CV])","link":"http://arxiv.org/abs/2203.04570","description":"<p>Vision transformer (ViT) has achieved competitive accuracy on a variety of\ncomputer vision applications, but its computational cost impedes the deployment\non resource-limited mobile devices.\n</p>\n<p>We explore the sparsity in ViT and observe that informative patches and heads\nare sufficient for accurate image recognition.\n</p>\n<p>In this paper, we propose a cascade pruning framework named CP-ViT by\npredicting sparsity in ViT models progressively and dynamically to reduce\ncomputational redundancy while minimizing the accuracy loss. Specifically, we\ndefine the cumulative score to reserve the informative patches and heads across\nthe ViT model for better accuracy. We also propose the dynamic pruning ratio\nadjustment technique based on layer-aware attention range. CP-ViT has great\ngeneral applicability for practical deployment, which can be applied to a wide\nrange of ViT models and can achieve superior accuracy with or without\nfine-tuning.\n</p>\n<p>Extensive experiments on ImageNet, CIFAR-10, and CIFAR-100 with various\npre-trained models have demonstrated the effectiveness and efficiency of\nCP-ViT. By progressively pruning 50\\% patches, our CP-ViT method reduces over\n40\\% FLOPs while maintaining accuracy loss within 1\\%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1\">Zhuoran Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yihong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhezhi He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Li Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jing_N/0/1/0/all/0/1\">Naifeng Jing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaoyao Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Neuro-vector-symbolic Architecture for Solving Raven's Progressive Matrices. (arXiv:2203.04571v1 [cs.LG])","link":"http://arxiv.org/abs/2203.04571","description":"<p>Neither deep neural networks nor symbolic AI alone have approached the kind\nof intelligence expressed in humans. This is mainly because neural networks are\nnot able to decompose distinct objects from their joint representation (the\nso-called binding problem), while symbolic AI suffers from exhaustive rule\nsearches, among other problems. These two problems are still pronounced in\nneuro-symbolic AI which aims to combine the best of the two paradigms. Here, we\nshow that the two problems can be addressed with our proposed\nneuro-vector-symbolic architecture (NVSA) by exploiting its powerful operators\non fixed-width holographic vectorized representations that serve as a common\nlanguage between neural networks and symbolic logical reasoning. The efficacy\nof NVSA is demonstrated by solving the Raven's progressive matrices. NVSA\nachieves a new record of 97.7% average accuracy in RAVEN, and 98.8% in I-RAVEN\ndatasets, with two orders of magnitude faster execution than the symbolic\nlogical reasoning on CPUs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hersche_M/0/1/0/all/0/1\">Michael Hersche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeqiri_M/0/1/0/all/0/1\">Mustafa Zeqiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benini_L/0/1/0/all/0/1\">Luca Benini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebastian_A/0/1/0/all/0/1\">Abu Sebastian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahimi_A/0/1/0/all/0/1\">Abbas Rahimi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-modal Brain Tumor Segmentation via Missing Modality Synthesis and Modality-level Attention Fusion. (arXiv:2203.04586v1 [eess.IV])","link":"http://arxiv.org/abs/2203.04586","description":"<p>Multi-modal magnetic resonance (MR) imaging provides great potential for\ndiagnosing and analyzing brain gliomas. In clinical scenarios, common MR\nsequences such as T1, T2 and FLAIR can be obtained simultaneously in a single\nscanning process. However, acquiring contrast enhanced modalities such as T1ce\nrequires additional time, cost, and injection of contrast agent. As such, it is\nclinically meaningful to develop a method to synthesize unavailable modalities\nwhich can also be used as additional inputs to downstream tasks (e.g., brain\ntumor segmentation) for performance enhancing. In this work, we propose an\nend-to-end framework named Modality-Level Attention Fusion Network (MAF-Net),\nwherein we innovatively conduct patchwise contrastive learning for extracting\nmulti-modal latent features and dynamically assigning attention weights to fuse\ndifferent modalities. Through extensive experiments on BraTS2020, our proposed\nMAF-Net is found to yield superior T1ce synthesis performance (SSIM of 0.8879\nand PSNR of 22.78) and accurate brain tumor segmentation (mean Dice scores of\n67.9%, 41.8% and 88.0% on segmenting the tumor core, enhancing tumor and whole\ntumor).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Huang_Z/0/1/0/all/0/1\">Ziqi Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_L/0/1/0/all/0/1\">Li Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheng_P/0/1/0/all/0/1\">Pujin Cheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Peng_L/0/1/0/all/0/1\">Linkai Peng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tang_X/0/1/0/all/0/1\">Xiaoying Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mapping global dynamics of benchmark creation and saturation in artificial intelligence. (arXiv:2203.04592v1 [cs.AI])","link":"http://arxiv.org/abs/2203.04592","description":"<p>Benchmarks are crucial to measuring and steering progress in artificial\nintelligence (AI). However, recent studies raised concerns over the state of AI\nbenchmarking, reporting issues such as benchmark overfitting, benchmark\nsaturation and increasing centralization of benchmark dataset creation. To\nfacilitate monitoring of the health of the AI benchmarking ecosystem, we\nintroduce methodologies for creating condensed maps of the global dynamics of\nbenchmark creation and saturation. We curated data for 1688 benchmarks covering\nthe entire domains of computer vision and natural language processing, and show\nthat a large fraction of benchmarks quickly trended towards near-saturation,\nthat many benchmarks fail to find widespread utilization, and that benchmark\nperformance gains for different AI tasks were prone to unforeseen bursts. We\nconclude that future work should focus on large-scale community collaboration\nand on mapping benchmark performance gains to real-world utility and impact of\nAI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Barbosa_Silva_A/0/1/0/all/0/1\">Adriano Barbosa-Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ott_S/0/1/0/all/0/1\">Simon Ott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blagec_K/0/1/0/all/0/1\">Kathrin Blagec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brauner_J/0/1/0/all/0/1\">Jan Brauner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samwald_M/0/1/0/all/0/1\">Matthias Samwald</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain Generalization using Pretrained Models without Fine-tuning. (arXiv:2203.04600v1 [cs.CV])","link":"http://arxiv.org/abs/2203.04600","description":"<p>Fine-tuning pretrained models is a common practice in domain generalization\n(DG) tasks. However, fine-tuning is usually computationally expensive due to\nthe ever-growing size of pretrained models. More importantly, it may cause\nover-fitting on source domain and compromise their generalization ability as\nshown in recent works. Generally, pretrained models possess some level of\ngeneralization ability and can achieve decent performance regarding specific\ndomains and samples. However, the generalization performance of pretrained\nmodels could vary significantly over different test domains even samples, which\nraises challenges for us to best leverage pretrained models in DG tasks. In\nthis paper, we propose a novel domain generalization paradigm to better\nleverage various pretrained models, named specialized ensemble learning for\ndomain generalization (SEDGE). It first trains a linear label space adapter\nupon fixed pretrained models, which transforms the outputs of the pretrained\nmodel to the label space of the target domain. Then, an ensemble network aware\nof model specialty is proposed to dynamically dispatch proper pretrained models\nto predict each test sample. Experimental studies on several benchmarks show\nthat SEDGE achieves significant performance improvements comparing to strong\nbaselines including state-of-the-art method in DG tasks and reduces the\ntrainable parameters by ~99% and the training time by ~99.5%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Ziyue Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_K/0/1/0/all/0/1\">Kan Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xinyang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haipeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dongsheng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attention-effective multiple instance learning on weakly stem cell colony segmentation. (arXiv:2203.04606v1 [eess.IV])","link":"http://arxiv.org/abs/2203.04606","description":"<p>The detection of induced pluripotent stem cell (iPSC) colonies often needs\nthe precise extraction of the colony features. However, existing computerized\nsystems relied on segmentation of contours by preprocessing for classifying the\ncolony conditions were task-extensive. To maximize the efficiency in\ncategorizing colony conditions, we propose a multiple instance learning (MIL)\nin weakly supervised settings. It is designed in a single model to produce weak\nsegmentation and classification of colonies without using finely labeled\nsamples. As a single model, we employ a U-net-like convolution neural network\n(CNN) to train on binary image-level labels for MIL colonies classification.\nFurthermore, to specify the object of interest we used a simple post-processing\nmethod. The proposed approach is compared over conventional methods using\nfive-fold cross-validation and receiver operating characteristic (ROC) curve.\nThe maximum accuracy of the MIL-net is 95%, which is 15 % higher than the\nconventional methods. Furthermore, the ability to interpret the location of the\niPSC colonies based on the image level label without using a pixel-wise ground\ntruth image is more appealing and cost-effective in colony condition\nrecognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yudistira_N/0/1/0/all/0/1\">Novanto Yudistira</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kavitha_M/0/1/0/all/0/1\">Muthu Subash Kavitha</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rajan_J/0/1/0/all/0/1\">Jeny Rajan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kurita_T/0/1/0/all/0/1\">Takio Kurita</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Practical No-box Adversarial Attacks with Training-free Hybrid Image Transformation. (arXiv:2203.04607v1 [cs.CV])","link":"http://arxiv.org/abs/2203.04607","description":"<p>In recent years, the adversarial vulnerability of deep neural networks (DNNs)\nhas raised increasing attention. Among all the threat models, no-box attacks\nare the most practical but extremely challenging since they neither rely on any\nknowledge of the target model or similar substitute model, nor access the\ndataset for training a new substitute model. Although a recent method has\nattempted such an attack in a loose sense, its performance is not good enough\nand computational overhead of training is expensive. In this paper, we move a\nstep forward and show the existence of a \\textbf{training-free} adversarial\nperturbation under the no-box threat model, which can be successfully used to\nattack different DNNs in real-time. Motivated by our observation that\nhigh-frequency component (HFC) domains in low-level features and plays a\ncrucial role in classification, we attack an image mainly by manipulating its\nfrequency components. Specifically, the perturbation is manipulated by\nsuppression of the original HFC and adding of noisy HFC. We empirically and\nexperimentally analyze the requirements of effective noisy HFC and show that it\nshould be regionally homogeneous, repeating and dense. Extensive experiments on\nthe ImageNet dataset demonstrate the effectiveness of our proposed no-box\nmethod. It attacks ten well-known models with a success rate of\n\\textbf{98.13\\%} on average, which outperforms state-of-the-art no-box attacks\nby \\textbf{29.39\\%}. Furthermore, our method is even competitive to mainstream\ntransfer-based black-box attacks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qilong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chaoning Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chaoqun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jingkuan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Lianli Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Heng Tao Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Object-Based Visual Camera Pose Estimation From Ellipsoidal Model and 3D-Aware Ellipse Prediction. (arXiv:2203.04613v1 [cs.CV])","link":"http://arxiv.org/abs/2203.04613","description":"<p>In this paper, we propose a method for initial camera pose estimation from\njust a single image which is robust to viewing conditions and does not require\na detailed model of the scene. This method meets the growing need of easy\ndeployment of robotics or augmented reality applications in any environments,\nespecially those for which no accurate 3D model nor huge amount of ground truth\ndata are available. It exploits the ability of deep learning techniques to\nreliably detect objects regardless of viewing conditions. Previous works have\nalso shown that abstracting the geometry of a scene of objects by an ellipsoid\ncloud allows to compute the camera pose accurately enough for various\napplication needs. Though promising, these approaches use the ellipses fitted\nto the detection bounding boxes as an approximation of the imaged objects. In\nthis paper, we go one step further and propose a learning-based method which\ndetects improved elliptic approximations of objects which are coherent with the\n3D ellipsoids in terms of perspective projection. Experiments prove that the\naccuracy of the computed pose significantly increases thanks to our method.\nThis is achieved with very little effort in terms of training data acquisition\n- a few hundred calibrated images of which only three need manual object\nannotation. Code and models are released at\nhttps://gitlab.inria.fr/tangram/3d-aware-ellipses-for-visual-localization\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zins_M/0/1/0/all/0/1\">Matthieu Zins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simon_G/0/1/0/all/0/1\">Gilles Simon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berger_M/0/1/0/all/0/1\">Marie-Odile Berger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uni4Eye: Unified 2D and 3D Self-supervised Pre-training via Masked Image Modeling Transformer for Ophthalmic Image Classification. (arXiv:2203.04614v1 [eess.IV])","link":"http://arxiv.org/abs/2203.04614","description":"<p>A large-scale labeled dataset is a key factor for the success of supervised\ndeep learning in computer vision. However, a limited number of annotated data\nis very common, especially in ophthalmic image analysis, since manual\nannotation is time-consuming and labor-intensive. Self-supervised learning\n(SSL) methods bring huge opportunities for better utilizing unlabeled data, as\nthey do not need massive annotations. With an attempt to use as many as\npossible unlabeled ophthalmic images, it is necessary to break the dimension\nbarrier, simultaneously making use of both 2D and 3D images. In this paper, we\npropose a universal self-supervised Transformer framework, named Uni4Eye, to\ndiscover the inherent image property and capture domain-specific feature\nembedding in ophthalmic images. Uni4Eye can serve as a global feature\nextractor, which builds its basis on a Masked Image Modeling task with a Vision\nTransformer (ViT) architecture. We employ a Unified Patch Embedding module to\nreplace the origin patch embedding module in ViT for jointly processing both 2D\nand 3D input images. Besides, we design a dual-branch multitask decoder module\nto simultaneously perform two reconstruction tasks on the input image and its\ngradient map, delivering discriminative representations for better convergence.\nWe evaluate the performance of our pre-trained Uni4Eye encoder by fine-tuning\nit on six downstream ophthalmic image classification tasks. The superiority of\nUni4Eye is successfully established through comparisons to other\nstate-of-the-art SSL pre-training methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Cai_Z/0/1/0/all/0/1\">Zhiyuan Cai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+He_H/0/1/0/all/0/1\">Huaqing He</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_L/0/1/0/all/0/1\">Li Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tang_X/0/1/0/all/0/1\">Xiaoying Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluation and Generation of Physical Adversarial Patch. (arXiv:2203.04623v1 [cs.CV])","link":"http://arxiv.org/abs/2203.04623","description":"<p>Recent studies have revealed the vulnerability of face recognition models\nagainst physical adversarial patches, which raises security concerns about the\ndeployed face recognition systems. However, it is still challenging to ensure\nthe reproducibility for most attack algorithms under complex physical\nconditions, which leads to the lack of a systematic evaluation of the existing\nmethods. It is therefore imperative to develop a framework that can enable a\ncomprehensive evaluation of the vulnerability of face recognition in the\nphysical world. To this end, we propose to simulate the complex transformations\nof faces in the physical world via 3D-face modeling, which serves as a digital\ncounterpart of physical faces. The generic framework allows us to control\ndifferent face variations and physical conditions to conduct reproducible\nevaluations comprehensively. With this digital simulator, we further propose a\nFace3DAdv method considering the 3D face transformations and realistic physical\nvariations. Extensive experiments validate that Face3DAdv can significantly\nimprove the effectiveness of diverse physically realizable adversarial patches\nin both simulated and physical environments, against various white-box and\nblack-box face recognition models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yinpeng Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_T/0/1/0/all/0/1\">Tianyu Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1\">Zihao Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jun Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D Dense Face Alignment with Fused Features by Aggregating CNNs and GCNs. (arXiv:2203.04643v1 [cs.CV])","link":"http://arxiv.org/abs/2203.04643","description":"<p>In this paper, we propose a novel multi-level aggregation network to regress\nthe coordinates of the vertices of a 3D face from a single 2D image in an\nend-to-end manner. This is achieved by seamlessly combining standard\nconvolutional neural networks (CNNs) with Graph Convolution Networks (GCNs). By\niteratively and hierarchically fusing the features across different layers and\nstages of the CNNs and GCNs, our approach can provide a dense face alignment\nand 3D face reconstruction simultaneously for the benefit of direct feature\nlearning of 3D face mesh. Experiments on several challenging datasets\ndemonstrate that our method outperforms state-of-the-art approaches on both 2D\nand 3D face alignment tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1\">Yanda Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_D/0/1/0/all/0/1\">Dongxu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yitian Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaoyun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yihong Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaowei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yalin Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Normal and Visibility Estimation of Human Face from a Single Image. (arXiv:2203.04647v1 [cs.CV])","link":"http://arxiv.org/abs/2203.04647","description":"<p>Recent work on the intrinsic image of humans starts to consider the\nvisibility of incident illumination and encodes the light transfer function by\nspherical harmonics. In this paper, we show that such a light transfer function\ncan be further decomposed into visibility and cosine terms related to surface\nnormal. Such decomposition allows us to recover the surface normal in addition\nto visibility. We propose a deep learning-based approach with a reconstruction\nloss for training on real-world images. Results show that compared with\nprevious works, the reconstruction of human face from our method better reveals\nthe surface normal and shading details especially around regions where\nvisibility effect is strong.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_F/0/1/0/all/0/1\">Fuzhi Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huo_Y/0/1/0/all/0/1\">Yuchi Huo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1\">Hujun Bao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ray Tracing-Guided Design of Plenoptic Cameras. (arXiv:2203.04660v1 [cs.CV])","link":"http://arxiv.org/abs/2203.04660","description":"<p>The design of a plenoptic camera requires the combination of two dissimilar\noptical systems, namely a main lens and an array of microlenses. And while the\nconstruction process of a conventional camera is mainly concerned with focusing\nthe image onto a single plane, in the case of plenoptic cameras there can be\nadditional requirements such as a predefined depth of field or a desired range\nof disparities in neighboring microlens images. Due to this complexity, the\nmanual creation of multiple plenoptic camera setups is often a time-consuming\ntask. In this work we assume a simulation framework as well as the main lens\ndata given and present a method to calculate the remaining aperture, sensor and\nmicrolens array parameters under different sets of constraints. Our ray\ntracing-based approach is shown to result in models outperforming their\npendants generated with the commonly used paraxial approximations in terms of\nimage quality, while still meeting the desired constraints. Both the\nimplementation and evaluation setup including 30 plenoptic camera designs are\nmade publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Michels_T/0/1/0/all/0/1\">Tim Michels</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koch_R/0/1/0/all/0/1\">Reinhard Koch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Creating Realistic Ground Truth Data for the Evaluation of Calibration Methods for Plenoptic and Conventional Cameras. (arXiv:2203.04661v1 [cs.CV])","link":"http://arxiv.org/abs/2203.04661","description":"<p>Camera calibration methods usually consist of capturing images of known\ncalibration patterns and using the detected correspondences to optimize the\nparameters of the assumed camera model. A meaningful evaluation of these\nmethods relies on the availability of realistic synthetic data. In previous\nworks concerned with conventional cameras the synthetic data was mainly created\nby rendering perfect images with a pinhole camera and subsequently adding\ndistortions and aberrations to the renderings and correspondences according to\nthe assumed camera model. This method can bias the evaluation since not every\ncamera perfectly complies with an assumed model. Furthermore, in the field of\nplenoptic camera calibration there is no synthetic ground truth data available\nat all. We address these problems by proposing a method based on backward ray\ntracing to create realistic ground truth data that can be used for an unbiased\nevaluation of calibration methods for both types of cameras.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Michels_T/0/1/0/all/0/1\">Tim Michels</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petersen_A/0/1/0/all/0/1\">Arne Petersen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koch_R/0/1/0/all/0/1\">Reinhard Koch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simulation of Plenoptic Cameras. (arXiv:2203.04662v1 [cs.CV])","link":"http://arxiv.org/abs/2203.04662","description":"<p>Plenoptic cameras enable the capturing of spatial as well as angular color\ninformation which can be used for various applications among which are image\nrefocusing and depth calculations. However, these cameras are expensive and\nresearch in this area currently lacks data for ground truth comparisons. In\nthis work we describe a flexible, easy-to-use Blender model for the different\nplenoptic camera types which is on the one hand able to provide the ground\ntruth data for research and on the other hand allows an inexpensive assessment\nof the cameras usefulness for the desired applications. Furthermore we show\nthat the rendering results exhibit the same image degradation effects as real\ncameras and make our simulation publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Michels_T/0/1/0/all/0/1\">Tim Michels</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petersen_A/0/1/0/all/0/1\">Arne Petersen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palmieri_L/0/1/0/all/0/1\">Luca Palmieri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koch_R/0/1/0/all/0/1\">Reinhard Koch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inadequately Pre-trained Models are Better Feature Extractors. (arXiv:2203.04668v1 [cs.CV])","link":"http://arxiv.org/abs/2203.04668","description":"<p>Pre-training has been a popular learning paradigm in deep learning era,\nespecially in annotation-insufficient scenario. Better ImageNet pre-trained\nmodels have been demonstrated, from the perspective of architecture, by\nprevious research to have better transferability to downstream tasks. However,\nin this paper, we found that during the same pre-training process, models at\nmiddle epochs, which is inadequately pre-trained, can outperform fully trained\nmodels when used as feature extractors (FE), while the fine-tuning (FT)\nperformance still grows with the source performance. This reveals that there is\nnot a solid positive correlation between top-1 accuracy on ImageNet and the\ntransferring result on target data. Based on the contradictory phenomenon\nbetween FE and FT that better feature extractor fails to be fine-tuned better\naccordingly, we conduct comprehensive analyses on features before softmax layer\nto provide insightful explanations. Our discoveries suggest that, during\npre-training, models tend to first learn spectral components corresponding to\nlarge singular values and the residual components contribute more when\nfine-tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_A/0/1/0/all/0/1\">Andong Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xingjian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhibing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1\">Di Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chengzhong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_D/0/1/0/all/0/1\">Dejing Dou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Structure-Aware Flow Generation for Human Body Reshaping. (arXiv:2203.04670v1 [cs.CV])","link":"http://arxiv.org/abs/2203.04670","description":"<p>Body reshaping is an important procedure in portrait photo retouching. Due to\nthe complicated structure and multifarious appearance of human bodies, existing\nmethods either fall back on the 3D domain via body morphable model or resort to\nkeypoint-based image deformation, leading to inefficiency and unsatisfied\nvisual quality. In this paper, we address these limitations by formulating an\nend-to-end flow generation architecture under the guidance of body structural\npriors, including skeletons and Part Affinity Fields, and achieve\nunprecedentedly controllable performance under arbitrary poses and garments. A\ncompositional attention mechanism is introduced for capturing both visual\nperceptual correlations and structural associations of the human body to\nreinforce the manipulation consistency among related parts. For a comprehensive\nevaluation, we construct the first large-scale body reshaping dataset, namely\nBR-5K, which contains 5,000 portrait photos as well as professionally retouched\ntargets. Extensive experiments demonstrate that our approach significantly\noutperforms existing state-of-the-art methods in terms of visual performance,\ncontrollability, and efficiency. The dataset is available at our website:\nhttps://github.com/JianqiangRen/BodyReshaping5K.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Jianqiang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yuan Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_B/0/1/0/all/0/1\">Biwen Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_M/0/1/0/all/0/1\">Miaomiao Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xuansong Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Align-Deform-Subtract: An Interventional Framework for Explaining Object Differences. (arXiv:2203.04694v1 [cs.CV])","link":"http://arxiv.org/abs/2203.04694","description":"<p>Given two object images, how can we explain their differences in terms of the\nunderlying object properties? To address this question, we propose\nAlign-Deform-Subtract (ADS) -- an interventional framework for explaining\nobject differences. By leveraging semantic alignments in image-space as\ncounterfactual interventions on the underlying object properties, ADS\niteratively quantifies and removes differences in object properties. The result\nis a set of \"disentangled\" error measures which explain object differences in\nterms of their underlying properties. Experiments on real and synthetic data\nillustrate the efficacy of the framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Eastwood_C/0/1/0/all/0/1\">Cian Eastwood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nanbo_L/0/1/0/all/0/1\">Li Nanbo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_C/0/1/0/all/0/1\">Christopher K. I. Williams</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FlexIT: Towards Flexible Semantic Image Translation. (arXiv:2203.04705v1 [cs.CV])","link":"http://arxiv.org/abs/2203.04705","description":"<p>Deep generative models, like GANs, have considerably improved the state of\nthe art in image synthesis, and are able to generate near photo-realistic\nimages in structured domains such as human faces. Based on this success, recent\nwork on image editing proceeds by projecting images to the GAN latent space and\nmanipulating the latent vector. However, these approaches are limited in that\nonly images from a narrow domain can be transformed, and with only a limited\nnumber of editing operations. We propose FlexIT, a novel method which can take\nany input image and a user-defined text instruction for editing. Our method\nachieves flexible and natural editing, pushing the limits of semantic image\ntranslation. First, FlexIT combines the input image and text into a single\ntarget point in the CLIP multimodal embedding space. Via the latent space of an\nauto-encoder, we iteratively transform the input image toward the target point,\nensuring coherence and quality with a variety of novel regularization terms. We\npropose an evaluation protocol for semantic image translation, and thoroughly\nevaluate our method on ImageNet. Code will be made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Couairon_G/0/1/0/all/0/1\">Guillaume Couairon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grechka_A/0/1/0/all/0/1\">Asya Grechka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verbeek_J/0/1/0/all/0/1\">Jakob Verbeek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwenk_H/0/1/0/all/0/1\">Holger Schwenk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cord_M/0/1/0/all/0/1\">Matthieu Cord</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Unified Transformer Framework for Group-based Segmentation: Co-Segmentation, Co-Saliency Detection and Video Salient Object Detection. (arXiv:2203.04708v1 [cs.CV])","link":"http://arxiv.org/abs/2203.04708","description":"<p>Humans tend to mine objects by learning from a group of images or several\nframes of video since we live in a dynamic world. In the computer vision area,\nmany researches focus on co-segmentation (CoS), co-saliency detection (CoSD)\nand video salient object detection (VSOD) to discover the co-occurrent objects.\nHowever, previous approaches design different networks on these similar tasks\nseparately, and they are difficult to apply to each other, which lowers the\nupper bound of the transferability of deep learning frameworks. Besides, they\nfail to take full advantage of the cues among inter- and intra-feature within a\ngroup of images. In this paper, we introduce a unified framework to tackle\nthese issues, term as UFO (Unified Framework for Co-Object Segmentation).\nSpecifically, we first introduce a transformer block, which views the image\nfeature as a patch token and then captures their long-range dependencies\nthrough the self-attention mechanism. This can help the network to excavate the\npatch structured similarities among the relevant objects. Furthermore, we\npropose an intra-MLP learning module to produce self-mask to enhance the\nnetwork to avoid partial activation. Extensive experiments on four CoS\nbenchmarks (PASCAL, iCoseg, Internet and MSRC), three CoSD benchmarks\n(Cosal2015, CoSOD3k, and CocA) and four VSOD benchmarks (DAVIS16, FBMS, ViSal\nand SegV2) show that our method outperforms other state-of-the-arts on three\ndifferent tasks in both accuracy and speed by using the same network\narchitecture , which can reach 140 FPS in real-time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yukun Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jingliang Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_R/0/1/0/all/0/1\">Ruizhou Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1\">Guosheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qingyao Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Defending Black-box Skeleton-based Human Activity Classifiers. (arXiv:2203.04713v1 [cs.CV])","link":"http://arxiv.org/abs/2203.04713","description":"<p>Deep learning has been regarded as the `go to' solution for many tasks today,\nbut its intrinsic vulnerability to malicious attacks has become a major\nconcern. The vulnerability is affected by a variety of factors including\nmodels, tasks, data, and attackers. Consequently, methods such as Adversarial\nTraining and Randomized Smoothing have been proposed to tackle the problem in a\nwide range of applications. In this paper, we investigate skeleton-based Human\nActivity Recognition, which is an important type of time-series data but\nunder-explored in defense against attacks. Our method is featured by (1) a new\nBayesian Energy-based formulation of robust discriminative classifiers, (2) a\nnew parameterization of the adversarial sample manifold of actions, and (3) a\nnew post-train Bayesian treatment on both the adversarial samples and the\nclassifier. We name our framework Bayesian Energy-based Adversarial Training or\nBEAT. BEAT is straightforward but elegant, which turns vulnerable black-box\nclassifiers into robust ones without sacrificing accuracy. It demonstrates\nsurprising and universal effectiveness across a wide range of action\nclassifiers and datasets, under various attacks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">He Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diao_Y/0/1/0/all/0/1\">Yunfeng Diao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1\">Zichang Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_G/0/1/0/all/0/1\">Guodong Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predicting conversion of mild cognitive impairment to Alzheimer's disease. (arXiv:2203.04725v1 [eess.IV])","link":"http://arxiv.org/abs/2203.04725","description":"<p>Alzheimer's disease (AD) is the most common age-related dementia. Mild\ncognitive impairment (MCI) is the early stage of cognitive decline before AD.\nIt is crucial to predict the MCI-to-AD conversion for precise management, which\nremains challenging due to the diversity of patients. Previous evidence shows\nthat the brain network generated from diffusion MRI promises to classify\ndementia using deep learning. However, the limited availability of diffusion\nMRI challenges the model training. In this study, we develop a self-supervised\ncontrastive learning approach to generate structural brain networks from\nroutine anatomical MRI under the guidance of diffusion MRI. The generated brain\nnetworks are applied to train a learning framework for predicting the MCI-to-AD\nconversion. Instead of directly modelling the AD brain networks, we train a\ngraph encoder and a variational autoencoder to model the healthy ageing\ntrajectories from brain networks of healthy controls. To predict the MCI-to-AD\nconversion, we further design a recurrent neural networks based approach to\nmodel the longitudinal deviation of patients' brain networks from the healthy\nageing trajectory. Numerical results show that the proposed methods outperform\nthe benchmarks in the prediction task. We also visualize the model\ninterpretation to explain the prediction and identify abnormal changes of white\nmatter tracts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wei_Y/0/1/0/all/0/1\">Yiran Wei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Price_S/0/1/0/all/0/1\">Stephen J. Price</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schonlieb_C/0/1/0/all/0/1\">Carola-Bibiane Sch&#xf6;nlieb</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_C/0/1/0/all/0/1\">Chao Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"P2M: A Processing-in-Pixel-in-Memory Paradigm for Resource-Constrained TinyML Applications. (arXiv:2203.04737v1 [cs.LG])","link":"http://arxiv.org/abs/2203.04737","description":"<p>The demand to process vast amounts of data generated from state-of-the-art\nhigh resolution cameras has motivated novel energy-efficient on-device AI\nsolutions. Visual data in such cameras are usually captured in the form of\nanalog voltages by a sensor pixel array, and then converted to the digital\ndomain for subsequent AI processing using analog-to-digital converters (ADC).\nRecent research has tried to take advantage of massively parallel low-power\nanalog/digital computing in the form of near- and in-sensor processing, in\nwhich the AI computation is performed partly in the periphery of the pixel\narray and partly in a separate on-board CPU/accelerator. Unfortunately,\nhigh-resolution input images still need to be streamed between the camera and\nthe AI processing unit, frame by frame, causing energy, bandwidth, and security\nbottlenecks. To mitigate this problem, we propose a novel\nProcessing-in-Pixel-in-memory (P2M) paradigm, that customizes the pixel array\nby adding support for analog multi-channel, multi-bit convolution and ReLU\n(Rectified Linear Units). Our solution includes a holistic algorithm-circuit\nco-design approach and the resulting P2M paradigm can be used as a drop-in\nreplacement for embedding memory-intensive first few layers of convolutional\nneural network (CNN) models within foundry-manufacturable CMOS image sensor\nplatforms. Our experimental results indicate that P2M reduces data transfer\nbandwidth from sensors and analog to digital conversions by ~21x, and the\nenergy-delay product (EDP) incurred in processing a MobileNetV2 model on a\nTinyML use case for visual wake words dataset (VWW) by up to ~11x compared to\nstandard near-processing or in-sensor implementations, without any significant\ndrop in test accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Datta_G/0/1/0/all/0/1\">Gourav Datta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kundu_S/0/1/0/all/0/1\">Souvik Kundu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1\">Zihan Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakkireddy_R/0/1/0/all/0/1\">Ravi Teja Lakkireddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beerel_P/0/1/0/all/0/1\">Peter A. Beerel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacob_A/0/1/0/all/0/1\">Ajey Jacob</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaiswal_A/0/1/0/all/0/1\">Akhilesh R. Jaiswal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Parallel Training of GRU Networks with a Multi-Grid Solver for Long Sequences. (arXiv:2203.04738v1 [cs.CV])","link":"http://arxiv.org/abs/2203.04738","description":"<p>Parallelizing Gated Recurrent Unit (GRU) networks is a challenging task, as\nthe training procedure of GRU is inherently sequential. Prior efforts to\nparallelize GRU have largely focused on conventional parallelization strategies\nsuch as data-parallel and model-parallel training algorithms. However, when the\ngiven sequences are very long, existing approaches are still inevitably\nperformance limited in terms of training time. In this paper, we present a\nnovel parallel training scheme (called parallel-in-time) for GRU based on a\nmultigrid reduction in time (MGRIT) solver. MGRIT partitions a sequence into\nmultiple shorter sub-sequences and trains the sub-sequences on different\nprocessors in parallel. The key to achieving speedup is a hierarchical\ncorrection of the hidden state to accelerate end-to-end communication in both\nthe forward and backward propagation phases of gradient descent. Experimental\nresults on the HMDB51 dataset, where each video is an image sequence,\ndemonstrate that the new parallel training scheme achieves up to 6.5$\\times$\nspeedup over a serial approach. As efficiency of our new parallelization\nstrategy is associated with the sequence length, our parallel GRU algorithm\nachieves significant performance improvement as the sequence length increases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moon_G/0/1/0/all/0/1\">Gordon Euhyun Moon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cyr_E/0/1/0/all/0/1\">Eric C. Cyr</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SkinningNet: Two-Stream Graph Convolutional Neural Network for Skinning Prediction of Synthetic Characters. (arXiv:2203.04746v1 [cs.CV])","link":"http://arxiv.org/abs/2203.04746","description":"<p>This work presents SkinningNet, an end-to-end Two-Stream Graph Neural Network\narchitecture that computes skinning weights from an input mesh and its\nassociated skeleton, without making any assumptions on shape class and\nstructure of the provided mesh. Whereas previous methods pre-compute\nhandcrafted features that relate the mesh and the skeleton or assume a fixed\ntopology of the skeleton, the proposed method extracts this information in an\nend-to-end learnable fashion by jointly learning the best relationship between\nmesh vertices and skeleton joints. The proposed method exploits the benefits of\nthe novel Multi-Aggregator Graph Convolution that combines the results of\ndifferent aggregators during the summarizing step of the Message-Passing\nscheme, helping the operation to generalize for unseen topologies. Experimental\nresults demonstrate the effectiveness of the contributions of our novel\narchitecture, with SkinningNet outperforming current state-of-the-art\nalternatives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mosella_Montoro_A/0/1/0/all/0/1\">Albert Mosella-Montoro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruiz_Hidalgo_J/0/1/0/all/0/1\">Javier Ruiz-Hidalgo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using Human Gaze For Surgical Activity Recognition. (arXiv:2203.04752v1 [eess.IV])","link":"http://arxiv.org/abs/2203.04752","description":"<p>Automatically recognizing surgical activities plays an important role in\nproviding feedback to surgeons, and is a fundamental step towards\ncomputer-aided surgical systems. Human gaze and visual saliency carry important\ninformation about visual attention, and can be used in computer vision systems.\nAlthough state-of-the-art surgical activity recognition models learn spatial\ntemporal features, none of these models make use of human gaze and visual\nsaliency. In this study, we propose to use human gaze with a spatial temporal\nattention mechanism for activity recognition in surgical videos. Our model\nconsists of an I3D-based architecture, learns spatio-temporal features using 3D\nconvolutions, as well as learning an attention map using human gaze. We\nevaluated our model on the Suturing task of JIGSAWS which is a publicly\navailable surgical video understanding dataset. Our evaluations on a subset of\nrandom video segments in this task suggest that our approach achieves promising\nresults with an accuracy of 86.2%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Awale_A/0/1/0/all/0/1\">Abdishakour Awale</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sarikaya_D/0/1/0/all/0/1\">Duygu Sarikaya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multiscale Convolutional Transformer with Center Mask Pretraining for Hyperspectral Image Classificationtion. (arXiv:2203.04771v1 [cs.CV])","link":"http://arxiv.org/abs/2203.04771","description":"<p>Hyperspectral images (HSI) not only have a broad macroscopic field of view\nbut also contain rich spectral information, and the types of surface objects\ncan be identified through spectral information, which is one of the main\napplications in hyperspectral image related research.In recent years, more and\nmore deep learning methods have been proposed, among which convolutional neural\nnetworks (CNN) are the most influential. However, CNN-based methods are\ndifficult to capture long-range dependencies, and also require a large amount\nof labeled data for model training.Besides, most of the self-supervised\ntraining methods in the field of HSI classification are based on the\nreconstruction of input samples, and it is difficult to achieve effective use\nof unlabeled samples. To address the shortcomings of CNN networks, we propose a\nnoval multi-scale convolutional embedding module for HSI to realize effective\nextraction of spatial-spectral information, which can be better combined with\nTransformer network.In order to make more efficient use of unlabeled data, we\npropose a new self-supervised pretask. Similar to Mask autoencoder, but our\npre-training method only masks the corresponding token of the central pixel in\nthe encoder, and inputs the remaining token into the decoder to reconstruct the\nspectral information of the central pixel.Such a pretask can better model the\nrelationship between the central feature and the domain feature, and obtain\nmore stable training results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yifan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_S/0/1/0/all/0/1\">Sen Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhongfan Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How many Observations are Enough? Knowledge Distillation for Trajectory Forecasting. (arXiv:2203.04781v1 [cs.CV])","link":"http://arxiv.org/abs/2203.04781","description":"<p>Accurate prediction of future human positions is an essential task for modern\nvideo-surveillance systems. Current state-of-the-art models usually rely on a\n\"history\" of past tracked locations (e.g., 3 to 5 seconds) to predict a\nplausible sequence of future locations (e.g., up to the next 5 seconds). We\nfeel that this common schema neglects critical traits of realistic\napplications: as the collection of input trajectories involves machine\nperception (i.e., detection and tracking), incorrect detection and\nfragmentation errors may accumulate in crowded scenes, leading to tracking\ndrifts. On this account, the model would be fed with corrupted and noisy input\ndata, thus fatally affecting its prediction performance.\n</p>\n<p>In this regard, we focus on delivering accurate predictions when only few\ninput observations are used, thus potentially lowering the risks associated\nwith automatic perception. To this end, we conceive a novel distillation\nstrategy that allows a knowledge transfer from a teacher network to a student\none, the latter fed with fewer observations (just two ones). We show that a\nproperly defined teacher supervision allows a student network to perform\ncomparably to state-of-the-art approaches that demand more observations.\nBesides, extensive experiments on common trajectory forecasting datasets\nhighlight that our student network better generalizes to unseen scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Monti_A/0/1/0/all/0/1\">Alessio Monti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Porrello_A/0/1/0/all/0/1\">Angelo Porrello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calderara_S/0/1/0/all/0/1\">Simone Calderara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coscia_P/0/1/0/all/0/1\">Pasquale Coscia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ballan_L/0/1/0/all/0/1\">Lamberto Ballan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cucchiara_R/0/1/0/all/0/1\">Rita Cucchiara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluation of YOLO Models with Sliced Inference for Small Object Detection. (arXiv:2203.04799v1 [cs.CV])","link":"http://arxiv.org/abs/2203.04799","description":"<p>Small object detection has major applications in the fields of UAVs,\nsurveillance, farming and many others. In this work we investigate the\nperformance of state of the art Yolo based object detection models for the task\nof small object detection as they are one of the most popular and easy to use\nobject detection models. We evaluated YOLOv5 and YOLOX models in this study. We\nalso investigate the effects of slicing aided inference and fine-tuning the\nmodel for slicing aided inference. We used the VisDrone2019Det dataset for\ntraining and evaluating our models. This dataset is challenging in the sense\nthat most objects are relatively small compared to the image sizes. This work\naims to benchmark the YOLOv5 and YOLOX models for small object detection. We\nhave seen that sliced inference increases the AP50 score in all experiments,\nthis effect was greater for the YOLOv5 models compared to the YOLOX models. The\neffects of sliced fine-tuning and sliced inference combined produced\nsubstantial improvement for all models. The highest AP50 score was achieved by\nthe YOLOv5- Large model on the VisDrone2019Det test-dev subset with the score\nbeing 48.8.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Keles_M/0/1/0/all/0/1\">Muhammed Can Keles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salmanoglu_B/0/1/0/all/0/1\">Batuhan Salmanoglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guzel_M/0/1/0/all/0/1\">Mehmet Serdar Guzel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gursoy_B/0/1/0/all/0/1\">Baran Gursoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bostanci_G/0/1/0/all/0/1\">Gazi Erkan Bostanci</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NeRF-Pose: A First-Reconstruct-Then-Regress Approach for Weakly-supervised 6D Object Pose Estimation. (arXiv:2203.04802v1 [cs.CV])","link":"http://arxiv.org/abs/2203.04802","description":"<p>Pose estimation of 3D objects in monocular images is a fundamental and\nlong-standing problem in computer vision. Existing deep learning approaches for\n6D pose estimation typically rely on the assumption of availability of 3D\nobject models and 6D pose annotations. However, precise annotation of 6D poses\nin real data is intricate, time-consuming and not scalable, while synthetic\ndata scales well but lacks realism. To avoid these problems, we present a\nweakly-supervised reconstruction-based pipeline, named NeRF-Pose, which needs\nonly 2D object segmentation and known relative camera poses during training.\nFollowing the first-reconstruct-then-regress idea, we first reconstruct the\nobjects from multiple views in the form of an implicit neural representation.\nThen, we train a pose regression network to predict pixel-wise 2D-3D\ncorrespondences between images and the reconstructed model. At inference, the\napproach only needs a single image as input. A NeRF-enabled PnP+RANSAC\nalgorithm is used to estimate stable and accurate pose from the predicted\ncorrespondences. Experiments on LineMod and LineMod-Occlusion show that the\nproposed method has state-of-the-art accuracy in comparison to the best 6D pose\nestimation methods in spite of being trained only with weak labels. Besides, we\nextend the Homebrewed DB dataset with more real training images to support the\nweakly supervised task and achieve compelling results on this dataset. The\nextended dataset and code will be released soon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shugurov_I/0/1/0/all/0/1\">Ivan Shugurov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Busam_B/0/1/0/all/0/1\">Benjamin Busam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shaowu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilic_S/0/1/0/all/0/1\">Slobodan Ilic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A high-precision self-supervised monocular visual odometry in foggy weather based on robust cycled generative adversarial networks and multi-task learning aided depth estimation. (arXiv:2203.04812v1 [cs.CV])","link":"http://arxiv.org/abs/2203.04812","description":"<p>This paper proposes a high-precision self-supervised monocular VO, which is\nspecifically designed for navigation in foggy weather. A cycled generative\nadversarial network is designed to obtain high-quality self-supervised loss via\nforcing the forward and backward half-cycle to output consistent estimation.\nMoreover, gradient-based loss and perceptual loss are introduced to eliminate\nthe interference of complex photometric change on self-supervised loss in foggy\nweather. To solve the ill-posed problem of depth estimation, a self-supervised\nmulti-task learning aided depth estimation module is designed based on the\nstrong correlation between the depth estimation and transmission map\ncalculation of hazy images in foggy weather. The experimental results on the\nsynthetic foggy KITTI dataset show that the proposed self-supervised monocular\nVO performs better in depth and pose estimation than other state-of-the-art\nmonocular VO in the literature, indicating the designed method is more suitable\nfor foggy weather.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiuyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jiangang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fengchao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_G/0/1/0/all/0/1\">Guowen An</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text-DIAE: Degradation Invariant Autoencoders for Text Recognition and Document Enhancement. (arXiv:2203.04814v1 [cs.CV])","link":"http://arxiv.org/abs/2203.04814","description":"<p>In this work, we propose Text-Degradation Invariant Auto Encoder (Text-DIAE)\naimed to solve two tasks, text recognition (handwritten or scene-text) and\ndocument image enhancement. We define three pretext tasks as learning\nobjectives to be optimized during pre-training without the usage of labelled\ndata. Each of the pre-text objectives is specifically tailored for the final\ndownstream tasks. We conduct several ablation experiments that show the\nimportance of each degradation for a specific domain. Exhaustive\nexperimentation shows that our method does not have limitations of previous\nstate-of-the-art based on contrastive losses while at the same time requiring\nessentially fewer data samples to converge. Finally, we demonstrate that our\nmethod surpasses the state-of-the-art significantly in existing supervised and\nself-supervised settings in handwritten and scene text recognition and document\nimage enhancement. Our code and trained models will be made publicly available\nat~\\url{ <a href=\"http://Upon_Acceptance\">this http URL</a>}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Souibgui_M/0/1/0/all/0/1\">Mohamed Ali Souibgui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biswas_S/0/1/0/all/0/1\">Sanket Biswas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mafla_A/0/1/0/all/0/1\">Andres Mafla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biten_A/0/1/0/all/0/1\">Ali Furkan Biten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fornes_A/0/1/0/all/0/1\">Alicia Forn&#xe9;s</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kessentini_Y/0/1/0/all/0/1\">Yousri Kessentini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Llados_J/0/1/0/all/0/1\">Josep Llad&#xf3;s</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomez_L/0/1/0/all/0/1\">Lluis Gomez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karatzas_D/0/1/0/all/0/1\">Dimosthenis Karatzas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A high-precision underwater object detection based on joint self-supervised deblurring and improved spatial transformer network. (arXiv:2203.04822v1 [cs.CV])","link":"http://arxiv.org/abs/2203.04822","description":"<p>Deep learning-based underwater object detection (UOD) remains a major\nchallenge due to the degraded visibility and difficulty to obtain sufficient\nunderwater object images captured from various perspectives for training. To\naddress these issues, this paper presents a high-precision UOD based on joint\nself-supervised deblurring and improved spatial transformer network. A\nself-supervised deblurring subnetwork is introduced into the designed\nmulti-task learning aided object detection architecture to force the shared\nfeature extraction module to output clean features for detection subnetwork.\nAiming at alleviating the limitation of insufficient photos from different\nperspectives, an improved spatial transformer network is designed based on\nperspective transformation, adaptively enriching image features within the\nnetwork. The experimental results show that the proposed UOD approach achieved\n47.9 mAP in URPC2017 and 70.3 mAP in URPC2018, outperforming many\nstate-of-the-art UOD methods and indicating the designed method is more\nsuitable for UOD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiuyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fengchao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jiangang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_G/0/1/0/all/0/1\">Guowen An</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CMX: Cross-Modal Fusion for RGB-X Semantic Segmentation with Transformers. (arXiv:2203.04838v1 [cs.CV])","link":"http://arxiv.org/abs/2203.04838","description":"<p>The performance of semantic segmentation of RGB images can be advanced by\nexploiting informative features from supplementary modalities. In this work, we\npropose CMX, a vision-transformer-based cross-modal fusion framework for RGB-X\nsemantic segmentation. To generalize to different sensing modalities\nencompassing various uncertainties, we consider that comprehensive cross-modal\ninteractions should be provided. CMX is built with two streams to extract\nfeatures from RGB images and the complementary modality (X-modality). In each\nfeature extraction stage, we design a Cross-Modal Feature Rectification Module\n(CM-FRM) to calibrate the feature of the current modality by combining the\nfeature from the other modality, in spatial- and channel-wise dimensions. With\nrectified feature pairs, we deploy a Feature Fusion Module (FFM) to mix them\nfor the final semantic prediction. FFM is constructed with a cross-attention\nmechanism, which enables exchange of long-range contexts, enhancing both\nmodalities' features at a global level. Extensive experiments show that CMX\ngeneralizes to diverse multi-modal combinations, achieving state-of-the-art\nperformances on four RGB-Depth benchmarks, as well as RGB-Thermal and\nRGB-Polarization datasets. Besides, to investigate the generalizability to\ndense-sparse data fusion, we establish a RGB-Event semantic segmentation\nbenchmark based on the EventScape dataset, on which CMX sets the new\nstate-of-the-art. Code is available at\nhttps://github.com/huaaaliu/RGBX_Semantic_Segmentation\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Huayao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiaming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kailun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xinxin Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stiefelhagen_R/0/1/0/all/0/1\">Rainer Stiefelhagen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Coarse-to-Fine Sparse Transformer for Hyperspectral Image Reconstruction. (arXiv:2203.04845v1 [cs.CV])","link":"http://arxiv.org/abs/2203.04845","description":"<p>Many algorithms have been developed to solve the inverse problem of coded\naperture snapshot spectral imaging (CASSI), i.e., recovering the 3D\nhyperspectral images (HSIs) from a 2D compressive measurement. In recent years,\nlearning-based methods have demonstrated promising performance and dominated\nthe mainstream research direction. However, existing CNN-based methods show\nlimitations in capturing long-range dependencies and non-local self-similarity.\nPrevious Transformer-based methods densely sample tokens, some of which are\nuninformative, and calculate the multi-head self-attention (MSA) between some\ntokens that are unrelated in content. This does not fit the spatially sparse\nnature of HSI signals and limits the model scalability. In this paper, we\npropose a novel Transformer-based method, coarse-to-fine sparse Transformer\n(CST), firstly embedding HSI sparsity into deep learning for HSI\nreconstruction. In particular, CST uses our proposed spectra-aware screening\nmechanism (SASM) for coarse patch selecting. Then the selected patches are fed\ninto our customized spectra-aggregation hashing multi-head self-attention\n(SAH-MSA) for fine pixel clustering and self-similarity capturing.\nComprehensive experiments show that our CST significantly outperforms\nstate-of-the-art methods while requiring cheaper computational costs. The code\nand models will be made public.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jing Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1\">Yuanhao Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaowan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haoqian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xin Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yulun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Timofte_R/0/1/0/all/0/1\">Radu Timofte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CEU-Net: Ensemble Semantic Segmentation of Hyperspectral Images Using Clustering. (arXiv:2203.04873v1 [cs.CV])","link":"http://arxiv.org/abs/2203.04873","description":"<p>Most semantic segmentation approaches of Hyperspectral images (HSIs) use and\nrequire preprocessing steps in the form of patching to accurately classify\ndiversified land cover in remotely sensed images. These approaches use patching\nto incorporate the rich neighborhood information in images and exploit the\nsimplicity and segmentability of the most common HSI datasets. In contrast,\nmost landmasses in the world consist of overlapping and diffused classes,\nmaking neighborhood information weaker than what is seen in common HSI\ndatasets. To combat this issue and generalize the segmentation models to more\ncomplex and diverse HSI datasets, in this work, we propose our novel flagship\nmodel: Clustering Ensemble U-Net (CEU-Net). CEU-Net uses the ensemble method to\ncombine spectral information extracted from convolutional neural network (CNN)\ntraining on a cluster of landscape pixels. Our CEU-Net model outperforms\nexisting state-of-the-art HSI semantic segmentation methods and gets\ncompetitive performance with and without patching when compared to baseline\nmodels. We highlight CEU-Net's high performance across Botswana, KSC, and\nSalinas datasets compared to HybridSN and AeroRIT methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Soucy_N/0/1/0/all/0/1\">Nicholas Soucy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sekeh_S/0/1/0/all/0/1\">Salimeh Yasaei Sekeh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VGQ-CNN: Moving Beyond Fixed Cameras and Top-Grasps for Grasp Quality Prediction. (arXiv:2203.04874v1 [cs.RO])","link":"http://arxiv.org/abs/2203.04874","description":"<p>We present the Versatile Grasp Quality Convolutional Neural Network\n(VGQ-CNN), a grasp quality prediction network for 6-DOF grasps. VGQ-CNN can be\nused when evaluating grasps for objects seen from a wide range of camera poses\nor mobile robots without the need to retrain the network. By defining the grasp\norientation explicitly as an input to the network, VGQ-CNN can evaluate 6-DOF\ngrasp poses, moving beyond the 4-DOF grasps used in most image-based grasp\nevaluation methods like GQ-CNN. We train VGQ-CNN on our new Versatile Grasp\ndataset (VG-dset), containing 6-DOF grasps observed from a wide range of camera\nposes. VGQ-CNN achieves a balanced accuracy of 82.1% on our test-split while\ngeneralising to a variety of camera poses. Meanwhile, it achieves competitive\nperformance for overhead cameras and top-grasps with a balanced accuracy of\n74.2% compared to GQ-CNN's 76.6%. We also propose a modified network\narchitecture, FAST-VGQ-CNN, that speeds up inference using a shared encoder\narchitecture and can make 128 grasp quality predictions in 12ms on a CPU. Code\nand data are available at https://figshare.com/s/b12b37b14b747b10524e.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Konrad_A/0/1/0/all/0/1\">A. Konrad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McDonald_J/0/1/0/all/0/1\">J. McDonald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Villing_R/0/1/0/all/0/1\">R. Villing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Image Representation Learning with Federated Sampled Softmax. (arXiv:2203.04888v1 [cs.LG])","link":"http://arxiv.org/abs/2203.04888","description":"<p>Learning image representations on decentralized data can bring many benefits\nin cases where data cannot be aggregated across data silos. Softmax cross\nentropy loss is highly effective and commonly used for learning image\nrepresentations. Using a large number of classes has proven to be particularly\nbeneficial for the descriptive power of such representations in centralized\nlearning. However, doing so on decentralized data with Federated Learning is\nnot straightforward as the demand on FL clients' computation and communication\nincreases proportionally to the number of classes. In this work we introduce\nfederated sampled softmax (FedSS), a resource-efficient approach for learning\nimage representation with Federated Learning. Specifically, the FL clients\nsample a set of classes and optimize only the corresponding model parameters\nwith respect to a sampled softmax objective that approximates the global full\nsoftmax objective. We examine the loss formulation and empirically show that\nour method significantly reduces the number of parameters transferred to and\noptimized by the client devices, while performing on par with the standard full\nsoftmax method. This work creates a possibility for efficiently learning image\nrepresentations on decentralized data with a large number of classes under the\nfederated setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Waghmare_S/0/1/0/all/0/1\">Sagar M. Waghmare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_H/0/1/0/all/0/1\">Hang Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huizhong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sirotenko_M/0/1/0/all/0/1\">Mikhail Sirotenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meron_T/0/1/0/all/0/1\">Tomer Meron</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Low-light Image and Video Enhancement via Selective Manipulation of Chromaticity. (arXiv:2203.04889v1 [cs.CV])","link":"http://arxiv.org/abs/2203.04889","description":"<p>Image acquisition in low-light conditions suffers from poor quality and\nsignificant degradation in visual aesthetics. This affects the visual\nperception of the acquired image and the performance of various computer vision\nand image processing algorithms applied after acquisition. Especially for\nvideos, the additional temporal domain makes it more challenging, wherein we\nneed to preserve quality in a temporally coherent manner. We present a simple\nyet effective approach for low-light image and video enhancement. To this end,\nwe introduce \"Adaptive Chromaticity\", which refers to an adaptive computation\nof image chromaticity. The above adaptivity allows us to avoid the costly step\nof low-light image decomposition into illumination and reflectance, employed by\nmany existing techniques. All stages in our method consist of only point-based\noperations and high-pass or low-pass filtering, thereby ensuring that the\namount of temporal incoherence is negligible when applied on a per-frame basis\nfor videos. Our results on standard lowlight image datasets show the efficacy\nof our algorithm and its qualitative and quantitative superiority over several\nstate-of-the-art techniques. For videos captured in the wild, we perform a user\nstudy to demonstrate the preference for our method in comparison to\nstate-of-the-art approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shekhar_S/0/1/0/all/0/1\">Sumit Shekhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reimann_M/0/1/0/all/0/1\">Max Reimann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Semmo_A/0/1/0/all/0/1\">Amir Semmo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasewaldt_S/0/1/0/all/0/1\">Sebastian Pasewaldt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dollner_J/0/1/0/all/0/1\">J&#xfc;rgen D&#xf6;llner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trapp_M/0/1/0/all/0/1\">Matthias Trapp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Joint Learning of Salient Object Detection, Depth Estimation and Contour Extraction. (arXiv:2203.04895v1 [cs.CV])","link":"http://arxiv.org/abs/2203.04895","description":"<p>Benefiting from color independence, illumination invariance and location\ndiscrimination attributed by the depth map, it can provide important\nsupplemental information for extracting salient objects in complex\nenvironments. However, high-quality depth sensors are expensive and can not be\nwidely applied. While general depth sensors produce the noisy and sparse depth\ninformation, which brings the depth-based networks with irreversible\ninterference. In this paper, we propose a novel multi-task and multi-modal\nfiltered transformer (MMFT) network for RGB-D salient object detection (SOD).\nSpecifically, we unify three complementary tasks: depth estimation, salient\nobject detection and contour estimation. The multi-task mechanism promotes the\nmodel to learn the task-aware features from the auxiliary tasks. In this way,\nthe depth information can be completed and purified. Moreover, we introduce a\nmulti-modal filtered transformer (MFT) module, which equips with three\nmodality-specific filters to generate the transformer-enhanced feature for each\nmodality. The proposed model works in a depth-free style during the testing\nphase. Experiments show that it not only significantly surpasses the\ndepth-based RGB-D SOD methods on multiple datasets, but also precisely predicts\na high-quality depth map and salient contour at the same time. And, the\nresulted depth map can help existing RGB-D SOD methods obtain significant\nperformance gain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xiaoqi Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_Y/0/1/0/all/0/1\">Youwei Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lihe Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Huchuan Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Model-Agnostic Multitask Fine-tuning for Few-shot Vision-Language Transfer Learning. (arXiv:2203.04904v1 [cs.MM])","link":"http://arxiv.org/abs/2203.04904","description":"<p>Despite achieving state-of-the-art zero-shot performance, existing\nvision-language models, e.g., CLIP, still fall short of domain-specific\nclassification tasks, e.g., Fungi Classification. In the context of few-shot\ntransfer learning, traditional fine-tuning fails to prevent highly expressive\nmodel from exploiting spurious correlations in the training data. On the other\nhand, although model-agnostic meta-learning (MAML) presents as a natural\nalternative for transfer learning, the expensive computation due to implicit\nsecond-order optimization limits its use in large-scale models and datasets. In\nthis work we aim to further improve the generalization of existing\nvision-language models on unseen tasks via a simple yet efficient fine-tuning\nstrategy based on uniform task sampling. We term our method as Model-Agnostic\nMultitask Fine-tuning (MAMF). Compared with MAML, MAMF discards the bi-level\noptimization and uses only first-order gradients, which makes it easily\nscalable and computationally efficient. Due to the uniform task sampling\nprocedure, MAMF consistently outperforms the classical fine-tuning method for\nfew-shot transfer learning on five benchmark datasets. Empirically, we further\ndiscover that the effectiveness of first-order MAML is highly dependent on the\nzero-shot performance of the pretrained model, and our simple algorithm can\noutperform first-order MAML on more challenging datasets with low zero-shot\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhenhailong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Manling Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Han Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What Matters For Meta-Learning Vision Regression Tasks?. (arXiv:2203.04905v1 [cs.CV])","link":"http://arxiv.org/abs/2203.04905","description":"<p>Meta-learning is widely used in few-shot classification and function\nregression due to its ability to quickly adapt to unseen tasks. However, it has\nnot yet been well explored on regression tasks with high dimensional inputs\nsuch as images. This paper makes two main contributions that help understand\nthis barely explored area. \\emph{First}, we design two new types of\ncross-category level vision regression tasks, namely object discovery and pose\nestimation of unprecedented complexity in the meta-learning domain for computer\nvision. To this end, we (i) exhaustively evaluate common meta-learning\ntechniques on these tasks, and (ii) quantitatively analyze the effect of\nvarious deep learning techniques commonly used in recent meta-learning\nalgorithms in order to strengthen the generalization capability: data\naugmentation, domain randomization, task augmentation and meta-regularization.\nFinally, we (iii) provide some insights and practical recommendations for\ntraining meta-learning algorithms on vision regression tasks. \\emph{Second}, we\npropose the addition of functional contrastive learning (FCL) over the task\nrepresentations in Conditional Neural Processes (CNPs) and train in an\nend-to-end fashion. The experimental results show that the results of prior\nwork are misleading as a consequence of a poor choice of the loss function as\nwell as too small meta-training sets. Specifically, we find that CNPs\noutperform MAML on most tasks without fine-tuning. Furthermore, we observe that\nnaive task augmentation without a tailored design results in underfitting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_N/0/1/0/all/0/1\">Ning Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ziesche_H/0/1/0/all/0/1\">Hanna Ziesche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vien_N/0/1/0/all/0/1\">Ngo Anh Vien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Volpp_M/0/1/0/all/0/1\">Michael Volpp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neumann_G/0/1/0/all/0/1\">Gerhard Neumann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pose Guided Multi-person Image Generation From Text. (arXiv:2203.04907v1 [cs.CV])","link":"http://arxiv.org/abs/2203.04907","description":"<p>Transformers have recently been shown to generate high quality images from\ntexts. However, existing methods struggle to create high fidelity full-body\nimages, especially multiple people. A person's pose has a high degree of\nfreedom that is difficult to describe using words only; this creates errors in\nthe generated image, such as incorrect body proportions and pose. We propose a\npose-guided text-to-image model, using pose as an additional input constraint.\nUsing the proposed Keypoint Pose Encoding (KPE) to encode human pose into low\ndimensional representation, our model can generate novel multi-person images\naccurately representing the pose and text descriptions provided, with minimal\nerrors. We demonstrate that KPE is invariant to changes in the target image\ndomain and image resolution; we show results on the Deepfashion dataset and\ncreate a new multi-person Deepfashion dataset to demonstrate the\nmulti-capabilities of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheong_S/0/1/0/all/0/1\">Soon Yau Cheong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mustafa_A/0/1/0/all/0/1\">Armin Mustafa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gilbert_A/0/1/0/all/0/1\">Andrew Gilbert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking data-driven point spread function modeling with a differentiable optical model. (arXiv:2203.04908v1 [astro-ph.IM])","link":"http://arxiv.org/abs/2203.04908","description":"<p>In astronomy, upcoming space telescopes with wide-field optical instruments\nhave a spatially varying point spread function (PSF). Certain scientific goals\nrequire a high-fidelity estimation of the PSF at target positions where no\ndirect measurement of the PSF is provided. Even though observations of the PSF\nare available at some positions of the field of view (FOV), they are\nundersampled, noisy, and integrated in wavelength in the instrument's passband.\nPSF modeling requires building a model from these observations that can infer a\nsuper-resolved PSF at any wavelength and any position in the FOV. Current\ndata-driven PSF models can tackle spatial variations and super-resolution, but\nare not capable of capturing chromatic variations. Our model, coined WaveDiff,\nproposes a paradigm shift in the data-driven modeling of the point spread\nfunction field of telescopes. By adding a differentiable optical forward model\ninto the modeling framework, we change the data-driven modeling space from the\npixels to the wavefront. The proposed model relies on efficient automatic\ndifferentiation technology as well as modern stochastic first-order\noptimization techniques recently developed by the thriving machine-learning\ncommunity. Our framework paves the way to building powerful models that are\nphysically motivated and do not require special calibration data. This paper\ndemonstrates the WaveDiff model on a simplified setting of a space telescope.\nThe proposed framework represents a performance breakthrough with respect to\nexisting data-driven approaches. The pixel reconstruction errors decrease\n6-fold at observation resolution and 44-fold for a 3x super-resolution. The\nellipticity errors are reduced by a factor of at least 20 and the size error by\na factor of more than 250. By only using noisy broad-band in-focus\nobservations, we successfully capture the PSF chromatic variations due to\ndiffraction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/astro-ph/1/au:+Liaudat_T/0/1/0/all/0/1\">Tobias Liaudat</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Starck_J/0/1/0/all/0/1\">Jean-Luc Starck</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Kilbinger_M/0/1/0/all/0/1\">Martin Kilbinger</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Frugier_P/0/1/0/all/0/1\">Pierre-Antoine Frugier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveling Down in Computer Vision: Pareto Inefficiencies in Fair Deep Classifiers. (arXiv:2203.04913v1 [cs.CV])","link":"http://arxiv.org/abs/2203.04913","description":"<p>Algorithmic fairness is frequently motivated in terms of a trade-off in which\noverall performance is decreased so as to improve performance on disadvantaged\ngroups where the algorithm would otherwise be less accurate. Contrary to this,\nwe find that applying existing fairness approaches to computer vision improve\nfairness by degrading the performance of classifiers across all groups (with\nincreased degradation on the best performing groups).\n</p>\n<p>Extending the bias-variance decomposition for classification to fairness, we\ntheoretically explain why the majority of fairness classifiers designed for low\ncapacity models should not be used in settings involving high-capacity models,\na scenario common to computer vision. We corroborate this analysis with\nextensive experimental support that shows that many of the fairness heuristics\nused in computer vision also degrade performance on the most disadvantaged\ngroups. Building on these insights, we propose an adaptive augmentation\nstrategy that, uniquely, of all methods tested, improves performance for the\ndisadvantaged groups.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zietlow_D/0/1/0/all/0/1\">Dominik Zietlow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lohaus_M/0/1/0/all/0/1\">Michael Lohaus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balakrishnan_G/0/1/0/all/0/1\">Guha Balakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kleindessner_M/0/1/0/all/0/1\">Matth&#xe4;us Kleindessner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Locatello_F/0/1/0/all/0/1\">Francesco Locatello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Russell_C/0/1/0/all/0/1\">Chris Russell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Triangular Character Animation Sampling with Motion, Emotion, and Relation. (arXiv:2203.04930v1 [cs.GR])","link":"http://arxiv.org/abs/2203.04930","description":"<p>Dramatic progress has been made in animating individual characters. However,\nwe still lack automatic control over activities between characters, especially\nthose involving interactions. In this paper, we present a novel energy-based\nframework to sample and synthesize animations by associating the characters'\nbody motions, facial expressions, and social relations. We propose a\nSpatial-Temporal And-Or graph (ST-AOG), a stochastic grammar model, to encode\nthe contextual relationship between motion, emotion, and relation, forming a\ntriangle in a conditional random field. We train our model from a labeled\ndataset of two-character interactions. Experiments demonstrate that our method\ncan recognize the social relation between two characters and sample new scenes\nof vivid motion and emotion using Markov Chain Monte Carlo (MCMC) given the\nsocial relation. Thus, our method can provide animators with an automatic way\nto generate 3D character animations, help synthesize interactions between\nNon-Player Characters (NPCs), and enhance machine emotion intelligence (EQ) in\nvirtual reality (VR).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yizhou Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_L/0/1/0/all/0/1\">Liang Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ai_W/0/1/0/all/0/1\">Wensi Ai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1\">Pan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Song-Chun Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the surprising tradeoff between ImageNet accuracy and perceptual similarity. (arXiv:2203.04946v1 [cs.CV])","link":"http://arxiv.org/abs/2203.04946","description":"<p>Perceptual distances between images, as measured in the space of pre-trained\ndeep features, have outperformed prior low-level, pixel-based metrics on\nassessing image similarity. While the capabilities of older and less accurate\nmodels such as AlexNet and VGG to capture perceptual similarity are well known,\nmodern and more accurate models are less studied. First, we observe a\nsurprising inverse correlation between ImageNet accuracy and Perceptual Scores\nof modern networks such as ResNets, EfficientNets, and Vision Transformers:\nthat is better classifiers achieve worse Perceptual Scores. Then, we perform a\nlarge-scale study and examine the ImageNet accuracy/Perceptual Score\nrelationship on varying the depth, width, number of training steps, weight\ndecay, label smoothing, and dropout. Higher accuracy improves Perceptual Score\nup to a certain point, but we uncover a Pareto frontier between accuracies and\nPerceptual Score in the mid-to-high accuracy regime. We explore this\nrelationship further using distortion invariance, spatial frequency\nsensitivity, and alternative perceptual functions. Interestingly we discover\nshallow ResNets, trained for less than 5 epochs only on ImageNet, whose\nemergent Perceptual Score matches the prior best networks trained directly on\nsupervised human perceptual judgements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_M/0/1/0/all/0/1\">Manoj Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Houlsby_N/0/1/0/all/0/1\">Neil Houlsby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalchbrenner_N/0/1/0/all/0/1\">Nal Kalchbrenner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cubuk_E/0/1/0/all/0/1\">Ekin D. Cubuk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Constrained Deep Networks: Lagrangian Optimization via Log-Barrier Extensions. (arXiv:1904.04205v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1904.04205","description":"<p>This study investigates imposing hard inequality constraints on the outputs\nof convolutional neural networks (CNN) during training. Several recent works\nshowed that the theoretical and practical advantages of Lagrangian optimization\nover simple penalties do not materialize in practice when dealing with modern\nCNNs involving millions of parameters. Therefore, constrained CNNs are\ntypically handled with penalties. We propose *log-barrier extensions*, which\napproximate Lagrangian optimization of constrained-CNN problems with a sequence\nof unconstrained losses. Unlike standard interior-point and log-barrier\nmethods, our formulation does not need an initial feasible solution. The\nproposed extension yields an upper bound on the duality gap -- generalizing the\nresult of standard log-barriers -- and yielding sub-optimality certificates for\nfeasible solutions. While sub-optimality is not guaranteed for non-convex\nproblems, this result shows that log-barrier extensions are a principled way to\napproximate Lagrangian optimization for constrained CNNs via implicit dual\nvariables. We report weakly supervised image segmentation experiments, with\nvarious constraints, showing that our formulation outperforms substantially the\nexisting constrained-CNN methods, in terms of accuracy, constraint satisfaction\nand training stability, more so when dealing with a large number of\nconstraints.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kervadec_H/0/1/0/all/0/1\">Hoel Kervadec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dolz_J/0/1/0/all/0/1\">Jose Dolz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Jing Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Desrosiers_C/0/1/0/all/0/1\">Christian Desrosiers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Granger_E/0/1/0/all/0/1\">Eric Granger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayed_I/0/1/0/all/0/1\">Ismail Ben Ayed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"U$^2$-Net: Going Deeper with Nested U-Structure for Salient Object Detection. (arXiv:2005.09007v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2005.09007","description":"<p>In this paper, we design a simple yet powerful deep network architecture,\nU$^2$-Net, for salient object detection (SOD). The architecture of our\nU$^2$-Net is a two-level nested U-structure. The design has the following\nadvantages: (1) it is able to capture more contextual information from\ndifferent scales thanks to the mixture of receptive fields of different sizes\nin our proposed ReSidual U-blocks (RSU), (2) it increases the depth of the\nwhole architecture without significantly increasing the computational cost\nbecause of the pooling operations used in these RSU blocks. This architecture\nenables us to train a deep network from scratch without using backbones from\nimage classification tasks. We instantiate two models of the proposed\narchitecture, U$^2$-Net (176.3 MB, 30 FPS on GTX 1080Ti GPU) and\nU$^2$-Net$^{\\dagger}$ (4.7 MB, 40 FPS), to facilitate the usage in different\nenvironments. Both models achieve competitive performance on six SOD datasets.\nThe code is available: https://github.com/NathanUA/U-2-Net.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qin_X/0/1/0/all/0/1\">Xuebin Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zichen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chenyang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dehghan_M/0/1/0/all/0/1\">Masood Dehghan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaiane_O/0/1/0/all/0/1\">Osmar R. Zaiane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jagersand_M/0/1/0/all/0/1\">Martin Jagersand</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UKPGAN: A General Self-Supervised Keypoint Detector. (arXiv:2011.11974v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.11974","description":"<p>Keypoint detection is an essential component for the object registration and\nalignment. In this work, we reckon keypoint detection as information\ncompression, and force the model to distill out irrelevant points of an object.\nBased on this, we propose UKPGAN, a general self-supervised 3D keypoint\ndetector where keypoints are detected so that they could reconstruct the\noriginal object shape. Two modules: GAN-based keypoint sparsity control and\nsalient information distillation modules are proposed to locate those important\nkeypoints. Extensive experiments show that our keypoints align well with human\nannotated keypoint labels, and can be applied to SMPL human bodies under\nvarious non-rigid deformations. Furthermore, our keypoint detector trained on\nclean object collections generalizes well to real-world scenarios, thus further\nimproves geometric registration when combined with off-the-shelf point\ndescriptors. Repeatability experiments show that our model is stable under both\nrigid and non-rigid transformations, with local reference frame estimation. Our\ncode is available on https://github.com/qq456cvb/UKPGAN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1\">Yang You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenhai Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ze_Y/0/1/0/all/0/1\">Yanjie Ze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yong-Lu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiming Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Cewu Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Canonical Voting: Towards Robust Oriented Bounding Box Detection in 3D Scenes. (arXiv:2011.12001v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.12001","description":"<p>3D object detection has attracted much attention thanks to the advances in\nsensors and deep learning methods for point clouds. Current state-of-the-art\nmethods like VoteNet regress direct offset towards object centers and box\norientations with an additional Multi-Layer-Perceptron network. Both their\noffset and orientation predictions are not accurate due to the fundamental\ndifficulty in rotation classification. In the work, we disentangle the direct\noffset into Local Canonical Coordinates (LCC), box scales and box orientations.\nOnly LCC and box scales are regressed, while box orientations are generated by\na canonical voting scheme. Finally, an LCC-aware back-projection checking\nalgorithm iteratively cuts out bounding boxes from the generated vote maps,\nwith the elimination of false positives. Our model achieves state-of-the-art\nperformance on three standard real-world benchmarks: ScanNet, SceneNN and SUN\nRGB-D. Our code is available on https://github.com/qq456cvb/CanonicalVoting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1\">Yang You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Z/0/1/0/all/0/1\">Zelin Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_Y/0/1/0/all/0/1\">Yujing Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chengkun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yong-Lu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lizhuang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiming Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Cewu Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PRISM: A Rich Class of Parameterized Submodular Information Measures for Guided Subset Selection. (arXiv:2103.00128v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.00128","description":"<p>With ever-increasing dataset sizes, subset selection techniques are becoming\nincreasingly important for a plethora of tasks. It is often necessary to guide\nthe subset selection to achieve certain desiderata, which includes focusing or\ntargeting certain data points, while avoiding others. Examples of such problems\ninclude: i)targeted learning, where the goal is to find subsets with rare\nclasses or rare attributes on which the model is underperforming, and ii)guided\nsummarization, where data (e.g., image collection, text, document or video) is\nsummarized for quicker human consumption with specific additional user intent.\nMotivated by such applications, we present PRISM, a rich class of PaRameterIzed\nSubmodular information Measures. Through novel functions and their\nparameterizations, PRISM offers a variety of modeling capabilities that enable\na trade-off between desired qualities of a subset like diversity or\nrepresentation and similarity/dissimilarity with a set of data points. We\ndemonstrate how PRISM can be applied to the two real-world problems mentioned\nabove, which require guided subset selection. In doing so, we show that PRISM\ninterestingly generalizes some past work, therein reinforcing its broad\nutility. Through extensive experiments on diverse datasets, we demonstrate the\nsuperiority of PRISM over the state-of-the-art in targeted learning and in\nguided image-collection summarization\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kothawade_S/0/1/0/all/0/1\">Suraj Kothawade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaushal_V/0/1/0/all/0/1\">Vishal Kaushal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramakrishnan_G/0/1/0/all/0/1\">Ganesh Ramakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bilmes_J/0/1/0/all/0/1\">Jeff Bilmes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyer_R/0/1/0/all/0/1\">Rishabh Iyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MOTR: End-to-End Multiple-Object Tracking with Transformer. (arXiv:2105.03247v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.03247","description":"<p>Temporal modeling of objects is a key challenge in multiple-object tracking\n(MOT). Existing methods track by associating detections through motion-based\nand appearance-based similarity heuristics. The post-processing nature of\nassociation prevents end-to-end exploitation of temporal variations in video\nsequence. In this paper, we propose MOTR, which extends DETR and introduces\n``track query'' to model the tracked instances in the entire video. Track query\nis transferred and updated frame-by-frame to perform iterative prediction over\ntime. We propose tracklet-aware label assignment to train track queries and\nnewborn object queries. We further propose temporal aggregation network and\ncollective average loss to enhance temporal relation modeling. Experimental\nresults on DanceTrack show that MOTR significantly outperforms state-of-the-art\nmethod, ByteTrack by 6.5% on HOTA metric. On MOT17, MOTR outperforms our\nconcurrent works, TrackFormer and TransTrack, on association performance. MOTR\ncan serve as a stronger baseline for future research on temporal modeling and\nTransformer-based trackers.Code is available at\nhttps://github.com/megvii-model/MOTR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_F/0/1/0/all/0/1\">Fangao Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_B/0/1/0/all/0/1\">Bin Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tiancai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yichen Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SCTN: Sparse Convolution-Transformer Network for Scene Flow Estimation. (arXiv:2105.04447v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.04447","description":"<p>We propose a novel scene flow estimation approach to capture and infer 3D\nmotions from point clouds. Estimating 3D motions for point clouds is\nchallenging, since a point cloud is unordered and its density is significantly\nnon-uniform. Such unstructured data poses difficulties in matching\ncorresponding points between point clouds, leading to inaccurate flow\nestimation. We propose a novel architecture named Sparse\nConvolution-Transformer Network (SCTN) that equips the sparse convolution with\nthe transformer. Specifically, by leveraging the sparse convolution, SCTN\ntransfers irregular point cloud into locally consistent flow features for\nestimating continuous and consistent motions within an object/local object\npart. We further propose to explicitly learn point relations using a point\ntransformer module, different from exiting methods. We show that the learned\nrelation-based contextual information is rich and helpful for matching\ncorresponding points, benefiting scene flow estimation. In addition, a novel\nloss function is proposed to adaptively encourage flow consistency according to\nfeature similarity. Extensive experiments demonstrate that our proposed\napproach achieves a new state of the art in scene flow estimation. Our approach\nachieves an error of 0.038 and 0.037 (EPE3D) on FlyingThings3D and KITTI Scene\nFlow respectively, which significantly outperforms previous methods by large\nmargins.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Cheng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giancola_S/0/1/0/all/0/1\">Silvio Giancola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bernard Ghanem</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SUPR-GAN: SUrgical PRediction GAN for Event Anticipation in Laparoscopic and Robotic Surgery. (arXiv:2105.04642v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.04642","description":"<p>Comprehension of surgical workflow is the foundation upon which artificial\nintelligence (AI) and machine learning (ML) holds the potential to assist\nintraoperative decision-making and risk mitigation. In this work, we move\nbeyond mere identification of past surgical phases, into the prediction of\nfuture surgical steps and specification of the transitions between them. We use\na novel Generative Adversarial Network (GAN) formulation to sample future\nsurgical phases trajectories conditioned on past video frames from laparoscopic\ncholecystectomy (LC) videos and compare it to state-of-the-art approaches for\nsurgical video analysis and alternative prediction methods. We demonstrate the\nGAN formulation's effectiveness through inferring and predicting the progress\nof LC videos. We quantify the horizon-accuracy trade-off and explored average\nperformance, as well as the performance on the more challenging, and clinically\nrelevant transitions between phases. Furthermore, we conduct a survey, asking\n16 surgeons of different specialties and educational levels to qualitatively\nevaluate predicted surgery phases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ban_Y/0/1/0/all/0/1\">Yutong Ban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosman_G/0/1/0/all/0/1\">Guy Rosman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eckhoff_J/0/1/0/all/0/1\">Jennifer A. Eckhoff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ward_T/0/1/0/all/0/1\">Thomas M. Ward</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashimoto_D/0/1/0/all/0/1\">Daniel A. Hashimoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kondo_T/0/1/0/all/0/1\">Taisei Kondo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iwaki_H/0/1/0/all/0/1\">Hidekazu Iwaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meireles_O/0/1/0/all/0/1\">Ozanan R. Meireles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rus_D/0/1/0/all/0/1\">Daniela Rus</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Map for Active Semantic Goal Navigation. (arXiv:2106.15648v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.15648","description":"<p>We consider the problem of object goal navigation in unseen environments.\nSolving this problem requires learning of contextual semantic priors, a\nchallenging endeavour given the spatial and semantic variability of indoor\nenvironments. Current methods learn to implicitly encode these priors through\ngoal-oriented navigation policy functions operating on spatial representations\nthat are limited to the agent's observable areas. In this work, we propose a\nnovel framework that actively learns to generate semantic maps outside the\nfield of view of the agent and leverages the uncertainty over the semantic\nclasses in the unobserved areas to decide on long term goals. We demonstrate\nthat through this spatial prediction strategy, we are able to learn semantic\npriors in scenes that can be leveraged in unknown environments. Additionally,\nwe show how different objectives can be defined by balancing exploration with\nexploitation during searching for semantic targets. Our method is validated in\nthe visually realistic environments of the Matterport3D dataset and show\nimproved results on object goal navigation over competitive baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Georgakis_G/0/1/0/all/0/1\">Georgios Georgakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bucher_B/0/1/0/all/0/1\">Bernadette Bucher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmeckpeper_K/0/1/0/all/0/1\">Karl Schmeckpeper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Siddharth Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daniilidis_K/0/1/0/all/0/1\">Kostas Daniilidis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"P-WAE: Generalized Patch-Wasserstein Autoencoder for Anomaly Screening. (arXiv:2108.03815v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.03815","description":"<p>Anomaly detection plays a pivotal role in numerous real-world scenarios, such\nas industrial automation and manufacturing intelligence. Recently, variational\ninference-based anomaly analysis has attracted researchers' and developers'\nattention. It aims to model the defect-free distribution so that anomalies can\nbe classified as out-of-distribution samples. Nevertheless, there are two\ndisturbing factors that need us to prioritize: (i) the simplistic prior latent\ndistribution inducing limited expressive capability; (ii) the strong\nprobability distance notion results in collapsed features. In this paper, we\npropose a novel Patch-wise Wasserstein AutoEncoder (P-WAE) architecture to\nalleviate those challenges. In particular, a patch-wise variational inference\nmodel coupled with solving the jigsaw puzzle is designed, which is a simple yet\neffective way to increase the expressiveness of the latent manifold. This makes\nusing the model on high-dimensional practical data possible. In addition, we\nleverage a weaker measure, sliced-Wasserstein distance, to achieve the\nequilibrium between the reconstruction fidelity and generalized\nrepresentations. Comprehensive experiments, conducted on the MVTec AD dataset,\ndemonstrate the superior performance of our proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yurong Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FEDI: Few-shot learning based on Earth Mover's Distance algorithm combined with deep residual network to identify diabetic retinopathy. (arXiv:2108.09711v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2108.09711","description":"<p>Diabetic retinopathy(DR) is the main cause of blindness in diabetic patients.\nHowever, DR can easily delay the occurrence of blindness through the diagnosis\nof the fundus. In view of the reality, it is difficult to collect a large\namount of diabetic retina data in clinical practice. This paper proposes a\nfew-shot learning model of a deep residual network based on Earth Mover's\nDistance algorithm to assist in diagnosing DR. We build training and validation\nclassification tasks for few-shot learning based on 39 categories of 1000\nsample data, train deep residual networks, and obtain experience maximization\npre-training models. Based on the weights of the pre-trained model, the Earth\nMover's Distance algorithm calculates the distance between the images, obtains\nthe similarity between the images, and changes the model's parameters to\nimprove the accuracy of the training model. Finally, the experimental\nconstruction of the small sample classification task of the test set to\noptimize the model further, and finally, an accuracy of 93.5667% on the\n3way10shot task of the diabetic retina test set. For the experimental code and\nresults, please refer to:\nhttps://github.com/panliangrui/few-shot-learning-funds.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Pan_L/0/1/0/all/0/1\">Liangrui Pan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ji_B/0/1/0/all/0/1\">Boya Ji</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xi_P/0/1/0/all/0/1\">Peng Xi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoqi Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chongcheawchamnan_M/0/1/0/all/0/1\">Mitchai Chongcheawchamnan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Peng_S/0/1/0/all/0/1\">Shaoliang Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DSNet: A Dual-Stream Framework for Weakly-Supervised Gigapixel Pathology Image Analysis. (arXiv:2109.05788v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.05788","description":"<p>We present a novel weakly-supervised framework for classifying whole slide\nimages (WSIs). WSIs, due to their gigapixel resolution, are commonly processed\nby patch-wise classification with patch-level labels. However, patch-level\nlabels require precise annotations, which is expensive and usually unavailable\non clinical data. With image-level labels only, patch-wise classification would\nbe sub-optimal due to inconsistency between the patch appearance and\nimage-level label. To address this issue, we posit that WSI analysis can be\neffectively conducted by integrating information at both high magnification\n(local) and low magnification (regional) levels. We auto-encode the visual\nsignals in each patch into a latent embedding vector representing local\ninformation, and down-sample the raw WSI to hardware-acceptable thumbnails\nrepresenting regional information. The WSI label is then predicted with a\nDual-Stream Network (DSNet), which takes the transformed local patch embeddings\nand multi-scale thumbnail images as inputs and can be trained by the\nimage-level label only. Experiments conducted on two large-scale public\ndatasets demonstrate that our method outperforms all recent state-of-the-art\nweakly-supervised WSI classification methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1\">Tiange Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chaoyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Dongnan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Fan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Heng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+ODonnell_L/0/1/0/all/0/1\">Lauren O&#x27;Donnell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_W/0/1/0/all/0/1\">Weidong Cai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Foreground Object Structure Transfer for Unsupervised Domain Adaptation. (arXiv:2109.06543v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.06543","description":"<p>Unsupervised domain adaptation aims to train a classification model from the\nlabeled source domain for the unlabeled target domain. Since the data\ndistributions of the two domains are different, the model often performs poorly\non the target domain. Existing methods align the feature distributions of the\nsource and target domains and learn domain-invariant features to improve the\nperformance of the model. However, the features are usually aligned as a whole,\nand the domain adaptation task fails to serve the classification, which will\nignore the class information and lead to misalignment.In this paper, we\ninvestigate those features that should be used for domain alignment, introduce\nprior knowledge to extract foreground features to guide the domain adaptation\ntask for classification tasks, and perform alignment in the local structure of\nobjects. We propose a method called Foreground Object Structure Transfer(FOST).\nThe key to FOST is the new clustering based condition, which combines the\nrelative position relationship of foreground objects. Based on this conditions,\nFOST makes the data distribution of the same class more compact in geometry. In\npractice, since the label of the target domain is not available, we use the\nclustering information of the source domain to assign pseudo labels to the\ntarget domain samples, and then according to the source domain data prior\nknowledge guides those positive features to maximum the inter-class distance\nbetween different classes and mimimum the intra-class distance. Extensive\nexperimental results on various benchmarks ($i.e.$ ImageCLEF-DA, Office-31,\nOffice-Home, Visda-2017) under different domain adaptation settings prove that\nour FOST compares favorably against the existing state-of-the-art domain\nadaptation methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1\">Jieren Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Le Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xiangyan Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_W/0/1/0/all/0/1\">Wenxuan Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Boyi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Ke Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Da_Q/0/1/0/all/0/1\">Qiaobo Da</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yue Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A review of deep learning methods for MRI reconstruction. (arXiv:2109.08618v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2109.08618","description":"<p>Following the success of deep learning in a wide range of applications,\nneural network-based machine-learning techniques have received significant\ninterest for accelerating magnetic resonance imaging (MRI) acquisition and\nreconstruction strategies. A number of ideas inspired by deep learning\ntechniques for computer vision and image processing have been successfully\napplied to nonlinear image reconstruction in the spirit of compressed sensing\nfor accelerated MRI. Given the rapidly growing nature of the field, it is\nimperative to consolidate and summarize the large number of deep learning\nmethods that have been reported in the literature, to obtain a better\nunderstanding of the field in general. This article provides an overview of the\nrecent developments in neural-network based approaches that have been proposed\nspecifically for improving parallel imaging. A general background and\nintroduction to parallel MRI is also given from a classical view of k-space\nbased reconstruction methods. Image domain based techniques that introduce\nimproved regularizers are covered along with k-space based methods which focus\non better interpolation strategies using neural networks. While the field is\nrapidly evolving with plenty of papers published each year, in this review, we\nattempt to cover broad categories of methods that have shown good performance\non publicly available data sets. Limitations and open problems are also\ndiscussed and recent efforts for producing open data sets and benchmarks for\nthe community are examined.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Pal_A/0/1/0/all/0/1\">Arghya Pal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rathi_Y/0/1/0/all/0/1\">Yogesh Rathi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AirLoop: Lifelong Loop Closure Detection. (arXiv:2109.08975v3 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2109.08975","description":"<p>Loop closure detection is an important building block that ensures the\naccuracy and robustness of simultaneous localization and mapping (SLAM)\nsystems. Due to their generalization ability, CNN-based approaches have\nreceived increasing attention. Although they normally benefit from training on\ndatasets that are diverse and reflective of the environments, new environments\noften emerge after the model is deployed. It is therefore desirable to\nincorporate the data newly collected during operation for incremental learning.\nNevertheless, simply finetuning the model on new data is infeasible since it\nmay cause the model's performance on previously learned data to degrade over\ntime, which is also known as the problem of catastrophic forgetting. In this\npaper, we present AirLoop, a method that leverages techniques from lifelong\nlearning to minimize forgetting when training loop closure detection models\nincrementally. We experimentally demonstrate the effectiveness of AirLoop on\nTartanAir, Nordland, and RobotCar datasets. To the best of our knowledge,\nAirLoop is one of the first works to achieve lifelong learning of deep loop\nclosure detectors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_D/0/1/0/all/0/1\">Dasong Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scherer_S/0/1/0/all/0/1\">Sebastian Scherer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AirDOS: Dynamic SLAM benefits from Articulated Objects. (arXiv:2109.09903v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2109.09903","description":"<p>Dynamic Object-aware SLAM (DOS) exploits object-level information to enable\nrobust motion estimation in dynamic environments. Existing methods mainly focus\non identifying and excluding dynamic objects from the optimization. In this\npaper, we show that feature-based visual SLAM systems can also benefit from the\npresence of dynamic articulated objects by taking advantage of two\nobservations: (1) The 3D structure of each rigid part of articulated object\nremains consistent over time; (2) The points on the same rigid part follow the\nsame motion. In particular, we present AirDOS, a dynamic object-aware system\nthat introduces rigidity and motion constraints to model articulated objects.\nBy jointly optimizing the camera pose, object motion, and the object 3D\nstructure, we can rectify the camera pose estimation, preventing tracking loss,\nand generate 4D spatio-temporal maps for both dynamic objects and static\nscenes. Experiments show that our algorithm improves the robustness of visual\nSLAM algorithms in challenging crowded urban environments. To the best of our\nknowledge, AirDOS is the first dynamic object-aware SLAM system demonstrating\nthat camera pose estimation can be improved by incorporating dynamic\narticulated objects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1\">Yuheng Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenshan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henein_M/0/1/0/all/0/1\">Mina Henein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scherer_S/0/1/0/all/0/1\">Sebastian Scherer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"L-Verse: Bidirectional Generation Between Image and Text. (arXiv:2111.11133v7 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.11133","description":"<p>Far beyond learning long-range interactions of natural language, transformers\nare becoming the de-facto standard for many vision tasks with their power and\nscalability. Especially with cross-modal tasks between image and text, vector\nquantized variational autoencoders (VQ-VAEs) are widely used to make a raw RGB\nimage into a sequence of feature vectors. To better leverage the correlation\nbetween image and text, we propose L-Verse, a novel architecture consisting of\nfeature-augmented variational autoencoder (AugVAE) and bidirectional\nauto-regressive transformer (BiART) for text-to-image and image-to-text\ngeneration. Our AugVAE shows the state-of-the-art reconstruction performance on\nImageNet1K validation set, along with the robustness to unseen images in the\nwild. Unlike other models, BiART can distinguish between image (or text) as a\nconditional reference and a generation target. L-Verse can be directly used for\nimage-to-text or text-to-image generation tasks without any finetuning or extra\nobject detection framework. In quantitative and qualitative experiments,\nL-Verse shows impressive results against previous methods in both image-to-text\nand text-to-image generation on MS-COCO Captions. We furthermore assess the\nscalability of L-Verse architecture on Conceptual Captions and present the\ninitial results of bidirectional vision-language representation learning on\ngeneral domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Taehoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_G/0/1/0/all/0/1\">Gwangmo Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sihaeng Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sangyun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_Y/0/1/0/all/0/1\">Yewon Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Soonyoung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seung Hwan Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Honglak Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bae_K/0/1/0/all/0/1\">Kyunghoon Bae</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Focal and Global Knowledge Distillation for Detectors. (arXiv:2111.11837v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.11837","description":"<p>Knowledge distillation has been applied to image classification successfully.\nHowever, object detection is much more sophisticated and most knowledge\ndistillation methods have failed on it. In this paper, we point out that in\nobject detection, the features of the teacher and student vary greatly in\ndifferent areas, especially in the foreground and background. If we distill\nthem equally, the uneven differences between feature maps will negatively\naffect the distillation. Thus, we propose Focal and Global Distillation (FGD).\nFocal distillation separates the foreground and background, forcing the student\nto focus on the teacher's critical pixels and channels. Global distillation\nrebuilds the relation between different pixels and transfers it from teachers\nto students, compensating for missing global information in focal distillation.\nAs our method only needs to calculate the loss on the feature map, FGD can be\napplied to various detectors. We experiment on various detectors with different\nbackbones and the results show that the student detector achieves excellent mAP\nimprovement. For example, ResNet-50 based RetinaNet, Faster RCNN, RepPoints and\nMask RCNN with our distillation method achieve 40.7%, 42.0%, 42.0% and 42.1%\nmAP on COCO2017, which are 3.3, 3.6, 3.4 and 2.9 higher than the baseline,\nrespectively. Our codes are available at https://github.com/yzd-v/FGD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhendong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhe Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xiaohu Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yuan Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zehuan Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Danpei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1\">Chun Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reconstruction Student with Attention for Student-Teacher Pyramid Matching. (arXiv:2111.15376v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.15376","description":"<p>Anomaly detection and localization are important problems in computer vision.\nRecently, Convolutional Neural Network (CNN) has been used for visual\ninspection. In particular, the scarcity of anomalous samples increases the\ndifficulty of this task, and unsupervised leaning based methods are attracting\nattention. We focus on Student-Teacher Feature Pyramid Matching (STPM) which\ncan be trained from only normal images with small number of epochs. Here we\nproposed a powerful method which compensates for the shortcomings of STPM.\nProposed method consists of two students and two teachers that a pair of\nstudent-teacher network is the same as STPM. The other student-teacher network\nhas a role to reconstruct the features of normal products. By reconstructing\nthe features of normal products from an abnormal image, it is possible to\ndetect abnormalities with higher accuracy by taking the difference between\nthem. The new student-teacher network uses attention modules and different\nteacher network from the original STPM. Attention mechanism acts to\nsuccessfully reconstruct the normal regions in an input image. Different\nteacher network prevents looking at the same regions as the original STPM. Six\nanomaly maps obtained from the two student-teacher networks are used to\ncalculate the final anomaly map. Student-teacher network for reconstructing\nfeatures improved AUC scores for pixel level and image level in comparison with\nthe original STPM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yamada_S/0/1/0/all/0/1\">Shinji Yamada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hotta_K/0/1/0/all/0/1\">Kazuhiro Hotta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Information Theoretic Representation Distillation. (arXiv:2112.00459v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.00459","description":"<p>Despite the empirical success of knowledge distillation, current\nstate-of-the-art methods are computationally expensive to train, which makes\nthem difficult to adopt in practice. To address this problem, we introduce two\ndistinct complementary losses inspired by a cheap entropy-like estimator. These\nlosses aim to maximise the correlation and mutual information between the\nstudent and teacher representations. Our method incurs significantly less\ntraining overheads than other approaches and achieves competitive performance\nto state-of-the-art on the knowledge distillation and cross-model transfer\ntasks. We further demonstrate the effectiveness of our method on a binary\ndistillation task, whereby it leads to a new state-of-the-art for binary\nquantisation and approaches the performance of a full precision model. The\ncode, evaluation protocols, and trained models will be publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Miles_R/0/1/0/all/0/1\">Roy Miles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_A/0/1/0/all/0/1\">Adri&#xe1;n L&#xf3;pez Rodr&#xed;guez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mikolajczyk_K/0/1/0/all/0/1\">Krystian Mikolajczyk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Active Learning for Domain Adaptation: An Energy-Based Approach. (arXiv:2112.01406v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2112.01406","description":"<p>Unsupervised domain adaptation has recently emerged as an effective paradigm\nfor generalizing deep neural networks to new target domains. However, there is\nstill enormous potential to be tapped to reach the fully supervised\nperformance. In this paper, we present a novel active learning strategy to\nassist knowledge transfer in the target domain, dubbed active domain\nadaptation. We start from an observation that energy-based models exhibit\n\\textit{free energy biases} when training (source) and test (target) data come\nfrom different distributions. Inspired by this inherent mechanism, we\nempirically reveal that a simple yet efficient energy-based sampling strategy\nsheds light on selecting the most valuable target samples than existing\napproaches requiring particular architectures or computation of the distances.\nOur algorithm, Energy-based Active Domain Adaptation (EADA), queries groups of\ntarget data that incorporate both domain characteristic and instance\nuncertainty into every selection round. Meanwhile, by aligning the free energy\nof target data compact around the source domain via a regularization term,\ndomain gap can be implicitly diminished. Through extensive experiments, we show\nthat EADA surpasses state-of-the-art methods on well-known challenging\nbenchmarks with substantial improvements, making it a useful option in the open\nworld. Code is available at https://github.com/BIT-DA/EADA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_B/0/1/0/all/0/1\">Binhui Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Longhui Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chi Harold Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xinjing Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guoren Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generate Point Clouds with Multiscale Details from Graph-Represented Structures. (arXiv:2112.06433v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.06433","description":"<p>As details are missing in most representations of structures, the lack of\ncontrollability to details is one of the major weaknesses in structure-based\ncontrollable point cloud generation. It is observable that definitions of\ndetails and structures are subjective. Details can be treated as structures on\nsmall scales. To represent structures in different scales at the same time, we\npresent a graph-based representation of structures called the Multiscale\nStructure Graph(MSG). Given structures in multiple scales, similar patterns of\nlocal structures can be found at different scales, positions, and angles. The\nknowledge learned from a local structure pattern shall be transferred to other\nsimilar patterns. An encoding and generation mechanism, namely the Multiscale\nStructure-based Point Cloud Generator(MSPCG) is proposed, which can\nsimultaneously learn point cloud generation from local patterns with\nmiscellaneous spatial properties. By editing the MSG, the proposed method\nsupports multiscale editions on point clouds. By generating point clouds from\nlocal structures and learning simultaneously in multiple scales, our MSPCG has\nbetter generalization ability and scalability. Trained on the ShapeNet, our\nMSPCG is able to generate point clouds for unseen categories and generate\nindoor scenes from a given structure. The experimental results show that our\nmethod significantly outperforms baseline methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Ximing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhengfu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1\">Cheng Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image-Adaptive YOLO for Object Detection in Adverse Weather Conditions. (arXiv:2112.08088v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.08088","description":"<p>Though deep learning-based object detection methods have achieved promising\nresults on the conventional datasets, it is still challenging to locate objects\nfrom the low-quality images captured in adverse weather conditions. The\nexisting methods either have difficulties in balancing the tasks of image\nenhancement and object detection, or often ignore the latent information\nbeneficial for detection. To alleviate this problem, we propose a novel\nImage-Adaptive YOLO (IA-YOLO) framework, where each image can be adaptively\nenhanced for better detection performance. Specifically, a differentiable image\nprocessing (DIP) module is presented to take into account the adverse weather\nconditions for YOLO detector, whose parameters are predicted by a small\nconvolutional neural net-work (CNN-PP). We learn CNN-PP and YOLOv3 jointly in\nan end-to-end fashion, which ensures that CNN-PP can learn an appropriate DIP\nto enhance the image for detection in a weakly supervised manner. Our proposed\nIA-YOLO approach can adaptively process images in both normal and adverse\nweather conditions. The experimental results are very encouraging,\ndemonstrating the effectiveness of our proposed IA-YOLO method in both foggy\nand low-light scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_G/0/1/0/all/0/1\">Gaofeng Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_R/0/1/0/all/0/1\">Runsheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Shi Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jianke Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End Learning of Multi-category 3D Pose and Shape Estimation. (arXiv:2112.10196v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.10196","description":"<p>In this paper, we study the representation of the shape and pose of objects\nusing their keypoints. Therefore, we propose an end-to-end method that\nsimultaneously detects 2D keypoints from an image and lifts them to 3D. The\nproposed method learns both 2D detection and 3D lifting only from 2D keypoints\nannotations. In addition to being end-to-end from images to 3D keypoints, our\nmethod also handles objects from multiple categories using a single neural\nnetwork. We use a Transformer-based architecture to detect the keypoints, as\nwell as to summarize the visual context of the image. This visual context\ninformation is then used while lifting the keypoints to 3D, to allow\ncontext-based reasoning for better performance. Our method can handle\nocclusions as well as a wide variety of object classes. Our experiments on\nthree benchmarks demonstrate that our method performs better than the\nstate-of-the-art. Our source code will be made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Can_Y/0/1/0/all/0/1\">Yigit Baran Can</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liniger_A/0/1/0/all/0/1\">Alexander Liniger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paudel_D/0/1/0/all/0/1\">Danda Pani Paudel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Fine-grained Class Clustering via Generative Adversarial Networks. (arXiv:2112.14971v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.14971","description":"<p>Unsupervised fine-grained class clustering is a practical yet challenging\ntask due to the difficulty of feature representations learning of subtle object\ndetails. We introduce C3-GAN, a method that leverages the categorical inference\npower of InfoGAN with contrastive learning. We aim to learn feature\nrepresentations that encourage a dataset to form distinct cluster boundaries in\nthe embedding space, while also maximizing the mutual information between the\nlatent code and its image observation. Our approach is to train a\ndiscriminator, which is also used for inferring clusters, to optimize the\ncontrastive loss, where image-latent pairs that maximize the mutual information\nare considered as positive pairs and the rest as negative pairs. Specifically,\nwe map the input of a generator, which was sampled from the categorical\ndistribution, to the embedding space of the discriminator and let them act as a\ncluster centroid. In this way, C3-GAN succeeded in learning a\nclustering-friendly embedding space where each cluster is distinctively\nseparable. Experimental results show that C3-GAN achieved the state-of-the-art\nclustering performance on four fine-grained image datasets, while also\nalleviating the mode collapse phenomenon. Code is available at\nhttps://github.com/naver-ai/c3-gan.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yunji Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ha_J/0/1/0/all/0/1\">Jung-Woo Ha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DenseTact: Optical Tactile Sensor for Dense Shape Reconstruction. (arXiv:2201.01367v4 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2201.01367","description":"<p>Increasing the performance of tactile sensing in robots enables versatile,\nin-hand manipulation. Vision-based tactile sensors have been widely used as\nrich tactile feedback has been shown to be correlated with increased\nperformance in manipulation tasks. Existing tactile sensor solutions with high\nresolution have limitations that include low accuracy, expensive components, or\nlack of scalability. In this paper, an inexpensive, scalable, and compact\ntactile sensor with high-resolution surface deformation modeling for surface\nreconstruction of the 3D sensor surface is proposed. By measuring the image\nfrom the fisheye camera, it is shown that the sensor can successfully estimate\nthe surface deformation in real-time (1.8ms) by using deep convolutional neural\nnetworks. This sensor in its design and sensing abilities represents a\nsignificant step toward better object in-hand localization, classification, and\nsurface estimation all enabled by high-resolution shape reconstruction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Do_W/0/1/0/all/0/1\">Won Kyung Do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kennedy_M/0/1/0/all/0/1\">Monroe Kennedy III</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spectral Compressive Imaging Reconstruction Using Convolution and Contextual Transformer. (arXiv:2201.05768v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2201.05768","description":"<p>Spectral compressive imaging (SCI) is able to encode the high-dimensional\nhyperspectral image to a 2D measurement, and then uses algorithms to\nreconstruct the spatio-spectral data-cube. At present, the main bottleneck of\nSCI is the reconstruction algorithm, and the state-of-the-art (SOTA)\nreconstruction methods generally face the problem of long reconstruction time\nand/or poor detail recovery. In this paper, we propose a novel hybrid network\nmodule, namely CCoT (Convolution and Contextual Transformer) block, which can\nacquire the inductive bias ability of convolution and the powerful modeling\nability of transformer simultaneously,and is conducive to improving the quality\nof reconstruction to restore fine details. We integrate the proposed CCoT block\ninto deep unfolding framework based on the generalized alternating projection\nalgorithm, and further propose the GAP-CCoT network. Finally, we apply the\nGAP-CCoT algorithm to SCI reconstruction. Through the experiments of extensive\nsynthetic and real data, our proposed model achieves higher reconstruction\nquality ($&gt;$2dB in PSNR on simulated benchmark datasets) and shorter running\ntime than existing SOTA algorithms by a large margin. The code and models will\nbe released to the public.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_L/0/1/0/all/0/1\">Lishun Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_Z/0/1/0/all/0/1\">Zongliang Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhong_Y/0/1/0/all/0/1\">Yong Zhong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yuan_X/0/1/0/all/0/1\">Xin Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scale-arbitrary Invertible Image Downscaling. (arXiv:2201.12576v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.12576","description":"<p>Conventional social media platforms usually downscale the HR images to\nrestrict their resolution to a specific size for saving transmission/storage\ncost, which leads to the super-resolution (SR) being highly ill-posed. Recent\ninvertible image downscaling methods jointly model the downscaling/upscaling\nproblems and achieve significant improvements. However, they only consider\nfixed integer scale factors that cannot downscale HR images with various\nresolutions to meet the resolution restriction of social media platforms. In\nthis paper, we propose a scale-Arbitrary Invertible image Downscaling Network\n(AIDN), to natively downscale HR images with arbitrary scale factors.\nMeanwhile, the HR information is embedded in the downscaled low-resolution (LR)\ncounterparts in a nearly imperceptible form such that our AIDN can also restore\nthe original HR images solely from the LR images. The key to supporting\narbitrary scale factors is our proposed Conditional Resampling Module (CRM)\nthat conditions the downscaling/upscaling kernels and sampling locations on\nboth scale factors and image content. Extensive experimental results\ndemonstrate that our AIDN achieves top performance for invertible downscaling\nwith both arbitrary integer and non-integer scale factors. Code will be\nreleased upon publication.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xing_J/0/1/0/all/0/1\">Jinbo Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Wenbo Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_T/0/1/0/all/0/1\">Tien-Tsin Wong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MHSnet: Multi-head and Spatial Attention Network with False-Positive Reduction for Pulmonary Nodules Detection. (arXiv:2201.13392v5 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2201.13392","description":"<p>The mortality of lung cancer has ranked high among cancers for many years.\nEarly detection of lung cancer is critical for disease prevention, cure, and\nmortality rate reduction. However, existing detection methods on pulmonary\nnodules introduce an excessive number of false positive proposals in order to\nachieve high sensitivity, which is not practical in clinical situations. In\nthis paper, we propose the multi-head detection and spatial\nsqueeze-and-attention network, MHSnet, to detect pulmonary nodules, in order to\naid doctors in the early diagnosis of lung cancers. Specifically, we first\nintroduce multi-head detectors and skip connections to customize for the\nvariety of nodules in sizes, shapes and types and capture multi-scale features.\nThen, we implement a spatial attention module to enable the network to focus on\ndifferent regions differently inspired by how experienced clinicians screen CT\nimages, which results in fewer false positive proposals. Lastly, we present a\nlightweight but effective false positive reduction module with the Linear\nRegression model to cut down the number of false positive proposals, without\nany constraints on the front network. Extensive experimental results compared\nwith the state-of-the-art models have shown the superiority of the MHSnet in\nterms of the average FROC, sensitivity and especially false discovery rate\n(2.98% and 2.18% improvement in terms of average FROC and sensitivity, 5.62%\nand 28.33% decrease in terms of false discovery rate and average candidates per\nscan). The false positive reduction module significantly decreases the average\nnumber of candidates generated per scan by 68.11% and the false discovery rate\nby 13.48%, which is promising to reduce distracted proposals for the downstream\ntasks based on the detection results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Mai_J/0/1/0/all/0/1\">Juanyun Mai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_M/0/1/0/all/0/1\">Minghao Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_J/0/1/0/all/0/1\">Jiayin Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shao_Y/0/1/0/all/0/1\">Yanbo Shao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Diao_Z/0/1/0/all/0/1\">Zhaoqi Diao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fu_X/0/1/0/all/0/1\">Xinliang Fu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1\">Yulong Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiao_J/0/1/0/all/0/1\">Jianyu Xiao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+You_J/0/1/0/all/0/1\">Jian You</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yin_A/0/1/0/all/0/1\">Airu Yin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_Y/0/1/0/all/0/1\">Yang Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qiu_X/0/1/0/all/0/1\">Xiangcheng Qiu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tao_J/0/1/0/all/0/1\">Jinsheng Tao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_B/0/1/0/all/0/1\">Bo Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ji_H/0/1/0/all/0/1\">Hua Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Grounding Answers for Visual Questions Asked by Visually Impaired People. (arXiv:2202.01993v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.01993","description":"<p>Visual question answering is the task of answering questions about images. We\nintroduce the VizWiz-VQA-Grounding dataset, the first dataset that visually\ngrounds answers to visual questions asked by people with visual impairments. We\nanalyze our dataset and compare it with five VQA-Grounding datasets to\ndemonstrate what makes it similar and different. We then evaluate the SOTA VQA\nand VQA-Grounding models and demonstrate that current SOTA algorithms often\nfail to identify the correct visual evidence where the answer is located. These\nmodels regularly struggle when the visual evidence occupies a small fraction of\nthe image, for images that are higher quality, as well as for visual questions\nthat require skills in text recognition. The dataset, evaluation server, and\nleaderboard all can be found at the following link:\nhttps://vizwiz.org/tasks-and-datasets/answer-grounding-for-vqa/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chongyan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anjum_S/0/1/0/all/0/1\">Samreen Anjum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurari_D/0/1/0/all/0/1\">Danna Gurari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Roto-Translation Equivariant Super-Resolution of Two-Dimensional Flows Using Convolutional Neural Networks. (arXiv:2202.11099v2 [physics.flu-dyn] UPDATED)","link":"http://arxiv.org/abs/2202.11099","description":"<p>Convolutional neural networks (CNNs) often apparently process vectors as\nquantities that have no direction such as colors in images. This study\ninvestigates the effect of considering vectors as geometric objects in terms of\nsuper-resolution of velocity on two-dimensional fluids. Vector is distinguished\nfrom scalar by the transformation law associated with a change in basis, which\ncan be incorporated as the prior knowledge using the equivariant deep learning.\nThe existing CNNs are converted into equivariant ones by rendering each layer\nequivariant with respect to rotation and translation. The training data in the\nlow- and high-resolution are generated with the downsampling or spectral\nnudging. With the date inheriting the rotational symmetry, the equivariant CNNs\nexhibit comparable accuracy with the non-equivariant ones. Owing to the smaller\nnumber of parameters in the equivariant CNNs, these models are trainable with a\nsmaller size of the data. In this case, the transformation law of vector should\nbe incorporated as the prior knowledge, where vector is treated as a quantity\nhaving direction. Two examples are presented to demonstrate the possibility of\nsymmetry of the data being broken. In the first case, the downsampling method\nrenders the correspondence between low- and high-resolution patterns dependent\non the orientation. In the second case, the input data are insufficient to\nrecognize the rotation of coordinates. In both cases, the accuracy of the CNNs\ndeteriorates if the equivariance is forcefully imposed, and the conventional\nCNNs should be used, where vector is not processed as a quantity having\ndirection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Yasuda_Y/0/1/0/all/0/1\">Yuki Yasuda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-supervised Deep Learning for Image Classification with Distribution Mismatch: A Survey. (arXiv:2203.00190v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.00190","description":"<p>Deep learning methodologies have been employed in several different fields,\nwith an outstanding success in image recognition applications, such as material\nquality control, medical imaging, autonomous driving, etc. Deep learning models\nrely on the abundance of labelled observations to train a prospective model.\nThese models are composed of millions of parameters to estimate, increasing the\nneed of more training observations. Frequently it is expensive to gather\nlabelled observations of data, making the usage of deep learning models not\nideal, as the model might over-fit data. In a semi-supervised setting,\nunlabelled data is used to improve the levels of accuracy and generalization of\na model with small labelled datasets. Nevertheless, in many situations\ndifferent unlabelled data sources might be available. This raises the risk of a\nsignificant distribution mismatch between the labelled and unlabelled datasets.\nSuch phenomena can cause a considerable performance hit to typical\nsemi-supervised deep learning frameworks, which often assume that both labelled\nand unlabelled datasets are drawn from similar distributions. Therefore, in\nthis paper we study the latest approaches for semi-supervised deep learning for\nimage recognition. Emphasis is made in semi-supervised deep learning models\ndesigned to deal with a distribution mismatch between the labelled and\nunlabelled datasets. We address open challenges with the aim to encourage the\ncommunity to tackle them, and overcome the high data demand of traditional deep\nlearning pipelines under real-world usage settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Calderon_Ramirez_S/0/1/0/all/0/1\">Saul Calderon-Ramirez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shengxiang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elizondo_D/0/1/0/all/0/1\">David Elizondo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Autoregressive Image Generation using Residual Quantization. (arXiv:2203.01941v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.01941","description":"<p>For autoregressive (AR) modeling of high-resolution images, vector\nquantization (VQ) represents an image as a sequence of discrete codes. A short\nsequence length is important for an AR model to reduce its computational costs\nto consider long-range interactions of codes. However, we postulate that\nprevious VQ cannot shorten the code sequence and generate high-fidelity images\ntogether in terms of the rate-distortion trade-off. In this study, we propose\nthe two-stage framework, which consists of Residual-Quantized VAE (RQ-VAE) and\nRQ-Transformer, to effectively generate high-resolution images. Given a fixed\ncodebook size, RQ-VAE can precisely approximate a feature map of an image and\nrepresent the image as a stacked map of discrete codes. Then, RQ-Transformer\nlearns to predict the quantized feature vector at the next position by\npredicting the next stack of codes. Thanks to the precise approximation of\nRQ-VAE, we can represent a 256$\\times$256 image as 8$\\times$8 resolution of the\nfeature map, and RQ-Transformer can efficiently reduce the computational costs.\nConsequently, our framework outperforms the existing AR models on various\nbenchmarks of unconditional and conditional image generation. Our approach also\nhas a significantly faster sampling speed than previous AR models to generate\nhigh-quality images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Doyup Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1\">Chiheon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Saehoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_M/0/1/0/all/0/1\">Minsu Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1\">Wook-Shin Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast Neural Architecture Search for Lightweight Dense Prediction Networks. (arXiv:2203.01994v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.01994","description":"<p>We present LDP, a lightweight dense prediction neural architecture search\n(NAS) framework. Starting from a pre-defined generic backbone, LDP applies the\nnovel Assisted Tabu Search for efficient architecture exploration. LDP is fast\nand suitable for various dense estimation problems, unlike previous NAS methods\nthat are either computational demanding or deployed only for a single subtask.\nThe performance of LPD is evaluated on monocular depth estimation, semantic\nsegmentation, and image super-resolution tasks on diverse datasets, including\nNYU-Depth-v2, KITTI, Cityscapes, COCO-stuff, DIV2K, Set5, Set14, BSD100,\nUrban100. Experiments show that the proposed framework yields consistent\nimprovements on all tested dense prediction tasks, while being $5\\%-315\\%$ more\ncompact in terms of the number of model parameters than prior arts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huynh_L/0/1/0/all/0/1\">Lam Huynh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahtu_E/0/1/0/all/0/1\">Esa Rahtu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matas_J/0/1/0/all/0/1\">Jiri Matas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heikkila_J/0/1/0/all/0/1\">Janne Heikkila</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformations in Learned Image Compression from a Modulation Perspective. (arXiv:2203.02158v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2203.02158","description":"<p>In this paper, a unified transformation method in learned image\ncompression(LIC) is proposed from the perspective of modulation. Firstly, the\nquantization in LIC is considered as a generalized channel with additive\nuniform noise. Moreover, the LIC is interpreted as a particular communication\nsystem according to the consistency in structures and optimization objectives.\nThus, the technology of communication systems can be applied to guide the\ndesign of modules in LIC. Furthermore, a unified transform method based on\nsignal modulation (TSM) is defined. In the view of TSM, the existing\ntransformation methods are mathematically reduced to a linear modulation. A\nseries of transformation methods, e.g. TPM and TJM, are obtained by extending\nto nonlinear modulation. The experimental results on various datasets and\nbackbone architectures verify that the effectiveness and robustness of the\nproposed method. More importantly, it further confirms the feasibility of\nguiding LIC design from a communication perspective. For example, when backbone\narchitecture is hyperprior combining context model, our method achieves\n3.52$\\%$ BD-rate reduction over GDN on Kodak dataset without increasing\ncomplexity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bao_Y/0/1/0/all/0/1\">Youneng Bao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meng_F/0/1/0/all/0/1\">Fangyang Meng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tan_W/0/1/0/all/0/1\">Wen Tan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_C/0/1/0/all/0/1\">Chao Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tian_Y/0/1/0/all/0/1\">Yonghong Tian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liang_Y/0/1/0/all/0/1\">Yongsheng Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OPAL: Occlusion Pattern Aware Loss for Unsupervised Light Field Disparity Estimation. (arXiv:2203.02231v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.02231","description":"<p>Light field disparity estimation is an essential task in computer vision with\nvarious applications. Although supervised learning-based methods have achieved\nboth higher accuracy and efficiency than traditional optimization-based\nmethods, the dependency on ground-truth disparity for training limits the\noverall generalization performance not to say for real-world scenarios where\nthe ground-truth disparity is hard to capture. In this paper, we argue that\nunsupervised methods can achieve comparable accuracy, but, more importantly,\nmuch higher generalization capacity and efficiency than supervised methods.\nSpecifically, we present the Occlusion Pattern Aware Loss, named OPAL, which\nsuccessfully extracts and encodes the general occlusion patterns inherent in\nthe light field for loss calculation. OPAL enables: i) accurate and robust\nestimation by effectively handling occlusions without using any ground-truth\ninformation for training and ii) much efficient performance by significantly\nreducing the network parameters required for accurate inference. Besides, a\ntransformer-based network and a refinement module are proposed for achieving\neven more accurate results. Extensive experiments demonstrate our method not\nonly significantly improves the accuracy compared with the SOTA unsupervised\nmethods, but also possesses strong generalization capacity, even for real-world\ndata, compared with supervised methods. Our code will be made publicly\navailable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jiayin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jingyao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_C/0/1/0/all/0/1\">Chao Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haoqian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tao Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Highly Accurate Dichotomous Image Segmentation. (arXiv:2203.03041v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.03041","description":"<p>We present a systematic study on a new task called dichotomous image\nsegmentation (DIS), which aims to segment highly accurate objects from natural\nimages. To this end, we collected the first large-scale dataset, called DIS5K,\nwhich contains 5,470 high-resolution (e.g., 2K, 4K or larger) images covering\ncamouflaged, salient, or meticulous objects in various backgrounds. All images\nare annotated with extremely fine-grained labels. In addition, we introduce a\nsimple intermediate supervision baseline (IS-Net) using both feature-level and\nmask-level guidance for DIS model training. Without tricks, IS-Net outperforms\nvarious cutting-edge baselines on the proposed DIS5K, making it a general\nself-learned supervision network that can help facilitate future research in\nDIS. Further, we design a new metric called human correction efforts (HCE)\nwhich approximates the number of mouse clicking operations required to correct\nthe false positives and false negatives. HCE is utilized to measure the gap\nbetween models and real-world applications and thus can complement existing\nmetrics. Finally, we conduct the largest-scale benchmark, evaluating 16\nrepresentative segmentation models, providing a more insightful discussion\nregarding object complexities, and showing several potential applications\n(e.g., background removal, art design, 3D reconstruction). Hoping these efforts\ncan open up promising directions for both academic and industries. We will\nrelease our DIS5K dataset, IS-Net baseline, HCE metric, and the complete\nbenchmark results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qin_X/0/1/0/all/0/1\">Xuebin Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1\">Hang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaobin Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1\">Deng-Ping Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_a/0/1/0/all/0/1\">and Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comprehensive Review of Deep Learning-Based 3D Point Cloud Completion Processing and Analysis. (arXiv:2203.03311v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.03311","description":"<p>Point cloud completion is a generation and estimation issue derived from the\npartial point clouds, which plays a vital role in the applications in 3D\ncomputer vision. The progress of deep learning (DL) has impressively improved\nthe capability and robustness of point cloud completion. However, the quality\nof completed point clouds is still needed to be further enhanced to meet the\npractical utilization. Therefore, this work aims to conduct a comprehensive\nsurvey on various methods, including point-based, convolution-based,\ngraph-based, and generative model-based approaches, etc. And this survey\nsummarizes the comparisons among these methods to provoke further research\ninsights. Besides, this review sums up the commonly used datasets and\nillustrates the applications of point cloud completion. Eventually, we also\ndiscussed possible research trends in this promptly expanding field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fei_B/0/1/0/all/0/1\">Ben Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Weidong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wenming Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhijun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yikang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1\">Tao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xing Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lipeng Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Student Become Decathlon Master in Retinal Vessel Segmentation via Dual-teacher Multi-target Domain Adaptation. (arXiv:2203.03631v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2203.03631","description":"<p>Unsupervised domain adaptation has been proposed recently to tackle the\nso-called domain shift between training data and test data with different\ndistributions. However, most of them only focus on single-target domain\nadaptation and cannot be applied to the scenario with multiple target domains.\nIn this paper, we propose RVms, a novel unsupervised multi-target domain\nadaptation approach to segment retinal vessels (RVs) from multimodal and\nmulticenter retinal images. RVms mainly consists of a style augmentation and\ntransfer (SAT) module and a dual-teacher knowledge distillation (DTKD) module.\nSAT augments and clusters images into source-similar domains and\nsource-dissimilar domains via B\\'ezier and Fourier transformations. DTKD\nutilizes the augmented and transformed data to train two teachers, one for\nsource-similar domains and the other for source-dissimilar domains. Afterwards,\nknowledge distillation is performed to iteratively distill different domain\nknowledge from teachers to a generic student. The local relative intensity\ntransformation is employed to characterize RVs in a domain invariant manner and\npromote the generalizability of teachers and student models. Moreover, we\nconstruct a new multimodal and multicenter vascular segmentation dataset from\nexisting publicly-available datasets, which can be used to benchmark various\ndomain adaptation and domain generalization methods. Through extensive\nexperiments, RVms is found to be very close to the target-trained Oracle in\nterms of segmenting the RVs, largely outperforming other state-of-the-art\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Peng_L/0/1/0/all/0/1\">Linkai Peng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_L/0/1/0/all/0/1\">Li Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheng_P/0/1/0/all/0/1\">Pujin Cheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+He_H/0/1/0/all/0/1\">Huaqing He</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tang_X/0/1/0/all/0/1\">Xiaoying Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Shadows can be Dangerous: Stealthy and Effective Physical-world Adversarial Attack by Natural Phenomenon. (arXiv:2203.03818v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.03818","description":"<p>Estimating the risk level of adversarial examples is essential for safely\ndeploying machine learning models in the real world. One popular approach for\nphysical-world attacks is to adopt the \"sticker-pasting\" strategy, which\nhowever suffers from some limitations, including difficulties in access to the\ntarget or printing by valid colors. A new type of non-invasive attacks emerged\nrecently, which attempt to cast perturbation onto the target by optics based\ntools, such as laser beam and projector. However, the added optical patterns\nare artificial but not natural. Thus, they are still conspicuous and\nattention-grabbed, and can be easily noticed by humans. In this paper, we study\na new type of optical adversarial examples, in which the perturbations are\ngenerated by a very common natural phenomenon, shadow, to achieve naturalistic\nand stealthy physical-world adversarial attack under the black-box setting. We\nextensively evaluate the effectiveness of this new attack on both simulated and\nreal-world environments. Experimental results on traffic sign recognition\ndemonstrate that our algorithm can generate adversarial examples effectively,\nreaching 98.23% and 90.47% success rates on LISA and GTSRB test sets\nrespectively, while continuously misleading a moving camera over 95% of the\ntime in real-world scenarios. We also offer discussions about the limitations\nand the defense mechanism of this attack.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1\">Yiqi Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xianming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_D/0/1/0/all/0/1\">Deming Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Junjun Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1\">Xiangyang Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Group Transformer: A General Vision Transformer Backbone with Dynamic Group Attention. (arXiv:2203.03937v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.03937","description":"<p>Recently, Transformers have shown promising performance in various vision\ntasks. To reduce the quadratic computation complexity caused by each query\nattending to all keys/values, various methods have constrained the range of\nattention within local regions, where each query only attends to keys/values\nwithin a hand-crafted window. However, these hand-crafted window partition\nmechanisms are data-agnostic and ignore their input content, so it is likely\nthat one query maybe attends to irrelevant keys/values. To address this issue,\nwe propose a Dynamic Group Attention (DG-Attention), which dynamically divides\nall queries into multiple groups and selects the most relevant keys/values for\neach group. Our DG-Attention can flexibly model more relevant dependencies\nwithout any spatial constraint that is used in hand-crafted window based\nattention. Built on the DG-Attention, we develop a general vision transformer\nbackbone named Dynamic Group Transformer (DGT). Extensive experiments show that\nour models can outperform the state-of-the-art methods on multiple common\nvision tasks, including image classification, semantic segmentation, object\ndetection, and instance segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kai Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tianyi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Cong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_G/0/1/0/all/0/1\">Guodong Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding person identification via gait. (arXiv:2203.04179v2 [cs.CR] UPDATED)","link":"http://arxiv.org/abs/2203.04179","description":"<p>Gait recognition is the process of identifying humans from their bipedal\nlocomotion such as walking or running. As such gait data is privacy sensitive\ninformation and should be anonymized. With the rise of more and higher quality\ngait recording techniques, such as depth cameras or motion capture suits, an\nincreasing amount of high-quality gait data becomes available which requires\nanonymization. As a first step towards developing anonymization techniques for\nhigh-quality gait data, we study different aspects of movement data to quantify\ntheir contribution to the gait recognition process. We first extract categories\nof features from the literature on human gait perception and then design\ncomputational experiments for each of the categories which we run against a\ngait recognition system. Our results show that gait anonymization is a\nchallenging process as the data is highly redundant and interdependent.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hanisch_S/0/1/0/all/0/1\">Simon Hanisch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muschter_E/0/1/0/all/0/1\">Evelyn Muschter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chatzipanagioti_A/0/1/0/all/0/1\">Adamantini Chatzipanagioti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shu-Chen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strufe_T/0/1/0/all/0/1\">Thorsten Strufe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-03-09T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","admin":"http://webns.net/mvcb/"}}]}]}