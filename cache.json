{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-02-24T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"A New Generation of Perspective API: Efficient Multilingual Character-level Transformers. (arXiv:2202.11176v1 [cs.CL])","link":"http://arxiv.org/abs/2202.11176","description":"<p>On the world wide web, toxic content detectors are a crucial line of defense\nagainst potentially hateful and offensive messages. As such, building highly\neffective classifiers that enable a safer internet is an important research\narea. Moreover, the web is a highly multilingual, cross-cultural community that\ndevelops its own lingo over time. As such, it is crucial to develop models that\nare effective across a diverse range of languages, usages, and styles. In this\npaper, we present the fundamentals behind the next version of the Perspective\nAPI from Google Jigsaw. At the heart of the approach is a single multilingual\ntoken-free Charformer model that is applicable across a range of languages,\ndomains, and tasks. We demonstrate that by forgoing static vocabularies, we\ngain flexibility across a variety of settings. We additionally outline the\ntechniques employed to make such a byte-level model efficient and feasible for\nproductionization. Through extensive experiments on multilingual toxic comment\nclassification benchmarks derived from real API traffic and evaluation on an\narray of code-switching, covert toxicity, emoji-based hate, human-readable\nobfuscation, distribution shift, and bias evaluation settings, we show that our\nproposed approach outperforms strong baselines. Finally, we present our\nfindings from deploying this system in production.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lees_A/0/1/0/all/0/1\">Alyssa Lees</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_V/0/1/0/all/0/1\">Vinh Q. Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sorensen_J/0/1/0/all/0/1\">Jeffrey Sorensen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_J/0/1/0/all/0/1\">Jai Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1\">Donald Metzler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasserman_L/0/1/0/all/0/1\">Lucy Vasserman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Augmented BERT Mutual Network in Multi-turn Spoken Dialogues. (arXiv:2202.11299v1 [cs.CL])","link":"http://arxiv.org/abs/2202.11299","description":"<p>Modern spoken language understanding (SLU) systems rely on sophisticated\nsemantic notions revealed in single utterances to detect intents and slots.\nHowever, they lack the capability of modeling multi-turn dynamics within a\ndialogue particularly in long-term slot contexts. Without external knowledge,\ndepending on limited linguistic legitimacy within a word sequence may overlook\ndeep semantic information across dialogue turns. In this paper, we propose to\nequip a BERT-based joint model with a knowledge attention module to mutually\nleverage dialogue contexts between two SLU tasks. A gating mechanism is further\nutilized to filter out irrelevant knowledge triples and to circumvent\ndistracting comprehension. Experimental results in two complicated multi-turn\ndialogue datasets have demonstrate by mutually modeling two SLU tasks with\nfiltered knowledge and dialogue contexts, our approach has considerable\nimprovements compared with several competitive baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Ting-Wei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Juang_B/0/1/0/all/0/1\">Biing-Hwang Juang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Commonsense Reasoning for Identifying and Understanding the Implicit Need of Help and Synthesizing Assistive Actions. (arXiv:2202.11337v1 [cs.AI])","link":"http://arxiv.org/abs/2202.11337","description":"<p>Human-Robot Interaction (HRI) is an emerging subfield of service robotics.\nWhile most existing approaches rely on explicit signals (i.e. voice, gesture)\nto engage, current literature is lacking solutions to address implicit user\nneeds. In this paper, we present an architecture to (a) detect user implicit\nneed of help and (b) generate a set of assistive actions without prior\nlearning. Task (a) will be performed using state-of-the-art solutions for Scene\nGraph Generation coupled to the use of commonsense knowledge; whereas, task (b)\nwill be performed using additional commonsense knowledge as well as a sentiment\nanalysis on graph structure. Finally, we propose an evaluation of our solution\nusing established benchmarks (e.g. ActionGenome dataset) along with human\nexperiments. The main motivation of our approach is the embedding of the\nperception-decision-action loop in a single architecture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Neau_M/0/1/0/all/0/1\">Ma&#xeb;lic Neau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santos_P/0/1/0/all/0/1\">Paulo Santos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bosser_A/0/1/0/all/0/1\">Anne-Gwenn Bosser</a> (ENIB), <a href=\"http://arxiv.org/find/cs/1/au:+Beu_N/0/1/0/all/0/1\">Nathan Beu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buche_C/0/1/0/all/0/1\">C&#xe9;dric Buche</a> (Lab-STICC\\_RAMBO)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prompt-Learning for Short Text Classification. (arXiv:2202.11345v1 [cs.CL])","link":"http://arxiv.org/abs/2202.11345","description":"<p>In the short text, the extreme short length, feature sparsity and high\nambiguity pose huge challenge to classification tasks. Recently, as an\neffective method for tuning Pre-trained Language Models for specific downstream\ntasks, prompt-learning has attracted vast amount of attention and research. The\nmain intuition behind the prompt-learning is to insert template into the input\nand convert the text classification tasks into equivalent cloze-style tasks.\nHowever, most prompt-learning methods expand label words manually or only\nconsider the class name for knowledge incorporating in cloze-style prediction,\nwhich will inevitably incurred omissions and bias in classification tasks. In\nthis paper, we propose a simple short text classification approach that makes\nuse of prompt-learning based on knowledgeable expansion, which can consider\nboth the short text itself and class name during expanding label words space.\nSpecifically, the top $N$ concepts related to the entity in short text are\nretrieved from the open Knowledge Graph like Probase, and we further refine the\nexpanded label words by the distance calculation between selected concepts and\nclass label. Experimental results show that our approach obtains obvious\nimprovement compared with other fine-tuning, prompt-learning and knowledgeable\nprompt-tuning methods, outperforming the state-of-the-art by up to 6 Accuracy\npoints on three well-known datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xinke Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiang_J/0/1/0/all/0/1\">Jipeng Qiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yunhao Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xindong Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enabling arbitrary translation objectives with Adaptive Tree Search. (arXiv:2202.11444v1 [cs.CL])","link":"http://arxiv.org/abs/2202.11444","description":"<p>We introduce an adaptive tree search algorithm, that can find high-scoring\noutputs under translation models that make no assumptions about the form or\nstructure of the search objective. This algorithm -- a deterministic variant of\nMonte Carlo tree search -- enables the exploration of new kinds of models that\nare unencumbered by constraints imposed to make decoding tractable, such as\nautoregressivity or conditional independence assumptions. When applied to\nautoregressive models, our algorithm has different biases than beam search has,\nwhich enables a new analysis of the role of decoding bias in autoregressive\nmodels. Empirically, we show that our adaptive tree search algorithm finds\noutputs with substantially better model scores compared to beam search in\nautoregressive models, and compared to reranking techniques in models whose\nscores do not decompose additively with respect to the words in the output. We\nalso characterise the correlation of several translation model objectives with\nrespect to BLEU. We find that while some standard models are poorly calibrated\nand benefit from the beam search bias, other often more robust models\n(autoregressive models tuned to maximize expected automatic metric scores, the\nnoisy channel model and a newly proposed objective) benefit from increasing\namounts of search using our proposed decoder, whereas the beam search bias\nlimits the improvements obtained from such objectives. Thus, we argue that as\nmodels improve, the improvements may be masked by over-reliance on beam search\nor reranking based methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ling_W/0/1/0/all/0/1\">Wang Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stokowiec_W/0/1/0/all/0/1\">Wojciech Stokowiec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Donato_D/0/1/0/all/0/1\">Domenic Donato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sartran_L/0/1/0/all/0/1\">Laurent Sartran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Lei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matthews_A/0/1/0/all/0/1\">Austin Matthews</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dyer_C/0/1/0/all/0/1\">Chris Dyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-shot Cross-lingual Transfer of Prompt-based Tuning with a Unified Multilingual Prompt. (arXiv:2202.11451v1 [cs.CL])","link":"http://arxiv.org/abs/2202.11451","description":"<p>Prompt-based tuning has been proven effective for pretrained language models\n(PLMs). While most of the existing work focuses on the monolingual prompts, we\nstudy the multilingual prompts for multilingual PLMs, especially in the\nzero-shot cross-lingual setting. To alleviate the effort of designing different\nprompts for multiple languages, we propose a novel model that uses a unified\nprompt for all languages, called UniPrompt. Different from the discrete prompts\nand soft prompts, the unified prompt is model-based and language-agnostic.\nSpecifically, the unified prompt is initialized by a multilingual PLM to\nproduce language-independent representation, after which is fused with the text\ninput. During inference, the prompts can be pre-computed so that no extra\ncomputation cost is needed. To collocate with the unified prompt, we propose a\nnew initialization method for the target label word to further improve the\nmodel's transferability across languages. Extensive experiments show that our\nproposed methods can significantly outperform the strong baselines across\ndifferent languages. We will release data and code to facilitate future\nresearch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Lianzhe Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shuming Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongdong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Houfeng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Short-answer scoring with ensembles of pretrained language models. (arXiv:2202.11558v1 [cs.CL])","link":"http://arxiv.org/abs/2202.11558","description":"<p>We investigate the effectiveness of ensembles of pretrained transformer-based\nlanguage models on short answer questions using the Kaggle Automated Short\nAnswer Scoring dataset. We fine-tune a collection of popular small, base, and\nlarge pretrained transformer-based language models, and train one feature-base\nmodel on the dataset with the aim of testing ensembles of these models. We used\nan early stopping mechanism and hyperparameter optimization in training. We\nobserve that generally that the larger models perform slightly better, however,\nthey still fall short of state-of-the-art results one their own. Once we\nconsider ensembles of models, there are ensembles of a number of large networks\nthat do produce state-of-the-art results, however, these ensembles are too\nlarge to realistically be put in a production environment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ormerod_C/0/1/0/all/0/1\">Christopher Ormerod</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Refining the state-of-the-art in Machine Translation, optimizing NMT for the JA <-> EN language pair by leveraging personal domain expertise. (arXiv:2202.11669v1 [cs.CL])","link":"http://arxiv.org/abs/2202.11669","description":"<p>Documenting the construction of an NMT (Neural Machine Translation) system\nfor En/Ja based on the Transformer architecture leveraging the OpenNMT\nframework. A systematic exploration of corpora pre-processing, hyperparameter\ntuning and model architecture is carried out to obtain optimal performance. The\nsystem is evaluated using standard auto-evaluation metrics such as BLEU, and my\nsubjective opinion as a Japanese linguist.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bieda_M/0/1/0/all/0/1\">Matthew Bieda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MuMiN: A Large-Scale Multilingual Multimodal Fact-Checked Misinformation Social Network Dataset. (arXiv:2202.11684v1 [cs.LG])","link":"http://arxiv.org/abs/2202.11684","description":"<p>Misinformation is becoming increasingly prevalent on social media and in news\narticles. It has become so widespread that we require algorithmic assistance\nutilising machine learning to detect such content. Training these machine\nlearning models require datasets of sufficient scale, diversity and quality.\nHowever, datasets in the field of automatic misinformation detection are\npredominantly monolingual, include a limited amount of modalities and are not\nof sufficient scale and quality. Addressing this, we develop a data collection\nand linking system (MuMiN-trawl), to build a public misinformation graph\ndataset (MuMiN), containing rich social media data (tweets, replies, users,\nimages, articles, hashtags) spanning 21 million tweets belonging to 26 thousand\nTwitter threads, each of which have been semantically linked to 13 thousand\nfact-checked claims across dozens of topics, events and domains, in 41\ndifferent languages, spanning more than a decade. The dataset is made available\nas a heterogeneous graph via a Python package (mumin). We provide baseline\nresults for two node classification tasks related to the veracity of a claim\ninvolving social media, and demonstrate that these are challenging tasks, with\nthe highest macro-average F1-score being 62.55% and 61.45% for the two tasks,\nrespectively. The MuMiN ecosystem is available at\nhttps://mumin-dataset.github.io/, including the data, documentation, tutorials\nand leaderboards.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nielsen_D/0/1/0/all/0/1\">Dan Saattrup Nielsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McConville_R/0/1/0/all/0/1\">Ryan McConville</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COLD Decoding: Energy-based Constrained Text Generation with Langevin Dynamics. (arXiv:2202.11705v1 [cs.CL])","link":"http://arxiv.org/abs/2202.11705","description":"<p>Many applications of text generation require incorporating different\nconstraints to control the semantics or style of generated text. These\nconstraints can be hard (e.g., ensuring certain keywords are included in the\noutput) and soft (e.g., contextualizing the output with the left- or right-hand\ncontext). In this paper, we present Energy-based Constrained Decoding with\nLangevin Dynamics (COLD), a decoding framework which unifies constrained\ngeneration as specifying constraints through an energy function, then\nperforming efficient differentiable reasoning over the constraints through\ngradient-based sampling. COLD decoding is a flexible framework that can be\napplied directly to off-the-shelf left-to-right language models without the\nneed for any task-specific fine-tuning, as demonstrated through three\nchallenging text generation applications: lexically-constrained generation,\nabductive reasoning, and counterfactual reasoning. Our experiments on these\nconstrained generation tasks point to the effectiveness of our approach, both\nin terms of automatic and human evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qin_L/0/1/0/all/0/1\">Lianhui Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Welleck_S/0/1/0/all/0/1\">Sean Welleck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khashabi_D/0/1/0/all/0/1\">Daniel Khashabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Finding Moments in Video Collections Using Natural Language. (arXiv:1907.12763v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1907.12763","description":"<p>We introduce the task of retrieving relevant video moments from a large\ncorpus of untrimmed, unsegmented videos given a natural language query. Our\ntask poses unique challenges as a system must efficiently identify both the\nrelevant videos and localize the relevant moments in the videos. To address\nthese challenges, we propose SpatioTemporal Alignment with Language (STAL), a\nmodel that represents a video moment as a set of regions within a series of\nshort video clips and aligns a natural language query to the moment's regions.\nOur alignment cost compares variable-length language and video features using\nsymmetric squared Chamfer distance, which allows for efficient indexing and\nretrieval of the video moments. Moreover, aligning language features to regions\nwithin a video moment allows for finer alignment compared to methods that\nextract only an aggregate feature from the entire video moment. We evaluate our\napproach on two recently proposed datasets for temporal localization of moments\nin video with natural language (DiDeMo and Charades-STA) extended to our video\ncorpus moment retrieval setting. We show that our STAL re-ranking model\noutperforms the recently proposed Moment Context Network on all criteria across\nall datasets on our proposed task, obtaining relative gains of 37% - 118% for\naverage recall and up to 30% for median rank. Moreover, our approach achieves\nmore than 130x faster retrieval and 8x smaller index size with a 1M video\ncorpus in an approximate setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Escorcia_V/0/1/0/all/0/1\">Victor Escorcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soldan_M/0/1/0/all/0/1\">Mattia Soldan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sivic_J/0/1/0/all/0/1\">Josef Sivic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bernard Ghanem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Russell_B/0/1/0/all/0/1\">Bryan Russell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain-based Latent Personal Analysis and its use for impersonation detection in social media. (arXiv:2004.02346v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2004.02346","description":"<p>Zipf's law defines an inverse proportion between a word's ranking in a given\ncorpus and its frequency in it, roughly dividing the vocabulary into frequent\nwords and infrequent ones. Here, we stipulate that within a domain an author's\nsignature can be derived from, in loose terms, the author's missing popular\nwords and frequently used infrequent-words. We devise a method, termed Latent\nPersonal Analysis (LPA), for finding domain-based attributes for entities in a\ndomain: their distance from the domain and their signature, which determines\nhow they most differ from a domain. We identify the most suitable distance\nmetric for the method among several and construct the distances and personal\nsignatures for authors, the domain's entities. The signature consists of both\nover-used terms (compared to the average), and missing popular terms. We\nvalidate the correctness and power of the signatures in identifying users and\nset existence conditions. We then show uses for the method in explainable\nauthorship attribution: we define algorithms that utilize LPA to identify two\ntypes of impersonation in social media: (1) authors with sockpuppets (multiple)\naccounts; (2) front users accounts, operated by several authors. We validate\nthe algorithms and employ them over a large scale dataset obtained from a\nsocial media site with over 4000 users. We corroborate these results using\ntemporal rate analysis. LPA can further be used to devise personal attributes\nin a wide range of scientific domains in which the constituents have a\nlong-tail distribution of elements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mokryn_O/0/1/0/all/0/1\">Osnat Mokryn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ben_Shoshan_H/0/1/0/all/0/1\">Hagit Ben-Shoshan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Introducing various Semantic Models for Amharic: Experimentation and Evaluation with multiple Tasks and Datasets. (arXiv:2011.01154v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2011.01154","description":"<p>The availability of different pre-trained semantic models enabled the quick\ndevelopment of machine learning components for downstream applications. Despite\nthe availability of abundant text data for low resource languages, only a few\nsemantic models are publicly available. Publicly available pre-trained models\nare usually built as a multilingual version of semantic models that can not fit\nwell for each language due to context variations. In this work, we introduce\ndifferent semantic models for Amharic. After we experiment with the existing\npre-trained semantic models, we trained and fine-tuned nine new different\nmodels using a monolingual text corpus. The models are build using word2Vec\nembeddings, distributional thesaurus (DT), contextual embeddings, and DT\nembeddings obtained via network embedding algorithms. Moreover, we employ these\nmodels for different NLP tasks and investigate their impact. We find that newly\ntrained models perform better than pre-trained multilingual models.\nFurthermore, models based on contextual embeddings from RoBERTA perform better\nthan the word2Vec models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yimam_S/0/1/0/all/0/1\">Seid Muhie Yimam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayele_A/0/1/0/all/0/1\">Abinew Ali Ayele</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkatesh_G/0/1/0/all/0/1\">Gopalakrishnan Venkatesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gashaw_I/0/1/0/all/0/1\">Ibrahim Gashaw</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biemann_C/0/1/0/all/0/1\">Chris Biemann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A2P-MANN: Adaptive Attention Inference Hops Pruned Memory-Augmented Neural Networks. (arXiv:2101.09693v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2101.09693","description":"<p>In this work, to limit the number of required attention inference hops in\nmemory-augmented neural networks, we propose an online adaptive approach called\nA2P-MANN. By exploiting a small neural network classifier, an adequate number\nof attention inference hops for the input query is determined. The technique\nresults in elimination of a large number of unnecessary computations in\nextracting the correct answer. In addition, to further lower computations in\nA2P-MANN, we suggest pruning weights of the final FC (fully-connected) layers.\nTo this end, two pruning approaches, one with negligible accuracy loss and the\nother with controllable loss on the final accuracy, are developed. The efficacy\nof the technique is assessed by using the twenty question-answering (QA) tasks\nof bAbI dataset. The analytical assessment reveals, on average, more than 42%\nfewer computations compared to the baseline MANN at the cost of less than 1%\naccuracy loss. In addition, when used along with the previously published\nzero-skipping technique, a computation count reduction of up to 68% is\nachieved. Finally, when the proposed approach (without zero-skipping) is\nimplemented on the CPU and GPU platforms, up to 43% runtime reduction is\nachieved.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ahmadzadeh_M/0/1/0/all/0/1\">Mohsen Ahmadzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamal_M/0/1/0/all/0/1\">Mehdi Kamal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Afzali_Kusha_A/0/1/0/all/0/1\">Ali Afzali-Kusha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pedram_M/0/1/0/all/0/1\">Massoud Pedram</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Charformer: Fast Character Transformers via Gradient-based Subword Tokenization. (arXiv:2106.12672v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.12672","description":"<p>State-of-the-art models in natural language processing rely on separate rigid\nsubword tokenization algorithms, which limit their generalization ability and\nadaptation to new settings. In this paper, we propose a new model inductive\nbias that learns a subword tokenization end-to-end as part of the model. To\nthis end, we introduce a soft gradient-based subword tokenization module (GBST)\nthat automatically learns latent subword representations from characters in a\ndata-driven fashion. Concretely, GBST enumerates candidate subword blocks and\nlearns to score them in a position-wise fashion using a block scoring network.\nWe additionally introduce Charformer, a deep Transformer model that integrates\nGBST and operates on the byte level. Via extensive experiments on English GLUE,\nmultilingual, and noisy text datasets, we show that Charformer outperforms a\nseries of competitive byte-level baselines while generally performing on par\nand sometimes outperforming subword-based models. Additionally, Charformer is\nfast, improving the speed of both vanilla byte-level and subword-level\nTransformers by 28%-100% while maintaining competitive quality. We believe this\nwork paves the way for highly performant token-free models that are trained\ncompletely end-to-end.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_V/0/1/0/all/0/1\">Vinh Q. Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruder_S/0/1/0/all/0/1\">Sebastian Ruder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_J/0/1/0/all/0/1\">Jai Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_H/0/1/0/all/0/1\">Hyung Won Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahri_D/0/1/0/all/0/1\">Dara Bahri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1\">Zhen Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baumgartner_S/0/1/0/all/0/1\">Simon Baumgartner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1\">Cong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1\">Donald Metzler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Small-Text: Active Learning for Text Classification in Python. (arXiv:2107.10314v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2107.10314","description":"<p>We present small-text, a simple and modular active learning library, which\noffers pool-based active learning for single- and multi-label text\nclassification in Python. It comes with various pre-implemented\nstate-of-the-art query strategies, including some that can leverage the GPU.\nClearly defined interfaces allow the combination of a multitude of classifiers,\nquery strategies, and stopping criteria, thereby facilitating a quick mix and\nmatch, and enabling a rapid development of both active learning experiments and\napplications. To make various classifiers accessible in a consistent way, it\nintegrates several well-known existing machine learning libraries, namely,\nscikit-learn, PyTorch, and huggingface transformers, where the latter\nintegrations are available as optionally installable extensions, making the\navailability of a GPU competely optional. The library is available under the\nMIT License at https://github.com/webis-de/small-text.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schroder_C/0/1/0/all/0/1\">Christopher Schr&#xf6;der</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_L/0/1/0/all/0/1\">Lydia M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niekler_A/0/1/0/all/0/1\">Andreas Niekler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potthast_M/0/1/0/all/0/1\">Martin Potthast</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"It's not Rocket Science : Interpreting Figurative Language in Narratives. (arXiv:2109.00087v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.00087","description":"<p>Figurative language is ubiquitous in English. Yet, the vast majority of NLP\nresearch focuses on literal language. Existing text representations by design\nrely on compositionality, while figurative language is often non-compositional.\nIn this paper, we study the interpretation of two non-compositional figurative\nlanguages (idioms and similes). We collected datasets of fictional narratives\ncontaining a figurative expression along with crowd-sourced plausible and\nimplausible continuations relying on the correct interpretation of the\nexpression. We then trained models to choose or generate the plausible\ncontinuation. Our experiments show that models based solely on pre-trained\nlanguage models perform substantially worse than humans on these tasks. We\nadditionally propose knowledge-enhanced models, adopting human strategies for\ninterpreting figurative language types : inferring meaning from the context and\nrelying on the constituent words' literal meanings. The knowledge-enhanced\nmodels improve the performance on both the discriminative and generative tasks,\nfurther bridging the gap from human performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chakrabarty_T/0/1/0/all/0/1\">Tuhin Chakrabarty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shwartz_V/0/1/0/all/0/1\">Vered Shwartz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformer Models for Text Coherence Assessment. (arXiv:2109.02176v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.02176","description":"<p>Coherence is an important aspect of text quality and is crucial for ensuring\nits readability. It is essential desirable for outputs from text generation\nsystems like summarization, question answering, machine translation, question\ngeneration, table-to-text, etc. An automated coherence scoring model is also\nhelpful in essay scoring or providing writing feedback. A large body of\nprevious work has leveraged entity-based methods, syntactic patterns, discourse\nrelations, and more recently traditional deep learning architectures for text\ncoherence assessment. Previous work suffers from drawbacks like the inability\nto handle long-range dependencies, out-of-vocabulary words, or model sequence\ninformation. We hypothesize that coherence assessment is a cognitively complex\ntask that requires deeper models and can benefit from other related tasks.\nAccordingly, in this paper, we propose four different Transformer-based\narchitectures for the task: vanilla Transformer, hierarchical Transformer,\nmulti-task learning-based model, and a model with fact-based input\nrepresentation. Our experiments with popular benchmark datasets across multiple\ndomains on four different coherence assessment tasks demonstrate that our\nmodels achieve state-of-the-art results outperforming existing models by a good\nmargin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abhishek_T/0/1/0/all/0/1\">Tushar Abhishek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rawat_D/0/1/0/all/0/1\">Daksh Rawat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_M/0/1/0/all/0/1\">Manish Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varma_V/0/1/0/all/0/1\">Vasudeva Varma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WenetSpeech: A 10000+ Hours Multi-domain Mandarin Corpus for Speech Recognition. (arXiv:2110.03370v5 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2110.03370","description":"<p>In this paper, we present WenetSpeech, a multi-domain Mandarin corpus\nconsisting of 10000+ hours high-quality labeled speech, 2400+ hours weakly\nlabeled speech, and about 10000 hours unlabeled speech, with 22400+ hours in\ntotal. We collect the data from YouTube and Podcast, which covers a variety of\nspeaking styles, scenarios, domains, topics, and noisy conditions. An optical\ncharacter recognition (OCR) based method is introduced to generate the\naudio/text segmentation candidates for the YouTube data on its corresponding\nvideo captions, while a high-quality ASR transcription system is used to\ngenerate audio/text pair candidates for the Podcast data. Then we propose a\nnovel end-to-end label error detection approach to further validate and filter\nthe candidates. We also provide three manually labelled high-quality test sets\nalong with WenetSpeech for evaluation -- Dev for cross-validation purpose in\ntraining, Test_Net, collected from Internet for matched test, and\nTest\\_Meeting, recorded from real meetings for more challenging mismatched\ntest. Baseline systems trained with WenetSpeech are provided for three popular\nspeech recognition toolkits, namely Kaldi, ESPnet, and WeNet, and recognition\nresults on the three test sets are also provided as benchmarks. To the best of\nour knowledge, WenetSpeech is the current largest open-sourced Mandarin speech\ncorpus with transcriptions, which benefits research on production-level speech\nrecognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Binbin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_H/0/1/0/all/0/1\">Hang Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_P/0/1/0/all/0/1\">Pengcheng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Q/0/1/0/all/0/1\">Qijie Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1\">Lei Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bu_H/0/1/0/all/0/1\">Hui Bu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaoyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_C/0/1/0/all/0/1\">Chenchen Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Di Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1\">Zhendong Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Controllable Semantic Parsing via Retrieval Augmentation. (arXiv:2110.08458v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.08458","description":"<p>In practical applications of semantic parsing, we often want to rapidly\nchange the behavior of the parser, such as enabling it to handle queries in a\nnew domain, or changing its predictions on certain targeted queries. While we\ncan introduce new training examples exhibiting the target behavior, a mechanism\nfor enacting such behavior changes without expensive model re-training would be\npreferable. To this end, we propose ControllAble Semantic Parser via Exemplar\nRetrieval (CASPER). Given an input query, the parser retrieves related\nexemplars from a retrieval index, augments them to the query, and then applies\na generative seq2seq model to produce an output parse. The exemplars act as a\ncontrol mechanism over the generic generative model: by manipulating the\nretrieval index or how the augmented query is constructed, we can manipulate\nthe behavior of the parser. On the MTOP dataset, in addition to achieving\nstate-of-the-art on the standard setup, we show that CASPER can parse queries\nin a new domain, adapt the prediction toward the specified patterns, or adapt\nto new semantic schemas without having to further re-train the model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pasupat_P/0/1/0/all/0/1\">Panupong Pasupat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guu_K/0/1/0/all/0/1\">Kelvin Guu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pre-Trained Language Models for Interactive Decision-Making. (arXiv:2202.01771v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2202.01771","description":"<p>Language model (LM) pre-training has proven useful for a wide variety of\nlanguage processing tasks, but can such pre-training be leveraged for more\ngeneral machine learning problems? We investigate the effectiveness of language\nmodeling to scaffold learning and generalization in autonomous decision-making.\nWe describe a framework for imitation learning in which goals and observations\nare represented as a sequence of embeddings, and translated into actions using\na policy network initialized with a pre-trained transformer LM. We demonstrate\nthat this framework enables effective combinatorial generalization across\ndifferent environments, such as VirtualHome and BabyAI. In particular, for test\ntasks involving novel goals or novel scenes, initializing policies with\nlanguage models improves task completion rates by 43.6% in VirtualHome. We\nhypothesize and investigate three possible factors underlying the effectiveness\nof LM-based policy initialization. We find that sequential representations (vs.\nfixed-dimensional feature vectors) and the LM objective (not just the\ntransformer architecture) are both important for generalization. Surprisingly,\nhowever, the format of the policy inputs encoding (e.g. as a natural language\nstring vs. an arbitrary sequential encoding) has little influence. Together,\nthese results suggest that language modeling induces representations that are\nuseful for modeling not just language, but also goals and plans; these\nrepresentations can aid learning and generalization even outside of language\nprocessing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Puig_X/0/1/0/all/0/1\">Xavier Puig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paxton_C/0/1/0/all/0/1\">Chris Paxton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yilun Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Clinton Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1\">Linxi Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">De-An Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akyurek_E/0/1/0/all/0/1\">Ekin Aky&#xfc;rek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1\">Anima Anandkumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andreas_J/0/1/0/all/0/1\">Jacob Andreas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mordatch_I/0/1/0/all/0/1\">Igor Mordatch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1\">Antonio Torralba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yuke Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reward Modeling for Mitigating Toxicity in Transformer-based Language Models. (arXiv:2202.09662v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.09662","description":"<p>Transformer-based language models are able to generate fluent text and be\nefficiently adapted across various natural language generation tasks. However,\nlanguage models that are pretrained on large unlabeled web text corpora have\nbeen shown to suffer from degenerating toxic content and social bias behaviors,\nconsequently hindering their safe deployment. Various detoxification methods\nwere proposed to mitigate the language model's toxicity; however, these methods\nstruggled to detoxify language models when conditioned on prompts that contain\nspecific social identities related to gender, race, or religion. In this study,\nwe propose Reinforce-Detoxify; A reinforcement learning-based method for\nmitigating toxicity in language models. We address the challenge of safety in\nlanguage models and propose a new reward model that is able to detect toxic\ncontent and mitigate unintended bias towards social identities in toxicity\nprediction. The experiments demonstrate that the Reinforce-Detoxify method for\nlanguage model detoxification outperforms existing detoxification approaches in\nautomatic evaluation metrics, indicating the ability of our approach in\nlanguage model detoxification and less prone to unintended bias toward social\nidentities in generated content.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Faal_F/0/1/0/all/0/1\">Farshid Faal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmitt_K/0/1/0/all/0/1\">Ketra Schmitt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StyleBERT: Chinese pretraining by font style information. (arXiv:2202.09955v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.09955","description":"<p>With the success of down streaming task using English pre-trained language\nmodel, the pre-trained Chinese language model is also necessary to get a better\nperformance of Chinese NLP task. Unlike the English language, Chinese has its\nspecial characters such as glyph information. So in this article, we propose\nthe Chinese pre-trained language model StyleBERT which incorporate the\nfollowing embedding information to enhance the savvy of language model, such as\nword, pinyin, five stroke and chaizi. The experiments show that the model\nachieves well performances on a wide range of Chinese NLP tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lv_C/0/1/0/all/0/1\">Chao Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Han Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1\">XinKai Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yunhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Ying Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenhao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jia Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1\">Shanshan Gu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Cluster Patterns for Abstractive Summarization. (arXiv:2202.10967v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.10967","description":"<p>Nowadays, pre-trained sequence-to-sequence models such as BERTSUM and BART\nhave shown state-of-the-art results in abstractive summarization. In these\nmodels, during fine-tuning, the encoder transforms sentences to context vectors\nin the latent space and the decoder learns the summary generation task based on\nthe context vectors. In our approach, we consider two clusters of salient and\nnon-salient context vectors, using which the decoder can attend more to salient\ncontext vectors for summary generation. For this, we propose a novel clustering\ntransformer layer between the encoder and the decoder, which first generates\ntwo clusters of salient and non-salient vectors, and then normalizes and\nshrinks the clusters to make them apart in the latent space. Our experimental\nresult shows that the proposed model outperforms the existing BART model by\nlearning these distinct cluster patterns, improving up to 4% in ROUGE and 0.3%\nin BERTScore on average in CNN/DailyMail and XSUM data sets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jo_S/0/1/0/all/0/1\">Sung-Guk Jo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jeong-Jae Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+On_B/0/1/0/all/0/1\">Byung-Won On</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Sentence Embedding with Generalized Pooling. (arXiv:1806.09828v1 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/1806.09828","description":"<p>Pooling is an essential component of a wide variety of sentence\nrepresentation and embedding models. This paper explores generalized pooling\nmethods to enhance sentence embedding. We propose vector-based multi-head\nattention that includes the widely used max pooling, mean pooling, and scalar\nself-attention as special cases. The model benefits from properly designed\npenalization terms to reduce redundancy in multi-head attention. We evaluate\nthe proposed model on three different tasks: natural language inference (NLI),\nauthor profiling, and sentiment classification. The experiments show that the\nproposed model achieves significant improvement over strong\nsentence-encoding-based methods, resulting in state-of-the-art performances on\nfour datasets. The proposed approach can be easily implemented for more\nproblems than we discuss in this paper.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_Z/0/1/0/all/0/1\">Zhen-Hua Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaodan Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated Attack Synthesis by Extracting Finite State Machines from Protocol Specification Documents. (arXiv:2202.09470v1 [cs.CR] CROSS LISTED)","link":"http://arxiv.org/abs/2202.09470","description":"<p>Automated attack discovery techniques, such as attacker synthesis or\nmodel-based fuzzing, provide powerful ways to ensure network protocols operate\ncorrectly and securely. Such techniques, in general, require a formal\nrepresentation of the protocol, often in the form of a finite state machine\n(FSM). Unfortunately, many protocols are only described in English prose, and\nimplementing even a simple network protocol as an FSM is time-consuming and\nprone to subtle logical errors. Automatically extracting protocol FSMs from\ndocumentation can significantly contribute to increased use of these techniques\nand result in more robust and secure protocol implementations.\n</p>\n<p>In this work we focus on attacker synthesis as a representative technique for\nprotocol security, and on RFCs as a representative format for protocol prose\ndescription. Unlike other works that rely on rule-based approaches or use\noff-the-shelf NLP tools directly, we suggest a data-driven approach for\nextracting FSMs from RFC documents. Specifically, we use a hybrid approach\nconsisting of three key steps: (1) large-scale word-representation learning for\ntechnical language, (2) focused zero-shot learning for mapping protocol text to\na protocol-independent information language, and (3) rule-based mapping from\nprotocol-independent information to a specific protocol FSM. We show the\ngeneralizability of our FSM extraction by using the RFCs for six different\nprotocols: BGPv4, DCCP, LTP, PPTP, SCTP and TCP. We demonstrate how automated\nextraction of an FSM from an RFC can be applied to the synthesis of attacks,\nwith TCP and DCCP as case-studies. Our approach shows that it is possible to\nautomate attacker synthesis against protocols by using textual specifications\nsuch as RFCs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pacheco_M/0/1/0/all/0/1\">Maria Leonor Pacheco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hippel_M/0/1/0/all/0/1\">Max von Hippel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weintraub_B/0/1/0/all/0/1\">Ben Weintraub</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldwasser_D/0/1/0/all/0/1\">Dan Goldwasser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nita_Rotaru_C/0/1/0/all/0/1\">Cristina Nita-Rotaru</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-02-23T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","dc":"http://purl.org/dc/elements/1.1/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Roto-Translation Equivariant Super-Resolution of Two-Dimensional Flows Using Convolutional Neural Networks. (arXiv:2202.11099v1 [physics.flu-dyn])","link":"http://arxiv.org/abs/2202.11099","description":"<p>Convolutional neural networks (CNNs) often process vectors as quantities\nhaving no direction like colors in images. This study investigates the effect\nof treating vectors as geometrical objects in terms of super-resolution of\nvelocity on two-dimensional fluids. Vector is distinguished from scalar by the\ntransformation law associated with a change in basis, which can be incorporated\nas the prior knowledge using the equivariant deep learning. We convert existing\nCNNs into equivariant ones by making each layer equivariant with respect to\nrotation and translation. The training data in the low- and high-resolution are\ngenerated with the downsampling or the spectral nudging. When the data inherit\nthe rotational symmetry, the equivariant CNNs show comparable accuracy with the\nnon-equivariant ones. Since the number of parameters is smaller in the\nequivariant CNNs, these models are trainable with a smaller size of the data.\nIn this case, the transformation law of vector should be incorporated as the\nprior knowledge, where vector is explicitly treated as a quantity having\ndirection. Two examples demonstrate that the symmetry of the data can be\nbroken. In the first case, a downsampling method makes the correspondence\nbetween low- and high-resolution patterns dependent on the orientation. In the\nsecond case, the input data are insufficient to recognize the rotation of\ncoordinates in the experiment with the spectral nudging. In both cases, the\naccuracy of the CNNs deteriorates if the equivariance is forced to be imposed,\nand the usage of conventional CNNs may be justified even though vector is\nprocessed as a quantity having no direction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Yasuda_Y/0/1/0/all/0/1\">Yuki Yasuda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning with Free Object Segments for Long-Tailed Instance Segmentation. (arXiv:2202.11124v1 [cs.CV])","link":"http://arxiv.org/abs/2202.11124","description":"<p>One fundamental challenge in building an instance segmentation model for a\nlarge number of classes in complex scenes is the lack of training examples,\nespecially for rare objects. In this paper, we explore the possibility to\nincrease the training examples without laborious data collection and\nannotation. We find that an abundance of instance segments can potentially be\nobtained freely from object-centric im-ages, according to two insights: (i) an\nobject-centric image usually contains one salient object in a simple\nbackground; (ii) objects from the same class often share similar appearances or\nsimilar contrasts to the background. Motivated by these insights, we propose a\nsimple and scalable framework FreeSeg for extracting and leveraging these\n\"free\" object foreground segments to facilitate model training in long-tailed\ninstance segmentation. Concretely, we employ off-the-shelf object foreground\nextraction techniques (e.g., image co-segmentation) to generate instance mask\ncandidates, followed by segments refinement and ranking. The resulting\nhigh-quality object segments can be used to augment the existing long-tailed\ndataset, e.g., by copying and pasting the segments onto the original training\nimages. On the LVIS benchmark, we show that FreeSeg yields substantial\nimprovements on top of strong baselines and achieves state-of-the-art accuracy\nfor segmenting rare object categories.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Cheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_T/0/1/0/all/0/1\">Tai-Yu Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianle Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_J/0/1/0/all/0/1\">Jike Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_W/0/1/0/all/0/1\">Wenjin Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_W/0/1/0/all/0/1\">Wei-Lun Chao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Indiscriminate Poisoning Attacks on Unsupervised Contrastive Learning. (arXiv:2202.11202v1 [cs.LG])","link":"http://arxiv.org/abs/2202.11202","description":"<p>Indiscriminate data poisoning attacks are quite effective against supervised\nlearning. However, not much is known about their impact on unsupervised\ncontrastive learning (CL). This paper is the first to consider indiscriminate\ndata poisoning attacks on contrastive learning, demonstrating the feasibility\nof such attacks, and their differences from indiscriminate poisoning of\nsupervised learning. We also highlight differences between contrastive learning\nalgorithms, and show that some algorithms (e.g., SimCLR) are more vulnerable\nthan others (e.g., MoCo). We differentiate between two types of data poisoning\nattacks: sample-wise attacks, which add specific noise to each image, cause the\nlargest drop in accuracy, but do not transfer well across SimCLR, MoCo, and\nBYOL. In contrast, attacks that use class-wise noise, though cause a smaller\ndrop in accuracy, transfer well across different CL algorithms. Finally, we\nshow that a new data augmentation based on matrix completion can be highly\neffective in countering data poisoning attacks on unsupervised contrastive\nlearning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Hao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_K/0/1/0/all/0/1\">Kaiwen Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katabi_D/0/1/0/all/0/1\">Dina Katabi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Label-Smoothed Backdoor Attack. (arXiv:2202.11203v1 [cs.CR])","link":"http://arxiv.org/abs/2202.11203","description":"<p>By injecting a small number of poisoned samples into the training set,\nbackdoor attacks aim to make the victim model produce designed outputs on any\ninput injected with pre-designed backdoors. In order to achieve a high attack\nsuccess rate using as few poisoned training samples as possible, most existing\nattack methods change the labels of the poisoned samples to the target class.\nThis practice often results in severe over-fitting of the victim model over the\nbackdoors, making the attack quite effective in output control but easier to be\nidentified by human inspection or automatic defense algorithms.\n</p>\n<p>In this work, we proposed a label-smoothing strategy to overcome the\nover-fitting problem of these attack methods, obtaining a\n\\textit{Label-Smoothed Backdoor Attack} (LSBA). In the LSBA, the label of the\npoisoned sample $\\bm{x}$ will be changed to the target class with a probability\nof $p_n(\\bm{x})$ instead of 100\\%, and the value of $p_n(\\bm{x})$ is\nspecifically designed to make the prediction probability the target class be\nonly slightly greater than those of the other classes. Empirical studies on\nseveral existing backdoor attacks show that our strategy can considerably\nimprove the stealthiness of these attacks and, at the same time, achieve a high\nattack success rate. In addition, our strategy makes it able to manually\ncontrol the prediction probability of the design output through manipulating\nthe applied and activated number of LSBAs\\footnote{Source code will be\npublished at \\url{https://github.com/v-mipeng/LabelSmoothedAttack.git}}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_M/0/1/0/all/0/1\">Minlong Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Z/0/1/0/all/0/1\">Zidi Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Mingming Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Ping Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Arbitrary Shape Text Detection using Transformers. (arXiv:2202.11221v1 [cs.CV])","link":"http://arxiv.org/abs/2202.11221","description":"<p>Recent text detection frameworks require several handcrafted components such\nas anchor generation, non-maximum suppression (NMS), or multiple processing\nstages (e.g. label generation) to detect arbitrarily shaped text images. In\ncontrast, we propose an end-to-end trainable architecture based on Detection\nusing Transformers (DETR), that outperforms previous state-of-the-art methods\nin arbitrary-shaped text detection. At its core, our proposed method leverages\na bounding box loss function that accurately measures the arbitrary detected\ntext regions' changes in scale and aspect ratio. This is possible due to a\nhybrid shape representation made from Bezier curves, that are further split\ninto piece-wise polygons. The proposed loss function is then a combination of a\ngeneralized-split-intersection-over-union loss defined over the piece-wise\npolygons and regularized by a Smooth-$\\ln$ regression over the Bezier curve's\ncontrol points. We evaluate our proposed model using Total-Text and CTW-1500\ndatasets for curved text, and MSRA-TD500 and ICDAR15 datasets for\nmulti-oriented text, and show that the proposed method outperforms the previous\nstate-of-the-art methods in arbitrary-shape text detection tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Raisi_Z/0/1/0/all/0/1\">Zobeir Raisi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Younes_G/0/1/0/all/0/1\">Georges Younes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zelek_J/0/1/0/all/0/1\">John Zelek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enabling Efficient Deep Convolutional Neural Network-based Sensor Fusion for Autonomous Driving. (arXiv:2202.11231v1 [cs.CV])","link":"http://arxiv.org/abs/2202.11231","description":"<p>Autonomous driving demands accurate perception and safe decision-making. To\nachieve this, automated vehicles are now equipped with multiple sensors (e.g.,\ncamera, Lidar, etc.), enabling them to exploit complementary environmental\ncontext by fusing data from different sensing modalities. With the success of\nDeep Convolutional Neural Network(DCNN), the fusion between DCNNs has been\nproved as a promising strategy to achieve satisfactory perception accuracy.\nHowever, mainstream existing DCNN fusion schemes conduct fusion by directly\nelement-wisely adding feature maps extracted from different modalities together\nat various stages, failing to consider whether the features being fused are\nmatched or not. Therefore, we first propose a feature disparity metric to\nquantitatively measure the degree of feature disparity between the feature maps\nbeing fused. We then propose Fusion-filter as a feature-matching techniques to\ntackle the feature-mismatching issue. We also propose a Layer-sharing technique\nin the deep layer that can achieve better accuracy with less computational\noverhead. Together with the help of the feature disparity to be an additional\nloss, our proposed technologies enable DCNN to learn corresponding feature maps\nwith similar characteristics and complementary visual context from different\nmodalities to achieve better accuracy. Experimental results demonstrate that\nour proposed fusion technique can achieve better accuracy on KITTI dataset with\nless computational resources demand.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1\">Xiaoming Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhendong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yang Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Retrieval Augmented Classification for Long-Tail Visual Recognition. (arXiv:2202.11233v1 [cs.CV])","link":"http://arxiv.org/abs/2202.11233","description":"<p>We introduce Retrieval Augmented Classification (RAC), a generic approach to\naugmenting standard image classification pipelines with an explicit retrieval\nmodule. RAC consists of a standard base image encoder fused with a parallel\nretrieval branch that queries a non-parametric external memory of pre-encoded\nimages and associated text snippets. We apply RAC to the problem of long-tail\nclassification and demonstrate a significant improvement over previous\nstate-of-the-art on Places365-LT and iNaturalist-2018 (14.5% and 6.7%\nrespectively), despite using only the training datasets themselves as the\nexternal information source. We demonstrate that RAC's retrieval module,\nwithout prompting, learns a high level of accuracy on tail classes. This, in\nturn, frees the base encoder to focus on common classes, and improve its\nperformance thereon. RAC represents an alternative approach to utilizing large,\npretrained models without requiring fine-tuning, as well as a first step\ntowards more effectively making use of external memory within common computer\nvision architectures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Long_A/0/1/0/all/0/1\">Alexander Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_W/0/1/0/all/0/1\">Wei Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ajanthan_T/0/1/0/all/0/1\">Thalaiyasingam Ajanthan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_V/0/1/0/all/0/1\">Vu Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Purkait_P/0/1/0/all/0/1\">Pulak Purkait</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_R/0/1/0/all/0/1\">Ravi Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blair_A/0/1/0/all/0/1\">Alan Blair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hengel_A/0/1/0/all/0/1\">Anton van den Hengel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FUNQUE: Fusion of Unified Quality Evaluators. (arXiv:2202.11241v1 [cs.CV])","link":"http://arxiv.org/abs/2202.11241","description":"<p>Fusion-based quality assessment has emerged as a powerful method for\ndeveloping high-performance quality models from quality models that\nindividually achieve lower performances. A prominent example of such an\nalgorithm is VMAF, which has been widely adopted as an industry standard for\nvideo quality prediction along with SSIM. In addition to advancing the\nstate-of-the-art, it is imperative to alleviate the computational burden\npresented by the use of a heterogeneous set of quality models. In this paper,\nwe unify \"atom\" quality models by computing them on a common transform domain\nthat accounts for the Human Visual System, and we propose FUNQUE, a quality\nmodel that fuses unified quality evaluators. We demonstrate that in comparison\nto the state-of-the-art, FUNQUE offers significant improvements in both\ncorrelation against subjective scores and efficiency, due to computation\nsharing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Venkataramanan_A/0/1/0/all/0/1\">Abhinau K. Venkataramanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stejerean_C/0/1/0/all/0/1\">Cosmin Stejerean</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bovik_A/0/1/0/all/0/1\">Alan C. Bovik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An End-to-End Cascaded Image Deraining and Object Detection Neural Network. (arXiv:2202.11279v1 [cs.CV])","link":"http://arxiv.org/abs/2202.11279","description":"<p>While the deep learning-based image deraining methods have made great\nprogress in recent years, there are two major shortcomings in their application\nin real-world situations. Firstly, the gap between the low-level vision task\nrepresented by rain removal and the high-level vision task represented by\nobject detection is significant, and the low-level vision task can hardly\ncontribute to the high-level vision task. Secondly, the quality of the\nderaining dataset needs to be improved. In fact, the rain lines in many\nbaselines have a large gap with the real rain lines, and the resolution of the\nderaining dataset images is generally not ideally. Meanwhile, there are few\ncommon datasets for both the low-level vision task and the high-level vision\ntask. In this paper, we explore the combination of the low-level vision task\nwith the high-level vision task. Specifically, we propose an end-to-end object\ndetection network for reducing the impact of rainfall, which consists of two\ncascaded networks, an improved image deraining network and an object detection\nnetwork, respectively. We also design the components of the loss function to\naccommodate the characteristics of the different sub-networks. We then propose\na dataset based on the KITTI dataset for rainfall removal and object detection,\non which our network surpasses the state-of-the-art with a significant\nimprovement in metrics. Besides, our proposed network is measured on driving\nvideos collected by self-driving vehicles and shows positive results for rain\nremoval and object detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kaige Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianming Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_J/0/1/0/all/0/1\">Jianchuang Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Huatao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_L/0/1/0/all/0/1\">Lin Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LPF-Defense: 3D Adversarial Defense based on Frequency Analysis. (arXiv:2202.11287v1 [cs.CV])","link":"http://arxiv.org/abs/2202.11287","description":"<p>Although 3D point cloud classification has recently been widely deployed in\ndifferent application scenarios, it is still very vulnerable to adversarial\nattacks. This increases the importance of robust training of 3D models in the\nface of adversarial attacks. Based on our analysis on the performance of\nexisting adversarial attacks, more adversarial perturbations are found in the\nmid and high-frequency components of input data. Therefore, by suppressing the\nhigh-frequency content in the training phase, the models robustness against\nadversarial examples is improved. Experiments showed that the proposed defense\nmethod decreases the success rate of six attacks on PointNet, PointNet++ ,, and\nDGCNN models. In particular, improvements are achieved with an average increase\nof classification accuracy by 3.8 % on drop100 attack and 4.26 % on drop200\nattack compared to the state-of-the-art methods. The method also improves\nmodels accuracy on the original dataset compared to other available methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Naderi_H/0/1/0/all/0/1\">Hanieh Naderi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Etemadi_A/0/1/0/all/0/1\">Arian Etemadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noorbakhsh_K/0/1/0/all/0/1\">Kimia Noorbakhsh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasaei_S/0/1/0/all/0/1\">Shohreh Kasaei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reliable Inlier Evaluation for Unsupervised Point Cloud Registration. (arXiv:2202.11292v1 [cs.CV])","link":"http://arxiv.org/abs/2202.11292","description":"<p>Unsupervised point cloud registration algorithm usually suffers from the\nunsatisfied registration precision in the partially overlapping problem due to\nthe lack of effective inlier evaluation. In this paper, we propose a\nneighborhood consensus based reliable inlier evaluation method for robust\nunsupervised point cloud registration. It is expected to capture the\ndiscriminative geometric difference between the source neighborhood and the\ncorresponding pseudo target neighborhood for effective inlier distinction.\nSpecifically, our model consists of a matching map refinement module and an\ninlier evaluation module. In our matching map refinement module, we improve the\npoint-wise matching map estimation by integrating the matching scores of\nneighbors into it. The aggregated neighborhood information potentially\nfacilitates the discriminative map construction so that high-quality\ncorrespondences can be provided for generating the pseudo target point cloud.\nBased on the observation that the outlier has the significant structure-wise\ndifference between its source neighborhood and corresponding pseudo target\nneighborhood while this difference for inlier is small, the inlier evaluation\nmodule exploits this difference to score the inlier confidence for each\nestimated correspondence. In particular, we construct an effective graph\nrepresentation for capturing this geometric difference between the\nneighborhoods. Finally, with the learned correspondences and the corresponding\ninlier confidence, we use the weighted SVD algorithm for transformation\nestimation. Under the unsupervised setting, we exploit the Huber function based\nglobal alignment loss, the local neighborhood consensus loss, and spatial\nconsistency loss for model optimization. The experimental results on extensive\ndatasets demonstrate that our unsupervised point cloud registration method can\nyield comparable performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yaqi Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hui_L/0/1/0/all/0/1\">Le Hui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Haobo Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jian Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are We Ready for Robust and Resilient SLAM? A Framework For Quantitative Characterization of SLAM Datasets. (arXiv:2202.11312v1 [cs.RO])","link":"http://arxiv.org/abs/2202.11312","description":"<p>Reliability of SLAM systems is considered one of the critical requirements in\nmany modern autonomous systems. This directed the efforts to developing many\nstate-of-the-art systems, creating challenging datasets, and introducing\nrigorous metrics to measure SLAM system performance. However, the link between\ndatasets and performance in the robustness/resilience context has rarely been\nexplored. In order to fill this void, characterization the operating conditions\nof SLAM systems is essential in order to provide an environment for\nquantitative measurement of robustness and resilience. In this paper, we argue\nthat for proper evaluation of SLAM performance, the characterization of SLAM\ndatasets serves as a critical first step. The study starts by reviewing\nprevious efforts for quantitative characterization of SLAM datasets. Then, the\nproblem of perturbations characterization is discussed and the linkage to SLAM\nrobustness/resilience is established. After that, we propose a novel, generic\nand extendable framework for quantitative analysis and comparison of SLAM\ndatasets. Additionally, a description of different characterization parameters\nis provided. Finally, we demonstrate the application of our framework by\npresenting the characterization results of three SLAM datasets: KITTI,\nEuroC-MAV, and TUM-VI highlighting the level of insights achieved by the\nproposed framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ali_I/0/1/0/all/0/1\">Islam Ali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hong Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Absolute Zero-Shot Learning. (arXiv:2202.11319v1 [cs.CV])","link":"http://arxiv.org/abs/2202.11319","description":"<p>Considering the increasing concerns about data copyright and privacy issues,\nwe present a novel Absolute Zero-Shot Learning (AZSL) paradigm, i.e., training\na classifier with zero real data. The key innovation is to involve a teacher\nmodel as the data safeguard to guide the AZSL model training without data\nleaking. The AZSL model consists of a generator and student network, which can\nachieve date-free knowledge transfer while maintaining the performance of the\nteacher network. We investigate `black-box' and `white-box' scenarios in AZSL\ntask as different levels of model security. Besides, we also provide discussion\nof teacher model in both inductive and transductive settings. Despite\nembarrassingly simple implementations and data-missing disadvantages, our AZSL\nframework can retain state-of-the-art ZSL and GZSL performance under the\n`white-box' scenario. Extensive qualitative and quantitative analysis also\ndemonstrates promising results when deploying the model under `black-box'\nscenario.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1\">Rui Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_F/0/1/0/all/0/1\">Fan Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Organisciak_D/0/1/0/all/0/1\">Daniel Organisciak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_J/0/1/0/all/0/1\">Jiyao Pu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Junyan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_H/0/1/0/all/0/1\">Haoran Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Peng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_X/0/1/0/all/0/1\">Xingsong Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_Y/0/1/0/all/0/1\">Yang Long</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EcoFusion: Energy-Aware Adaptive Sensor Fusion for Efficient Autonomous Vehicle Perception. (arXiv:2202.11330v1 [cs.CV])","link":"http://arxiv.org/abs/2202.11330","description":"<p>Autonomous vehicles use multiple sensors, large deep-learning models, and\npowerful hardware platforms to perceive the environment and navigate safely. In\nmany contexts, some sensing modalities negatively impact perception while\nincreasing energy consumption. We propose EcoFusion: an energy-aware sensor\nfusion approach that uses context to adapt the fusion method and reduce energy\nconsumption without affecting perception performance. EcoFusion performs up to\n9.5% better at object detection than existing fusion methods with approximately\n60% less energy and 58% lower latency on the industry-standard Nvidia Drive PX2\nhardware platform. We also propose several context-identification strategies,\nimplement a joint optimization between energy and performance, and present\nscenario-specific results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Malawade_A/0/1/0/all/0/1\">Arnav Vaibhav Malawade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mortlock_T/0/1/0/all/0/1\">Trier Mortlock</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faruque_M/0/1/0/all/0/1\">Mohammad Abdullah Al Faruque</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Commonsense Reasoning for Identifying and Understanding the Implicit Need of Help and Synthesizing Assistive Actions. (arXiv:2202.11337v1 [cs.AI])","link":"http://arxiv.org/abs/2202.11337","description":"<p>Human-Robot Interaction (HRI) is an emerging subfield of service robotics.\nWhile most existing approaches rely on explicit signals (i.e. voice, gesture)\nto engage, current literature is lacking solutions to address implicit user\nneeds. In this paper, we present an architecture to (a) detect user implicit\nneed of help and (b) generate a set of assistive actions without prior\nlearning. Task (a) will be performed using state-of-the-art solutions for Scene\nGraph Generation coupled to the use of commonsense knowledge; whereas, task (b)\nwill be performed using additional commonsense knowledge as well as a sentiment\nanalysis on graph structure. Finally, we propose an evaluation of our solution\nusing established benchmarks (e.g. ActionGenome dataset) along with human\nexperiments. The main motivation of our approach is the embedding of the\nperception-decision-action loop in a single architecture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Neau_M/0/1/0/all/0/1\">Ma&#xeb;lic Neau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santos_P/0/1/0/all/0/1\">Paulo Santos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bosser_A/0/1/0/all/0/1\">Anne-Gwenn Bosser</a> (ENIB), <a href=\"http://arxiv.org/find/cs/1/au:+Beu_N/0/1/0/all/0/1\">Nathan Beu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buche_C/0/1/0/all/0/1\">C&#xe9;dric Buche</a> (Lab-STICC\\_RAMBO)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deepfake Detection for Facial Images with Facemasks. (arXiv:2202.11359v1 [cs.CV])","link":"http://arxiv.org/abs/2202.11359","description":"<p>Hyper-realistic face image generation and manipulation have givenrise to\nnumerous unethical social issues, e.g., invasion of privacy,threat of security,\nand malicious political maneuvering, which re-sulted in the development of\nrecent deepfake detection methodswith the rising demands of deepfake forensics.\nProposed deepfakedetection methods to date have shown remarkable detection\nperfor-mance and robustness. However, none of the suggested deepfakedetection\nmethods assessed the performance of deepfakes withthe facemask during the\npandemic crisis after the outbreak of theCovid-19. In this paper, we thoroughly\nevaluate the performance ofstate-of-the-art deepfake detection models on the\ndeepfakes withthe facemask. Also, we propose two approaches to enhance\nthemasked deepfakes detection:face-patchandface-crop. The experi-mental\nevaluations on both methods are assessed through the base-line deepfake\ndetection models on the various deepfake datasets.Our extensive experiments\nshow that, among the two methods,face-cropperforms better than theface-patch,\nand could be a trainmethod for deepfake detection models to detect fake faces\nwithfacemask in real world.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ko_D/0/1/0/all/0/1\">Donggeun Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sangjun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jinyong Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_S/0/1/0/all/0/1\">Saebyeol Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_D/0/1/0/all/0/1\">Donghee Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woo_S/0/1/0/all/0/1\">Simon S. Woo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Localizing Small Apples in Complex Apple Orchard Environments. (arXiv:2202.11372v1 [cs.CV])","link":"http://arxiv.org/abs/2202.11372","description":"<p>The localization of fruits is an essential first step in automated\nagricultural pipelines for yield estimation or fruit picking. One example of\nthis is the localization of apples in images of entire apple trees. Since the\napples are very small objects in such scenarios, we tackle this problem by\nadapting the object proposal generation system AttentionMask that focuses on\nsmall objects. We adapt AttentionMask by either adding a new module for very\nsmall apples or integrating it into a tiling framework. Both approaches clearly\noutperform standard object proposal generation systems on the MinneApple\ndataset covering complex apple orchard environments. Our evaluation further\nanalyses the improvement w.r.t. the apple sizes and shows the different\ncharacteristics of our two approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wilms_C/0/1/0/all/0/1\">Christian Wilms</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johanson_R/0/1/0/all/0/1\">Robert Johanson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frintrop_S/0/1/0/all/0/1\">Simone Frintrop</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Skeleton Sequence and RGB Frame Based Multi-Modality Feature Fusion Network for Action Recognition. (arXiv:2202.11374v1 [cs.CV])","link":"http://arxiv.org/abs/2202.11374","description":"<p>Action recognition has been a heated topic in computer vision for its wide\napplication in vision systems. Previous approaches achieve improvement by\nfusing the modalities of the skeleton sequence and RGB video. However, such\nmethods have a dilemma between the accuracy and efficiency for the high\ncomplexity of the RGB video network. To solve the problem, we propose a\nmulti-modality feature fusion network to combine the modalities of the skeleton\nsequence and RGB frame instead of the RGB video, as the key information\ncontained by the combination of skeleton sequence and RGB frame is close to\nthat of the skeleton sequence and RGB video. In this way, the complementary\ninformation is retained while the complexity is reduced by a large margin. To\nbetter explore the correspondence of the two modalities, a two-stage fusion\nframework is introduced in the network. In the early fusion stage, we introduce\na skeleton attention module that projects the skeleton sequence on the single\nRGB frame to help the RGB frame focus on the limb movement regions. In the late\nfusion stage, we propose a cross-attention module to fuse the skeleton feature\nand the RGB feature by exploiting the correlation. Experiments on two\nbenchmarks NTU RGB+D and SYSU show that the proposed model achieves competitive\nperformance compared with the state-of-the-art methods while reduces the\ncomplexity of the network.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaoguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Ye Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haoyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_H/0/1/0/all/0/1\">Honglin Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yan Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Peilin Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-scale Sparse Representation-Based Shadow Inpainting for Retinal OCT Images. (arXiv:2202.11377v1 [cs.CV])","link":"http://arxiv.org/abs/2202.11377","description":"<p>Inpainting shadowed regions cast by superficial blood vessels in retinal\noptical coherence tomography (OCT) images is critical for accurate and robust\nmachine analysis and clinical diagnosis. Traditional sequence-based approaches\nsuch as propagating neighboring information to gradually fill in the missing\nregions are cost-effective. But they generate less satisfactory outcomes when\ndealing with larger missing regions and texture-rich structures. Emerging deep\nlearning-based methods such as encoder-decoder networks have shown promising\nresults in natural image inpainting tasks. However, they typically need a long\ncomputational time for network training in addition to the high demand on the\nsize of datasets, which makes it difficult to be applied on often small medical\ndatasets. To address these challenges, we propose a novel multi-scale shadow\ninpainting framework for OCT images by synergically applying sparse\nrepresentation and deep learning: sparse representation is used to extract\nfeatures from a small amount of training images for further inpainting and to\nregularize the image after the multi-scale image fusion, while convolutional\nneural network (CNN) is employed to enhance the image quality. During the image\ninpainting, we divide preprocessed input images into different branches based\non the shadow width to harvest complementary information from different scales.\nFinally, a sparse representation-based regularizing module is designed to\nrefine the generated contents after multi-scale feature aggregation.\nExperiments are conducted to compare our proposal versus both traditional and\ndeep learning-based techniques on synthetic and real-world shadows. Results\ndemonstrate that our proposed method achieves favorable image inpainting in\nterms of visual quality and quantitative metrics, especially when wide shadows\nare presented.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yaoqi Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yufan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hongshan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiaxuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_P/0/1/0/all/0/1\">Peiyao Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_Y/0/1/0/all/0/1\">Yu Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_Y/0/1/0/all/0/1\">Yuye Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yikai Su</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Teacher Knowledge Distillation for Incremental Implicitly-Refined Classification. (arXiv:2202.11384v1 [cs.CV])","link":"http://arxiv.org/abs/2202.11384","description":"<p>Incremental learning methods can learn new classes continually by distilling\nknowledge from the last model (as a teacher model) to the current model (as a\nstudent model) in the sequentially learning process. However, these methods\ncannot work for Incremental Implicitly-Refined Classification (IIRC), an\nincremental learning extension where the incoming classes could have two\ngranularity levels, a superclass label and a subclass label. This is because\nthe previously learned superclass knowledge may be occupied by the subclass\nknowledge learned sequentially. To solve this problem, we propose a novel\nMulti-Teacher Knowledge Distillation (MTKD) strategy. To preserve the subclass\nknowledge, we use the last model as a general teacher to distill the previous\nknowledge for the student model. To preserve the superclass knowledge, we use\nthe initial model as a superclass teacher to distill the superclass knowledge\nas the initial model contains abundant superclass knowledge. However,\ndistilling knowledge from two teacher models could result in the student model\nmaking some redundant predictions. We further propose a post-processing\nmechanism, called as Top-k prediction restriction to reduce the redundant\npredictions. Our experimental results on IIRC-ImageNet120 and IIRC-CIFAR100\nshow that the proposed method can achieve better classification accuracy\ncompared with existing state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Longhui Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weng_Z/0/1/0/all/0/1\">Zhenyu Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuqing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yuesheng Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Metric Learning-Based Semi-Supervised Regression With Alternate Learning. (arXiv:2202.11388v1 [cs.CV])","link":"http://arxiv.org/abs/2202.11388","description":"<p>This paper introduces a novel deep metric learning-based semi-supervised\nregression (DML-S2R) method for parameter estimation problems. The proposed\nDML-S2R method aims to mitigate the problems of insufficient amount of labeled\nsamples without collecting any additional samples with target values. To this\nend, the proposed DML-S2R method is made up of two main steps: i) pairwise\nsimilarity modeling with scarce labeled data; and ii) triplet-based metric\nlearning with abundant unlabeled data. The first step aims to model pairwise\nsample similarities by using a small number of labeled samples. This is\nachieved by estimating the target value differences of labeled samples with a\nSiamese neural network (SNN). The second step aims to learn a triplet-based\nmetric space (in which similar samples are close to each other and dissimilar\nsamples are far apart from each other) when the number of labeled samples is\ninsufficient. This is achieved by employing the SNN of the first step for\ntriplet-based deep metric learning that exploits not only labeled samples but\nalso unlabeled samples. For the end-to-end training of DML-S2R, we investigate\nan alternate learning strategy for the two steps. Due to this strategy, the\nencoded information in each step becomes a guidance for learning the other\nstep. The experimental results confirm the success of DML-S2R compared to the\nstate-of-the-art semi-supervised regression methods. The code of the proposed\nmethod is publicly available at https://git.tu-berlin.de/rsim/DML-S2R.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zell_A/0/1/0/all/0/1\">Adina Zell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sumbul_G/0/1/0/all/0/1\">Gencer Sumbul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demir_B/0/1/0/all/0/1\">Beg&#xfc;m Demir</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mixed-Block Neural Architecture Search for Medical Image Segmentation. (arXiv:2202.11401v1 [eess.IV])","link":"http://arxiv.org/abs/2202.11401","description":"<p>Deep Neural Networks (DNNs) have the potential for making various clinical\nprocedures more time-efficient by automating medical image segmentation. Due to\ntheir strong, in some cases human-level, performance, they have become the\nstandard approach in this field. The design of the best possible medical image\nsegmentation DNNs, however, is task-specific. Neural Architecture Search (NAS),\ni.e., the automation of neural network design, has been shown to have the\ncapability to outperform manually designed networks for various tasks. However,\nthe existing NAS methods for medical image segmentation have explored a quite\nlimited range of types of DNN architectures that can be discovered. In this\nwork, we propose a novel NAS search space for medical image segmentation\nnetworks. This search space combines the strength of a generalised\nencoder-decoder structure, well known from U-Net, with network blocks that have\nproven to have a strong performance in image classification tasks. The search\nis performed by looking for the best topology of multiple cells simultaneously\nwith the configuration of each cell within, allowing for interactions between\ntopology and cell-level attributes. From experiments on two publicly available\ndatasets, we find that the networks discovered by our proposed NAS method have\nbetter performance than well-known handcrafted segmentation networks, and\noutperform networks found with other NAS approaches that perform only topology\nsearch, and topology-level search followed by cell-level search.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bosma_M/0/1/0/all/0/1\">Martijn M.A. Bosma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dushatskiy_A/0/1/0/all/0/1\">Arkadiy Dushatskiy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Grewal_M/0/1/0/all/0/1\">Monika Grewal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Alderliesten_T/0/1/0/all/0/1\">Tanja Alderliesten</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bosman_P/0/1/0/all/0/1\">Peter A. N. Bosman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ProFormer: Learning Data-efficient Representations of Body Movement with Prototype-based Feature Augmentation and Visual Transformers. (arXiv:2202.11423v1 [cs.CV])","link":"http://arxiv.org/abs/2202.11423","description":"<p>Automatically understanding human behaviour allows household robots to\nidentify the most critical needs and plan how to assist the human according to\nthe current situation. However, the majority of such methods are developed\nunder the assumption that a large amount of labelled training examples is\navailable for all concepts-of-interest. Robots, on the other hand, operate in\nconstantly changing unstructured environments, and need to adapt to novel\naction categories from very few samples. Methods for data-efficient recognition\nfrom body poses increasingly leverage skeleton sequences structured as\nimage-like arrays and then used as input to convolutional neural networks. We\nlook at this paradigm from the perspective of transformer networks, for the\nfirst time exploring visual transformers as data-efficient encoders of skeleton\nmovement. In our pipeline, body pose sequences cast as image-like\nrepresentations are converted into patch embeddings and then passed to a visual\ntransformer backbone optimized with deep metric learning. Inspired by recent\nsuccess of feature enhancement methods in semi-supervised learning, we further\nintroduce ProFormer -- an improved training strategy which uses soft-attention\napplied on iteratively estimated action category prototypes used to augment the\nembeddings and compute an auxiliary consistency loss. Extensive experiments\nconsistently demonstrate the effectiveness of our approach for one-shot\nrecognition from body poses, achieving state-of-the-art results on multiple\ndatasets and surpassing the best published approach on the challenging NTU-120\none-shot benchmark by 1.84%. Our code will be made publicly available at\nhttps://github.com/KPeng9510/ProFormer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_K/0/1/0/all/0/1\">Kunyu Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roitberg_A/0/1/0/all/0/1\">Alina Roitberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kailun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiaming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stiefelhagen_R/0/1/0/all/0/1\">Rainer Stiefelhagen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Novel Self-Supervised Cross-Modal Image Retrieval Method In Remote Sensing. (arXiv:2202.11429v1 [cs.CV])","link":"http://arxiv.org/abs/2202.11429","description":"<p>Due to the availability of multi-modal remote sensing (RS) image archives,\none of the most important research topics is the development of cross-modal RS\nimage retrieval (CM-RSIR) methods that search semantically similar images\nacross different modalities. Existing CM-RSIR methods require annotated\ntraining images (which is time-consuming, costly and not feasible to gather in\nlarge-scale applications) and do not concurrently address intra- and\ninter-modal similarity preservation and inter-modal discrepancy elimination. In\nthis paper, we introduce a novel self-supervised cross-modal image retrieval\nmethod that aims to: i) model mutual-information between different modalities\nin a self-supervised manner; ii) retain the distributions of modal-specific\nfeature spaces similar; and iii) define most similar images within each\nmodality without requiring any annotated training images. To this end, we\npropose a novel objective including three loss functions that simultaneously:\ni) maximize mutual information of different modalities for inter-modal\nsimilarity preservation; ii) minimize the angular distance of multi-modal image\ntuples for the elimination of inter-modal discrepancies; and iii) increase\ncosine similarity of most similar images within each modality for the\ncharacterization of intra-modal similarities. Experimental results show the\neffectiveness of the proposed method compared to state-of-the-art methods. The\ncode of the proposed method is publicly available at\nhttps://git.tu-berlin.de/rsim/SS-CM-RSIR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sumbul_G/0/1/0/all/0/1\">Gencer Sumbul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_M/0/1/0/all/0/1\">Markus M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demir_B/0/1/0/all/0/1\">Beg&#xfc;m Demir</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On PAC-Bayesian reconstruction guarantees for VAEs. (arXiv:2202.11455v1 [cs.LG])","link":"http://arxiv.org/abs/2202.11455","description":"<p>Despite its wide use and empirical successes, the theoretical understanding\nand study of the behaviour and performance of the variational autoencoder (VAE)\nhave only emerged in the past few years. We contribute to this recent line of\nwork by analysing the VAE's reconstruction ability for unseen test data,\nleveraging arguments from the PAC-Bayes theory. We provide generalisation\nbounds on the theoretical reconstruction error, and provide insights on the\nregularisation effect of VAE objectives. We illustrate our theoretical results\nwith supporting experiments on classical benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cherief_Abdellatif_B/0/1/0/all/0/1\">Badr-Eddine Ch&#xe9;rief-Abdellatif</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yuyang Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doucet_A/0/1/0/all/0/1\">Arnaud Doucet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guedj_B/0/1/0/all/0/1\">Benjamin Guedj</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SLOGAN: Handwriting Style Synthesis for Arbitrary-Length and Out-of-Vocabulary Text. (arXiv:2202.11456v1 [cs.CV])","link":"http://arxiv.org/abs/2202.11456","description":"<p>Large amounts of labeled data are urgently required for the training of\nrobust text recognizers. However, collecting handwriting data of diverse\nstyles, along with an immense lexicon, is considerably expensive. Although data\nsynthesis is a promising way to relieve data hunger, two key issues of\nhandwriting synthesis, namely, style representation and content embedding,\nremain unsolved. To this end, we propose a novel method that can synthesize\nparameterized and controllable handwriting Styles for arbitrary-Length and\nOut-of-vocabulary text based on a Generative Adversarial Network (GAN), termed\nSLOGAN. Specifically, we propose a style bank to parameterize the specific\nhandwriting styles as latent vectors, which are input to a generator as style\npriors to achieve the corresponding handwritten styles. The training of the\nstyle bank requires only the writer identification of the source images, rather\nthan attribute annotations. Moreover, we embed the text content by providing an\neasily obtainable printed style image, so that the diversity of the content can\nbe flexibly achieved by changing the input printed image. Finally, the\ngenerator is guided by dual discriminators to handle both the handwriting\ncharacteristics that appear as separated characters and in a series of cursive\njoins. Our method can synthesize words that are not included in the training\nvocabulary and with various new styles. Extensive experiments have shown that\nhigh-quality text images with great style diversity and rich vocabulary can be\nsynthesized using our method, thereby enhancing the robustness of the\nrecognizer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1\">Canjie Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yuanzhi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1\">Lianwen Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhe Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_D/0/1/0/all/0/1\">Dezhi Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Thermal hand image segmentation for biometric recognition. (arXiv:2202.11462v1 [cs.CV])","link":"http://arxiv.org/abs/2202.11462","description":"<p>In this paper we present a method to identify people by means of thermal (TH)\nand visible (VIS) hand images acquired simultaneously with a TESTO 882-3\ncamera. In addition, we also present a new database specially acquired for this\nwork. The real challenge when dealing with TH images is the cold finger areas,\nwhich can be confused with the acquisition surface. This problem is solved by\ntaking advantage of the VIS information. We have performed different tests to\nshow how TH and VIS images work in identification problems. Experimental\nresults reveal that TH hand image is as suitable for biometric recognition\nsystems as VIS hand images, and better results are obtained when combining this\ninformation. A Biometric Dispersion Matcher has been used as a feature vector\ndimensionality reduction technique as well as a classification task. Its\nselection criteria helps to reduce the length of the vectors used to perform\nidentification up to a hundred measurements. Identification rates reach a\nmaximum value of 98.3% under these conditions, when using a database of 104\npeople.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Font_Aragones_X/0/1/0/all/0/1\">Xavier Font-Aragones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faundez_Zanuy_M/0/1/0/all/0/1\">Marcos Faundez-Zanuy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mekyska_J/0/1/0/all/0/1\">Jiri Mekyska</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reconstruction Task Finds Universal Winning Tickets. (arXiv:2202.11484v1 [cs.CV])","link":"http://arxiv.org/abs/2202.11484","description":"<p>Pruning well-trained neural networks is effective to achieve a promising\naccuracy-efficiency trade-off in computer vision regimes. However, most of\nexisting pruning algorithms only focus on the classification task defined on\nthe source domain. Different from the strong transferability of the original\nmodel, a pruned network is hard to transfer to complicated downstream tasks\nsuch as object detection arXiv:arch-ive/2012.04643. In this paper, we show that\nthe image-level pretrain task is not capable of pruning models for diverse\ndownstream tasks. To mitigate this problem, we introduce image reconstruction,\na pixel-level task, into the traditional pruning framework. Concretely, an\nautoencoder is trained based on the original model, and then the pruning\nprocess is optimized with both autoencoder and classification losses. The\nempirical study on benchmark downstream tasks shows that the proposed method\ncan outperform state-of-the-art results explicitly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruichen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Binghui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Q/0/1/0/all/0/1\">Qi Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liwei Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Augmentation based unsupervised domain adaptation. (arXiv:2202.11486v1 [eess.IV])","link":"http://arxiv.org/abs/2202.11486","description":"<p>The insertion of deep learning in medical image analysis had lead to the\ndevelopment of state-of-the art strategies in several applications such a\ndisease classification, as well as abnormality detection and segmentation.\nHowever, even the most advanced methods require a huge and diverse amount of\ndata to generalize. Because in realistic clinical scenarios, data acquisition\nand annotation is expensive, deep learning models trained on small and\nunrepresentative data tend to outperform when deployed in data that differs\nfrom the one used for training (e.g data from different scanners). In this\nwork, we proposed a domain adaptation methodology to alleviate this problem in\nsegmentation models. Our approach takes advantage of the properties of\nadversarial domain adaptation and consistency training to achieve more robust\nadaptation. Using two datasets with white matter hyperintensities (WMH)\nannotations, we demonstrated that the proposed method improves model\ngeneralization even in corner cases where individual strategies tend to fail.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Orbes_Arteaga_M/0/1/0/all/0/1\">Mauricio Orbes-Arteaga</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Varsavsky_T/0/1/0/all/0/1\">Thomas Varsavsky</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sorensen_L/0/1/0/all/0/1\">Lauge Sorensen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nielsen_M/0/1/0/all/0/1\">Mads Nielsen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pai_A/0/1/0/all/0/1\">Akshay Pai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ourselin_S/0/1/0/all/0/1\">Sebastien Ourselin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Modat_M/0/1/0/all/0/1\">Marc Modat</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cardoso_M/0/1/0/all/0/1\">M Jorge Cardoso</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MITI: SLAM Benchmark for Laparoscopic Surgery. (arXiv:2202.11496v1 [eess.IV])","link":"http://arxiv.org/abs/2202.11496","description":"<p>We propose a new benchmark for evaluating stereoscopic visual-inertial\ncomputer vision algorithms (SLAM/ SfM/ 3D Reconstruction/ Visual-Inertial\nOdometry) for minimally invasive surgical (MIS) interventions in the abdomen.\nOur MITI Dataset available at [https://mediatum.ub.tum.<a href=\"/abs/de/1621941\">de/1621941</a>] provides all\nthe necessary data by a complete recording of a handheld surgical intervention\nat Research Hospital Rechts der Isar of TUM. It contains multimodal sensor\ninformation from IMU, stereoscopic video, and infrared (IR) tracking as ground\ntruth for evaluation. Furthermore, calibration for the stereoscope,\naccelerometer, magnetometer, the rigid transformations in the sensor setup, and\ntime-offsets are available. We wisely chose a suitable intervention that\ncontains very few cutting and tissue deformation and shows a full scan of the\nabdomen with a handheld camera such that it is ideal for testing SLAM\nalgorithms. Intending to promote the progress of visual-inertial algorithms\ndesigned for MIS application, we hope that our clinical training dataset helps\nand enables researchers to enhance algorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hartwig_R/0/1/0/all/0/1\">Regine Hartwig</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ostler_D/0/1/0/all/0/1\">Daniel Ostler</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rosenthal_J/0/1/0/all/0/1\">Jean-Claude Rosenthal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Feussner_H/0/1/0/all/0/1\">Hubertus Feu&#xdf;ner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wilhelm_D/0/1/0/all/0/1\">Dirk Wilhelm</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wollherr_D/0/1/0/all/0/1\">Dirk Wollherr</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual-tactile sensing for Real-time liquid Volume Estimation in Grasping. (arXiv:2202.11503v1 [cs.RO])","link":"http://arxiv.org/abs/2202.11503","description":"<p>We propose a deep visuo-tactile model for realtime estimation of the liquid\ninside a deformable container in a proprioceptive way.We fuse two sensory\nmodalities, i.e., the raw visual inputs from the RGB camera and the tactile\ncues from our specific tactile sensor without any extra sensor calibrations.The\nrobotic system is well controlled and adjusted based on the estimation model in\nreal time. The main contributions and novelties of our work are listed as\nfollows: 1) Explore a proprioceptive way for liquid volume estimation by\ndeveloping an end-to-end predictive model with multi-modal convolutional\nnetworks, which achieve a high precision with an error of around 2 ml in the\nexperimental validation. 2) Propose a multi-task learning architecture which\ncomprehensively considers the losses from both classification and regression\ntasks, and comparatively evaluate the performance of each variant on the\ncollected data and actual robotic platform. 3) Utilize the proprioceptive\nrobotic system to accurately serve and control the requested volume of liquid,\nwhich is continuously flowing into a deformable container in real time. 4)\nAdaptively adjust the grasping plan to achieve more stable grasping and\nmanipulation according to the real-time liquid volume prediction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Fan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1\">Ruixing Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Youcan Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Jia Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenping Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly-supervised learning for image-based classification of primary melanomas into genomic immune subgroups. (arXiv:2202.11524v1 [eess.IV])","link":"http://arxiv.org/abs/2202.11524","description":"<p>Determining early-stage prognostic markers and stratifying patients for\neffective treatment are two key challenges for improving outcomes for melanoma\npatients. Previous studies have used tumour transcriptome data to stratify\npatients into immune subgroups, which were associated with differential\nmelanoma specific survival and potential treatment strategies. However,\nacquiring transcriptome data is a time-consuming and costly process. Moreover,\nit is not routinely used in the current clinical workflow. Here we attempt to\novercome this by developing deep learning models to classify gigapixel H&amp;E\nstained pathology slides, which are well established in clinical workflows,\ninto these immune subgroups. Previous subtyping approaches have employed\nsupervised learning which requires fully annotated data, or have only examined\nsingle genetic mutations in melanoma patients. We leverage a multiple-instance\nlearning approach, which only requires slide-level labels and uses an attention\nmechanism to highlight regions of high importance to the classification.\nMoreover, we show that pathology-specific self-supervised models generate\nbetter representations compared to pathology-agnostic models for improving our\nmodel performance, achieving a mean AUC of 0.76 for classifying histopathology\nimages as high or low immune subgroups. We anticipate that this method may\nallow us to find new biomarkers of high importance and could act as a tool for\nclinicians to infer the immune landscape of tumours and stratify patients,\nwithout needing to carry out additional expensive genetic tests.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Godson_L/0/1/0/all/0/1\">Lucy Godson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Alemi_N/0/1/0/all/0/1\">Navid Alemi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nsengimana_J/0/1/0/all/0/1\">Jeremie Nsengimana</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cook_G/0/1/0/all/0/1\">Graham P. Cook</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Clarke_E/0/1/0/all/0/1\">Emily L. Clarke</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Treanor_D/0/1/0/all/0/1\">Darren Treanor</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bishop_D/0/1/0/all/0/1\">D. Timothy Bishop</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Newton_Bishop_J/0/1/0/all/0/1\">Julia Newton-Bishop</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gooya_A/0/1/0/all/0/1\">Ali Gooya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diffractive optical system design by cascaded propagation. (arXiv:2202.11535v1 [physics.optics])","link":"http://arxiv.org/abs/2202.11535","description":"<p>Modern design of complex optical systems relies heavily on computational\ntools. These typically utilize geometrical optics as well as Fourier optics,\nwhich enables the use of diffractive elements to manipulate light with features\non the scale of a wavelength. Fourier optics is typically used for designing\nthin elements, placed in the system's aperture, generating a shift-invariant\nPoint Spread Function (PSF). A major bottleneck in applying Fourier Optics in\nmany cases of interest, e.g. when dealing with multiple, or out-of-aperture\nelements, comes from numerical complexity. In this work, we propose and\nimplement an efficient and differentiable propagation model based on the\nCollins integral, which enables the optimization of diffraction optical systems\nwith unprecedented design freedom using backpropagation. We demonstrate the\napplicability of our method, numerically and experimentally, by engineering\nshift-variant PSFs via thin plate elements placed in arbitrary planes inside\ncomplex imaging systems, performing cascaded optimization of multiple planes,\nand designing optimal machine-vision systems by deep learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Ferdman_B/0/1/0/all/0/1\">Boris Ferdman</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Saguy_A/0/1/0/all/0/1\">Alon Saguy</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Alalouf_O/0/1/0/all/0/1\">Onit Alalouf</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Shechtman_Y/0/1/0/all/0/1\">Yoav Shechtman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Transformers for Unsupervised Object Discovery using Normalized Cut. (arXiv:2202.11539v1 [cs.CV])","link":"http://arxiv.org/abs/2202.11539","description":"<p>Transformers trained with self-supervised learning using self-distillation\nloss (DINO) have been shown to produce attention maps that highlight salient\nforeground objects. In this paper, we demonstrate a graph-based approach that\nuses the self-supervised transformer features to discover an object from an\nimage. Visual tokens are viewed as nodes in a weighted graph with edges\nrepresenting a connectivity score based on the similarity of tokens. Foreground\nobjects can then be segmented using a normalized graph-cut to group\nself-similar regions. We solve the graph-cut problem using spectral clustering\nwith generalized eigen-decomposition and show that the second smallest\neigenvector provides a cutting solution since its absolute value indicates the\nlikelihood that a token belongs to a foreground object. Despite its simplicity,\nthis approach significantly boosts the performance of unsupervised object\ndiscovery: we improve over the recent state of the art LOST by a margin of\n6.9%, 8.1%, and 8.1% respectively on the VOC07, VOC12, and COCO20K. The\nperformance can be further improved by adding a second stage class-agnostic\ndetector (CAD). Our proposed method can be easily extended to unsupervised\nsaliency detection and weakly supervised object detection. For unsupervised\nsaliency detection, we improve IoU for 4.9%, 5.2%, 12.9% on ECSSD, DUTS,\nDUT-OMRON respectively compared to previous state of the art. For weakly\nsupervised object detection, we achieve competitive performance on CUB and\nImageNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yangtao Wang</a> (M-PSI), <a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xi Shen</a> (LIGM), <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shell Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yuan Yuan</a> (MIT CSAIL), <a href=\"http://arxiv.org/find/cs/1/au:+Crowley_J/0/1/0/all/0/1\">James Crowley</a> (M-PSI), <a href=\"http://arxiv.org/find/cs/1/au:+Vaufreydaz_D/0/1/0/all/0/1\">Dominique Vaufreydaz</a> (M-PSI)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Amodal Panoptic Segmentation. (arXiv:2202.11542v1 [cs.CV])","link":"http://arxiv.org/abs/2202.11542","description":"<p>Humans have the remarkable ability to perceive objects as a whole, even when\nparts of them are occluded. This ability of amodal perception forms the basis\nof our perceptual and cognitive understanding of our world. To enable robots to\nreason with this capability, we formulate and propose a novel task that we name\namodal panoptic segmentation. The goal of this task is to simultaneously\npredict the pixel-wise semantic segmentation labels of the visible regions of\nstuff classes and the instance segmentation labels of both the visible and\noccluded regions of thing classes. To facilitate research on this new task, we\nextend two established benchmark datasets with pixel-level amodal panoptic\nsegmentation labels that we make publicly available as KITTI-360-APS and\nBDD100K-APS. We present several strong baselines, along with the amodal\npanoptic quality (APQ) and amodal parsing coverage (APC) metrics to quantify\nthe performance in an interpretable manner. Furthermore, we propose the novel\namodal panoptic segmentation network (APSNet), as a first step towards\naddressing this task by explicitly modeling the complex relationships between\nthe occluders and occludes. Extensive experimental evaluations demonstrate that\nAPSNet achieves state-of-the-art performance on both benchmarks and more\nimportantly exemplifies the utility of amodal recognition. The benchmarks are\navailable at <a href=\"http://amodal-panoptic.cs.uni-freiburg.de.\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohan_R/0/1/0/all/0/1\">Rohit Mohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valada_A/0/1/0/all/0/1\">Abhinav Valada</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Bayesian ICP Covariance Estimation. (arXiv:2202.11607v1 [cs.RO])","link":"http://arxiv.org/abs/2202.11607","description":"<p>Covariance estimation for the Iterative Closest Point (ICP) point cloud\nregistration algorithm is essential for state estimation and sensor fusion\npurposes. We argue that a major source of error for ICP is in the input data\nitself, from the sensor noise to the scene geometry. Benefiting from recent\ndevelopments in deep learning for point clouds, we propose a data-driven\napproach to learn an error model for ICP. We estimate covariances modeling\ndata-dependent heteroscedastic aleatoric uncertainty, and epistemic uncertainty\nusing a variational Bayesian approach. The system evaluation is performed on\nLiDAR odometry on different datasets, highlighting good results in comparison\nto the state of the art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maio_A/0/1/0/all/0/1\">Andrea De Maio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lacroix_S/0/1/0/all/0/1\">Simon Lacroix</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image Classification on Small Datasets via Masked Feature Mixing. (arXiv:2202.11616v1 [cs.CV])","link":"http://arxiv.org/abs/2202.11616","description":"<p>Deep convolutional neural networks require large amounts of labeled data\nsamples. For many real-world applications, this is a major limitation which is\ncommonly treated by augmentation methods. In this work, we address the problem\nof learning deep neural networks on small datasets. Our proposed architecture\ncalled ChimeraMix learns a data augmentation by generating compositions of\ninstances. The generative model encodes images in pairs, combines the features\nguided by a mask, and creates new samples. For evaluation, all methods are\ntrained from scratch without any additional data. Several experiments on\nbenchmark datasets, e.g. ciFAIR-10, STL-10, and ciFAIR-100, demonstrate the\nsuperior performance of ChimeraMix compared to current state-of-the-art methods\nfor classification on small datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Reinders_C/0/1/0/all/0/1\">Christoph Reinders</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schubert_F/0/1/0/all/0/1\">Frederik Schubert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosenhahn_B/0/1/0/all/0/1\">Bodo Rosenhahn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Anomaly Detection in 3D Point Clouds using Deep Geometric Descriptors. (arXiv:2202.11660v1 [cs.CV])","link":"http://arxiv.org/abs/2202.11660","description":"<p>We present a new method for the unsupervised detection of geometric anomalies\nin high-resolution 3D point clouds. In particular, we propose an adaptation of\nthe established student-teacher anomaly detection framework to three\ndimensions. A student network is trained to match the output of a pretrained\nteacher network on anomaly-free point clouds. When applied to test data,\nregression errors between the teacher and the student allow reliable\nlocalization of anomalous structures. To construct an expressive teacher\nnetwork that extracts dense local geometric descriptors, we introduce a novel\nself-supervised pretraining strategy. The teacher is trained by reconstructing\nlocal receptive fields and does not require annotations. Extensive experiments\non the comprehensive MVTec 3D Anomaly Detection dataset highlight the\neffectiveness of our approach, which outperforms the next-best method by a\nlarge margin. Ablation studies show that our approach meets the requirements of\npractical applications regarding performance, runtime, and memory consumption.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bergmann_P/0/1/0/all/0/1\">Paul Bergmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sattlegger_D/0/1/0/all/0/1\">David Sattlegger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Human Motion Detection Using Sharpened Dimensionality Reduction and Clustering. (arXiv:2202.11667v1 [cs.CV])","link":"http://arxiv.org/abs/2202.11667","description":"<p>Sharpened dimensionality reduction (SDR), which belongs to the class of\nmultidimensional projection techniques, has recently been introduced to tackle\nthe challenges in the exploratory and visual analysis of high-dimensional data.\nSDR has been applied to various real-world datasets, such as human activity\nsensory data and astronomical datasets. However, manually labeling the samples\nfrom the generated projection are expensive. To address this problem, we\npropose here to use clustering methods such as k-means, Hierarchical\nClustering, Density-Based Spatial Clustering of Applications with Noise\n(DBSCAN), and Spectral Clustering to easily label the 2D projections of\nhigh-dimensional data. We test our pipeline of SDR and the clustering methods\non a range of synthetic and real-world datasets, including two different public\nhuman activity datasets extracted from smartphone accelerometer or gyroscope\nrecordings of various movements. We apply clustering to assess the visual\ncluster separation of SDR, both qualitatively and quantitatively. We conclude\nthat clustering SDR results yields better labeling results than clustering\nplain DR, and that k-means is the recommended clustering method for SDR in\nterms of clustering accuracy, ease-of-use, and computational scalability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Heo_J/0/1/0/all/0/1\">Jeewon Heo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Youngjoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roerdink_J/0/1/0/all/0/1\">Jos B.T.M. Roerdink</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Paying U-Attention to Textures: Multi-Stage Hourglass Vision Transformer for Universal Texture Synthesis. (arXiv:2202.11703v1 [cs.CV])","link":"http://arxiv.org/abs/2202.11703","description":"<p>We present a novel U-Attention vision Transformer for universal texture\nsynthesis. We exploit the natural long-range dependencies enabled by the\nattention mechanism to allow our approach to synthesize diverse textures while\npreserving their structures in a single inference. We propose a multi-stage\nhourglass backbone that attends to the global structure and performs patch\nmapping at varying scales in a coarse-to-fine-to-coarse stream. Further\ncompleted by skip connection and convolution designs that propagate and fuse\ninformation at different scales, our U-Attention architecture unifies attention\nto microstructures, mesostructures and macrostructures, and progressively\nrefines synthesis results at successive stages. We show that our method\nachieves stronger 2$\\times$ synthesis than previous work on both stochastic and\nstructured textures while generalizing to unseen textures without fine-tuning.\nAblation studies demonstrate the effectiveness of each component of our\narchitecture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Shouchang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deschaintre_V/0/1/0/all/0/1\">Valentin Deschaintre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noll_D/0/1/0/all/0/1\">Douglas Noll</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roullier_A/0/1/0/all/0/1\">Arthur Roullier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Finding Moments in Video Collections Using Natural Language. (arXiv:1907.12763v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1907.12763","description":"<p>We introduce the task of retrieving relevant video moments from a large\ncorpus of untrimmed, unsegmented videos given a natural language query. Our\ntask poses unique challenges as a system must efficiently identify both the\nrelevant videos and localize the relevant moments in the videos. To address\nthese challenges, we propose SpatioTemporal Alignment with Language (STAL), a\nmodel that represents a video moment as a set of regions within a series of\nshort video clips and aligns a natural language query to the moment's regions.\nOur alignment cost compares variable-length language and video features using\nsymmetric squared Chamfer distance, which allows for efficient indexing and\nretrieval of the video moments. Moreover, aligning language features to regions\nwithin a video moment allows for finer alignment compared to methods that\nextract only an aggregate feature from the entire video moment. We evaluate our\napproach on two recently proposed datasets for temporal localization of moments\nin video with natural language (DiDeMo and Charades-STA) extended to our video\ncorpus moment retrieval setting. We show that our STAL re-ranking model\noutperforms the recently proposed Moment Context Network on all criteria across\nall datasets on our proposed task, obtaining relative gains of 37% - 118% for\naverage recall and up to 30% for median rank. Moreover, our approach achieves\nmore than 130x faster retrieval and 8x smaller index size with a 1M video\ncorpus in an approximate setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Escorcia_V/0/1/0/all/0/1\">Victor Escorcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soldan_M/0/1/0/all/0/1\">Mattia Soldan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sivic_J/0/1/0/all/0/1\">Josef Sivic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bernard Ghanem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Russell_B/0/1/0/all/0/1\">Bryan Russell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Segmentation of structural parts of rosebush plants with 3D point-based deep learning methods. (arXiv:2012.11489v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.11489","description":"<p>Segmentation of structural parts of 3D models of plants is an important step\nfor plant phenotyping, especially for monitoring architectural and\nmorphological traits. Current state-of-the art approaches rely on hand-crafted\n3D local features for modeling geometric variations in plant structures. While\nrecent advancements in deep learning on point clouds have the potential of\nextracting relevant local and global characteristics, the scarcity of labeled\n3D plant data impedes the exploration of this potential. We adapted six recent\npoint-based deep learning architectures (PointNet, PointNet++, DGCNN, PointCNN,\nShellNet, RIConv) for segmentation of structural parts of rosebush models. We\ngenerated 3D synthetic rosebush models to provide adequate amount of labeled\ndata for modification and pre-training of these architectures. To evaluate\ntheir performance on real rosebush plants, we used the ROSE-X data set of fully\nannotated point cloud models. We provided experiments with and without the\nincorporation of synthetic data to demonstrate the potential of point-based\ndeep learning techniques even with limited labeled data of real plants. The\nexperimental results show that PointNet++ produces the highest segmentation\naccuracy among the six point-based deep learning methods. The advantage of\nPointNet++ is that it provides a flexibility in the scales of the hierarchical\norganization of the point cloud data. Pre-training with synthetic 3D models\nboosted the performance of all architectures, except for PointNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Turgut_K/0/1/0/all/0/1\">Kaya Turgut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dutagaci_H/0/1/0/all/0/1\">Helin Dutagaci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galopin_G/0/1/0/all/0/1\">Gilles Galopin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rousseau_D/0/1/0/all/0/1\">David Rousseau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Vision Transformer. (arXiv:2012.12556v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.12556","description":"<p>Transformer, first applied to the field of natural language processing, is a\ntype of deep neural network mainly based on the self-attention mechanism.\nThanks to its strong representation capabilities, researchers are looking at\nways to apply transformer to computer vision tasks. In a variety of visual\nbenchmarks, transformer-based models perform similar to or better than other\ntypes of networks such as convolutional and recurrent neural networks. Given\nits high performance and less need for vision-specific inductive bias,\ntransformer is receiving more and more attention from the computer vision\ncommunity. In this paper, we review these vision transformer models by\ncategorizing them in different tasks and analyzing their advantages and\ndisadvantages. The main categories we explore include the backbone network,\nhigh/mid-level vision, low-level vision, and video processing. We also include\nefficient transformer methods for pushing transformer into real device-based\napplications. Furthermore, we also take a brief look at the self-attention\nmechanism in computer vision, as it is the base component in transformer.\nToward the end of this paper, we discuss the challenges and provide several\nfurther research directions for vision transformers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1\">Kai Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunhe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hanting Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xinghao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jianyuan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhenhua Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yehui Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_A/0/1/0/all/0/1\">An Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chunjing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yixing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhaohui Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yiman Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Universal Background Subtraction based on Arithmetic Distribution Neural Network. (arXiv:2104.08390v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.08390","description":"<p>We propose a universal background subtraction framework based on the\nArithmetic Distribution Neural Network (ADNN) for learning the distributions of\ntemporal pixels. In our ADNN model, the arithmetic distribution operations are\nutilized to introduce the arithmetic distribution layers, including the product\ndistribution layer and the sum distribution layer. Furthermore, in order to\nimprove the accuracy of the proposed approach, an improved Bayesian refinement\nmodel based on neighboring information, with a GPU implementation, is\nincorporated. In the forward pass and backpropagation of the proposed\narithmetic distribution layers, histograms are considered as probability\ndensity functions rather than matrices. Thus, the proposed approach is able to\nutilize the probability information of the histogram and achieve promising\nresults with a very simple architecture compared to traditional convolutional\nneural networks. Evaluations using standard benchmarks demonstrate the\nsuperiority of the proposed approach compared to state-of-the-art traditional\nand deep learning methods. To the best of our knowledge, this is the first\nmethod to propose network layers based on arithmetic distribution operations\nfor learning distributions during background subtraction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chenqiu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_K/0/1/0/all/0/1\">Kangkang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basu_A/0/1/0/all/0/1\">Anup Basu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Superpixel-based Knowledge Infusion in Deep Neural Networks for Image Classification. (arXiv:2105.09448v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.09448","description":"<p>Superpixels are higher-order perceptual groups of pixels in an image, often\ncarrying much more information than the raw pixels. There is an inherent\nrelational structure to the relationship among different superpixels of an\nimage such as adjacent superpixels are neighbours of each other. Our interest\nhere is to treat these relative positions of various superpixels as relational\ninformation of an image. This relational information can convey higher-order\nspatial information about the image, such as the relationship between\nsuperpixels representing two eyes in an image of a cat. That is, two eyes are\nplaced adjacent to each other in a straight line or the mouth is below the\nnose. Our motive in this paper is to assist computer vision models,\nspecifically those based on Deep Neural Networks (DNNs), by incorporating this\nhigher-order information from superpixels. We construct a hybrid model that\nleverages (a) Convolutional Neural Network (CNN) to deal with spatial\ninformation in an image and (b) Graph Neural Network (GNN) to deal with\nrelational superpixel information in the image. The proposed model is learned\nusing a generic hybrid loss function. Our experiments are extensive, and we\nevaluate the predictive performance of our proposed hybrid vision model on\nseven different image classification datasets from a variety of domains such as\ndigit and object recognition, biometrics, medical imaging. The results\ndemonstrate that the relational superpixel information processed by a GNN can\nimprove the performance of a standard CNN-based vision system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chhablani_G/0/1/0/all/0/1\">Gunjan Chhablani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Abheesht Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandey_H/0/1/0/all/0/1\">Harshit Pandey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dash_T/0/1/0/all/0/1\">Tirtharaj Dash</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Online Coreset Selection for Rehearsal-based Continual Learning. (arXiv:2106.01085v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.01085","description":"<p>A dataset is a shred of crucial evidence to describe a task. However, each\ndata point in the dataset does not have the same potential, as some of the data\npoints can be more representative or informative than others. This unequal\nimportance among the data points may have a large impact in rehearsal-based\ncontinual learning, where we store a subset of the training examples (coreset)\nto be replayed later to alleviate catastrophic forgetting. In continual\nlearning, the quality of the samples stored in the coreset directly affects the\nmodel's effectiveness and efficiency. The coreset selection problem becomes\neven more important under realistic settings, such as imbalanced continual\nlearning or noisy data scenarios. To tackle this problem, we propose Online\nCoreset Selection (OCS), a simple yet effective method that selects the most\nrepresentative and informative coreset at each iteration and trains them in an\nonline manner. Our proposed method maximizes the model's adaptation to a\ncurrent dataset while selecting high-affinity samples to past tasks, which\ndirectly inhibits catastrophic forgetting. We validate the effectiveness of our\ncoreset selection mechanism over various standard, imbalanced, and noisy\ndatasets against strong continual learning baselines, demonstrating that it\nimproves task adaptation and prevents catastrophic forgetting in a\nsample-efficient manner.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1\">Jaehong Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madaan_D/0/1/0/all/0/1\">Divyam Madaan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_E/0/1/0/all/0/1\">Eunho Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Sung Ju Hwang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting consistency for semi-supervised semantic segmentation. (arXiv:2106.07075v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.07075","description":"<p>Semi-supervised learning is especially interesting in the dense prediction\ncontext due to high cost of pixel-level ground truth. Unfortunately, most such\napproaches are evaluated on outdated architectures which hamper research due to\nvery slow training and high requirements on GPU RAM. We address this concern by\npresenting a simple and effective baseline which works very well both on\nstandard and efficient architectures. Our baseline is based on one-way\nconsistency and non-linear geometric and photometric perturbations. We show\nadvantage of perturbing only the student branch and present a plausible\nexplanation of such behaviour. Experiments on Cityscapes and CIFAR-10\ndemonstrate competitive performance with respect to prior work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Grubisic_I/0/1/0/all/0/1\">Ivan Grubi&#x161;i&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orsic_M/0/1/0/all/0/1\">Marin Or&#x161;i&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Segvic_S/0/1/0/all/0/1\">Sini&#x161;a &#x160;egvi&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Hybrid mmWave and Camera System for Long-Range Depth Imaging. (arXiv:2106.07856v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.07856","description":"<p>mmWave radars offer excellent depth resolution owing to their high bandwidth\nat mmWave radio frequencies. Yet, they suffer intrinsically from poor angular\nresolution, that is an order-of-magnitude worse than camera systems, and are\ntherefore not a capable 3-D imaging solution in isolation. We propose\nMetamoran, a system that combines the complimentary strengths of radar and\ncamera systems to obtain depth images at high azimuthal resolutions at\ndistances of several tens of meters with high accuracy, all from a single fixed\nvantage point. Metamoran enables rich long-range depth imaging outdoors with\napplications to roadside safety infrastructure, surveillance and wide-area\nmapping. Our key insight is to use the high azimuth resolution from cameras\nusing computer vision techniques, including image segmentation and monocular\ndepth estimation, to obtain object shapes and use these as priors for our novel\nspecular beamforming algorithm. We also design this algorithm to work in\ncluttered environments with weak reflections and in partially occluded\nscenarios. We perform a detailed evaluation of Metamoran's depth imaging and\nsensing capabilities in 200 diverse scenes at a major U.S. city. Our evaluation\nshows that Metamoran estimates the depth of an object up to 60~m away with a\nmedian error of 28~cm, an improvement of 13$\\times$ compared to a naive\nradar+camera baseline and 23$\\times$ compared to monocular depth estimation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Prabhakara_A/0/1/0/all/0/1\">Akarsh Prabhakara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Diana Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Munir_S/0/1/0/all/0/1\">Sirajum Munir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sankanaryanan_A/0/1/0/all/0/1\">Aswin Sankanaryanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rowe_A/0/1/0/all/0/1\">Anthony Rowe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Swarun Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FastSHAP: Real-Time Shapley Value Estimation. (arXiv:2107.07436v2 [stat.ML] UPDATED)","link":"http://arxiv.org/abs/2107.07436","description":"<p>Shapley values are widely used to explain black-box models, but they are\ncostly to calculate because they require many model evaluations. We introduce\nFastSHAP, a method for estimating Shapley values in a single forward pass using\na learned explainer model. FastSHAP amortizes the cost of explaining many\ninputs via a learning approach inspired by the Shapley value's weighted least\nsquares characterization, and it can be trained using standard stochastic\ngradient optimization. We compare FastSHAP to existing estimation approaches,\nrevealing that it generates high-quality explanations with orders of magnitude\nspeedup.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Jethani_N/0/1/0/all/0/1\">Neil Jethani</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sudarshan_M/0/1/0/all/0/1\">Mukund Sudarshan</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Covert_I/0/1/0/all/0/1\">Ian Covert</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lee_S/0/1/0/all/0/1\">Su-In Lee</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ranganath_R/0/1/0/all/0/1\">Rajesh Ranganath</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CycleMLP: A MLP-like Architecture for Dense Prediction. (arXiv:2107.10224v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.10224","description":"<p>This paper presents a simple MLP-like architecture, CycleMLP, which is a\nversatile backbone for visual recognition and dense predictions. As compared to\nmodern MLP architectures, e.g., MLP-Mixer, ResMLP, and gMLP, whose\narchitectures are correlated to image size and thus are infeasible in object\ndetection and segmentation, CycleMLP has two advantages compared to modern\napproaches. (1) It can cope with various image sizes. (2) It achieves linear\ncomputational complexity to image size by using local windows. In contrast,\nprevious MLPs have $O(N^2)$ computations due to fully spatial connections. We\nbuild a family of models which surpass existing MLPs and even state-of-the-art\nTransformer-based models, e.g., Swin Transformer, while using fewer parameters\nand FLOPs. We expand the MLP-like models' applicability, making them a\nversatile backbone for dense prediction tasks. CycleMLP achieves competitive\nresults on object detection, instance segmentation, and semantic segmentation.\nIn particular, CycleMLP-Tiny outperforms Swin-Tiny by 1.3% mIoU on ADE20K\ndataset with fewer FLOPs. Moreover, CycleMLP also shows excellent zero-shot\nrobustness on ImageNet-C dataset. Code is available at\nhttps://github.com/ShoufaChen/CycleMLP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shoufa Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1\">Enze Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_C/0/1/0/all/0/1\">Chongjian Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1\">Runjian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1\">Ding Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RGB Image Classification with Quantum Convolutional Ansaetze. (arXiv:2107.11099v2 [quant-ph] UPDATED)","link":"http://arxiv.org/abs/2107.11099","description":"<p>With the rapid growth of qubit numbers and coherence times in quantum\nhardware technology, implementing shallow neural networks on the so-called\nNoisy Intermediate-Scale Quantum (NISQ) devices has attracted a lot of\ninterest. Many quantum (convolutional) circuit ansaetze are proposed for\ngrayscale images classification tasks with promising empirical results.\nHowever, when applying these ansaetze on RGB images, the intra-channel\ninformation that is useful for vision tasks is not extracted effectively. In\nthis paper, we propose two types of quantum circuit ansaetze to simulate\nconvolution operations on RGB images, which differ in the way how inter-channel\nand intra-channel information are extracted. To the best of our knowledge, this\nis the first work of a quantum convolutional circuit to deal with RGB images\neffectively, with a higher test accuracy compared to the purely classical CNNs.\nWe also investigate the relationship between the size of quantum circuit ansatz\nand the learnability of the hybrid quantum-classical convolutional neural\nnetwork. Through experiments based on CIFAR-10 and MNIST datasets, we\ndemonstrate that a larger size of the quantum circuit ansatz improves\npredictive performance in multiclass classification tasks, providing useful\ninsights for near term quantum algorithm developments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/quant-ph/1/au:+Jing_Y/0/1/0/all/0/1\">Yu Jing</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Li_X/0/1/0/all/0/1\">Xiaogang Li</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Yang_Y/0/1/0/all/0/1\">Yang Yang</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Wu_C/0/1/0/all/0/1\">Chonghang Wu</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Fu_W/0/1/0/all/0/1\">Wenbing Fu</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Hu_W/0/1/0/all/0/1\">Wei Hu</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Li_Y/0/1/0/all/0/1\">Yuanyuan Li</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Xu_H/0/1/0/all/0/1\">Hua Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Dynamic 3D Spontaneous Micro-expression Database: Establishment and Evaluation. (arXiv:2108.00166v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.00166","description":"<p>Micro-expressions are spontaneous, unconscious facial movements that show\npeople's true inner emotions and have great potential in related fields of\npsychological testing. Since the face is a 3D deformation object, the\noccurrence of an expression can arouse spatial deformation of the face, but\nlimited by the available databases are 2D videos, lacking the description of 3D\nspatial information of micro-expressions. Therefore, we proposed a new\nmicro-expression database containing 2D video sequences and 3D point clouds\nsequences. The database includes 373 micro-expressions sequences, and these\nsamples were classified using the objective method based on facial action\ncoding system, as well as the non-objective method that combines video contents\nand participants' self-reports. We extracted 2D and 3D features using the local\nbinary patterns on three orthogonal planes (LBP-TOP) and curvature algorithms,\nrespectively, and evaluated the classification accuracies of these two features\nand their fusion results with leave-one-subject-out (LOSO) and 10-fold\ncross-validation. Further, we performed various neural network algorithms for\ndatabase classification, the results show that classification accuracies are\nimproved by fusing 3D features than using only 2D features. The database offers\noriginal and cropped micro-expression samples, which will facilitate the\nexploration and research on 3D Spatio-temporal features of micro-expressions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fengping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Siqi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_C/0/1/0/all/0/1\">Chun Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_D/0/1/0/all/0/1\">Danmin Miao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ultrafast Focus Detection for Automated Microscopy. (arXiv:2108.12050v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2108.12050","description":"<p>Technological advancements in modern scientific instruments, such as scanning\nelectron microscopes (SEMs), have significantly increased data acquisition\nrates and image resolutions enabling new questions to be explored; however, the\nresulting data volumes and velocities, combined with automated experiments, are\nquickly overwhelming scientists as there remain crucial steps that require\nhuman intervention, for example reviewing image focus. We present a fast\nout-of-focus detection algorithm for electron microscopy images collected\nserially and demonstrate that it can be used to provide near-real-time quality\ncontrol for neuroscience workflows. Our technique, \\textit{Multi-scale\nHistologic Feature Detection}, adapts classical computer vision techniques and\nis based on detecting various fine-grained histologic features. We exploit the\ninherent parallelism in the technique to employ GPU primitives in order to\naccelerate characterization. We show that our method can detect of out-of-focus\nconditions within just 20ms. To make these capabilities generally available, we\ndeploy our feature detector as an on-demand service and show that it can be\nused to determine the degree of focus in approximately 230ms, enabling\nnear-real-time use.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Levental_M/0/1/0/all/0/1\">Maksim Levental</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chard_R/0/1/0/all/0/1\">Ryan Chard</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chard_K/0/1/0/all/0/1\">Kyle Chard</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Foster_I/0/1/0/all/0/1\">Ian Foster</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wildenberg_G/0/1/0/all/0/1\">Gregg A. Wildenberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Patch-based Medical Image Segmentation using Matrix Product State Tensor Networks. (arXiv:2109.07138v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.07138","description":"<p>Tensor networks are efficient factorisations of high-dimensional tensors into\na network of lower-order tensors. They have been most commonly used to model\nentanglement in quantum many-body systems and more recently are witnessing\nincreased applications in supervised machine learning. In this work, we\nformulate image segmentation in a supervised setting with tensor networks. The\nkey idea is to first lift the pixels in image patches to exponentially\nhigh-dimensional feature spaces and using a linear decision hyper-plane to\nclassify the input pixels into foreground and background classes. The\nhigh-dimensional linear model itself is approximated using the matrix product\nstate (MPS) tensor network. The MPS is weight-shared between the\nnon-overlapping image patches resulting in our strided tensor network model.\nThe performance of the proposed model is evaluated on three 2D- and one 3D-\nbiomedical imaging datasets. The performance of the proposed tensor network\nsegmentation model is compared with relevant baseline methods. In the 2D\nexperiments, the tensor network model yields competitive performance compared\nto the baseline methods while being more resource efficient.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Selvan_R/0/1/0/all/0/1\">Raghavendra Selvan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dam_E/0/1/0/all/0/1\">Erik B Dam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flensborg_S/0/1/0/all/0/1\">S&#xf8;ren Alexander Flensborg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petersen_J/0/1/0/all/0/1\">Jens Petersen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ISF-GAN: An Implicit Style Function for High-Resolution Image-to-Image Translation. (arXiv:2109.12492v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.12492","description":"<p>Recently, there has been an increasing interest in image editing methods that\nemploy pre-trained unconditional image generators (e.g., StyleGAN). However,\napplying these methods to translate images to multiple visual domains remains\nchallenging. Existing works do not often preserve the domain-invariant part of\nthe image (e.g., the identity in human face translations), they do not usually\nhandle multiple domains, or do not allow for multi-modal translations. This\nwork proposes an implicit style function (ISF) to straightforwardly achieve\nmulti-modal and multi-domain image-to-image translation from pre-trained\nunconditional generators. The ISF manipulates the semantics of an input latent\ncode to make the image generated from it lying in the desired visual domain.\nOur results in human face and animal manipulations show significantly improved\nresults over the baselines. Our model enables cost-effective multi-modal\nunsupervised image-to-image translations at high resolution using pre-trained\nunconditional GANs. The code and data are available at:\n\\url{https://github.com/yhlleo/stylegan-mmuit}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yahui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yajing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_L/0/1/0/all/0/1\">Linchao Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1\">Nicu Sebe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lepri_B/0/1/0/all/0/1\">Bruno Lepri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nadai_M/0/1/0/all/0/1\">Marco De Nadai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Cluster Separation Using High-Dimensional Sharpened Dimensionality Reduction. (arXiv:2110.00317v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.00317","description":"<p>Applying dimensionality reduction (DR) to large, high-dimensional data sets\ncan be challenging when distinguishing the underlying high-dimensional data\nclusters in a 2D projection for exploratory analysis. We address this problem\nby first sharpening the clusters in the original high-dimensional data prior to\nthe DR step using Local Gradient Clustering (LGC). We then project the\nsharpened data from the high-dimensional space to 2D by a user-selected DR\nmethod. The sharpening step aids this method to preserve cluster separation in\nthe resulting 2D projection. With our method, end-users can label each distinct\ncluster to further analyze an otherwise unlabeled data set. Our\n`High-Dimensional Sharpened DR' (HD-SDR) method, tested on both synthetic and\nreal-world data sets, is favorable to DR methods with poor cluster separation\nand yields a better visual cluster separation than these DR methods with no\nsharpening. Our method achieves good quality (measured by quality metrics) and\nscales computationally well with large high-dimensional data. To illustrate its\nconcrete applications, we further apply HD-SDR on a recent astronomical\ncatalog.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Youngjoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Telea_A/0/1/0/all/0/1\">Alexandru C. Telea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trager_S/0/1/0/all/0/1\">Scott C. Trager</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roerdink_J/0/1/0/all/0/1\">Jos B. T. M. Roerdink</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Regress Bodies from Images using Differentiable Semantic Rendering. (arXiv:2110.03480v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.03480","description":"<p>Learning to regress 3D human body shape and pose (e.g.~SMPL parameters) from\nmonocular images typically exploits losses on 2D keypoints, silhouettes, and/or\npart-segmentation when 3D training data is not available. Such losses, however,\nare limited because 2D keypoints do not supervise body shape and segmentations\nof people in clothing do not match projected minimally-clothed SMPL shapes. To\nexploit richer image information about clothed people, we introduce\nhigher-level semantic information about clothing to penalize clothed and\nnon-clothed regions of the image differently. To do so, we train a body\nregressor using a novel Differentiable Semantic Rendering - DSR loss. For\nMinimally-Clothed regions, we define the DSR-MC loss, which encourages a tight\nmatch between a rendered SMPL body and the minimally-clothed regions of the\nimage. For clothed regions, we define the DSR-C loss to encourage the rendered\nSMPL body to be inside the clothing mask. To ensure end-to-end differentiable\ntraining, we learn a semantic clothing prior for SMPL vertices from thousands\nof clothed human scans. We perform extensive qualitative and quantitative\nexperiments to evaluate the role of clothing semantics on the accuracy of 3D\nhuman pose and shape estimation. We outperform all previous state-of-the-art\nmethods on 3DPW and Human3.6M and obtain on par results on MPI-INF-3DHP. Code\nand trained models are available for research at https://dsr.is.tue.mpg.de/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dwivedi_S/0/1/0/all/0/1\">Sai Kumar Dwivedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Athanasiou_N/0/1/0/all/0/1\">Nikos Athanasiou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kocabas_M/0/1/0/all/0/1\">Muhammed Kocabas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Black_M/0/1/0/all/0/1\">Michael J. Black</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Handwritten Mathematical Expression Recognition via Attention Aggregation based Bi-directional Mutual Learning. (arXiv:2112.03603v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.03603","description":"<p>Handwritten mathematical expression recognition aims to automatically\ngenerate LaTeX sequences from given images. Currently, attention-based\nencoder-decoder models are widely used in this task. They typically generate\ntarget sequences in a left-to-right (L2R) manner, leaving the right-to-left\n(R2L) contexts unexploited. In this paper, we propose an Attention aggregation\nbased Bi-directional Mutual learning Network (ABM) which consists of one shared\nencoder and two parallel inverse decoders (L2R and R2L). The two decoders are\nenhanced via mutual distillation, which involves one-to-one knowledge transfer\nat each training step, making full use of the complementary information from\ntwo inverse directions. Moreover, in order to deal with mathematical symbols in\ndiverse scales, an Attention Aggregation Module (AAM) is proposed to\neffectively integrate multi-scale coverage attentions. Notably, in the\ninference phase, given that the model already learns knowledge from two inverse\ndirections, we only use the L2R branch for inference, keeping the original\nparameter size and inference speed. Extensive experiments demonstrate that our\nproposed approach achieves the recognition accuracy of 56.85 % on CROHME 2014,\n52.92 % on CROHME 2016, and 53.96 % on CROHME 2019 without data augmentation\nand model ensembling, substantially outperforming the state-of-the-art methods.\nThe source code is available in https://github.com/XH-B/ABM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bian_X/0/1/0/all/0/1\">Xiaohang Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1\">Bo Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xin_X/0/1/0/all/0/1\">Xiaozhe Xin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianwu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_X/0/1/0/all/0/1\">Xuefeng Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanfeng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Exploring Pose Estimation as an Auxiliary Learning Task for Visible-Infrared Person Re-identification. (arXiv:2201.03859v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.03859","description":"<p>Visible-infrared person re-identification (VI-ReID) has been challenging due\nto the existence of large discrepancies between visible and infrared\nmodalities. Most pioneering approaches reduce intra-class variations and\ninter-modality discrepancies by learning modality-shared and ID-related\nfeatures. However, an explicit modality-shared cue, i.e., body keypoints, has\nnot been fully exploited in VI-ReID. Additionally, existing feature learning\nparadigms imposed constraints on either global features or partitioned feature\nstripes, which neglect the prediction consistency of global and part features.\nTo address the above problems, we exploit Pose Estimation as an auxiliary\nlearning task to assist the VI-ReID task in an end-to-end framework. By jointly\ntraining these two tasks in a mutually beneficial manner, our model learns\nhigher quality modality-shared and ID-related features. On top of it, the\nlearnings of global features and local features are seamlessly synchronized by\nHierarchical Feature Constraint (HFC), where the former supervises the latter\nusing the knowledge distillation strategy. Experimental results on two\nbenchmark VI-ReID datasets show that the proposed method consistently improves\nstate-of-the-art methods by significant margins. Specifically, our method\nachieves nearly 20$\\%$ mAP improvements against the state-of-the-art method on\nthe RegDB dataset. Our intriguing findings highlight the usage of auxiliary\ntask learning in VI-ReID.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Miao_Y/0/1/0/all/0/1\">Yunqi Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_N/0/1/0/all/0/1\">Nianchang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jungong Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GradMax: Growing Neural Networks using Gradient Information. (arXiv:2201.05125v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2201.05125","description":"<p>The architecture and the parameters of neural networks are often optimized\nindependently, which requires costly retraining of the parameters whenever the\narchitecture is modified. In this work we instead focus on growing the\narchitecture without requiring costly retraining. We present a method that adds\nnew neurons during training without impacting what is already learned, while\nimproving the training dynamics. We achieve the latter by maximizing the\ngradients of the new weights and find the optimal initialization efficiently by\nmeans of the singular value decomposition (SVD). We call this technique\nGradient Maximizing Growth (GradMax) and demonstrate its effectiveness in\nvariety of vision tasks and architectures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Evci_U/0/1/0/all/0/1\">Utku Evci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merrienboer_B/0/1/0/all/0/1\">Bart van Merri&#xeb;nboer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Unterthiner_T/0/1/0/all/0/1\">Thomas Unterthiner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vladymyrov_M/0/1/0/all/0/1\">Max Vladymyrov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pedregosa_F/0/1/0/all/0/1\">Fabian Pedregosa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Inexact Unlearning Requires Revisiting Forgetting. (arXiv:2201.06640v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2201.06640","description":"<p>Existing methods in inexact unlearning are evaluated by measuring\nindistinguishability from models retrained after removing the deletion set. We\nargue that achieving indistinguishability is unnecessary and its practical\nrelaxations are insufficient. We formulate the goal of unlearning as forgetting\nall information specific to the deletion set while maintaining high utility and\nresource efficiency.\n</p>\n<p>We introduce a novel test for forgetting called Interclass Confusion (IC).\nDespite being a black-box test, IC can investigate whether information from the\ndeletion set was erased until the early layers of the network. We analyze two\naspects of forgetting: (i) memorization and (ii) property generalization. We\nempirically show that two simple unlearning methods, exact-unlearning and\ncatastrophic-forgetting the final k layers of a network, outperforms prior\nunlearning methods when scaled to large deletion sets. Overall, we believe our\nformulation and the IC test will guide the design of better unlearning\nalgorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Goel_S/0/1/0/all/0/1\">Shashwat Goel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prabhu_A/0/1/0/all/0/1\">Ameya Prabhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumaraguru_P/0/1/0/all/0/1\">Ponnurangam Kumaraguru</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLA-NeRF: Category-Level Articulated Neural Radiance Field. (arXiv:2202.00181v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.00181","description":"<p>We propose CLA-NeRF -- a Category-Level Articulated Neural Radiance Field\nthat can perform view synthesis, part segmentation, and articulated pose\nestimation. CLA-NeRF is trained at the object category level using no CAD\nmodels and no depth, but a set of RGB images with ground truth camera poses and\npart segments. During inference, it only takes a few RGB views (i.e., few-shot)\nof an unseen 3D object instance within the known category to infer the object\npart segmentation and the neural radiance field. Given an articulated pose as\ninput, CLA-NeRF can perform articulation-aware volume rendering to generate the\ncorresponding RGB image at any camera pose. Moreover, the articulated pose of\nan object can be estimated via inverse rendering. In our experiments, we\nevaluate the framework across five categories on both synthetic and real-world\ndata. In all cases, our method shows realistic deformation results and accurate\narticulated pose estimation. We believe that both few-shot articulated object\nrendering and articulated pose estimation open doors for robots to perceive and\ninteract with unseen articulated objects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tseng_W/0/1/0/all/0/1\">Wei-Cheng Tseng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_H/0/1/0/all/0/1\">Hung-Ju Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yen-Chen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Min Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Guide Local Feature Matching by Overlap Estimation. (arXiv:2202.09050v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.09050","description":"<p>Local image feature matching under large appearance, viewpoint, and distance\nchanges is challenging yet important. Conventional methods detect and match\ntentative local features across the whole images, with heuristic consistency\nchecks to guarantee reliable matches. In this paper, we introduce a novel\nOverlap Estimation method conditioned on image pairs with TRansformer, named\nOETR, to constrain local feature matching in the commonly visible region. OETR\nperforms overlap estimation in a two-step process of feature correlation and\nthen overlap regression. As a preprocessing module, OETR can be plugged into\nany existing local feature detection and matching pipeline, to mitigate\npotential view angle or scale variance. Intensive experiments show that OETR\ncan boost state-of-the-art local feature matching performance substantially,\nespecially for image pairs with small shared regions. The code will be publicly\navailable at https://github.com/AbyssGaze/OETR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Ying Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">Dihe Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jianlin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RDP-Net: Region Detail Preserving Network for Change Detection. (arXiv:2202.09745v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2202.09745","description":"<p>Change detection (CD) is an essential earth observation technique. It\ncaptures the dynamic information of land objects. With the rise of deep\nlearning, neural networks (NN) have shown great potential in CD. However,\ncurrent NN models introduce backbone architectures that lose the detail\ninformation during learning. Moreover, current NN models are heavy in\nparameters, which prevents their deployment on edge devices such as drones. In\nthis work, we tackle this issue by proposing RDP-Net: a region detail\npreserving network for CD. We propose an efficient training strategy that\nquantifies the importance of individual samples during the warmup period of NN\ntraining. Then, we perform non-uniform sampling based on the importance score\nso that the NN could learn detail information from easy to hard. Next, we\npropose an effective edge loss that improves the network's attention on details\nsuch as boundaries and small regions. As a result, we provide a NN model that\nachieves the state-of-the-art empirical performance in CD with only 1.70M\nparameters. We hope our RDP-Net would benefit the practical CD applications on\ncompact devices and could inspire more people to bring change detection to a\nnew level with the efficient training strategy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chen_H/0/1/0/all/0/1\">Hongjia Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pu_F/0/1/0/all/0/1\">Fangling Pu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_R/0/1/0/all/0/1\">Rui Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tang_R/0/1/0/all/0/1\">Rui Tang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_X/0/1/0/all/0/1\">Xin Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Feature based Cross-slide Registration. (arXiv:2202.09971v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.09971","description":"<p>Cross-slide image analysis provides additional information by analysing the\nexpression of different biomarkers as compared to a single slide analysis.\nSlides stained with different biomarkers are analysed side by side which may\nreveal unknown relations between the different biomarkers. During the slide\npreparation, a tissue section may be placed at an arbitrary orientation as\ncompared to other sections of the same tissue block. The problem is compounded\nby the fact that tissue contents are likely to change from one section to the\nnext and there may be unique artefacts on some of the slides. This makes\nregistration of each section to a reference section of the same tissue block an\nimportant pre-requisite task before any cross-slide analysis. We propose a deep\nfeature based registration (DFBR) method which utilises data-driven features to\nestimate the rigid transformation. We adopted a multi-stage strategy for\nimproving the quality of registration. We also developed a visualisation tool\nto view registered pairs of WSIs at different magnifications. With the help of\nthis tool, one can apply a transformation on the fly without the need to\ngenerate transformed source WSI in a pyramidal form. We compared the\nperformance of data-driven features with that of hand-crafted features on the\nCOMET dataset. Our approach can align the images with low registration errors.\nGenerally, the success of non-rigid registration is dependent on the quality of\nrigid registration. To evaluate the efficacy of the DFBR method, the first two\nsteps of the ANHIR winner's framework are replaced with our DFBR to register\nchallenge provided image pairs. The modified framework produce comparable\nresults to that of challenge winning team.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Awan_R/0/1/0/all/0/1\">Ruqayya Awan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raza_S/0/1/0/all/0/1\">Shan E Ahmed Raza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lotz_J/0/1/0/all/0/1\">Johannes Lotz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajpoot_N/0/1/0/all/0/1\">Nasir M. Rajpoot</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Winning Solution to the iFLYTEK Challenge 2021 Cultivated Land Extraction from High-Resolution Remote Sensing Image. (arXiv:2202.10974v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.10974","description":"<p>Extracting cultivated land accurately from high-resolution remote images is a\nbasic task for precision agriculture. This report introduces our solution to\nthe iFLYTEK challenge 2021 cultivated land extraction from high-resolution\nremote sensing image. The challenge requires segmenting cultivated land objects\nin very high-resolution multispectral remote sensing images. We established a\nhighly effective and efficient pipeline to solve this problem. We first divided\nthe original images into small tiles and separately performed instance\nsegmentation on each tile. We explored several instance segmentation algorithms\nthat work well on natural images and developed a set of effective methods that\nare applicable to remote sensing images. Then we merged the prediction results\nof all small tiles into seamless, continuous segmentation results through our\nproposed overlap-tile fusion strategy. We achieved the first place among 486\nteams in the challenge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuqiu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Gang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_L/0/1/0/all/0/1\">Liang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaolin Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-02-23T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/"}}]}]}