{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-06-07T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Sentences as connection paths: A neural language architecture of sentence structure in the brain. (arXiv:2206.01725v1 [cs.CL])","link":"http://arxiv.org/abs/2206.01725","description":"<p>This article presents a neural language architecture of sentence structure in\nthe brain, in which sentences are temporal connection paths that interconnect\nneural structures underlying their words. Words remain 'in-situ', hence they\nare always content-addressable. Arbitrary and novel sentences (with novel\nwords) can be created with 'neural blackboards' for words and sentences. Hence,\nthe unlimited productivity of natural language can be achieved with a 'fixed'\nsmall world like network structure. The article focuses on the neural\nblackboard for sentences. The architecture uses only one 'connection matrix'\nfor binding all structural relations between words in sentences. Its ability to\nrepresent arbitrary (English) sentences is discussed in detail, based on a\ncomprehensive analysis of them. The architecture simulates intra-cranial brain\nactivity observed during sentence processing and fMRI observations related to\nsentence complexity and ambiguity. The simulations indicate that the observed\neffects relate to global control over the architecture, not to the sentence\nstructures involved, which predicts higher activity differences related to\ncomplexity and ambiguity with higher comprehension capacity. Other aspects\ndiscussed are the 'intrinsic' sentence structures provided by connection paths\nand their relation to scope and inflection, the use of a dependency parser for\ncontrol of binding, long-distance dependencies and gaps, question answering,\nambiguity resolution based on backward processing without explicit\nbacktracking, garden paths, and performance difficulties related to embeddings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Velde_F/0/1/0/all/0/1\">Frank van der Velde</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"[Re] Badder Seeds: Reproducing the Evaluation of Lexical Methods for Bias Measurement. (arXiv:2206.01767v1 [cs.CL])","link":"http://arxiv.org/abs/2206.01767","description":"<p>Combating bias in NLP requires bias measurement. Bias measurement is almost\nalways achieved by using lexicons of seed terms, i.e. sets of words specifying\nstereotypes or dimensions of interest. This reproducibility study focuses on\nthe original authors' main claim that the rationale for the construction of\nthese lexicons needs thorough checking before usage, as the seeds used for bias\nmeasurement can themselves exhibit biases. The study aims to evaluate the\nreproducibility of the quantitative and qualitative results presented in the\npaper and the conclusions drawn thereof. We reproduce most of the results\nsupporting the original authors' general claim: seed sets often suffer from\nbiases that affect their performance as a baseline for bias metrics. Generally,\nour results mirror the original paper's. They are slightly different on select\noccasions, but not in ways that undermine the paper's general intent to show\nthe fragility of seed sets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Togt_J/0/1/0/all/0/1\">Jille van der Togt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tiyavorabun_L/0/1/0/all/0/1\">Lea Tiyavorabun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosati_M/0/1/0/all/0/1\">Matteo Rosati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Starace_G/0/1/0/all/0/1\">Giulio Starace</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"QAGCN: A Graph Convolutional Network-based Multi-Relation Question Answering System. (arXiv:2206.01818v1 [cs.AI])","link":"http://arxiv.org/abs/2206.01818","description":"<p>Answering multi-relation questions over knowledge graphs is a challenging\ntask as it requires multi-step reasoning over a huge number of possible paths.\nReasoning-based methods with complex reasoning mechanisms, such as\nreinforcement learning-based sequential decision making, have been regarded as\nthe default pathway for this task. However, these mechanisms are difficult to\nimplement and train, which hampers their reproducibility and transferability to\nnew domains. In this paper, we propose QAGCN - a simple but effective and novel\nmodel that leverages attentional graph convolutional networks that can perform\nmulti-step reasoning during the encoding of knowledge graphs. As a consequence,\ncomplex reasoning mechanisms are avoided. In addition, to improve efficiency,\nwe retrieve answers using highly-efficient embedding computations and, for\nbetter interpretability, we extract interpretable paths for returned answers.\nOn widely adopted benchmark datasets, the proposed model has been demonstrated\ncompetitive against state-of-the-art methods that rely on complex reasoning\nmechanisms. We also conducted extensive experiments to scrutinize the\nefficiency and contribution of each component of our model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ruijie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rossetto_L/0/1/0/all/0/1\">Luca Rossetto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cochez_M/0/1/0/all/0/1\">Michael Cochez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bernstein_A/0/1/0/all/0/1\">Abraham Bernstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Relevance in Dialogue: Is Less More? An Empirical Comparison of Existing Metrics, and a Novel Simple Metric. (arXiv:2206.01823v1 [cs.CL])","link":"http://arxiv.org/abs/2206.01823","description":"<p>In this work, we evaluate various existing dialogue relevance metrics, find\nstrong dependency on the dataset, often with poor correlation with human scores\nof relevance, and propose modifications to reduce data requirements and domain\nsensitivity while improving correlation. Our proposed metric achieves\nstate-of-the-art performance on the HUMOD dataset while reducing measured\nsensitivity to dataset by 37%-66%. We achieve this without fine-tuning a\npretrained language model, and using only 3,750 unannotated human dialogues and\na single negative example. Despite these limitations, we demonstrate\ncompetitive performance on four datasets from different domains. Our code,\nincluding our metric and experiments, is open sourced.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Berlot_Attwell_I/0/1/0/all/0/1\">Ian Berlot-Attwell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudzicz_F/0/1/0/all/0/1\">Frank Rudzicz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Kallima: A Clean-label Framework for Textual Backdoor Attacks. (arXiv:2206.01832v1 [cs.CR])","link":"http://arxiv.org/abs/2206.01832","description":"<p>Although Deep Neural Network (DNN) has led to unprecedented progress in\nvarious natural language processing (NLP) tasks, research shows that deep\nmodels are extremely vulnerable to backdoor attacks. The existing backdoor\nattacks mainly inject a small number of poisoned samples into the training\ndataset with the labels changed to the target one. Such mislabeled samples\nwould raise suspicion upon human inspection, potentially revealing the attack.\nTo improve the stealthiness of textual backdoor attacks, we propose the first\nclean-label framework Kallima for synthesizing mimesis-style backdoor samples\nto develop insidious textual backdoor attacks. We modify inputs belonging to\nthe target class with adversarial perturbations, making the model rely more on\nthe backdoor trigger. Our framework is compatible with most existing backdoor\ntriggers. The experimental results on three benchmark datasets demonstrate the\neffectiveness of the proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaoyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yinpeng Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zeyu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_S/0/1/0/all/0/1\">Shengfang Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Q/0/1/0/all/0/1\">Qingni Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhonghai Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Clues: Bridging Vision and Language Foundations for Image Paragraph Captioning. (arXiv:2206.01843v1 [cs.CV])","link":"http://arxiv.org/abs/2206.01843","description":"<p>People say, \"A picture is worth a thousand words\". Then how can we get the\nrich information out of the image? We argue that by using visual clues to\nbridge large pretrained vision foundation models and language models, we can do\nso without any extra cross-modal training. Thanks to the strong zero-shot\ncapability of foundation models, we start by constructing a rich semantic\nrepresentation of the image (e.g., image tags, object attributes / locations,\ncaptions) as a structured textual prompt, called visual clues, using a vision\nfoundation model. Based on visual clues, we use large language model to produce\na series of comprehensive descriptions for the visual content, which is then\nverified by the vision model again to select the candidate that aligns best\nwith the image. We evaluate the quality of generated descriptions by\nquantitative and qualitative measurement. The results demonstrate the\neffectiveness of such a structured semantic representation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yujia Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Luowei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1\">Xiyang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bach_N/0/1/0/all/0/1\">Nguyen Bach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Ce Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1\">Michael Zeng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extreme Compression for Pre-trained Transformers Made Simple and Efficient. (arXiv:2206.01859v1 [cs.CL])","link":"http://arxiv.org/abs/2206.01859","description":"<p>Extreme compression, particularly ultra-low bit precision (binary/ternary)\nquantization, has been proposed to fit large NLP models on resource-constraint\ndevices. However, to preserve the accuracy for such aggressive compression\nschemes, cutting-edge methods usually introduce complicated compression\npipelines, e.g., multi-stage expensive knowledge distillation with extensive\nhyperparameter tuning. Also, they oftentimes focus less on smaller transformer\nmodels that have already been heavily compressed via knowledge distillation and\nlack a systematic study to show the effectiveness of their methods. In this\npaper, we perform a very comprehensive systematic study to measure the impact\nof many key hyperparameters and training strategies from previous works. As a\nresult, we find out that previous baselines for ultra-low bit precision\nquantization are significantly under-trained. Based on our study, we propose a\nsimple yet effective compression pipeline for extreme compression, named XTC.\nXTC demonstrates that (1) we can skip the pre-training knowledge distillation\nto obtain a 5-layer BERT while achieving better performance than previous\nstate-of-the-art methods, e.g., the 6-layer TinyBERT; (2) extreme quantization\nplus layer reduction is able to reduce the model size by 50x, resulting in new\nstate-of-the-art results on GLUE tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiaoxia Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1\">Zhewei Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Minjia Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Conglong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yuxiong He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers. (arXiv:2206.01861v1 [cs.CL])","link":"http://arxiv.org/abs/2206.01861","description":"<p>How to efficiently serve ever-larger trained natural language models in\npractice has become exceptionally challenging even for powerful cloud servers\ndue to their prohibitive memory/computation requirements. In this work, we\npresent an efficient and affordable post-training quantization approach to\ncompress large Transformer-based models, termed as ZeroQuant. ZeroQuant is an\nend-to-end quantization and inference pipeline with three main components: (1)\na fine-grained hardware-friendly quantization scheme for both weight and\nactivations; (2) a novel affordable layer-by-layer knowledge distillation\nalgorithm (LKD) even without the access to the original training data; (3) a\nhighly-optimized quantization system backend support to remove the\nquantization/dequantization overhead. As such, we are able to show that: (1)\nZeroQuant can reduce the precision for weights and activations to INT8 in a\ncost-free way for both BERT and GPT3-style models with minimal accuracy impact,\nwhich leads to up to 5.19x/4.16x speedup on those models compared to FP16\ninference; (2) ZeroQuant plus LKD affordably quantize the weights in the\nfully-connected module to INT4 along with INT8 weights in the attention module\nand INT8 activations, resulting in 3x memory footprint reduction compared to\nthe FP16 model; (3) ZeroQuant can be directly applied to two of the largest\nopen-sourced language models, including GPT-J6B and GPT-NeoX20, for which our\nINT8 model achieves similar accuracy as the FP16 model but achieves up to 5.2x\nbetter efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1\">Zhewei Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aminabadi_R/0/1/0/all/0/1\">Reza Yazdani Aminabadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Minjia Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiaoxia Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Conglong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yuxiong He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Initial Study into Application of Feature Density and Linguistically-backed Embedding to Improve Machine Learning-based Cyberbullying Detection. (arXiv:2206.01889v1 [cs.CL])","link":"http://arxiv.org/abs/2206.01889","description":"<p>In this research, we study the change in the performance of machine learning\n(ML) classifiers when various linguistic preprocessing methods of a dataset\nwere used, with the specific focus on linguistically-backed embeddings in\nConvolutional Neural Networks (CNN). Moreover, we study the concept of Feature\nDensity and confirm its potential to comparatively predict the performance of\nML classifiers, including CNN. The research was conducted on a Formspring\ndataset provided in a Kaggle competition on automatic cyberbullying detection.\nThe dataset was re-annotated by objective experts (psychologists), as the\nimportance of professional annotation in cyberbullying research has been\nindicated multiple times. The study confirmed the effectiveness of Neural\nNetworks in cyberbullying detection and the correlation between classifier\nperformance and Feature Density while also proposing a new approach of training\nvarious linguistically-backed embeddings for Convolutional Neural Networks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Eronen_J/0/1/0/all/0/1\">Juuso Eronen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ptaszynski_M/0/1/0/all/0/1\">Michal Ptaszynski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masui_F/0/1/0/all/0/1\">Fumito Masui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leliwa_G/0/1/0/all/0/1\">Gniewosz Leliwa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wroczynski_M/0/1/0/all/0/1\">Michal Wroczynski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piech_M/0/1/0/all/0/1\">Mateusz Piech</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smywinski_Pohl_A/0/1/0/all/0/1\">Aleksander Smywinski-Pohl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated Audio Captioning with Epochal Difficult Captions for Curriculum Learning. (arXiv:2206.01918v1 [cs.CL])","link":"http://arxiv.org/abs/2206.01918","description":"<p>In this paper, we propose an algorithm, Epochal Difficult Captions, to\nsupplement the training of any model for the Automated Audio Captioning task.\nEpochal Difficult Captions is an elegant evolution to the keyword estimation\ntask that previous work have used to train the encoder of the AAC model.\nEpochal Difficult Captions modifies the target captions based on a curriculum\nand a difficulty level determined as a function of current epoch. Epochal\nDifficult Captions can be used with any model architecture and is a lightweight\nfunction that does not increase training time. We test our results on three\nsystems and show that using Epochal Difficult Captions consistently improves\nperformance\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Koh_A/0/1/0/all/0/1\">Andrew Koh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tiwari_S/0/1/0/all/0/1\">Soham Tiwari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siong_C/0/1/0/all/0/1\">Chng Eng Siong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring the Potential of Feature Density in Estimating Machine Learning Classifier Performance with Application to Cyberbullying Detection. (arXiv:2206.01949v1 [cs.CL])","link":"http://arxiv.org/abs/2206.01949","description":"<p>In this research. we analyze the potential of Feature Density (HD) as a way\nto comparatively estimate machine learning (ML) classifier performance prior to\ntraining. The goal of the study is to aid in solving the problem of\nresource-intensive training of ML models which is becoming a serious issue due\nto continuously increasing dataset sizes and the ever rising popularity of Deep\nNeural Networks (DNN). The issue of constantly increasing demands for more\npowerful computational resources is also affecting the environment, as training\nlarge-scale ML models are causing alarmingly-growing amounts of CO2, emissions.\nOur approach 1s to optimize the resource-intensive training of ML models for\nNatural Language Processing to reduce the number of required experiments\niterations. We expand on previous attempts on improving classifier training\nefficiency with FD while also providing an insight to the effectiveness of\nvarious linguistically-backed feature preprocessing methods for dialog\nclassification, specifically cyberbullying detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Eronen_J/0/1/0/all/0/1\">Juuso Eronen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ptaszynski_M/0/1/0/all/0/1\">Michal Ptaszynski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masui_F/0/1/0/all/0/1\">Fumito Masui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leliwa_G/0/1/0/all/0/1\">Gniewosz Leliwa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wroczynski_M/0/1/0/all/0/1\">Michal Wroczynski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comparing Performance of Different Linguistically-Backed Word Embeddings for Cyberbullying Detection. (arXiv:2206.01950v1 [cs.CL])","link":"http://arxiv.org/abs/2206.01950","description":"<p>In most cases, word embeddings are learned only from raw tokens or in some\ncases, lemmas. This includes pre-trained language models like BERT. To\ninvestigate on the potential of capturing deeper relations between lexical\nitems and structures and to filter out redundant information, we propose to\npreserve the morphological, syntactic and other types of linguistic information\nby combining them with the raw tokens or lemmas. This means, for example,\nincluding parts-of-speech or dependency information within the used lexical\nfeatures. The word embeddings can then be trained on the combinations instead\nof just raw tokens. It is also possible to later apply this method to the\npre-training of huge language models and possibly enhance their performance.\nThis would aid in tackling problems which are more sophisticated from the point\nof view of linguistic representation, such as detection of cyberbullying.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Eronen_J/0/1/0/all/0/1\">Juuso Eronen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ptaszynski_M/0/1/0/all/0/1\">Michal Ptaszynski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masui_F/0/1/0/all/0/1\">Fumito Masui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Instance-wise Prompt Tuning for Pretrained Language Models. (arXiv:2206.01958v1 [cs.CL])","link":"http://arxiv.org/abs/2206.01958","description":"<p>Prompt Learning has recently gained great popularity in bridging the gap\nbetween pretraining tasks and various downstream tasks. It freezes Pretrained\nLanguage Models (PLMs) and only tunes a few task-related parameters (prompts)\nfor downstream tasks, greatly reducing the cost of tuning giant models. The key\nenabler of this is the idea of querying PLMs with task-specific knowledge\nimplicated in prompts. This paper reveals a major limitation of existing\nmethods that the indiscriminate prompts for all input data in a task ignore the\nintrinsic knowledge from input data, resulting in sub-optimal performance. We\nintroduce Instance-wise Prompt Tuning (IPT), the first prompt learning paradigm\nthat injects knowledge from the input data instances to the prompts, thereby\nproviding PLMs with richer and more concrete context information. We devise a\nseries of strategies to produce instance-wise prompts, addressing various\nconcerns like model quality and cost-efficiency. Across multiple tasks and\nresource settings, IPT significantly outperforms task-based prompt learning\nmethods, and achieves comparable performance to conventional finetuning with\nonly 0.5% - 1.5% of tuned parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yuezihan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Junyang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hanyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1\">An Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hongxia Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_B/0/1/0/all/0/1\">Bin Cui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking the Openness of CLIP. (arXiv:2206.01986v1 [cs.CV])","link":"http://arxiv.org/abs/2206.01986","description":"<p>Contrastive Language-Image Pre-training (CLIP) has demonstrated great\npotential in realizing open-vocabulary image classification in a matching\nstyle, because of its holistic use of natural language supervision that covers\nunconstrained real-world visual concepts. However, it is, in turn, also\ndifficult to evaluate and analyze the openness of CLIP-like models, since they\nare in theory open to any vocabulary but the actual accuracy varies. To address\nthe insufficiency of conventional studies on openness, we resort to an\nincremental view and define the extensibility, which essentially approximates\nthe model's ability to deal with new visual concepts, by evaluating openness\nthrough vocabulary expansions. Our evaluation based on extensibility shows that\nCLIP-like models are hardly truly open and their performances degrade as the\nvocabulary expands to different degrees. Further analysis reveals that the\nover-estimation of openness is not because CLIP-like models fail to capture the\ngeneral similarity of image and text features of novel visual concepts, but\nbecause of the confusion among competing text features, that is, they are not\nstable with respect to the vocabulary. In light of this, we propose to improve\nthe openness of CLIP from the perspective of feature space by enforcing the\ndistinguishability of text features. Our method retrieves relevant texts from\nthe pre-training corpus to enhance prompts for inference, which boosts the\nextensibility and stability of CLIP even without fine-tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1\">Shuhuai Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xuancheng Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1\">Guangxiang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xu Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Atypical lexical abbreviations identification in Russian medical texts. (arXiv:2206.01987v1 [cs.CL])","link":"http://arxiv.org/abs/2206.01987","description":"<p>Abbreviation is a method of word formation that aims to construct the\nshortened term from the first letters of the initial phrase. Implicit\nabbreviations frequently cause the comprehension difficulties for unprepared\nreaders. In this paper, we propose an efficient ML-based algorithm which allows\nto identify the abbreviations in Russian texts. The method achieves ROC AUC\nscore 0.926 and F1 score 0.706 which are confirmed as competitive in comparison\nwith the baselines. Along with the pipeline, we also establish first to our\nknowledge Russian dataset that is relevant for the desired task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Berdichevskaia_A/0/1/0/all/0/1\">Anna Berdichevskaia</a> (NUST &quot;MISiS&quot;)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Actuarial Applications of Natural Language Processing Using Transformers: Case Studies for Using Text Features in an Actuarial Context. (arXiv:2206.02014v1 [cs.CL])","link":"http://arxiv.org/abs/2206.02014","description":"<p>This tutorial demonstrates workflows to incorporate text data into actuarial\nclassification and regression tasks. The main focus is on methods employing\ntransformer-based models. A dataset of car accident descriptions with an\naverage length of 400 words, available in English and German, and a dataset\nwith short property insurance claims descriptions are used to demonstrate these\ntechniques. The case studies tackle challenges related to a multi-lingual\nsetting and long input sequences. They also show ways to interpret model\noutput, to assess and improve model performance, by fine-tuning the models to\nthe domain of application or to a specific prediction task. Finally, the\ntutorial provides practical approaches to handle classification tasks in\nsituations with no or only few labeled data. The results achieved by using the\nlanguage-understanding skills of off-the-shelf natural language processing\n(NLP) models with only minimal pre-processing and fine-tuning clearly\ndemonstrate the power of transfer learning for practical applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Troxler_A/0/1/0/all/0/1\">Andreas Troxler</a> (AT Analytics), <a href=\"http://arxiv.org/find/cs/1/au:+Schelldorfer_J/0/1/0/all/0/1\">J&#xfc;rg Schelldorfer</a> (Swiss Re)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual Neural Machine Translation with Deep Encoder and Multiple Shallow Decoders. (arXiv:2206.02079v1 [cs.CL])","link":"http://arxiv.org/abs/2206.02079","description":"<p>Recent work in multilingual translation advances translation quality\nsurpassing bilingual baselines using deep transformer models with increased\ncapacity. However, the extra latency and memory costs introduced by this\napproach may make it unacceptable for efficiency-constrained applications. It\nhas recently been shown for bilingual translation that using a deep encoder and\nshallow decoder (DESD) can reduce inference latency while maintaining\ntranslation quality, so we study similar speed-accuracy trade-offs for\nmultilingual translation. We find that for many-to-one translation we can\nindeed increase decoder speed without sacrificing quality using this approach,\nbut for one-to-many translation, shallow decoders cause a clear quality drop.\nTo ameliorate this drop, we propose a deep encoder with multiple shallow\ndecoders (DEMSD) where each shallow decoder is responsible for a disjoint\nsubset of target languages. Specifically, the DEMSD model with 2-layer decoders\nis able to obtain a 1.8x speedup on average compared to a standard transformer\nmodel with no drop in translation quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kong_X/0/1/0/all/0/1\">Xiang Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Renduchintala_A/0/1/0/all/0/1\">Adithya Renduchintala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cross_J/0/1/0/all/0/1\">James Cross</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yuqing Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jiatao Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xian Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LAE: Language-Aware Encoder for Monolingual and Multilingual ASR. (arXiv:2206.02093v1 [cs.CL])","link":"http://arxiv.org/abs/2206.02093","description":"<p>Despite the rapid progress in automatic speech recognition (ASR) research,\nrecognizing multilingual speech using a unified ASR system remains highly\nchallenging. Previous works on multilingual speech recognition mainly focus on\ntwo directions: recognizing multiple monolingual speech or recognizing\ncode-switched speech that uses different languages interchangeably within a\nsingle utterance. However, a pragmatic multilingual recognizer is expected to\nbe compatible with both directions. In this work, a novel language-aware\nencoder (LAE) architecture is proposed to handle both situations by\ndisentangling language-specific information and generating frame-level\nlanguage-aware representations during encoding. In the LAE, the primary\nencoding is implemented by the shared block while the language-specific blocks\nare used to extract specific representations for each language. To learn\nlanguage-specific information discriminatively, a language-aware training\nmethod is proposed to optimize the language-specific blocks in LAE. Experiments\nconducted on Mandarin-English code-switched speech suggest that the proposed\nLAE is capable of discriminating different languages in frame-level and shows\nsuperior performance on both monolingual and multilingual ASR tasks. With\neither a real-recorded or simulated code-switched dataset, the proposed LAE\nachieves statistically significant improvements on both CTC and neural\ntransducer systems. Code is released\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1\">Jinchuan Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jianwei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chunlei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weng_C/0/1/0/all/0/1\">Chao Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuexian Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dong Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Speech Detection Task Against Asian Hate: BERT the Central, While Data-Centric Studies the Crucial. (arXiv:2206.02114v1 [cs.CL])","link":"http://arxiv.org/abs/2206.02114","description":"<p>With the epidemic continuing, hatred against Asians is intensifying in\ncountries outside Asia, especially among the Chinese. Thus, there is an urgent\nneed to detect and prevent hate speech toward Asians effectively. In this work,\nwe first create COVID-HATE-2022, an annotated dataset that is an extension of\nthe anti-Asian hate speech dataset on Twitter, including 2,035 annotated tweets\nfetched in early February 2022, which are labeled based on specific criteria,\nand we present the comprehensive collection of scenarios of hate and non-hate\ntweets in the dataset. Second, we fine-tune the BERT models based on the\nrelevant datasets, and demonstrate strategies including 1) cleaning the\nhashtags, usernames being @, URLs, and emojis before the fine-tuning process,\nand 2) training with the data while validating with the \"clean\" data (and the\nopposite) are not effective for improving performance. Third, we investigate\nthe performance of advanced fine-tuning strategies with 1) model-centric\napproaches, such as discriminative fine-tuning, gradual unfreezing, and warmup\nsteps, and 2) data-centric approaches, which incorporate data trimming and data\naugmenting, and show that both strategies generally improve the performance,\nwhile data-centric ones outperform the others, which demonstrate the\nfeasibility and effectiveness of the data-centric approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lian_X/0/1/0/all/0/1\">Xin Lian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Multimodal Corpus for Emotion Recognition in Sarcasm. (arXiv:2206.02119v1 [cs.CL])","link":"http://arxiv.org/abs/2206.02119","description":"<p>While sentiment and emotion analysis have been studied extensively, the\nrelationship between sarcasm and emotion has largely remained unexplored. A\nsarcastic expression may have a variety of underlying emotions. For example, \"I\nlove being ignored\" belies sadness, while \"my mobile is fabulous with a battery\nbackup of only 15 minutes!\" expresses frustration. Detecting the emotion behind\na sarcastic expression is non-trivial yet an important task. We undertake the\ntask of detecting the emotion in a sarcastic statement, which to the best of\nour knowledge, is hitherto unexplored. We start with the recently released\nmultimodal sarcasm detection dataset (MUStARD) pre-annotated with 9 emotions.\nWe identify and correct 343 incorrect emotion labels (out of 690). We double\nthe size of the dataset, label it with emotions along with valence and arousal\nwhich are important indicators of emotional intensity. Finally, we label each\nsarcastic utterance with one of the four sarcasm types-Propositional, Embedded,\nLikeprefixed and Illocutionary, with the goal of advancing sarcasm detection\nresearch. Exhaustive experimentation with multimodal (text, audio, and video)\nfusion models establishes a benchmark for exact emotion recognition in sarcasm\nand outperforms the state-of-art sarcasm detection. We release the dataset\nenriched with various annotations and the code for research purposes:\nhttps://github.com/apoorva-nunna/MUStARD_Plus_Plus\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ray_A/0/1/0/all/0/1\">Anupama Ray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Shubham Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nunna_A/0/1/0/all/0/1\">Apoorva Nunna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharyya_P/0/1/0/all/0/1\">Pushpak Bhattacharyya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dict-TTS: Learning to Pronounce with Prior Dictionary Knowledge for Text-to-Speech. (arXiv:2206.02147v1 [eess.AS])","link":"http://arxiv.org/abs/2206.02147","description":"<p>Polyphone disambiguation aims to capture accurate pronunciation knowledge\nfrom natural text sequences for reliable Text-to-speech (TTS) systems. However,\nprevious approaches require substantial annotated training data and additional\nefforts from language experts, making it difficult to extend high-quality\nneural TTS systems to out-of-domain daily conversations and countless languages\nworldwide. This paper tackles the polyphone disambiguation problem from a\nconcise and novel perspective: we propose Dict-TTS, a semantic-aware generative\ntext-to-speech model with an online website dictionary (the existing prior\ninformation in the natural language). Specifically, we design a\nsemantics-to-pronunciation attention (S2PA) module to match the semantic\npatterns between the input text sequence and the prior semantics in the\ndictionary and obtain the corresponding pronunciations; The S2PA module can be\neasily trained with the end-to-end TTS model without any annotated phoneme\nlabels. Experimental results in three languages show that our model outperforms\nseveral strong baseline models in terms of pronunciation accuracy and improves\nthe prosody modeling of TTS systems. Further extensive analyses with different\nlinguistic encoders demonstrate that each design in Dict-TTS is effective.\nAudio samples are available at \\url{https://dicttts.github.io/DictTTS-Demo/}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Jiang_Z/0/1/0/all/0/1\">Ziyue Jiang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhe_S/0/1/0/all/0/1\">Su Zhe</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhou Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_Q/0/1/0/all/0/1\">Qian Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ren_Y/0/1/0/all/0/1\">Yi Ren</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jinglin Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ye_Z/0/1/0/all/0/1\">Zhenhui Ye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sentiment Analysis of Online Travel Reviews Based on Capsule Network and Sentiment Lexicon. (arXiv:2206.02160v1 [cs.CL])","link":"http://arxiv.org/abs/2206.02160","description":"<p>With the development of online travel services, it has great application\nprospects to timely mine users' evaluation emotions for travel services and use\nthem as indicators to guide the improvement of online travel service quality.\nIn this paper, we study the text sentiment classification of online travel\nreviews based on social media online comments and propose the SCCL model based\non capsule network and sentiment lexicon. SCCL model aims at the lack of\nconsideration of local features and emotional semantic features of the text in\nthe language model that can efficiently extract text context features like BERT\nand GRU. Then make the following improvements to their shortcomings. On the one\nhand, based on BERT-BiGRU, the capsule network is introduced to extract local\nfeatures while retaining good context features. On the other hand, the\nsentiment lexicon is introduced to extract the emotional sequence of the text\nto provide richer emotional semantic features for the model. To enhance the\nuniversality of the sentiment lexicon, the improved SO-PMI algorithm based on\nTF-IDF is used to expand the lexicon, so that the lexicon can also perform well\nin the field of online travel reviews.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jia Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1\">Junping Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1\">Yingxia Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1\">Ang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Near-Term Advances in Quantum Natural Language Processing. (arXiv:2206.02171v1 [cs.CL])","link":"http://arxiv.org/abs/2206.02171","description":"<p>This paper describes experiments showing that some problems in natural\nlanguage processing can already be addressed using quantum computers. The\nexamples presented here include topic classification using both a quantum\nsupport vector machine and a bag-of-words approach, bigram modeling that can be\napplied to sequences of words and formal concepts, and ambiguity resolution in\nverb-noun composition.\n</p>\n<p>While the datasets used are still small, the systems described have been run\non physical quantum computers. These implementations and their results are\ndescribed along with the algorithms and mathematical approaches used.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Widdows_D/0/1/0/all/0/1\">Dominic Widdows</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_D/0/1/0/all/0/1\">Daiwei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zimmerman_C/0/1/0/all/0/1\">Chase Zimmerman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Simple Meta-learning Paradigm for Zero-shot Intent Classification with Mixture Attention Mechanism. (arXiv:2206.02179v1 [cs.CL])","link":"http://arxiv.org/abs/2206.02179","description":"<p>Zero-shot intent classification is a vital and challenging task in dialogue\nsystems, which aims to deal with numerous fast-emerging unacquainted intents\nwithout annotated training data. To obtain more satisfactory performance, the\ncrucial points lie in two aspects: extracting better utterance features and\nstrengthening the model generalization ability. In this paper, we propose a\nsimple yet effective meta-learning paradigm for zero-shot intent\nclassification. To learn better semantic representations for utterances, we\nintroduce a new mixture attention mechanism, which encodes the pertinent word\noccurrence patterns by leveraging the distributional signature attention and\nmulti-layer perceptron attention simultaneously. To strengthen the transfer\nability of the model from seen classes to unseen classes, we reformulate\nzero-shot intent classification with a meta-learning strategy, which trains the\nmodel by simulating multiple zero-shot classification tasks on seen categories,\nand promotes the model generalization ability with a meta-adapting procedure on\nmimic unseen categories. Extensive experiments on two real-world dialogue\ndatasets in different languages show that our model outperforms other strong\nbaselines on both standard and generalized zero-shot intent classification\ntasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Han Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Siyang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaotong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Feng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Junjie Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xianchao Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Performance Comparison of Simple Transformer and Res-CNN-BiLSTM for Cyberbullying Classification. (arXiv:2206.02206v1 [cs.CL])","link":"http://arxiv.org/abs/2206.02206","description":"<p>The task of text classification using Bidirectional based LSTM architectures\nis computationally expensive and time consuming to train. For this,\ntransformers were discovered which effectively give good performance as\ncompared to the traditional deep learning architectures. In this paper we\npresent a performance based comparison between simple transformer based network\nand Res-CNN-BiLSTM based network for cyberbullying text classification problem.\nThe results obtained show that transformer we trained with 0.65 million\nparameters has significantly being able to beat the performance of\nRes-CNN-BiLSTM with 48.82 million parameters for faster training speeds and\nmore generalized metrics. The paper also compares the 1-dimensional character\nlevel embedding network and 100-dimensional glove embedding network with\ntransformer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Joshi_R/0/1/0/all/0/1\">Raunak Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Abhishek Gupta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stylistic Fingerprints, POS-tags and Inflected Languages: A Case Study in Polish. (arXiv:2206.02208v1 [cs.CL])","link":"http://arxiv.org/abs/2206.02208","description":"<p>In stylometric investigations, frequencies of the most frequent words (MFWs)\nand character n-grams outperform other style-markers, even if their performance\nvaries significantly across languages. In inflected languages, word endings\nplay a prominent role, and hence different word forms cannot be recognized\nusing generic text tokenization. Countless inflected word forms make\nfrequencies sparse, making most statistical procedures complicated. Presumably,\napplying one of the NLP techniques, such as lemmatization and/or parsing, might\nincrease the performance of classification. The aim of this paper is to examine\nthe usefulness of grammatical features (as assessed via POS-tag n-grams) and\nlemmatized forms in recognizing authorial profiles, in order to address the\nunderlying issue of the degree of freedom of choice within lexis and grammar.\nUsing a corpus of Polish novels, we performed a series of supervised authorship\nattribution benchmarks, in order to compare the classification accuracy for\ndifferent types of lexical and syntactic style-markers. Even if the performance\nof POS-tags as well as lemmatized forms was notoriously worse than that of\nlexical markers, the difference was not substantial and never exceeded ca. 15%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Eder_M/0/1/0/all/0/1\">Maciej Eder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gorski_R/0/1/0/all/0/1\">Rafa&#x142;. L. G&#xf3;rski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Variable-rate hierarchical CPC leads to acoustic unit discovery in speech. (arXiv:2206.02211v1 [cs.SD])","link":"http://arxiv.org/abs/2206.02211","description":"<p>The success of deep learning comes from its ability to capture the\nhierarchical structure of data by learning high-level representations defined\nin terms of low-level ones. In this paper we explore self-supervised learning\nof hierarchical representations of speech by applying multiple levels of\nContrastive Predictive Coding (CPC). We observe that simply stacking two CPC\nmodels does not yield significant improvements over single-level architectures.\nInspired by the fact that speech is often described as a sequence of discrete\nunits unevenly distributed in time, we propose a model in which the output of a\nlow-level CPC module is non-uniformly downsampled to directly minimize the loss\nof a high-level CPC module. The latter is designed to also enforce a prior of\nseparability and discreteness in its representations by enforcing dissimilarity\nof successive high-level representations through focused negative sampling, and\nby quantization of the prediction targets. Accounting for the structure of the\nspeech signal improves upon single-level CPC features and enhances the\ndisentanglement of the learned representations, as measured by downstream\nspeech recognition tasks, while resulting in a meaningful segmentation of the\nsignal that closely resembles phone boundaries.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cuervo_S/0/1/0/all/0/1\">Santiago Cuervo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lancucki_A/0/1/0/all/0/1\">Adrian &#x141;a&#x144;cucki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marxer_R/0/1/0/all/0/1\">Ricard Marxer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rychlikowski_P/0/1/0/all/0/1\">Pawe&#x142; Rychlikowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chorowski_J/0/1/0/all/0/1\">Jan Chorowski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Finetuning a Kalaallisut-English machine translation system using web-crawled data. (arXiv:2206.02230v1 [cs.CL])","link":"http://arxiv.org/abs/2206.02230","description":"<p>West Greenlandic, known by native speakers as Kalaallisut, is an extremely\nlow-resource polysynthetic language spoken by around 56,000 people in\nGreenland. Here, we attempt to finetune a pretrained Kalaallisut-to-English\nneural machine translation (NMT) system using web-crawled pseudoparallel\nsentences from around 30 multilingual websites. We compile a corpus of over\n93,000 Kalaallisut sentences and over 140,000 Danish sentences, then use\ncross-lingual sentence embeddings and approximate nearest-neighbors search in\nan attempt to mine near-translations from these corpora. Finally, we translate\nthe Danish sentence to English to obtain a synthetic Kalaallisut-English\naligned corpus. Although the resulting dataset is too small and noisy to\nimprove the pretrained MT model, we believe that with additional resources, we\ncould construct a better pseudoparallel corpus and achieve more promising\nresults on MT. We also note other possible uses of the monolingual Kalaallisut\ndata and discuss directions for future work. We make the code and data for our\nexperiments publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jones_A/0/1/0/all/0/1\">Alex Jones</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Cross-lingual Textual Style Transfer with Large Multilingual Language Models. (arXiv:2206.02252v1 [cs.CL])","link":"http://arxiv.org/abs/2206.02252","description":"<p>Detoxification is a task of generating text in polite style while preserving\nmeaning and fluency of the original toxic text. Existing detoxification methods\nare designed to work in one exact language. This work investigates multilingual\nand cross-lingual detoxification and the behavior of large multilingual models\nlike in this setting. Unlike previous works we aim to make large language\nmodels able to perform detoxification without direct fine-tuning in given\nlanguage. Experiments show that multilingual models are capable of performing\nmultilingual style transfer. However, models are not able to perform\ncross-lingual detoxification and direct fine-tuning on exact language is\ninevitable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moskovskiy_D/0/1/0/all/0/1\">Daniil Moskovskiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dementieva_D/0/1/0/all/0/1\">Daryna Dementieva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panchenko_A/0/1/0/all/0/1\">Alexander Panchenko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Annotation Error Detection: Analyzing the Past and Present for a More Coherent Future. (arXiv:2206.02280v1 [cs.CL])","link":"http://arxiv.org/abs/2206.02280","description":"<p>Annotated data is an essential ingredient in natural language processing for\ntraining and evaluating machine learning models. It is therefore very desirable\nfor the annotations to be of high quality. Recent work, however, has shown that\nseveral popular datasets contain a surprising amount of annotation errors or\ninconsistencies. To alleviate this issue, many methods for annotation error\ndetection have been devised over the years. While researchers show that their\napproaches work well on their newly introduced datasets, they rarely compare\ntheir methods to previous work or on the same datasets. This raises strong\nconcerns on methods' general performance and makes it difficult to asses their\nstrengths and weaknesses. We therefore reimplement 18 methods for detecting\npotential annotation errors and evaluate them on 9 English datasets for text\nclassification as well as token and span labeling. In addition, we define a\nuniform evaluation setup including a new formalization of the annotation error\ndetection task, evaluation protocol and general best practices. To facilitate\nfuture research and reproducibility, we release our datasets and\nimplementations in an easy-to-use and open source software package.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Klie_J/0/1/0/all/0/1\">Jan-Christoph Klie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Webber_B/0/1/0/all/0/1\">Bonnie Webber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pretrained Models for Multilingual Federated Learning. (arXiv:2206.02291v1 [cs.CL])","link":"http://arxiv.org/abs/2206.02291","description":"<p>Since the advent of Federated Learning (FL), research has applied these\nmethods to natural language processing (NLP) tasks. Despite a plethora of\npapers in FL for NLP, no previous works have studied how multilingual text\nimpacts FL algorithms. Furthermore, multilingual text provides an interesting\navenue to examine the impact of non-IID text (e.g. different languages) on FL\nin naturally occurring data. We explore three multilingual language tasks,\nlanguage modeling, machine translation, and text classification using differing\nfederated and non-federated learning algorithms. Our results show that using\npretrained models reduces the negative effects of FL, helping them to perform\nnear or better than centralized (no privacy) learning, even when using non-IID\npartitioning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Weller_O/0/1/0/all/0/1\">Orion Weller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marone_M/0/1/0/all/0/1\">Marc Marone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Braverman_V/0/1/0/all/0/1\">Vladimir Braverman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lawrie_D/0/1/0/all/0/1\">Dawn Lawrie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durme_B/0/1/0/all/0/1\">Benjamin Van Durme</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Advance of Making Language Models Better Reasoners. (arXiv:2206.02336v1 [cs.CL])","link":"http://arxiv.org/abs/2206.02336","description":"<p>Large language models such as GPT-3 and PaLM have shown remarkable\nperformance in few-shot learning. However, they still struggle with reasoning\ntasks such as the arithmetic benchmark GSM8K. Recent advances deliberately\nguide the language model to generate a chain of reasoning steps before\nproducing the final answer, successfully boosting the GSM8K benchmark from\n17.9% to 58.1% in terms of problem solving rate. In this paper, we propose a\nnew approach, DiVeRSe (Diverse Verifier on Reasoning Step), to further advance\ntheir reasoning capability. DiVeRSe first explores different prompts to enhance\nthe diversity in reasoning paths. Second, DiVeRSe introduces a verifier to\ndistinguish good answers from bad answers for a better weighted voting.\nFinally, DiVeRSe verifies the correctness of each single step rather than all\nthe steps in a whole. We conduct extensive experiments using the latest\nlanguage model code-davinci-002 and demonstrate that DiVeRSe can achieve new\nstate-of-the-art performance on six out of eight reasoning benchmarks (e.g.,\nGSM8K 74.4% to 83.2%), outperforming the PaLM model with 540B parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yifei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zeqi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shizhuo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Q/0/1/0/all/0/1\">Qiang Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1\">Jian-Guang Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Automated Fact-Checking. (arXiv:2108.11896v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.11896","description":"<p>Fact-checking has become increasingly important due to the speed with which\nboth information and misinformation can spread in the modern media ecosystem.\nTherefore, researchers have been exploring how fact-checking can be automated,\nusing techniques based on natural language processing, machine learning,\nknowledge representation, and databases to automatically predict the veracity\nof claims. In this paper, we survey automated fact-checking stemming from\nnatural language processing, and discuss its connections to related tasks and\ndisciplines. In this process, we present an overview of existing datasets and\nmodels, aiming to unify the various definitions given and identify common\nconcepts. Finally, we highlight challenges for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zhijiang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schlichtkrull_M/0/1/0/all/0/1\">Michael Schlichtkrull</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vlachos_A/0/1/0/all/0/1\">Andreas Vlachos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"N24News: A New Dataset for Multimodal News Classification. (arXiv:2108.13327v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.13327","description":"<p>Current news datasets merely focus on text features on the news and rarely\nleverage the feature of images, excluding numerous essential features for news\nclassification. In this paper, we propose a new dataset, N24News, which is\ngenerated from New York Times with 24 categories and contains both text and\nimage information in each news. We use a multitask multimodal method and the\nexperimental results show multimodal news classification performs better than\ntext-only news classification. Depending on the length of the text, the\nclassification accuracy can be increased by up to 8.11%. Our research reveals\nthe relationship between the performance of a multimodal classifier and its\nsub-classifiers, and also the possible improvements when applying multimodal in\nnews classification. N24News is shown to have great potential to prompt the\nmultimodal news studies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_X/0/1/0/all/0/1\">Xu Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangxie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jie Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text analysis and deep learning: A network approach. (arXiv:2110.04151v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.04151","description":"<p>Much information available to applied researchers is contained within written\nlanguage or spoken text. Deep language models such as BERT have achieved\nunprecedented success in many applications of computational linguistics.\nHowever, much less is known about how these models can be used to analyze\nexisting text. We propose a novel method that combines transformer models with\nnetwork analysis to form a self-referential representation of language use\nwithin a corpus of interest. Our approach produces linguistic relations\nstrongly consistent with the underlying model as well as mathematically\nwell-defined operations on them, while reducing the amount of discretionary\nchoices of representation and distance measures. It represents, to the best of\nour knowledge, the first unsupervised method to extract semantic networks\ndirectly from deep language models. We illustrate our approach in a semantic\nanalysis of the term \"founder\". Using the entire corpus of Harvard Business\nReview from 1980 to 2020, we find that ties in our network track the semantics\nof discourse over time, and across contexts, identifying and relating clusters\nof semantic and syntactic relations. Finally, we discuss how this method can\nalso complement and inform analyses of the behavior of deep learning models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Marquart_I/0/1/0/all/0/1\">Ingo Marquart</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KG-FiD: Infusing Knowledge Graph in Fusion-in-Decoder for Open-Domain Question Answering. (arXiv:2110.04330v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.04330","description":"<p>Current Open-Domain Question Answering (ODQA) model paradigm often contains a\nretrieving module and a reading module. Given an input question, the reading\nmodule predicts the answer from the relevant passages which are retrieved by\nthe retriever. The recent proposed Fusion-in-Decoder (FiD), which is built on\ntop of the pretrained generative model T5, achieves the state-of-the-art\nperformance in the reading module. Although being effective, it remains\nconstrained by inefficient attention on all retrieved passages which contain a\nlot of noise. In this work, we propose a novel method KG-FiD, which filters\nnoisy passages by leveraging the structural relationship among the retrieved\npassages with a knowledge graph. We initiate the passage node embedding from\nthe FiD encoder and then use graph neural network (GNN) to update the\nrepresentation for reranking. To improve the efficiency, we build the GNN on\ntop of the intermediate layer output of the FiD encoder and only pass a few top\nreranked passages into the higher layers of encoder and decoder for answer\ngeneration. We also apply the proposed GNN based reranking method to enhance\nthe passage retrieval results in the retrieving module. Extensive experiments\non common ODQA benchmark datasets (Natural Question and TriviaQA) demonstrate\nthat KG-FiD can improve vanilla FiD by up to 1.5% on answer exact match score\nand achieve comparable performance with FiD with only 40% of computation cost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Donghan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yuwei Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Wenhao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuohang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yichong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yiming Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1\">Michael Zeng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adapting to the Long Tail: A Meta-Analysis of Transfer Learning Research for Language Understanding Tasks. (arXiv:2111.01340v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.01340","description":"<p>Natural language understanding (NLU) has made massive progress driven by\nlarge benchmarks, but benchmarks often leave a long tail of infrequent\nphenomena underrepresented. We reflect on the question: have transfer learning\nmethods sufficiently addressed the poor performance of benchmark-trained models\non the long tail? We conceptualize the long tail using macro-level dimensions\n(e.g., underrepresented genres, topics, etc.), and perform a qualitative\nmeta-analysis of 100 representative papers on transfer learning research for\nNLU. Our analysis asks three questions: (i) Which long tail dimensions do\ntransfer learning studies target? (ii) Which properties of adaptation methods\nhelp improve performance on the long tail? (iii) Which methodological gaps have\ngreatest negative impact on long tail performance? Our answers highlight major\navenues for future research in transfer learning for the long tail. Lastly,\nusing our meta-analysis framework, we perform a case study comparing the\nperformance of various adaptation methods on clinical narratives, which\nprovides interesting insights that may enable us to make progress along these\nfuture avenues.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Naik_A/0/1/0/all/0/1\">Aakanksha Naik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lehman_J/0/1/0/all/0/1\">Jill Lehman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rose_C/0/1/0/all/0/1\">Carolyn Rose</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deciphering Speech: a Zero-Resource Approach to Cross-Lingual Transfer in ASR. (arXiv:2111.06799v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.06799","description":"<p>We present a method for cross-lingual training an ASR system using absolutely\nno transcribed training data from the target language, and with no phonetic\nknowledge of the language in question. Our approach uses a novel application of\na decipherment algorithm, which operates given only unpaired speech and text\ndata from the target language. We apply this decipherment to phone sequences\ngenerated by a universal phone recogniser trained on out-of-language speech\ncorpora, which we follow with flat-start semi-supervised training to obtain an\nacoustic model for the new language. To the best of our knowledge, this is the\nfirst practical approach to zero-resource cross-lingual ASR which does not rely\non any hand-crafted phonetic information. We carry out experiments on read\nspeech from the GlobalPhone corpus, and show that it is possible to learn a\ndecipherment model on just 20 minutes of data from the target language. When\nused to generate pseudo-labels for semi-supervised training, we obtain WERs\nthat range from 32.5% to just 1.9% absolute worse than the equivalent fully\nsupervised models trained on the same data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Klejch_O/0/1/0/all/0/1\">Ondrej Klejch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wallington_E/0/1/0/all/0/1\">Electra Wallington</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bell_P/0/1/0/all/0/1\">Peter Bell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Temporal Effects on Pre-trained Models for Language Processing Tasks. (arXiv:2111.12790v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.12790","description":"<p>Keeping the performance of language technologies optimal as time passes is of\ngreat practical interest. We study temporal effects on model performance on\ndownstream language tasks, establishing a nuanced terminology for such\ndiscussion and identifying factors essential to conduct a robust study. We\npresent experiments for several tasks in English where the label correctness is\nnot dependent on time and demonstrate the importance of distinguishing between\ntemporal model deterioration and temporal domain adaptation for systems using\npre-trained representations. We find that depending on the task, temporal model\ndeterioration is not necessarily a concern. Temporal domain adaptation however\nis beneficial in all cases, with better performance for a given time period\npossible when the system is trained on temporally more recent data. Therefore,\nwe also examine the efficacy of two approaches for temporal domain adaptation\nwithout human annotations on new data. Self-labeling shows consistent\nimprovement and notably, for named entity recognition, leads to better temporal\nadaptation than even human annotations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_O/0/1/0/all/0/1\">Oshin Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nenkova_A/0/1/0/all/0/1\">Ani Nenkova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Causal Distillation for Language Models. (arXiv:2112.02505v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.02505","description":"<p>Distillation efforts have led to language models that are more compact and\nefficient without serious drops in performance. The standard approach to\ndistillation trains a student model against two objectives: a task-specific\nobjective (e.g., language modeling) and an imitation objective that encourages\nthe hidden states of the student model to be similar to those of the larger\nteacher model. In this paper, we show that it is beneficial to augment\ndistillation with a third objective that encourages the student to imitate the\ncausal computation process of the teacher through interchange intervention\ntraining(IIT). IIT pushes the student model to become a causal abstraction of\nthe teacher model - a simpler model with the same causal structure. IIT is\nfully differentiable, easily implemented, and combines flexibly with other\nobjectives. Compared with standard distillation of BERT, distillation via IIT\nresults in lower perplexity on Wikipedia (masked language modeling) and marked\nimprovements on the GLUE benchmark (natural language understanding), SQuAD\n(question answering), and CoNLL-2003 (named entity recognition).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhengxuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geiger_A/0/1/0/all/0/1\">Atticus Geiger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rozner_J/0/1/0/all/0/1\">Josh Rozner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kreiss_E/0/1/0/all/0/1\">Elisa Kreiss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Hanson Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Icard_T/0/1/0/all/0/1\">Thomas Icard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potts_C/0/1/0/all/0/1\">Christopher Potts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goodman_N/0/1/0/all/0/1\">Noah D. Goodman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ZeroBERTo: Leveraging Zero-Shot Text Classification by Topic Modeling. (arXiv:2201.01337v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.01337","description":"<p>Traditional text classification approaches often require a good amount of\nlabeled data, which is difficult to obtain, especially in restricted domains or\nless widespread languages. This lack of labeled data has led to the rise of\nlow-resource methods, that assume low data availability in natural language\nprocessing. Among them, zero-shot learning stands out, which consists of\nlearning a classifier without any previously labeled data. The best results\nreported with this approach use language models such as Transformers, but fall\ninto two problems: high execution time and inability to handle long texts as\ninput. This paper proposes a new model, ZeroBERTo, which leverages an\nunsupervised clustering step to obtain a compressed data representation before\nthe classification task. We show that ZeroBERTo has better performance for long\ninputs and shorter execution time, outperforming XLM-R by about 12% in the F1\nscore in the FolhaUOL dataset. Keywords: Low-Resource NLP, Unlabeled data,\nZero-Shot Learning, Topic Modeling, Transformers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alcoforado_A/0/1/0/all/0/1\">Alexandre Alcoforado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferraz_T/0/1/0/all/0/1\">Thomas Palmeira Ferraz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gerber_R/0/1/0/all/0/1\">Rodrigo Gerber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bustos_E/0/1/0/all/0/1\">Enzo Bustos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_A/0/1/0/all/0/1\">Andr&#xe9; Seidel Oliveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Veloso_B/0/1/0/all/0/1\">Bruno Miguel Veloso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siqueira_F/0/1/0/all/0/1\">Fabio Levy Siqueira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Costa_A/0/1/0/all/0/1\">Anna Helena Reali Costa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain Representative Keywords Selection: A Probabilistic Approach. (arXiv:2203.10365v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.10365","description":"<p>We propose a probabilistic approach to select a subset of a \\textit{target\ndomain representative keywords} from a candidate set, contrasting with a\ncontext domain. Such a task is crucial for many downstream tasks in natural\nlanguage processing. To contrast the target domain and the context domain, we\nadapt the \\textit{two-component mixture model} concept to generate a\ndistribution of candidate keywords. It provides more importance to the\n\\textit{distinctive} keywords of the target domain than common keywords\ncontrasting with the context domain. To support the \\textit{representativeness}\nof the selected keywords towards the target domain, we introduce an\n\\textit{optimization algorithm} for selecting the subset from the generated\ncandidate distribution. We have shown that the optimization algorithm can be\nefficiently implemented with a near-optimal approximation guarantee. Finally,\nextensive experiments on multiple domains demonstrate the superiority of our\napproach over other baselines for the tasks of keyword summary generation and\ntrending keywords selection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Akash_P/0/1/0/all/0/1\">Pritom Saha Akash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kevin Chen-Chuan Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunyao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Popa_L/0/1/0/all/0/1\">Lucian Popa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_C/0/1/0/all/0/1\">ChengXiang Zhai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text Transformations in Contrastive Self-Supervised Learning: A Review. (arXiv:2203.12000v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.12000","description":"<p>Contrastive self-supervised learning has become a prominent technique in\nrepresentation learning. The main step in these methods is to contrast\nsemantically similar and dissimilar pairs of samples. However, in the domain of\nNatural Language Processing (NLP), the augmentation methods used in creating\nsimilar pairs with regard to contrastive learning (CL) assumptions are\nchallenging. This is because, even simply modifying a word in the input might\nchange the semantic meaning of the sentence, and hence, would violate the\ndistributional hypothesis. In this review paper, we formalize the contrastive\nlearning framework, emphasize the considerations that need to be addressed in\nthe data transformation step, and review the state-of-the-art methods and\nevaluations for contrastive representation learning in NLP. Finally, we\ndescribe some challenges and potential directions for learning better text\nrepresentations using contrastive methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharjee_A/0/1/0/all/0/1\">Amrita Bhattacharjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karami_M/0/1/0/all/0/1\">Mansooreh Karami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Huan Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions. (arXiv:2203.12667v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.12667","description":"<p>A long-term goal of AI research is to build intelligent agents that can\ncommunicate with humans in natural language, perceive the environment, and\nperform real-world tasks. Vision-and-Language Navigation (VLN) is a fundamental\nand interdisciplinary research topic towards this goal, and receives increasing\nattention from natural language processing, computer vision, robotics, and\nmachine learning communities. In this paper, we review contemporary studies in\nthe emerging field of VLN, covering tasks, evaluation metrics, methods, etc.\nThrough structured analysis of current progress and challenges, we highlight\nthe limitations of current VLN and opportunities for future work. This paper\nserves as a thorough reference for the VLN research community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jing Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stefani_E/0/1/0/all/0/1\">Eliana Stefani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomason_J/0/1/0/all/0/1\">Jesse Thomason</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Eric Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-label classification for biomedical literature: an overview of the BioCreative VII LitCovid Track for COVID-19 literature topic annotations. (arXiv:2204.09781v3 [cs.DL] UPDATED)","link":"http://arxiv.org/abs/2204.09781","description":"<p>The COVID-19 pandemic has been severely impacting global society since\nDecember 2019. Massive research has been undertaken to understand the\ncharacteristics of the virus and design vaccines and drugs. The related\nfindings have been reported in biomedical literature at a rate of about 10,000\narticles on COVID-19 per month. Such rapid growth significantly challenges\nmanual curation and interpretation. For instance, LitCovid is a literature\ndatabase of COVID-19-related articles in PubMed, which has accumulated more\nthan 200,000 articles with millions of accesses each month by users worldwide.\nOne primary curation task is to assign up to eight topics (e.g., Diagnosis and\nTreatment) to the articles in LitCovid. Despite the continuing advances in\nbiomedical text mining methods, few have been dedicated to topic annotations in\nCOVID-19 literature. To close the gap, we organized the BioCreative LitCovid\ntrack to call for a community effort to tackle automated topic annotation for\nCOVID-19 literature. The BioCreative LitCovid dataset, consisting of over\n30,000 articles with manually reviewed topics, was created for training and\ntesting. It is one of the largest multilabel classification datasets in\nbiomedical scientific literature. 19 teams worldwide participated and made 80\nsubmissions in total. Most teams used hybrid systems based on transformers. The\nhighest performing submissions achieved 0.8875, 0.9181, and 0.9394 for macro\nF1-score, micro F1-score, and instance-based F1-score, respectively. The level\nof participation and results demonstrate a successful track and help close the\ngap between dataset curation and method development. The dataset is publicly\navailable via https://ftp.ncbi.nlm.nih.gov/pub/lu/LitCovid/biocreative/ for\nbenchmarking and further development.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qingyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allot_A/0/1/0/all/0/1\">Alexis Allot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leaman_R/0/1/0/all/0/1\">Robert Leaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dogan_R/0/1/0/all/0/1\">Rezarta Islamaj Do&#x11f;an</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1\">Jingcheng Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_L/0/1/0/all/0/1\">Li Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shuo Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuefu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bagherzadeh_P/0/1/0/all/0/1\">Parsa Bagherzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bergler_S/0/1/0/all/0/1\">Sabine Bergler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatnagar_A/0/1/0/all/0/1\">Aakash Bhatnagar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhavsar_N/0/1/0/all/0/1\">Nidhir Bhavsar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yung-Chun Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Sheng-Jie Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_W/0/1/0/all/0/1\">Wentai Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongtong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tavchioski_I/0/1/0/all/0/1\">Ilija Tavchioski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pollak_S/0/1/0/all/0/1\">Senja Pollak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_S/0/1/0/all/0/1\">Shubo Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jinfeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Otmakhova_Y/0/1/0/all/0/1\">Yulia Otmakhova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yepes_A/0/1/0/all/0/1\">Antonio Jimeno Yepes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Hang Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Honghan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dufour_R/0/1/0/all/0/1\">Richard Dufour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labrak_Y/0/1/0/all/0/1\">Yanis Labrak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chatterjee_N/0/1/0/all/0/1\">Niladri Chatterjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tandon_K/0/1/0/all/0/1\">Kushagri Tandon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laleye_F/0/1/0/all/0/1\">Fr&#xe9;jus Laleye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rakotoson_L/0/1/0/all/0/1\">Lo&#xef;c Rakotoson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chersoni_E/0/1/0/all/0/1\">Emmanuele Chersoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jinghang Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Friedrich_A/0/1/0/all/0/1\">Annemarie Friedrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pujari_S/0/1/0/all/0/1\">Subhash Chandra Pujari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chizhikova_M/0/1/0/all/0/1\">Mariia Chizhikova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sivadasan_N/0/1/0/all/0/1\">Naveen Sivadasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sivadasan_N/0/1/0/all/0/1\">Naveen Sivadasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhiyong Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Long Document Re-ranking with Modular Re-ranker. (arXiv:2205.04275v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2205.04275","description":"<p>Long document re-ranking has been a challenging problem for neural re-rankers\nbased on deep language models like BERT. Early work breaks the documents into\nshort passage-like chunks. These chunks are independently mapped to scalar\nscores or latent vectors, which are then pooled into a final relevance score.\nThese encode-and-pool methods however inevitably introduce an information\nbottleneck: the low dimension representations. In this paper, we propose\ninstead to model full query-to-document interaction, leveraging the attention\noperation and modular Transformer re-ranker framework. First, document chunks\nare encoded independently with an encoder module. An interaction module then\nencodes the query and performs joint attention from the query to all document\nchunk representations. We demonstrate that the model can use this new degree of\nfreedom to aggregate important information from the entire document. Our\nexperiments show that this design produces effective re-ranking on two\nclassical IR collections Robust04 and ClueWeb09, and a large-scale supervised\ncollection MS-MARCO document ranking.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Luyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callan_J/0/1/0/all/0/1\">Jamie Callan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ClusterEA: Scalable Entity Alignment with Stochastic Training and Normalized Mini-batch Similarities. (arXiv:2205.10312v2 [cs.DB] UPDATED)","link":"http://arxiv.org/abs/2205.10312","description":"<p>Entity alignment (EA) aims at finding equivalent entities in different\nknowledge graphs (KGs). Embedding-based approaches have dominated the EA task\nin recent years. Those methods face problems that come from the geometric\nproperties of embedding vectors, including hubness and isolation. To solve\nthese geometric problems, many normalization approaches have been adopted for\nEA. However, the increasing scale of KGs renders it hard for EA models to adopt\nthe normalization processes, thus limiting their usage in real-world\napplications. To tackle this challenge, we present ClusterEA, a general\nframework that is capable of scaling up EA models and enhancing their results\nby leveraging normalization methods on mini-batches with a high entity\nequivalent rate. ClusterEA contains three components to align entities between\nlarge-scale KGs, including stochastic training, ClusterSampler, and\nSparseFusion. It first trains a large-scale Siamese GNN for EA in a stochastic\nfashion to produce entity embeddings. Based on the embeddings, a novel\nClusterSampler strategy is proposed for sampling highly overlapped\nmini-batches. Finally, ClusterEA incorporates SparseFusion, which normalizes\nlocal and global similarity and then fuses all similarity matrices to obtain\nthe final similarity matrix. Extensive experiments with real-life datasets on\nEA benchmarks offer insight into the proposed framework, and suggest that it is\ncapable of outperforming the state-of-the-art scalable EA framework by up to 8\ntimes in terms of Hits@1.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yunjun Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaoze Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Junyang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tianyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pengfei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transcormer: Transformer for Sentence Scoring with Sliding Language Modeling. (arXiv:2205.12986v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12986","description":"<p>Sentence scoring aims at measuring the likelihood score of a sentence and is\nwidely used in many natural language processing scenarios, like reranking,\nwhich is to select the best sentence from multiple candidates. Previous works\non sentence scoring mainly adopted either causal language modeling (CLM) like\nGPT or masked language modeling (MLM) like BERT, which have some limitations:\n1) CLM only utilizes unidirectional information for the probability estimation\nof a sentence without considering bidirectional context, which affects the\nscoring quality; 2) MLM can only estimate the probability of partial tokens at\na time and thus requires multiple forward passes to estimate the probability of\nthe whole sentence, which incurs large computation and time cost. In this\npaper, we propose \\textit{Transcormer} -- a Transformer model with a novel\n\\textit{sliding language modeling} (SLM) for sentence scoring. Specifically,\nour SLM adopts a triple-stream self-attention mechanism to estimate the\nprobability of all tokens in a sentence with bidirectional context and only\nrequires a single forward pass. SLM can avoid the limitations of CLM (only\nunidirectional context) and MLM (multiple forward passes) and inherit their\nadvantages, and thus achieve high effectiveness and efficiency in scoring.\nExperimental results on multiple tasks demonstrate that our method achieves\nbetter performance than other language modelings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1\">Kaitao Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leng_Y/0/1/0/all/0/1\">Yichong Leng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xu Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yicheng Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dongsheng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Causal Inference for Explainable Automatic Program Repair. (arXiv:2205.13342v2 [cs.SE] UPDATED)","link":"http://arxiv.org/abs/2205.13342","description":"<p>Deep learning models have made significant progress in automatic program\nrepair. However, the black-box nature of these methods has restricted their\npractical applications. To address this challenge, this paper presents an\ninterpretable approach for program repair based on sequence-to-sequence models\nwith causal inference and our method is called CPR, short for causal program\nrepair. Our CPR can generate explanations in the process of decision making,\nwhich consists of groups of causally related input-output tokens. Firstly, our\nmethod infers these relations by querying the model with inputs disturbed by\ndata augmentation. Secondly, it generates a graph over tokens from the\nresponses and solves a partitioning problem to select the most relevant\ncomponents. The experiments on four programming languages (Java, C, Python, and\nJavaScript) show that CPR can generate causal graphs for reasonable\ninterpretations and boost the performance of bug fixing in automatic program\nrepair.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianzong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_S/0/1/0/all/0/1\">Shijing Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhitao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_X/0/1/0/all/0/1\">Xiaoyang Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_Z/0/1/0/all/0/1\">Zhenhou Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jing Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transfer Language Selection for Zero-Shot Cross-Lingual Abusive Language Detection. (arXiv:2206.00962v1 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2206.00962","description":"<p>We study the selection of transfer languages for automatic abusive language\ndetection. Instead of preparing a dataset for every language, we demonstrate\nthe effectiveness of cross-lingual transfer learning for zero-shot abusive\nlanguage detection. This way we can use existing data from higher-resource\nlanguages to build better detection systems for low-resource languages. Our\ndatasets are from seven different languages from three language families. We\nmeasure the distance between the languages using several language similarity\nmeasures, especially by quantifying the World Atlas of Language Structures. We\nshow that there is a correlation between linguistic similarity and classifier\nperformance. This discovery allows us to choose an optimal transfer language\nfor zero shot abusive language detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Eronen_J/0/1/0/all/0/1\">Juuso Eronen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ptaszynski_M/0/1/0/all/0/1\">Michal Ptaszynski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masui_F/0/1/0/all/0/1\">Fumito Masui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arata_M/0/1/0/all/0/1\">Masaki Arata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leliwa_G/0/1/0/all/0/1\">Gniewosz Leliwa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wroczynski_M/0/1/0/all/0/1\">Michal Wroczynski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-06-06T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"A review of machine learning approaches, challenges and prospects for computational tumor pathology. (arXiv:2206.01728v1 [eess.IV])","link":"http://arxiv.org/abs/2206.01728","description":"<p>Computational pathology is part of precision oncology medicine. The\nintegration of high-throughput data including genomics, transcriptomics,\nproteomics, metabolomics, pathomics, and radiomics into clinical practice\nimproves cancer treatment plans, treatment cycles, and cure rates, and helps\ndoctors open up innovative approaches to patient prognosis. In the past decade,\nrapid advances in artificial intelligence, chip design and manufacturing, and\nmobile computing have facilitated research in computational pathology and have\nthe potential to provide better-integrated solutions for whole-slide images,\nmulti-omics data, and clinical informatics. However, tumor computational\npathology now brings some challenges to the application of tumour screening,\ndiagnosis and prognosis in terms of data integration, hardware processing,\nnetwork sharing bandwidth and machine learning technology. This review\ninvestigates image preprocessing methods in computational pathology from a\npathological and technical perspective, machine learning-based methods, and\napplications of computational pathology in breast, colon, prostate, lung, and\nvarious tumour disease scenarios. Finally, the challenges and prospects of\nmachine learning in computational pathology applications are discussed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Pan_L/0/1/0/all/0/1\">Liangrui Pan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Feng_Z/0/1/0/all/0/1\">Zhichao Feng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Peng_S/0/1/0/all/0/1\">Shaoliang Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Empirical Study of Quality Image Assessment for Synthesis of Fetal Head Ultrasound Imaging with DCGANs. (arXiv:2206.01731v1 [eess.IV])","link":"http://arxiv.org/abs/2206.01731","description":"<p>In this work, we present an empirical study of DCGANs for synthetic\ngeneration of fetal head ultrasound, consisting of hyperparameter heuristics\nand image quality assessment. We present experiments to show the impact of\ndifferent image sizes, epochs, data size input, and learning rates for quality\nimage assessment on four metrics: mutual information (MI), fr\\'echet inception\ndistance (FID), peak-signal-to-noise ratio (PSNR), and local binary pattern\nvector (LBPv). The results show that FID and LBPv have stronger relationship\nwith clinical image quality scores. The resources to reproduce this work are\navailable at \\url{https://github.com/xfetus/miua2022}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bautista_T/0/1/0/all/0/1\">Thea Bautista</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Matthew_J/0/1/0/all/0/1\">Jacqueline Matthew</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kerdegari_H/0/1/0/all/0/1\">Hamideh Kerdegari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pereira_L/0/1/0/all/0/1\">Laura Peralta Pereira</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xochicale_M/0/1/0/all/0/1\">Miguel Xochicale</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial RAW: Image-Scaling Attack Against Imaging Pipeline. (arXiv:2206.01733v1 [cs.CV])","link":"http://arxiv.org/abs/2206.01733","description":"<p>Deep learning technologies have become the backbone for the development of\ncomputer vision. With further explorations, deep neural networks have been\nfound vulnerable to well-designed adversarial attacks. Most of the vision\ndevices are equipped with image signal processing (ISP) pipeline to implement\nRAW-to-RGB transformations and embedded into data preprocessing module for\nefficient image processing. Actually, ISP pipeline can introduce adversarial\nbehaviors to post-capture images while data preprocessing may destroy attack\npatterns. However, none of the existing adversarial attacks takes into account\nthe impacts of both ISP pipeline and data preprocessing. In this paper, we\ndevelop an image-scaling attack targeting on ISP pipeline, where the crafted\nadversarial RAW can be transformed into attack image that presents entirely\ndifferent appearance once being scaled to a specific-size image. We first\nconsider the gradient-available ISP pipeline, i.e., the gradient information\ncan be directly used in the generation process of adversarial RAW to launch the\nattack. To make the adversarial attack more applicable, we further consider the\ngradient-unavailable ISP pipeline, in which a proxy model that well learns the\nRAW-to-RGB transformations is proposed as the gradient oracles. Extensive\nexperiments show that the proposed adversarial attacks can craft adversarial\nRAW data against the target ISP pipelines with high attack rates.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junjian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Honglong Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using UAS Imagery and Computer Vision to Support Site-Specific Weed Control in Corn. (arXiv:2206.01734v1 [cs.CV])","link":"http://arxiv.org/abs/2206.01734","description":"<p>Currently, weed control in a corn field is performed by a blanket application\nof herbicides that do not consider spatial distribution information of weeds\nand also uses an extensive amount of chemical herbicides. To reduce the amount\nof chemicals, we used drone-based high-resolution imagery and computer-vision\ntechniques to perform site-specific weed control in corn.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sapkota_R/0/1/0/all/0/1\">Ranjan Sapkota</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flores_P/0/1/0/all/0/1\">Paulo Flores</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Examining the behaviour of state-of-the-art convolutional neural networks for brain tumor detection with and without transfer learning. (arXiv:2206.01735v1 [eess.IV])","link":"http://arxiv.org/abs/2206.01735","description":"<p>Distinguishing normal from malignant and determining the tumor type are\ncritical components of brain tumor diagnosis. Two different kinds of dataset\nare investigated using state-of-the-art CNN models in this research work. One\ndataset(binary) has images of normal and tumor types, while\nanother(multi-class) provides all images of tumors classified as glioma,\nmeningioma, or pituitary. The experiments were conducted in these dataset with\ntransfer learning from pre-trained weights from ImageNet as well as\ninitializing the weights randomly. The experimental environment is equivalent\nfor all models in this study in order to make a fair comparison. For both of\nthe dataset, the validation set are same for all the models where train data is\n60% while the rest is 40% for validation. With the proposed techniques in this\nresearch, the EfficientNet-B5 architecture outperforms all the state-of-the-art\nmodels in the binary-classification dataset with the accuracy of 99.75% and\n98.61% accuracy for the multi-class dataset. This research also demonstrates\nthe behaviour of convergence of validation loss in different weight\ninitialization techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ahamed_M/0/1/0/all/0/1\">Md. Atik Ahamed</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sadia_R/0/1/0/all/0/1\">Rabeya Tus Sadia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Adversarial Training to Improve Adversarial Robustness of DNNs for Medical Image Segmentation and Detection. (arXiv:2206.01736v1 [eess.IV])","link":"http://arxiv.org/abs/2206.01736","description":"<p>Recent methods based on Deep Neural Networks (DNNs) have reached high\naccuracy for medical image analysis, including the three basic tasks:\nsegmentation, landmark detection, and object detection. It is known that DNNs\nare vulnerable to adversarial attacks, and the adversarial robustness of DNNs\ncould be improved by adding adversarial noises to training data (i.e.,\nadversarial training). In this study, we show that the standard adversarial\ntraining (SAT) method has a severe issue that limits its practical use: it\ngenerates a fixed level of noise for DNN training, and it is difficult for the\nuser to choose an appropriate noise level, because a high noise level may lead\nto a large reduction in model performance, and a low noise level may have\nlittle effect. To resolve this issue, we have designed a novel adaptive-margin\nadversarial training (AMAT) method that generates adaptive adversarial noises\nfor DNN training, which are dynamically tailored for each individual training\nsample. We have applied our AMAT method to state-of-the-art DNNs for the three\nbasic tasks, using five publicly available datasets. The experimental results\ndemonstrate that our AMAT method outperforms the SAT method in adversarial\nrobustness on noisy data and prediction accuracy on clean data. Please contact\nthe author for the source code.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ma_L/0/1/0/all/0/1\">Linhai Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liang_L/0/1/0/all/0/1\">Liang Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MaxStyle: Adversarial Style Composition for Robust Medical Image Segmentation. (arXiv:2206.01737v1 [eess.IV])","link":"http://arxiv.org/abs/2206.01737","description":"<p>Convolutional neural networks (CNNs) have achieved remarkable segmentation\naccuracy on benchmark datasets where training and test sets are from the same\ndomain, yet their performance can degrade significantly on unseen domains,\nwhich hinders the deployment of CNNs in many clinical scenarios. Most existing\nworks improve model out-of-domain (OOD) robustness by collecting multi-domain\ndatasets for training, which is expensive and may not always be feasible due to\nprivacy and logistical issues. In this work, we focus on improving model\nrobustness using a single-domain dataset only. We propose a novel data\naugmentation framework called MaxStyle, which maximizes the effectiveness of\nstyle augmentation for model OOD performance. It attaches an auxiliary\nstyle-augmented image decoder to a segmentation network for robust feature\nlearning and data augmentation. Importantly, MaxStyle augments data with\nimproved image style diversity and hardness, by expanding the style space with\nnoise and searching for the worst-case style composition of latent features via\nadversarial training. With extensive experiments on multiple public cardiac and\nprostate MR datasets, we demonstrate that MaxStyle leads to significantly\nimproved out-of-distribution robustness against unseen corruptions as well as\ncommon distribution shifts across multiple, different, unseen sites and unknown\nimage sequences under both low- and high-training data settings. The code can\nbe found at https://github.com/cherise215/MaxStyle.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1\">Zeju Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ouyang_C/0/1/0/all/0/1\">Cheng Ouyang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sinclair_M/0/1/0/all/0/1\">Matt Sinclair</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bai_W/0/1/0/all/0/1\">Wenjia Bai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rueckert_D/0/1/0/all/0/1\">Daniel Rueckert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RIDDLE: Lidar Data Compression with Range Image Deep Delta Encoding. (arXiv:2206.01738v1 [eess.IV])","link":"http://arxiv.org/abs/2206.01738","description":"<p>Lidars are depth measuring sensors widely used in autonomous driving and\naugmented reality. However, the large volume of data produced by lidars can\nlead to high costs in data storage and transmission. While lidar data can be\nrepresented as two interchangeable representations: 3D point clouds and range\nimages, most previous work focus on compressing the generic 3D point clouds. In\nthis work, we show that directly compressing the range images can leverage the\nlidar scanning pattern, compared to compressing the unprojected point clouds.\nWe propose a novel data-driven range image compression algorithm, named RIDDLE\n(Range Image Deep DeLta Encoding). At its core is a deep model that predicts\nthe next pixel value in a raster scanning order, based on contextual laser\nshots from both the current and past scans (represented as a 4D point cloud of\nspherical coordinates and time). The deltas between predictions and original\nvalues can then be compressed by entropy encoding. Evaluated on the Waymo Open\nDataset and KITTI, our method demonstrates significant improvement in the\ncompression rate (under the same distortion) compared to widely used point\ncloud and range image compression algorithms as well as recent deep methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhou_X/0/1/0/all/0/1\">Xuanyu Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qi_C/0/1/0/all/0/1\">Charles R. Qi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_Y/0/1/0/all/0/1\">Yin Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Anguelov_D/0/1/0/all/0/1\">Dragomir Anguelov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mutual- and Self- Prototype Alignment for Semi-supervised Medical Image Segmentation. (arXiv:2206.01739v1 [eess.IV])","link":"http://arxiv.org/abs/2206.01739","description":"<p>Semi-supervised learning methods have been explored in medical image\nsegmentation tasks due to the scarcity of pixel-level annotation in the real\nscenario. Proto-type alignment based consistency constraint is an intuitional\nand plausible solu-tion to explore the useful information in the unlabeled\ndata. In this paper, we propose a mutual- and self- prototype alignment (MSPA)\nframework to better utilize the unlabeled data. In specific, mutual-prototype\nalignment enhances the information interaction between labeled and unlabeled\ndata. The mutual-prototype alignment imposes two consistency constraints in\nreverse directions between the unlabeled and labeled data, which enables the\nconsistent embedding and model discriminability on unlabeled data. The proposed\nself-prototype alignment learns more stable region-wise features within\nunlabeled images, which optimizes the classification margin in semi-supervised\nsegmentation by boosting the intra-class compactness and inter-class separation\non the feature space. Extensive experimental results on three medical datasets\ndemonstrate that with a small amount of labeled data, MSPA achieves large\nimprovements by leveraging the unlabeled data. Our method also outperforms\nseven state-of-the-art semi-supervised segmentation methods on all three\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhenxi Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tian_C/0/1/0/all/0/1\">Chunna Tian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jiao_Z/0/1/0/all/0/1\">Zhicheng Jiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Denoising Fast X-Ray Fluorescence Raster Scans of Paintings. (arXiv:2206.01740v1 [eess.IV])","link":"http://arxiv.org/abs/2206.01740","description":"<p>Macro x-ray fluorescence (XRF) imaging of cultural heritage objects, while a\npopular non-invasive technique for providing elemental distribution maps, is a\nslow acquisition process in acquiring high signal-to-noise ratio XRF volumes.\nTypically on the order of tenths of a second per pixel, a raster scanning probe\ncounts the number of photons at different energies emitted by the object under\nx-ray illumination. In an effort to reduce the scan times without sacrificing\nelemental map and XRF volume quality, we propose using dictionary learning with\na Poisson noise model as well as a color image-based prior to restore noisy,\nrapidly acquired XRF data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chopp_H/0/1/0/all/0/1\">Henry Chopp</a>, <a href=\"http://arxiv.org/find/eess/1/au:+McGeachy_A/0/1/0/all/0/1\">Alicia McGeachy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Alfeld_M/0/1/0/all/0/1\">Matthias Alfeld</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cossairt_O/0/1/0/all/0/1\">Oliver Cossairt</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Walton_M/0/1/0/all/0/1\">Marc Walton</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Katsaggelos_A/0/1/0/all/0/1\">Aggelos Katsaggelos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Patcher: Patch Transformers with Mixture of Experts for Precise Medical Image Segmentation. (arXiv:2206.01741v1 [eess.IV])","link":"http://arxiv.org/abs/2206.01741","description":"<p>We present a new encoder-decoder Vision Transformer architecture, Patcher,\nfor medical image segmentation. Unlike standard Vision Transformers, it employs\nPatcher blocks that segment an image into large patches, each of which is\nfurther divided into small patches. Transformers are applied to the small\npatches within a large patch, which constrains the receptive field of each\npixel. We intentionally make the large patches overlap to enhance intra-patch\ncommunication. The encoder employs a cascade of Patcher blocks with increasing\nreceptive fields to extract features from local to global levels. This design\nallows Patcher to benefit from both the coarse-to-fine feature extraction\ncommon in CNNs and the superior spatial relationship modeling of Transformers.\nWe also propose a new mixture-of-experts (MoE) based decoder, which treats the\nfeature maps from the encoder as experts and selects a suitable set of expert\nfeatures to predict the label for each pixel. The use of MoE enables better\nspecializations of the expert features and reduces interference between them\nduring inference. Extensive experiments demonstrate that Patcher outperforms\nstate-of-the-art Transformer- and CNN-based approaches significantly on stroke\nlesion segmentation and polyp segmentation. Code for Patcher will be released\nwith publication to facilitate future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ou_Y/0/1/0/all/0/1\">Yanglan Ou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yuan_Y/0/1/0/all/0/1\">Ye Yuan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_X/0/1/0/all/0/1\">Xiaolei Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wong_S/0/1/0/all/0/1\">Stephen T.C. Wong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Volpi_J/0/1/0/all/0/1\">John Volpi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">James Z. Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wong_K/0/1/0/all/0/1\">Kelvin Wong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Probabilistic Structural Representation for Biomedical Image Segmentation. (arXiv:2206.01742v1 [eess.IV])","link":"http://arxiv.org/abs/2206.01742","description":"<p>Accurate segmentation of various fine-scale structures from biomedical images\nis a very important yet challenging problem. Existing methods use topological\ninformation as an additional training loss, but are ultimately learning a\npixel-wise representation. In this paper, we propose the first deep learning\nmethod to learn a structural representation. We use discrete Morse theory and\npersistent homology to construct an one-parameter family of structures as the\nstructural representation space. Furthermore, we learn a probabilistic model\nthat can do inference tasks on such a structural representation space. We\nempirically demonstrate the strength of our method, i.e., generating true\nstructures rather than pixel-maps with better topological integrity, and\nfacilitating a human-in-the-loop annotation pipeline using the sampling of\nstructures and structure-aware uncertainty.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hu_X/0/1/0/all/0/1\">Xiaoling Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Samaras_D/0/1/0/all/0/1\">Dimitris Samaras</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_C/0/1/0/all/0/1\">Chao Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Orthogonal Transform based Generative Adversarial Network for Image Dehazing. (arXiv:2206.01743v1 [eess.IV])","link":"http://arxiv.org/abs/2206.01743","description":"<p>Image dehazing has become one of the crucial preprocessing steps for any\ncomputer vision task. Most of the dehazing methods try to estimate the\ntransmission map along with the atmospheric light to get the dehazed image in\nthe image domain. In this paper, we propose a novel end-to-end architecture\nthat directly estimates dehazed image in Krawtchouk transform domain. For this\na customized Krawtchouk Convolution Layer (KCL) in the architecture is added.\nKCL is constructed using Krawtchouk basis functions which converts the image\nfrom the spatial domain to the Krawtchouk transform domain. Another convolution\nlayer is added at the end of the architecture named as Inverse Krawtchouk\nConvolution Layer (IKCL) which converts the image back to the spatial domain\nfrom the transform domain. It has been observed that the haze is mainly present\nin lower frequencies of hazy images, wherein the Krawtchouk transform helps to\nanalyze the high and low frequencies of the images separately. We have divided\nour architecture into two branches, the upper branch deals with the higher\nfrequencies while the lower branch deals with the lower frequencies of the\nimage. The lower branch is made deeper in terms of the layers as compared to\nthe upper branch to address the haze present in the lower frequencies. Using\nthe proposed Orthogonal Transform based Generative Adversarial Network (OTGAN)\narchitecture for image dehazing, we were able to achieve competitive results\nwhen compared to the present state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kumar_A/0/1/0/all/0/1\">Ahlad Kumar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sanathra_M/0/1/0/all/0/1\">Mantra Sanathra</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khare_M/0/1/0/all/0/1\">Manish Khare</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khare_V/0/1/0/all/0/1\">Vijeta Khare</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detection of Fibrosis in Cine Magnetic Resonance Images Using Artificial Intelligence Techniques. (arXiv:2206.01745v1 [eess.IV])","link":"http://arxiv.org/abs/2206.01745","description":"<p>Background: Artificial intelligence techniques have demonstrated great\npotential in cardiology, especially to detect imperceptible patterns for the\nhuman eye. In this sense, these techniques seem to be adequate to identify\npatterns in the myocardial texture which could lead to characterize and\nquantify fibrosis. Purpose: The aim of this study was to postulate a new\nartificial intelligence method to identify fibrosis in cine cardiac magnetic\nresonance (CMR) imaging. Methods: A retrospective observational study was\ncarried out in a population of 75 subjects from a clinical center of San Carlos\nde Bariloche. The proposed method analyzes the myocardial texture in cine CMR\nimages using a convolutional neural network to determine local myocardial\ntissue damage. Results: An accuracy of 89% for quantifying local tissue damage\nwas observed for the validation data set and 70% for the test set. In addition,\nthe qualitative analysis showed a high spatial correlation in lesion location.\nConclusions: The postulated method enables to spatially identify fibrosis using\nonly the information from cine nuclear magnetic resonance studies,\ndemonstrating the potential of this technique to quantify myocardial viability\nin the future or to study the lesions etiology\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Curiale_A/0/1/0/all/0/1\">Ariel. H. Curiale</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cabrera_F/0/1/0/all/0/1\">Facundo Cabrera</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jimenez_P/0/1/0/all/0/1\">Pablo Jimenez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Medus_J/0/1/0/all/0/1\">Jorgelina Medus</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mato_G/0/1/0/all/0/1\">Germ&#xc1;n Mato</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Calandrelli_M/0/1/0/all/0/1\">Mat&#xcd;as E. Calandrelli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Quantification of Volumes and Biventricular Function in Cardiac Resonance. Validation of a New Artificial Intelligence Approach. (arXiv:2206.01746v1 [eess.IV])","link":"http://arxiv.org/abs/2206.01746","description":"<p>Background: Artificial intelligence techniques have shown great potential in\ncardiology, especially in quantifying cardiac biventricular function, volume,\nmass, and ejection fraction (EF). However, its use in clinical practice is not\nstraightforward due to its poor reproducibility with cases from daily practice,\namong other reasons. Objectives: To validate a new artificial intelligence tool\nin order to quantify the cardiac biventricular function (volume, mass, and EF).\nTo analyze its robustness in the clinical area, and the computational times\ncompared with conventional methods. Methods: A total of 189 patients were\nanalyzed: 89 from a regional center and 100 from a public center. The method\nproposes two convolutional networks that include anatomical information of the\nheart to reduce classification errors. Results: A high concordance (Pearson\ncoefficient) was observed between manual quantification and the proposed\nquantification of cardiac function (0.98, 0.92, 0.96 and 0.8 for volumes and\nbiventricular EF) in about 5 seconds per study. Conclusions: This method\nquantifies biventricular function and volumes in seconds with an accuracy\nequivalent to that of a specialist.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Curiale_A/0/1/0/all/0/1\">Ariel H. Curiale</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Calandrelli_M/0/1/0/all/0/1\">Mat&#xcd;as E. Calandrelli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dellazoppa_L/0/1/0/all/0/1\">Lucca Dellazoppa</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Trevisan_M/0/1/0/all/0/1\">Mariano Trevisan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+BociAn_J/0/1/0/all/0/1\">Jorge Luis Boci&#xc1;n</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bonifacio_J/0/1/0/all/0/1\">Juan Pablo Bonifacio</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mato_G/0/1/0/all/0/1\">Germ&#xc1;n Mato</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Radar Guided Dynamic Visual Attention for Resource-Efficient RGB Object Detection. (arXiv:2206.01772v1 [cs.CV])","link":"http://arxiv.org/abs/2206.01772","description":"<p>An autonomous system's perception engine must provide an accurate\nunderstanding of the environment for it to make decisions. Deep learning based\nobject detection networks experience degradation in the performance and\nrobustness for small and far away objects due to a reduction in object's\nfeature map as we move to higher layers of the network. In this work, we\npropose a novel radar-guided spatial attention for RGB images to improve the\nperception quality of autonomous vehicles operating in a dynamic environment.\nIn particular, our method improves the perception of small and long range\nobjects, which are often not detected by the object detectors in RGB mode. The\nproposed method consists of two RGB object detectors, namely the Primary\ndetector and a lightweight Secondary detector. The primary detector takes a\nfull RGB image and generates primary detections. Next, the radar proposal\nframework creates regions of interest (ROIs) for object proposals by projecting\nthe radar point cloud onto the 2D RGB image. These ROIs are cropped and fed to\nthe secondary detector to generate secondary detections which are then fused\nwith the primary detections via non-maximum suppression. This method helps in\nrecovering the small objects by preserving the object's spatial features\nthrough an increase in their receptive field. We evaluate our fusion method on\nthe challenging nuScenes dataset and show that our fusion method with SSD-lite\nas primary and secondary detector improves the baseline primary yolov3\ndetector's recall by 14% while requiring three times fewer computational\nresources.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumawat_H/0/1/0/all/0/1\">Hemant Kumawat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukhopadhyay_S/0/1/0/all/0/1\">Saibal Mukhopadhyay</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Monkeypox Image Data collection. (arXiv:2206.01774v1 [eess.IV])","link":"http://arxiv.org/abs/2206.01774","description":"<p>This paper explains the initial Monkeypox Open image data collection\nprocedure. It was created by assembling images collected from websites,\nnewspapers, and online portals and currently contains around 1905 images after\ndata augmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ahsan_M/0/1/0/all/0/1\">Md Manjurul Ahsan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Uddin_M/0/1/0/all/0/1\">Muhammad Ramiz Uddin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Luna_S/0/1/0/all/0/1\">Shahana Akter Luna</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Real-Time Super-Resolution for Real-World Images on Mobile Devices. (arXiv:2206.01777v1 [cs.CV])","link":"http://arxiv.org/abs/2206.01777","description":"<p>Image Super-Resolution (ISR), which aims at recovering High-Resolution (HR)\nimages from the corresponding Low-Resolution (LR) counterparts. Although recent\nprogress in ISR has been remarkable. However, they are way too computationally\nintensive to be deployed on edge devices, since most of the recent approaches\nare deep learning-based. Besides, these methods always fail in real-world\nscenes, since most of them adopt a simple fixed \"ideal\" bicubic downsampling\nkernel from high-quality images to construct LR/HR training pairs which may\nlose track of frequency-related details. In this work, an approach for\nreal-time ISR on mobile devices is presented, which is able to deal with a wide\nrange of degradations in real-world scenarios. Extensive experiments on\ntraditional super-resolution datasets (Set5, Set14, BSD100, Urban100, Manga109,\nDIV2K) and real-world images with a variety of degradations demonstrate that\nour method outperforms the state-of-art methods, resulting in higher PSNR and\nSSIM, lower noise and better visual quality. Most importantly, our method\nachieves real-time performance on mobile or edge devices.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jie Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1\">Zibo Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_J/0/1/0/all/0/1\">Jiaming Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_C/0/1/0/all/0/1\">Chiu Man Ho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"R2U++: A Multiscale Recurrent Residual U-Net with Dense Skip Connections for Medical Image Segmentation. (arXiv:2206.01793v1 [eess.IV])","link":"http://arxiv.org/abs/2206.01793","description":"<p>U-Net is a widely adopted neural network in the domain of medical image\nsegmentation. Despite its quick embracement by the medical imaging community,\nits performance suffers on complicated datasets. The problem can be ascribed to\nits simple feature extracting blocks: encoder/decoder, and the semantic gap\nbetween encoder and decoder. Variants of U-Net (such as R2U-Net) have been\nproposed to address the problem of simple feature extracting blocks by making\nthe network deeper, but it does not deal with the semantic gap problem. On the\nother hand, another variant UNET++ deals with the semantic gap problem by\nintroducing dense skip connections but has simple feature extraction blocks. To\novercome these issues, we propose a new U-Net based medical image segmentation\narchitecture R2U++. In the proposed architecture, the adapted changes from\nvanilla U-Net are: (1) the plain convolutional backbone is replaced by a deeper\nrecurrent residual convolution block. The increased field of view with these\nblocks aids in extracting crucial features for segmentation which is proven by\nimprovement in the overall performance of the network. (2) The semantic gap\nbetween encoder and decoder is reduced by dense skip pathways. These pathways\naccumulate features coming from multiple scales and apply concatenation\naccordingly. The modified architecture has embedded multi-depth models, and an\nensemble of outputs taken from varying depths improves the performance on\nforeground objects appearing at various scales in the images. The performance\nof R2U++ is evaluated on four distinct medical imaging modalities: electron\nmicroscopy (EM), X-rays, fundus, and computed tomography (CT). The average gain\nachieved in IoU score is 1.5+-0.37% and in dice score is 0.9+-0.33% over\nUNET++, whereas, 4.21+-2.72 in IoU and 3.47+-1.89 in dice score over R2U-Net\nacross different medical imaging segmentation datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Mubashar_M/0/1/0/all/0/1\">Mehreen Mubashar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ali_H/0/1/0/all/0/1\">Hazrat Ali</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gronlund_C/0/1/0/all/0/1\">Christer Gronlund</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Azmat_S/0/1/0/all/0/1\">Shoaib Azmat</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Additive MIL: Intrinsic Interpretability for Pathology. (arXiv:2206.01794v1 [cs.CV])","link":"http://arxiv.org/abs/2206.01794","description":"<p>Multiple Instance Learning (MIL) has been widely applied in pathology towards\nsolving critical problems such as automating cancer diagnosis and grading,\npredicting patient prognosis, and therapy response. Deploying these models in a\nclinical setting requires careful inspection of these black boxes during\ndevelopment and deployment to identify failures and maintain physician trust.\nIn this work, we propose a simple formulation of MIL models, which enables\ninterpretability while maintaining similar predictive performance. Our Additive\nMIL models enable spatial credit assignment such that the contribution of each\nregion in the image can be exactly computed and visualized. We show that our\nspatial credit assignment coincides with regions used by pathologists during\ndiagnosis and improves upon classical attention heatmaps from attention MIL\nmodels. We show that any existing MIL model can be made additive with a simple\nchange in function composition. We also show how these models can debug model\nfailures, identify spurious features, and highlight class-wise regions of\ninterest, enabling their use in high-stakes environments such as clinical\ndecision-making.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Javed_S/0/1/0/all/0/1\">Syed Ashar Javed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Juyal_D/0/1/0/all/0/1\">Dinkar Juyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padigela_H/0/1/0/all/0/1\">Harshith Padigela</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taylor_Weiner_A/0/1/0/all/0/1\">Amaro Taylor-Weiner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Limin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prakash_A/0/1/0/all/0/1\">Aaditya Prakash</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning sRGB-to-Raw-RGB De-rendering with Content-Aware Metadata. (arXiv:2206.01813v1 [cs.CV])","link":"http://arxiv.org/abs/2206.01813","description":"<p>Most camera images are rendered and saved in the standard RGB (sRGB) format\nby the camera's hardware. Due to the in-camera photo-finishing routines,\nnonlinear sRGB images are undesirable for computer vision tasks that assume a\ndirect relationship between pixel values and scene radiance. For such\napplications, linear raw-RGB sensor images are preferred. Saving images in\ntheir raw-RGB format is still uncommon due to the large storage requirement and\nlack of support by many imaging applications. Several \"raw reconstruction\"\nmethods have been proposed that utilize specialized metadata sampled from the\nraw-RGB image at capture time and embedded in the sRGB image. This metadata is\nused to parameterize a mapping function to de-render the sRGB image back to its\noriginal raw-RGB format when needed. Existing raw reconstruction methods rely\non simple sampling strategies and global mapping to perform the de-rendering.\nThis paper shows how to improve the de-rendering results by jointly learning\nsampling and reconstruction. Our experiments show that our learned sampling can\nadapt to the image content to produce better raw reconstructions than existing\nmethods. We also describe an online fine-tuning strategy for the reconstruction\nnetwork to improve results further.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nam_S/0/1/0/all/0/1\">Seonghyeon Nam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Punnappurath_A/0/1/0/all/0/1\">Abhijith Punnappurath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brubaker_M/0/1/0/all/0/1\">Marcus A. Brubaker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_M/0/1/0/all/0/1\">Michael S. Brown</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EAANet: Efficient Attention Augmented Convolutional Networks. (arXiv:2206.01821v1 [cs.CV])","link":"http://arxiv.org/abs/2206.01821","description":"<p>Humans can effectively find salient regions in complex scenes. Self-attention\nmechanisms were introduced into Computer Vision (CV) to achieve this. Attention\nAugmented Convolutional Network (AANet) is a mixture of convolution and\nself-attention, which increases the accuracy of a typical ResNet. However, The\ncomplexity of self-attention is O(n2) in terms of computation and memory usage\nwith respect to the number of input tokens. In this project, we propose EAANet:\nEfficient Attention Augmented Convolutional Networks, which incorporates\nefficient self-attention mechanisms in a convolution and self-attention hybrid\narchitecture to reduce the model's memory footprint. Our best model show\nperformance improvement over AA-Net and ResNet18. We also explore different\nmethods to augment Convolutional Network with self-attention mechanisms and\nshow the difficulty of training those methods compared to ResNet. Finally, we\nshow that augmenting efficient self-attention mechanisms with ResNet scales\nbetter with input size than normal self-attention mechanisms. Therefore, our\nEAANet is more capable of working with high-resolution images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Runqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_T/0/1/0/all/0/1\">Tianshu Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Gamma Generalized Normal Distribution: A Descriptor of SAR Imagery. (arXiv:2206.01826v1 [stat.ME])","link":"http://arxiv.org/abs/2206.01826","description":"<p>We propose a new four-parameter distribution for modeling synthetic aperture\nradar (SAR) imagery named the gamma generalized normal (GGN) by combining the\ngamma and generalized normal distributions. A mathematical characterization of\nthe new distribution is provided by identifying the limit behavior and by\ncalculating the density and moment expansions. The GGN model performance is\nevaluated on both synthetic and actual data and, for that, maximum likelihood\nestimation and random number generation are discussed. The proposed\ndistribution is compared with the beta generalized normal distribution (BGN),\nwhich has already shown to appropriately represent SAR imagery. The performance\nof these two distributions are measured by means of statistics which provide\nevidence that the GGN can outperform the BGN distribution in some contexts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Cordeiro_G/0/1/0/all/0/1\">G. M. Cordeiro</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Cintra_R/0/1/0/all/0/1\">R. J. Cintra</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Rego_L/0/1/0/all/0/1\">L. C. R&#xea;go</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Nascimento_A/0/1/0/all/0/1\">A. D. C. Nascimento</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Drawing out of Distribution with Neuro-Symbolic Generative Models. (arXiv:2206.01829v1 [cs.LG])","link":"http://arxiv.org/abs/2206.01829","description":"<p>Learning general-purpose representations from perceptual inputs is a hallmark\nof human intelligence. For example, people can write out numbers or characters,\nor even draw doodles, by characterizing these tasks as different instantiations\nof the same generic underlying process -- compositional arrangements of\ndifferent forms of pen strokes. Crucially, learning to do one task, say\nwriting, implies reasonable competence at another, say drawing, on account of\nthis shared process. We present Drawing out of Distribution (DooD), a\nneuro-symbolic generative model of stroke-based drawing that can learn such\ngeneral-purpose representations. In contrast to prior work, DooD operates\ndirectly on images, requires no supervision or expensive test-time inference,\nand performs unsupervised amortised inference with a symbolic stroke model that\nbetter enables both interpretability and generalization. We evaluate DooD on\nits ability to generalise across both data and tasks. We first perform\nzero-shot transfer from one dataset (e.g. MNIST) to another (e.g. Quickdraw),\nacross five different datasets, and show that DooD clearly outperforms\ndifferent baselines. An analysis of the learnt representations further\nhighlights the benefits of adopting a symbolic stroke model. We then adopt a\nsubset of the Omniglot challenge tasks, and evaluate its ability to generate\nnew exemplars (both unconditionally and conditionally), and perform one-shot\nclassification, showing that DooD matches the state of the art. Taken together,\nwe demonstrate that DooD does indeed capture general-purpose representations\nacross both data and task, and takes a further step towards building general\nand robust concept-learning systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yichao Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1\">Joshua B. Tenenbaum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1\">Tuan Anh Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siddharth_N/0/1/0/all/0/1\">N. Siddharth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spatial Feature Mapping for 6DoF Object Pose Estimation. (arXiv:2206.01831v1 [cs.CV])","link":"http://arxiv.org/abs/2206.01831","description":"<p>This work aims to estimate 6Dof (6D) object pose in background clutter.\nConsidering the strong occlusion and background noise, we propose to utilize\nthe spatial structure for better tackling this challenging task. Observing that\nthe 3D mesh can be naturally abstracted by a graph, we build the graph using 3D\npoints as vertices and mesh connections as edges. We construct the\ncorresponding mapping from 2D image features to 3D points for filling the graph\nand fusion of the 2D and 3D features. Afterward, a Graph Convolutional Network\n(GCN) is applied to help the feature exchange among objects' points in 3D\nspace. To address the problem of rotation symmetry ambiguity for objects, a\nspherical convolution is utilized and the spherical features are combined with\nthe convolutional features that are mapped to the graph. Predefined 3D\nkeypoints are voted and the 6DoF pose is obtained via the fitting optimization.\nTwo scenarios of inference, one with the depth information and the other\nwithout it are discussed. Tested on the datasets of YCB-Video and LINEMOD, the\nexperiments demonstrate the effectiveness of our proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mei_J/0/1/0/all/0/1\">Jianhan Mei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xudong Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1\">Henghui Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Coffee Roast Intelligence. (arXiv:2206.01841v1 [cs.CV])","link":"http://arxiv.org/abs/2206.01841","description":"<p>As the coffee industry has grown, there would be more demand for roasted\ncoffee beans, as well as increased rivalry for selling coffee and attracting\ncustomers. As the flavor of each variety of coffee is dependent on the degree\nof roasting of the coffee beans, it is vital to maintain a consistent quality\nrelated to the degree of roasting. Each barista has their own method for\ndetermining the degree of roasting. However, extrinsic circumstances such as\nlight, fatigue, and other factors may alter their judgment. As a result, the\nquality of the coffee cannot be controlled. The Coffee Roast Intelligence\napplication is a machine learning-based study of roasted coffee bean degrees\nclassification produced as an Android application platform that identifies the\ncolor of coffee beans by photographing or uploading them while roasting. This\napplication displays the text showing at what level the coffee beans have been\nroasted, as well as informs the percent chance of class prediction to the\nconsumers. Users may also keep track of the result of the predictions related\nto the roasting level of coffee beans.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ontoum_S/0/1/0/all/0/1\">Sakdipat Ontoum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khemanantakul_T/0/1/0/all/0/1\">Thitaree Khemanantakul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sroison_P/0/1/0/all/0/1\">Pornphat Sroison</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Triyason_T/0/1/0/all/0/1\">Tuul Triyason</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanapa_B/0/1/0/all/0/1\">Bunthit Watanapa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Clues: Bridging Vision and Language Foundations for Image Paragraph Captioning. (arXiv:2206.01843v1 [cs.CV])","link":"http://arxiv.org/abs/2206.01843","description":"<p>People say, \"A picture is worth a thousand words\". Then how can we get the\nrich information out of the image? We argue that by using visual clues to\nbridge large pretrained vision foundation models and language models, we can do\nso without any extra cross-modal training. Thanks to the strong zero-shot\ncapability of foundation models, we start by constructing a rich semantic\nrepresentation of the image (e.g., image tags, object attributes / locations,\ncaptions) as a structured textual prompt, called visual clues, using a vision\nfoundation model. Based on visual clues, we use large language model to produce\na series of comprehensive descriptions for the visual content, which is then\nverified by the vision model again to select the candidate that aligns best\nwith the image. We evaluate the quality of generated descriptions by\nquantitative and qualitative measurement. The results demonstrate the\neffectiveness of such a structured semantic representation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yujia Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Luowei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1\">Xiyang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bach_N/0/1/0/all/0/1\">Nguyen Bach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Ce Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1\">Michael Zeng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Poisson2Sparse: Self-Supervised Poisson Denoising From a Single Image. (arXiv:2206.01856v1 [cs.CV])","link":"http://arxiv.org/abs/2206.01856","description":"<p>Image enhancement approaches often assume that the noise is signal\nindependent, and approximate the degradation model as zero-mean additive\nGaussian noise. However, this assumption does not hold for biomedical imaging\nsystems where sensor-based sources of noise are proportional to signal\nstrengths, and the noise is better represented as a Poisson process. In this\nwork, we explore a sparsity and dictionary learning-based approach and present\na novel self-supervised learning method for single-image denoising where the\nnoise is approximated as a Poisson process, requiring no clean ground-truth\ndata. Specifically, we approximate traditional iterative optimization\nalgorithms for image denoising with a recurrent neural network which enforces\nsparsity with respect to the weights of the network. Since the sparse\nrepresentations are based on the underlying image, it is able to suppress the\nspurious components (noise) in the image patches, thereby introducing implicit\nregularization for denoising task through the network structure. Experiments on\ntwo bio-imaging datasets demonstrate that our method outperforms the\nstate-of-the-art approaches in terms of PSNR and SSIM. Our qualitative results\ndemonstrate that, in addition to higher performance on standard quantitative\nmetrics, we are able to recover much more subtle details than other compared\napproaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ta_C/0/1/0/all/0/1\">Calvin-Khang Ta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aich_A/0/1/0/all/0/1\">Abhishek Aich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Akash Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_Chowdhury_A/0/1/0/all/0/1\">Amit K. Roy-Chowdhury</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image Data collection and implementation of deep learning-based model in detecting Monkeypox disease using modified VGG16. (arXiv:2206.01862v1 [eess.IV])","link":"http://arxiv.org/abs/2206.01862","description":"<p>While the world is still attempting to recover from the damage caused by the\nbroad spread of COVID-19, the Monkeypox virus poses a new threat of becoming a\nglobal pandemic. Although the Monkeypox virus itself is not deadly and\ncontagious as COVID-19, still every day, new patients case has been reported\nfrom many nations. Therefore, it will be no surprise if the world ever faces\nanother global pandemic due to the lack of proper precautious steps. Recently,\nMachine learning (ML) has demonstrated huge potential in image-based diagnoses\nsuch as cancer detection, tumor cell identification, and COVID-19 patient\ndetection. Therefore, a similar application can be adopted to diagnose the\nMonkeypox-related disease as it infected the human skin, which image can be\nacquired and further used in diagnosing the disease. Considering this\nopportunity, in this work, we introduce a newly developed \"Monkeypox2022\"\ndataset that is publicly available to use and can be obtained from our shared\nGitHub repository. The dataset is created by collecting images from multiple\nopen-source and online portals that do not impose any restrictions on use, even\nfor commercial purposes, hence giving a safer path to use and disseminate such\ndata when constructing and deploying any type of ML model. Further, we propose\nand evaluate a modified VGG16 model, which includes two distinct studies: Study\nOne and Two. Our exploratory computational results indicate that our suggested\nmodel can identify Monkeypox patients with an accuracy of $97\\pm1.8\\%$\n(AUC=97.2) and $88\\pm0.8\\%$ (AUC=0.867) for Study One and Two, respectively.\nAdditionally, we explain our model's prediction and feature extraction\nutilizing Local Interpretable Model-Agnostic Explanations (LIME) help to a\ndeeper insight into specific features that characterize the onset of the\nMonkeypox virus.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ahsan_M/0/1/0/all/0/1\">Md Manjurul Ahsan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Uddin_M/0/1/0/all/0/1\">Muhammad Ramiz Uddin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Farjana_M/0/1/0/all/0/1\">Mithila Farjana</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sakib_A/0/1/0/all/0/1\">Ahmed Nazmus Sakib</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Momin_K/0/1/0/all/0/1\">Khondhaker Al Momin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Luna_S/0/1/0/all/0/1\">Shahana Akter Luna</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recurrent Image Registration using Mutual Attention based Network. (arXiv:2206.01863v1 [cs.CV])","link":"http://arxiv.org/abs/2206.01863","description":"<p>Image registration is an important task in medical imaging which estimates\nthe spatial transformation between different images. Many previous studies have\nused learning-based methods for multi-stage registration to perform 3D image\nregistration to improve performance. The performance of the multi-stage\napproach, however, is limited by the size of the receptive field where complex\nmotion does not occur at a single spatial scale. We propose a new registration\nnetwork combining recursive network architecture and mutual attention mechanism\nto overcome these limitations. Compared with the previous deep learning\nmethods, our network based on the recursive structure achieves the highest\naccuracy in lung Computed Tomography (CT) data set (Dice score of 92\\% and\naverage surface distance of 3.8mm for lungs) and one of the most accurate\nresults in abdominal CT data set with 9 organs of various sizes (Dice score of\n55\\% and average surface distance of 7.8mm). We also showed that adding 3\nrecursive networks is sufficient to achieve the state-of-the-art results\nwithout a significant increase in the inference time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Jian-Qing Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Ziyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1\">Baoru Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_N/0/1/0/all/0/1\">Ngee Han Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vincent_T/0/1/0/all/0/1\">Tonia Vincent</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papiez_B/0/1/0/all/0/1\">Bartlomiej W. Papiez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SPGNet: Spatial Projection Guided 3D Human Pose Estimation in Low Dimensional Space. (arXiv:2206.01867v1 [cs.CV])","link":"http://arxiv.org/abs/2206.01867","description":"<p>We propose a method SPGNet for 3D human pose estimation that mixes\nmulti-dimensional re-projection into supervised learning. In this method, the\n2D-to-3D-lifting network predicts the global position and coordinates of the 3D\nhuman pose. Then, we re-project the estimated 3D pose back to the 2D key points\nalong with spatial adjustments. The loss functions compare the estimated 3D\npose with the 3D pose ground truth, and re-projected 2D pose with the input 2D\npose. In addition, we propose a kinematic constraint to restrict the predicted\ntarget with constant human bone length. Based on the estimation results for the\ndataset Human3.6M, our approach outperforms many state-of-the-art methods both\nqualitatively and quantitatively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zihan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1\">Ruimin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Mengxuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_G/0/1/0/all/0/1\">Guanfang Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basu_A/0/1/0/all/0/1\">Anup Basu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Face Recognition Accuracy Across Demographics: Shining a Light Into the Problem. (arXiv:2206.01881v1 [cs.CV])","link":"http://arxiv.org/abs/2206.01881","description":"<p>This is the first work that we are aware of to explore how the level of\nbrightness of the skin region in a pair of face images impacts face recognition\naccuracy. Image pairs with both images having mean face skin brightness in an\nupper-middle range of brightness are found to have the highest matching\naccuracy across demographics and matchers. Image pairs with both images having\nmean face skin brightness that is too dark or too light are found to have an\nincreased false match rate (FMR). Image pairs with strongly different face skin\nbrightness are found to have decreased FMR and increased false non-match rate\n(FNMR). Using a brightness information metric that captures the variation in\nbrightness in the face skin region, the variation in matching accuracy is shown\nto correlate with the level of information available in the face skin region.\nFor operational scenarios where image acquisition is controlled, we propose\nacquiring images with lighting adjusted to yield face skin brightness in a\nnarrow range.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Haiyu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Albiero_V/0/1/0/all/0/1\">V&#xed;tor Albiero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnapriya_K/0/1/0/all/0/1\">K. S. Krishnapriya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+King_M/0/1/0/all/0/1\">Michael C. King</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bowyer_K/0/1/0/all/0/1\">Kevin W. Bowyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Superimposed Divide-and-Conquer Image Recognition Method for SEM Images of Nanoparticles on The Surface of Monocrystalline silicon with High Aggregation Degree. (arXiv:2206.01884v1 [cs.CV])","link":"http://arxiv.org/abs/2206.01884","description":"<p>The nanoparticle size and distribution information in the SEM images of\nsilicon crystals are generally counted by manual methods. The realization of\nautomatic machine recognition is significant in materials science. This paper\nproposed a superposition partitioning image recognition method to realize\nautomatic recognition and information statistics of silicon crystal\nnanoparticle SEM images. Especially for the complex and highly aggregated\ncharacteristics of silicon crystal particle size, an accurate recognition step\nand contour statistics method based on morphological processing are given. This\nmethod has technical reference value for the recognition of Monocrystalline\nsilicon surface nanoparticle images under different SEM shooting conditions.\nBesides, it outperforms other methods in terms of recognition accuracy and\nalgorithm efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_R/0/1/0/all/0/1\">Ruiling Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_J/0/1/0/all/0/1\">Jiayang Niu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modeling of Textures to Predict Immune Cell Status and Survival of Brain Tumour Patients. (arXiv:2206.01897v1 [eess.IV])","link":"http://arxiv.org/abs/2206.01897","description":"<p>Radiomics has shown a capability for different types of cancers such as\nglioma to predict the clinical outcome. It can have a non-invasive means of\nevaluating the immunotherapy response prior to treatment. However, the use of\ndeep convolutional neural networks (CNNs)-based radiomics requires large\ntraining image sets. To avoid this problem, we investigate a new imaging\nfeatures that model distribution with a Gaussian mixture model (GMM) of learned\n3D CNN features. Using these deep radiomic features (DRFs), we aim to predict\nthe immune marker status (low versus high) and overall survival for glioma\npatients. We extract the DRFs by aggregating the activation maps of a\npre-trained 3D-CNN within labeled tumor regions of MRI scans that corresponded\nimmune markers of 151 patients. Our experiments are performed to assess the\nrelationship between the proposed DRFs, three immune cell markers (Macrophage\nM1, Neutrophils and T Cells Follicular Helper), and measure their association\nwith overall survival. Using the random forest (RF) model, DRFs was able to\npredict the immune marker status with area under the ROC curve (AUC) of 78.67,\n83.93 and 75.67\\% for Macrophage M1, Neutrophils and T Cells Follicular Helper,\nrespectively. Combined the immune markers with DRFs and clinical variables,\nKaplan-Meier estimator and Log-rank test achieved the most significant\ndifference between predicted groups of patients (short-term versus long-term\nsurvival) with p\\,=\\,4.31$\\times$10$^{-7}$ compared to p\\,=\\,0.03 for Immune\ncell markers, p\\,=\\,0.07 for clinical variables , and\np\\,=\\,1.45$\\times$10$^{-5}$ for DRFs. Our findings indicate that the proposed\nfeatures (DRFs) used in RF models may significantly consider prognosticating\npatients with brain tumour prior to surgery through regularly acquired imaging\ndata.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chaddad_A/0/1/0/all/0/1\">Ahmad Chaddad</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_M/0/1/0/all/0/1\">Mingli Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hassan_L/0/1/0/all/0/1\">Lama Hassan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Niazi_T/0/1/0/all/0/1\">Tamim Niazi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Saliency Attack: Towards Imperceptible Black-box Adversarial Attack. (arXiv:2206.01898v1 [cs.LG])","link":"http://arxiv.org/abs/2206.01898","description":"<p>Deep neural networks are vulnerable to adversarial examples, even in the\nblack-box setting where the attacker is only accessible to the model output.\nRecent studies have devised effective black-box attacks with high query\nefficiency. However, such performance is often accompanied by compromises in\nattack imperceptibility, hindering the practical use of these approaches. In\nthis paper, we propose to restrict the perturbations to a small salient region\nto generate adversarial examples that can hardly be perceived. This approach is\nreadily compatible with many existing black-box attacks and can significantly\nimprove their imperceptibility with little degradation in attack success rate.\nFurther, we propose the Saliency Attack, a new black-box attack aiming to\nrefine the perturbations in the salient region to achieve even better\nimperceptibility. Extensive experiments show that compared to the\nstate-of-the-art black-box attacks, our approach achieves much better\nimperceptibility scores, including most apparent distortion (MAD), $L_0$ and\n$L_2$ distances, and also obtains significantly higher success rates judged by\na human-like threshold on MAD. Importantly, the perturbations generated by our\napproach are interpretable to some extent. Finally, it is also demonstrated to\nbe robust to different detection-based defenses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dai_Z/0/1/0/all/0/1\">Zeyu Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shengcai Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_K/0/1/0/all/0/1\">Ke Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qing Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Radiomic Analysis for Predicting Coronavirus Disease 2019 in Computerized Tomography and X-ray Images. (arXiv:2206.01903v1 [eess.IV])","link":"http://arxiv.org/abs/2206.01903","description":"<p>This paper proposes to encode the distribution of features learned from a\nconvolutional neural network using a Gaussian Mixture Model. These parametric\nfeatures, called GMM-CNN, are derived from chest computed tomography and X-ray\nscans of patients with Coronavirus Disease 2019. We use the proposed GMM-CNN\nfeatures as input to a robust classifier based on random forests to\ndifferentiate between COVID-19 and other pneumonia cases. Our experiments\nassess the advantage of GMM-CNN features compared to standard CNN\nclassification on test images. Using a random forest classifier (80\\% samples\nfor training; 20\\% samples for testing), GMM-CNN features encoded with two\nmixture components provided a significantly better performance than standard\nCNN classification (p\\,$&lt;$\\,0.05). Specifically, our method achieved an\naccuracy in the range of 96.00\\,--\\,96.70\\% and an area under the ROC curve in\nthe range of 99.29\\,--\\,99.45\\%, with the best performance obtained by\ncombining GMM-CNN features from both computed tomography and X-ray images. Our\nresults suggest that the proposed GMM-CNN features could improve the prediction\nof COVID-19 in chest computed tomography and X-ray scans.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chaddad_A/0/1/0/all/0/1\">Ahmad Chaddad</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hassan_L/0/1/0/all/0/1\">Lama Hassan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Desrosiers_C/0/1/0/all/0/1\">Christian Desrosiers</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video-based Human-Object Interaction Detection from Tubelet Tokens. (arXiv:2206.01908v1 [cs.CV])","link":"http://arxiv.org/abs/2206.01908","description":"<p>We present a novel vision Transformer, named TUTOR, which is able to learn\ntubelet tokens, served as highly-abstracted spatiotemporal representations, for\nvideo-based human-object interaction (V-HOI) detection. The tubelet tokens\nstructurize videos by agglomerating and linking semantically-related patch\ntokens along spatial and temporal domains, which enjoy two benefits: 1)\nCompactness: each tubelet token is learned by a selective attention mechanism\nto reduce redundant spatial dependencies from others; 2) Expressiveness: each\ntubelet token is enabled to align with a semantic instance, i.e., an object or\na human, across frames, thanks to agglomeration and linking. The effectiveness\nand efficiency of TUTOR are verified by extensive experiments. Results shows\nour method outperforms existing works by large margins, with a relative mAP\ngain of $16.14\\%$ on VidHOI and a 2 points gain on CAD-120 as well as a $4\n\\times$ speedup.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tu_D/0/1/0/all/0/1\">Danyang Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Wei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_X/0/1/0/all/0/1\">Xiongkuo Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_G/0/1/0/all/0/1\">Guangtao Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1\">Wei Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Spike Gating Flow: A Hierarchical Structure Based Spiking Neural Network for Online Gesture Recognition. (arXiv:2206.01910v1 [cs.CV])","link":"http://arxiv.org/abs/2206.01910","description":"<p>Action recognition is an exciting research avenue for artificial intelligence\nsince it may be a game changer in the emerging industrial fields such as\nrobotic visions and automobiles. However, current deep learning faces major\nchallenges for such applications because of the huge computational cost and the\ninefficient learning. Hence, we develop a novel brain-inspired Spiking Neural\nNetwork (SNN) based system titled Spiking Gating Flow (SGF) for online action\nlearning. The developed system consists of multiple SGF units which assembled\nin a hierarchical manner. A single SGF unit involves three layers: a feature\nextraction layer, an event-driven layer and a histogram-based training layer.\nTo demonstrate the developed system capabilities, we employ a standard Dynamic\nVision Sensor (DVS) gesture classification as a benchmark. The results indicate\nthat we can achieve 87.5% accuracy which is comparable with Deep Learning (DL),\nbut at smaller training/inference data number ratio 1.5:1. And only a single\ntraining epoch is required during the learning process. Meanwhile, to the best\nof our knowledge, this is the highest accuracy among the non-backpropagation\nalgorithm based SNNs. At last, we conclude the few-shot learning paradigm of\nthe developed network: 1) a hierarchical structure-based network design\ninvolves human prior knowledge; 2) SNNs for content based global dynamic\nfeature detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zihao Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanhong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Q/0/1/0/all/0/1\">Qiaosha Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Tie Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_F/0/1/0/all/0/1\">Fangbo Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiansong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1\">C.-J. Richard Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Junwen Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yuan Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Nerfels: Renderable Neural Codes for Improved Camera Pose Estimation. (arXiv:2206.01916v1 [cs.CV])","link":"http://arxiv.org/abs/2206.01916","description":"<p>This paper presents a framework that combines traditional keypoint-based\ncamera pose optimization with an invertible neural rendering mechanism. Our\nproposed 3D scene representation, Nerfels, is locally dense yet globally\nsparse. As opposed to existing invertible neural rendering systems which\noverfit a model to the entire scene, we adopt a feature-driven approach for\nrepresenting scene-agnostic, local 3D patches with renderable codes. By\nmodelling a scene only where local features are detected, our framework\neffectively generalizes to unseen local regions in the scene via an optimizable\ncode conditioning mechanism in the neural renderer, all while maintaining the\nlow memory footprint of a sparse 3D map representation. Our model can be\nincorporated to existing state-of-the-art hand-crafted and learned local\nfeature pose estimators, yielding improved performance when evaluating on\nScanNet for wide camera baseline scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Avraham_G/0/1/0/all/0/1\">Gil Avraham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Straub_J/0/1/0/all/0/1\">Julian Straub</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_T/0/1/0/all/0/1\">Tianwei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Tsun-Yi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Germain_H/0/1/0/all/0/1\">Hugo Germain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sweeney_C/0/1/0/all/0/1\">Chris Sweeney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balntas_V/0/1/0/all/0/1\">Vasileios Balntas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Novotny_D/0/1/0/all/0/1\">David Novotny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DeTone_D/0/1/0/all/0/1\">Daniel DeTone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Newcombe_R/0/1/0/all/0/1\">Richard Newcombe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Pixels to Objects: Cubic Visual Attention for Visual Question Answering. (arXiv:2206.01923v1 [cs.CV])","link":"http://arxiv.org/abs/2206.01923","description":"<p>Recently, attention-based Visual Question Answering (VQA) has achieved great\nsuccess by utilizing question to selectively target different visual areas that\nare related to the answer. Existing visual attention models are generally\nplanar, i.e., different channels of the last conv-layer feature map of an image\nshare the same weight. This conflicts with the attention mechanism because CNN\nfeatures are naturally spatial and channel-wise. Also, visual attention models\nare usually conducted on pixel-level, which may cause region discontinuous\nproblems. In this paper, we propose a Cubic Visual Attention (CVA) model by\nsuccessfully applying a novel channel and spatial attention on object regions\nto improve VQA task. Specifically, instead of attending to pixels, we first\ntake advantage of the object proposal networks to generate a set of object\ncandidates and extract their associated conv features. Then, we utilize the\nquestion to guide channel attention and spatial attention calculation based on\nthe con-layer feature map. Finally, the attended visual features and the\nquestion are combined to infer the answer. We assess the performance of our\nproposed CVA on three public image QA datasets, including COCO-QA, VQA and\nVisual7W. Experimental results show that our proposed method significantly\noutperforms the state-of-the-arts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jingkuan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_P/0/1/0/all/0/1\">Pengpeng Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Lianli Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Heng Tao Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Occlusion-Resistant Instance Segmentation of Piglets in Farrowing Pens Using Center Clustering Network. (arXiv:2206.01942v1 [cs.CV])","link":"http://arxiv.org/abs/2206.01942","description":"<p>Computer vision enables the development of new approaches to monitor the\nbehavior, health, and welfare of animals. Instance segmentation is a\nhigh-precision method in computer vision for detecting individual animals of\ninterest. This method can be used for in-depth analysis of animals, such as\nexamining their subtle interactive behaviors, from videos and images. However,\nexisting deep-learning-based instance segmentation methods have been mostly\ndeveloped based on public datasets, which largely omit heavy occlusion\nproblems; therefore, these methods have limitations in real-world applications\ninvolving object occlusions, such as farrowing pen systems used on pig farms in\nwhich the farrowing crates often impede the sow and piglets. In this paper, we\npropose a novel occlusion-resistant Center Clustering Network for instance\nsegmentation, dubbed as CClusnet-Inseg. Specifically, CClusnet-Inseg uses each\npixel to predict object centers and trace these centers to form masks based on\nclustering results, which consists of a network for segmentation and center\noffset vector map, Density-Based Spatial Clustering of Applications with Noise\n(DBSCAN) algorithm, Centers-to-Mask (C2M) and Remain-Centers-to-Mask (RC2M)\nalgorithms, and a pseudo-occlusion generator (POG). In all, 4,600 images were\nextracted from six videos collected from six farrowing pens to train and\nvalidate our method. CClusnet-Inseg achieves a mean average precision (mAP) of\n83.6; it outperformed YOLACT++ and Mask R-CNN, which had mAP values of 81.2 and\n74.7, respectively. We conduct comprehensive ablation studies to demonstrate\nthe advantages and effectiveness of core modules of our method. In addition, we\napply CClusnet-Inseg to multi-object tracking for animal monitoring, and the\npredicted object center that is a conjunct output could serve as an\nocclusion-resistant representation of the location of an object.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_E/0/1/0/all/0/1\">Endai Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_A/0/1/0/all/0/1\">Axiu Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yongjian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_H/0/1/0/all/0/1\">Haiming Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ceballos_M/0/1/0/all/0/1\">Maria Camila Ceballos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parsons_T/0/1/0/all/0/1\">Thomas D. Parsons</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1\">Junhui Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kai Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"C$^3$Fusion: Consistent Contrastive Colon Fusion, Towards Deep SLAM in Colonoscopy. (arXiv:2206.01961v1 [cs.CV])","link":"http://arxiv.org/abs/2206.01961","description":"<p>3D colon reconstruction from Optical Colonoscopy (OC) to detect non-examined\nsurfaces remains an unsolved problem. The challenges arise from the nature of\noptical colonoscopy data, characterized by highly reflective low-texture\nsurfaces, drastic illumination changes and frequent tracking loss. Recent\nmethods demonstrate compelling results, but suffer from: (1) frangible\nframe-to-frame (or frame-to-model) pose estimation resulting in many tracking\nfailures; or (2) rely on point-based representations at the cost of scan\nquality. In this paper, we propose a novel reconstruction framework that\naddresses these issues end to end, which result in both quantitatively and\nqualitatively accurate and robust 3D colon reconstruction. Our SLAM approach,\nwhich employs correspondences based on contrastive deep features, and deep\nconsistent depth maps, estimates globally optimized poses, is able to recover\nfrom frequent tracking failures, and estimates a global consistent 3D model;\nall within a single framework. We perform an extensive experimental evaluation\non multiple synthetic and real colonoscopy videos, showing high-quality results\nand comparisons against relevant baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Posner_E/0/1/0/all/0/1\">Erez Posner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zholkover_A/0/1/0/all/0/1\">Adi Zholkover</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frank_N/0/1/0/all/0/1\">Netanel Frank</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouhnik_M/0/1/0/all/0/1\">Moshe Bouhnik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking the Openness of CLIP. (arXiv:2206.01986v1 [cs.CV])","link":"http://arxiv.org/abs/2206.01986","description":"<p>Contrastive Language-Image Pre-training (CLIP) has demonstrated great\npotential in realizing open-vocabulary image classification in a matching\nstyle, because of its holistic use of natural language supervision that covers\nunconstrained real-world visual concepts. However, it is, in turn, also\ndifficult to evaluate and analyze the openness of CLIP-like models, since they\nare in theory open to any vocabulary but the actual accuracy varies. To address\nthe insufficiency of conventional studies on openness, we resort to an\nincremental view and define the extensibility, which essentially approximates\nthe model's ability to deal with new visual concepts, by evaluating openness\nthrough vocabulary expansions. Our evaluation based on extensibility shows that\nCLIP-like models are hardly truly open and their performances degrade as the\nvocabulary expands to different degrees. Further analysis reveals that the\nover-estimation of openness is not because CLIP-like models fail to capture the\ngeneral similarity of image and text features of novel visual concepts, but\nbecause of the confusion among competing text features, that is, they are not\nstable with respect to the vocabulary. In light of this, we propose to improve\nthe openness of CLIP from the perspective of feature space by enforcing the\ndistinguishability of text features. Our method retrieves relevant texts from\nthe pre-training corpus to enhance prompts for inference, which boosts the\nextensibility and stability of CLIP even without fine-tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1\">Shuhuai Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xuancheng Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1\">Guangxiang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xu Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-modal Clinical Graph Transformer for Ophthalmic Report Generation. (arXiv:2206.01988v1 [cs.CV])","link":"http://arxiv.org/abs/2206.01988","description":"<p>Automatic generation of ophthalmic reports using data-driven neural networks\nhas great potential in clinical practice. When writing a report,\nophthalmologists make inferences with prior clinical knowledge. This knowledge\nhas been neglected in prior medical report generation methods. To endow models\nwith the capability of incorporating expert knowledge, we propose a Cross-modal\nclinical Graph Transformer (CGT) for ophthalmic report generation (ORG), in\nwhich clinical relation triples are injected into the visual features as prior\nknowledge to drive the decoding procedure. However, two major common Knowledge\nNoise (KN) issues may affect models' effectiveness. 1) Existing general\nbiomedical knowledge bases such as the UMLS may not align meaningfully to the\nspecific context and language of the report, limiting their utility for\nknowledge injection. 2) Incorporating too much knowledge may divert the visual\nfeatures from their correct meaning. To overcome these limitations, we design\nan automatic information extraction scheme based on natural language processing\nto obtain clinical entities and relations directly from in-domain training\nreports. Given a set of ophthalmic images, our CGT first restores a sub-graph\nfrom the clinical graph and injects the restored triples into visual features.\nThen visible matrix is employed during the encoding procedure to limit the\nimpact of knowledge. Finally, reports are predicted by the encoded cross-modal\nfeatures via a Transformer decoder. Extensive experiments on the large-scale\nFFA-IR benchmark demonstrate that the proposed CGT is able to outperform\nprevious benchmark methods and achieve state-of-the-art performances.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mingjie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_W/0/1/0/all/0/1\">Wenjia Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verspoor_K/0/1/0/all/0/1\">Karin Verspoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1\">Shirui Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xiaojun Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CAINNFlow: Convolutional block Attention modules and Invertible Neural Networks Flow for anomaly detection and localization tasks. (arXiv:2206.01992v1 [cs.CV])","link":"http://arxiv.org/abs/2206.01992","description":"<p>Detection of object anomalies is crucial in industrial processes, but\nunsupervised anomaly detection and localization is particularly important due\nto the difficulty of obtaining a large number of defective samples and the\nunpredictable types of anomalies in real life. Among the existing unsupervised\nanomaly detection and localization methods, the NF-based scheme has achieved\nbetter results. However, the two subnets (complex functions) si(ui) and ti(ui)\nin NF are usually multilayer perceptrons, which need to squeeze the input\nvisual features from 2D flattening to 1D, destroying the spatial location\nrelationship in the feature map and losing the spatial structure information.\nIn order to retain and effectively extract spatial structure information, we\ndesign in this study a complex function model with alternating CBAM embedded in\na stacked 3*3 full convolution, which is able to retain and effectively extract\nspatial structure information in the normalized flow model. Extensive\nexperimental results on the MVTec AD dataset show that CAINNFlow achieves\nadvanced levels of accuracy and inference efficiency based on CNN and\nTransformer backbone networks as feature extractors, and CAINNFlow achieves a\npixel-level AUC of 98.76\\% for anomaly detection in MVTec AD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1\">Ruiqing Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Fan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Mengyuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1\">Dongyu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinfeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jingrong Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1\">Qianjin Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Linghan Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MSR: Making Self-supervised learning Robust to Aggressive Augmentations. (arXiv:2206.01999v1 [cs.CV])","link":"http://arxiv.org/abs/2206.01999","description":"<p>Most recent self-supervised learning methods learn visual representation by\ncontrasting different augmented views of images. Compared with supervised\nlearning, more aggressive augmentations have been introduced to further improve\nthe diversity of training pairs. However, aggressive augmentations may distort\nimages' structures leading to a severe semantic shift problem that augmented\nviews of the same image may not share the same semantics, thus degrading the\ntransfer performance. To address this problem, we propose a new SSL paradigm,\nwhich counteracts the impact of semantic shift by balancing the role of weak\nand aggressively augmented pairs. Specifically, semantically inconsistent pairs\nare of minority and we treat them as noisy pairs. Note that deep neural\nnetworks (DNNs) have a crucial memorization effect that DNNs tend to first\nmemorize clean (majority) examples before overfitting to noisy (minority)\nexamples. Therefore, we set a relatively large weight for aggressively\naugmented data pairs at the early learning stage. With the training going on,\nthe model begins to overfit noisy pairs. Accordingly, we gradually reduce the\nweights of aggressively augmented pairs. In doing so, our method can better\nembrace the aggressive augmentations and neutralize the semantic shift problem.\nExperiments show that our model achieves 73.1% top-1 accuracy on ImageNet-1K\nwith ResNet-50 for 200 epochs, which is a 2.5% improvement over BYOL. Moreover,\nexperiments also demonstrate that the learned representations can transfer well\nfor various downstream tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yingbin Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_E/0/1/0/all/0/1\">Erkun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhaoqing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yuxuan Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1\">Bo Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_C/0/1/0/all/0/1\">Cheng Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dadong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tongliang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CVNets: High Performance Library for Computer Vision. (arXiv:2206.02002v1 [cs.CV])","link":"http://arxiv.org/abs/2206.02002","description":"<p>We introduce CVNets, a high-performance open-source library for training deep\nneural networks for visual recognition tasks, including classification,\ndetection, and segmentation. CVNets supports image and video understanding\ntools, including data loading, data transformations, novel data sampling\nmethods, and implementations of several standard networks with similar or\nbetter performance than previous studies.\n</p>\n<p>Our source code is available at: \\url{https://github.com/apple/ml-cvnets}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mehta_S/0/1/0/all/0/1\">Sachin Mehta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdolhosseini_F/0/1/0/all/0/1\">Farzad Abdolhosseini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rastegari_M/0/1/0/all/0/1\">Mohammad Rastegari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"APES: Articulated Part Extraction from Sprite Sheets. (arXiv:2206.02015v1 [cs.CV])","link":"http://arxiv.org/abs/2206.02015","description":"<p>Rigged puppets are one of the most prevalent representations to create 2D\ncharacter animations. Creating these puppets requires partitioning characters\ninto independently moving parts. In this work, we present a method to\nautomatically identify such articulated parts from a small set of character\nposes shown in a sprite sheet, which is an illustration of the character that\nartists often draw before puppet creation. Our method is trained to infer\narticulated parts, e.g. head, torso and limbs, that can be re-assembled to best\nreconstruct the given poses. Our results demonstrate significantly better\nperformance than alternatives qualitatively and quantitatively.Our project page\nhttps://zhan-xu.github.io/parts/ includes our code and data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fisher_M/0/1/0/all/0/1\">Matthew Fisher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aneja_D/0/1/0/all/0/1\">Deepali Aneja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dudhat_R/0/1/0/all/0/1\">Rushikesh Dudhat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_L/0/1/0/all/0/1\">Li Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalogerakis_E/0/1/0/all/0/1\">Evangelos Kalogerakis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Implicit Neural Representation for Mesh-Free Inverse Obstacle Scattering. (arXiv:2206.02027v1 [cs.CV])","link":"http://arxiv.org/abs/2206.02027","description":"<p>Implicit representation of shapes as level sets of multilayer perceptrons has\nrecently flourished in different shape analysis, compression, and\nreconstruction tasks. In this paper, we introduce an implicit neural\nrepresentation-based framework for solving the inverse obstacle scattering\nproblem in a mesh-free fashion. We efficiently express the obstacle shape as\nthe zero-level set of a signed distance function which is implicitly determined\nby a small number of network parameters. To solve the direct scattering\nproblem, we implement the implicit boundary integral method. It uses\nprojections of the grid points in the tubular neighborhood onto the boundary to\ncompute the PDE solution instead of a grid-size-dependent extraction method of\nsurface points such as Marching Cubes. The implicit representation conveniently\nhandles the shape perturbation in the optimization process. To update the\nshape, we use PyTorch's automatic differentiation to backpropagate the loss\nfunction w.r.t. the network parameters, allowing us to avoid complex and\nerror-prone manual derivation of the shape derivative. The proposed framework\nmakes the inverse scattering problem more tractable with fewer parameters to\noptimize in comparison to the memory-inefficient grid-based approaches and\noutputs high-quality reconstruction results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vlasic_T/0/1/0/all/0/1\">Tin Vla&#x161;i&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Hieu Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dokmanic_I/0/1/0/all/0/1\">Ivan Dokmani&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Guided Deep Metric Learning. (arXiv:2206.02029v1 [cs.CV])","link":"http://arxiv.org/abs/2206.02029","description":"<p>Deep Metric Learning (DML) methods have been proven relevant for visual\nsimilarity learning. However, they sometimes lack generalization properties\nbecause they are trained often using an inappropriate sample selection strategy\nor due to the difficulty of the dataset caused by a distributional shift in the\ndata. These represent a significant drawback when attempting to learn the\nunderlying data manifold. Therefore, there is a pressing need to develop better\nways of obtaining generalization and representation of the underlying manifold.\nIn this paper, we propose a novel approach to DML that we call Guided Deep\nMetric Learning, a novel architecture oriented to learning more compact\nclusters, improving generalization under distributional shifts in DML. This\nnovel architecture consists of two independent models: A multi-branch master\nmodel, inspired from a Few-Shot Learning (FSL) perspective, generates a reduced\nhypothesis space based on prior knowledge from labeled data, which guides or\nregularizes the decision boundary of a student model during training under an\noffline knowledge distillation scheme. Experiments have shown that the proposed\nmethod is capable of a better manifold generalization and representation to up\nto 40% improvement (Recall@1, CIFAR10), using guidelines suggested by Musgrave\net al. to perform a more fair and realistic comparison, which is currently\nabsent in the literature\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Zapata_J/0/1/0/all/0/1\">Jorge Gonzalez-Zapata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reyes_Amezcua_I/0/1/0/all/0/1\">Ivan Reyes-Amezcua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flores_Araiza_D/0/1/0/all/0/1\">Daniel Flores-Araiza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mendez_Ruiz_M/0/1/0/all/0/1\">Mauricio Mendez-Ruiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ochoa_Ruiz_G/0/1/0/all/0/1\">Gilberto Ochoa-Ruiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mendez_Vazquez_A/0/1/0/all/0/1\">Andres Mendez-Vazquez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Speaker-specific Lip-to-Speech Generation. (arXiv:2206.02050v1 [cs.CV])","link":"http://arxiv.org/abs/2206.02050","description":"<p>Understanding the lip movement and inferring the speech from it is\nnotoriously difficult for the common person. The task of accurate lip-reading\ngets help from various cues of the speaker and its contextual or environmental\nsetting. Every speaker has a different accent and speaking style, which can be\ninferred from their visual and speech features. This work aims to understand\nthe correlation/mapping between speech and the sequence of lip movement of\nindividual speakers in an unconstrained and large vocabulary. We model the\nframe sequence as a prior to the transformer in an auto-encoder setting and\nlearned a joint embedding that exploits temporal properties of both audio and\nvideo. We learn temporal synchronization using deep metric learning, which\nguides the decoder to generate speech in sync with input lip movements. The\npredictive posterior thus gives us the generated speech in speaker speaking\nstyle. We have trained our model on the Grid and Lip2Wav Chemistry lecture\ndataset to evaluate single speaker natural speech generation tasks from lip\nmovement in an unconstrained natural setting. Extensive evaluation using\nvarious qualitative and quantitative metrics with human evaluation also shows\nthat our method outperforms the Lip2Wav Chemistry dataset(large vocabulary in\nan unconstrained setting) by a good margin across almost all evaluation metrics\nand marginally outperforms the state-of-the-art on GRID dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Varshney_M/0/1/0/all/0/1\">Munender Varshney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yadav_R/0/1/0/all/0/1\">Ravindra Yadav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Namboodiri_V/0/1/0/all/0/1\">Vinay P. Namboodiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hegde_R/0/1/0/all/0/1\">Rajesh M Hegde</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PIDNet: A Real-time Semantic Segmentation Network Inspired from PID Controller. (arXiv:2206.02066v1 [cs.CV])","link":"http://arxiv.org/abs/2206.02066","description":"<p>Two-branch network architecture has shown its efficiency and effectiveness\nfor real-time semantic segmentation tasks. However, direct fusion of low-level\ndetails and high-level semantics will lead to a phenomenon that the detailed\nfeatures are easily overwhelmed by surrounding contextual information, namely\novershoot in this paper, which limits the improvement of the accuracy of\nexisted two-branch models. In this paper, we bridge a connection between\nConvolutional Neural Network (CNN) and Proportional-Integral-Derivative (PID)\ncontroller and reveal that the two-branch network is nothing but a\nProportional-Integral (PI) controller, which inherently suffers from the\nsimilar overshoot issue. To alleviate this issue, we propose a novel\nthree-branch network architecture: PIDNet, which possesses three branches to\nparse the detailed, context and boundary information (derivative of semantics),\nrespectively, and employs boundary attention to guide the fusion of detailed\nand context branches in final stage. The family of PIDNets achieve the best\ntrade-off between inference speed and accuracy and their test accuracy\nsurpasses all the existed models with similar inference speed on Cityscapes,\nCamVid and COCO-Stuff datasets. Especially, PIDNet-S achieves 78.6% mIOU with\ninference speed of 93.2 FPS on Cityscapes test set and 81.6% mIOU with speed of\n153.7 FPS on CamVid test set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jiacong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Z/0/1/0/all/0/1\">Zixiang Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharyya_S/0/1/0/all/0/1\">Shankar P. Bhattacharyya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"All One Needs to Know about Priors for Deep Image Restoration and Enhancement: A Survey. (arXiv:2206.02070v1 [cs.CV])","link":"http://arxiv.org/abs/2206.02070","description":"<p>Image restoration and enhancement is a process of improving the image quality\nby removing degradations, such as noise, blur, and resolution degradation. Deep\nlearning (DL) has recently been applied to image restoration and enhancement.\nDue to its ill-posed property, plenty of works have explored priors to\nfacilitate training deep neural networks (DNNs). However, the importance of\npriors has not been systematically studied and analyzed by far in the research\ncommunity. Therefore, this paper serves as the first study that provides a\ncomprehensive overview of recent advancements of priors for deep image\nrestoration and enhancement. Our work covers five primary contents: (1) A\ntheoretical analysis of priors for deep image restoration and enhancement; (2)\nA hierarchical and structural taxonomy of priors commonly used in the DL-based\nmethods; (3) An insightful discussion on each prior regarding its principle,\npotential, and applications; (4) A summary of crucial problems by highlighting\nthe potential future directions to spark more research in the community; (5) An\nopen-source repository that provides a taxonomy of all mentioned works and code\nlinks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yunfan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yiqi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yunhao Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1\">Xu Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lin Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Fast Adaptation of Pretrained Contrastive Models for Multi-channel Video-Language Retrieval. (arXiv:2206.02082v1 [cs.CV])","link":"http://arxiv.org/abs/2206.02082","description":"<p>Multi-channel video-language retrieval require models to understand\ninformation from different modalities (e.g. video+question, video+speech) and\nreal-world knowledge to correctly link a video with a textual response or\nquery. Fortunately, multimodal contrastive models have been shown to be highly\neffective at aligning entities in images/videos and text, e.g., CLIP; text\ncontrastive models have been extensively studied recently for their strong\nability of producing discriminative sentence embeddings, e.g., SimCSE. Their\nabilities are exactly needed by multi-channel video-language retrieval.\nHowever, it is not clear how to quickly adapt these two lines of models to\nmulti-channel video-language retrieval-style tasks. In this paper, we identify\na principled model design space with two axes: how to represent videos and how\nto fuse video and text information. Based on categorization of recent methods,\nwe investigate the options of representing videos using continuous feature\nvectors or discrete text tokens; for the fusion method, we explore a multimodal\ntransformer or a pretrained contrastive text model. We extensively evaluate the\nfour combinations on five video-language datasets. We surprisingly find that\ndiscrete text tokens coupled with a pretrained contrastive text model yields\nthe best performance. This combination can even outperform state-of-the-art on\nthe iVQA dataset without the additional training on millions of video-language\ndata. Further analysis shows that this is because representing videos as text\ntokens captures the key visual information with text tokens that are naturally\naligned with text models and the text models obtained rich knowledge during\ncontrastive pretraining process. All the empirical analysis we obtain for the\nfour variants establishes a solid foundation for future research on leveraging\nthe rich knowledge of pretrained contrastive models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xudong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tiwari_S/0/1/0/all/0/1\">Simran Tiwari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shiyuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Manling Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shou_M/0/1/0/all/0/1\">Mike Zheng Shou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shih-Fu Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards the Creation of a Nutrition and Food Group Based Image Database. (arXiv:2206.02086v1 [cs.CV])","link":"http://arxiv.org/abs/2206.02086","description":"<p>Food classification is critical to the analysis of nutrients comprising foods\nreported in dietary assessment. Advances in mobile and wearable sensors,\ncombined with new image based methods, particularly deep learning based\napproaches, have shown great promise to improve the accuracy of food\nclassification to assess dietary intake. However, these approaches are\ndata-hungry and their performances are heavily reliant on the quantity and\nquality of the available datasets for training the food classification model.\nExisting food image datasets are not suitable for fine-grained food\nclassification and the following nutrition analysis as they lack fine-grained\nand transparently derived food group based identification which are often\nprovided by trained dietitians with expert domain knowledge. In this paper, we\npropose a framework to create a nutrition and food group based image database\nthat contains both visual and hierarchical food categorization information to\nenhance links to the nutrient profile of each food. We design a protocol for\nlinking food group based food codes in the U.S. Department of Agriculture's\n(USDA) Food and Nutrient Database for Dietary Studies (FNDDS) to a food image\ndataset, and implement a web-based annotation tool for efficient deployment of\nthis protocol.Our proposed method is used to build a nutrition and food group\nbased image database including 16,114 food images representing the 74 most\nfrequently consumed What We Eat in America (WWEIA) food sub-categories in the\nUnited States with 1,865 USDA food code matched to a nutrient database, the\nUSDA FNDDS nutrient database.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shao_Z/0/1/0/all/0/1\">Zeman Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jiangpeng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Ya-Yuan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Luotao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cowan_A/0/1/0/all/0/1\">Alexandra Cowan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eicher_Miller_H/0/1/0/all/0/1\">Heather Eicher-Miller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Fengqing Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Accurate Scoliosis Vertebral Landmark Localization on X-ray Images via Shape-constrained Multi-stage Cascaded CNNs. (arXiv:2206.02087v1 [cs.CV])","link":"http://arxiv.org/abs/2206.02087","description":"<p>Vertebral landmark localization is a crucial step for variant spine-related\nclinical applications, which requires detecting the corner points of 17\nvertebrae. However, the neighbor landmarks often disturb each other for the\nhomogeneous appearance of vertebrae, which makes vertebral landmark\nlocalization extremely difficult. In this paper, we propose multi-stage\ncascaded convolutional neural networks (CNNs) to split the single task into two\nsequential steps, i.e., center point localization to roughly locate 17 center\npoints of vertebrae, and corner point localization to find 4 corner points for\neach vertebra without distracted by others. Landmarks in each step are located\ngradually from a set of initialized points by regressing offsets via cascaded\nCNNs. Principal Component Analysis (PCA) is employed to preserve a shape\nconstraint in offset regression to resist the mutual attraction of vertebrae.\nWe evaluate our method on the AASCE dataset that consists of 609 tight spinal\nanterior-posterior X-ray images and each image contains 17 vertebrae composed\nof the thoracic and lumbar spine for spinal shape characterization.\nExperimental results demonstrate our superior performance of vertebral landmark\nlocalization over other state-of-the-arts with the relative error decreasing\nfrom 3.2e-3 to 7.2e-4.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhiwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_J/0/1/0/all/0/1\">Jinxin Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yunqiao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yuanhuai Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xin Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Point-to-Voxel Knowledge Distillation for LiDAR Semantic Segmentation. (arXiv:2206.02099v1 [cs.CV])","link":"http://arxiv.org/abs/2206.02099","description":"<p>This article addresses the problem of distilling knowledge from a large\nteacher model to a slim student network for LiDAR semantic segmentation.\nDirectly employing previous distillation approaches yields inferior results due\nto the intrinsic challenges of point cloud, i.e., sparsity, randomness and\nvarying density. To tackle the aforementioned problems, we propose the\nPoint-to-Voxel Knowledge Distillation (PVD), which transfers the hidden\nknowledge from both point level and voxel level. Specifically, we first\nleverage both the pointwise and voxelwise output distillation to complement the\nsparse supervision signals. Then, to better exploit the structural information,\nwe divide the whole point cloud into several supervoxels and design a\ndifficulty-aware sampling strategy to more frequently sample supervoxels\ncontaining less-frequent classes and faraway objects. On these supervoxels, we\npropose inter-point and inter-voxel affinity distillation, where the similarity\ninformation between points and voxels can help the student model better capture\nthe structural information of the surrounding environment. We conduct extensive\nexperiments on two popular LiDAR segmentation benchmarks, i.e., nuScenes and\nSemanticKITTI. On both benchmarks, our PVD consistently outperforms previous\ndistillation approaches by a large margin on three representative backbones,\ni.e., Cylinder3D, SPVNAS and MinkowskiNet. Notably, on the challenging nuScenes\nand SemanticKITTI datasets, our method can achieve roughly 75% MACs reduction\nand 2x speedup on the competitive Cylinder3D model and rank 1st on the\nSemanticKITTI leaderboard among all published algorithms. Our code is available\nat https://github.com/cardwing/Codes-for-PVKD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yuenan Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xinge Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yuexin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1\">Chen Change Loy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yikang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AUTM Flow: Atomic Unrestricted Time Machine for Monotonic Normalizing Flows. (arXiv:2206.02102v1 [cs.LG])","link":"http://arxiv.org/abs/2206.02102","description":"<p>Nonlinear monotone transformations are used extensively in normalizing flows\nto construct invertible triangular mappings from simple distributions to\ncomplex ones. In existing literature, monotonicity is usually enforced by\nrestricting function classes or model parameters and the inverse transformation\nis often approximated by root-finding algorithms as a closed-form inverse is\nunavailable. In this paper, we introduce a new integral-based approach termed\n\"Atomic Unrestricted Time Machine (AUTM)\", equipped with unrestricted\nintegrands and easy-to-compute explicit inverse. AUTM offers a versatile and\nefficient way to the design of normalizing flows with explicit inverse and\nunrestricted function classes or parameters. Theoretically, we present a\nconstructive proof that AUTM is universal: all monotonic normalizing flows can\nbe viewed as limits of AUTM flows. We provide a concrete example to show how to\napproximate any given monotonic normalizing flow using AUTM flows with\nguaranteed convergence. The result implies that AUTM can be used to transform\nan existing flow into a new one equipped with explicit inverse and unrestricted\nparameters. The performance of the new approach is evaluated on high\ndimensional density estimation, variational inference and image generation.\nExperiments demonstrate superior speed and memory efficiency of AUTM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1\">Difeng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1\">Yuliang Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Huan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1\">Qiang Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xi_Y/0/1/0/all/0/1\">Yuanzhe Xi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ContraCLIP: Interpretable GAN generation driven by pairs of contrasting sentences. (arXiv:2206.02104v1 [cs.CV])","link":"http://arxiv.org/abs/2206.02104","description":"<p>This work addresses the problem of discovering non-linear interpretable paths\nin the latent space of pre-trained GANs in a model-agnostic manner. In the\nproposed method, the discovery is driven by a set of pairs of natural language\nsentences with contrasting semantics, named semantic dipoles, that serve as the\nlimits of the interpretation that we require by the trainable latent paths to\nencode. By using the pre-trained CLIP encoder, the sentences are projected into\nthe vision-language space, where they serve as dipoles, and where RBF-based\nwarping functions define a set of non-linear directional paths, one for each\nsemantic dipole, allowing in this way traversals from one semantic pole to the\nother. By defining an objective that discovers paths in the latent space of\nGANs that generate changes along the desired paths in the vision-language\nembedding space, we provide an intuitive way of controlling the underlying\ngenerative factors and address some of the limitations of the state-of-the-art\nworks, namely, that a) they are typically tailored to specific GAN\narchitectures (i.e., StyleGAN), b) they disregard the relative position of the\nmanipulated and the original image in the image embedding and the relative\nposition of the image and the text embeddings, and c) they lead to abrupt image\nmanipulations and quickly arrive at regions of low density and, thus, low image\nquality, providing limited control of the generative factors. We provide\nextensive qualitative and quantitative results that demonstrate our claims with\ntwo pre-trained GANs, and make the code and the pre-trained models publicly\navailable at: https://github.com/chi0tzp/ContraCLIP\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tzelepis_C/0/1/0/all/0/1\">Christos Tzelepis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oldfield_J/0/1/0/all/0/1\">James Oldfield</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tzimiropoulos_G/0/1/0/all/0/1\">Georgios Tzimiropoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patras_I/0/1/0/all/0/1\">Ioannis Patras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Computer Vision-based Characterization of Large-scale Jet Flames using a Synthetic Infrared Image Generation Approach. (arXiv:2206.02110v1 [cs.CV])","link":"http://arxiv.org/abs/2206.02110","description":"<p>Among the different kinds of fire accidents that can occur during industrial\nactivities that involve hazardous materials, jet fires are one of the\nlesser-known types. This is because they are often involved in a process that\ngenerates a sequence of other accidents of greater magnitude, known as domino\neffect. Flame impingement usually causes domino effects, and jet fires present\nspecific features that can significantly increase the probability of this\nhappening. These features become relevant from a risk analysis perspective,\nmaking their proper characterization a crucial task. Deep Learning approaches\nhave become extensively used for tasks such as jet fire characterization;\nhowever, these methods are heavily dependent on the amount of data and the\nquality of the labels. Data acquisition of jet fires involve expensive\nexperiments, especially so if infrared imagery is used. Therefore, this paper\nproposes the use of Generative Adversarial Networks to produce plausible\ninfrared images from visible ones, making experiments less expensive and\nallowing for other potential applications. The results suggest that it is\npossible to realistically replicate the results for experiments carried out\nusing both visible and infrared cameras. The obtained results are compared with\nsome previous experiments, and it is shown that similar results were obtained.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Perez_Guerrero_C/0/1/0/all/0/1\">Carmina P&#xe9;rez-Guerrero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ciprian_Sanchez_J/0/1/0/all/0/1\">Jorge Francisco Cipri&#xe1;n-S&#xe1;nchez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palacios_A/0/1/0/all/0/1\">Adriana Palacios</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ochoa_Ruiz_G/0/1/0/all/0/1\">Gilberto Ochoa-Ruiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Mendoza_M/0/1/0/all/0/1\">Miguel Gonzalez-Mendoza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foroughi_V/0/1/0/all/0/1\">Vahid Foroughi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pastor_E/0/1/0/all/0/1\">Elsa Pastor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_Hernandez_G/0/1/0/all/0/1\">Gerardo Rodriguez-Hernandez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cannot See the Forest for the Trees: Aggregating Multiple Viewpoints to Better Classify Objects in Videos. (arXiv:2206.02116v1 [cs.CV])","link":"http://arxiv.org/abs/2206.02116","description":"<p>Recently, both long-tailed recognition and object tracking have made great\nadvances individually. TAO benchmark presented a mixture of the two,\nlong-tailed object tracking, in order to further reflect the aspect of the\nreal-world. To date, existing solutions have adopted detectors showing\nrobustness in long-tailed distributions, which derive per-frame results. Then,\nthey used tracking algorithms that combine the temporally independent\ndetections to finalize tracklets. However, as the approaches did not take\ntemporal changes in scenes into account, inconsistent classification results in\nvideos led to low overall performance. In this paper, we present a set\nclassifier that improves accuracy of classifying tracklets by aggregating\ninformation from multiple viewpoints contained in a tracklet. To cope with\nsparse annotations in videos, we further propose augmentation of tracklets that\ncan maximize data efficiency. The set classifier is plug-and-playable to\nexisting object trackers, and highly improves the performance of long-tailed\nobject tracking. By simply attaching our method to QDTrack on top of\nResNet-101, we achieve the new state-of-the-art, 19.9% and 15.7% TrackAP_50 on\nTAO validation and test sets, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Sukjun Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heo_M/0/1/0/all/0/1\">Miran Heo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1\">Seoung Wug Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seon Joo Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ShapePU: A New PU Learning Framework Regularized by Global Consistency for Scribble Supervised Cardiac Segmentation. (arXiv:2206.02118v1 [cs.CV])","link":"http://arxiv.org/abs/2206.02118","description":"<p>Cardiac segmentation is an essential step for the diagnosis of cardiovascular\ndiseases. However, pixel-wise dense labeling is both costly and time-consuming.\nScribble, as a form of sparse annotation, is more accessible than full\nannotations. However, it's particularly challenging to train a segmentation\nnetwork with weak supervision from scribbles. To tackle this problem, we\npropose a new scribble-guided method for cardiac segmentation, based on the\nPositive-Unlabeled (PU) learning framework and global consistency\nregularization, and termed as ShapePU. To leverage unlabeled pixels via PU\nlearning, we first present an Expectation-Maximization (EM) algorithm to\nestimate the proportion of each class in the unlabeled pixels. Given the\nestimated ratios, we then introduce the marginal probability maximization to\nidentify the classes of unlabeled pixels. To exploit shape knowledge, we apply\ncutout operations to training images, and penalize the inconsistent\nsegmentation results. Evaluated on two open datasets, i.e, ACDC and MSCMRseg,\nour scribble-supervised ShapePU surpassed the fully supervised approach\nrespectively by 1.4% and 9.8% in average Dice, and outperformed the\nstate-of-the-art weakly supervised and PU learning methods by large margins.\nOur code is available at https://github.com/BWGZK/ShapePU.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Ke Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_X/0/1/0/all/0/1\">Xiahai Zhuang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MPANet: Multi-Patch Attention For Infrared Small Target object Detection. (arXiv:2206.02120v1 [cs.CV])","link":"http://arxiv.org/abs/2206.02120","description":"<p>Infrared small target detection (ISTD) has attracted widespread attention and\nbeen applied in various fields. Due to the small size of infrared targets and\nthe noise interference from complex backgrounds, the performance of ISTD using\nconvolutional neural networks (CNNs) is restricted. Moreover, the constriant\nthat long-distance dependent features can not be encoded by the vanilla CNNs\nalso impairs the robustness of capturing targets' shapes and locations in\ncomplex scenarios. To this end, a multi-patch attention network (MPANet) based\non the axial-attention encoder and the multi-scale patch branch (MSPB)\nstructure is proposed. Specially, an axial-attention-improved encoder\narchitecture is designed to highlight the effective features of small targets\nand suppress background noises. Furthermore, the developed MSPB structure fuses\nthe coarse-grained and fine-grained features from different semantic scales.\nExtensive experiments on the SIRST dataset show the superiority performance and\neffectiveness of the proposed MPANet compared to the state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1\">Ao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhanchao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_R/0/1/0/all/0/1\">Ran Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Federated Adversarial Training with Transformers. (arXiv:2206.02131v1 [cs.LG])","link":"http://arxiv.org/abs/2206.02131","description":"<p>Federated learning (FL) has emerged to enable global model training over\ndistributed clients' data while preserving its privacy. However, the global\ntrained model is vulnerable to the evasion attacks especially, the adversarial\nexamples (AEs), carefully crafted samples to yield false classification.\nAdversarial training (AT) is found to be the most promising approach against\nevasion attacks and it is widely studied for convolutional neural network\n(CNN). Recently, vision transformers have been found to be effective in many\ncomputer vision tasks. To the best of the authors' knowledge, there is no work\nthat studied the feasibility of AT in a FL process for vision transformers.\nThis paper investigates such feasibility with different federated model\naggregation methods and different vision transformer models with different\ntokenization and classification head techniques. In order to improve the robust\naccuracy of the models with the not independent and identically distributed\n(Non-IID), we propose an extension to FedAvg aggregation method, called\nFedWAvg. By measuring the similarities between the last layer of the global\nmodel and the last layer of the client updates, FedWAvg calculates the weights\nto aggregate the local models updates. The experiments show that FedWAvg\nimproves the robust accuracy when compared with other state-of-the-art\naggregation methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aldahdooh_A/0/1/0/all/0/1\">Ahmed Aldahdooh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamidouche_W/0/1/0/all/0/1\">Wassim Hamidouche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deforges_O/0/1/0/all/0/1\">Olivier D&#xe9;forges</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LDRNet: Enabling Real-time Document Localization on Mobile Devices. (arXiv:2206.02136v1 [cs.CV])","link":"http://arxiv.org/abs/2206.02136","description":"<p>While Identity Document Verification (IDV) technology on mobile devices\nbecomes ubiquitous in modern business operations, the risk of identity theft\nand fraud is increasing. The identity document holder is normally required to\nparticipate in an online video interview to circumvent impostors. However, the\ncurrent IDV process depends on an additional human workforce to support online\nstep-by-step guidance which is inefficient and expensive. The performance of\nexisting AI-based approaches cannot meet the real-time and lightweight demands\nof mobile devices. In this paper, we address those challenges by designing an\nedge intelligence-assisted approach for real-time IDV. Aiming at improving the\nresponsiveness of the IDV process, we propose a new document localization model\nfor mobile devices, LDRNet, to Localize the identity Document in Real-time. On\nthe basis of a lightweight backbone network, we build three prediction branches\nfor LDRNet, the corner points prediction, the line borders prediction and the\ndocument classification. We design novel supplementary targets, the\nequal-division points, and use a new loss function named Line Loss, to improve\nthe speed and accuracy of our approach. In addition to the IDV process, LDRNet\nis an efficient and reliable document localization alternative for all kinds of\nmobile applications. As a matter of proof, we compare the performance of LDRNet\nwith other popular approaches on localizing general document datasets. The\nexperimental results show that LDRNet runs at a speed up to 790 FPS which is\n47x faster, while still achieving comparable Jaccard Index(JI) in single-model\nand single-scale tests.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Han Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_H/0/1/0/all/0/1\">Holland Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Huaming Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recurrent Video Restoration Transformer with Guided Deformable Attention. (arXiv:2206.02146v1 [cs.CV])","link":"http://arxiv.org/abs/2206.02146","description":"<p>Video restoration aims at restoring multiple high-quality frames from\nmultiple low-quality frames. Existing video restoration methods generally fall\ninto two extreme cases, i.e., they either restore all frames in parallel or\nrestore the video frame by frame in a recurrent way, which would result in\ndifferent merits and drawbacks. Typically, the former has the advantage of\ntemporal information fusion. However, it suffers from large model size and\nintensive memory consumption; the latter has a relatively small model size as\nit shares parameters across frames; however, it lacks long-range dependency\nmodeling ability and parallelizability. In this paper, we attempt to integrate\nthe advantages of the two cases by proposing a recurrent video restoration\ntransformer, namely RVRT. RVRT processes local neighboring frames in parallel\nwithin a globally recurrent framework which can achieve a good trade-off\nbetween model size, effectiveness, and efficiency. Specifically, RVRT divides\nthe video into multiple clips and uses the previously inferred clip feature to\nestimate the subsequent clip feature. Within each clip, different frame\nfeatures are jointly updated with implicit feature aggregation. Across\ndifferent clips, the guided deformable attention is designed for clip-to-clip\nalignment, which predicts multiple relevant locations from the whole inferred\nclip and aggregates their features by the attention mechanism. Extensive\nexperiments on video super-resolution, deblurring, and denoising show that the\nproposed RVRT achieves state-of-the-art performance on benchmark datasets with\nbalanced model size, testing memory and runtime.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jingyun Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1\">Yuchen Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_X/0/1/0/all/0/1\">Xiaoyu Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranjan_R/0/1/0/all/0/1\">Rakesh Ranjan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilg_E/0/1/0/all/0/1\">Eddy Ilg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Green_S/0/1/0/all/0/1\">Simon Green</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Jiezhang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Timofte_R/0/1/0/all/0/1\">Radu Timofte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HPGNN: Using Hierarchical Graph Neural Networks for Outdoor Point Cloud Processing. (arXiv:2206.02153v1 [cs.CV])","link":"http://arxiv.org/abs/2206.02153","description":"<p>Inspired by recent improvements in point cloud processing for autonomous\nnavigation, we focus on using hierarchical graph neural networks for processing\nand feature learning over large-scale outdoor LiDAR point clouds. We observe\nthat existing GNN based methods fail to overcome challenges of scale and\nirregularity of points in outdoor datasets. Addressing the need to preserve\nstructural details while learning over a larger volume efficiently, we propose\nHierarchical Point Graph Neural Network (HPGNN). It learns node features at\nvarious levels of graph coarseness to extract information. This enables to\nlearn over a large point cloud while retaining fine details that existing\npoint-level graph networks struggle to achieve. Connections between multiple\nlevels enable a point to learn features in multiple scales, in a few\niterations. We design HPGNN as a purely GNN-based approach, so that it offers\nmodular expandability as seen with other point-based and Graph network\nbaselines. To illustrate the improved processing capability, we compare\nprevious point based and GNN models for semantic segmentation with our HPGNN,\nachieving a significant improvement for GNNs (+36.7 mIoU) on the SemanticKITTI\ndataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thieshanthan_A/0/1/0/all/0/1\">Arulmolivarman Thieshanthan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niwarthana_A/0/1/0/all/0/1\">Amashi Niwarthana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Somarathne_P/0/1/0/all/0/1\">Pamuditha Somarathne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wickremasinghe_T/0/1/0/all/0/1\">Tharindu Wickremasinghe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodrigo_R/0/1/0/all/0/1\">Ranga Rodrigo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vanilla Feature Distillation for Improving the Accuracy-Robustness Trade-Off in Adversarial Training. (arXiv:2206.02158v1 [cs.CV])","link":"http://arxiv.org/abs/2206.02158","description":"<p>Adversarial training has been widely explored for mitigating attacks against\ndeep models. However, most existing works are still trapped in the dilemma\nbetween higher accuracy and stronger robustness since they tend to fit a model\ntowards robust features (not easily tampered with by adversaries) while\nignoring those non-robust but highly predictive features. To achieve a better\nrobustness-accuracy trade-off, we propose the Vanilla Feature Distillation\nAdversarial Training (VFD-Adv), which conducts knowledge distillation from a\npre-trained model (optimized towards high accuracy) to guide adversarial\ntraining towards higher accuracy, i.e., preserving those non-robust but\npredictive features. More specifically, both adversarial examples and their\nclean counterparts are forced to be aligned in the feature space by distilling\npredictive representations from the pre-trained/clean model, while previous\nworks barely utilize predictive features from clean models. Therefore, the\nadversarial training model is updated towards maximally preserving the accuracy\nas gaining robustness. A key advantage of our method is that it can be\nuniversally adapted to and boost existing works. Exhaustive experiments on\nvarious datasets, classification models, and adversarial training algorithms\ndemonstrate the effectiveness of our proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_G/0/1/0/all/0/1\">Guodong Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhibo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xiaowei Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhifei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Hengchang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1\">Zhan Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_K/0/1/0/all/0/1\">Kui Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MotionCNN: A Strong Baseline for Motion Prediction in Autonomous Driving. (arXiv:2206.02163v1 [cs.CV])","link":"http://arxiv.org/abs/2206.02163","description":"<p>To plan a safe and efficient route, an autonomous vehicle should anticipate\nfuture motions of other agents around it. Motion prediction is an extremely\nchallenging task that recently gained significant attention within the research\ncommunity. In this work, we present a simple and yet very strong baseline for\nmultimodal motion prediction based purely on Convolutional Neural Networks.\nWhile being easy-to-implement, the proposed approach achieves competitive\nperformance compared to the state-of-the-art methods and ranks 3rd on the 2021\nWaymo Open Dataset Motion Prediction Challenge. Our source code is publicly\navailable at GitHub\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Konev_S/0/1/0/all/0/1\">Stepan Konev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brodt_K/0/1/0/all/0/1\">Kirill Brodt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanakoyeu_A/0/1/0/all/0/1\">Artsiom Sanakoyeu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-Supervised Learning for Mars Imagery Classification and Segmentation. (arXiv:2206.02180v1 [cs.CV])","link":"http://arxiv.org/abs/2206.02180","description":"<p>With the progress of Mars exploration, numerous Mars image data are collected\nand need to be analyzed. However, due to the imbalance and distortion of\nMartian data, the performance of existing computer vision models is\nunsatisfactory. In this paper, we introduce a semi-supervised framework for\nmachine vision on Mars and try to resolve two specific tasks: classification\nand segmentation. Contrastive learning is a powerful representation learning\ntechnique. However, there is too much information overlap between Martian data\nsamples, leading to a contradiction between contrastive learning and Martian\ndata. Our key idea is to reconcile this contradiction with the help of\nannotations and further take advantage of unlabeled data to improve\nperformance. For classification, we propose to ignore inner-class pairs on\nlabeled data as well as neglect negative pairs on unlabeled data, forming\nsupervised inter-class contrastive learning and unsupervised similarity\nlearning. For segmentation, we extend supervised inter-class contrastive\nlearning into an element-wise mode and use online pseudo labels for supervision\non unlabeled areas. Experimental results show that our learning strategies can\nimprove the classification and segmentation models by a large margin and\noutperform state-of-the-art approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenjing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Lilang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1\">Zejia Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiaying Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Functional Ensemble Distillation. (arXiv:2206.02183v1 [cs.LG])","link":"http://arxiv.org/abs/2206.02183","description":"<p>Bayesian models have many desirable properties, most notable is their ability\nto generalize from limited data and to properly estimate the uncertainty in\ntheir predictions. However, these benefits come at a steep computational cost\nas Bayesian inference, in most cases, is computationally intractable. One\npopular approach to alleviate this problem is using a Monte-Carlo estimation\nwith an ensemble of models sampled from the posterior. However, this approach\nstill comes at a significant computational cost, as one needs to store and run\nmultiple models at test time. In this work, we investigate how to best distill\nan ensemble's predictions using an efficient model. First, we argue that\ncurrent approaches that simply return distribution over predictions cannot\ncompute important properties, such as the covariance between predictions, which\ncan be valuable for further processing. Second, in many limited data settings,\nall ensemble members achieve nearly zero training loss, namely, they produce\nnear-identical predictions on the training set which results in sub-optimal\ndistilled models. To address both problems, we propose a novel and general\ndistillation approach, named Functional Ensemble Distillation (FED), and we\ninvestigate how to best distill an ensemble in this setting. We find that\nlearning the distilled model via a simple augmentation scheme in the form of\nmixup augmentation significantly boosts the performance. We evaluated our\nmethod on several tasks and showed that it achieves superior results in both\naccuracy and uncertainty estimation compared to current approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Penso_C/0/1/0/all/0/1\">Coby Penso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Achituve_I/0/1/0/all/0/1\">Idan Achituve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fetaya_E/0/1/0/all/0/1\">Ethan Fetaya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"M2FNet: Multi-modal Fusion Network for Emotion Recognition in Conversation. (arXiv:2206.02187v1 [cs.CV])","link":"http://arxiv.org/abs/2206.02187","description":"<p>Emotion Recognition in Conversations (ERC) is crucial in developing\nsympathetic human-machine interaction. In conversational videos, emotion can be\npresent in multiple modalities, i.e., audio, video, and transcript. However,\ndue to the inherent characteristics of these modalities, multi-modal ERC has\nalways been considered a challenging undertaking. Existing ERC research focuses\nmainly on using text information in a discussion, ignoring the other two\nmodalities. We anticipate that emotion recognition accuracy can be improved by\nemploying a multi-modal approach. Thus, in this study, we propose a Multi-modal\nFusion Network (M2FNet) that extracts emotion-relevant features from visual,\naudio, and text modality. It employs a multi-head attention-based fusion\nmechanism to combine emotion-rich latent representations of the input data. We\nintroduce a new feature extractor to extract latent features from the audio and\nvisual modality. The proposed feature extractor is trained with a novel\nadaptive margin-based triplet loss function to learn emotion-relevant features\nfrom the audio and visual data. In the domain of ERC, the existing methods\nperform well on one benchmark dataset but not on others. Our results show that\nthe proposed M2FNet architecture outperforms all other methods in terms of\nweighted average F1 score on well-known MELD and IEMOCAP datasets and sets a\nnew state-of-the-art performance in ERC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chudasama_V/0/1/0/all/0/1\">Vishal Chudasama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kar_P/0/1/0/all/0/1\">Purbayan Kar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gudmalwar_A/0/1/0/all/0/1\">Ashish Gudmalwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_N/0/1/0/all/0/1\">Nirmesh Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wasnik_P/0/1/0/all/0/1\">Pankaj Wasnik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Onoe_N/0/1/0/all/0/1\">Naoyuki Onoe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FOF: Learning Fourier Occupancy Field for Monocular Real-time Human Reconstruction. (arXiv:2206.02194v1 [cs.CV])","link":"http://arxiv.org/abs/2206.02194","description":"<p>The advent of deep learning has led to significant progress in monocular\nhuman reconstruction. However, existing representations, such as parametric\nmodels, voxel grids, meshes and implicit neural representations, have\ndifficulties achieving high-quality results and real-time speed at the same\ntime. In this paper, we propose Fourier Occupancy Field (FOF), a novel\npowerful, efficient and flexible 3D representation, for monocular real-time and\naccurate human reconstruction. The FOF represents a 3D object with a 2D field\northogonal to the view direction where at each 2D position the occupancy field\nof the object along the view direction is compactly represented with the first\nfew terms of Fourier series, which retains the topology and neighborhood\nrelation in the 2D domain. A FOF can be stored as a multi-channel image, which\nis compatible with 2D convolutional neural networks and can bridge the gap\nbetween 3D geometries and 2D images. The FOF is very flexible and extensible,\ne.g., parametric models can be easily integrated into a FOF as a prior to\ngenerate more robust results. Based on FOF, we design the first 30+FPS\nhigh-fidelity real-time monocular human reconstruction framework. We\ndemonstrate the potential of FOF on both public dataset and real captured data.\nThe code will be released for research purposes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_Q/0/1/0/all/0/1\">Qiao Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yebin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_Y/0/1/0/all/0/1\">Yu-Kun Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jingyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kun Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GridShift: A Faster Mode-seeking Algorithm for Image Segmentation and Object Tracking. (arXiv:2206.02200v1 [cs.CV])","link":"http://arxiv.org/abs/2206.02200","description":"<p>In machine learning and computer vision, mean shift (MS) qualifies as one of\nthe most popular mode-seeking algorithms used for clustering and image\nsegmentation. It iteratively moves each data point to the weighted mean of its\nneighborhood data points. The computational cost required to find the neighbors\nof each data point is quadratic to the number of data points. Consequently, the\nvanilla MS appears to be very slow for large-scale datasets. To address this\nissue, we propose a mode-seeking algorithm called GridShift, with significant\nspeedup and principally based on MS. To accelerate, GridShift employs a\ngrid-based approach for neighbor search, which is linear in the number of data\npoints. In addition, GridShift moves the active grid cells (grid cells\nassociated with at least one data point) in place of data points towards the\nhigher density, a step that provides more speedup. The runtime of GridShift is\nlinear in the number of active grid cells and exponential in the number of\nfeatures. Therefore, it is ideal for large-scale low-dimensional applications\nsuch as object tracking and image segmentation. Through extensive experiments,\nwe showcase the superior performance of GridShift compared to other MS-based as\nwell as state-of-the-art algorithms in terms of accuracy and runtime on\nbenchmark datasets for image segmentation. Finally, we provide a new\nobject-tracking algorithm based on GridShift and show promising results for\nobject tracking compared to CamShift and meanshift++.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Abhishek Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ajani_O/0/1/0/all/0/1\">Oladayo S. Ajani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1\">Swagatam Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mallipeddi_R/0/1/0/all/0/1\">Rammohan Mallipeddi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D Convolutional with Attention for Action Recognition. (arXiv:2206.02203v1 [cs.CV])","link":"http://arxiv.org/abs/2206.02203","description":"<p>Human action recognition is one of the challenging tasks in computer vision.\nThe current action recognition methods use computationally expensive models for\nlearning spatio-temporal dependencies of the action. Models utilizing RGB\nchannels and optical flow separately, models using a two-stream fusion\ntechnique, and models consisting of both convolutional neural network (CNN) and\nlong-short term memory (LSTM) network are few examples of such complex models.\nMoreover, fine-tuning such complex models is computationally expensive as well.\nThis paper proposes a deep neural network architecture for learning such\ndependencies consisting of a 3D convolutional layer, fully connected (FC)\nlayers, and attention layer, which is simpler to implement and gives a\ncompetitive performance on the UCF-101 dataset. The proposed method first\nlearns spatial and temporal features of actions through 3D-CNN, and then the\nattention mechanism helps the model to locate attention to essential features\nfor recognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shrestha_L/0/1/0/all/0/1\">Labina Shrestha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dubey_S/0/1/0/all/0/1\">Shikha Dubey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olimov_F/0/1/0/all/0/1\">Farrukh Olimov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rafique_M/0/1/0/all/0/1\">Muhammad Aasim Rafique</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeon_M/0/1/0/all/0/1\">Moongu Jeon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"U(1) Symmetry-breaking Observed in Generic CNN Bottleneck Layers. (arXiv:2206.02220v1 [cs.CV])","link":"http://arxiv.org/abs/2206.02220","description":"<p>We report on a significant discovery linking deep convolutional neural\nnetworks (CNN) to biological vision and fundamental particle physics. A model\nof information propagation in a CNN is proposed via an analogy to an optical\nsystem, where bosonic particles (i.e. photons) are concentrated as the 2D\nspatial resolution of the image collapses to a focal point $1\\times 1=1$. A 3D\nspace $(x,y,t)$ is defined by $(x,y)$ coordinates in the image plane and CNN\nlayer $t$, where a principal ray $(0,0,t)$ runs in the direction of information\npropagation through both the optical axis and the image center pixel located at\n$(x,y)=(0,0)$, about which the sharpest possible spatial focus is limited to a\ncircle of confusion in the image plane. Our novel insight is to model the\nprincipal optical ray $(0,0,t)$ as geometrically equivalent to the medial\nvector in the positive orthant $I(x,y) \\in R^{N+}$ of a $N$-channel activation\nspace, e.g. along the greyscale (or luminance) vector $(t,t,t)$ in $RGB$ colour\nspace. Information is thus concentrated into an energy potential\n$E(x,y,t)=\\|I(x,y,t)\\|^2$, which, particularly for bottleneck layers $t$ of\ngeneric CNNs, is highly concentrated and symmetric about the spatial origin\n$(0,0,t)$ and exhibits the well-known \"Sombrero\" potential of the boson\nparticle. This symmetry is broken in classification, where bottleneck layers of\ngeneric pre-trained CNN models exhibit a consistent class-specific bias towards\nan angle $\\theta \\in U(1)$ defined simultaneously in the image plane and in\nactivation feature space. Initial observations validate our hypothesis from\ngeneric pre-trained CNN activation maps and a bare-bones memory-based\nclassification scheme, with no training or tuning. Training from scratch using\na random $U(1)$ class label the leads to improved classification in all cases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bouchard_L/0/1/0/all/0/1\">Louis-Fran&#xe7;ois Bouchard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lazreg_M/0/1/0/all/0/1\">Mohsen Ben Lazreg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toews_M/0/1/0/all/0/1\">Matthew Toews</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Physically Inspired Constraint for Unsupervised Regularized Ultrasound Elastography. (arXiv:2206.02225v1 [eess.IV])","link":"http://arxiv.org/abs/2206.02225","description":"<p>Displacement estimation is a critical step of virtually all Ultrasound\nElastography (USE) techniques. Two main features make this task unique compared\nto the general optical flow problem: the high-frequency nature of ultrasound\nradio-frequency (RF) data and the governing laws of physics on the displacement\nfield. Recently, the architecture of the optical flow networks has been\nmodified to be able to use RF data. Also, semi-supervised and unsupervised\ntechniques have been employed for USE by considering prior knowledge of\ndisplacement continuity in the form of the first- and second-derivative\nregularizers. Despite these attempts, no work has considered the tissue\ncompression pattern, and displacements in axial and lateral directions have\nbeen assumed to be independent. However, tissue motion pattern is governed by\nlaws of physics in USE, rendering the axial and the lateral displacements\nhighly correlated. In this paper, we propose Physically Inspired ConsTraint for\nUnsupervised Regularized Elastography (PICTURE), where we impose constraints on\nthe Poisson's ratio to improve lateral displacement estimates. Experiments on\nphantom and in vivo data show that PICTURE substantially improves the quality\nof the lateral displacement estimation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Tehrani_A/0/1/0/all/0/1\">Ali K. Z. Tehrani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rivaz_H/0/1/0/all/0/1\">Hassan Rivaz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Two Decades of Bengali Handwritten Digit Recognition: A Survey. (arXiv:2206.02234v1 [cs.CV])","link":"http://arxiv.org/abs/2206.02234","description":"<p>Handwritten Digit Recognition (HDR) is one of the most challenging tasks in\nthe domain of Optical Character Recognition (OCR). Irrespective of language,\nthere are some inherent challenges of HDR, which mostly arise due to the\nvariations in writing styles across individuals, writing medium and\nenvironment, inability to maintain the same strokes while writing any digit\nrepeatedly, etc. In addition to that, the structural complexities of the digits\nof a particular language may lead to ambiguous scenarios of HDR. Over the\nyears, researchers have developed numerous offline and online HDR pipelines,\nwhere different image processing techniques are combined with traditional\nMachine Learning (ML)-based and/or Deep Learning (DL)-based architectures.\nAlthough evidence of extensive review studies on HDR exists in the literature\nfor languages, such as: English, Arabic, Indian, Farsi, Chinese, etc., few\nsurveys on Bengali HDR (BHDR) can be found, which lack a comprehensive analysis\nof the challenges, the underlying recognition process, and possible future\ndirections. In this paper, the characteristics and inherent ambiguities of\nBengali handwritten digits along with a comprehensive insight of two decades of\nthe state-of-the-art datasets and approaches towards offline BHDR have been\nanalyzed. Furthermore, several real-life application-specific studies, which\ninvolve BHDR, have also been discussed in detail. This paper will also serve as\na compendium for researchers interested in the science behind offline BHDR,\ninstigating the exploration of newer avenues of relevant research that may\nfurther lead to better offline recognition of Bengali handwritten digits in\ndifferent application areas.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rahman_A/0/1/0/all/0/1\">A.B.M. Ashikur Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1\">Md. Bakhtiar Hasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_S/0/1/0/all/0/1\">Sabbir Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_T/0/1/0/all/0/1\">Tasnim Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ashmafee_M/0/1/0/all/0/1\">Md. Hamjajul Ashmafee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kabir_M/0/1/0/all/0/1\">Mohammad Ridwan Kabir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kabir_M/0/1/0/all/0/1\">Md. Hasanul Kabir</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Annotation and Learning for 3D Hand Pose Estimation: A Survey. (arXiv:2206.02257v1 [cs.CV])","link":"http://arxiv.org/abs/2206.02257","description":"<p>In this survey, we present comprehensive analysis of 3D hand pose estimation\nfrom the perspective of efficient annotation and learning. In particular, we\nstudy recent approaches for 3D hand pose annotation and learning methods with\nlimited annotated data. In 3D hand pose estimation, collecting 3D hand pose\nannotation is a key step in developing hand pose estimators and their\napplications, such as video understanding, AR/VR, and robotics. However,\nacquiring annotated 3D hand poses is cumbersome, e.g., due to the difficulty of\naccessing 3D information and occlusion. Motivated by elucidating how recent\nworks address the annotation issue, we investigated annotation methods\nclassified as manual, synthetic-model-based, hand-sensor-based, and\ncomputational approaches. Since these annotation methods are not always\navailable on a large scale, we examined methods of learning 3D hand poses when\nwe do not have enough annotated data, namely self-supervised pre-training,\nsemi-supervised learning, and domain adaptation. Based on the analysis of these\nefficient annotation and learning, we further discuss limitations and possible\nfuture directions of this field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ohkawa_T/0/1/0/all/0/1\">Takehiko Ohkawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Furuta_R/0/1/0/all/0/1\">Ryosuke Furuta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sato_Y/0/1/0/all/0/1\">Yoichi Sato</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SealID: Saimaa ringed seal re-identification dataset. (arXiv:2206.02260v1 [cs.CV])","link":"http://arxiv.org/abs/2206.02260","description":"<p>Wildlife camera traps and crowd-sourced image material provide novel\npossibilities to monitor endangered animal species. However, massive image\nvolumes that these methods produce are overwhelming for researchers to go\nthrough manually which calls for automatic systems to perform the analysis. The\nanalysis task that has gained the most attention is the re-identification of\nindividuals, as it allows, for example, to study animal migration or to\nestimate the population size. The Saimaa ringed seal (Pusa hispida saimensis)\nis an endangered subspecies only found in the Lake Saimaa, Finland, and is one\nof the few existing freshwater seal species. Ringed seals have permanent pelage\npatterns that are unique to each individual which can be used for the\nidentification of individuals. Large variation in poses further exacerbated by\nthe deformable nature of seals together with varying appearance and low\ncontrast between the ring pattern and the rest of the pelage makes the Saimaa\nringed seal re-identification task very challenging, providing a good benchmark\nto evaluate state-of-the-art re-identification methods. Therefore, we make our\nSaimaa ringed seal image (SealID) dataset (N=57) publicly available for\nresearch purposes. In this paper, the dataset is described, the evaluation\nprotocol for re-identification methods is proposed, and the results for two\nbaseline methods HotSpotter and NORPPA are provided. The SealID dataset has\nbeen made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nepovinnykh_E/0/1/0/all/0/1\">Ekaterina Nepovinnykh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eerola_T/0/1/0/all/0/1\">Tuomas Eerola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biard_V/0/1/0/all/0/1\">Vincent Biard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mutka_P/0/1/0/all/0/1\">Piia Mutka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niemi_M/0/1/0/all/0/1\">Marja Niemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalviainen_H/0/1/0/all/0/1\">Heikki K&#xe4;lvi&#xe4;inen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kunnasranta_M/0/1/0/all/0/1\">Mervi Kunnasranta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Individual Grevy's Zebra Identification via Deep 3D Fitting and Metric Learning. (arXiv:2206.02261v1 [cs.CV])","link":"http://arxiv.org/abs/2206.02261","description":"<p>This paper combines deep learning techniques for species detection, 3D model\nfitting, and metric learning in one pipeline to perform individual animal\nidentification from photographs by exploiting unique coat patterns. This is the\nfirst work to attempt this and, compared to traditional 2D bounding box or\nsegmentation based CNN identification pipelines, the approach provides\neffective and explicit view-point normalisation and allows for a straight\nforward visualisation of the learned biometric population space. Note that due\nto the use of metric learning the pipeline is also readily applicable to open\nset and zero shot re-identification scenarios. We apply the proposed approach\nto individual Grevy's zebra (Equus grevyi) identification and show in a small\nstudy on the SMALST dataset that the use of 3D model fitting can indeed benefit\nperformance. In particular, back-projected textures from 3D fitted models\nimprove identification accuracy from 48.0% to 56.8% compared to 2D bounding box\napproaches for the dataset. Whilst the study is far too small accurately to\nestimate the full performance potential achievable in larger-scale real-world\napplication settings and in comparisons against polished tools, our work lays\nthe conceptual and practical foundations for a next step in animal biometrics\ntowards deep metric learning driven, fully 3D-aware animal identification in\nopen population settings. We publish network weights and relevant facilitating\nsource code with this paper for full reproducibility and as inspiration for\nfurther research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stennett_M/0/1/0/all/0/1\">Maria Stennett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rubenstein_D/0/1/0/all/0/1\">Daniel I. Rubenstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burghardt_T/0/1/0/all/0/1\">Tilo Burghardt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Estimating Building Energy Efficiency From Street View Imagery, Aerial Imagery, and Land Surface Temperature Data. (arXiv:2206.02270v1 [cs.CV])","link":"http://arxiv.org/abs/2206.02270","description":"<p>In the race towards carbon neutrality, the building sector has fallen behind\nand bears the potential to endanger the progress made across other industries.\nThis is because buildings exhibit a life span of several decades which creates\nsubstantial inertia in the face of climate change. This inertia is further\nexacerbated by the scale of the existing building stock. With several billion\noperational buildings around the globe, working towards a carbon-neutral\nbuilding sector requires solutions which enable stakeholders to accurately\nidentify and retrofit subpar buildings at scale. However, improving the energy\nefficiency of the existing building stock through retrofits in a targeted and\nefficient way remains challenging. This is because, as of today, the energy\nefficiency of buildings is generally determined by on-site visits of certified\nenergy auditors which makes the process slow, costly, and geographically\nincomplete. In order to accelerate the identification of promising retrofit\ntargets, this work proposes a new method which can estimate a building's energy\nefficiency using purely remotely sensed data such as street view and aerial\nimagery, OSM-derived footprint areas, and satellite-borne land surface\ntemperature (LST) measurements. We find that in the binary setting of\ndistinguishing efficient from inefficient buildings, our end-to-end deep\nlearning model achieves a macro-averaged F1-score of 62.06\\%. As such, this\nwork shows the potential and complementary nature of remotely sensed data in\npredicting building attributes such as energy efficiency and opens up new\nopportunities for future work to integrate additional data sources.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mayer_K/0/1/0/all/0/1\">Kevin Mayer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haas_L/0/1/0/all/0/1\">Lukas Haas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Autoregressive Model for Multi-Pass SAR Change Detection Based on Image Stacks. (arXiv:2206.02278v1 [eess.IV])","link":"http://arxiv.org/abs/2206.02278","description":"<p>Change detection is an important synthetic aperture radar (SAR) application,\nusually used to detect changes on the ground scene measurements in different\nmoments in time. Traditionally, change detection algorithm (CDA) is mainly\ndesigned for two synthetic aperture radar (SAR) images retrieved at different\ninstants. However, more images can be used to improve the algorithms\nperformance, witch emerges as a research topic on SAR change detection. Image\nstack information can be treated as a data series over time and can be modeled\nby autoregressive (AR) models. Thus, we present some initial findings on SAR\nchange detection based on image stack considering AR models. Applying AR model\nfor each pixel position in the image stack, we obtained an estimated image of\nthe ground scene which can be used as a reference image for CDA. The\nexperimental results reveal that ground scene estimates by the AR models is\naccurate and can be used for change detection applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Palm_B/0/1/0/all/0/1\">B. G. Palm</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Alves_D/0/1/0/all/0/1\">D. I. Alves</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vu_V/0/1/0/all/0/1\">V. T. Vu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pettersson_M/0/1/0/all/0/1\">M. I. Pettersson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bayer_F/0/1/0/all/0/1\">F. M. Bayer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cintra_R/0/1/0/all/0/1\">R. J. Cintra</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Machado_R/0/1/0/all/0/1\">R. Machado</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dammert_P/0/1/0/all/0/1\">P. Dammert</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hellsten_H/0/1/0/all/0/1\">H. Hellsten</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"E^2VTS: Energy-Efficient Video Text Spotting from Unmanned Aerial Vehicles. (arXiv:2206.02281v1 [cs.CV])","link":"http://arxiv.org/abs/2206.02281","description":"<p>Unmanned Aerial Vehicles (UAVs) based video text spotting has been\nextensively used in civil and military domains. UAV's limited battery capacity\nmotivates us to develop an energy-efficient video text spotting solution. In\nthis paper, we first revisit RCNN's crop &amp; resize training strategy and\nempirically find that it outperforms aligned RoI sampling on a real-world video\ntext dataset captured by UAV. To reduce energy consumption, we further propose\na multi-stage image processor that takes videos' redundancy, continuity, and\nmixed degradation into account. Lastly, the model is pruned and quantized\nbefore deployed on Raspberry Pi. Our proposed energy-efficient video text\nspotting solution, dubbed as E^2VTS, outperforms all previous methods by\nachieving a competitive tradeoff between energy efficiency and performance. All\nour codes and pre-trained models are available at\nhttps://github.com/wuzhenyusjtu/LPCVC20-VideoTextSpotting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhenyu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhenyu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pi_P/0/1/0/all/0/1\">Pengcheng Pi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_Y/0/1/0/all/0/1\">Yunhe Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jiayi Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1\">Jianchao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_X/0/1/0/all/0/1\">Xiangru Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Ji Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tagged-MRI2Audio with Attention Guided Heterogeneous Translator. (arXiv:2206.02284v1 [cs.SD])","link":"http://arxiv.org/abs/2206.02284","description":"<p>Understanding the underlying relationship between tongue and oropharyngeal\nmuscle deformation seen in tagged-MRI and intelligible speech plays an\nimportant role in advancing speech motor control theories and treatment of\nspeech related-disorders. Because of their heterogeneous representations,\nhowever, direct mapping between the two modalities -- i.e., two-dimensional\n(mid-sagittal slice) plus time tagged-MRI sequence and its corresponding\none-dimensional waveform -- is not straightforward. Instead, we resort to\ntwo-dimensional spectrograms as an intermediate representation, which contains\nboth pitch and resonance, from which to develop an end-to-end deep learning\nframework to translate from a sequence of tagged-MRI to its corresponding audio\nwaveform with limited dataset size. Our framework is based on a novel fully\nconvolutional asymmetry translator with guidance of a self residual attention\nstrategy to specifically exploit the moving muscular structures during speech.\nIn addition, we leverage a pairwise correlation of the samples with the same\nutterances with a latent space representation disentanglement strategy.\nFurthermore, we incorporate an adversarial training approach with generative\nadversarial networks to offer improved realism on our generated spectrograms.\nOur experimental results, carried out with a total of 63 tagged-MRI sequences\nalongside speech acoustics, showed that our framework enabled the generation of\nclear audio waveforms from a sequence of tagged-MRI, surpassing competing\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaofeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_F/0/1/0/all/0/1\">Fangxu Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prince_J/0/1/0/all/0/1\">Jerry L. Prince</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuo_J/0/1/0/all/0/1\">Jiachen Zhuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stone_M/0/1/0/all/0/1\">Maureen Stone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fakhri_G/0/1/0/all/0/1\">Georges El Fakhri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woo_J/0/1/0/all/0/1\">Jonghye Woo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AugLoss: A Learning Methodology for Real-World Dataset Corruption. (arXiv:2206.02286v1 [cs.LG])","link":"http://arxiv.org/abs/2206.02286","description":"<p>Deep Learning (DL) models achieve great successes in many domains. However,\nDL models increasingly face safety and robustness concerns, including noisy\nlabeling in the training stage and feature distribution shifts in the testing\nstage. Previous works made significant progress in addressing these problems,\nbut the focus has largely been on developing solutions for only one problem at\na time. For example, recent work has argued for the use of tunable robust loss\nfunctions to mitigate label noise, and data augmentation (e.g., AugMix) to\ncombat distribution shifts. As a step towards addressing both problems\nsimultaneously, we introduce AugLoss, a simple but effective methodology that\nachieves robustness against both train-time noisy labeling and test-time\nfeature distribution shifts by unifying data augmentation and robust loss\nfunctions. We conduct comprehensive experiments in varied settings of\nreal-world dataset corruption to showcase the gains achieved by AugLoss\ncompared to previous state-of-the-art methods. Lastly, we hope this work will\nopen new directions for designing more robust and reliable DL models under\nreal-world corruptions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Otstot_K/0/1/0/all/0/1\">Kyle Otstot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cava_J/0/1/0/all/0/1\">John Kevin Cava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sypherd_T/0/1/0/all/0/1\">Tyler Sypherd</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sankar_L/0/1/0/all/0/1\">Lalitha Sankar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ACT: Semi-supervised Domain-adaptive Medical Image Segmentation with Asymmetric Co-Training. (arXiv:2206.02288v1 [cs.CV])","link":"http://arxiv.org/abs/2206.02288","description":"<p>We aim to develop semi-supervised domain adaptation (SSDA) for medical image\nsegmentation, which is largely underexplored. We propose to exploit both\nlabeled source and target domain data, in addition to unlabeled target data in\na unified manner. Specifically, we present a novel asymmetric co-training (ACT)\nframework to integrate these subsets and avoid the domination of the source\ndomain data. Following a divide-and-conquer strategy, we explicitly decouple\nthe label supervisions in SSDA into two asymmetric sub-tasks, including\nsemi-supervised learning (SSL) and UDA, and leverage different knowledge from\ntwo segmentors to take into account the distinction between the source and\ntarget label supervisions. The knowledge learned in the two modules is then\nadaptively integrated with ACT, by iteratively teaching each other, based on\nthe confidence-aware pseudo-label. In addition, pseudo label noise is\nwell-controlled with an exponential MixUp decay scheme for smooth propagation.\nExperiments on cross-modality brain tumor MRI segmentation tasks using the\nBraTS18 database showed, even with limited labeled target samples, ACT yielded\nmarked improvements over UDA and state-of-the-art SSDA methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaofeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_F/0/1/0/all/0/1\">Fangxu Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shusharina_N/0/1/0/all/0/1\">Nadya Shusharina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_R/0/1/0/all/0/1\">Ruth Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuo_C/0/1/0/all/0/1\">C-C Jay Kuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fakhri_G/0/1/0/all/0/1\">Georges El Fakhri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woo_J/0/1/0/all/0/1\">Jonghye Woo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HIFI-Net: A Novel Network for Enhancement to Underwater Images. (arXiv:2206.02295v1 [cs.CV])","link":"http://arxiv.org/abs/2206.02295","description":"<p>A novel network for enhancement to underwater images is proposed in this\npaper. It contains a Reinforcement Fusion Module for Haar wavelet images\n(RFM-Haar) based on Reinforcement Fusion Unit (RFU), which is used to fuse an\noriginal image and some important information within it. Fusion is achieved for\nbetter enhancement. As this network make \"Haar Images into Fusion Images\", it\nis called HIFI-Net. The experimental results show the proposed HIFI-Net\nperforms best among many state-of-the-art methods on three datasets at three\nnormal metrics and a new metric.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jiajia Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_J/0/1/0/all/0/1\">Junbin Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yan Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Di Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bootstrapping Semi-supervised Medical Image Segmentation with Anatomical-aware Contrastive Distillation. (arXiv:2206.02307v1 [cs.CV])","link":"http://arxiv.org/abs/2206.02307","description":"<p>Contrastive learning has shown great promise over annotation scarcity\nproblems in the context of medical image segmentation. Existing approaches\ntypically assume a balanced class distribution for both labeled and unlabeled\nmedical images. However, medical image data in reality is commonly imbalanced\n(i.e., multi-class label imbalance), which naturally yields blurry contours and\nusually incorrectly labels rare objects. Moreover, it remains unclear whether\nall negative samples are equally negative. In this work, we present ACTION, an\nAnatomical-aware ConTrastive dIstillatiON framework, for semi-supervised\nmedical image segmentation. Specifically, we first develop an iterative\ncontrastive distillation algorithm by softly labeling the negatives rather than\nbinary supervision between positive and negative pairs. We also capture more\nsemantically similar features from the randomly chosen negative set compared to\nthe positives to enforce the diversity of the sampled data. Second, we raise a\nmore important question: Can we really handle imbalanced samples to yield\nbetter performance? Hence, the key innovation in ACTION is to learn global\nsemantic relationship across the entire dataset and local anatomical features\namong the neighbouring pixels with minimal additional memory footprint. During\nthe training, we introduce anatomical contrast by actively sampling a sparse\nset of hard negative pixels, which can generate smoother segmentation\nboundaries and more accurate predictions. Extensive experiments across two\nbenchmark datasets and different unlabeled settings show that ACTION performs\ncomparable or better than the current state-of-the-art supervised and\nsemi-supervised methods. Our code and models will be publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1\">Chenyu You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1\">Weicheng Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Staib_L/0/1/0/all/0/1\">Lawrence Staib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duncan_J/0/1/0/all/0/1\">James S. Duncan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluation-oriented Knowledge Distillation for Deep Face Recognition. (arXiv:2206.02325v1 [cs.CV])","link":"http://arxiv.org/abs/2206.02325","description":"<p>Knowledge distillation (KD) is a widely-used technique that utilizes large\nnetworks to improve the performance of compact models. Previous KD approaches\nusually aim to guide the student to mimic the teacher's behavior completely in\nthe representation space. However, such one-to-one corresponding constraints\nmay lead to inflexible knowledge transfer from the teacher to the student,\nespecially those with low model capacities. Inspired by the ultimate goal of KD\nmethods, we propose a novel Evaluation oriented KD method (EKD) for deep face\nrecognition to directly reduce the performance gap between the teacher and\nstudent models during training. Specifically, we adopt the commonly used\nevaluation metrics in face recognition, i.e., False Positive Rate (FPR) and\nTrue Positive Rate (TPR) as the performance indicator. According to the\nevaluation protocol, the critical pair relations that cause the TPR and FPR\ndifference between the teacher and student models are selected. Then, the\ncritical relations in the student are constrained to approximate the\ncorresponding ones in the teacher by a novel rank-based loss function, giving\nmore flexibility to the student with low capacity. Extensive experimental\nresults on popular benchmarks demonstrate the superiority of our EKD over\nstate-of-the-art competitors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yuge Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiaxiang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xingkun Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1\">Shouhong Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"JigsawHSI: a network for Hyperspectral Image classification. (arXiv:2206.02327v1 [cs.CV])","link":"http://arxiv.org/abs/2206.02327","description":"<p>This article describes the performance of JigsawHSI,a convolutional neural\nnetwork (CNN) based on Inception but tailored for geoscientific analyses, on\nclassification with the Indian Pines, Pavia University and Salinas\nhyperspectral image data sets. The network is compared against HybridSN, a\nspectral-spatial 3D-CNN followed by 2D-CNN that achieves state-of-the-art\nresults in the datasets. This short article proves that JigsawHSI is able to\nmeet or exceed HybridSN performance in all three cases. Additionally, the code\nand toolkit are made available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moraga_J/0/1/0/all/0/1\">Jaime Moraga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duzgun_H/0/1/0/all/0/1\">H. Sebnem Duzgun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MASNet:Improve Performance of Siamese Networks with Mutual-attention for Remote Sensing Change Detection Tasks. (arXiv:2206.02331v1 [cs.CV])","link":"http://arxiv.org/abs/2206.02331","description":"<p>Siamese networks are widely used for remote sensing change detection tasks. A\nvanilla siamese network has two identical feature extraction branches which\nshare weights, these two branches work independently and the feature maps are\nnot fused until about to be sent to a decoder head. However we find that it is\ncritical to exchange information between two feature extraction branches at\nearly stage for change detection task. In this work we present Mutual-Attention\nSiamese Network (MASNet), a general siamese network with mutual-attention\nplug-in, so to exchange information between the two feature extraction\nbranches. We show that our modification improve the performance of siamese\nnetworks on multi change detection datasets, and it works for both\nconvolutional neural network and visual transformer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hongbin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Yupeng Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qiankun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1\">Jun Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yonggang Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OrdinalCLIP: Learning Rank Prompts for Language-Guided Ordinal Regression. (arXiv:2206.02338v1 [cs.CV])","link":"http://arxiv.org/abs/2206.02338","description":"<p>This paper presents a language-powered paradigm for ordinal regression.\nExisting methods usually treat each rank as a category and employ a set of\nweights to learn these concepts. These methods are easy to overfit and usually\nattain unsatisfactory performance as the learned concepts are mainly derived\nfrom the training set. Recent large pre-trained vision-language models like\nCLIP have shown impressive performance on various visual tasks. In this paper,\nwe propose to learn the rank concepts from the rich semantic CLIP latent space.\nSpecifically, we reformulate this task as an image-language matching problem\nwith a contrastive objective, which regards labels as text and obtains a\nlanguage prototype from a text encoder for each rank. While prompt engineering\nfor CLIP is extremely time-consuming, we propose OrdinalCLIP, a differentiable\nprompting method for adapting CLIP for ordinal regression. OrdinalCLIP consists\nof learnable context tokens and learnable rank embeddings; The learnable rank\nembeddings are constructed by explicitly modeling numerical continuity,\nresulting in well-ordered, compact language prototypes in the CLIP space. Once\nlearned, we can only save the language prototypes and discard the huge language\nmodel, resulting in zero additional computational overhead compared with the\nlinear head counterpart. Experimental results show that our paradigm achieves\ncompetitive performance in general ordinal regression tasks, and gains\nimprovements in few-shot and distribution shift settings for age estimation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wanhua Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaoke Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zheng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yansong Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiwen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WHU-Stereo: A Challenging Benchmark for Stereo Matching of High-Resolution Satellite Images. (arXiv:2206.02342v1 [cs.CV])","link":"http://arxiv.org/abs/2206.02342","description":"<p>Stereo matching of high-resolution satellite images (HRSI) is still a\nfundamental but challenging task in the field of photogrammetry and remote\nsensing. Recently, deep learning (DL) methods, especially convolutional neural\nnetworks (CNNs), have demonstrated tremendous potential for stereo matching on\npublic benchmark datasets. However, datasets for stereo matching of satellite\nimages are scarce. To facilitate further research, this paper creates and\npublishes a challenging dataset, termed WHU-Stereo, for stereo matching DL\nnetwork training and testing. This dataset is created by using airborne LiDAR\npoint clouds and high-resolution stereo imageries taken from the Chinese\nGaoFen-7 satellite (GF-7). The WHU-Stereo dataset contains more than 1700\nepipolar rectified image pairs, which cover six areas in China and includes\nvarious kinds of landscapes. We have assessed the accuracy of ground-truth\ndisparity maps, and it is proved that our dataset achieves comparable precision\ncompared with existing state-of-the-art stereo matching datasets. To verify its\nfeasibility, in experiments, the hand-crafted SGM stereo matching algorithm and\nrecent deep learning networks have been tested on the WHU-Stereo dataset.\nExperimental results show that deep learning networks can be well trained and\nachieves higher performance than hand-crafted SGM algorithm, and the dataset\nhas great potential in remote sensing application. The WHU-Stereo dataset can\nserve as a challenging benchmark for stereo matching of high-resolution\nsatellite images, and performance evaluation of deep learning models. Our\ndataset is available at https://github.com/Sheng029/WHU-Stereo\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shenhong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Sheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">San Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1\">Wanshou Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lin Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Graph Multimodal Model for Text Classification in Videos. (arXiv:2206.02343v1 [cs.CV])","link":"http://arxiv.org/abs/2206.02343","description":"<p>The extraction of text information in videos serves as a critical step\ntowards semantic understanding of videos. It usually involved in two steps: (1)\ntext recognition and (2) text classification. To localize texts in videos, we\ncan resort to large numbers of text recognition methods based on OCR\ntechnology. However, to our knowledge, there is no existing work focused on the\nsecond step of video text classification, which will limit the guidance to\ndownstream tasks such as video indexing and browsing. In this paper, we are the\nfirst to address this new task of video text classification by fusing\nmultimodal information to deal with the challenging scenario where different\ntypes of video texts may be confused with various colors, unknown fonts and\ncomplex layouts. In addition, we tailor a specific module called CorrelationNet\nto reinforce feature representation by explicitly extracting layout\ninformation. Furthermore, contrastive learning is utilized to explore inherent\nconnections between samples using plentiful unlabeled videos. Finally, we\nconstruct a new well-defined industrial dataset from the news domain, called\nTI-News, which is dedicated to building and evaluating video text recognition\nand classification applications. Extensive experiments on TI-News demonstrate\nthe effectiveness of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Ye Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Changchong Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1\">Di Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_B/0/1/0/all/0/1\">Bo Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Anomaly Detection with Test Time Augmentation and Consistency Evaluation. (arXiv:2206.02345v1 [cs.CV])","link":"http://arxiv.org/abs/2206.02345","description":"<p>Deep neural networks are known to be vulnerable to unseen data: they may\nwrongly assign high confidence stcores to out-distribuion samples. Recent works\ntry to solve the problem using representation learning methods and specific\nmetrics. In this paper, we propose a simple, yet effective post-hoc anomaly\ndetection algorithm named Test Time Augmentation Anomaly Detection (TTA-AD),\ninspired by a novel observation. Specifically, we observe that in-distribution\ndata enjoy more consistent predictions for its original and augmented versions\non a trained network than out-distribution data, which separates\nin-distribution and out-distribution samples. Experiments on various\nhigh-resolution image benchmark datasets demonstrate that TTA-AD achieves\ncomparable or better detection performance under dataset-vs-dataset anomaly\ndetection settings with a 60%~90\\% running time reduction of existing\nclassifier-based algorithms. We provide empirical verification that the key to\nTTA-AD lies in the remaining classes between augmented features, which has long\nbeen partially ignored by previous works. Additionally, we use RUNS as a\nsurrogate to analyze our algorithm theoretically.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Haowei He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teng_J/0/1/0/all/0/1\">Jiaye Teng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yang Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Structured Binary Neural Networks for Image Recognition. (arXiv:1909.09934v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1909.09934","description":"<p>We propose methods to train convolutional neural networks (CNNs) with both\nbinarized weights and activations, leading to quantized models that are\nspecifically friendly to mobile devices with limited power capacity and\ncomputation resources. Previous works on quantizing CNNs often seek to\napproximate the floating-point information using a set of discrete values,\nwhich we call value approximation, typically assuming the same architecture as\nthe full-precision networks. Here we take a novel \"structure approximation\"\nview of quantization -- it is very likely that different architectures designed\nfor low-bit networks may be better for achieving good performance. In\nparticular, we propose a \"network decomposition\" strategy, termed Group-Net, in\nwhich we divide the network into groups. Thus, each full-precision group can be\neffectively reconstructed by aggregating a set of homogeneous binary branches.\nIn addition, we learn effective connections among groups to improve the\nrepresentation capability. Moreover, the proposed Group-Net shows strong\ngeneralization to other tasks. For instance, we extend Group-Net for accurate\nsemantic segmentation by embedding rich context into the binary structure.\nFurthermore, for the first time, we apply binary neural networks to object\ndetection. Experiments on both classification, semantic segmentation and object\ndetection tasks demonstrate the superior performance of the proposed methods\nover various quantized networks in the literature. Our methods outperform the\nprevious best binary neural networks in terms of accuracy and computation\nefficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_B/0/1/0/all/0/1\">Bohan Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1\">Mingkui Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Peng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lingqiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reid_I/0/1/0/all/0/1\">Ian Reid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How to Train Your Dragon: Tamed Warping Network for Semantic Video Segmentation. (arXiv:2005.01344v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2005.01344","description":"<p>Real-time semantic segmentation on high-resolution videos is challenging due\nto the strict requirements of speed. Recent approaches have utilized the\ninter-frame continuity to reduce redundant computation by warping the feature\nmaps across adjacent frames, greatly speeding up the inference phase. However,\ntheir accuracy drops significantly owing to the imprecise motion estimation and\nerror accumulation. In this paper, we propose to introduce a simple and\neffective correction stage right after the warping stage to form a framework\nnamed Tamed Warping Network (TWNet), aiming to improve the accuracy and\nrobustness of warping-based models. The experimental results on the Cityscapes\ndataset show that with the correction, the accuracy (mIoU) significantly\nincreases from 67.3% to 71.6%, and the speed edges down from 65.5 FPS to 61.8\nFPS. For non-rigid categories such as \"human\" and \"object\", the improvements of\nIoU are even higher than 18 percentage points.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Junyi Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Songyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yifeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fuxian Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_J/0/1/0/all/0/1\">Jiabao Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xi Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SAGE: Sequential Attribute Generator for Analyzing Glioblastomas using Limited Dataset. (arXiv:2005.07225v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2005.07225","description":"<p>While deep learning approaches have shown remarkable performance in many\nimaging tasks, most of these methods rely on availability of large quantities\nof data. Medical image data, however, is scarce and fragmented. Generative\nAdversarial Networks (GANs) have recently been very effective in handling such\ndatasets by generating more data. If the datasets are very small, however, GANs\ncannot learn the data distribution properly, resulting in less diverse or\nlow-quality results. One such limited dataset is that for the concurrent gain\nof 19 and 20 chromosomes (19/20 co-gain), a mutation with positive prognostic\nvalue in Glioblastomas (GBM). In this paper, we detect imaging biomarkers for\nthe mutation to streamline the extensive and invasive prognosis pipeline. Since\nthis mutation is relatively rare, i.e. small dataset, we propose a novel\ngenerative framework - the Sequential Attribute GEnerator (SAGE), that\ngenerates detailed tumor imaging features while learning from a limited\ndataset. Experiments show that not only does SAGE generate high quality tumors\nwhen compared to standard Deep Convolutional GAN (DC-GAN) and Wasserstein GAN\nwith Gradient Penalty (WGAN-GP), it also captures the imaging biomarkers\naccurately.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Jonnalagedda_P/0/1/0/all/0/1\">Padmaja Jonnalagedda</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Weinberg_B/0/1/0/all/0/1\">Brent Weinberg</a> (MD, PhD), <a href=\"http://arxiv.org/find/eess/1/au:+Allen_J/0/1/0/all/0/1\">Jason Allen</a> (MD, PhD), <a href=\"http://arxiv.org/find/eess/1/au:+Min_T/0/1/0/all/0/1\">Taejin L. Min</a> (MD), <a href=\"http://arxiv.org/find/eess/1/au:+Bhanu_S/0/1/0/all/0/1\">Shiv Bhanu</a> (MD), <a href=\"http://arxiv.org/find/eess/1/au:+Bhanu_B/0/1/0/all/0/1\">Bir Bhanu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using Unlabeled Data for Increasing Low-Shot Classification Accuracy of Relevant and Open-Set Irrelevant Images. (arXiv:2010.00721v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2010.00721","description":"<p>In search, exploration, and reconnaissance tasks performed with autonomous\nground vehicles, an image classification capability is needed for specifically\nidentifying targeted objects (relevant classes) and at the same time recognize\nwhen a candidate image does not belong to anyone of the relevant classes\n(irrelevant images). In this paper, we present an open-set low-shot classifier\nthat uses, during its training, a modest number (less than 40) of labeled\nimages for each relevant class, and unlabeled irrelevant images that are\nrandomly selected at each epoch of the training process. The new classifier is\ncapable of identifying images from the relevant classes, determining when a\ncandidate image is irrelevant, and it can further recognize categories of\nirrelevant images that were not included in the training (unseen). The proposed\nlow-shot classifier can be attached as a top layer to any pre-trained feature\nextractor when constructing a Convolutional Neural Network.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kasapis_S/0/1/0/all/0/1\">Spiridon Kasapis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Geng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smereka_J/0/1/0/all/0/1\">Jonathon Smereka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vlahopoulos_N/0/1/0/all/0/1\">Nickolas Vlahopoulos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Far Can We Get with Neural Networks Straight from JPEG?. (arXiv:2012.14426v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.14426","description":"<p>Convolutional neural networks (CNNs) have achieved astonishing advances over\nthe past decade, defining state-of-the-art in several computer vision tasks.\nCNNs are capable of learning robust representations of the data directly from\nthe RGB pixels. However, most image data are usually available in compressed\nformat, from which the JPEG is the most widely used due to transmission and\nstorage purposes demanding a preliminary decoding process that have a high\ncomputational load and memory usage. For this reason, deep learning methods\ncapable of leaning directly from the compressed domain have been gaining\nattention in recent years. These methods adapt typical CNNs to work on the\ncompressed domain, but the common architectural modifications lead to an\nincrease in computational complexity and the number of parameters. In this\npaper, we investigate the usage of CNNs that are designed to work directly with\nthe DCT coefficients available in JPEG compressed images, proposing a\nhandcrafted and data-driven techniques for reducing the computational\ncomplexity and the number of parameters for these models in order to keep their\ncomputational cost similar to their RGB baselines. We make initial ablation\nstudies on a subset of ImageNet in order to analyse the impact of different\nfrequency ranges, image resolution, JPEG quality and classification task\ndifficulty on the performance of the models. Then, we evaluate the models on\nthe complete ImageNet dataset. Our results indicate that DCT models are capable\nof obtaining good performance, and that it is possible to reduce the\ncomputational complexity and the number of parameters from these models while\nretaining a similar classification accuracy through the use of our proposed\ntechniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Santos_S/0/1/0/all/0/1\">Samuel Felipe dos Santos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1\">Nicu Sebe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Almeida_J/0/1/0/all/0/1\">Jurandy Almeida</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-point dimensionality reduction to improve projection layout reliability. (arXiv:2101.06224v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.06224","description":"<p>In ordinary Dimensionality Reduction (DR), each data instance in a high\ndimensional space (original space), or on a distance matrix denoting original\nspace distances, is mapped to (projected onto) one point in a low dimensional\nspace (visual space), building a layout of projected points trying to preserve\nas much as possible some property of data such as distances, neighbourhood\nrelationships, and/or topology structures, with the ultimate goal of\napproximating semantic properties of data with preserved geometric properties\nor topology structures in visual space. In this paper, the concept of\nMulti-point Dimensionality Reduction is elaborated on where each data instance\ncan be mapped to (projected onto) possibly more than one point in visual space\nby providing the first general solution (algorithm) for it as a move in the\ndirection of improving reliablity, usability and interpretability of\ndimensionality reduction. Furthermore by allowing the points in visual space to\nbe split into two layers while maintaining the possibility of having more than\none projection (mapping) per data instance , the benefit of separating more\nreliable points from less reliable points is dicussed notwithstanding the\neffort to improve less reliable points. The proposed solution (algorithm) in\nthis paper, named Layered Vertex Splitting Data Embedding (LVSDE), is built\nupon and extends a combination of ordinary DR and graph drawing techniques.\nBased on the experiments of this paper on some data sets, the particular\nproposed algorithm (LVSDE) practically outperforms popular ordinary DR methods\nvisually (semantics, group separation, subgroup detection or combinational\ngroup detection) in a way that is easily explainable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Barahimi_F/0/1/0/all/0/1\">Farshad Barahimi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Monocular Depth Estimation through Virtual-world Supervision and Real-world SfM Self-Supervision. (arXiv:2103.12209v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.12209","description":"<p>Depth information is essential for on-board perception in autonomous driving\nand driver assistance. Monocular depth estimation (MDE) is very appealing since\nit allows for appearance and depth being on direct pixelwise correspondence\nwithout further calibration. Best MDE models are based on Convolutional Neural\nNetworks (CNNs) trained in a supervised manner, i.e., assuming pixelwise ground\ntruth (GT). Usually, this GT is acquired at training time through a calibrated\nmulti-modal suite of sensors. However, also using only a monocular system at\ntraining time is cheaper and more scalable. This is possible by relying on\nstructure-from-motion (SfM) principles to generate self-supervision.\nNevertheless, problems of camouflaged objects, visibility changes,\nstatic-camera intervals, textureless areas, and scale ambiguity, diminish the\nusefulness of such self-supervision. In this paper, we perform monocular depth\nestimation by virtual-world supervision (MonoDEVS) and real-world SfM\nself-supervision. We compensate the SfM self-supervision limitations by\nleveraging virtual-world images with accurate semantic and depth supervision\nand addressing the virtual-to-real domain gap. Our MonoDEVSNet outperforms\nprevious MDE CNNs trained on monocular and even stereo sequences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gurram_A/0/1/0/all/0/1\">Akhil Gurram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuna_A/0/1/0/all/0/1\">Ahmet Faruk Tuna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_F/0/1/0/all/0/1\">Fengyi Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Urfalioglu_O/0/1/0/all/0/1\">Onay Urfalioglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lopez_A/0/1/0/all/0/1\">Antonio M. L&#xf3;pez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Instance-level Image Retrieval using Reranking Transformers. (arXiv:2103.12236v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.12236","description":"<p>Instance-level image retrieval is the task of searching in a large database\nfor images that match an object in a query image. To address this task, systems\nusually rely on a retrieval step that uses global image descriptors, and a\nsubsequent step that performs domain-specific refinements or reranking by\nleveraging operations such as geometric verification based on local features.\nIn this work, we propose Reranking Transformers (RRTs) as a general model to\nincorporate both local and global features to rerank the matching images in a\nsupervised fashion and thus replace the relatively expensive process of\ngeometric verification. RRTs are lightweight and can be easily parallelized so\nthat reranking a set of top matching results can be performed in a single\nforward-pass. We perform extensive experiments on the Revisited Oxford and\nParis datasets, and the Google Landmarks v2 dataset, showing that RRTs\noutperform previous reranking approaches while using much fewer local\ndescriptors. Moreover, we demonstrate that, unlike existing approaches, RRTs\ncan be optimized jointly with the feature extractor, which can lead to feature\nrepresentations tailored to downstream tasks and further accuracy improvements.\nThe code and trained models are publicly available at\nhttps://github.com/uvavision/RerankingTransformer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_F/0/1/0/all/0/1\">Fuwen Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Jiangbo Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ordonez_V/0/1/0/all/0/1\">Vicente Ordonez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MLAN: Multi-Level Adversarial Network for Domain Adaptive Semantic Segmentation. (arXiv:2103.12991v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.12991","description":"<p>Recent progresses in domain adaptive semantic segmentation demonstrate the\neffectiveness of adversarial learning (AL) in unsupervised domain adaptation.\nHowever, most adversarial learning based methods align source and target\ndistributions at a global image level but neglect the inconsistency around\nlocal image regions. This paper presents a novel multi-level adversarial\nnetwork (MLAN) that aims to address inter-domain inconsistency at both global\nimage level and local region level optimally. MLAN has two novel designs,\nnamely, region-level adversarial learning (RL-AL) and co-regularized\nadversarial learning (CR-AL). Specifically, RL-AL models prototypical regional\ncontext-relations explicitly in the feature space of a labelled source domain\nand transfers them to an unlabelled target domain via adversarial learning.\nCR-AL fuses region-level AL and image-level AL optimally via mutual\nregularization. In addition, we design a multi-level consistency map that can\nguide domain adaptation in both input space ($i.e.$, image-to-image\ntranslation) and output space ($i.e.$, self-training) effectively. Extensive\nexperiments show that MLAN outperforms the state-of-the-art with a large margin\nconsistently across multiple datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiaxing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_D/0/1/0/all/0/1\">Dayan Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shijian Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_A/0/1/0/all/0/1\">Aoran Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Object Detection via Probabilistic Ensembling. (arXiv:2104.02904v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.02904","description":"<p>Object detection with multimodal inputs can improve many safety-critical\nsystems such as autonomous vehicles (AVs). Motivated by AVs that operate in\nboth day and night, we study multimodal object detection with RGB and thermal\ncameras, since the latter provides much stronger object signatures under poor\nillumination. We explore strategies for fusing information from different\nmodalities. Our key contribution is a probabilistic ensembling technique,\nProbEn, a simple non-learned method that fuses together detections from\nmulti-modalities. We derive ProbEn from Bayes' rule and first principles that\nassume conditional independence across modalities. Through probabilistic\nmarginalization, ProbEn elegantly handles missing modalities when detectors do\nnot fire on the same object. Importantly, ProbEn also notably improves\nmultimodal detection even when the conditional independence assumption does not\nhold, e.g., fusing outputs from other fusion methods (both off-the-shelf and\ntrained in-house). We validate ProbEn on two benchmarks containing both aligned\n(KAIST) and unaligned (FLIR) multimodal images, showing that ProbEn outperforms\nprior work by more than 13% in relative performance!\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yi-Ting Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jinghao Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Z/0/1/0/all/0/1\">Zelin Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mertz_C/0/1/0/all/0/1\">Christoph Mertz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_S/0/1/0/all/0/1\">Shu Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanan_D/0/1/0/all/0/1\">Deva Ramanan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The art of defense: letting networks fool the attacker. (arXiv:2104.02963v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.02963","description":"<p>Robust environment perception is critical for autonomous cars, and\nadversarial defenses are the most effective and widely studied ways to improve\nthe robustness of environment perception. However, all of previous defense\nmethods decrease the natural accuracy, and the nature of the DNNs itself has\nbeen overlooked. To this end, in this paper, we propose a novel adversarial\ndefense for 3D point cloud classifier that makes full use of the nature of the\nDNNs. Due to the disorder of point cloud, all point cloud classifiers have the\nproperty of permutation invariant to the input point cloud. Based on this\nnature, we design invariant transformations defense (IT-Defense). We show that,\neven after accounting for obfuscated gradients, our IT-Defense is a resilient\ndefense against state-of-the-art (SOTA) 3D attacks. Moreover, IT-Defense do not\nhurt clean accuracy compared to previous SOTA 3D defenses. Our code is\navailable at: {\\footnotesize{\\url{https://github.com/cuge1995/IT-Defense}}}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jinlai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yinpeng Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Binbin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_B/0/1/0/all/0/1\">Bo Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jihong Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuang_M/0/1/0/all/0/1\">Minchi Kuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Houqing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1\">Yanmei Meng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Very Lightweight Photo Retouching Network with Conditional Sequential Modulation. (arXiv:2104.06279v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.06279","description":"<p>Photo retouching aims at improving the aesthetic visual quality of images\nthat suffer from photographic defects, especially for poor contrast, over/under\nexposure, and inharmonious saturation. In practice, photo retouching can be\naccomplished by a series of image processing operations. As most commonly-used\nretouching operations are pixel-independent, i.e., the manipulation on one\npixel is uncorrelated with its neighboring pixels, we can take advantage of\nthis property and design a specialized algorithm for efficient global photo\nretouching. We analyze these global operations and find that they can be\nmathematically formulated by a Multi-Layer Perceptron (MLP). Based on this\nobservation, we propose an extremely lightweight framework -- Conditional\nSequential Retouching Network (CSRNet). Benefiting from the utilization of\n$1\\times1$ convolution, CSRNet only contains less than 37K trainable\nparameters, which are orders of magnitude smaller than existing learning-based\nmethods. Experiments show that our method achieves state-of-the-art performance\non the benchmark MIT-Adobe FiveK dataset quantitively and qualitatively. In\naddition to achieve global photo retouching, the proposed framework can be\neasily extended to learn local enhancement effects. The extended model, namely\nCSRNet-L, also achieves competitive results in various local enhancement tasks.\nCodes are available at https://github.com/lyh-18/CSRNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yihao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jingwen He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiangyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhengwen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hengyuan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1\">Chao Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Action Segmentation by Joint Representation Learning and Online Clustering. (arXiv:2105.13353v6 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.13353","description":"<p>We present a novel approach for unsupervised activity segmentation which uses\nvideo frame clustering as a pretext task and simultaneously performs\nrepresentation learning and online clustering. This is in contrast with prior\nworks where representation learning and clustering are often performed\nsequentially. We leverage temporal information in videos by employing temporal\noptimal transport. In particular, we incorporate a temporal regularization term\nwhich preserves the temporal order of the activity into the standard optimal\ntransport module for computing pseudo-label cluster assignments. The temporal\noptimal transport module enables our approach to learn effective\nrepresentations for unsupervised activity segmentation. Furthermore, previous\nmethods require storing learned features for the entire dataset before\nclustering them in an offline manner, whereas our approach processes one\nmini-batch at a time in an online manner. Extensive evaluations on three public\ndatasets, i.e. 50-Salads, YouTube Instructions, and Breakfast, and our dataset,\ni.e., Desktop Assembly, show that our approach performs on par with or better\nthan previous methods, despite having significantly less memory constraints.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Sateesh Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haresh_S/0/1/0/all/0/1\">Sanjay Haresh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_A/0/1/0/all/0/1\">Awais Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konin_A/0/1/0/all/0/1\">Andrey Konin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zia_M/0/1/0/all/0/1\">M. Zeeshan Zia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_Q/0/1/0/all/0/1\">Quoc-Huy Tran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Redundant representations help generalization in wide neural networks. (arXiv:2106.03485v3 [stat.ML] UPDATED)","link":"http://arxiv.org/abs/2106.03485","description":"<p>Deep neural networks (DNNs) defy the classical bias-variance trade-off:\nadding parameters to a DNN that interpolates its training data will typically\nimprove its generalization performance. Explaining the mechanism behind this\n``benign overfitting'' in deep networks remains an outstanding challenge. Here,\nwe study the last hidden layer representations of various state-of-the-art\nconvolutional neural networks and find that if the last hidden representation\nis wide enough, its neurons tend to split into groups that carry identical\ninformation, and differ from each other only by statistically independent\nnoise. The number of such groups increases linearly with the width of the\nlayer, but only if the width is above a critical value. We show that redundant\nneurons appear only when the training process reaches interpolation and the\ntraining error is zero.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Doimo_D/0/1/0/all/0/1\">Diego Doimo</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Glielmo_A/0/1/0/all/0/1\">Aldo Glielmo</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Goldt_S/0/1/0/all/0/1\">Sebastian Goldt</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Laio_A/0/1/0/all/0/1\">Alessandro Laio</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spectral Unsupervised Domain Adaptation for Visual Recognition. (arXiv:2106.06112v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.06112","description":"<p>Though unsupervised domain adaptation (UDA) has achieved very impressive\nprogress recently, it remains a great challenge due to missing target\nannotations and the rich discrepancy between source and target distributions.\nWe propose Spectral UDA (SUDA), an effective and efficient UDA technique that\nworks in the spectral space and can generalize across different visual\nrecognition tasks. SUDA addresses the UDA challenges from two perspectives.\nFirst, it introduces a spectrum transformer (ST) that mitigates inter-domain\ndiscrepancies by enhancing domain-invariant spectra while suppressing\ndomain-variant spectra of source and target samples simultaneously. Second, it\nintroduces multi-view spectral learning that learns useful unsupervised\nrepresentations by maximizing mutual information among multiple ST-generated\nspectral views of each target sample. Extensive experiments show that SUDA\nachieves superior accuracy consistently across different visual tasks in object\ndetection, semantic segmentation and image classification. Additionally, SUDA\nalso works with the transformer-based network and achieves state-of-the-art\nperformance on object detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jingyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiaxing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1\">Zichen Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shijian Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning of feature points without additional supervision improves reinforcement learning from images. (arXiv:2106.07995v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.07995","description":"<p>In many control problems that include vision, optimal controls can be\ninferred from the location of the objects in the scene. This information can be\nrepresented using feature points, which is a list of spatial locations in\nlearned feature maps of an input image. Previous works show that feature points\nlearned using unsupervised pre-training or human supervision can provide good\nfeatures for control tasks. In this paper, we show that it is possible to learn\nefficient feature point representations end-to-end, without the need for\nunsupervised pre-training, decoders, or additional losses. Our proposed\narchitecture consists of a differentiable feature point extractor that feeds\nthe coordinates of the estimated feature points directly to a soft actor-critic\nagent. The proposed algorithm yields performance competitive to the\nstate-of-the art on DeepMind Control Suite tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Boney_R/0/1/0/all/0/1\">Rinu Boney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilin_A/0/1/0/all/0/1\">Alexander Ilin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kannala_J/0/1/0/all/0/1\">Juho Kannala</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Graph Representation Learning for Road Type Classification. (arXiv:2107.07791v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2107.07791","description":"<p>We present a novel learning-based approach to graph representations of road\nnetworks employing state-of-the-art graph convolutional neural networks. Our\napproach is applied to realistic road networks of 17 cities from Open Street\nMap. While edge features are crucial to generate descriptive graph\nrepresentations of road networks, graph convolutional networks usually rely on\nnode features only. We show that the highly representative edge features can\nstill be integrated into such networks by applying a line graph transformation.\nWe also propose a method for neighborhood sampling based on a topological\nneighborhood composed of both local and global neighbors. We compare the\nperformance of learning representations using different types of neighborhood\naggregation functions in transductive and inductive tasks and in supervised and\nunsupervised learning. Furthermore, we propose a novel aggregation approach,\nGraph Attention Isomorphism Network, GAIN. Our results show that GAIN\noutperforms state-of-the-art methods on the road type classification problem.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gharaee_Z/0/1/0/all/0/1\">Zahra Gharaee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kowshik_S/0/1/0/all/0/1\">Shreyas Kowshik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stromann_O/0/1/0/all/0/1\">Oliver Stromann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Felsberg_M/0/1/0/all/0/1\">Michael Felsberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Classification of Abnormal Hand Movement for Aiding in Autism Detection: Machine Learning Study. (arXiv:2108.07917v6 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.07917","description":"<p>A formal autism diagnosis can be an inefficient and lengthy process. Families\nmay wait months or longer before receiving a diagnosis for their child despite\nevidence that earlier intervention leads to better treatment outcomes. Digital\ntechnologies which detect the presence of behaviors related to autism can scale\naccess to pediatric diagnoses. This work aims to demonstrate the feasibility of\ndeep learning technologies for detecting hand flapping from unstructured home\nvideos as a first step towards validating whether models and digital\ntechnologies can be leveraged to aid with autism diagnoses. We used the\nSelf-Stimulatory Behavior Dataset (SSBD), which contains 75 videos of hand\nflapping, head banging, and spinning exhibited by children. From all the hand\nflapping videos, we extracted 100 positive and control videos of hand flapping,\neach between 2 to 5 seconds in duration. Utilizing both\nlandmark-driven-approaches and MobileNet V2's pretrained convolutional layers,\nour highest performing model achieved a testing F1 score of 84% (90% precision\nand 80% recall) when evaluating with 5-fold cross validation 100 times. This\nwork provides the first step towards developing precise deep learning methods\nfor activity detection of autism-related behaviors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lakkapragada_A/0/1/0/all/0/1\">Anish Lakkapragada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kline_A/0/1/0/all/0/1\">Aaron Kline</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mutlu_O/0/1/0/all/0/1\">Onur Cezmi Mutlu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paskov_K/0/1/0/all/0/1\">Kelley Paskov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chrisman_B/0/1/0/all/0/1\">Brianna Chrisman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stockham_N/0/1/0/all/0/1\">Nate Stockham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Washington_P/0/1/0/all/0/1\">Peter Washington</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wall_D/0/1/0/all/0/1\">Dennis Wall</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards A Fairer Landmark Recognition Dataset. (arXiv:2108.08874v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.08874","description":"<p>We introduce a new landmark recognition dataset, which is created with a\nfocus on fair worldwide representation. While previous work proposes to collect\nas many images as possible from web repositories, we instead argue that such\napproaches can lead to biased data. To create a more comprehensive and\nequitable dataset, we start by defining the fair relevance of a landmark to the\nworld population. These relevances are estimated by combining anonymized Google\nMaps user contribution statistics with the contributors' demographic\ninformation. We present a stratification approach and analysis which leads to a\nmuch fairer coverage of the world, compared to existing datasets. The resulting\ndatasets are used to evaluate computer vision models as part of the the Google\nLandmark Recognition and RetrievalChallenges 2021.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_Z/0/1/0/all/0/1\">Zu Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Araujo_A/0/1/0/all/0/1\">Andr&#xe9; Araujo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_B/0/1/0/all/0/1\">Bingyi Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Askew_C/0/1/0/all/0/1\">Cam Askew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sim_J/0/1/0/all/0/1\">Jack Sim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Green_M/0/1/0/all/0/1\">Mike Green</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yilla_N/0/1/0/all/0/1\">N&#x27;Mah Fodiatu Yilla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weyand_T/0/1/0/all/0/1\">Tobias Weyand</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LLVIP: A Visible-infrared Paired Dataset for Low-light Vision. (arXiv:2108.10831v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.10831","description":"<p>It is very challenging for various visual tasks such as image fusion,\npedestrian detection and image-to-image translation in low light conditions due\nto the loss of effective target areas. In this case, infrared and visible\nimages can be used together to provide both rich detail information and\neffective target areas. In this paper, we present LLVIP, a visible-infrared\npaired dataset for low-light vision. This dataset contains 30976 images, or\n15488 pairs, most of which were taken at very dark scenes, and all of the\nimages are strictly aligned in time and space. Pedestrians in the dataset are\nlabeled. We compare the dataset with other visible-infrared datasets and\nevaluate the performance of some popular visual algorithms including image\nfusion, pedestrian detection and image-to-image translation on the dataset. The\nexperimental results demonstrate the complementary effect of fusion on image\ninformation, and find the deficiency of existing algorithms of the three visual\ntasks in very low-light conditions. We believe the LLVIP dataset will\ncontribute to the community of computer vision by promoting image fusion,\npedestrian detection and image-to-image translation in very low-light\napplications. The dataset is being released in\nhttps://bupt-ai-cz.github.io/LLVIP. Raw data is also provided for further\nresearch such as image registration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1\">Xinyu Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chuang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Minzhen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_W/0/1/0/all/0/1\">Wenqi Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shengjie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wenli Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Model Adaptation: Historical Contrastive Learning for Unsupervised Domain Adaptation without Source Data. (arXiv:2110.03374v6 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.03374","description":"<p>Unsupervised domain adaptation aims to align a labeled source domain and an\nunlabeled target domain, but it requires to access the source data which often\nraises concerns in data privacy, data portability and data transmission\nefficiency. We study unsupervised model adaptation (UMA), or called\nUnsupervised Domain Adaptation without Source Data, an alternative setting that\naims to adapt source-trained models towards target distributions without\naccessing source data. To this end, we design an innovative historical\ncontrastive learning (HCL) technique that exploits historical source hypothesis\nto make up for the absence of source data in UMA. HCL addresses the UMA\nchallenge from two perspectives. First, it introduces historical contrastive\ninstance discrimination (HCID) that learns from target samples by contrasting\ntheir embeddings which are generated by the currently adapted model and the\nhistorical models. With the historical models, HCID encourages UMA to learn\ninstance-discriminative target representations while preserving the source\nhypothesis. Second, it introduces historical contrastive category\ndiscrimination (HCCD) that pseudo-labels target samples to learn\ncategory-discriminative target representations. Specifically, HCCD re-weights\npseudo labels according to their prediction consistency across the current and\nhistorical models. Extensive experiments show that HCL outperforms and\nstate-of-the-art methods consistently across a variety of visual tasks and\nsetups.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiaxing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_D/0/1/0/all/0/1\">Dayan Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_A/0/1/0/all/0/1\">Aoran Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shijian Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vector-quantized Image Modeling with Improved VQGAN. (arXiv:2110.04627v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.04627","description":"<p>Pretraining language models with next-token prediction on massive text\ncorpora has delivered phenomenal zero-shot, few-shot, transfer learning and\nmulti-tasking capabilities on both generative and discriminative language\ntasks. Motivated by this success, we explore a Vector-quantized Image Modeling\n(VIM) approach that involves pretraining a Transformer to predict rasterized\nimage tokens autoregressively. The discrete image tokens are encoded from a\nlearned Vision-Transformer-based VQGAN (ViT-VQGAN). We first propose multiple\nimprovements over vanilla VQGAN from architecture to codebook learning,\nyielding better efficiency and reconstruction fidelity. The improved ViT-VQGAN\nfurther improves vector-quantized image modeling tasks, including\nunconditional, class-conditioned image generation and unsupervised\nrepresentation learning. When trained on ImageNet at \\(256\\times256\\)\nresolution, we achieve Inception Score (IS) of 175.1 and Fr'echet Inception\nDistance (FID) of 4.17, a dramatic improvement over the vanilla VQGAN, which\nobtains 70.6 and 17.04 for IS and FID, respectively. Based on ViT-VQGAN and\nunsupervised pretraining, we further evaluate the pretrained Transformer by\naveraging intermediate features, similar to Image GPT (iGPT). This\nImageNet-pretrained VIM-L significantly beats iGPT-L on linear-probe accuracy\nfrom 60.3% to 73.2% for a similar model size. VIM-L also outperforms iGPT-XL\nwhich is trained with extra web image data and larger model size.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jiahui Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koh_J/0/1/0/all/0/1\">Jing Yu Koh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Han Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_R/0/1/0/all/0/1\">Ruoming Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1\">James Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ku_A/0/1/0/all/0/1\">Alexander Ku</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yuanzhong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baldridge_J/0/1/0/all/0/1\">Jason Baldridge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yonghui Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Class Cell Detection Using Spatial Context Representation. (arXiv:2110.04886v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.04886","description":"<p>In digital pathology, both detection and classification of cells are\nimportant for automatic diagnostic and prognostic tasks. Classifying cells into\nsubtypes, such as tumor cells, lymphocytes or stromal cells is particularly\nchallenging. Existing methods focus on morphological appearance of individual\ncells, whereas in practice pathologists often infer cell classes through their\nspatial context. In this paper, we propose a novel method for both detection\nand classification that explicitly incorporates spatial contextual information.\nWe use the spatial statistical function to describe local density in both a\nmulti-class and a multi-scale manner. Through representation learning and deep\nclustering techniques, we learn advanced cell representation with both\nappearance and spatial context. On various benchmarks, our method achieves\nbetter performance than state-of-the-arts, especially on the classification\ntask. We also create a new dataset for multi-class cell detection and\nclassification in breast cancer and we make both our code and data publicly\navailable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abousamra_S/0/1/0/all/0/1\">Shahira Abousamra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belinsky_D/0/1/0/all/0/1\">David Belinsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arnam_J/0/1/0/all/0/1\">John Van Arnam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allard_F/0/1/0/all/0/1\">Felicia Allard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yee_E/0/1/0/all/0/1\">Eric Yee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1\">Rajarsi Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurc_T/0/1/0/all/0/1\">Tahsin Kurc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samaras_D/0/1/0/all/0/1\">Dimitris Samaras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saltz_J/0/1/0/all/0/1\">Joel Saltz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chao Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"K-Lane: Lidar Lane Dataset and Benchmark for Urban Roads and Highways. (arXiv:2110.11048v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.11048","description":"<p>Lane detection is a critical function for autonomous driving. With the recent\ndevelopment of deep learning and the publication of camera lane datasets and\nbenchmarks, camera lane detection networks (CLDNs) have been remarkably\ndeveloped. Unfortunately, CLDNs rely on camera images which are often distorted\nnear the vanishing line and prone to poor lighting condition. This is in\ncontrast with Lidar lane detection networks (LLDNs), which can directly extract\nthe lane lines on the bird's eye view (BEV) for motion planning and operate\nrobustly under various lighting conditions. However, LLDNs have not been\nactively studied, mostly due to the absence of large public lidar lane\ndatasets. In this paper, we introduce KAIST-Lane (K-Lane), the world's first\nand the largest public urban road and highway lane dataset for Lidar. K-Lane\nhas more than 15K frames and contains annotations of up to six lanes under\nvarious road and traffic conditions, e.g., occluded roads of multiple occlusion\nlevels, roads at day and night times, merging (converging and diverging) and\ncurved lanes. We also provide baseline networks we term Lidar lane detection\nnetworks utilizing global feature correlator (LLDN-GFC). LLDN-GFC exploits the\nspatial characteristics of lane lines on the point cloud, which are sparse,\nthin, and stretched along the entire ground plane of the point cloud. From\nexperimental results, LLDN-GFC achieves the state-of-the-art performance with\nan F1- score of 82.1%, on the K-Lane. Moreover, LLDN-GFC shows strong\nperformance under various lighting conditions, which is unlike CLDNs, and also\nrobust even in the case of severe occlusions, unlike LLDNs using the\nconventional CNN. The K-Lane, LLDN-GFC training code, pre-trained models, and\ncomplete development kits including evaluation, visualization and annotation\ntools are available at https://github.com/kaist-avelab/k-lane.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Paek_D/0/1/0/all/0/1\">Donghee Paek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_S/0/1/0/all/0/1\">Seung-Hyun Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wijaya_K/0/1/0/all/0/1\">Kevin Tirta Wijaya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncertainty-Guided Lung Nodule Segmentation with Feature-Aware Attention. (arXiv:2110.12372v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2110.12372","description":"<p>Since radiologists have different training and clinical experiences, they may\nprovide various segmentation annotations for a lung nodule. Conventional\nstudies choose a single annotation as the learning target by default, but they\nwaste valuable information of consensus or disagreements ingrained in the\nmultiple annotations. This paper proposes an Uncertainty-Guided Segmentation\nNetwork (UGS-Net), which learns the rich visual features from the regions that\nmay cause segmentation uncertainty and contributes to a better segmentation\nresult. With an Uncertainty-Aware Module, this network can provide a\nMulti-Confidence Mask (MCM), pointing out regions with different segmentation\nuncertainty levels. Moreover, this paper introduces a Feature-Aware Attention\nModule to enhance the learning of the nodule boundary and density differences.\nExperimental results show that our method can predict the nodule regions with\ndifferent uncertainty levels and achieve superior performance in LIDC-IDRI\ndataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yang_H/0/1/0/all/0/1\">Han Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shen_L/0/1/0/all/0/1\">Lu Shen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_M/0/1/0/all/0/1\">Mengke Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Q/0/1/0/all/0/1\">Qiuli Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Unified Survey on Anomaly, Novelty, Open-Set, and Out-of-Distribution Detection: Solutions and Future Challenges. (arXiv:2110.14051v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.14051","description":"<p>Machine learning models often encounter samples that are diverged from the\ntraining distribution. Failure to recognize an out-of-distribution (OOD)\nsample, and consequently assign that sample to an in-class label significantly\ncompromises the reliability of a model. The problem has gained significant\nattention due to its importance for safety deploying models in open-world\nsettings. Detecting OOD samples is challenging due to the intractability of\nmodeling all possible unknown distributions. To date, several research domains\ntackle the problem of detecting unfamiliar samples, including anomaly\ndetection, novelty detection, one-class learning, open set recognition, and\nout-of-distribution detection. Despite having similar and shared concepts,\nout-of-distribution, open-set, and anomaly detection have been investigated\nindependently. Accordingly, these research avenues have not cross-pollinated,\ncreating research barriers. While some surveys intend to provide an overview of\nthese approaches, they seem to only focus on a specific domain without\nexamining the relationship between different domains. This survey aims to\nprovide a cross-domain and comprehensive review of numerous eminent works in\nrespective areas while identifying their commonalities. Researchers can benefit\nfrom the overview of research advances in different fields and develop future\nmethodology synergistically. Furthermore, to the best of our knowledge, while\nthere are surveys in anomaly detection or one-class learning, there is no\ncomprehensive or up-to-date survey on out-of-distribution detection, which our\nsurvey covers extensively. Finally, having a unified cross-domain perspective,\nwe discuss and shed light on future lines of research, intending to bring these\nfields closer together.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Salehi_M/0/1/0/all/0/1\">Mohammadreza Salehi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirzaei_H/0/1/0/all/0/1\">Hossein Mirzaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hendrycks_D/0/1/0/all/0/1\">Dan Hendrycks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yixuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohban_M/0/1/0/all/0/1\">Mohammad Hossein Rohban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabokrou_M/0/1/0/all/0/1\">Mohammad Sabokrou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CodEx: A Modular Framework for Joint Temporal De-blurring and Tomographic Reconstruction. (arXiv:2111.06069v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2111.06069","description":"<p>In many computed tomography (CT) imaging applications, it is important to\nrapidly collect data from an object that is moving or changing with time.\nTomographic acquisition is generally assumed to be step-and-shoot, where the\nobject is rotated to each desired angle, and a view is taken. However,\nstep-and-shoot acquisition is slow and can waste photons, so in practice\nfly-scanning is done where the object is continuously rotated while collecting\ndata. However, this can result in motion-blurred views and consequently\nreconstructions with severe motion artifacts.\n</p>\n<p>In this paper, we introduce CodEx, a modular framework for joint de-blurring\nand tomographic reconstruction that can effectively invert the motion blur\nintroduced in sparse view fly-scanning. The method is a synergistic combination\nof a novel acquisition method with a novel non-convex Bayesian reconstruction\nalgorithm. CodEx works by encoding the acquisition with a known binary code\nthat the reconstruction algorithm then inverts. Using a well chosen binary code\nto encode the measurements can improve the accuracy of the inversion process.\nThe CodEx reconstruction method uses the alternating direction method of\nmultipliers (ADMM) to split the inverse problem into iterative deblurring and\nreconstruction sub-problems, making reconstruction practical to implement. We\npresent reconstruction results on both simulated and binned experimental data\nto demonstrate the effectiveness of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Majee_S/0/1/0/all/0/1\">Soumendu Majee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Aslan_S/0/1/0/all/0/1\">Selin Aslan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gursoy_D/0/1/0/all/0/1\">Doga Gursoy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bouman_C/0/1/0/all/0/1\">Charles A. Bouman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised Re-renderable Facial Albedo Reconstruction from Single Image. (arXiv:2111.08282v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.08282","description":"<p>Reconstructing high-fidelity 3D facial texture from a single image is a quite\nchallenging task due to the lack of complete face information and the domain\ngap between the 3D face and 2D image. Further, obtaining re-renderable 3D faces\nhas become a strongly desired property in many applications, where the term\n're-renderable' demands the facial texture to be spatially complete and\ndisentangled with environmental illumination. In this paper, we propose a new\nself-supervised deep learning framework for reconstructing high-quality and\nre-renderable facial albedos from single-view images in-the-wild. Our main idea\nis to first utilize a prior generation module based on the 3DMM proxy model to\nproduce an unwrapped texture and a globally parameterized prior albedo. Then we\napply a detail refinement module to synthesize the final texture with both\nhigh-frequency details and completeness. To further make facial textures\ndisentangled with illumination, we propose a novel detailed illumination\nrepresentation which is reconstructed with the detailed albedo together. We\nalso design several novel regularization losses on both the albedo and\nillumination maps to facilitate the disentanglement of these two factors.\nFinally, by leveraging a differentiable renderer, each face attribute can be\njointly trained in a self-supervised manner without requiring ground-truth\nfacial reflectance. Extensive comparisons and ablation studies on challenging\ndatasets demonstrate that our framework outperforms state-of-the-art\napproaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Mingxin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jianwei Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zhanglin Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaopeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_D/0/1/0/all/0/1\">Dong-Ming Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring dual-attention mechanism with multi-scale feature extraction scheme for skin lesion segmentation. (arXiv:2111.08708v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2111.08708","description":"<p>Automatic segmentation of skin lesions from dermoscopic images is a\nchallenging task due to the irregular lesion boundaries, poor contrast between\nthe lesion and the background, and the presence of artifacts. In this work, a\nnew convolutional neural network-based approach is proposed for skin lesion\nsegmentation. In this work, a novel multi-scale feature extraction module is\nproposed for extracting more discriminative features for dealing with the\nchallenges related to complex skin lesions; this module is embedded in the\nUNet, replacing the convolutional layers in the standard architecture. Further\nin this work, two different attention mechanisms refine the feature extracted\nby the encoder and the post-upsampled features. This work was evaluated using\nthe two publicly available datasets, including ISBI2017 and ISIC2018 datasets.\nThe proposed method reported an accuracy, recall, and JSI of 97.5%, 94.29%,\n91.16% on the ISBI2017 dataset and 95.92%, 95.37%, 91.52% on the ISIC2018\ndataset. It outperformed the existing methods and the top-ranked models in the\nrespective competitions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chowdary_G/0/1/0/all/0/1\">G Jignesh Chowdary</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yathisha_G/0/1/0/all/0/1\">G V S N Durga Yathisha</a>, <a href=\"http://arxiv.org/find/eess/1/au:+G_S/0/1/0/all/0/1\">Suganya G</a>, <a href=\"http://arxiv.org/find/eess/1/au:+M_P/0/1/0/all/0/1\">Premalatha M</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-Shot Object Detection via Association and DIscrimination. (arXiv:2111.11656v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.11656","description":"<p>Object detection has achieved substantial progress in the last decade.\nHowever, detecting novel classes with only few samples remains challenging,\nsince deep learning under low data regime usually leads to a degraded feature\nspace. Existing works employ a holistic fine-tuning paradigm to tackle this\nproblem, where the model is first pre-trained on all base classes with abundant\nsamples, and then it is used to carve the novel class feature space.\nNonetheless, this paradigm is still imperfect. Durning fine-tuning, a novel\nclass may implicitly leverage the knowledge of multiple base classes to\nconstruct its feature space, which induces a scattered feature space, hence\nviolating the inter-class separability. To overcome these obstacles, we propose\na two-step fine-tuning framework, Few-shot object detection via Association and\nDIscrimination (FADI), which builds up a discriminative feature space for each\nnovel class with two integral steps. 1) In the association step, in contrast to\nimplicitly leveraging multiple base classes, we construct a compact novel class\nfeature space via explicitly imitating a specific base class feature space.\nSpecifically, we associate each novel class with a base class according to\ntheir semantic similarity. After that, the feature space of a novel class can\nreadily imitate the well-trained feature space of the associated base class. 2)\nIn the discrimination step, to ensure the separability between the novel\nclasses and associated base classes, we disentangle the classification branches\nfor base and novel classes. To further enlarge the inter-class separability\nbetween all classes, a set-specialized margin loss is imposed. Extensive\nexperiments on Pascal VOC and MS-COCO datasets demonstrate FADI achieves new\nSOTA performance, significantly improving the baseline in any shot/split by\n+18.7. Notably, the advantage is most announced on extremely few-shot\nscenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yuhang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiaqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Ying Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1\">Dahua Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Point-BERT: Pre-training 3D Point Cloud Transformers with Masked Point Modeling. (arXiv:2111.14819v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.14819","description":"<p>We present Point-BERT, a new paradigm for learning Transformers to generalize\nthe concept of BERT to 3D point cloud. Inspired by BERT, we devise a Masked\nPoint Modeling (MPM) task to pre-train point cloud Transformers. Specifically,\nwe first divide a point cloud into several local point patches, and a point\ncloud Tokenizer with a discrete Variational AutoEncoder (dVAE) is designed to\ngenerate discrete point tokens containing meaningful local information. Then,\nwe randomly mask out some patches of input point clouds and feed them into the\nbackbone Transformers. The pre-training objective is to recover the original\npoint tokens at the masked locations under the supervision of point tokens\nobtained by the Tokenizer. Extensive experiments demonstrate that the proposed\nBERT-style pre-training strategy significantly improves the performance of\nstandard point cloud Transformers. Equipped with our pre-training strategy, we\nshow that a pure Transformer architecture attains 93.8% accuracy on ModelNet40\nand 83.1% accuracy on the hardest setting of ScanObjectNN, surpassing carefully\ndesigned point cloud models with much fewer hand-made designs. We also\ndemonstrate that the representations learned by Point-BERT transfer well to new\ntasks and domains, where our models largely advance the state-of-the-art of\nfew-shot point cloud classification task. The code and pre-trained models are\navailable at https://github.com/lulutang0608/Point-BERT\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xumin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_L/0/1/0/all/0/1\">Lulu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_Y/0/1/0/all/0/1\">Yongming Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Tiejun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiwen Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dual-Flow Transformation Network for Deformable Image Registration with Region Consistency Constraint. (arXiv:2112.02249v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.02249","description":"<p>Deformable image registration is able to achieve fast and accurate alignment\nbetween a pair of images and thus plays an important role in many medical image\nstudies. The current deep learning (DL)-based image registration approaches\ndirectly learn the spatial transformation from one image to another by\nleveraging a convolutional neural network, requiring ground truth or similarity\nmetric. Nevertheless, these methods only use a global similarity energy\nfunction to evaluate the similarity of a pair of images, which ignores the\nsimilarity of regions of interest (ROIs) within images. Moreover, DL-based\nmethods often estimate global spatial transformations of image directly, which\nnever pays attention to region spatial transformations of ROIs within images.\nIn this paper, we present a novel dual-flow transformation network with region\nconsistency constraint which maximizes the similarity of ROIs within a pair of\nimages and estimates both global and region spatial transformations\nsimultaneously. Experiments on four public 3D MRI datasets show that the\nproposed method achieves the best registration performance in accuracy and\ngeneralization compared with other state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xinke Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yibo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1\">Yong Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Differentiable Gaussianization Layers for Inverse Problems Regularized by Deep Generative Models. (arXiv:2112.03860v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.03860","description":"<p>Deep generative models such as GANs and normalizing flows are powerful\npriors. They can regularize inverse problems to reduce ill-posedness and attain\nhigh-quality results. However, the latent vector of such deep generative models\ncan fall out of the desired high-dimensional standard Gaussian distribution\nduring an inversion, particularly in the presence of noise in data or\ninaccurate forward models. In such a case, deep generative models are\nineffective in attaining high-fidelity solutions. To address this issue, we\npropose to reparameterize and Gaussianize the latent vector using novel\ndifferentiable data-dependent layers wherein custom operators are defined by\nsolving optimization problems. These proposed layers constrain an inversion to\nfind feasible in-distribution solutions. We tested and validated our technique\non three inversion tasks: compressive-sensing MRI, image deblurring, and\neikonal tomography (a nonlinear PDE-constrained inverse problem), using two\nrepresentative deep generative models: StyleGAN2 and Glow, and achieved\nstate-of-the-art results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dongzhuo Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Shaping Visual Representations with Attributes for Few-Shot Recognition. (arXiv:2112.06398v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.06398","description":"<p>Few-shot recognition aims to recognize novel categories under low-data\nregimes. Some recent few-shot recognition methods introduce auxiliary semantic\nmodality, i.e., category attribute information, into representation learning,\nwhich enhances the feature discrimination and improves the recognition\nperformance. Most of these existing methods only consider the attribute\ninformation of support set while ignoring the query set, resulting in a\npotential loss of performance. In this letter, we propose a novel\nattribute-shaped learning (ASL) framework, which can jointly perform query\nattributes generation and discriminative visual representation learning for\nfew-shot recognition. Specifically, a visual-attribute predictor (VAP) is\nconstructed to predict the attributes of queries. By leveraging the attributes\ninformation, an attribute-visual attention module (AVAM) is designed, which can\nadaptively utilize attributes and visual representations to learn more\ndiscriminative features. Under the guidance of attribute modality, our method\ncan learn enhanced semantic-aware representation for classification.\nExperiments demonstrate that our method can achieve competitive results on CUB\nand SUN benchmarks. Our source code is available at:\n\\url{https://github.com/chenhaoxing/ASL}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haoxing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Huaxiong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yaohui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chunlin Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Structure-Aware Image Segmentation with Homotopy Warping. (arXiv:2112.07812v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.07812","description":"<p>Besides per-pixel accuracy, topological correctness is also crucial for the\nsegmentation of images with fine-scale structures, e.g., satellite images and\nbiomedical images. In this paper, by leveraging the theory of digital topology,\nwe identify locations in an image that are critical for topology. By focusing\non these critical locations, we propose a new homotopy warping loss to train\ndeep image segmentation networks for better topological accuracy. To\nefficiently identity these topologically critical locations, we propose a new\nalgorithm exploiting the distance transform. The proposed algorithm, as well as\nthe loss function, naturally generalize to different topological structures in\nboth 2D and 3D settings. The proposed loss function helps deep nets achieve\nbetter performance in terms of topology-aware metrics, outperforming\nstate-of-the-art structure/topology-aware segmentation methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaoling Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MVSS-Net: Multi-View Multi-Scale Supervised Networks for Image Manipulation Detection. (arXiv:2112.08935v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.08935","description":"<p>As manipulating images by copy-move, splicing and/or inpainting may lead to\nmisinterpretation of the visual content, detecting these sorts of manipulations\nis crucial for media forensics. Given the variety of possible attacks on the\ncontent, devising a generic method is nontrivial. Current deep learning based\nmethods are promising when training and test data are well aligned, but perform\npoorly on independent tests. Moreover, due to the absence of authentic test\nimages, their image-level detection specificity is in doubt. The key question\nis how to design and train a deep neural network capable of learning\ngeneralizable features sensitive to manipulations in novel data, whilst\nspecific to prevent false alarms on the authentic. We propose multi-view\nfeature learning to jointly exploit tampering boundary artifacts and the noise\nview of the input image. As both clues are meant to be semantic-agnostic, the\nlearned features are thus generalizable. For effectively learning from\nauthentic images, we train with multi-scale (pixel / edge / image) supervision.\nWe term the new network MVSS-Net and its enhanced version MVSS-Net++.\nExperiments are conducted in both within-dataset and cross-dataset scenarios,\nshowing that MVSS-Net++ performs the best, and exhibits better robustness\nagainst JPEG compression, Gaussian blur and screenshot based image\nre-capturing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1\">Chengbo Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xinru Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1\">Ruohan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Juan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xirong Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unpaired Referring Expression Grounding via Bidirectional Cross-Modal Matching. (arXiv:2201.06686v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.06686","description":"<p>Referring expression grounding is an important and challenging task in\ncomputer vision. To avoid the laborious annotation in conventional referring\ngrounding, unpaired referring grounding is introduced, where the training data\nonly contains a number of images and queries without correspondences. The few\nexisting solutions to unpaired referring grounding are still preliminary, due\nto the challenges of learning image-text matching and lack of the top-down\nguidance with unpaired data. In this paper, we propose a novel bidirectional\ncross-modal matching (BiCM) framework to address these challenges.\nParticularly, we design a query-aware attention map (QAM) module that\nintroduces top-down perspective via generating query-specific visual attention\nmaps. A cross-modal object matching (COM) module is further introduced, which\nexploits the recently emerged image-text matching pretrained model, CLIP, to\npredict the target objects from a bottom-up perspective. The top-down and\nbottom-up predictions are then integrated via a similarity funsion (SF) module.\nWe also propose a knowledge adaptation matching (KAM) module that leverages\nunpaired training data to adapt pretrained knowledge to the target dataset and\ntask. Experiments show that our framework outperforms previous works by 6.55%\nand 9.94% on two popular grounding datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Hengcan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayat_M/0/1/0/all/0/1\">Munawar Hayat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jianfei Cai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Kernelized Dense Geometric Matching. (arXiv:2202.00667v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.00667","description":"<p>Geometric matching is a challenging computer vision task that involves\nfinding correspondences between two views of a 3D scene. Dense geometric\nmatching, i.e., finding every matching pixel, is an appealing approach due to,\namong other things, the capacity for sub-pixel accuracy and low-texture\nrobustness. While previous results have shown that sparse and semi-sparse\nmethods are better suited than dense approaches for geometry estimation, we\npropose a novel dense method that outperforms them. We accomplish this by\nformulating dense global matching as a probabilistic regression task using deep\nkernels, in contrast to typical correlation volume processing. Furthermore, we\nshow that replacing local correlation with warped feature stacking in the\nrefinement stage further boosts performance. Finally, we observe that a\nsystematic attenuation of the model confidence improves geometry estimation\nresults. Our full approach, $\\textbf{D}$eep $\\textbf{K}$ernelized Dense\nGeometric $\\textbf{M}$atching, sets a new state-of-the-art on the competitive\nHPatches, YFCC100m, MegaDepth-1500, and ScanNet-1500 geometry estimation\nbenchmarks. We provide code for all our experiments, instructions for\ndownloading datasets, and pretrained models, at https://github.com/Parskatt/dkm\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Edstedt_J/0/1/0/all/0/1\">Johan Edstedt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wadenback_M/0/1/0/all/0/1\">M&#xe5;rten Wadenb&#xe4;ck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Felsberg_M/0/1/0/all/0/1\">Michael Felsberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Student Dangerous Behavior Detection in School. (arXiv:2202.09550v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.09550","description":"<p>Video surveillance systems have been installed to ensure the student safety\nin schools. However, discovering dangerous behaviors, such as fighting and\nfalling down, usually depends on untimely human observations. In this paper, we\nfocus on detecting dangerous behaviors of students automatically, which faces\nnumerous challenges, such as insufficient datasets, confusing postures,\nkeyframes detection and prompt response. To address these challenges, we first\nbuild a danger behavior dataset with locations and labels from surveillance\nvideos, and transform action recognition of long videos to an object detection\ntask that avoids keyframes detection. Then, we propose a novel end-to-end\ndangerous behavior detection method, named DangerDet, that combines multi-scale\nbody features and keypoints-based pose features. We could improve the accuracy\nof behavior classification due to the highly correlation between pose and\nbehavior. On our dataset, DangerDet achieves 71.0\\% mAP with about 11 FPS. It\nkeeps a better balance between the accuracy and time cost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Huayi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_F/0/1/0/all/0/1\">Fei Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Hongtao Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformers in Medical Image Analysis: A Review. (arXiv:2202.12165v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.12165","description":"<p>Transformers have dominated the field of natural language processing, and\nrecently impacted the computer vision area. In the field of medical image\nanalysis, Transformers have also been successfully applied to full-stack\nclinical applications, including image synthesis/reconstruction, registration,\nsegmentation, detection, and diagnosis. Our paper aims to promote awareness and\napplication of Transformers in the field of medical image analysis.\nSpecifically, we first overview the core concepts of the attention mechanism\nbuilt into Transformers and other basic components. Second, we review various\nTransformer architectures tailored for medical image applications and discuss\ntheir limitations. Within this review, we investigate key challenges revolving\naround the use of Transformers in different learning paradigms, improving the\nmodel efficiency, and their coupling with other techniques. We hope this review\ncan give a comprehensive picture of Transformers to the readers in the field of\nmedical image analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1\">Kelei He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1\">Chen Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhuoyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rekik_I/0/1/0/all/0/1\">Islem Rekik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1\">Zihao Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_W/0/1/0/all/0/1\">Wen Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Junfeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_D/0/1/0/all/0/1\">Dinggang Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Point Cloud Representation Learning with Deep Neural Networks: A Survey. (arXiv:2202.13589v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.13589","description":"<p>Point cloud data have been widely explored due to its superior accuracy and\nrobustness under various adverse situations. Meanwhile, deep neural networks\n(DNNs) have achieved very impressive success in various applications such as\nsurveillance and autonomous driving. The convergence of point cloud and DNNs\nhas led to many deep point cloud models, largely trained under the supervision\nof large-scale and densely-labelled point cloud data. Unsupervised point cloud\nrepresentation learning, which aims to learn general and useful point cloud\nrepresentations from unlabelled point cloud data, has recently attracted\nincreasing attention due to the constraint in large-scale point cloud\nlabelling. This paper provides a comprehensive review of unsupervised point\ncloud representation learning using DNNs. It first describes the motivation,\ngeneral pipelines as well as terminologies of the recent studies. Relevant\nbackground including widely adopted point cloud datasets and DNN architectures\nis then briefly presented. This is followed by an extensive discussion of\nexisting unsupervised point cloud representation learning methods according to\ntheir technical approaches. We also quantitatively benchmark and discuss the\nreviewed methods over multiple widely adopted point cloud datasets. Finally, we\nshare our humble opinion about several challenges and problems that could be\npursued in future research in unsupervised point cloud representation learning.\nA project associated with this survey has been built at\nhttps://github.com/xiaoaoran/3d_url_survey.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_A/0/1/0/all/0/1\">Aoran Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiaxing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_D/0/1/0/all/0/1\">Dayan Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaoqin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shijian Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NeW CRFs: Neural Window Fully-connected CRFs for Monocular Depth Estimation. (arXiv:2203.01502v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.01502","description":"<p>Estimating the accurate depth from a single image is challenging since it is\ninherently ambiguous and ill-posed. While recent works design increasingly\ncomplicated and powerful networks to directly regress the depth map, we take\nthe path of CRFs optimization. Due to the expensive computation, CRFs are\nusually performed between neighborhoods rather than the whole graph. To\nleverage the potential of fully-connected CRFs, we split the input into windows\nand perform the FC-CRFs optimization within each window, which reduces the\ncomputation complexity and makes FC-CRFs feasible. To better capture the\nrelationships between nodes in the graph, we exploit the multi-head attention\nmechanism to compute a multi-head potential function, which is fed to the\nnetworks to output an optimized depth map. Then we build a bottom-up-top-down\nstructure, where this neural window FC-CRFs module serves as the decoder, and a\nvision transformer serves as the encoder. The experiments demonstrate that our\nmethod significantly improves the performance across all metrics on both the\nKITTI and NYUv2 datasets, compared to previous methods. Furthermore, the\nproposed method can be directly applied to panorama images and outperforms all\nprevious panorama methods on the MatterPort3D dataset. Project page:\nhttps://weihaosky.github.io/newcrfs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_W/0/1/0/all/0/1\">Weihao Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1\">Xiaodong Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Z/0/1/0/all/0/1\">Zuozhuo Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Siyu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_P/0/1/0/all/0/1\">Ping Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Depth-Independent Depth Completion via Least Square Estimation. (arXiv:2203.03317v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.03317","description":"<p>The depth completion task aims to complete a per-pixel dense depth map from a\nsparse depth map. In this paper, we propose an efficient least square based\ndepth-independent method to complete the sparse depth map utilizing the RGB\nimage and the sparse depth map in two independent stages. In this way can we\ndecouple the neural network and the sparse depth input, so that when some\nfeatures of the sparse depth map change, such as the sparsity, our method can\nstill produce a promising result. Moreover, due to the positional encoding and\nlinear procession in our pipeline, we can easily produce a super-resolution\ndense depth map of high quality. We also test the generalization of our method\non different datasets compared to some state-of-the-art algorithms. Experiments\non the benchmark show that our method produces competitive performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fang_X/0/1/0/all/0/1\">Xianze Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunkai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zexi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_R/0/1/0/all/0/1\">Rong Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Learning for Cross-Domain Open World Recognition. (arXiv:2203.09257v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.09257","description":"<p>The ability to evolve is fundamental for any valuable autonomous agent whose\nknowledge cannot remain limited to that injected by the manufacturer. Consider\nfor example a home assistant robot: it should be able to incrementally learn\nnew object categories when requested, but also to recognize the same objects in\ndifferent environments (rooms) and poses (hand-held/on the floor/above\nfurniture), while rejecting unknown ones. Despite its importance, this scenario\nhas started to raise interest in the robotic community only recently and the\nrelated research is still in its infancy, with existing experimental testbeds\nbut no tailored methods. With this work, we propose the first learning approach\nthat deals with all the previously mentioned challenges at once by exploiting a\nsingle contrastive objective. We show how it learns a feature space perfectly\nsuitable to incrementally include new classes and is able to capture knowledge\nwhich generalizes across a variety of visual domains. Our method is endowed\nwith a tailored effective stopping criterion for each learning episode and\nexploits a self-paced thresholding strategy that provides the classifier with a\nreliable rejection option. Both these novel contributions are based on the\nobservation of the data statistics and do not need manual tuning. An extensive\nexperimental analysis confirms the effectiveness of the proposed approach in\nestablishing the new state-of-the-art. The code is available at\nhttps://github.com/FrancescoCappio/Contrastive_Open_World.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Borlino_F/0/1/0/all/0/1\">Francesco Cappio Borlino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bucci_S/0/1/0/all/0/1\">Silvia Bucci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tommasi_T/0/1/0/all/0/1\">Tatiana Tommasi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Operator Sketching for Deep Unrolling Networks. (arXiv:2203.11156v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.11156","description":"<p>In this work we propose a new paradigm for designing efficient deep unrolling\nnetworks using operator sketching. The deep unrolling networks are currently\nthe state-of-the-art solutions for imaging inverse problems. However, for\nhigh-dimensional imaging tasks, especially the 3D cone-beam X-ray CT and 4D MRI\nimaging, the deep unrolling schemes typically become inefficient both in terms\nof memory and computation, due to the need of computing multiple times the\nhigh-dimensional forward and adjoint operators. Recently researchers have found\nthat such limitations can be partially addressed by stochastic unrolling with\nsubsets of operators, inspired by the success of stochastic first-order\noptimization. In this work, we propose a further acceleration upon stochastic\nunrolling, using sketching techniques to approximate products in the\nhigh-dimensional image space. The operator sketching can be jointly applied\nwith stochastic unrolling for the best acceleration and compression\nperformance. Our numerical experiments on X-ray CT image reconstruction\ndemonstrate the remarkable effectiveness of our sketched unrolling schemes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Junqi Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_S/0/1/0/all/0/1\">Subhadip Mukherjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schonlieb_C/0/1/0/all/0/1\">Carola-Bibiane Sch&#xf6;nlieb</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions. (arXiv:2203.12667v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.12667","description":"<p>A long-term goal of AI research is to build intelligent agents that can\ncommunicate with humans in natural language, perceive the environment, and\nperform real-world tasks. Vision-and-Language Navigation (VLN) is a fundamental\nand interdisciplinary research topic towards this goal, and receives increasing\nattention from natural language processing, computer vision, robotics, and\nmachine learning communities. In this paper, we review contemporary studies in\nthe emerging field of VLN, covering tasks, evaluation metrics, methods, etc.\nThrough structured analysis of current progress and challenges, we highlight\nthe limitations of current VLN and opportunities for future work. This paper\nserves as a thorough reference for the VLN research community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jing Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stefani_E/0/1/0/all/0/1\">Eliana Stefani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomason_J/0/1/0/all/0/1\">Jesse Thomason</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Eric Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LiDAR Snowfall Simulation for Robust 3D Object Detection. (arXiv:2203.15118v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.15118","description":"<p>3D object detection is a central task for applications such as autonomous\ndriving, in which the system needs to localize and classify surrounding traffic\nagents, even in the presence of adverse weather. In this paper, we address the\nproblem of LiDAR-based 3D object detection under snowfall. Due to the\ndifficulty of collecting and annotating training data in this setting, we\npropose a physically based method to simulate the effect of snowfall on real\nclear-weather LiDAR point clouds. Our method samples snow particles in 2D space\nfor each LiDAR line and uses the induced geometry to modify the measurement for\neach LiDAR beam accordingly. Moreover, as snowfall often causes wetness on the\nground, we also simulate ground wetness on LiDAR point clouds. We use our\nsimulation to generate partially synthetic snowy LiDAR data and leverage these\ndata for training 3D object detection models that are robust to snowfall. We\nconduct an extensive evaluation using several state-of-the-art 3D object\ndetection methods and show that our simulation consistently yields significant\nperformance gains on the real snowy STF dataset compared to clear-weather\nbaselines and competing simulation approaches, while not sacrificing\nperformance in clear weather. Our code is available at\nwww.github.com/SysCV/LiDAR_snow_sim.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hahner_M/0/1/0/all/0/1\">Martin Hahner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sakaridis_C/0/1/0/all/0/1\">Christos Sakaridis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bijelic_M/0/1/0/all/0/1\">Mario Bijelic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heide_F/0/1/0/all/0/1\">Felix Heide</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fisher Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1\">Dengxin Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge-based Entity Prediction for Improved Machine Perception in Autonomous Systems. (arXiv:2203.16616v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2203.16616","description":"<p>Knowledge-based entity prediction (KEP) is a novel task that aims to improve\nmachine perception in autonomous systems. KEP leverages relational knowledge\nfrom heterogeneous sources in predicting potentially unrecognized entities. In\nthis paper, we provide a formal definition of KEP as a knowledge completion\ntask. Three potential solutions are then introduced, which employ several\nmachine learning and data mining techniques. Finally, the applicability of KEP\nis demonstrated on two autonomous systems from different domains; namely,\nautonomous driving and smart manufacturing. We argue that in complex real-world\nsystems, the use of KEP would significantly improve machine perception while\npushing the current technology one step closer to achieving full autonomy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wickramarachchi_R/0/1/0/all/0/1\">Ruwan Wickramarachchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henson_C/0/1/0/all/0/1\">Cory Henson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheth_A/0/1/0/all/0/1\">Amit Sheth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PixelFolder: An Efficient Progressive Pixel Synthesis Network for Image Generation. (arXiv:2204.00833v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.00833","description":"<p>Pixel synthesis is a promising research paradigm for image generation, which\ncan well exploit pixel-wise prior knowledge for generation. However, existing\nmethods still suffer from excessive memory footprint and computation overhead.\nIn this paper, we propose a progressive pixel synthesis network towards\nefficient image generation, coined as PixelFolder. Specifically, PixelFolder\nformulates image generation as a progressive pixel regression problem and\nsynthesizes images by a multi-stage paradigm, which can greatly reduce the\noverhead caused by large tensor transformations. In addition, we introduce\nnovel pixel folding operations to further improve model efficiency while\nmaintaining pixel-wise prior knowledge for end-to-end regression. With these\ninnovative designs, we greatly reduce the expenditure of pixel synthesis, e.g.,\nreducing 90% computation and 57% parameters compared to the latest pixel\nsynthesis method called CIPS. To validate our approach, we conduct extensive\nexperiments on two benchmark datasets, namely FFHQ and LSUN Church. The\nexperimental results show that with much less expenditure, PixelFolder obtains\nnew state-of-the-art (SOTA) performance on two benchmark datasets, i.e., 3.77\nFID and 2.45 FID on FFHQ and LSUN Church, respectively. Meanwhile, PixelFolder\nis also more efficient than the SOTA methods like StyleGAN2, reducing about 74%\ncomputation and 36% parameters, respectively. These results greatly validate\nthe effectiveness of the proposed PixelFolder.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jing He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yiyi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1\">Jun Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yunhang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiaoshuai Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"E^2TAD: An Energy-Efficient Tracking-based Action Detector. (arXiv:2204.04416v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.04416","description":"<p>Video action detection (spatio-temporal action localization) is usually the\nstarting point for human-centric intelligent analysis of videos nowadays. It\nhas high practical impacts for many applications across robotics, security,\nhealthcare, etc. The two-stage paradigm of Faster R-CNN inspires a standard\nparadigm of video action detection in object detection, i.e., firstly\ngenerating person proposals and then classifying their actions. However, none\nof the existing solutions could provide fine-grained action detection to the\n\"who-when-where-what\" level. This paper presents a tracking-based solution to\naccurately and efficiently localize predefined key actions spatially (by\npredicting the associated target IDs and locations) and temporally (by\npredicting the time in exact frame indices). This solution won first place in\nthe UAV-Video Track of 2021 Low-Power Computer Vision Challenge (LPCVC).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xin Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhenyu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_H/0/1/0/all/0/1\">Hao-Yu Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_S/0/1/0/all/0/1\">Siqi Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_T/0/1/0/all/0/1\">Taiyu Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhenyu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pi_P/0/1/0/all/0/1\">Pengcheng Pi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1\">Zhou Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_G/0/1/0/all/0/1\">Gang Hua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CPGNet: Cascade Point-Grid Fusion Network for Real-Time LiDAR Semantic Segmentation. (arXiv:2204.09914v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.09914","description":"<p>LiDAR semantic segmentation essential for advanced autonomous driving is\nrequired to be accurate, fast, and easy-deployed on mobile platforms. Previous\npoint-based or sparse voxel-based methods are far away from real-time\napplications since time-consuming neighbor searching or sparse 3D convolution\nare employed. Recent 2D projection-based methods, including range view and\nmulti-view fusion, can run in real time, but suffer from lower accuracy due to\ninformation loss during the 2D projection. Besides, to improve the performance,\nprevious methods usually adopt test time augmentation (TTA), which further\nslows down the inference process. To achieve a better speed-accuracy trade-off,\nwe propose Cascade Point-Grid Fusion Network (CPGNet), which ensures both\neffectiveness and efficiency mainly by the following two techniques: 1) the\nnovel Point-Grid (PG) fusion block extracts semantic features mainly on the 2D\nprojected grid for efficiency, while summarizes both 2D and 3D features on 3D\npoint for minimal information loss; 2) the proposed transformation consistency\nloss narrows the gap between the single-time model inference and TTA. The\nexperiments on the SemanticKITTI and nuScenes benchmarks demonstrate that the\nCPGNet without ensemble models or TTA is comparable with the state-of-the-art\nRPVNet, while it runs 4.7 times faster.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoyan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Gang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_H/0/1/0/all/0/1\">Hongyu Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhenhua Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Attention Emerges from Recurrent Sparse Reconstruction. (arXiv:2204.10962v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.10962","description":"<p>Visual attention helps achieve robust perception under noise, corruption, and\ndistribution shifts in human vision, which are areas where modern neural\nnetworks still fall short. We present VARS, Visual Attention from Recurrent\nSparse reconstruction, a new attention formulation built on two prominent\nfeatures of the human visual attention mechanism: recurrency and sparsity.\nRelated features are grouped together via recurrent connections between\nneurons, with salient objects emerging via sparse regularization. VARS adopts\nan attractor network with recurrent connections that converges toward a stable\npattern over time. Network layers are represented as ordinary differential\nequations (ODEs), formulating attention as a recurrent attractor network that\nequivalently optimizes the sparse reconstruction of input using a dictionary of\n\"templates\" encoding underlying patterns of data. We show that self-attention\nis a special case of VARS with a single-step optimization and no sparsity\nconstraint. VARS can be readily used as a replacement for self-attention in\npopular vision transformers, consistently improving their robustness across\nvarious benchmarks. Code is released on GitHub (https://github.com/bfshi/VARS).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1\">Baifeng Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yale Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_N/0/1/0/all/0/1\">Neel Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1\">Trevor Darrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data-Efficient Backdoor Attacks. (arXiv:2204.12281v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.12281","description":"<p>Recent studies have proven that deep neural networks are vulnerable to\nbackdoor attacks. Specifically, by mixing a small number of poisoned samples\ninto the training set, the behavior of the trained model can be maliciously\ncontrolled. Existing attack methods construct such adversaries by randomly\nselecting some clean data from the benign set and then embedding a trigger into\nthem. However, this selection strategy ignores the fact that each poisoned\nsample contributes inequally to the backdoor injection, which reduces the\nefficiency of poisoning. In this paper, we formulate improving the poisoned\ndata efficiency by the selection as an optimization problem and propose a\nFiltering-and-Updating Strategy (FUS) to solve it. The experimental results on\nCIFAR-10 and ImageNet-10 indicate that the proposed method is effective: the\nsame attack success rate can be achieved with only 47% to 75% of the poisoned\nsample volume compared to the random selection strategy. More importantly, the\nadversaries selected according to one setting can generalize well to other\nsettings, exhibiting strong transferability. The prototype code of our method\nis now available at https://github.com/xpf/Data-Efficient-Backdoor-Attacks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xia_P/0/1/0/all/0/1\">Pengfei Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Ziqiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attention-based Knowledge Distillation in Multi-attention Tasks: The Impact of a DCT-driven Loss. (arXiv:2205.01997v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.01997","description":"<p>Knowledge Distillation (KD) is a strategy for the definition of a set of\ntransferability gangways to improve the efficiency of Convolutional Neural\nNetworks. Feature-based Knowledge Distillation is a subfield of KD that relies\non intermediate network representations, either unaltered or depth-reduced via\nmaximum activation maps, as the source knowledge. In this paper, we propose and\nanalyse the use of a 2D frequency transform of the activation maps before\ntransferring them. We pose that\\textemdash by using global image cues rather\nthan pixel estimates, this strategy enhances knowledge transferability in tasks\nsuch as scene recognition, defined by strong spatial and contextual\nrelationships between multiple and varied concepts. To validate the proposed\nmethod, an extensive evaluation of the state-of-the-art in scene recognition is\npresented. Experimental results provide strong evidences that the proposed\nstrategy enables the student network to better focus on the relevant image\nareas learnt by the teacher network, hence leading to better descriptive\nfeatures and higher transferred performance than every other state-of-the-art\nalternative. We publicly release the training and evaluation framework used\nalong this paper at\n<a href=\"http://www-vpu.eps.uam.es/publications/DCTBasedKDForSceneRecognition.\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lopez_Cifuentes_A/0/1/0/all/0/1\">Alejandro L&#xf3;pez-Cifuentes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Escudero_Vinolo_M/0/1/0/all/0/1\">Marcos Escudero-Vi&#xf1;olo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bescos_J/0/1/0/all/0/1\">Jes&#xfa;s Besc&#xf3;s</a>, <a href=\"http://arxiv.org/find/cs/1/au:+SanMiguel_J/0/1/0/all/0/1\">Juan C. SanMiguel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End Rubbing Restoration Using Generative Adversarial Networks. (arXiv:2205.03743v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.03743","description":"<p>Rubbing restorations are significant for preserving world cultural history.\nIn this paper, we propose the RubbingGAN model for restoring incomplete rubbing\ncharacters. Specifically, we collect characters from the Zhang Menglong Bei and\nbuild up the first rubbing restoration dataset. We design the first generative\nadversarial network for rubbing restoration. Based on the dataset we collect,\nwe apply the RubbingGAN to learn the Zhang Menglong Bei font style and restore\nthe characters. The results of experiments show that RubbingGAN can repair both\nslightly and severely incomplete rubbing characters fast and effectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1\">Gongbo Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zijie Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Ming Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero and R2D2: A Large-scale Chinese Cross-modal Benchmark and A Vision-Language Framework. (arXiv:2205.03860v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.03860","description":"<p>Vision-language pre-training (VLP) on large-scale datasets has shown premier\nperformance on various downstream tasks. A complete and fair benchmark (i.e.,\nincluding large-scale pre-training datasets and diverse downstream tasks) is\nessential for VLP. While there are plenty of benchmarks with English corpus,\nbuilding a rich benchmark for VLP with other languages, such as Chinese,\nremains a critical problem. To this end, we build a large-scale Chinese\ncross-modal benchmark called Zero for the research community to fairly compare\nVLP models. We release two pre-training datasets and five fine-tuning datasets\nfor downstream tasks. Alongside, we propose a novel pre-training framework of\npre-Ranking + Ranking for cross-modal learning. Specifically, we apply global\ncontrastive pre-ranking to learn the individual representations of images and\ntexts, respectively. We then fuse the representations in a fine-grained ranking\nmanner via an image-text cross encoder and a text-image cross encoder. To\nfurther enhance the capability of the model, we propose a two-way distillation\nstrategy consisting of target-guided Distillation and feature-guided\nDistillation. For brevity, we name our model R2D2. We achieve state-of-the-art\nperformance on four public cross-modal datasets and the proposed five\ndownstream datasets. When conducting zero-shot tasks on Flickr30k-CN, COCO-CN,\nand MUGE, R2D2 pre-trained on a 250 million dataset achieves significant\nimprovements of 4.7%, 5.4%, and 6.3% in mean recall compared to the\nstate-of-the-art. The datasets, models, and codes are available at\nhttps://github.com/yuxie11/R2D2\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1\">Chunyu Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1\">Heng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jianfei Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jincheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_F/0/1/0/all/0/1\">Fanjing Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiaoyu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morimitsu_H/0/1/0/all/0/1\">Henrique Morimitsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_L/0/1/0/all/0/1\">Lin Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dexin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leng_D/0/1/0/all/0/1\">Dawei Leng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1\">Xiangyang Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yafeng Deng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain Invariant Masked Autoencoders for Self-supervised Learning from Multi-domains. (arXiv:2205.04771v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.04771","description":"<p>Generalizing learned representations across significantly different visual\ndomains is a fundamental yet crucial ability of the human visual system. While\nrecent self-supervised learning methods have achieved good performances with\nevaluation set on the same domain as the training set, they will have an\nundesirable performance decrease when tested on a different domain. Therefore,\nthe self-supervised learning from multiple domains task is proposed to learn\ndomain-invariant features that are not only suitable for evaluation on the same\ndomain as the training set but also can be generalized to unseen domains. In\nthis paper, we propose a Domain-invariant Masked AutoEncoder (DiMAE) for\nself-supervised learning from multi-domains, which designs a new pretext task,\n\\emph{i.e.,} the cross-domain reconstruction task, to learn domain-invariant\nfeatures. The core idea is to augment the input image with style noise from\ndifferent domains and then reconstruct the image from the embedding of the\naugmented image, regularizing the encoder to learn domain-invariant features.\nTo accomplish the idea, DiMAE contains two critical designs, 1)\ncontent-preserved style mix, which adds style information from other domains to\ninput while persevering the content in a parameter-free manner, and 2) multiple\ndomain-specific decoders, which recovers the corresponding domain style of\ninput to the encoded domain-invariant features for reconstruction. Experiments\non PACS and DomainNet illustrate that DiMAE achieves considerable gains\ncompared with recent state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haiyang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Meilin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yizhou Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Shixiang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Feng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_L/0/1/0/all/0/1\">Lei Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1\">Rui Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1\">Wanli Ouyang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Individual Topology Structure of Eye Movement Trajectories. (arXiv:2205.10667v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.10667","description":"<p>Traditionally, extracting patterns from eye movement data relies on\nstatistics of different macro-events such as fixations and saccades. This\nrequires an additional preprocessing step to separate the eye movement\nsubtypes, often with a number of parameters on which the classification results\ndepend. Besides that, definitions of such macro events are formulated in\ndifferent ways by different researchers.\n</p>\n<p>We propose an application of a new class of features to the quantitative\nanalysis of personal eye movement trajectories structure. This new class of\nfeatures based on algebraic topology allows extracting patterns from different\nmodalities of gaze such as time series of coordinates and amplitudes, heatmaps,\nand point clouds in a unified way at all scales from micro to macro. We\nexperimentally demonstrate the competitiveness of the new class of features\nwith the traditional ones and their significant synergy while being used\ntogether for the person authentication task on the recently published eye\nmovement trajectories dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Onuchin_A/0/1/0/all/0/1\">Arsenii Onuchin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kachan_O/0/1/0/all/0/1\">Oleg Kachan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Evolutionary Approach to Dynamic Introduction of Tasks in Large-scale Multitask Learning Systems. (arXiv:2205.12755v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2205.12755","description":"<p>Multitask learning assumes that models capable of learning from multiple\ntasks can achieve better quality and efficiency via knowledge transfer, a key\nfeature of human learning. Though, state of the art ML models rely on high\ncustomization for each task and leverage size and data scale rather than\nscaling the number of tasks. Also, continual learning, that adds the temporal\naspect to multitask, is often focused to the study of common pitfalls such as\ncatastrophic forgetting instead of being studied at a large scale as a critical\ncomponent to build the next generation artificial intelligence. We propose an\nevolutionary method that can generate a large scale multitask model, and can\nsupport the dynamic and continuous addition of new tasks. The generated\nmultitask model is sparsely activated and integrates a task-based routing that\nguarantees bounded compute cost and fewer added parameters per task as the\nmodel expands. The proposed method relies on a knowledge compartmentalization\ntechnique to achieve immunity against catastrophic forgetting and other common\npitfalls such as gradient interference and negative transfer. We empirically\nshow that the proposed method can jointly solve and achieve competitive results\non 69image classification tasks, for example achieving the best test accuracy\nreported fora model trained only on public data for competitive tasks such as\ncifar10: 99.43%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gesmundo_A/0/1/0/all/0/1\">Andrea Gesmundo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dean_J/0/1/0/all/0/1\">Jeff Dean</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Strengthening Skeletal Action Recognizers via Leveraging Temporal Patterns. (arXiv:2205.14405v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.14405","description":"<p>Skeleton sequences are compact and lightweight. Numerous skeleton-based\naction recognizers have been proposed to classify human behaviors. In this\nwork, we aim to incorporate components that are compatible with existing models\nand further improve their accuracy. To this end, we design two temporal\naccessories: discrete cosine encoding (DCE) and chronological loss (CRL). DCE\nfacilitates models to analyze motion patterns from the frequency domain and\nmeanwhile alleviates the influence of signal noise. CRL guides networks to\nexplicitly capture the sequence's chronological order. These two components\nconsistently endow many recently-proposed action recognizers with accuracy\nboosts, achieving new state-of-the-art (SOTA) accuracy on two large benchmark\ndatasets (NTU60 and NTU120).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1\">Zhenyue Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_P/0/1/0/all/0/1\">Pan Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Dongwoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anwar_S/0/1/0/all/0/1\">Saeed Anwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gedeon_T/0/1/0/all/0/1\">Tom Gedeon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Guided Diffusion Model for Adversarial Purification. (arXiv:2205.14969v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.14969","description":"<p>With wider application of deep neural networks (DNNs) in various algorithms\nand frameworks, security threats have become one of the concerns. Adversarial\nattacks disturb DNN-based image classifiers, in which attackers can\nintentionally add imperceptible adversarial perturbations on input images to\nfool the classifiers. In this paper, we propose a novel purification approach,\nreferred to as guided diffusion model for purification (GDMP), to help protect\nclassifiers from adversarial attacks. The core of our approach is to embed\npurification into the diffusion denoising process of a Denoised Diffusion\nProbabilistic Model (DDPM), so that its diffusion process could submerge the\nadversarial perturbations with gradually added Gaussian noises, and both of\nthese noises can be simultaneously removed following a guided denoising\nprocess. On our comprehensive experiments across various datasets, the proposed\nGDMP is shown to reduce the perturbations raised by adversarial attacks to a\nshallow range, thereby significantly improving the correctness of\nclassification. GDMP improves the robust accuracy by 5%, obtaining 90.1% under\nPGD attack on the CIFAR10 dataset. Moreover, GDMP achieves 70.94% robustness on\nthe challenging ImageNet dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jinyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Z/0/1/0/all/0/1\">Zhaoyang Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1\">Dahua Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1\">Bo Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1\">Hongfei Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeVRF: Fast Deformable Voxel Radiance Fields for Dynamic Scenes. (arXiv:2205.15723v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.15723","description":"<p>Modeling dynamic scenes is important for many applications such as virtual\nreality and telepresence. Despite achieving unprecedented fidelity for novel\nview synthesis in dynamic scenes, existing methods based on Neural Radiance\nFields (NeRF) suffer from slow convergence (i.e., model training time measured\nin days). In this paper, we present DeVRF, a novel representation to accelerate\nlearning dynamic radiance fields. The core of DeVRF is to model both the 3D\ncanonical space and 4D deformation field of a dynamic, non-rigid scene with\nexplicit and discrete voxel-based representations. However, it is quite\nchallenging to train such a representation which has a large number of model\nparameters, often resulting in overfitting issues. To overcome this challenge,\nwe devise a novel static-to-dynamic learning paradigm together with a new data\ncapture setup that is convenient to deploy in practice. This paradigm unlocks\nefficient learning of deformable radiance fields via utilizing the 3D\nvolumetric canonical space learnt from multi-view static images to ease the\nlearning of 4D voxel deformation field with only few-view dynamic sequences. To\nfurther improve the efficiency of our DeVRF and its synthesized novel view's\nquality, we conduct thorough explorations and identify a set of strategies. We\nevaluate DeVRF on both synthetic and real-world dynamic scenes with different\ntypes of deformation. Experiments demonstrate that DeVRF achieves two orders of\nmagnitude speedup (100x faster) with on-par high-fidelity results compared to\nthe previous state-of-the-art approaches. The code and dataset will be released\nin https://github.com/showlab/DeVRF.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jia-Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yan-Pei Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_W/0/1/0/all/0/1\">Weijia Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenqiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">David Junhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keppo_J/0/1/0/all/0/1\">Jussi Keppo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1\">Ying Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qie_X/0/1/0/all/0/1\">Xiaohu Qie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shou_M/0/1/0/all/0/1\">Mike Zheng Shou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Labeling Where Adapting Fails: Cross-Domain Semantic Segmentation with Point Supervision via Active Selection. (arXiv:2206.00181v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.00181","description":"<p>Training models dedicated to semantic segmentation requires a large amount of\npixel-wise annotated data. Due to their costly nature, these annotations might\nnot be available for the task at hand. To alleviate this problem, unsupervised\ndomain adaptation approaches aim at aligning the feature distributions between\nthe labeled source and the unlabeled target data. While these strategies lead\nto noticeable improvements, their effectiveness remains limited. To guide the\ndomain adaptation task more efficiently, previous works attempted to include\nhuman interactions in this process under the form of sparse single-pixel\nannotations in the target data. In this work, we propose a new domain\nadaptation framework for semantic segmentation with annotated points via active\nselection. First, we conduct an unsupervised domain adaptation of the model;\nfrom this adaptation, we use an entropy-based uncertainty measurement for\ntarget points selection. Finally, to minimize the domain gap, we propose a\ndomain adaptation framework utilizing these target points annotated by human\nannotators. Experimental results on benchmark datasets show the effectiveness\nof our methods against existing unsupervised domain adaptation approaches. The\npropose pipeline is generic and can be included as an extra module to existing\ndomain adaptation strategies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_F/0/1/0/all/0/1\">Fei Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rameau_F/0/1/0/all/0/1\">Francois Rameau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Junsik Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kweon_I/0/1/0/all/0/1\">In So Kweon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Anomaly detection in surveillance videos using transformer based attention model. (arXiv:2206.01524v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.01524","description":"<p>Surveillance footage can catch a wide range of realistic anomalies. This\nresearch suggests using a weakly supervised strategy to avoid annotating\nanomalous segments in training videos, which is time consuming. In this\napproach only video level labels are used to obtain frame level anomaly scores.\nWeakly supervised video anomaly detection (WSVAD) suffers from the wrong\nidentification of abnormal and normal instances during the training process.\nTherefore it is important to extract better quality features from the available\nvideos. WIth this motivation, the present paper uses better quality\ntransformer-based features named Videoswin Features followed by the attention\nlayer based on dilated convolution and self attention to capture long and short\nrange dependencies in temporal domain. This gives us a better understanding of\navailable videos. The proposed framework is validated on real-world dataset\ni.e. ShanghaiTech Campus dataset which results in competitive performance than\ncurrent state-of-the-art methods. The model and the code are available at\nhttps://github.com/kapildeshpande/Anomaly-Detection-in-Surveillance-Videos\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deshpande_K/0/1/0/all/0/1\">Kapil Deshpande</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Punn_N/0/1/0/all/0/1\">Narinder Singh Punn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sonbhadra_S/0/1/0/all/0/1\">Sanjay Kumar Sonbhadra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1\">Sonali Agarwal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OmniXAI: A Library for Explainable AI. (arXiv:2206.01612v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2206.01612","description":"<p>We introduce OmniXAI, an open-source Python library of eXplainable AI (XAI),\nwhich offers omni-way explainable AI capabilities and various interpretable\nmachine learning techniques to address the pain points of understanding and\ninterpreting the decisions made by machine learning (ML) in practice. OmniXAI\naims to be a one-stop comprehensive library that makes explainable AI easy for\ndata scientists, ML researchers and practitioners who need explanation for\nvarious types of data, models and explanation methods at different stages of ML\nprocess (data exploration, feature engineering, model development, evaluation,\nand decision-making, etc). In particular, our library includes a rich family of\nexplanation methods integrated in a unified interface, which supports multiple\ndata types (tabular data, images, texts, time-series), multiple types of ML\nmodels (traditional ML in Scikit-learn and deep learning models in\nPyTorch/TensorFlow), and a range of diverse explanation methods including\n\"model-specific\" and \"model-agnostic\" ones (such as feature-attribution\nexplanation, counterfactual explanation, gradient-based explanation, etc). For\npractitioners, the library provides an easy-to-use unified interface to\ngenerate the explanations for their applications by only writing a few lines of\ncodes, and also a GUI dashboard for visualization of different explanations for\nmore insights about decisions. In this technical report, we present OmniXAI's\ndesign principles, system architectures, and major functionalities, and also\ndemonstrate several example use cases across different types of data, tasks,\nand models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wenzhuo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1\">Hung Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1\">Silvio Savarese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoi_S/0/1/0/all/0/1\">Steven C.H. Hoi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Polyp-PVT: Polyp Segmentation with Pyramid Vision Transformers. (arXiv:2108.06932v4 [eess.IV] CROSS LISTED)","link":"http://arxiv.org/abs/2108.06932","description":"<p>Most polyp segmentation methods use CNNs as their backbone, leading to two\nkey issues when exchanging information between the encoder and decoder: 1)\ntaking into account the differences in contribution between different-level\nfeatures; and 2) designing an effective mechanism for fusing these features.\nDifferent from existing CNN-based methods, we adopt a transformer encoder,\nwhich learns more powerful and robust representations. In addition, considering\nthe image acquisition influence and elusive properties of polyps, we introduce\nthree novel modules, including a cascaded fusion module (CFM), a camouflage\nidentification module (CIM), a and similarity aggregation module (SAM). Among\nthese, the CFM is used to collect the semantic and location information of\npolyps from high-level features, while the CIM is applied to capture polyp\ninformation disguised in low-level features. With the help of the SAM, we\nextend the pixel features of the polyp area with high-level semantic position\ninformation to the entire polyp area, thereby effectively fusing cross-level\nfeatures. The proposed model, named Polyp-PVT, effectively suppresses noises in\nthe features and significantly improves their expressive capabilities.\nExtensive experiments on five widely adopted datasets show that the proposed\nmodel is more robust to various challenging situations (e.g., appearance\nchanges, small objects) than existing methods, and achieves the new\nstate-of-the-art performance. The proposed model is available at\nhttps://github.com/DengPingFan/Polyp-PVT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Dong_B/0/1/0/all/0/1\">Bo Dong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_W/0/1/0/all/0/1\">Wenhai Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fan_D/0/1/0/all/0/1\">Deng-Ping Fan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Jinpeng Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fu_H/0/1/0/all/0/1\">Huazhu Fu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-06-06T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}}]}]}