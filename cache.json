{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-06-08T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Towards Job-Transition-Tag Graph for a Better Job Title Representation Learning. (arXiv:2206.02782v1 [cs.LG])","link":"http://arxiv.org/abs/2206.02782","description":"<p>Works on learning job title representation are mainly based on\n\\textit{Job-Transition Graph}, built from the working history of talents.\nHowever, since these records are usually messy, this graph is very sparse,\nwhich affects the quality of the learned representation and hinders further\nanalysis. To address this specific issue, we propose to enrich the graph with\nadditional nodes that improve the quality of job title representation.\nSpecifically, we construct \\textit{Job-Transition-Tag Graph}, a heterogeneous\ngraph containing two types of nodes, i.e., job titles and tags (i.e., words\nrelated to job responsibilities or functionalities). Along this line, we\nreformulate job title representation learning as the task of learning node\nembedding on the \\textit{Job-Transition-Tag Graph}. Experiments on two datasets\nshow the interest of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hudelot_C/0/1/0/all/0/1\">C&#xe9;line Hudelot</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FedNST: Federated Noisy Student Training for Automatic Speech Recognition. (arXiv:2206.02797v1 [eess.AS])","link":"http://arxiv.org/abs/2206.02797","description":"<p>Federated Learning (FL) enables training state-of-the-art Automatic Speech\nRecognition (ASR) models on user devices (clients) in distributed systems,\nhence preventing transmission of raw user data to a central server. A key\nchallenge facing practical adoption of FL for ASR is obtaining ground-truth\nlabels on the clients. Existing approaches rely on clients to manually\ntranscribe their speech, which is impractical for obtaining large training\ncorpora. A promising alternative is using semi-/self-supervised learning\napproaches to leverage unlabelled user data. To this end, we propose a new\nFederated ASR method called FedNST for noisy student training of distributed\nASR models with private unlabelled user data. We explore various facets of\nFedNST , such as training models with different proportions of unlabelled and\nlabelled data, and evaluate the proposed approach on 1173 simulated clients.\nEvaluating FedNST on LibriSpeech, where 960 hours of speech data is split\nequally into server (labelled) and client (unlabelled) data, showed a 22.5%\nrelative word error rate reduction (WERR) over a supervised baseline trained\nonly on server data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Mehmood_H/0/1/0/all/0/1\">Haaris Mehmood</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dobrowolska_A/0/1/0/all/0/1\">Agnieszka Dobrowolska</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Saravanan_K/0/1/0/all/0/1\">Karthikeyan Saravanan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ozay_M/0/1/0/all/0/1\">Mete Ozay</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Bird's-Eye Tutorial of Graph Attention Architectures. (arXiv:2206.02849v1 [cs.LG])","link":"http://arxiv.org/abs/2206.02849","description":"<p>Graph Neural Networks (GNNs) have shown tremendous strides in performance for\ngraph-structured problems especially in the domains of natural language\nprocessing, computer vision and recommender systems. Inspired by the success of\nthe transformer architecture, there has been an ever-growing body of work on\nattention variants of GNNs attempting to advance the state of the art in many\nof these problems. Incorporating \"attention\" into graph mining has been viewed\nas a way to overcome the noisiness, heterogenity and complexity associated with\ngraph-structured data as well as to encode soft-inductive bias. It is hence\ncrucial and advantageous to study these variants from a bird's-eye view to\nassess their strengths and weaknesses. We provide a systematic and focused\ntutorial centered around attention based GNNs in a hope to benefit researchers\ndealing with graph-structured problems. Our tutorial looks at GNN variants from\nthe point of view of the attention function and iteratively builds the reader's\nunderstanding of different graph attention variants.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dhole_K/0/1/0/all/0/1\">Kaustubh D. Dhole</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Carl Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"No Parameter Left Behind: How Distillation and Model Size Affect Zero-Shot Retrieval. (arXiv:2206.02873v1 [cs.IR])","link":"http://arxiv.org/abs/2206.02873","description":"<p>Recent work has shown that small distilled language models are strong\ncompetitors to models that are orders of magnitude larger and slower in a wide\nrange of information retrieval tasks. This has made distilled and dense models,\ndue to latency constraints, the go-to choice for deployment in real-world\nretrieval applications. In this work, we question this practice by showing that\nthe number of parameters and early query-document interaction play a\nsignificant role in the generalization ability of retrieval models. Our\nexperiments show that increasing model size results in marginal gains on\nin-domain test sets, but much larger gains in new domains never seen during\nfine-tuning. Furthermore, we show that rerankers largely outperform dense ones\nof similar size in several tasks. Our largest reranker reaches the state of the\nart in 12 of the 18 datasets of the Benchmark-IR (BEIR) and surpasses the\nprevious state of the art by 3 average points. Finally, we confirm that\nin-domain effectiveness is not a good indicator of zero-shot effectiveness.\nCode is available at\nhttps://github.com/guilhermemr04/scaling-zero-shot-retrieval.git\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rosa_G/0/1/0/all/0/1\">Guilherme Moraes Rosa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bonifacio_L/0/1/0/all/0/1\">Luiz Bonifacio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeronymo_V/0/1/0/all/0/1\">Vitor Jeronymo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abonizio_H/0/1/0/all/0/1\">Hugo Abonizio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fadaee_M/0/1/0/all/0/1\">Marzieh Fadaee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lotufo_R/0/1/0/all/0/1\">Roberto Lotufo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nogueira_R/0/1/0/all/0/1\">Rodrigo Nogueira</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Norm Participation Grounds Language. (arXiv:2206.02885v1 [cs.CL])","link":"http://arxiv.org/abs/2206.02885","description":"<p>The striking recent advances in eliciting seemingly meaningful language\nbehaviour from language-only machine learning models have only made more\napparent, through the surfacing of clear limitations, the need to go beyond the\nlanguage-only mode and to ground these models \"in the world\". Proposals for\ndoing so vary in the details, but what unites them is that the solution is\nsought in the addition of non-linguistic data types such as images or video\nstreams, while largely keeping the mode of learning constant. I propose a\ndifferent, and more wide-ranging conception of how grounding should be\nunderstood: What grounds language is its normative nature. There are standards\nfor doing things right, these standards are public and authoritative, while at\nthe same time acceptance of authority can and must be disputed and negotiated,\nin interactions in which only bearers of normative status can rightfully\nparticipate. What grounds language, then, is the determined use that language\nusers make of it, and what it is grounded in is the community of language\nusers. I sketch this idea, and draw some conclusions for work on computational\nmodelling of meaningful language use.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schlangen_D/0/1/0/all/0/1\">David Schlangen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discriminative Models Can Still Outperform Generative Models in Aspect Based Sentiment Analysis. (arXiv:2206.02892v1 [cs.CL])","link":"http://arxiv.org/abs/2206.02892","description":"<p>Aspect-based Sentiment Analysis (ABSA) helps to explain customers' opinions\ntowards products and services. In the past, ABSA models were discriminative,\nbut more recently generative models have been used to generate aspects and\npolarities directly from text. In contrast, discriminative models commonly\nfirst select aspects from the text, and then classify the aspect's polarity.\nPrevious results showed that generative models outperform discriminative models\non several English ABSA datasets. Here, we evaluate and contrast two\nstate-of-the-art discriminative and generative models in several settings:\ncross-lingual, cross-domain, and cross-lingual and domain, to understand\ngeneralizability in settings other than English mono-lingual in-domain. Our\nmore thorough evaluation shows that, contrary to previous studies,\ndiscriminative models can still outperform generative models in almost all\nsettings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mullick_D/0/1/0/all/0/1\">Dhruv Mullick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fyshe_A/0/1/0/all/0/1\">Alona Fyshe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bilal Ghanem</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Schema-Guided Event Graph Completion. (arXiv:2206.02921v1 [cs.LG])","link":"http://arxiv.org/abs/2206.02921","description":"<p>We tackle a new task, event graph completion, which aims to predict missing\nevent nodes for event graphs. Existing link prediction or graph completion\nmethods have difficulty dealing with event graphs because they are usually\ndesigned for a single large graph such as a social network or a knowledge\ngraph, rather than multiple small dynamic event graphs. Moreover, they can only\npredict missing edges rather than missing nodes. In this work, we propose to\nutilize event schema, a template that describes the stereotypical structure of\nevent graphs, to address the above issues. Our schema-guided event graph\ncompletion approach first maps an instance event graph to a subgraph of the\nschema graph by a heuristic subgraph matching algorithm. Then it predicts\nwhether a candidate event node in the schema graph should be added to the\ninstantiated schema subgraph by characterizing two types of local topology of\nthe schema graph: neighbors of the candidate node and the subgraph, and paths\nthat connect the candidate node and the subgraph. These two modules are later\ncombined together for the final prediction. We also propose a self-supervised\nstrategy to construct training samples, as well as an inference algorithm that\nis specifically designed to complete event graphs. Extensive experimental\nresults on four datasets demonstrate that our proposed method achieves\nstate-of-the-art performance, with 4.3% to 19.4% absolute F1 gains over the\nbest baseline method on the four datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zixuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sha Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yizhou Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_H/0/1/0/all/0/1\">Hanghang Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olive_J/0/1/0/all/0/1\">Joseph P. Olive</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neuro-Symbolic Causal Language Planning with Commonsense Prompting. (arXiv:2206.02928v1 [cs.CL])","link":"http://arxiv.org/abs/2206.02928","description":"<p>Language planning aims to implement complex high-level goals by decomposition\ninto sequential simpler low-level steps. Such procedural reasoning ability is\nessential for applications such as household robots and virtual assistants.\nAlthough language planning is a basic skill set for humans in daily life, it\nremains a challenge for large language models (LLMs) that lack deep-level\ncommonsense knowledge in the real world. Previous methods require either manual\nexemplars or annotated programs to acquire such ability from LLMs. In contrast,\nthis paper proposes Neuro-Symbolic Causal Language Planner (CLAP) that elicits\nprocedural knowledge from the LLMs with commonsense-infused prompting.\nPre-trained knowledge in LLMs is essentially an unobserved confounder that\ncauses spurious correlations between tasks and action plans. Through the lens\nof a Structural Causal Model (SCM), we propose an effective strategy in CLAP to\nconstruct prompts as a causal intervention toward our SCM. Using graph sampling\ntechniques and symbolic program executors, our strategy formalizes the\nstructured causal prompts from commonsense knowledge bases. CLAP obtains\nstate-of-the-art performance on WikiHow and RobotHow, achieving a relative\nimprovement of 5.28% in human evaluations under the counterfactual setting.\nThis indicates the superiority of CLAP in causal language planning semantically\nand sequentially.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yujie Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1\">Weixi Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wanrong Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wenda Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Eric Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eckstein_M/0/1/0/all/0/1\">Miguel Eckstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Knowledge Graph Embedding via Iterative Self-Semantic Knowledge Distillation. (arXiv:2206.02963v1 [cs.LG])","link":"http://arxiv.org/abs/2206.02963","description":"<p>Knowledge graph embedding (KGE) has been intensively investigated for link\nprediction by projecting entities and relations into continuous vector spaces.\nCurrent popular high-dimensional KGE methods obtain quite slight performance\ngains while require enormous computation and memory costs. In contrast to\nhigh-dimensional KGE models, training low-dimensional models is more efficient\nand worthwhile for better deployments to practical intelligent systems.\nHowever, the model expressiveness of semantic information in knowledge graphs\n(KGs) is highly limited in the low dimension parameter space. In this paper, we\npropose iterative self-semantic knowledge distillation strategy to improve the\nKGE model expressiveness in the low dimension space. KGE model combined with\nour proposed strategy plays the teacher and student roles alternatively during\nthe whole training process. Specifically, at a certain iteration, the model is\nregarded as a teacher to provide semantic information for the student. At next\niteration, the model is regard as a student to incorporate the semantic\ninformation transferred from the teacher. We also design a novel semantic\nextraction block to extract iteration-based semantic information for the\ntraining model self-distillation. Iteratively incorporating and accumulating\niteration-based semantic information enables the low-dimensional model to be\nmore expressive for better link prediction in KGs. There is only one model\nduring the whole training, which alleviates the increase of computational\nexpensiveness and memory requirements. Furthermore, the proposed strategy is\nmodel-agnostic and can be seamlessly combined with other KGE models. Consistent\nand significant performance gains in experimental evaluations on four standard\ndatasets demonstrate the effectiveness of the proposed self-distillation\nstrategy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhehui Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Defang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Can Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yan Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Dual-Encoders with Question and Answer Cross-Embeddings for Answer Retrieval. (arXiv:2206.02978v1 [cs.CL])","link":"http://arxiv.org/abs/2206.02978","description":"<p>Dual-Encoders is a promising mechanism for answer retrieval in question\nanswering (QA) systems. Currently most conventional Dual-Encoders learn the\nsemantic representations of questions and answers merely through matching\nscore. Researchers proposed to introduce the QA interaction features in scoring\nfunction but at the cost of low efficiency in inference stage. To keep\nindependent encoding of questions and answers during inference stage,\nvariational auto-encoder is further introduced to reconstruct answers\n(questions) from question (answer) embeddings as an auxiliary task to enhance\nQA interaction in representation learning in training stage. However, the needs\nof text generation and answer retrieval are different, which leads to hardness\nin training. In this work, we propose a framework to enhance the Dual-Encoders\nmodel with question answer cross-embeddings and a novel Geometry Alignment\nMechanism (GAM) to align the geometry of embeddings from Dual-Encoders with\nthat from Cross-Encoders. Extensive experimental results show that our\nframework significantly improves Dual-Encoders model and outperforms the\nstate-of-the-art method on multiple answer retrieval datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanmeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1\">Jun Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Ye Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianfei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rong_W/0/1/0/all/0/1\">Wenge Rong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Z/0/1/0/all/0/1\">Zongcheng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shaojun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jing Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DynaMaR: Dynamic Prompt with Mask Token Representation. (arXiv:2206.02982v1 [cs.CL])","link":"http://arxiv.org/abs/2206.02982","description":"<p>Recent research has shown that large language models pretrained using\nunsupervised approaches can achieve significant performance improvement on many\ndownstream tasks. Typically when adapting these language models to downstream\ntasks, like a classification or regression task, we employ a fine-tuning\nparadigm in which the sentence representation from the language model is input\nto a task-specific head; the model is then fine-tuned end-to-end. However, with\nthe emergence of models like GPT-3, prompt-based fine-tuning has been proven to\nbe a successful approach for few-shot tasks. Inspired by this work, we study\ndiscrete prompt technologies in practice. There are two issues that arise with\nthe standard prompt approach. First, it can overfit on the prompt template.\nSecond, it requires manual effort to formulate the downstream task as a\nlanguage model problem. In this paper, we propose an improvement to\nprompt-based fine-tuning that addresses these two issues. We refer to our\napproach as DynaMaR -- Dynamic Prompt with Mask Token Representation. Results\nshow that DynaMaR can achieve an average improvement of 10% in few-shot\nsettings and improvement of 3.7% in data-rich settings over the standard\nfine-tuning approach on four e-commerce applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiaodi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajagopalan_S/0/1/0/all/0/1\">Sunny Rajagopalan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nigam_P/0/1/0/all/0/1\">Priyanka Nigam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Weiyi Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_B/0/1/0/all/0/1\">Belinda Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chilimbi_T/0/1/0/all/0/1\">Trishul Chilimbi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Insight into The Intricacies of Lingual Paraphrasing Pragmatic Discourse on The Purpose of Synonyms. (arXiv:2206.02983v1 [cs.CL])","link":"http://arxiv.org/abs/2206.02983","description":"<p>The term \"paraphrasing\" refers to the process of presenting the sense of an\ninput text in a new way while preserving fluency. Scientific research\ndistribution is gaining traction, allowing both rookie and experienced\nscientists to participate in their respective fields. As a result, there is now\na massive demand for paraphrase tools that may efficiently and effectively\nassist scientists in modifying statements in order to avoid plagiarism. Natural\nLanguage Processing (NLP) is very much important in the realm of the process of\ndocument paraphrasing. We analyze and discuss existing studies on paraphrasing\nin the English language in this paper. Finally, we develop an algorithm to\nparaphrase any text document or paragraphs using WordNet and Natural Language\nTool Kit (NLTK) and maintain \"Using Synonyms\" techniques to achieve our result.\nFor 250 paragraphs, our algorithm achieved a paraphrase accuracy of 94.8%\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nahian_J/0/1/0/all/0/1\">Jabir Al Nahian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masum_A/0/1/0/all/0/1\">Abu Kaisar Mohammad Masum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Syed_M/0/1/0/all/0/1\">Muntaser Mansur Syed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abujar_S/0/1/0/all/0/1\">Sheikh Abujar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DiMS: Distilling Multiple Steps of Iterative Non-Autoregressive Transformers. (arXiv:2206.02999v1 [cs.CL])","link":"http://arxiv.org/abs/2206.02999","description":"<p>The computational benefits of iterative non-autoregressive transformers\ndecrease as the number of decoding steps increases. As a remedy, we introduce\nDistill Multiple Steps (DiMS), a simple yet effective distillation technique to\ndecrease the number of required steps to reach a certain translation quality.\nThe distilled model enjoys the computational benefits of early iterations while\npreserving the enhancements from several iterative steps. DiMS relies on two\nmodels namely student and teacher. The student is optimized to predict the\noutput of the teacher after multiple decoding steps while the teacher follows\nthe student via a slow-moving average. The moving average keeps the teacher's\nknowledge updated and enhances the quality of the labels provided by the\nteacher. During inference, the student is used for translation and no\nadditional computation is added. We verify the effectiveness of DiMS on various\nmodels obtaining improvements of up to 7 BLEU points on distilled and 12 BLEU\npoints on raw WMT datasets for single-step translation. We release our code at\nhttps://github.com/layer6ai-labs/DiMS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Norouzi_S/0/1/0/all/0/1\">Sajad Norouzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hosseinzadeh_R/0/1/0/all/0/1\">Rasa Hosseinzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_F/0/1/0/all/0/1\">Felipe Perez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Volkovs_M/0/1/0/all/0/1\">Maksims Volkovs</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Plot Writing From Pre-Trained Language Models. (arXiv:2206.03021v1 [cs.CL])","link":"http://arxiv.org/abs/2206.03021","description":"<p>Pre-trained language models (PLMs) fail to generate long-form narrative text\nbecause they do not consider global structure. As a result, the generated texts\nare often incohesive, repetitive, or lack content. Recent work in story\ngeneration reintroduced explicit content planning in the form of prompts,\nkeywords, or semantic frames. Trained on large parallel corpora, these models\ncan generate more logical event sequences and thus more contentful stories.\nHowever, these intermediate representations are often not in natural language\nand cannot be utilized by PLMs without fine-tuning. We propose generating story\nplots using off-the-shelf PLMs while maintaining the benefit of content\nplanning to generate cohesive and contentful stories. Our proposed method,\nScratchPlot, first prompts a PLM to compose a content plan. Then, we generate\nthe story's body and ending conditioned on the content plan. Furthermore, we\ntake a generate-and-rank approach by using additional PLMs to rank the\ngenerated (story, ending) pairs. We benchmark our method with various baselines\nand achieved superior results in both human and automatic evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yiping Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadam_V/0/1/0/all/0/1\">Vishakha Kadam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wanvarie_D/0/1/0/all/0/1\">Dittaya Wanvarie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OCHADAI at SemEval-2022 Task 2: Adversarial Training for Multilingual Idiomaticity Detection. (arXiv:2206.03025v1 [cs.CL])","link":"http://arxiv.org/abs/2206.03025","description":"<p>We propose a multilingual adversarial training model for determining whether\na sentence contains an idiomatic expression. Given that a key challenge with\nthis task is the limited size of annotated data, our model relies on\npre-trained contextual representations from different multi-lingual\nstate-of-the-art transformer-based language models (i.e., multilingual BERT and\nXLM-RoBERTa), and on adversarial training, a training method for further\nenhancing model generalization and robustness. Without relying on any\nhuman-crafted features, knowledge bases, or additional datasets other than the\ntarget datasets, our model achieved competitive results and ranked 6th place in\nSubTask A (zero-shot) setting and 15th place in SubTask A (one-shot) setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pereira_L/0/1/0/all/0/1\">Lis Kanashiro Pereira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kobayashi_I/0/1/0/all/0/1\">Ichiro Kobayashi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CitySpec: An Intelligent Assistant System for Requirement Specification in Smart Cities. (arXiv:2206.03132v1 [cs.AI])","link":"http://arxiv.org/abs/2206.03132","description":"<p>An increasing number of monitoring systems have been developed in smart\ncities to ensure that real-time operations of a city satisfy safety and\nperformance requirements. However, many existing city requirements are written\nin English with missing, inaccurate, or ambiguous information. There is a high\ndemand for assisting city policy makers in converting human-specified\nrequirements to machine-understandable formal specifications for monitoring\nsystems. To tackle this limitation, we build CitySpec, the first intelligent\nassistant system for requirement specification in smart cities. To create\nCitySpec, we first collect over 1,500 real-world city requirements across\ndifferent domains from over 100 cities and extract city-specific knowledge to\ngenerate a dataset of city vocabulary with 3,061 words. We also build a\ntranslation model and enhance it through requirement synthesis and develop a\nnovel online learning framework with validation under uncertainty. The\nevaluation results on real-world city requirements show that CitySpec increases\nthe sentence-level accuracy of requirement specification from 59.02% to 86.64%,\nand has strong adaptability to a new city and a new domain (e.g., F1 score for\nrequirements in Seattle increases from 77.6% to 93.75% with online learning).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zirong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_I/0/1/0/all/0/1\">Isaac Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haoxiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Preum_S/0/1/0/all/0/1\">Sarah Preum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stankovic_J/0/1/0/all/0/1\">John A. Stankovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1\">Meiyi Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Intra-agent speech permits zero-shot task acquisition. (arXiv:2206.03139v1 [cs.LG])","link":"http://arxiv.org/abs/2206.03139","description":"<p>Human language learners are exposed to a trickle of informative,\ncontext-sensitive language, but a flood of raw sensory data. Through both\nsocial language use and internal processes of rehearsal and practice, language\nlearners are able to build high-level, semantic representations that explain\ntheir perceptions. Here, we take inspiration from such processes of \"inner\nspeech\" in humans (Vygotsky, 1934) to better understand the role of intra-agent\nspeech in embodied behavior. First, we formally pose intra-agent speech as a\nsemi-supervised problem and develop two algorithms that enable visually\ngrounded captioning with little labeled language data. We then experimentally\ncompute scaling curves over different amounts of labeled data and compare the\ndata efficiency against a supervised learning baseline. Finally, we incorporate\nintra-agent speech into an embodied, mobile manipulator agent operating in a 3D\nvirtual world, and show that with as few as 150 additional image captions,\nintra-agent speech endows the agent with the ability to manipulate and answer\nquestions about a new object without any related task-directed experience\n(zero-shot). Taken together, our experiments suggest that modelling intra-agent\nspeech is effective in enabling embodied agents to learn new tasks efficiently\nand without direct interaction experience.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_C/0/1/0/all/0/1\">Chen Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carnevale_F/0/1/0/all/0/1\">Federico Carnevale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Georgiev_P/0/1/0/all/0/1\">Petko Georgiev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santoro_A/0/1/0/all/0/1\">Adam Santoro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guy_A/0/1/0/all/0/1\">Aurelia Guy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muldal_A/0/1/0/all/0/1\">Alistair Muldal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hung_C/0/1/0/all/0/1\">Chia-Chun Hung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abramson_J/0/1/0/all/0/1\">Josh Abramson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lillicrap_T/0/1/0/all/0/1\">Timothy Lillicrap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wayne_G/0/1/0/all/0/1\">Gregory Wayne</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Speaker-Guided Encoder-Decoder Framework for Emotion Recognition in Conversation. (arXiv:2206.03173v1 [cs.CL])","link":"http://arxiv.org/abs/2206.03173","description":"<p>The emotion recognition in conversation (ERC) task aims to predict the\nemotion label of an utterance in a conversation. Since the dependencies between\nspeakers are complex and dynamic, which consist of intra- and inter-speaker\ndependencies, the modeling of speaker-specific information is a vital role in\nERC. Although existing researchers have proposed various methods of speaker\ninteraction modeling, they cannot explore dynamic intra- and inter-speaker\ndependencies jointly, leading to the insufficient comprehension of context and\nfurther hindering emotion prediction. To this end, we design a novel speaker\nmodeling scheme that explores intra- and inter-speaker dependencies jointly in\na dynamic manner. Besides, we propose a Speaker-Guided Encoder-Decoder (SGED)\nframework for ERC, which fully exploits speaker information for the decoding of\nemotion. We use different existing methods as the conversational context\nencoder of our framework, showing the high scalability and flexibility of the\nproposed framework. Experimental results demonstrate the superiority and\neffectiveness of SGED.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bao_Y/0/1/0/all/0/1\">Yinan Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Q/0/1/0/all/0/1\">Qianwen Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_L/0/1/0/all/0/1\">Lingwei Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Songlin Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fooling Explanations in Text Classifiers. (arXiv:2206.03178v1 [cs.LG])","link":"http://arxiv.org/abs/2206.03178","description":"<p>State-of-the-art text classification models are becoming increasingly reliant\non deep neural networks (DNNs). Due to their black-box nature, faithful and\nrobust explanation methods need to accompany classifiers for deployment in\nreal-life scenarios. However, it has been shown in vision applications that\nexplanation methods are susceptible to local, imperceptible perturbations that\ncan significantly alter the explanations without changing the predicted\nclasses. We show here that the existence of such perturbations extends to text\nclassifiers as well. Specifically, we introduceTextExplanationFooler (TEF), a\nnovel explanation attack algorithm that alters text input samples imperceptibly\nso that the outcome of widely-used explanation methods changes considerably\nwhile leaving classifier predictions unchanged. We evaluate the performance of\nthe attribution robustness estimation performance in TEF on five sequence\nclassification datasets, utilizing three DNN architectures and three\ntransformer architectures for each dataset. TEF can significantly decrease the\ncorrelation between unchanged and perturbed input attributions, which shows\nthat all models and explanation methods are susceptible to TEF perturbations.\nMoreover, we evaluate how the perturbations transfer to other model\narchitectures and attribution methods, and show that TEF perturbations are also\neffective in scenarios where the target model and explanation method are\nunknown. Finally, we introduce a semi-universal attack that is able to compute\nfast, computationally light perturbations with no knowledge of the attacked\nclassifier nor explanation method. Overall, our work shows that explanations in\ntext classifiers are very fragile and users need to carefully address their\nrobustness before relying on them in critical applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ivankay_A/0/1/0/all/0/1\">Adam Ivankay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Girardi_I/0/1/0/all/0/1\">Ivan Girardi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marchiori_C/0/1/0/all/0/1\">Chiara Marchiori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frossard_P/0/1/0/all/0/1\">Pascal Frossard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data Governance in the Age of Large-Scale Data-Driven Language Technology. (arXiv:2206.03216v1 [cs.CY])","link":"http://arxiv.org/abs/2206.03216","description":"<p>The recent emergence and adoption of Machine Learning technology, and\nspecifically of Large Language Models, has drawn attention to the need for\nsystematic and transparent management of language data. This work proposes an\napproach to global language data governance that attempts to organize data\nmanagement amongst stakeholders, values, and rights. Our proposal is informed\nby prior work on distributed governance that accounts for human values and\ngrounded by an international research collaboration that brings together\nresearchers and practitioners from 60 countries. The framework we present is a\nmulti-party international governance structure focused on language data, and\nincorporating technical and organizational tools needed to support its work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jernite_Y/0/1/0/all/0/1\">Yacine Jernite</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Huu Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biderman_S/0/1/0/all/0/1\">Stella Biderman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rogers_A/0/1/0/all/0/1\">Anna Rogers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masoud_M/0/1/0/all/0/1\">Maraim Masoud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danchev_V/0/1/0/all/0/1\">Valentin Danchev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1\">Samson Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luccioni_A/0/1/0/all/0/1\">Alexandra Sasha Luccioni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subramani_N/0/1/0/all/0/1\">Nishant Subramani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dupont_G/0/1/0/all/0/1\">G&#xe9;rard Dupont</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dodge_J/0/1/0/all/0/1\">Jesse Dodge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_K/0/1/0/all/0/1\">Kyle Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talat_Z/0/1/0/all/0/1\">Zeerak Talat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_I/0/1/0/all/0/1\">Isaac Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1\">Dragomir Radev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikpoor_S/0/1/0/all/0/1\">Somaieh Nikpoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frohberg_J/0/1/0/all/0/1\">J&#xf6;rg Frohberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gokaslan_A/0/1/0/all/0/1\">Aaron Gokaslan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henderson_P/0/1/0/all/0/1\">Peter Henderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bommasani_R/0/1/0/all/0/1\">Rishi Bommasani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitchell_M/0/1/0/all/0/1\">Margaret Mitchell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rites de Passage: Elucidating Displacement to Emplacement of Refugees. (arXiv:2206.03248v1 [cs.CY])","link":"http://arxiv.org/abs/2206.03248","description":"<p>Social media deliberations allow to explore refugee-related is-sues. AI-based\nstudies have investigated refugee issues mostly around a specific event and\nconsidered unimodal approaches. Contrarily, we have employed a multimodal\narchitecture for probing the refugee journeys from their home to host nations.\nWe draw insights from Arnold van Gennep's anthropological work 'Les Rites de\nPassage', which systematically analyzed an individual's transition from one\ngroup or society to another. Based on Gennep's\nseparation-transition-incorporation framework, we have identified four phases\nof refugee journeys: Arrival of Refugees, Temporal stay at Asylums,\nRehabilitation, and Integration of Refugees into the host nation. We collected\n0.23 million multimodal tweets from April 2020 to March 2021 for testing this\nproposed frame-work. We find that a combination of transformer-based language\nmodels and state-of-the-art image recognition models, such as fusion of\nBERT+LSTM and InceptionV4, can out-perform unimodal models. Subsequently, to\ntest the practical implication of our proposed model in real-time, we have\nconsidered 0.01 million multimodal tweets related to the 2022 Ukrainian refugee\ncrisis. An F1-score of 71.88 % for this 2022 crisis confirms the\ngeneralizability of our proposed framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khatua_A/0/1/0/all/0/1\">Aparup Khatua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nejdl_W/0/1/0/all/0/1\">Wolfgang Nejdl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LegoNN: Building Modular Encoder-Decoder Models. (arXiv:2206.03318v1 [cs.CL])","link":"http://arxiv.org/abs/2206.03318","description":"<p>State-of-the-art encoder-decoder models (e.g. for machine translation (MT) or\nspeech recognition (ASR)) are constructed and trained end-to-end as an atomic\nunit. No component of the model can be (re-)used without the others. We\ndescribe LegoNN, a procedure for building encoder-decoder architectures with\ndecoder modules that can be reused across various MT and ASR tasks, without the\nneed for any fine-tuning. To achieve reusability, the interface between each\nencoder and decoder modules is grounded to a sequence of marginal distributions\nover a discrete vocabulary pre-defined by the model designer. We present two\napproaches for ingesting these marginals; one is differentiable, allowing the\nflow of gradients across the entire network, and the other is\ngradient-isolating. To enable portability of decoder modules between MT tasks\nfor different source languages and across other tasks like ASR, we introduce a\nmodality agnostic encoder which consists of a length control mechanism to\ndynamically adapt encoders' output lengths in order to match the expected input\nlength range of pre-trained decoders. We present several experiments to\ndemonstrate the effectiveness of LegoNN models: a trained language generation\nLegoNN decoder module from German-English (De-En) MT task can be reused with no\nfine-tuning for the Europarl English ASR and the Romanian-English (Ro-En) MT\ntasks to match or beat respective baseline models. When fine-tuned towards the\ntarget task for few thousand updates, our LegoNN models improved the Ro-En MT\ntask by 1.5 BLEU points, and achieved 12.5% relative WER reduction for the\nEuroparl ASR task. Furthermore, to show its extensibility, we compose a LegoNN\nASR model from three modules -- each has been learned within different\nend-to-end trained models on three different datasets -- boosting the WER\nreduction to 19.5%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dalmia_S/0/1/0/all/0/1\">Siddharth Dalmia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okhonko_D/0/1/0/all/0/1\">Dmytro Okhonko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1\">Mike Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Edunov_S/0/1/0/all/0/1\">Sergey Edunov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metze_F/0/1/0/all/0/1\">Florian Metze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1\">Abdelrahman Mohamed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Searching for Optimal Subword Tokenization in Cross-domain NER. (arXiv:2206.03352v1 [cs.CL])","link":"http://arxiv.org/abs/2206.03352","description":"<p>Input distribution shift is one of the vital problems in unsupervised domain\nadaptation (UDA). The most popular UDA approaches focus on domain-invariant\nrepresentation learning, trying to align the features from different domains\ninto similar feature distributions. However, these approaches ignore the direct\nalignment of input word distributions between domains, which is a vital factor\nin word-level classification tasks such as cross-domain NER. In this work, we\nshed new light on cross-domain NER by introducing a subword-level solution,\nX-Piece, for input word-level distribution shift in NER. Specifically, we\nre-tokenize the input words of the source domain to approach the target subword\ndistribution, which is formulated and solved as an optimal transport problem.\nAs this approach focuses on the input level, it can also be combined with\nprevious DIRL methods for further improvement. Experimental results show the\neffectiveness of the proposed method based on BERT-tagger on four benchmark NER\ndatasets. Also, the proposed method is proved to benefit DIRL methods such as\nDANN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_R/0/1/0/all/0/1\">Ruotian Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1\">Yiding Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xuanting Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1\">Di Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sirui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gui_T/0/1/0/all/0/1\">Tao Gui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"cViL: Cross-Lingual Training of Vision-Language Models using Knowledge Distillation. (arXiv:2206.03354v1 [cs.CL])","link":"http://arxiv.org/abs/2206.03354","description":"<p>Vision-and-language tasks are gaining popularity in the research community,\nbut the focus is still mainly on English. We propose a pipeline that utilizes\nEnglish-only vision-language models to train a monolingual model for a target\nlanguage. We propose to extend OSCAR+, a model which leverages object tags as\nanchor points for learning image-text alignments, to train on visual question\nanswering datasets in different languages. We propose a novel approach to\nknowledge distillation to train the model in other languages using parallel\nsentences. Compared to other models that use the target language in the\npretraining corpora, we can leverage an existing English model to transfer the\nknowledge to the target language using significantly lesser resources. We also\nrelease a large-scale visual question answering dataset in Japanese and Hindi\nlanguage. Though we restrict our work to visual question answering, our model\ncan be extended to any sequence-level classification task, and it can be\nextended to other languages as well. This paper focuses on two languages for\nthe visual question answering task - Japanese and Hindi. Our pipeline\noutperforms the current state-of-the-art models by a relative increase of 4.4%\nand 13.4% respectively in accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_K/0/1/0/all/0/1\">Kshitij Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gautam_D/0/1/0/all/0/1\">Devansh Gautam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mamidi_R/0/1/0/all/0/1\">Radhika Mamidi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RAAT: Relation-Augmented Attention Transformer for Relation Modeling in Document-Level Event Extraction. (arXiv:2206.03377v1 [cs.CL])","link":"http://arxiv.org/abs/2206.03377","description":"<p>In document-level event extraction (DEE) task, event arguments always scatter\nacross sentences (across-sentence issue) and multiple events may lie in one\ndocument (multi-event issue). In this paper, we argue that the relation\ninformation of event arguments is of great significance for addressing the\nabove two issues, and propose a new DEE framework which can model the relation\ndependencies, called Relation-augmented Document-level Event Extraction\n(ReDEE). More specifically, this framework features a novel and tailored\ntransformer, named as Relation-augmented Attention Transformer (RAAT). RAAT is\nscalable to capture multi-scale and multi-amount argument relations. To further\nleverage relation information, we introduce a separate event relation\nprediction task and adopt multi-task learning method to explicitly enhance\nevent extraction performance. Extensive experiments demonstrate the\neffectiveness of the proposed method, which can achieve state-of-the-art\nperformance on two public datasets. Our code is available at https://github.\ncom/TencentYoutuResearch/RAAT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yuan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zhuoxuan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1\">Di Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_B/0/1/0/all/0/1\">Bo Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tutel: Adaptive Mixture-of-Experts at Scale. (arXiv:2206.03382v1 [cs.DC])","link":"http://arxiv.org/abs/2206.03382","description":"<p>In recent years, Mixture-of-Experts (MoE) has emerged as a promising\ntechnique for deep learning that can scale the model capacity to trillion-plus\nparameters while reducing the computing cost via sparse computation. While MoE\nopens a new frontier of exceedingly large models, its implementation over\nthousands of GPUs has been limited due to mismatch between the dynamic nature\nof MoE and static parallelism/pipelining of the system. We present Tutel, a\nhighly scalable stack design and implementation for MoE with dynamically\nadaptive parallelism and pipelining. Tutel delivers adaptive parallelism\nswitching and adaptive pipelining at runtime, which achieves up to 1.74x and\n2.00x single MoE layer speedup, respectively. We also propose a novel\ntwo-dimensional hierarchical algorithm for MoE communication speedup that\noutperforms the previous state-of-the-art up to 20.7x over 2,048 GPUs.\nAggregating all techniques, Tutel finally delivers 4.96x and 5.75x speedup of a\nsingle MoE layer on 16 GPUs and 2,048 GPUs, respectively, over Fairseq: Meta's\nFacebook AI Research Sequence-to-Sequence Toolkit (Tutel is now partially\nadopted by Fairseq). Tutel source code is available in public:\nhttps://github.com/microsoft/tutel . Our evaluation shows that Tutel\nefficiently and effectively runs a real-world MoE-based model named SwinV2-MoE,\nbuilt upon Swin Transformer V2, a state-of-the-art computer vision\narchitecture. On efficiency, Tutel accelerates SwinV2-MoE, achieving up to\n1.55x and 2.11x speedup in training and inference over Fairseq, respectively.\nOn effectiveness, the SwinV2-MoE model achieves superior accuracy in both\npre-training and down-stream computer vision tasks such as COCO object\ndetection than the counterpart dense model, indicating the readiness of Tutel\nfor end-to-end real-world model training and inference. SwinV2-MoE is open\nsourced in https://github.com/microsoft/Swin-Transformer .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hwang_C/0/1/0/all/0/1\">Changho Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_W/0/1/0/all/0/1\">Wei Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1\">Yifan Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Ziyue Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ze Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Han Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zilong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salas_R/0/1/0/all/0/1\">Rafael Salas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jose_J/0/1/0/all/0/1\">Jithin Jose</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ram_P/0/1/0/all/0/1\">Prabhat Ram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chau_J/0/1/0/all/0/1\">Joe Chau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_P/0/1/0/all/0/1\">Peng Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Mao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1\">Yongqiang Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gender Bias in Word Embeddings: A Comprehensive Analysis of Frequency, Syntax, and Semantics. (arXiv:2206.03390v1 [cs.CY])","link":"http://arxiv.org/abs/2206.03390","description":"<p>The statistical regularities in language corpora encode well-known social\nbiases into word embeddings. Here, we focus on gender to provide a\ncomprehensive analysis of group-based biases in widely-used static English word\nembeddings trained on internet corpora (GloVe 2014, fastText 2017). Using the\nSingle-Category Word Embedding Association Test, we demonstrate the widespread\nprevalence of gender biases that also show differences in: (1) frequencies of\nwords associated with men versus women; (b) part-of-speech tags in\ngender-associated words; (c) semantic categories in gender-associated words;\nand (d) valence, arousal, and dominance in gender-associated words.\n</p>\n<p>First, in terms of word frequency: we find that, of the 1,000 most frequent\nwords in the vocabulary, 77% are more associated with men than women, providing\ndirect evidence of a masculine default in the everyday language of the\nEnglish-speaking world. Second, turning to parts-of-speech: the top\nmale-associated words are typically verbs (e.g., fight, overpower) while the\ntop female-associated words are typically adjectives and adverbs (e.g., giving,\nemotionally). Gender biases in embeddings also permeate parts-of-speech. Third,\nfor semantic categories: bottom-up, cluster analyses of the top 1,000 words\nassociated with each gender. The top male-associated concepts include roles and\ndomains of big tech, engineering, religion, sports, and violence; in contrast,\nthe top female-associated concepts are less focused on roles, including,\ninstead, female-specific slurs and sexual content, as well as appearance and\nkitchen terms. Fourth, using human ratings of word valence, arousal, and\ndominance from a ~20,000 word lexicon, we find that male-associated words are\nhigher on arousal and dominance, while female-associated words are higher on\nvalence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Caliskan_A/0/1/0/all/0/1\">Aylin Caliskan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ajay_P/0/1/0/all/0/1\">Pimparkar Parth Ajay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charlesworth_T/0/1/0/all/0/1\">Tessa Charlesworth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolfe_R/0/1/0/all/0/1\">Robert Wolfe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banaji_M/0/1/0/all/0/1\">Mahzarin R. Banaji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Influence of Dataset Partitioning on Dysfluency Detection Systems. (arXiv:2206.03400v1 [eess.AS])","link":"http://arxiv.org/abs/2206.03400","description":"<p>This paper empirically investigates the influence of different data splits\nand splitting strategies on the performance of dysfluency detection systems.\nFor this, we perform experiments using wav2vec 2.0 models with a classification\nhead as well as support vector machines (SVM) in conjunction with the features\nextracted from the wav2vec 2.0 model to detect dysfluencies. We train and\nevaluate the systems with different non-speaker-exclusive and speaker-exclusive\nsplits of the Stuttering Events in Podcasts (SEP-28k) dataset to shed some\nlight on the variability of results w.r.t. to the partition method used.\nFurthermore, we show that the SEP-28k dataset is dominated by only a few\nspeakers, making it difficult to evaluate. To remedy this problem, we created\nSEP-28k-Extended (SEP-28k-E), containing semi-automatically generated speaker\nand gender information for the SEP-28k corpus, and suggest different data\nsplits, each useful for evaluating other aspects of methods for dysfluency\ndetection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bayerl_S/0/1/0/all/0/1\">Sebastian P. Bayerl</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wagner_D/0/1/0/all/0/1\">Dominik Wagner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Noth_E/0/1/0/all/0/1\">Elmar N&#xf6;th</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bocklet_T/0/1/0/all/0/1\">Tobias Bocklet</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Riedhammer_K/0/1/0/all/0/1\">Korbinian Riedhammer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revealing Single Frame Bias for Video-and-Language Learning. (arXiv:2206.03428v1 [cs.CV])","link":"http://arxiv.org/abs/2206.03428","description":"<p>Training an effective video-and-language model intuitively requires multiple\nframes as model inputs. However, it is unclear whether using multiple frames is\nbeneficial to downstream tasks, and if yes, whether the performance gain is\nworth the drastically-increased computation and memory costs resulting from\nusing more frames. In this work, we explore single-frame models for\nvideo-and-language learning. On a diverse set of video-and-language tasks\n(including text-to-video retrieval and video question answering), we show the\nsurprising result that, with large-scale pre-training and a proper frame\nensemble strategy at inference time, a single-frame trained model that does not\nconsider temporal information can achieve better performance than existing\nmethods that use multiple frames for training. This result reveals the\nexistence of a strong \"static appearance bias\" in popular video-and-language\ndatasets. Therefore, to allow for a more comprehensive evaluation of\nvideo-and-language models, we propose two new retrieval tasks based on existing\nfine-grained action recognition datasets that encourage temporal modeling. Our\ncode is available at https://github.com/jayleicn/singularity\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1\">Jie Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berg_T/0/1/0/all/0/1\">Tamara L. Berg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Correcting Sociodemographic Selection Biases for Population Prediction from Social Media. (arXiv:1911.03855v4 [cs.SI] UPDATED)","link":"http://arxiv.org/abs/1911.03855","description":"<p>Social media is increasingly used for large-scale population predictions,\nsuch as estimating community health statistics. However, social media users are\nnot typically a representative sample of the intended population -- a\n\"selection bias\". Within the social sciences, such a bias is typically\naddressed with restratification techniques, where observations are reweighted\naccording to how under- or over-sampled their socio-demographic groups are.\nYet, restratifaction is rarely evaluated for improving prediction. In this\ntwo-part study, we first evaluate standard, \"out-of-the-box\" restratification\ntechniques, finding they provide no improvement and often even degraded\nprediction accuracies across four tasks of esimating U.S. county population\nhealth statistics from Twitter. The core reasons for degraded performance seem\nto be tied to their reliance on either sparse or shrunken estimates of each\npopulation's socio-demographics. In the second part of our study, we develop\nand evaluate Robust Poststratification, which consists of three methods to\naddress these problems: (1) estimator redistribution to account for shrinking,\nas well as (2) adaptive binning and (3) informed smoothing to handle sparse\nsocio-demographic estimates. We show that each of these methods leads to\nsignificant improvement in prediction accuracies over the standard\nrestratification approaches. Taken together, Robust Poststratification enables\nstate-of-the-art prediction accuracies, yielding a 53.0% increase in variance\nexplained (R^2) in the case of surveyed life satisfaction, and a 17.8% average\nincrease across all tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Giorgi_S/0/1/0/all/0/1\">Salvatore Giorgi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lynn_V/0/1/0/all/0/1\">Veronica Lynn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_K/0/1/0/all/0/1\">Keshav Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_F/0/1/0/all/0/1\">Farhan Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matz_S/0/1/0/all/0/1\">Sandra Matz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ungar_L/0/1/0/all/0/1\">Lyle Ungar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_H/0/1/0/all/0/1\">H. Andrew Schwartz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Twitter Corpus of the #BlackLivesMatter Movement And Counter Protests: 2013 to 2021. (arXiv:2009.00596v3 [cs.SI] UPDATED)","link":"http://arxiv.org/abs/2009.00596","description":"<p>Black Lives Matter (BLM) is a decentralized social movement protesting\nviolence against Black individuals and communities, with a focus on police\nbrutality. The movement gained significant attention following the killings of\nAhmaud Arbery, Breonna Taylor, and George Floyd in 2020. The #BlackLivesMatter\nsocial media hashtag has come to represent the grassroots movement, with\nsimilar hashtags counter protesting the BLM movement, such as #AllLivesMatter,\nand #BlueLivesMatter. We introduce a data set of 63.9 million tweets from 13.0\nmillion users from over 100 countries which contain one of the following\nkeywords: BlackLivesMatter, AllLivesMatter, and BlueLivesMatter. This data set\ncontains all currently available tweets from the beginning of the BLM movement\nin 2013 to 2021. We summarize the data set and show temporal trends in use of\nboth the BlackLivesMatter keyword and keywords associated with counter\nmovements. Additionally, for each keyword, we create and release a set of\nLatent Dirichlet Allocation (LDA) topics (i.e., automatically clustered groups\nof semantically co-occuring words) to aid researchers in identifying linguistic\npatterns across the three keywords.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Giorgi_S/0/1/0/all/0/1\">Salvatore Giorgi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guntuku_S/0/1/0/all/0/1\">Sharath Chandra Guntuku</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Himelein_Wachowiak_M/0/1/0/all/0/1\">McKenzie Himelein-Wachowiak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwarteng_A/0/1/0/all/0/1\">Amy Kwarteng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Sy Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1\">Muhammad Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Curtis_B/0/1/0/all/0/1\">Brenda Curtis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SummScreen: A Dataset for Abstractive Screenplay Summarization. (arXiv:2104.07091v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07091","description":"<p>We introduce SummScreen, a summarization dataset comprised of pairs of TV\nseries transcripts and human written recaps. The dataset provides a challenging\ntestbed for abstractive summarization for several reasons. Plot details are\noften expressed indirectly in character dialogues and may be scattered across\nthe entirety of the transcript. These details must be found and integrated to\nform the succinct plot descriptions in the recaps. Also, TV scripts contain\ncontent that does not directly pertain to the central plot but rather serves to\ndevelop characters or provide comic relief. This information is rarely\ncontained in recaps. Since characters are fundamental to TV series, we also\npropose two entity-centric evaluation metrics. Empirically, we characterize the\ndataset by evaluating several methods, including neural models and those based\non nearest neighbors. An oracle extractive approach outperforms all benchmarked\nmodels according to automatic metrics, showing that the neural models are\nunable to fully exploit the input transcripts. Human evaluation and qualitative\nanalysis reveal that our non-oracle models are competitive with their oracle\ncounterparts in terms of generating faithful plot events and can benefit from\nbetter content selectors. Both oracle and non-oracle models generate unfaithful\nfacts, suggesting future research directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mingda Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_Z/0/1/0/all/0/1\">Zewei Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiseman_S/0/1/0/all/0/1\">Sam Wiseman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gimpel_K/0/1/0/all/0/1\">Kevin Gimpel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SAS: Self-Augmentation Strategy for Language Model Pre-training. (arXiv:2106.07176v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.07176","description":"<p>The core of self-supervised learning for pre-training language models\nincludes pre-training task design as well as appropriate data augmentation.\nMost data augmentations in language model pre-training are context-independent.\nA seminal contextualized augmentation was recently proposed in ELECTRA and\nachieved state-of-the-art performance by introducing an auxiliary generation\nnetwork (generator) to produce contextualized data augmentation for the\ntraining of a main discrimination network (discriminator). This design,\nhowever, introduces extra computation cost of the generator and a need to\nadjust the relative capability between the generator and the discriminator. In\nthis paper, we propose a self-augmentation strategy (SAS) where a single\nnetwork is utilized for both regular pre-training and contextualized data\naugmentation for the training in later epochs. Essentially, this strategy\neliminates a separate generator and uses the single network to jointly conduct\ntwo pre-training tasks with MLM (Masked Language Modeling) and RTD (Replaced\nToken Detection) heads. It avoids the challenge to search for an appropriate\nsize of the generator, which is critical to the performance as evidenced in\nELECTRA and its subsequent variant models. In addition, SAS is a general\nstrategy that can be seamlessly combined with many new techniques emerging\nrecently or in the future, such as the disentangled attention mechanism from\nDeBERTa. Our experiments show that SAS is able to outperform ELECTRA and other\nstate-of-the-art models in the GLUE tasks with similar or less computation\ncost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yifei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jingqiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1\">Ru He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_L/0/1/0/all/0/1\">Liangzhu Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Cheng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Ying Nian Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Boosting Search Engines with Interactive Agents. (arXiv:2109.00527v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.00527","description":"<p>This paper presents first successful steps in designing search agents that\nlearn meta-strategies for iterative query refinement in information-seeking\ntasks. Our approach uses machine reading to guide the selection of refinement\nterms from aggregated search results. Agents are then empowered with simple but\neffective search operators to exert fine-grained and transparent control over\nqueries and search results. We develop a novel way of generating synthetic\nsearch sessions, which leverages the power of transformer-based language models\nthrough (self-)supervised learning. We also present a reinforcement learning\nagent with dynamically constrained actions that learns interactive search\nstrategies from scratch. Our search agents obtain retrieval and answer quality\nperformance comparable to recent neural methods, using only a traditional\nterm-based BM25 ranking function and interpretable discrete reranking and\nfiltering actions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Adolphs_L/0/1/0/all/0/1\">Leonard Adolphs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boerschinger_B/0/1/0/all/0/1\">Benjamin Boerschinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buck_C/0/1/0/all/0/1\">Christian Buck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huebscher_M/0/1/0/all/0/1\">Michelle Chen Huebscher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ciaramita_M/0/1/0/all/0/1\">Massimiliano Ciaramita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Espeholt_L/0/1/0/all/0/1\">Lasse Espeholt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hofmann_T/0/1/0/all/0/1\">Thomas Hofmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kilcher_Y/0/1/0/all/0/1\">Yannic Kilcher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rothe_S/0/1/0/all/0/1\">Sascha Rothe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sessa_P/0/1/0/all/0/1\">Pier Giuseppe Sessa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saralegui_L/0/1/0/all/0/1\">Lierni Sestorain Saralegui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Low-Resource Named Entity Recognition Based on Multi-hop Dependency Trigger. (arXiv:2109.07118v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.07118","description":"<p>This paper presents a simple and effective approach in low-resource named\nentity recognition (NER) based on multi-hop dependency trigger. Dependency\ntrigger refer to salient nodes relative to a entity in the dependency graph of\na context sentence. Our main observation is that there often exists trigger\nwhich play an important role to recognize the location and type of entity in\nsentence. Previous research has used manual labelling of trigger. Our main\ncontribution is to propose use a syntactic parser to automatically annotate\ntrigger. Experiments on two English datasets (CONLL 2003 and BC5CDR) show that\nthe proposed method is comparable to the previous trigger-based NER model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiangxu Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Theories on Styles to their Transfer in Text: Bridging the Gap with a Hierarchical Survey. (arXiv:2110.15871v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.15871","description":"<p>Humans are naturally endowed with the ability to write in a particular style.\nThey can, for instance, re-phrase a formal letter in an informal way, convey a\nliteral message with the use of figures of speech or edit a novel mimicking the\nstyle of some well-known authors. Automating this form of creativity\nconstitutes the goal of style transfer. As a natural language generation task,\nstyle transfer aims at rewriting existing texts, and specifically, it creates\nparaphrases that exhibit some desired stylistic attributes. From a practical\nperspective, it envisions beneficial applications, like chat-bots that modulate\ntheir communicative style to appear empathetic, or systems that automatically\nsimplify technical articles for a non-expert audience. Several style-aware\nparaphrasing methods have attempted to tackle style transfer. A handful of\nsurveys give a methodological overview of the field, but they do not support\nresearchers to focus on specific styles. With this paper, we aim at providing a\ncomprehensive discussion of the styles that have received attention in the\ntransfer task. We organize them in a hierarchy, highlighting the challenges for\nthe definition of each of them, and pointing out gaps in the current research\nlandscape. The hierarchy comprises two main groups. One encompasses styles that\npeople modulate arbitrarily, along the lines of registers and genres. The other\ngroup corresponds to unintentionally expressed styles, due to an author's\npersonal characteristics. Hence, our review shows how these groups relate to\none another, and where specific styles, including some that have not yet been\nexplored, belong in the hierarchy. Moreover, we summarize the methods employed\nfor different stylistic families, hinting researchers towards those that would\nbe the most fitting for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Troiano_E/0/1/0/all/0/1\">Enrica Troiano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Velutharambath_A/0/1/0/all/0/1\">Aswathy Velutharambath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klinger_R/0/1/0/all/0/1\">Roman Klinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"First is Better Than Last for Training Data Influence. (arXiv:2202.11844v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2202.11844","description":"<p>The ability to identify influential training examples enables us to debug\ntraining data and explain model behavior. Existing techniques to do so are\nbased on the flow of training data influence through the model parameters. For\nlarge models in NLP applications, it is often computationally infeasible to\nstudy this flow through all model parameters, therefore techniques usually pick\nthe last layer of weights. However, we observe that since the activation\nconnected to the last layer of weights contains ``shared logic'', the data\ninfluenced calculated via the last layer weights prone to a ``cancellation\neffect'', where the data influence of different examples have large magnitude\nthat contradicts each other. The cancellation effect lowers the discriminative\npower of the influence score, and deleting influential examples according to\nthis measure often does not change the model's behavior by much. To mitigate\nthis, we propose a technique called TracIn-WE that modifies a method called\nTracIn to operate on the word embedding layer instead of the last layer, where\nthe cancellation effect is less severe. One potential concern is that influence\nbased on the word embedding layer may not encode sufficient high level\ninformation. However, we find that gradients (unlike embeddings) do not suffer\nfrom this, possibly because they chain through higher layers. We show that\nTracIn-WE significantly outperforms other data influence methods applied on the\nlast layer by 4-10 on the case deletion evaluation on three language\nclassification tasks. In addition, TracIn-WE can produce scores not just at the\nlevel of the overall training input, but also at the level of words within the\ntraining input, a further aid in debugging.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yeh_C/0/1/0/all/0/1\">Chih-Kuan Yeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taly_A/0/1/0/all/0/1\">Ankur Taly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sundararajan_M/0/1/0/all/0/1\">Mukund Sundararajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Frederick Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravikumar_P/0/1/0/all/0/1\">Pradeep Ravikumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep neural networks for fine-grained surveillance of overdose mortality. (arXiv:2202.12448v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.12448","description":"<p>Surveillance of drug overdose deaths relies on death certificates for\nidentification of the substances that caused death. Drugs and drug classes can\nbe identified through the International Classification of Diseases, 10th\nRevision (ICD-10) codes present on death certificates. However, ICD-10 codes do\nnot always provide high levels of specificity in drug identification. To\nachieve more fine-grained identification of substances on a death certificate,\nthe free-text cause of death section, completed by the medical certifier, must\nbe analyzed. Current methods for analyzing free-text death certificates rely\nsolely on look-up tables for identifying specific substances, which must be\nfrequently updated and maintained. To improve identification of drugs on death\ncertificates, a deep learning named-entity recognition model was developed,\nwhich achieved an F1-score of 99.13%. This model can identify new drug\nmisspellings and novel substances that are not present on current surveillance\nlook-up tables, enhancing the surveillance of drug overdose deaths.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ward_P/0/1/0/all/0/1\">Patrick J. Ward</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Young_A/0/1/0/all/0/1\">April M. Young</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Slavova_S/0/1/0/all/0/1\">Svetla Slavova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liford_M/0/1/0/all/0/1\">Madison Liford</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daniels_L/0/1/0/all/0/1\">Lara Daniels</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucas_R/0/1/0/all/0/1\">Ripley Lucas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kavuluru_R/0/1/0/all/0/1\">Ramakanth Kavuluru</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neuro-symbolic Natural Logic with Introspective Revision for Natural Language Inference. (arXiv:2203.04857v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.04857","description":"<p>We introduce a neuro-symbolic natural logic framework based on reinforcement\nlearning with introspective revision. The model samples and rewards specific\nreasoning paths through policy gradient, in which the introspective revision\nalgorithm modifies intermediate symbolic reasoning steps to discover\nreward-earning operations as well as leverages external knowledge to alleviate\nspurious reasoning and training inefficiency. The framework is supported by\nproperly designed local relation models to avoid input entangling, which helps\nensure the interpretability of the proof paths. The proposed model has built-in\ninterpretability and shows superior capability in monotonicity inference,\nsystematic generalization, and interpretability, compared to previous models on\nthe existing datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yufei Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaoyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaodan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Greenspan_M/0/1/0/all/0/1\">Michael Greenspan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Relevant CommonSense Subgraphs for \"What if...\" Procedural Reasoning. (arXiv:2203.11187v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.11187","description":"<p>We study the challenge of learning causal reasoning over procedural text to\nanswer \"What if...\" questions when external commonsense knowledge is required.\nWe propose a novel multi-hop graph reasoning model to 1) efficiently extract a\ncommonsense subgraph with the most relevant information from a large knowledge\ngraph; 2) predict the causal answer by reasoning over the representations\nobtained from the commonsense subgraph and the contextual interactions between\nthe questions and context. We evaluate our model on WIQA benchmark and achieve\nstate-of-the-art performance compared to the recent models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Chen Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kordjamshidi_P/0/1/0/all/0/1\">Parisa Kordjamshidi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving In-Context Few-Shot Learning via Self-Supervised Training. (arXiv:2205.01703v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.01703","description":"<p>Self-supervised pretraining has made few-shot learning possible for many NLP\ntasks. But the pretraining objectives are not typically adapted specifically\nfor in-context few-shot learning. In this paper, we propose to use\nself-supervision in an intermediate training stage between pretraining and\ndownstream few-shot usage with the goal to teach the model to perform\nin-context few shot learning. We propose and evaluate four self-supervised\nobjectives on two benchmarks. We find that the intermediate self-supervision\nstage produces models that outperform strong baselines. Ablation study shows\nthat several factors affect the downstream performance, such as the amount of\ntraining data and the diversity of the self-supervised objectives.\nHuman-annotated cross-task supervision and self-supervision are complementary.\nQualitative analysis suggests that the self-supervised-trained models are\nbetter at following task requirements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mingda Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1\">Jingfei Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasunuru_R/0/1/0/all/0/1\">Ramakanth Pasunuru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihaylov_T/0/1/0/all/0/1\">Todor Mihaylov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyer_S/0/1/0/all/0/1\">Srini Iyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoyanov_V/0/1/0/all/0/1\">Veselin Stoyanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kozareva_Z/0/1/0/all/0/1\">Zornitsa Kozareva</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparse Conditional Hidden Markov Model for Weakly Supervised Named Entity Recognition. (arXiv:2205.14228v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.14228","description":"<p>Weakly supervised named entity recognition methods train label models to\naggregate the token annotations of multiple noisy labeling functions (LFs)\nwithout seeing any manually annotated labels. To work well, the label model\nneeds to contextually identify and emphasize well-performed LFs while\ndown-weighting the under-performers. However, evaluating the LFs is challenging\ndue to the lack of ground truths. To address this issue, we propose the sparse\nconditional hidden Markov model (Sparse-CHMM). Instead of predicting the entire\nemission matrix as other HMM-based methods, Sparse-CHMM focuses on estimating\nits diagonal elements, which are considered as the reliability scores of the\nLFs. The sparse scores are then expanded to the full-fledged emission matrix\nwith pre-defined expansion functions. We also augment the emission with\nweighted XOR scores, which track the probabilities of an LF observing incorrect\nentities. Sparse-CHMM is optimized through unsupervised learning with a\nthree-stage training pipeline that reduces the training difficulty and prevents\nthe model from falling into local optima. Compared with the baselines in the\nWrench benchmark, Sparse-CHMM achieves a 3.01 average F1 score improvement on\nfive comprehensive datasets. Experiments show that each component of\nSparse-CHMM is effective, and the estimated LF reliabilities strongly correlate\nwith true LF F1 scores.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yinghao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1\">Le Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Pre-Trained Language Models to Streamline Natural Language Interaction for Self-Tracking. (arXiv:2205.15503v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.15503","description":"<p>Current natural language interaction for self-tracking tools largely depends\non bespoke implementation optimized for a specific tracking theme and data\nformat, which is neither generalizable nor scalable to a tremendous design\nspace of self-tracking. However, training machine learning models in the\ncontext of self-tracking is challenging due to the wide variety of tracking\ntopics and data formats. In this paper, we propose a novel NLP task for\nself-tracking that extracts close- and open-ended information from a\nretrospective activity log described as a plain text, and a domain-agnostic,\nGPT-3-based NLU framework that performs this task. The framework augments the\nprompt using synthetic samples to transform the task into 10-shot learning, to\naddress a cold-start problem in bootstrapping a new tracking topic. Our\npreliminary evaluation suggests that our approach significantly outperforms the\nbaseline QA models. Going further, we discuss future application domains toward\nwhich the NLP and HCI researchers can collaborate.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Young-Ho Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sungdong Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1\">Minsuk Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sang-Woo Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Variable-rate hierarchical CPC leads to acoustic unit discovery in speech. (arXiv:2206.02211v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2206.02211","description":"<p>The success of deep learning comes from its ability to capture the\nhierarchical structure of data by learning high-level representations defined\nin terms of low-level ones. In this paper we explore self-supervised learning\nof hierarchical representations of speech by applying multiple levels of\nContrastive Predictive Coding (CPC). We observe that simply stacking two CPC\nmodels does not yield significant improvements over single-level architectures.\nInspired by the fact that speech is often described as a sequence of discrete\nunits unevenly distributed in time, we propose a model in which the output of a\nlow-level CPC module is non-uniformly downsampled to directly minimize the loss\nof a high-level CPC module. The latter is designed to also enforce a prior of\nseparability and discreteness in its representations by enforcing dissimilarity\nof successive high-level representations through focused negative sampling, and\nby quantization of the prediction targets. Accounting for the structure of the\nspeech signal improves upon single-level CPC features and enhances the\ndisentanglement of the learned representations, as measured by downstream\nspeech recognition tasks, while resulting in a meaningful segmentation of the\nsignal that closely resembles phone boundaries.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cuervo_S/0/1/0/all/0/1\">Santiago Cuervo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lancucki_A/0/1/0/all/0/1\">Adrian &#x141;a&#x144;cucki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marxer_R/0/1/0/all/0/1\">Ricard Marxer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rychlikowski_P/0/1/0/all/0/1\">Pawe&#x142; Rychlikowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chorowski_J/0/1/0/all/0/1\">Jan Chorowski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Advance of Making Language Models Better Reasoners. (arXiv:2206.02336v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.02336","description":"<p>Large language models such as GPT-3 and PaLM have shown remarkable\nperformance in few-shot learning. However, they still struggle with reasoning\ntasks such as the arithmetic benchmark GSM8K. Recent advances deliberately\nguide the language model to generate a chain of reasoning steps before\nproducing the final answer, successfully boosting the GSM8K benchmark from\n17.9% to 58.1% in terms of problem solving rate. In this paper, we propose a\nnew approach, DiVeRSe (Diverse Verifier on Reasoning Step), to further advance\ntheir reasoning capability. DiVeRSe first explores different prompts to enhance\nthe diversity in reasoning paths. Second, DiVeRSe introduces a verifier to\ndistinguish good answers from bad answers for a better weighted voting.\nFinally, DiVeRSe verifies the correctness of each single step rather than all\nthe steps in a whole. We conduct extensive experiments using the latest\nlanguage model code-davinci-002 and demonstrate that DiVeRSe can achieve new\nstate-of-the-art performance on six out of eight reasoning benchmarks (e.g.,\nGSM8K 74.4% to 83.2%), outperforming the PaLM model with 540B parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yifei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zeqi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shizhuo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Q/0/1/0/all/0/1\">Qiang Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1\">Jian-Guang Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UTTS: Unsupervised TTS with Conditional Disentangled Sequential Variational Auto-encoder. (arXiv:2206.02512v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2206.02512","description":"<p>In this paper, we propose a novel unsupervised text-to-speech (UTTS)\nframework which does not require text-audio pairs for the TTS acoustic modeling\n(AM). UTTS is a multi-speaker speech synthesizer developed from the perspective\nof disentangled speech representation learning. The framework offers a flexible\nchoice of a speaker's duration model, timbre feature (identity) and content for\nTTS inference. We leverage recent advancements in self-supervised speech\nrepresentation learning as well as speech synthesis front-end techniques for\nthe system development. Specifically, we utilize a lexicon to map input text to\nthe phoneme sequence, which is expanded to the frame-level forced alignment\n(FA) with a speaker-dependent duration model. Then, we develop an alignment\nmapping module that converts the FA to the unsupervised alignment (UA).\nFinally, a Conditional Disentangled Sequential Variational Auto-encoder\n(C-DSVAE), serving as the self-supervised TTS AM, takes the predicted UA and a\ntarget speaker embedding to generate the mel spectrogram, which is ultimately\nconverted to waveform with a neural vocoder. We show how our method enables\nspeech synthesis without using a paired TTS corpus. Experiments demonstrate\nthat UTTS can synthesize speech of high naturalness and intelligibility\nmeasured by human and objective evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lian_J/0/1/0/all/0/1\">Jiachen Lian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_C/0/1/0/all/0/1\">Chunlei Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Anumanchipalli_G/0/1/0/all/0/1\">Gopala Krishna Anumanchipalli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_D/0/1/0/all/0/1\">Dong Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Dysfluencies in Stuttering Therapy Using wav2vec 2.0. (arXiv:2204.03417v1 [eess.AS] CROSS LISTED)","link":"http://arxiv.org/abs/2204.03417","description":"<p>Stuttering is a varied speech disorder that harms an individual's\ncommunication ability. Persons who stutter (PWS) often use speech therapy to\ncope with their condition. Improving speech recognition systems for people with\nsuch non-typical speech or tracking the effectiveness of speech therapy would\nrequire systems that can detect dysfluencies while at the same time being able\nto detect speech techniques acquired in therapy.\n</p>\n<p>This paper shows that fine-tuning wav2vec 2.0 for the classification of\nstuttering on a sizeable English corpus containing stuttered speech, in\nconjunction with multi-task learning, boosts the effectiveness of the\ngeneral-purpose wav2vec 2.0 features for detecting stuttering in speech; both\nwithin and across languages. We evaluate our method on Fluencybank and the\nGerman therapy-centric Kassel State of Fluency (KSoF) dataset by training\nSupport Vector Machine classifiers using features extracted from the fine-tuned\nmodels for six different stuttering-related events types: blocks,\nprolongations, sound repetitions, word repetitions, interjections, and -\nspecific to therapy - speech modifications. Using embeddings from the\nfine-tuned models leads to relative classification performance gains up to 27\\%\nw.r.t. F1-score.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bayerl_S/0/1/0/all/0/1\">Sebastian P. Bayerl</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wagner_D/0/1/0/all/0/1\">Dominik Wagner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Noth_E/0/1/0/all/0/1\">Elmar N&#xf6;th</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Riedhammer_K/0/1/0/all/0/1\">Korbinian Riedhammer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Vocal Fatigue with Neural Embeddings. (arXiv:2204.03428v1 [eess.AS] CROSS LISTED)","link":"http://arxiv.org/abs/2204.03428","description":"<p>Vocal fatigue refers to the feeling of tiredness and weakness of voice due to\nextended utilization. This paper investigates the effectiveness of neural\nembeddings for the detection of vocal fatigue. We compare x-vectors,\nECAPA-TDNN, and wav2vec 2.0 embeddings on a corpus of academic spoken English.\nLow-dimensional mappings of the data reveal that neural embeddings capture\ninformation about the change in vocal characteristics of a speaker during\nprolonged voice usage. We show that vocal fatigue can be reliably predicted\nusing all three kinds of neural embeddings after only 50 minutes of continuous\nspeaking when temporal smoothing and normalization are applied to the extracted\nembeddings. We employ support vector machines for classification and achieve\naccuracy scores of 81% using x-vectors, 85% using ECAPA-TDNN embeddings, and\n82% using wav2vec 2.0 embeddings as input features. We obtain an accuracy score\nof 76%, when the trained system is applied to a different speaker and recording\nenvironment without any adaptation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bayerl_S/0/1/0/all/0/1\">Sebastian P. Bayerl</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wagner_D/0/1/0/all/0/1\">Dominik Wagner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Baumann_I/0/1/0/all/0/1\">Ilja Baumann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Riedhammer_K/0/1/0/all/0/1\">Korbinian Riedhammer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bocklet_T/0/1/0/all/0/1\">Tobias Bocklet</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-06-07T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"FIFA: Making Fairness More Generalizable in Classifiers Trained on Imbalanced Data. (arXiv:2206.02792v1 [cs.LG])","link":"http://arxiv.org/abs/2206.02792","description":"<p>Algorithmic fairness plays an important role in machine learning and imposing\nfairness constraints during learning is a common approach. However, many\ndatasets are imbalanced in certain label classes (e.g. \"healthy\") and sensitive\nsubgroups (e.g. \"older patients\"). Empirically, this imbalance leads to a lack\nof generalizability not only of classification, but also of fairness\nproperties, especially in over-parameterized models. For example,\nfairness-aware training may ensure equalized odds (EO) on the training data,\nbut EO is far from being satisfied on new users. In this paper, we propose a\ntheoretically-principled, yet Flexible approach that is\nImbalance-Fairness-Aware (FIFA). Specifically, FIFA encourages both\nclassification and fairness generalization and can be flexibly combined with\nmany existing fair learning methods with logits-based losses. While our main\nfocus is on EO, FIFA can be directly applied to achieve equalized opportunity\n(EqOpt); and under certain conditions, it can also be applied to other fairness\nnotions. We demonstrate the power of FIFA by combining it with a popular fair\nclassification algorithm, and the resulting algorithm achieves significantly\nbetter fairness generalization on several real-world datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1\">Zhun Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiayao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Linjun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_T/0/1/0/all/0/1\">Ting Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coley_Y/0/1/0/all/0/1\">Yates Coley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_W/0/1/0/all/0/1\">Weijie J. Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1\">James Zou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FedNST: Federated Noisy Student Training for Automatic Speech Recognition. (arXiv:2206.02797v1 [eess.AS])","link":"http://arxiv.org/abs/2206.02797","description":"<p>Federated Learning (FL) enables training state-of-the-art Automatic Speech\nRecognition (ASR) models on user devices (clients) in distributed systems,\nhence preventing transmission of raw user data to a central server. A key\nchallenge facing practical adoption of FL for ASR is obtaining ground-truth\nlabels on the clients. Existing approaches rely on clients to manually\ntranscribe their speech, which is impractical for obtaining large training\ncorpora. A promising alternative is using semi-/self-supervised learning\napproaches to leverage unlabelled user data. To this end, we propose a new\nFederated ASR method called FedNST for noisy student training of distributed\nASR models with private unlabelled user data. We explore various facets of\nFedNST , such as training models with different proportions of unlabelled and\nlabelled data, and evaluate the proposed approach on 1173 simulated clients.\nEvaluating FedNST on LibriSpeech, where 960 hours of speech data is split\nequally into server (labelled) and client (unlabelled) data, showed a 22.5%\nrelative word error rate reduction (WERR) over a supervised baseline trained\nonly on server data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Mehmood_H/0/1/0/all/0/1\">Haaris Mehmood</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dobrowolska_A/0/1/0/all/0/1\">Agnieszka Dobrowolska</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Saravanan_K/0/1/0/all/0/1\">Karthikeyan Saravanan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ozay_M/0/1/0/all/0/1\">Mete Ozay</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EVC-Net: Multi-scale V-Net with Conditional Random Fields for Brain Extraction. (arXiv:2206.02837v1 [eess.IV])","link":"http://arxiv.org/abs/2206.02837","description":"<p>Brain extraction is one of the first steps of pre-processing 3D brain MRI\ndata. It is a prerequisite for any forthcoming brain imaging analyses. However,\nit is not a simple segmentation problem due to the complex structure of the\nbrain and human head. Although multiple solutions have been proposed in the\nliterature, we are still far from having truly robust methods. While previous\nmethods have used machine learning with structural/geometric priors, with the\ndevelopment of deep learning in computer vision tasks, there has been an\nincrease in proposed convolutional neural network architectures for this\nsemantic segmentation task. Yet, most models focus on improving the training\ndata and loss functions with little change in the architecture. In this paper,\nwe propose a novel architecture we call EVC-Net. EVC-Net adds lower scale\ninputs on each encoder block. This enhances the multi-scale scheme of the V-Net\narchitecture, hence increasing the efficiency of the model. Conditional Random\nFields, a popular approach for image segmentation before the deep learning era,\nare re-introduced here as an additional step for refining the network's output\nto capture fine-grained results in segmentation. We compare our model to\nstate-of-the-art methods such as HD-BET, Synthstrip and brainy. Results show\nthat even with limited training resources, EVC-Net achieves higher Dice\nCoefficient and Jaccard Index along with lower surface distance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Park_J/0/1/0/all/0/1\">Jong Sung Park</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fadnavis_S/0/1/0/all/0/1\">Shreyas Fadnavis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Garyfallidis_E/0/1/0/all/0/1\">Eleftherios Garyfallidis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Invertible Sharpening Network for MRI Reconstruction Enhancement. (arXiv:2206.02838v1 [eess.IV])","link":"http://arxiv.org/abs/2206.02838","description":"<p>High-quality MRI reconstruction plays a critical role in clinical\napplications. Deep learning-based methods have achieved promising results on\nMRI reconstruction. However, most state-of-the-art methods were designed to\noptimize the evaluation metrics commonly used for natural images, such as PSNR\nand SSIM, whereas the visual quality is not primarily pursued. Compared to the\nfully-sampled images, the reconstructed images are often blurry, where\nhigh-frequency features might not be sharp enough for confident clinical\ndiagnosis. To this end, we propose an invertible sharpening network\n(InvSharpNet) to improve the visual quality of MRI reconstructions. During\ntraining, unlike the traditional methods that learn to map the input data to\nthe ground truth, InvSharpNet adapts a backward training strategy that learns a\nblurring transform from the ground truth (fully-sampled image) to the input\ndata (blurry reconstruction). During inference, the learned blurring transform\ncan be inverted to a sharpening transform leveraging the network's\ninvertibility. The experiments on various MRI datasets demonstrate that\nInvSharpNet can improve reconstruction sharpness with few artifacts. The\nresults were also evaluated by radiologists, indicating better visual quality\nand diagnostic confidence of our proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Dong_S/0/1/0/all/0/1\">Siyuan Dong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_E/0/1/0/all/0/1\">Eric Z. Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_L/0/1/0/all/0/1\">Lin Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1\">Xiao Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yikang Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_T/0/1/0/all/0/1\">Terrence Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_S/0/1/0/all/0/1\">Shanhui Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spatial Acoustic Projection for 3D Imaging Sonar Reconstruction. (arXiv:2206.02840v1 [cs.RO])","link":"http://arxiv.org/abs/2206.02840","description":"<p>In this work we present a novel method for reconstructing 3D surfaces using a\nmulti-beam imaging sonar. We integrate the intensities measured by the sonar\nfrom different viewpoints for fixed cell positions in a 3D grid. For each cell\nwe integrate a feature vector that holds the mean intensity for a discretized\nrange of viewpoints. Based on the feature vectors and independent sparse range\nmeasurements that act as ground truth information, we train convolutional\nneural networks that allow us to predict the signed distance and direction to\nthe nearest surface for each cell. The predicted signed distances can be\nprojected into a truncated signed distance field (TSDF) along the predicted\ndirections. Utilizing the marching cubes algorithm, a polygon mesh can be\nrendered from the TSDF. Our method allows a dense 3D reconstruction from a\nlimited set of viewpoints and was evaluated on three real-world datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arnold_S/0/1/0/all/0/1\">Sascha Arnold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wehbe_B/0/1/0/all/0/1\">Bilal Wehbe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Deeper Dive Into What Deep Spatiotemporal Networks Encode: Quantifying Static vs. Dynamic Information. (arXiv:2206.02846v1 [cs.CV])","link":"http://arxiv.org/abs/2206.02846","description":"<p>Deep spatiotemporal models are used in a variety of computer vision tasks,\nsuch as action recognition and video object segmentation. Currently, there is a\nlimited understanding of what information is captured by these models in their\nintermediate representations. For example, while it has been observed that\naction recognition algorithms are heavily influenced by visual appearance in\nsingle static frames, there is no quantitative methodology for evaluating such\nstatic bias in the latent representation compared to bias toward dynamic\ninformation (e.g. motion). We tackle this challenge by proposing a novel\napproach for quantifying the static and dynamic biases of any spatiotemporal\nmodel. To show the efficacy of our approach, we analyse two widely studied\ntasks, action recognition and video object segmentation. Our key findings are\nthreefold: (i) Most examined spatiotemporal models are biased toward static\ninformation; although, certain two-stream architectures with cross-connections\nshow a better balance between the static and dynamic information captured. (ii)\nSome datasets that are commonly assumed to be biased toward dynamics are\nactually biased toward static information. (iii) Individual units (channels) in\nan architecture can be biased toward static, dynamic or a combination of the\ntwo.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kowal_M/0/1/0/all/0/1\">Matthew Kowal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siam_M/0/1/0/all/0/1\">Mennatullah Siam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1\">Md Amirul Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruce_N/0/1/0/all/0/1\">Neil D. B. Bruce</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wildes_R/0/1/0/all/0/1\">Richard P. Wildes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Derpanis_K/0/1/0/all/0/1\">Konstantinos G. Derpanis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring the Potential of SAR Data for Cloud Removal in Optical Satellite Imagery. (arXiv:2206.02850v1 [cs.CV])","link":"http://arxiv.org/abs/2206.02850","description":"<p>The challenge of the cloud removal task can be alleviated with the aid of\nSynthetic Aperture Radar (SAR) images that can penetrate cloud cover. However,\nthe large domain gap between optical and SAR images as well as the severe\nspeckle noise of SAR images may cause significant interference in SAR-based\ncloud removal, resulting in performance degeneration. In this paper, we propose\na novel global-local fusion based cloud removal (GLF-CR) algorithm to leverage\nthe complementary information embedded in SAR images. Exploiting the power of\nSAR information to promote cloud removal entails two aspects. The first, global\nfusion, guides the relationship among all local optical windows to maintain the\nstructure of the recovered region consistent with the remaining cloud-free\nregions. The second, local fusion, transfers complementary information embedded\nin the SAR image that corresponds to cloudy areas to generate reliable texture\ndetails of the missing regions, and uses dynamic filtering to alleviate the\nperformance degradation caused by speckle noise. Extensive evaluation\ndemonstrates that the proposed algorithm can yield high quality cloud-free\nimages and performs favorably against state-of-the-art cloud removal\nalgorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1\">Fang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yilei Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ebel_P/0/1/0/all/0/1\">Patrick Ebel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Lei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_G/0/1/0/all/0/1\">Gui-Song Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiao Xiang Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SpikiLi: A Spiking Simulation of LiDAR based Real-time Object Detection for Autonomous Driving. (arXiv:2206.02876v1 [cs.CV])","link":"http://arxiv.org/abs/2206.02876","description":"<p>Spiking Neural Networks are a recent and new neural network design approach\nthat promises tremendous improvements in power efficiency, computation\nefficiency, and processing latency. They do so by using asynchronous\nspike-based data flow, event-based signal generation, processing, and modifying\nthe neuron model to resemble biological neurons closely. While some initial\nworks have shown significant initial evidence of applicability to common deep\nlearning tasks, their applications in complex real-world tasks has been\nrelatively low. In this work, we first illustrate the applicability of spiking\nneural networks to a complex deep learning task namely Lidar based 3D object\ndetection for automated driving. Secondly, we make a step-by-step demonstration\nof simulating spiking behavior using a pre-trained convolutional neural\nnetwork. We closely model essential aspects of spiking neural networks in\nsimulation and achieve equivalent run-time and accuracy on a GPU. When the\nmodel is realized on a neuromorphic hardware, we expect to have significantly\nimproved power efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohapatra_S/0/1/0/all/0/1\">Sambit Mohapatra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mesquida_T/0/1/0/all/0/1\">Thomas Mesquida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hodaei_M/0/1/0/all/0/1\">Mona Hodaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yogamani_S/0/1/0/all/0/1\">Senthil Yogamani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gotzig_H/0/1/0/all/0/1\">Heinrich Gotzig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mader_P/0/1/0/all/0/1\">Patrick Mader</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mesh-based Dynamics with Occlusion Reasoning for Cloth Manipulation. (arXiv:2206.02881v1 [cs.RO])","link":"http://arxiv.org/abs/2206.02881","description":"<p>Self-occlusion is challenging for cloth manipulation, as it makes it\ndifficult to estimate the full state of the cloth. Ideally, a robot trying to\nunfold a crumpled or folded cloth should be able to reason about the cloth's\noccluded regions. We leverage recent advances in pose estimation for cloth to\nbuild a system that uses explicit occlusion reasoning to unfold a crumpled\ncloth. Specifically, we first learn a model to reconstruct the mesh of the\ncloth. However, the model will likely have errors due to the complexities of\nthe cloth configurations and due to ambiguities from occlusions. Our main\ninsight is that we can further refine the predicted reconstruction by\nperforming test-time finetuning with self-supervised losses. The obtained\nreconstructed mesh allows us to use a mesh-based dynamics model for planning\nwhile reasoning about occlusions. We evaluate our system both on cloth\nflattening as well as on cloth canonicalization, in which the objective is to\nmanipulate the cloth into a canonical pose. Our experiments show that our\nmethod significantly outperforms prior methods that do not explicitly account\nfor occlusions or perform test-time optimization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zixuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xingyu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Held_D/0/1/0/all/0/1\">David Held</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Polymorphic-GAN: Generating Aligned Samples across Multiple Domains with Learned Morph Maps. (arXiv:2206.02903v1 [cs.CV])","link":"http://arxiv.org/abs/2206.02903","description":"<p>Modern image generative models show remarkable sample quality when trained on\na single domain or class of objects. In this work, we introduce a generative\nadversarial network that can simultaneously generate aligned image samples from\nmultiple related domains. We leverage the fact that a variety of object classes\nshare common attributes, with certain geometric differences. We propose\nPolymorphic-GAN which learns shared features across all domains and a\nper-domain morph layer to morph shared features according to each domain. In\ncontrast to previous works, our framework allows simultaneous modelling of\nimages with highly varying geometries, such as images of human faces, painted\nand artistic faces, as well as multiple different animal faces. We demonstrate\nthat our model produces aligned samples for all domains and show how it can be\nused for applications such as segmentation transfer and cross-domain image\nediting, as well as training in low-data regimes. Additionally, we apply our\nPolymorphic-GAN on image-to-image translation tasks and show that we can\ngreatly surpass previous approaches in cases where the geometric differences\nbetween domains are large.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seung Wook Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kreis_K/0/1/0/all/0/1\">Karsten Kreis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Daiqing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1\">Antonio Torralba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fidler_S/0/1/0/all/0/1\">Sanja Fidler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Treatment Plan Representations for Content Based Image Retrieval. (arXiv:2206.02912v1 [cs.CV])","link":"http://arxiv.org/abs/2206.02912","description":"<p>Objective: Knowledge based planning (KBP) typically involves training an\nend-to-end deep learning model to predict dose distributions. However, training\nend-to-end KBP methods may be associated with practical limitations due to the\nlimited size of medical datasets that are often used. To address these\nlimitations, we propose a content based image retrieval (CBIR) method for\nretrieving dose distributions of previously planned patients based on\nanatomical similarity. Approach: Our proposed CBIR method trains a\nrepresentation model that produces latent space embeddings of a patient's\nanatomical information. The latent space embeddings of new patients are then\ncompared against those of previous patients in a database for image retrieval\nof dose distributions. Summary metrics (e.g. dose-volume histogram, conformity\nindex, homogeneity index, etc.) are computed and can then be utilized in\nsubsequent automated planning. All source code for this project is available on\ngithub. Main Results: The retrieval performance of various CBIR methods is\nevaluated on a dataset consisting of both publicly available plans and clinical\nplans from our institution. This study compares various encoding methods,\nranging from simple autoencoders to more recent Siamese networks like SimSiam,\nand the best performance was observed for the multitask Siamese network.\nSignificance: Applying CBIR to inform subsequent treatment planning potentially\naddresses many limitations associated with end-to-end KBP. Our current results\ndemonstrate that excellent image retrieval performance can be obtained through\nslight changes to previously developed Siamese networks. We hope to integrate\nCBIR into automated planning workflow in future works, potentially through\nmethods like the MetaPlanner framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Charles Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasudevan_V/0/1/0/all/0/1\">Varun Vasudevan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pastor_Serrano_O/0/1/0/all/0/1\">Oscar Pastor-Serrano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1\">Md Tauhidul Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nomura_Y/0/1/0/all/0/1\">Yusuke Nomura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_L/0/1/0/all/0/1\">Lei Xing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Remember the Past: Distilling Datasets into Addressable Memories for Neural Networks. (arXiv:2206.02916v1 [cs.LG])","link":"http://arxiv.org/abs/2206.02916","description":"<p>We propose an algorithm that compresses the critical information of a large\ndataset into compact addressable memories. These memories can then be recalled\nto quickly re-train a neural network and recover the performance (instead of\nstoring and re-training on the full original dataset).\n</p>\n<p>Building upon the dataset distillation framework, we make a key observation\nthat a shared common representation allows for more efficient and effective\ndistillation. Concretely, we learn a set of bases (aka \"memories\") which are\nshared between classes and combined through learned flexible addressing\nfunctions to generate a diverse set of training examples. This leads to several\nbenefits: 1) the size of compressed data does not necessarily grow linearly\nwith the number of classes; 2) an overall higher compression rate with more\neffective distillation is achieved; and 3) more generalized queries are allowed\nbeyond recalling the original classes.\n</p>\n<p>We demonstrate state-of-the-art results on the dataset distillation task\nacross five benchmarks, including up to 16.5% and 9.7% in retained accuracy\nimprovement when distilling CIFAR10 and CIFAR100 respectively. We then leverage\nour framework to perform continual learning, achieving state-of-the-art results\non four benchmarks, with 23.2% accuracy improvement on MANY.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1\">Zhiwei Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Russakovsky_O/0/1/0/all/0/1\">Olga Russakovsky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond Faithfulness: A Framework to Characterize and Compare Saliency Methods. (arXiv:2206.02958v1 [cs.LG])","link":"http://arxiv.org/abs/2206.02958","description":"<p>Saliency methods calculate how important each input feature is to a machine\nlearning model's prediction, and are commonly used to understand model\nreasoning. \"Faithfulness\", or how fully and accurately the saliency output\nreflects the underlying model, is an oft-cited desideratum for these methods.\nHowever, explanation methods must necessarily sacrifice certain information in\nservice of user-oriented goals such as simplicity. To that end, and akin to\nperformance metrics, we frame saliency methods as abstractions: individual\ntools that provide insight into specific aspects of model behavior and entail\ntradeoffs. Using this framing, we describe a framework of nine dimensions to\ncharacterize and compare the properties of saliency methods. We group these\ndimensions into three categories that map to different phases of the\ninterpretation process: methodology, or how the saliency is calculated;\nsensitivity, or relationships between the saliency result and the underlying\nmodel or input; and, perceptibility, or how a user interprets the result. As we\nshow, these dimensions give us a granular vocabulary for describing and\ncomparing saliency methods -- for instance, allowing us to develop \"saliency\ncards\" as a form of documentation, or helping downstream users understand\ntradeoffs and choose a method for a particular use case. Moreover, by situating\nexisting saliency methods within this framework, we identify opportunities for\nfuture work, including filling gaps in the landscape and developing new\nevaluation metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Boggust_A/0/1/0/all/0/1\">Angie Boggust</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suresh_H/0/1/0/all/0/1\">Harini Suresh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strobelt_H/0/1/0/all/0/1\">Hendrik Strobelt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guttag_J/0/1/0/all/0/1\">John V. Guttag</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Satyanarayan_A/0/1/0/all/0/1\">Arvind Satyanarayan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HMRNet: High and Multi-Resolution Network with Bidirectional Feature Calibration for Brain Structure Segmentation in Radiotherapy. (arXiv:2206.02959v1 [eess.IV])","link":"http://arxiv.org/abs/2206.02959","description":"<p>Accurate segmentation of Anatomical brain Barriers to Cancer spread (ABCs)\nplays an important role for automatic delineation of Clinical Target Volume\n(CTV) of brain tumors in radiotherapy. Despite that variants of U-Net are\nstate-of-the-art segmentation models, they have limited performance when\ndealing with ABCs structures with various shapes and sizes, especially thin\nstructures (e.g., the falx cerebri) that span only few slices. To deal with\nthis problem, we propose a High and Multi-Resolution Network (HMRNet) that\nconsists of a multi-scale feature learning branch and a high-resolution branch,\nwhich can maintain the high-resolution contextual information and extract more\nrobust representations of anatomical structures with various scales. We further\ndesign a Bidirectional Feature Calibration (BFC) block to enable the two\nbranches to generate spatial attention maps for mutual feature calibration.\nConsidering the different sizes and positions of ABCs structures, our network\nwas applied after a rough localization of each structure to obtain fine\nsegmentation results. Experiments on the MICCAI 2020 ABCs challenge dataset\nshowed that: 1) Our proposed two-stage segmentation strategy largely\noutperformed methods segmenting all the structures in just one stage; 2) The\nproposed HMRNet with two branches can maintain high-resolution representations\nand is effective to improve the performance on thin structures; 3) The proposed\nBFC block outperformed existing attention methods using monodirectional feature\ncalibration. Our method won the second place of ABCs 2020 challenge and has a\npotential for more accurate and reasonable delineation of CTV of brain tumors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Fu_H/0/1/0/all/0/1\">Hao Fu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_G/0/1/0/all/0/1\">Guotai Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lei_W/0/1/0/all/0/1\">Wenhui Lei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_W/0/1/0/all/0/1\">Wei Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_Q/0/1/0/all/0/1\">Qianfei Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_S/0/1/0/all/0/1\">Shichuan Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_K/0/1/0/all/0/1\">Kang Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_S/0/1/0/all/0/1\">Shaoting Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Masked Unsupervised Self-training for Zero-shot Image Classification. (arXiv:2206.02967v1 [cs.CV])","link":"http://arxiv.org/abs/2206.02967","description":"<p>State-of-the-art computer vision models are mostly trained with supervised\nlearning using human-labeled images, which limits their scalability due to the\nexpensive annotation cost. While self-supervised representation learning has\nachieved impressive progress, it still requires a second stage of finetuning on\nlabeled data. On the other hand, models pre-trained with large-scale text-image\nsupervision (e.g., CLIP) have enabled zero-shot transfer to downstream image\nclassification tasks. However, the zero-shot performance of CLIP-like models\nare often insufficient for real-world adoption. In this paper, we aim to\nleverage the abundant unlabeled data to improve the performance of a\npre-trained zero-shot classifier on downstream tasks. We propose Masked\nUnsupervised Self-Training (MUST), a new approach which leverages two different\nand complimentary sources of supervision: pseudo-labels and raw images. MUST\njointly optimizes three objectives to learn both class-level global feature and\npixel-level local feature and enforces a regularization between the two. We\ndemonstrate the efficacy of MUST on 8 downstream tasks across a variety of\ndomains, where it improves upon CLIP by a large margin and narrows the\nperformance gap between unsupervised and supervised classification. For\ninstance, MUST achieves a zero-shot top-1 accuracy of 77.7% on ImageNet using\nViT-B, +9.4% higher than CLIP. Our code is available at\nhttps://github.com/salesforce/MUST.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junnan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1\">Silvio Savarese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoi_S/0/1/0/all/0/1\">Steven C.H. Hoi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DETR++: Taming Your Multi-Scale Detection Transformer. (arXiv:2206.02977v1 [cs.CV])","link":"http://arxiv.org/abs/2206.02977","description":"<p>Convolutional Neural Networks (CNN) have dominated the field of detection\never since the success of AlexNet in ImageNet classification [12]. With the\nsweeping reform of Transformers [27] in natural language processing, Carion et\nal. [2] introduce the Transformer-based detection method, i.e., DETR. However,\ndue to the quadratic complexity in the self-attention mechanism in the\nTransformer, DETR is never able to incorporate multi-scale features as\nperformed in existing CNN-based detectors, leading to inferior results in small\nobject detection. To mitigate this issue and further improve performance of\nDETR, in this work, we investigate different methods to incorporate multi-scale\nfeatures and find that a Bi-directional Feature Pyramid (BiFPN) works best with\nDETR in further raising the detection precision. With this discovery, we\npropose DETR++, a new architecture that improves detection results by 1.9% AP\non MS COCO 2017, 11.5% AP on RICO icon detection, and 9.1% AP on RICO layout\nextraction over existing baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lijuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zang_X/0/1/0/all/0/1\">Xiaoxue Zang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Frederick Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xinying Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jindong Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Structured Context Transformer for Generic Event Boundary Detection. (arXiv:2206.02985v1 [cs.CV])","link":"http://arxiv.org/abs/2206.02985","description":"<p>Generic Event Boundary Detection (GEBD) aims to detect moments where humans\nnaturally perceive as event boundaries. In this paper, we present Structured\nContext Transformer (or SC-Transformer) to solve the GEBD task, which can be\ntrained in an end-to-end fashion. Specifically, we use the backbone\nconvolutional neural network (CNN) to extract the features of each video frame.\nTo capture temporal context information of each frame, we design the structure\ncontext transformer (SC-Transformer) by re-partitioning input frame sequence.\nNote that, the overall computation complexity of SC-Transformer is linear to\nthe video length. After that, the group similarities are computed to capture\nthe differences between frames. Then, a lightweight fully convolutional network\nis used to determine the event boundaries based on the grouped similarity maps.\nTo remedy the ambiguities of boundary annotations, the Gaussian kernel is\nadopted to preprocess the ground-truth event boundaries to further boost the\naccuracy. Extensive experiments conducted on the challenging Kinetics-GEBD and\nTAPOS datasets demonstrate the effectiveness of the proposed method compared to\nthe state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Congcong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinyao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_D/0/1/0/all/0/1\">Dexiang Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yufei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Libo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_T/0/1/0/all/0/1\">Tiejian Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1\">Longyin Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TadML: A fast temporal action detection with Mechanics-MLP. (arXiv:2206.02997v1 [cs.CV])","link":"http://arxiv.org/abs/2206.02997","description":"<p>Temporal Action Detection(TAD) is a crucial but challenging task in video\nunderstanding.It is aimed at detecting both the type and start-end frame for\neach action instance in a long, untrimmed video.Most current models adopt both\nRGB and Optical-Flow streams for the TAD task. Thus, original RGB frames must\nbe converted manually into Optical-Flow frames with additional computation and\ntime cost, which is an obstacle to achieve real-time processing. At present,\nmany models adopt two-stage strategies, which would slow the inference speed\ndown and complicatedly tuning on proposals generating.By comparison, we propose\na one-stage anchor-free temporal localization method with RGB stream only, in\nwhich a novel Newtonian \\emph{Mechanics-MLP} architecture is established. It\nhas comparable accuracy with all existing state-of-the-art models, while\nsurpasses the inference speed of these methods by a large margin. The typical\ninference speed in this paper is astounding 4.44 video per second on THUMOS14.\nIn applications, because there is no need to convert optical flow, the\ninference speed will be faster.It also proves that \\emph{MLP} has great\npotential in downstream tasks such as TAD. The source code is available at\n\\url{https://github.com/BonedDeng/TadML}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_B/0/1/0/all/0/1\">Bowen Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Dongchang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PP-OCRv3: More Attempts for the Improvement of Ultra Lightweight OCR System. (arXiv:2206.03001v1 [cs.CV])","link":"http://arxiv.org/abs/2206.03001","description":"<p>Optical character recognition (OCR) technology has been widely used in\nvarious scenes, as shown in Figure 1. Designing a practical OCR system is still\na meaningful but challenging task. In previous work, considering the efficiency\nand accuracy, we proposed a practical ultra lightweight OCR system (PP-OCR),\nand an optimized version PP-OCRv2. In order to further improve the performance\nof PP-OCRv2, a more robust OCR system PP-OCRv3 is proposed in this paper.\nPP-OCRv3 upgrades the text detection model and text recognition model in 9\naspects based on PP-OCRv2. For text detector, we introduce a PAN module with\nlarge receptive field named LK-PAN, a FPN module with residual attention\nmechanism named RSE-FPN, and DML distillation strategy. For text recognizer,\nthe base model is replaced from CRNN to SVTR, and we introduce lightweight text\nrecognition network SVTR LCNet, guided training of CTC by attention, data\naugmentation strategy TextConAug, better pre-trained model by self-supervised\nTextRotNet, UDML, and UIM to accelerate the model and improve the effect.\nExperiments on real data show that the hmean of PP-OCRv3 is 5% higher than\nPP-OCRv2 under comparable inference speed. All the above mentioned models are\nopen-sourced and the code is available in the GitHub repository PaddleOCR which\nis powered by PaddlePaddle.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chenxia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weiwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_R/0/1/0/all/0/1\">Ruoyu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1\">Xiaoting Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_K/0/1/0/all/0/1\">Kaitao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yongkun Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yuning Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Lingfeng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_B/0/1/0/all/0/1\">Baohua Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaoguang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dianhai Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yanjun Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformer-based Personalized Attention Mechanism (PersAM) for Medical Images with Clinical Records. (arXiv:2206.03003v1 [eess.IV])","link":"http://arxiv.org/abs/2206.03003","description":"<p>In medical image diagnosis, identifying the attention region, i.e., the\nregion of interest for which the diagnosis is made, is an important task.\nVarious methods have been developed to automatically identify target regions\nfrom given medical images. However, in actual medical practice, the diagnosis\nis made based not only on the images but also on a variety of clinical records.\nThis means that pathologists examine medical images with some prior knowledge\nof the patients and that the attention regions may change depending on the\nclinical records. In this study, we propose a method called the Personalized\nAttention Mechanism (PersAM), by which the attention regions in medical images\nare adaptively changed according to the clinical records. The primary idea of\nthe PersAM method is to encode the relationships between the medical images and\nclinical records using a variant of Transformer architecture. To demonstrate\nthe effectiveness of the PersAM method, we applied it to a large-scale digital\npathology problem of identifying the subtypes of 842 malignant lymphoma\npatients based on their gigapixel whole slide images and clinical records.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Takagi_Y/0/1/0/all/0/1\">Yusuke Takagi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hashimoto_N/0/1/0/all/0/1\">Noriaki Hashimoto</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Masuda_H/0/1/0/all/0/1\">Hiroki Masuda</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Miyoshi_H/0/1/0/all/0/1\">Hiroaki Miyoshi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ohshima_K/0/1/0/all/0/1\">Koichi Ohshima</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hontani_H/0/1/0/all/0/1\">Hidekata Hontani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Takeuchi_I/0/1/0/all/0/1\">Ichiro Takeuchi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Knowledge Distillation based Self-Supervised Learning for Covid-19 Detection from Chest X-Ray Images. (arXiv:2206.03009v1 [eess.IV])","link":"http://arxiv.org/abs/2206.03009","description":"<p>The global outbreak of the Coronavirus 2019 (COVID-19) has overloaded\nworldwide healthcare systems. Computer-aided diagnosis for COVID-19 fast\ndetection and patient triage is becoming critical. This paper proposes a novel\nself-knowledge distillation based self-supervised learning method for COVID-19\ndetection from chest X-ray images. Our method can use self-knowledge of images\nbased on similarities of their visual features for self-supervised learning.\nExperimental results show that our method achieved an HM score of 0.988, an AUC\nof 0.999, and an accuracy of 0.957 on the largest open COVID-19 chest X-ray\ndataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Li_G/0/1/0/all/0/1\">Guang Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Togo_R/0/1/0/all/0/1\">Ren Togo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ogawa_T/0/1/0/all/0/1\">Takahiro Ogawa</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Haseyama_M/0/1/0/all/0/1\">Miki Haseyama</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MS-RNN: A Flexible Multi-Scale Framework for Spatiotemporal Predictive Learning. (arXiv:2206.03010v1 [cs.CV])","link":"http://arxiv.org/abs/2206.03010","description":"<p>Spatiotemporal predictive learning is to predict future frames changes\nthrough historical prior knowledge. Previous work improves prediction\nperformance by making the network wider and deeper, but this also brings huge\nmemory overhead, which seriously hinders the development and application of the\ntechnology. Scale is another dimension to improve model performance in common\ncomputer vision task, which can decrease the computing requirements and better\nsense of context. Such an important improvement point has not been considered\nand explored by recent RNN models. In this paper, learning from the benefit of\nmulti-scale, we propose a general framework named Multi-Scale RNN (MS-RNN) to\nboost recent RNN models. We verify the MS-RNN framework by exhaustive\nexperiments on 4 different datasets (Moving MNIST, KTH, TaxiBJ, and HKO-7) and\nmultiple popular RNN models (ConvLSTM, TrajGRU, PredRNN, PredRNN++, MIM, and\nMotionRNN). The results show the efficiency that the RNN models incorporating\nour framework have much lower memory cost but better performance than before.\nOur code is released at \\url{https://github.com/mazhf/MS-RNN}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhifeng Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jie Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TriBYOL: Triplet BYOL for Self-Supervised Representation Learning. (arXiv:2206.03012v1 [cs.CV])","link":"http://arxiv.org/abs/2206.03012","description":"<p>This paper proposes a novel self-supervised learning method for learning\nbetter representations with small batch sizes. Many self-supervised learning\nmethods based on certain forms of the siamese network have emerged and received\nsignificant attention. However, these methods need to use large batch sizes to\nlearn good representations and require heavy computational resources. We\npresent a new triplet network combined with a triple-view loss to improve the\nperformance of self-supervised representation learning with small batch sizes.\nExperimental results show that our method can drastically outperform\nstate-of-the-art self-supervised learning methods on several datasets in\nsmall-batch cases. Our method provides a feasible solution for self-supervised\nlearning with real-world high-resolution images that uses small batch sizes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Togo_R/0/1/0/all/0/1\">Ren Togo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ogawa_T/0/1/0/all/0/1\">Takahiro Ogawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haseyama_M/0/1/0/all/0/1\">Miki Haseyama</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Devil is in the Labels: Noisy Label Correction for Robust Scene Graph Generation. (arXiv:2206.03014v1 [cs.CV])","link":"http://arxiv.org/abs/2206.03014","description":"<p>Unbiased SGG has achieved significant progress over recent years. However,\nalmost all existing SGG models have overlooked the ground-truth annotation\nqualities of prevailing SGG datasets, i.e., they always assume: 1) all the\nmanually annotated positive samples are equally correct; 2) all the\nun-annotated negative samples are absolutely background. In this paper, we\nargue that both assumptions are inapplicable to SGG: there are numerous \"noisy\"\ngroundtruth predicate labels that break these two assumptions, and these noisy\nsamples actually harm the training of unbiased SGG models. To this end, we\npropose a novel model-agnostic NoIsy label CorrEction strategy for SGG: NICE.\nNICE can not only detect noisy samples but also reassign more high-quality\npredicate labels to them. After the NICE training, we can obtain a cleaner\nversion of SGG dataset for model training. Specifically, NICE consists of three\ncomponents: negative Noisy Sample Detection (Neg-NSD), positive NSD (Pos-NSD),\nand Noisy Sample Correction (NSC). Firstly, in Neg-NSD, we formulate this task\nas an out-of-distribution detection problem, and assign pseudo labels to all\ndetected noisy negative samples. Then, in Pos-NSD, we use a clustering-based\nalgorithm to divide all positive samples into multiple sets, and treat the\nsamples in the noisiest set as noisy positive samples. Lastly, in NSC, we use a\nsimple but effective weighted KNN to reassign new predicate labels to noisy\npositive samples. Extensive results on different backbones and tasks have\nattested to the effectiveness and generalization abilities of each component of\nNICE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Long Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yifeng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhimeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Songyang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jun Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Development of Automatic Endotracheal Tube and Carina Detection on Portable Supine Chest Radiographs using Artificial Intelligence. (arXiv:2206.03017v1 [cs.CV])","link":"http://arxiv.org/abs/2206.03017","description":"<p>The image quality of portable supine chest radiographs is inherently poor due\nto low contrast and high noise. The endotracheal intubation detection requires\nthe locations of the endotracheal tube (ETT) tip and carina. The goal is to\nfind the distance between the ETT tip and the carina in chest radiography. To\novercome such a problem, we propose a feature extraction method with Mask\nR-CNN. The Mask R-CNN predicts a tube and a tracheal bifurcation in an image.\nThen, the feature extraction method is used to find the feature point of the\nETT tip and that of the carina. Therefore, the ETT-carina distance can be\nobtained. In our experiments, our results can exceed 96\\% in terms of recall\nand precision. Moreover, the object error is less than $4.7751\\pm 5.3420$ mm,\nand the ETT-carina distance errors are less than $5.5432\\pm 6.3100$ mm. The\nexternal validation shows that the proposed method is a high-robustness system.\nAccording to the Pearson correlation coefficient, we have a strong correlation\nbetween the board-certified intensivists and our result in terms of ETT-carina\ndistance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chi-Yeh Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Min-Hsin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yung-Nien Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_C/0/1/0/all/0/1\">Chao-Han Lai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning Techniques for Visual Counting. (arXiv:2206.03033v1 [cs.CV])","link":"http://arxiv.org/abs/2206.03033","description":"<p>In this thesis, I investigated and enhanced the visual counting task, which\nautomatically estimates the number of objects in still images or video frames.\nRecently, due to the growing interest in it, several CNN-based solutions have\nbeen suggested by the scientific community. These artificial neural networks\nprovide a way to automatically learn effective representations from raw visual\ndata and can be successfully employed to address typical challenges\ncharacterizing this task, such as different illuminations and object scales.\nBut apart from these difficulties, I targeted some other crucial limitations in\nthe adoption of CNNs, proposing solutions that I experimentally evaluated in\nthe context of the counting task which turns out to be particularly affected by\nthese shortcomings.\n</p>\n<p>In particular, I tackled the problem related to the lack of data needed for\ntraining current CNN-based solutions. Given that the budget for labeling is\nlimited, data scarcity still represents an open problem, particularly evident\nin tasks such as the counting one, where the objects to be labeled are\nthousands per image. Specifically, I introduced synthetic datasets gathered\nfrom virtual environments, where the training labels are automatically\ncollected. I proposed Domain Adaptation strategies aiming at mitigating the\ndomain gap existing between the training and test data distributions. I\npresented a counting strategy where I took advantage of the redundant\ninformation characterizing datasets labeled by multiple annotators. Moreover, I\ntackled the engineering challenges coming out of the adoption of CNN techniques\nin environments with limited power resources. I introduced solutions for\ncounting vehicles directly onboard embedded vision systems. Finally, I designed\nan embedded modular Computer Vision-based system that can carry out several\ntasks to help monitor individual and collective human safety rules.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ciampi_L/0/1/0/all/0/1\">Luca Ciampi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COVIDx CT-3: A Large-scale, Multinational, Open-Source Benchmark Dataset for Computer-aided COVID-19 Screening from Chest CT Images. (arXiv:2206.03043v1 [eess.IV])","link":"http://arxiv.org/abs/2206.03043","description":"<p>Computed tomography (CT) has been widely explored as a COVID-19 screening and\nassessment tool to complement RT-PCR testing. To assist radiologists with\nCT-based COVID-19 screening, a number of computer-aided systems have been\nproposed; however, many proposed systems are built using CT data which is\nlimited in both quantity and diversity. Motivated to support efforts in the\ndevelopment of machine learning-driven screening systems, we introduce COVIDx\nCT-3, a large-scale multinational benchmark dataset for detection of COVID-19\ncases from chest CT images. COVIDx CT-3 includes 431,205 CT slices from 6,068\npatients across at least 17 countries, which to the best of our knowledge\nrepresents the largest, most diverse dataset of COVID-19 CT images in\nopen-access form. Additionally, we examine the data diversity and potential\nbiases of the COVIDx CT-3 dataset, finding that significant geographic and\nclass imbalances remain despite efforts to curate data from a wide variety of\nsources.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Tuinstra_T/0/1/0/all/0/1\">Tia Tuinstra</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gunraj_H/0/1/0/all/0/1\">Hayden Gunraj</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wong_A/0/1/0/all/0/1\">Alexander Wong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Layered Depth Refinement with Mask Guidance. (arXiv:2206.03048v1 [cs.CV])","link":"http://arxiv.org/abs/2206.03048","description":"<p>Depth maps are used in a wide range of applications from 3D rendering to 2D\nimage effects such as Bokeh. However, those predicted by single image depth\nestimation (SIDE) models often fail to capture isolated holes in objects and/or\nhave inaccurate boundary regions. Meanwhile, high-quality masks are much easier\nto obtain, using commercial auto-masking tools or off-the-shelf methods of\nsegmentation and matting or even by manual editing. Hence, in this paper, we\nformulate a novel problem of mask-guided depth refinement that utilizes a\ngeneric mask to refine the depth prediction of SIDE models. Our framework\nperforms layered refinement and inpainting/outpainting, decomposing the depth\nmap into two separate layers signified by the mask and the inverse mask. As\ndatasets with both depth and mask annotations are scarce, we propose a\nself-supervised learning scheme that uses arbitrary masks and RGB-D datasets.\nWe empirically show that our method is robust to different types of masks and\ninitial depth predictions, accurately refining depth values in inner and outer\nmask boundary regions. We further analyze our model with an ablation study and\ndemonstrate results on real applications. More information can be found at\nhttps://sooyekim.github.io/MaskDepth/ .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Soo Ye Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niklaus_S/0/1/0/all/0/1\">Simon Niklaus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1\">Yifei Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Simon Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhe Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Munchurl Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Siamese Encoder-based Spatial-Temporal Mixer for Growth Trend Prediction of Lung Nodules on CT Scans. (arXiv:2206.03049v1 [eess.IV])","link":"http://arxiv.org/abs/2206.03049","description":"<p>In the management of lung nodules, we are desirable to predict nodule\nevolution in terms of its diameter variation on Computed Tomography (CT) scans\nand then provide a follow-up recommendation according to the predicted result\nof the growing trend of the nodule. In order to improve the performance of\ngrowth trend prediction for lung nodules, it is vital to compare the changes of\nthe same nodule in consecutive CT scans. Motivated by this, we screened out\n4,666 subjects with more than two consecutive CT scans from the National Lung\nScreening Trial (NLST) dataset to organize a temporal dataset called NLSTt. In\nspecific, we first detect and pair regions of interest (ROIs) covering the same\nnodule based on registered CT scans. After that, we predict the texture\ncategory and diameter size of the nodules through models. Last, we annotate the\nevolution class of each nodule according to its changes in diameter. Based on\nthe built NLSTt dataset, we propose a siamese encoder to simultaneously exploit\nthe discriminative features of 3D ROIs detected from consecutive CT scans. Then\nwe novelly design a spatial-temporal mixer (STM) to leverage the interval\nchanges of the same nodule in sequential 3D ROIs and capture spatial\ndependencies of nodule regions and the current 3D ROI. According to the\nclinical diagnosis routine, we employ hierarchical loss to pay more attention\nto growing nodules. The extensive experiments on our organized dataset\ndemonstrate the advantage of our proposed method. We also conduct experiments\non an in-house dataset to evaluate the clinical utility of our method by\ncomparing it against skilled clinicians.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Fang_J/0/1/0/all/0/1\">Jiansheng Fang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jingwen Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_A/0/1/0/all/0/1\">Anwei Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yan_Y/0/1/0/all/0/1\">Yuguang Yan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hou_Y/0/1/0/all/0/1\">Yonghe Hou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Song_C/0/1/0/all/0/1\">Chao Song</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_H/0/1/0/all/0/1\">Hongbo Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jiang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spatial Parsing and Dynamic Temporal Pooling networks for Human-Object Interaction detection. (arXiv:2206.03061v1 [cs.CV])","link":"http://arxiv.org/abs/2206.03061","description":"<p>The key of Human-Object Interaction(HOI) recognition is to infer the\nrelationship between human and objects. Recently, the image's Human-Object\nInteraction(HOI) detection has made significant progress. However, there is\nstill room for improvement in video HOI detection performance. Existing\none-stage methods use well-designed end-to-end networks to detect a video\nsegment and directly predict an interaction.\n</p>\n<p>It makes the model learning and further optimization of the network more\ncomplex. This paper introduces the Spatial Parsing and Dynamic Temporal Pooling\n(SPDTP) network, which takes the entire video as a spatio-temporal graph with\nhuman and object nodes as input. Unlike existing methods, our proposed network\npredicts the difference between interactive and non-interactive pairs through\nexplicit spatial parsing, and then performs interaction recognition. Moreover,\nwe propose a learnable and differentiable Dynamic Temporal Module(DTM) to\nemphasize the keyframes of the video and suppress the redundant frame.\nFurthermore, the experimental results show that SPDTP can pay more attention to\nactive human-object pairs and valid keyframes. Overall, we achieve\nstate-of-the-art performance on CAD-120 dataset and Something-Else dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_G/0/1/0/all/0/1\">Guangming Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhen_W/0/1/0/all/0/1\">Wu Zhen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_L/0/1/0/all/0/1\">Lan Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_P/0/1/0/all/0/1\">Peiyi Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1\">Ning Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_C/0/1/0/all/0/1\">Cong Hua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Object Scan Context: Object-centric Spatial Descriptor for Place Recognition within 3D Point Cloud Map. (arXiv:2206.03062v1 [cs.CV])","link":"http://arxiv.org/abs/2206.03062","description":"<p>Place recognition technology endows a SLAM algorithm with the ability to\neliminate accumulated errors and to relocalize itself. Existing methods on\npoint cloud-based place recognition often leverage the matching of global\ndescriptors which are lidar-centric. These methods have the following two major\ndefects: place recognition cannot be performed when the distance between the\ntwo point clouds is far, and only the rotation angle can be calculated without\nthe offset in the X and Y direction. To solve these two problems, we propose a\nnovel global descriptor, which is built around the Main Object, in this way,\ndescriptors are no longer dependent on the observation position. We analyze the\ntheory that this method can perfectly solve the above two problems, and conduct\na lot of experiments in KITTI and some extreme scenarios, which show that our\nmethod has obvious advantages over traditional methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1\">Haodong Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yudong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_S/0/1/0/all/0/1\">Shengyin Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xue Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jian Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Minimum Efforts to Build an End-to-End Spatial-Temporal Action Detector. (arXiv:2206.03064v1 [cs.CV])","link":"http://arxiv.org/abs/2206.03064","description":"<p>Spatial-temporal action detection is a vital part of video understanding.\nCurrent spatial-temporal action detection methods will first use an object\ndetector to obtain person candidate proposals. Then, the model will classify\nthe person candidates into different action categories. So-called two-stage\nmethods are heavy and hard to apply in real-world applications. Some existing\nmethods use a unified model structure, But they perform badly with the vanilla\nmodel and often need extra modules to boost the performance. In this paper, we\nexplore the strategy to build an end-to-end spatial-temporal action detector\nwith minimal modifications. To this end, we propose a new method named ME-STAD,\nwhich solves the spatial-temporal action detection problem in an end-to-end\nmanner. Besides the model design, we propose a novel labeling strategy to deal\nwith sparse annotations in spatial-temporal datasets. The proposed ME-STAD\nachieves better results (2.2% mAP boost) than original two-stage detectors and\naround 80% FLOPs reduction. Moreover, our proposed ME-STAD only has minimum\nmodifications with previous methods and does not require extra components. Our\ncode will be made public.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sui_L/0/1/0/all/0/1\">Lin Sui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chen-Lin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_L/0/1/0/all/0/1\">Lixin Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_F/0/1/0/all/0/1\">Feng Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recent Advances for Quantum Neural Networks in Generative Learning. (arXiv:2206.03066v1 [quant-ph])","link":"http://arxiv.org/abs/2206.03066","description":"<p>Quantum computers are next-generation devices that hold promise to perform\ncalculations beyond the reach of classical computers. A leading method towards\nachieving this goal is through quantum machine learning, especially quantum\ngenerative learning. Due to the intrinsic probabilistic nature of quantum\nmechanics, it is reasonable to postulate that quantum generative learning\nmodels (QGLMs) may surpass their classical counterparts. As such, QGLMs are\nreceiving growing attention from the quantum physics and computer science\ncommunities, where various QGLMs that can be efficiently implemented on\nnear-term quantum machines with potential computational advantages are\nproposed. In this paper, we review the current progress of QGLMs from the\nperspective of machine learning. Particularly, we interpret these QGLMs,\ncovering quantum circuit born machines, quantum generative adversarial\nnetworks, quantum Boltzmann machines, and quantum autoencoders, as the quantum\nextension of classical generative learning models. In this context, we explore\ntheir intrinsic relation and their fundamental differences. We further\nsummarize the potential applications of QGLMs in both conventional machine\nlearning tasks and quantum physics. Last, we discuss the challenges and further\nresearch directions for QGLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/quant-ph/1/au:+Tian_J/0/1/0/all/0/1\">Jinkai Tian</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Sun_X/0/1/0/all/0/1\">Xiaoyu Sun</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Du_Y/0/1/0/all/0/1\">Yuxuan Du</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Zhao_S/0/1/0/all/0/1\">Shanshan Zhao</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Liu_Q/0/1/0/all/0/1\">Qing Liu</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Zhang_K/0/1/0/all/0/1\">Kaining Zhang</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Yi_W/0/1/0/all/0/1\">Wei Yi</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Huang_W/0/1/0/all/0/1\">Wanrong Huang</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Wang_C/0/1/0/all/0/1\">Chaoyue Wang</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Wu_X/0/1/0/all/0/1\">Xingyao Wu</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Hsieh_M/0/1/0/all/0/1\">Min-Hsiu Hsieh</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Liu_T/0/1/0/all/0/1\">Tongliang Liu</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Yang_W/0/1/0/all/0/1\">Wenjing Yang</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pushing the Limits of Learning-based Traversability Analysis for Autonomous Driving on CPU. (arXiv:2206.03083v1 [cs.RO])","link":"http://arxiv.org/abs/2206.03083","description":"<p>Self-driving vehicles and autonomous ground robots require a reliable and\naccurate method to analyze the traversability of the surrounding environment\nfor safe navigation. This paper proposes and evaluates a real-time machine\nlearning-based Traversability Analysis method that combines geometric features\nwith appearance-based features in a hybrid approach based on a SVM classifier.\nIn particular, we show that integrating a new set of geometric and visual\nfeatures and focusing on important implementation details enables a noticeable\nboost in performance and reliability. The proposed approach has been compared\nwith state-of-the-art Deep Learning approaches on a public dataset of outdoor\ndriving scenarios. It reaches an accuracy of 89.2% in scenarios of varying\ncomplexity, demonstrating its effectiveness and robustness. The method runs\nfully on CPU and reaches comparable results with respect to the other methods,\noperates faster, and requires fewer hardware resources.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fusaro_D/0/1/0/all/0/1\">Daniel Fusaro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olivastri_E/0/1/0/all/0/1\">Emilio Olivastri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Evangelista_D/0/1/0/all/0/1\">Daniele Evangelista</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Imperoli_M/0/1/0/all/0/1\">Marco Imperoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menegatti_E/0/1/0/all/0/1\">Emanuele Menegatti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pretto_A/0/1/0/all/0/1\">Alberto Pretto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Online Deep Clustering with Video Track Consistency. (arXiv:2206.03086v1 [cs.CV])","link":"http://arxiv.org/abs/2206.03086","description":"<p>Several unsupervised and self-supervised approaches have been developed in\nrecent years to learn visual features from large-scale unlabeled datasets.\nTheir main drawback however is that these methods are hardly able to recognize\nvisual features of the same object if it is simply rotated or the perspective\nof the camera changes. To overcome this limitation and at the same time exploit\na useful source of supervision, we take into account video object tracks.\nFollowing the intuition that two patches in a track should have similar visual\nrepresentations in a learned feature space, we adopt an unsupervised\nclustering-based approach and constrain such representations to be labeled as\nthe same category since they likely belong to the same object or object part.\nExperimental results on two downstream tasks on different datasets demonstrate\nthe effectiveness of our Online Deep Clustering with Video Track Consistency\n(ODCT) approach compared to prior work, which did not leverage temporal\ninformation. In addition we show that exploiting an unsupervised\nclass-agnostic, yet noisy, track generator yields to better accuracy compared\nto relying on costly and precise track annotations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alfani_A/0/1/0/all/0/1\">Alessandra Alfani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Becattini_F/0/1/0/all/0/1\">Federico Becattini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seidenari_L/0/1/0/all/0/1\">Lorenzo Seidenari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bimbo_A/0/1/0/all/0/1\">Alberto Del Bimbo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Critical Regularizations for Neural Surface Reconstruction in the Wild. (arXiv:2206.03087v1 [cs.CV])","link":"http://arxiv.org/abs/2206.03087","description":"<p>Neural implicit functions have recently shown promising results on surface\nreconstructions from multiple views. However, current methods still suffer from\nexcessive time complexity and poor robustness when reconstructing unbounded or\ncomplex scenes. In this paper, we present RegSDF, which shows that proper point\ncloud supervisions and geometry regularizations are sufficient to produce\nhigh-quality and robust reconstruction results. Specifically, RegSDF takes an\nadditional oriented point cloud as input, and optimizes a signed distance field\nand a surface light field within a differentiable rendering framework. We also\nintroduce the two critical regularizations for this optimization. The first one\nis the Hessian regularization that smoothly diffuses the signed distance values\nto the entire distance field given noisy and incomplete input. And the second\none is the minimal surface regularization that compactly interpolates and\nextrapolates the missing geometry. Extensive experiments are conducted on DTU,\nBlendedMVS, and Tanks and Temples datasets. Compared with recent neural surface\nreconstruction approaches, RegSDF is able to reconstruct surfaces with fine\ndetails even for open scenes with complex topologies and unstructured camera\ntrajectories.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jingyang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yao Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shiwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_T/0/1/0/all/0/1\">Tian Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McKinnon_D/0/1/0/all/0/1\">David McKinnon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsin_Y/0/1/0/all/0/1\">Yanghai Tsin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quan_L/0/1/0/all/0/1\">Long Quan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dual Swin-Transformer based Mutual Interactive Network for RGB-D Salient Object Detection. (arXiv:2206.03105v1 [cs.CV])","link":"http://arxiv.org/abs/2206.03105","description":"<p>Salient Object Detection is the task of predicting the human attended region\nin a given scene. Fusing depth information has been proven effective in this\ntask. The main challenge of this problem is how to aggregate the complementary\ninformation from RGB modality and depth modality. However, conventional deep\nmodels heavily rely on CNN feature extractors, and the long-range contextual\ndependencies are usually ignored. In this work, we propose Dual\nSwin-Transformer based Mutual Interactive Network. We adopt Swin-Transformer as\nthe feature extractor for both RGB and depth modality to model the long-range\ndependencies in visual inputs. Before fusing the two branches of features into\none, attention-based modules are applied to enhance features from each\nmodality. We design a self-attention-based cross-modality interaction module\nand a gated modality attention module to leverage the complementary information\nbetween the two modalities. For the saliency decoding, we create different\nstages enhanced with dense connections and keep a decoding memory while the\nmulti-level encoding features are considered simultaneously. Considering the\ninaccurate depth map issue, we collect the RGB features of early stages into a\nskip convolution module to give more guidance from RGB modality to the final\nsaliency prediction. In addition, we add edge supervision to regularize the\nfeature learning process. Comprehensive experiments on five standard RGB-D SOD\nbenchmark datasets over four evaluation metrics demonstrate the superiority of\nthe proposed DTMINet method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_C/0/1/0/all/0/1\">Chao Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwong_S/0/1/0/all/0/1\">Sam Kwong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MIRNF: Medical Image Registration via Neural Fields. (arXiv:2206.03111v1 [cs.CV])","link":"http://arxiv.org/abs/2206.03111","description":"<p>Image registration is widely used in medical image analysis to provide\nspatial correspondences between two images. Recently learning-based methods\nutilizing convolutional neural networks (CNNs) have been proposed for solving\nimage registration problems. The learning-based methods tend to be much faster\nthan traditional optimization-based methods, but the accuracy improvements\ngained from the complex CNN-based methods are modest. Here we introduce a new\ndeep-neural net-based image registration framework, named \\textbf{MIRNF}, which\nrepresents the correspondence mapping with a continuous function implemented\nvia Neural Fields. MIRNF outputs either a deformation vector or velocity vector\ngiven a 3D coordinate as input. To ensure the mapping is diffeomorphic, the\nvelocity vector output from MIRNF is integrated using the Neural ODE solver to\nderive the correspondences between two images. Furthermore, we propose a hybrid\ncoordinate sampler along with a cascaded architecture to achieve the\nhigh-similarity mapping performance and low-distortion deformation fields. We\nconduct experiments on two 3D MR brain scan datasets, showing that our proposed\nframework provides state-of-art registration performance while maintaining\ncomparable optimization time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1\">Shanlin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1\">Kun Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_D/0/1/0/all/0/1\">Deying Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1\">Chenyu You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xiaohui Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Wavelet Prior Attention Learning in Axial Inpainting Network. (arXiv:2206.03113v1 [cs.CV])","link":"http://arxiv.org/abs/2206.03113","description":"<p>Image inpainting is the task of filling masked or unknown regions of an image\nwith visually realistic contents, which has been remarkably improved by Deep\nNeural Networks (DNNs) recently. Essentially, as an inverse problem, the\ninpainting has the underlying challenges of reconstructing semantically\ncoherent results without texture artifacts. Many previous efforts have been\nmade via exploiting attention mechanisms and prior knowledge, such as edges and\nsemantic segmentation. However, these works are still limited in practice by an\navalanche of learnable prior parameters and prohibitive computational burden.\nTo this end, we propose a novel model -- Wavelet prior attention learning in\nAxial Inpainting Network (WAIN), whose generator contains the encoder, decoder,\nas well as two key components of Wavelet image Prior Attention (WPA) and\nstacked multi-layer Axial-Transformers (ATs). Particularly, the WPA guides the\nhigh-level feature aggregation in the multi-scale frequency domain, alleviating\nthe textual artifacts. Stacked ATs employ unmasked clues to help model\nreasonable features along with low-level features of horizontal and vertical\naxes, improving the semantic coherence. Extensive quantitative and qualitative\nexperiments on Celeba-HQ and Places2 datasets are conducted to validate that\nour WAIN can achieve state-of-the-art performance over the competitors. The\ncodes and models will be released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_C/0/1/0/all/0/1\">Chenjie Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengrong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuntao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yanwei Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Training of Handwritten Word Recognition for Synthetic-to-Real Adaptation. (arXiv:2206.03149v1 [cs.CV])","link":"http://arxiv.org/abs/2206.03149","description":"<p>Performances of Handwritten Text Recognition (HTR) models are largely\ndetermined by the availability of labeled and representative training samples.\nHowever, in many application scenarios labeled samples are scarce or costly to\nobtain. In this work, we propose a self-training approach to train a HTR model\nsolely on synthetic samples and unlabeled data. The proposed training scheme\nuses an initial model trained on synthetic data to make predictions for the\nunlabeled target dataset. Starting from this initial model with rather poor\nperformance, we show that a considerable adaptation is possible by training\nagainst the predicted pseudo-labels. Moreover, the investigated self-training\nstrategy does not require any manually annotated training samples. We evaluate\nthe proposed method on four widely used benchmark datasets and show its\neffectiveness on closing the gap to a model trained in a fully-supervised\nmanner.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wolf_F/0/1/0/all/0/1\">Fabian Wolf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fink_G/0/1/0/all/0/1\">Gernot A. Fink</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Utility of Equivariant Message Passing in Cortical Mesh Segmentation. (arXiv:2206.03164v1 [cs.CV])","link":"http://arxiv.org/abs/2206.03164","description":"<p>The automated segmentation of cortical areas has been a long-standing\nchallenge in medical image analysis. The complex geometry of the cortex is\ncommonly represented as a polygon mesh, whose segmentation can be addressed by\ngraph-based learning methods. When cortical meshes are misaligned across\nsubjects, current methods produce significantly worse segmentation results,\nlimiting their ability to handle multi-domain data. In this paper, we\ninvestigate the utility of E(n)-equivariant graph neural networks (EGNNs),\ncomparing their performance against plain graph neural networks (GNNs). Our\nevaluation shows that GNNs outperform EGNNs on aligned meshes, due to their\nability to leverage the presence of a global coordinate system. On misaligned\nmeshes, the performance of plain GNNs drop considerably, while E(n)-equivariant\nmessage passing maintains the same segmentation results. The best results can\nalso be obtained by using plain GNNs on realigned data (co-registered meshes in\na global coordinate system).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Unyi_D/0/1/0/all/0/1\">D&#xe1;niel Unyi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Insalata_F/0/1/0/all/0/1\">Ferdinando Insalata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Velickovic_P/0/1/0/all/0/1\">Petar Veli&#x10d;kovi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gyires_Toth_B/0/1/0/all/0/1\">B&#xe1;lint Gyires-T&#xf3;th</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Image Captioning with Control Signal of Sentence Quality. (arXiv:2206.03196v1 [cs.CV])","link":"http://arxiv.org/abs/2206.03196","description":"<p>In the dataset of image captioning, each image is aligned with several\ncaptions. Despite the fact that the quality of these descriptions varies,\nexisting captioning models treat them equally in the training process. In this\npaper, we propose a new control signal of sentence quality, which is taken as\nan additional input to the captioning model. By integrating the control signal\ninformation, captioning models are aware of the quality level of the target\nsentences and handle them differently. Moreover, we propose a novel\nreinforcement training method specially designed for the control signal of\nsentence quality: Quality-oriented Self-Annotated Training (Q-SAT). Equipped\nwith R-Drop strategy, models controlled by the highest quality level surpass\nbaseline models a lot on accuracy-based evaluation metrics, which validates the\neffectiveness of our proposed methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhangzi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_H/0/1/0/all/0/1\">Hong Qu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Omnivision forecasting: combining satellite observations with sky images for improved intra-hour solar energy predictions. (arXiv:2206.03207v1 [cs.CV])","link":"http://arxiv.org/abs/2206.03207","description":"<p>Integration of intermittent renewable energy sources into electric grids in\nlarge proportions is challenging. A well-established approach aimed at\naddressing this difficulty involves the anticipation of the upcoming energy\nsupply variability to adapt the response of the grid. In solar energy,\nshort-term changes in electricity production caused by occluding clouds can be\npredicted at different time scales from all-sky cameras (up to 30-min ahead)\nand satellite observations (up to 6h ahead). In this study, we integrate these\ntwo complementary points of view on the cloud cover in a single machine\nlearning framework to improve intra-hour (up to 60-min ahead) irradiance\nforecasting. Both deterministic and probabilistic predictions are evaluated in\ndifferent weather conditions (clear-sky, cloudy, overcast) and with different\ninput configurations (sky images, satellite observations and/or past irradiance\nvalues). Our results show that the hybrid model benefits predictions in\nclear-sky conditions and improves longer-term forecasting. This study lays the\ngroundwork for future novel approaches of combining sky images and satellite\nobservations in a single learning framework to advance solar nowcasting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Paletta_Q/0/1/0/all/0/1\">Quentin Paletta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arbod_G/0/1/0/all/0/1\">Guillaume Arbod</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lasenby_J/0/1/0/all/0/1\">Joan Lasenby</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Neural Patchworks: Coping with Large Segmentation Tasks. (arXiv:2206.03210v1 [cs.CV])","link":"http://arxiv.org/abs/2206.03210","description":"<p>Convolutional neural networks are the way to solve arbitrary image\nsegmentation tasks. However, when images are large, memory demands often exceed\nthe available resources, in particular on a common GPU. Especially in\nbiomedical imaging, where 3D images are common, the problems are apparent. A\ntypical approach to solve this limitation is to break the task into smaller\nsubtasks by dividing images into smaller image patches. Another approach, if\napplicable, is to look at the 2D image sections separately, and to solve the\nproblem in 2D. Often, the loss of global context makes such approaches less\neffective; important global information might not be present in the current\nimage patch, or the selected 2D image section. Here, we propose Deep Neural\nPatchworks (DNP), a segmentation framework that is based on hierarchical and\nnested stacking of patch-based networks that solves the dilemma between global\ncontext and memory limitations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Reisert_M/0/1/0/all/0/1\">Marco Reisert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Russe_M/0/1/0/all/0/1\">Maximilian Russe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elsheikh_S/0/1/0/all/0/1\">Samer Elsheikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kellner_E/0/1/0/all/0/1\">Elias Kellner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skibbe_H/0/1/0/all/0/1\">Henrik Skibbe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards better Interpretable and Generalizable AD detection using Collective Artificial Intelligence. (arXiv:2206.03247v1 [eess.IV])","link":"http://arxiv.org/abs/2206.03247","description":"<p>Accurate diagnosis and prognosis of Alzheimer's disease are crucial for\ndeveloping new therapies and reducing the associated costs. Recently, with the\nadvances of convolutional neural networks, deep learning methods have been\nproposed to automate these two tasks using structural MRI. However, these\nmethods often suffer from a lack of interpretability and generalization and\nhave limited prognosis performance. In this paper, we propose a novel deep\nframework designed to overcome these limitations. Our pipeline consists of two\nstages. In the first stage, 125 3D U-Nets are used to estimate voxelwise grade\nscores over the whole brain. The resulting 3D maps are then fused to construct\nan interpretable 3D grading map indicating the disease severity at the\nstructure level. As a consequence, clinicians can use this map to detect the\nbrain structures affected by the disease. In the second stage, the grading map\nand subject's age are used to perform classification with a graph convolutional\nneural network. Experimental results based on 2106 subjects demonstrated\ncompetitive performance of our deep framework compared to state-of-the-art\nmethods on different datasets for both AD diagnosis and prognosis. Moreover, we\nfound that using a large number of U-Nets processing different overlapping\nbrain areas improved the generalization capacity of the proposed methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_H/0/1/0/all/0/1\">Huy-Dung Nguyen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Clement_M/0/1/0/all/0/1\">Micha&#xeb;l Cl&#xe9;ment</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mansencal_B/0/1/0/all/0/1\">Boris Mansencal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Coupe_P/0/1/0/all/0/1\">Pierrick Coup&#xe9;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Effectiveness of Fine-tuning Versus Meta-reinforcement Learning. (arXiv:2206.03271v1 [cs.LG])","link":"http://arxiv.org/abs/2206.03271","description":"<p>Intelligent agents should have the ability to leverage knowledge from\npreviously learned tasks in order to learn new ones quickly and efficiently.\nMeta-learning approaches have emerged as a popular solution to achieve this.\nHowever, meta-reinforcement learning (meta-RL) algorithms have thus far been\nrestricted to simple environments with narrow task distributions. Moreover, the\nparadigm of pretraining followed by fine-tuning to adapt to new tasks has\nemerged as a simple yet effective solution in supervised and self-supervised\nlearning. This calls into question the benefits of meta-learning approaches\nalso in reinforcement learning, which typically come at the cost of high\ncomplexity. We hence investigate meta-RL approaches in a variety of\nvision-based benchmarks, including Procgen, RLBench, and Atari, where\nevaluations are made on completely novel tasks. Our findings show that when\nmeta-learning approaches are evaluated on different tasks (rather than\ndifferent variations of the same task), multi-task pretraining with fine-tuning\non new tasks performs equally as well, or better, than meta-pretraining with\nmeta test-time adaptation. This is encouraging for future research, as\nmulti-task pretraining tends to be simpler and computationally cheaper than\nmeta-RL. From these findings, we advocate for evaluating future meta-RL methods\non more challenging tasks and including multi-task pretraining with fine-tuning\nas a simple, yet strong baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mandi_Z/0/1/0/all/0/1\">Zhao Mandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1\">Pieter Abbeel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+James_S/0/1/0/all/0/1\">Stephen James</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NeMF: Neural Motion Fields for Kinematic Animation. (arXiv:2206.03287v1 [cs.CV])","link":"http://arxiv.org/abs/2206.03287","description":"<p>We present an implicit neural representation to learn the spatio-temporal\nspace of kinematic motions. Unlike previous work that represents motion as\ndiscrete sequential samples, we propose to express the vast motion space as a\ncontinuous function over time, hence the name Neural Motion Fields (NeMF).\nSpecifically, we use a neural network to learn this function for miscellaneous\nsets of motions, which is designed to be a generative model conditioned on a\ntemporal coordinate $t$ and a random vector $z$ for controlling the style. The\nmodel is then trained as a Variational Autoencoder (VAE) with motion encoders\nto sample the latent space. We train our model with diverse human motion\ndataset and quadruped dataset to prove its versatility, and finally deploy it\nas a generic motion prior to solve task-agnostic problems and show its\nsuperiority in different motion generation and editing applications, such as\nmotion interpolation, in-betweening, and re-navigating.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_C/0/1/0/all/0/1\">Chengan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saito_J/0/1/0/all/0/1\">Jun Saito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zachary_J/0/1/0/all/0/1\">James Zachary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rushmeier_H/0/1/0/all/0/1\">Holly Rushmeier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yi Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Parotid Gland MRI Segmentation Based on Swin-Unet and Multimodal Images. (arXiv:2206.03336v1 [eess.IV])","link":"http://arxiv.org/abs/2206.03336","description":"<p>Parotid gland tumors account for approximately 2% to 10% of head and neck\ntumors. Preoperative tumor localization, differential diagnosis, and subsequent\nselection of appropriate treatment for parotid gland tumors is critical.\nHowever, the relative rarity of these tumors and the highly dispersed tissue\ntypes have left an unmet need for a subtle differential diagnosis of such\nneoplastic lesions based on preoperative radiomics. Recently, deep learning\nmethods have developed rapidly, especially Transformer beats the traditional\nconvolutional neural network in computer vision. Many new Transformer-based\nnetworks have been proposed for computer vision tasks. In this study,\nmulticenter multimodal parotid gland MRI images were collected. The Swin-Unet\nwhich was based on Transformer was used. MRI images of STIR, T1 and T2\nmodalities were combined into a three-channel data to train the network. We\nachieved segmentation of the region of interest for parotid gland and tumor.\nThe DSC of the model on the test set was 88.63%, MPA was 99.31%, MIoU was\n83.99%, and HD was 3.04. Then a series of comparison experiments were designed\nin this paper to further validate the segmentation performance of the\nalgorithm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Dai_Y/0/1/0/all/0/1\">Yin Dai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Z/0/1/0/all/0/1\">Zi&#x27;an Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_F/0/1/0/all/0/1\">Fayu Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_S/0/1/0/all/0/1\">Siqi Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_S/0/1/0/all/0/1\">Sheng Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shi_L/0/1/0/all/0/1\">Lifu Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fu_J/0/1/0/all/0/1\">Jun Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"cViL: Cross-Lingual Training of Vision-Language Models using Knowledge Distillation. (arXiv:2206.03354v1 [cs.CL])","link":"http://arxiv.org/abs/2206.03354","description":"<p>Vision-and-language tasks are gaining popularity in the research community,\nbut the focus is still mainly on English. We propose a pipeline that utilizes\nEnglish-only vision-language models to train a monolingual model for a target\nlanguage. We propose to extend OSCAR+, a model which leverages object tags as\nanchor points for learning image-text alignments, to train on visual question\nanswering datasets in different languages. We propose a novel approach to\nknowledge distillation to train the model in other languages using parallel\nsentences. Compared to other models that use the target language in the\npretraining corpora, we can leverage an existing English model to transfer the\nknowledge to the target language using significantly lesser resources. We also\nrelease a large-scale visual question answering dataset in Japanese and Hindi\nlanguage. Though we restrict our work to visual question answering, our model\ncan be extended to any sequence-level classification task, and it can be\nextended to other languages as well. This paper focuses on two languages for\nthe visual question answering task - Japanese and Hindi. Our pipeline\noutperforms the current state-of-the-art models by a relative increase of 4.4%\nand 13.4% respectively in accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_K/0/1/0/all/0/1\">Kshitij Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gautam_D/0/1/0/all/0/1\">Devansh Gautam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mamidi_R/0/1/0/all/0/1\">Radhika Mamidi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An efficient semi-supervised quality control system trained using physics-based MRI-artefact generators and adversarial training. (arXiv:2206.03359v1 [eess.IV])","link":"http://arxiv.org/abs/2206.03359","description":"<p>Large medical imaging data sets are becoming increasingly available. A common\nchallenge in these data sets is to ensure that each sample meets minimum\nquality requirements devoid of significant artefacts. Despite a wide range of\nexisting automatic methods having been developed to identify imperfections and\nartefacts in medical imaging, they mostly rely on data-hungry methods. In\nparticular, the lack of sufficient scans with artefacts available for training\nhas created a barrier in designing and deploying machine learning in clinical\nresearch. To tackle this problem, we propose a novel framework having four main\ncomponents: (1) a set of artefact generators inspired by magnetic resonance\nphysics to corrupt brain MRI scans and augment a training dataset, (2) a set of\nabstract and engineered features to represent images compactly, (3) a feature\nselection process that depends on the class of artefact to improve\nclassification performance, and (4) a set of Support Vector Machine (SVM)\nclassifiers trained to identify artefacts. Our novel contributions are\nthreefold: first, we use the novel physics-based artefact generators to\ngenerate synthetic brain MRI scans with controlled artefacts as a data\naugmentation technique. This will avoid the labour-intensive collection and\nlabelling process of scans with rare artefacts. Second, we propose a large pool\nof abstract and engineered image features developed to identify 9 different\nartefacts for structural MRI. Finally, we use an artefact-based feature\nselection block that, for each class of artefacts, finds the set of features\nthat provide the best classification performance. We performed validation\nexperiments on a large data set of scans with artificially-generated artefacts,\nand in a multiple sclerosis clinical trial where real artefacts were identified\nby experts, showing that the proposed pipeline outperforms traditional methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ravi_D/0/1/0/all/0/1\">Daniele Ravi</a> (for the Alzheimer&#x27;s Disease Neuroimaging Initiative), <a href=\"http://arxiv.org/find/eess/1/au:+Barkhof_F/0/1/0/all/0/1\">Frederik Barkhof</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Alexander_D/0/1/0/all/0/1\">Daniel C. Alexander</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Parker_G/0/1/0/all/0/1\">Geoffrey JM Parker</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Eshaghi_A/0/1/0/all/0/1\">Arman Eshaghi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical Similarity Learning for Aliasing Suppression Image Super-Resolution. (arXiv:2206.03361v1 [cs.CV])","link":"http://arxiv.org/abs/2206.03361","description":"<p>As a highly ill-posed issue, single image super-resolution (SISR) has been\nwidely investigated in recent years. The main task of SISR is to recover the\ninformation loss caused by the degradation procedure. According to the Nyquist\nsampling theory, the degradation leads to aliasing effect and makes it hard to\nrestore the correct textures from low-resolution (LR) images. In practice,\nthere are correlations and self-similarities among the adjacent patches in the\nnatural images. This paper considers the self-similarity and proposes a\nhierarchical image super-resolution network (HSRNet) to suppress the influence\nof aliasing. We consider the SISR issue in the optimization perspective, and\npropose an iterative solution pattern based on the half-quadratic splitting\n(HQS) method. To explore the texture with local image prior, we design a\nhierarchical exploration block (HEB) and progressive increase the receptive\nfield. Furthermore, multi-level spatial attention (MSA) is devised to obtain\nthe relations of adjacent feature and enhance the high-frequency information,\nwhich acts as a crucial role for visual experience. Experimental result shows\nHSRNet achieves better quantitative and visual performance than other works,\nand remits the aliasing more effectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuqing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_Q/0/1/0/all/0/1\">Qi Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1\">Xin Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shanshe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Siwei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1\">Wen Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Localizing Semantic Patches for Accelerating Image Classification. (arXiv:2206.03367v1 [cs.CV])","link":"http://arxiv.org/abs/2206.03367","description":"<p>Existing works often focus on reducing the architecture redundancy for\naccelerating image classification but ignore the spatial redundancy of the\ninput image. This paper proposes an efficient image classification pipeline to\nsolve this problem. We first pinpoint task-aware regions over the input image\nby a lightweight patch proposal network called AnchorNet. We then feed these\nlocalized semantic patches with much smaller spatial redundancy into a general\nclassification network. Unlike the popular design of deep CNN, we aim to\ncarefully design the Receptive Field of AnchorNet without intermediate\nconvolutional paddings. This ensures the exact mapping from a high-level\nspatial location to the specific input image patch. The contribution of each\npatch is interpretable. Moreover, AnchorNet is compatible with any downstream\narchitecture. Experimental results on ImageNet show that our method outperforms\nSOTA dynamic inference methods with fewer inference costs. Our code is\navailable at https://github.com/winycg/AnchorNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chuanguang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_Z/0/1/0/all/0/1\">Zhulin An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yongjun Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IL-MCAM: An interactive learning and multi-channel attention mechanism-based weakly supervised colorectal histopathology image classification approach. (arXiv:2206.03368v1 [cs.CV])","link":"http://arxiv.org/abs/2206.03368","description":"<p>In recent years, colorectal cancer has become one of the most significant\ndiseases that endanger human health. Deep learning methods are increasingly\nimportant for the classification of colorectal histopathology images. However,\nexisting approaches focus more on end-to-end automatic classification using\ncomputers rather than human-computer interaction. In this paper, we propose an\nIL-MCAM framework. It is based on attention mechanisms and interactive\nlearning. The proposed IL-MCAM framework includes two stages: automatic\nlearning (AL) and interactivity learning (IL). In the AL stage, a multi-channel\nattention mechanism model containing three different attention mechanism\nchannels and convolutional neural networks is used to extract multi-channel\nfeatures for classification. In the IL stage, the proposed IL-MCAM framework\ncontinuously adds misclassified images to the training set in an interactive\napproach, which improves the classification ability of the MCAM model. We\ncarried out a comparison experiment on our dataset and an extended experiment\non the HE-NCT-CRC-100K dataset to verify the performance of the proposed\nIL-MCAM framework, achieving classification accuracies of 98.98% and 99.77%,\nrespectively. In addition, we conducted an ablation experiment and an\ninterchangeability experiment to verify the ability and interchangeability of\nthe three channels. The experimental results show that the proposed IL-MCAM\nframework has excellent performance in the colorectal histopathological image\nclassification tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haoyuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoyan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahaman_M/0/1/0/all/0/1\">Md Mamunur Rahaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Weiming Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yixin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wanli Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Changhao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Hongzan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xinyu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grzegorzek_M/0/1/0/all/0/1\">Marcin Grzegorzek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Garment Avatars: Realistic Cloth Driving using Pattern Registration. (arXiv:2206.03373v1 [cs.CV])","link":"http://arxiv.org/abs/2206.03373","description":"<p>Virtual telepresence is the future of online communication. Clothing is an\nessential part of a person's identity and self-expression. Yet, ground truth\ndata of registered clothes is currently unavailable in the required resolution\nand accuracy for training telepresence models for realistic cloth animation.\nHere, we propose an end-to-end pipeline for building drivable representations\nfor clothing. The core of our approach is a multi-view patterned cloth tracking\nalgorithm capable of capturing deformations with high accuracy. We further rely\non the high-quality data produced by our tracking method to build a Garment\nAvatar: an expressive and fully-drivable geometry model for a piece of\nclothing. The resulting model can be animated using a sparse set of views and\nproduces highly realistic reconstructions which are faithful to the driving\nsignals. We demonstrate the efficacy of our pipeline on a realistic virtual\ntelepresence application, where a garment is being reconstructed from two\nviews, and a user can pick and swap garment design as they wish. In addition,\nwe show a challenging scenario when driven exclusively with body pose, our\ndrivable garment avatar is capable of producing realistic cloth geometry of\nsignificantly higher quality than the state-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Halimi_O/0/1/0/all/0/1\">Oshri Halimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prada_F/0/1/0/all/0/1\">Fabian Prada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stuyck_T/0/1/0/all/0/1\">Tuur Stuyck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_D/0/1/0/all/0/1\">Donglai Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bagautdinov_T/0/1/0/all/0/1\">Timur Bagautdinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_H/0/1/0/all/0/1\">He Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kimmel_R/0/1/0/all/0/1\">Ron Kimmel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shiratori_T/0/1/0/all/0/1\">Takaaki Shiratori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chenglei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheikh_Y/0/1/0/all/0/1\">Yaser Sheikh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Shape, Light & Material Decomposition from Images using Monte Carlo Rendering and Denoising. (arXiv:2206.03380v1 [cs.GR])","link":"http://arxiv.org/abs/2206.03380","description":"<p>Recent advances in differentiable rendering have enabled high-quality\nreconstruction of 3D scenes from multi-view images. Most methods rely on simple\nrendering algorithms: pre-filtered direct lighting or learned representations\nof irradiance. We show that a more realistic shading model, incorporating ray\ntracing and Monte Carlo integration, substantially improves decomposition into\nshape, materials &amp; lighting. Unfortunately, Monte Carlo integration provides\nestimates with significant noise, even at large sample counts, which makes\ngradient-based inverse rendering very challenging. To address this, we\nincorporate multiple importance sampling and denoising in a novel inverse\nrendering pipeline. This substantially improves convergence and enables\ngradient-based optimization at low sample counts. We present an efficient\nmethod to jointly reconstruct geometry (explicit triangle meshes), materials,\nand lighting, which substantially improves material and light separation\ncompared to previous work. We argue that denoising can become an integral part\nof high quality inverse rendering pipelines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hasselgren_J/0/1/0/all/0/1\">Jon Hasselgren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hofmann_N/0/1/0/all/0/1\">Nikolai Hofmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Munkberg_J/0/1/0/all/0/1\">Jacob Munkberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tutel: Adaptive Mixture-of-Experts at Scale. (arXiv:2206.03382v1 [cs.DC])","link":"http://arxiv.org/abs/2206.03382","description":"<p>In recent years, Mixture-of-Experts (MoE) has emerged as a promising\ntechnique for deep learning that can scale the model capacity to trillion-plus\nparameters while reducing the computing cost via sparse computation. While MoE\nopens a new frontier of exceedingly large models, its implementation over\nthousands of GPUs has been limited due to mismatch between the dynamic nature\nof MoE and static parallelism/pipelining of the system. We present Tutel, a\nhighly scalable stack design and implementation for MoE with dynamically\nadaptive parallelism and pipelining. Tutel delivers adaptive parallelism\nswitching and adaptive pipelining at runtime, which achieves up to 1.74x and\n2.00x single MoE layer speedup, respectively. We also propose a novel\ntwo-dimensional hierarchical algorithm for MoE communication speedup that\noutperforms the previous state-of-the-art up to 20.7x over 2,048 GPUs.\nAggregating all techniques, Tutel finally delivers 4.96x and 5.75x speedup of a\nsingle MoE layer on 16 GPUs and 2,048 GPUs, respectively, over Fairseq: Meta's\nFacebook AI Research Sequence-to-Sequence Toolkit (Tutel is now partially\nadopted by Fairseq). Tutel source code is available in public:\nhttps://github.com/microsoft/tutel . Our evaluation shows that Tutel\nefficiently and effectively runs a real-world MoE-based model named SwinV2-MoE,\nbuilt upon Swin Transformer V2, a state-of-the-art computer vision\narchitecture. On efficiency, Tutel accelerates SwinV2-MoE, achieving up to\n1.55x and 2.11x speedup in training and inference over Fairseq, respectively.\nOn effectiveness, the SwinV2-MoE model achieves superior accuracy in both\npre-training and down-stream computer vision tasks such as COCO object\ndetection than the counterpart dense model, indicating the readiness of Tutel\nfor end-to-end real-world model training and inference. SwinV2-MoE is open\nsourced in https://github.com/microsoft/Swin-Transformer .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hwang_C/0/1/0/all/0/1\">Changho Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_W/0/1/0/all/0/1\">Wei Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1\">Yifan Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Ziyue Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ze Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Han Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zilong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salas_R/0/1/0/all/0/1\">Rafael Salas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jose_J/0/1/0/all/0/1\">Jithin Jose</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ram_P/0/1/0/all/0/1\">Prabhat Ram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chau_J/0/1/0/all/0/1\">Joe Chau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_P/0/1/0/all/0/1\">Peng Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Mao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1\">Yongqiang Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards a General Purpose CNN for Long Range Dependencies in $\\mathrm{N}$D. (arXiv:2206.03398v1 [cs.LG])","link":"http://arxiv.org/abs/2206.03398","description":"<p>The use of Convolutional Neural Networks (CNNs) is widespread in Deep\nLearning due to a range of desirable model properties which result in an\nefficient and effective machine learning framework. However, performant CNN\narchitectures must be tailored to specific tasks in order to incorporate\nconsiderations such as the input length, resolution, and dimentionality. In\nthis work, we overcome the need for problem-specific CNN architectures with our\nContinuous Convolutional Neural Network (CCNN): a single CNN architecture\nequipped with continuous convolutional kernels that can be used for tasks on\ndata of arbitrary resolution, dimensionality and length without structural\nchanges. Continuous convolutional kernels model long range dependencies at\nevery layer, and remove the need for downsampling layers and task-dependent\ndepths needed in current CNN architectures. We show the generality of our\napproach by applying the same CCNN to a wide set of tasks on sequential\n(1$\\mathrm{D}$) and visual data (2$\\mathrm{D}$). Our CCNN performs\ncompetitively and often outperforms the current state-of-the-art across all\ntasks considered.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Romero_D/0/1/0/all/0/1\">David W. Romero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knigge_D/0/1/0/all/0/1\">David M. Knigge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_A/0/1/0/all/0/1\">Albert Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bekkers_E/0/1/0/all/0/1\">Erik J. Bekkers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gavves_E/0/1/0/all/0/1\">Efstratios Gavves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tomczak_J/0/1/0/all/0/1\">Jakub M. Tomczak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoogendoorn_M/0/1/0/all/0/1\">Mark Hoogendoorn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast and Robust Non-Rigid Registration Using Accelerated Majorization-Minimization. (arXiv:2206.03410v1 [cs.CV])","link":"http://arxiv.org/abs/2206.03410","description":"<p>Non-rigid registration, which deforms a source shape in a non-rigid way to\nalign with a target shape, is a classical problem in computer vision. Such\nproblems can be challenging because of imperfect data (noise, outliers and\npartial overlap) and high degrees of freedom. Existing methods typically adopt\nthe $\\ell_{p}$ type robust norm to measure the alignment error and regularize\nthe smoothness of deformation, and use a proximal algorithm to solve the\nresulting non-smooth optimization problem. However, the slow convergence of\nsuch algorithms limits their wide applications. In this paper, we propose a\nformulation for robust non-rigid registration based on a globally smooth robust\nnorm for alignment and regularization, which can effectively handle outliers\nand partial overlaps. The problem is solved using the majorization-minimization\nalgorithm, which reduces each iteration to a convex quadratic problem with a\nclosed-form solution. We further apply Anderson acceleration to speed up the\nconvergence of the solver, enabling the solver to run efficiently on devices\nwith limited compute capability. Extensive experiments demonstrate the\neffectiveness of our method for non-rigid alignment between two shapes with\noutliers and partial overlaps, with quantitative evaluation showing that it\noutperforms state-of-the-art methods in terms of registration accuracy and\ncomputational speed. The source code is available at\nhttps://github.com/yaoyx689/AMM_NRR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yuxin Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_B/0/1/0/all/0/1\">Bailin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Weiwei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Juyong Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring the combination of deep-learning based direct segmentation and deformable image registration for cone-beam CT based auto-segmentation for adaptive radiotherapy. (arXiv:2206.03413v1 [physics.med-ph])","link":"http://arxiv.org/abs/2206.03413","description":"<p>CBCT-based online adaptive radiotherapy (ART) calls for accurate\nauto-segmentation models to reduce the time cost for physicians to edit\ncontours, since the patient is immobilized on the treatment table waiting for\ntreatment to start. However, auto-segmentation of CBCT images is a difficult\ntask, majorly due to low image quality and lack of true labels for training a\ndeep learning (DL) model. Meanwhile CBCT auto-segmentation in ART is a unique\ntask compared to other segmentation problems, where manual contours on planning\nCT (pCT) are available. To make use of this prior knowledge, we propose to\ncombine deformable image registration (DIR) and direct segmentation (DS) on\nCBCT for head and neck patients. First, we use deformed pCT contours derived\nfrom multiple DIR methods between pCT and CBCT as pseudo labels for training.\nSecond, we use deformed pCT contours as bounding box to constrain the region of\ninterest for DS. Meanwhile deformed pCT contours are used as pseudo labels for\ntraining, but are generated from different DIR algorithms from bounding box.\nThird, we fine-tune the model with bounding box on true labels. We found that\nDS on CBCT trained with pseudo labels and without utilizing any prior knowledge\nhas very poor segmentation performance compared to DIR-only segmentation.\nHowever, adding deformed pCT contours as bounding box in the DS network can\ndramatically improve segmentation performance, comparable to DIR-only\nsegmentation. The DS model with bounding box can be further improved by\nfine-tuning it with some real labels. Experiments showed that 7 out of 19\nstructures have at least 0.2 dice similarity coefficient increase compared to\nDIR-only segmentation. Utilizing deformed pCT contours as pseudo labels for\ntraining and as bounding box for shape and location feature extraction in a DS\nmodel is a good way to combine DIR and DS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Liang_X/0/1/0/all/0/1\">Xiao Liang</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Morgan_H/0/1/0/all/0/1\">Howard Morgan</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Bai_T/0/1/0/all/0/1\">Ti Bai</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Dohopolski_M/0/1/0/all/0/1\">Michael Dohopolski</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Nguyen_D/0/1/0/all/0/1\">Dan Nguyen</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Jiang_S/0/1/0/all/0/1\">Steve Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revealing Single Frame Bias for Video-and-Language Learning. (arXiv:2206.03428v1 [cs.CV])","link":"http://arxiv.org/abs/2206.03428","description":"<p>Training an effective video-and-language model intuitively requires multiple\nframes as model inputs. However, it is unclear whether using multiple frames is\nbeneficial to downstream tasks, and if yes, whether the performance gain is\nworth the drastically-increased computation and memory costs resulting from\nusing more frames. In this work, we explore single-frame models for\nvideo-and-language learning. On a diverse set of video-and-language tasks\n(including text-to-video retrieval and video question answering), we show the\nsurprising result that, with large-scale pre-training and a proper frame\nensemble strategy at inference time, a single-frame trained model that does not\nconsider temporal information can achieve better performance than existing\nmethods that use multiple frames for training. This result reveals the\nexistence of a strong \"static appearance bias\" in popular video-and-language\ndatasets. Therefore, to allow for a more comprehensive evaluation of\nvideo-and-language models, we propose two new retrieval tasks based on existing\nfine-grained action recognition datasets that encourage temporal modeling. Our\ncode is available at https://github.com/jayleicn/singularity\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1\">Jie Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berg_T/0/1/0/all/0/1\">Tamara L. Berg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating Long Videos of Dynamic Scenes. (arXiv:2206.03429v1 [cs.CV])","link":"http://arxiv.org/abs/2206.03429","description":"<p>We present a video generation model that accurately reproduces object motion,\nchanges in camera viewpoint, and new content that arises over time. Existing\nvideo generation methods often fail to produce new content as a function of\ntime while maintaining consistencies expected in real environments, such as\nplausible dynamics and object persistence. A common failure case is for content\nto never change due to over-reliance on inductive biases to provide temporal\nconsistency, such as a single latent code that dictates content for the entire\nvideo. On the other extreme, without long-term consistency, generated videos\nmay morph unrealistically between different scenes. To address these\nlimitations, we prioritize the time axis by redesigning the temporal latent\nrepresentation and learning long-term consistency from data by training on\nlonger videos. To this end, we leverage a two-phase training strategy, where we\nseparately train using longer videos at a low resolution and shorter videos at\na high resolution. To evaluate the capabilities of our model, we introduce two\nnew benchmark datasets with explicit focus on long-term temporal dynamics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Brooks_T/0/1/0/all/0/1\">Tim Brooks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hellsten_J/0/1/0/all/0/1\">Janne Hellsten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aittala_M/0/1/0/all/0/1\">Miika Aittala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Ting-Chun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aila_T/0/1/0/all/0/1\">Timo Aila</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lehtinen_J/0/1/0/all/0/1\">Jaakko Lehtinen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Ming-Yu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Efros_A/0/1/0/all/0/1\">Alexei A. Efros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karras_T/0/1/0/all/0/1\">Tero Karras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robot Self-Calibration Using Actuated 3D Sensors. (arXiv:2206.03430v1 [cs.RO])","link":"http://arxiv.org/abs/2206.03430","description":"<p>Both, robot and hand-eye calibration haven been object to research for\ndecades. While current approaches manage to precisely and robustly identify the\nparameters of a robot's kinematic model, they still rely on external devices,\nsuch as calibration objects, markers and/or external sensors. Instead of trying\nto fit the recorded measurements to a model of a known object, this paper\ntreats robot calibration as an offline SLAM problem, where scanning poses are\nlinked to a fixed point in space by a moving kinematic chain. As such, the\npresented framework allows robot calibration using nothing but an arbitrary\neye-in-hand depth sensor, thus enabling fully autonomous self-calibration\nwithout any external tools. My new approach is utilizes a modified version of\nthe Iterative Closest Point algorithm to run bundle adjustment on multiple 3D\nrecordings estimating the optimal parameters of the kinematic model. A detailed\nevaluation of the system is shown on a real robot with various attached 3D\nsensors. The presented results show that the system reaches precision\ncomparable to a dedicated external tracking system at a fraction of its cost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peters_A/0/1/0/all/0/1\">Arne Peters</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised Domain Adaptation in Crowd Counting. (arXiv:2206.03431v1 [cs.CV])","link":"http://arxiv.org/abs/2206.03431","description":"<p>Self-training crowd counting has not been attentively explored though it is\none of the important challenges in computer vision. In practice, the fully\nsupervised methods usually require an intensive resource of manual annotation.\nIn order to address this challenge, this work introduces a new approach to\nutilize existing datasets with ground truth to produce more robust predictions\non unlabeled datasets, named domain adaptation, in crowd counting. While the\nnetwork is trained with labeled data, samples without labels from the target\ndomain are also added to the training process. In this process, the entropy map\nis computed and minimized in addition to the adversarial training process\ndesigned in parallel. Experiments on Shanghaitech, UCF_CC_50, and UCF-QNRF\ndatasets prove a more generalized improvement of our method over the other\nstate-of-the-arts in the cross-domain setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_P/0/1/0/all/0/1\">Pha Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Truong_T/0/1/0/all/0/1\">Thanh-Dat Truong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Miaoqing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yi Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_N/0/1/0/all/0/1\">Ngan Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luu_K/0/1/0/all/0/1\">Khoa Luu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can CNNs Be More Robust Than Transformers?. (arXiv:2206.03452v1 [cs.CV])","link":"http://arxiv.org/abs/2206.03452","description":"<p>The recent success of Vision Transformers is shaking the long dominance of\nConvolutional Neural Networks (CNNs) in image recognition for a decade.\nSpecifically, in terms of robustness on out-of-distribution samples, recent\nresearch finds that Transformers are inherently more robust than CNNs,\nregardless of different training setups. Moreover, it is believed that such\nsuperiority of Transformers should largely be credited to their\nself-attention-like architectures per se. In this paper, we question that\nbelief by closely examining the design of Transformers. Our findings lead to\nthree highly effective architecture designs for boosting robustness, yet simple\nenough to be implemented in several lines of code, namely a) patchifying input\nimages, b) enlarging kernel size, and c) reducing activation layers and\nnormalization layers. Bringing these components together, we are able to build\npure CNN architectures without any attention-like operations that is as robust\nas, or even more robust than, Transformers. We hope this work can help the\ncommunity better understand the design of robust neural architectures. The code\nis publicly available at https://github.com/UCSC-VLAA/RobustCNN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zeyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yutong Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuyin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1\">Cihang Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast Unsupervised Brain Anomaly Detection and Segmentation with Diffusion Models. (arXiv:2206.03461v1 [cs.CV])","link":"http://arxiv.org/abs/2206.03461","description":"<p>Deep generative models have emerged as promising tools for detecting\narbitrary anomalies in data, dispensing with the necessity for manual\nlabelling. Recently, autoregressive transformers have achieved state-of-the-art\nperformance for anomaly detection in medical imaging. Nonetheless, these models\nstill have some intrinsic weaknesses, such as requiring images to be modelled\nas 1D sequences, the accumulation of errors during the sampling process, and\nthe significant inference times associated with transformers. Denoising\ndiffusion probabilistic models are a class of non-autoregressive generative\nmodels recently shown to produce excellent samples in computer vision\n(surpassing Generative Adversarial Networks), and to achieve log-likelihoods\nthat are competitive with transformers while having fast inference times.\nDiffusion models can be applied to the latent representations learnt by\nautoencoders, making them easily scalable and great candidates for application\nto high dimensional data, such as medical images. Here, we propose a method\nbased on diffusion models to detect and segment anomalies in brain imaging. By\ntraining the models on healthy data and then exploring its diffusion and\nreverse steps across its Markov chain, we can identify anomalous areas in the\nlatent space and hence identify anomalies in the pixel space. Our diffusion\nmodels achieve competitive performance compared with autoregressive approaches\nacross a series of experiments with 2D CT and MRI data involving synthetic and\nreal pathological lesions with much reduced inference times, making their usage\nclinically viable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pinaya_W/0/1/0/all/0/1\">Walter H. L. Pinaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Graham_M/0/1/0/all/0/1\">Mark S. Graham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gray_R/0/1/0/all/0/1\">Robert Gray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Costa_P/0/1/0/all/0/1\">Pedro F Da Costa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tudosiu_P/0/1/0/all/0/1\">Petru-Daniel Tudosiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wright_P/0/1/0/all/0/1\">Paul Wright</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mah_Y/0/1/0/all/0/1\">Yee H. Mah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+MacKinnon_A/0/1/0/all/0/1\">Andrew D. MacKinnon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teo_J/0/1/0/all/0/1\">James T. Teo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jager_R/0/1/0/all/0/1\">Rolf Jager</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Werring_D/0/1/0/all/0/1\">David Werring</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rees_G/0/1/0/all/0/1\">Geraint Rees</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nachev_P/0/1/0/all/0/1\">Parashkev Nachev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ourselin_S/0/1/0/all/0/1\">Sebastien Ourselin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cardoso_M/0/1/0/all/0/1\">M. Jorge Cardoso</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SHRED: 3D Shape Region Decomposition with Learned Local Operations. (arXiv:2206.03480v1 [cs.CV])","link":"http://arxiv.org/abs/2206.03480","description":"<p>We present SHRED, a method for 3D SHape REgion Decomposition. SHRED takes a\n3D point cloud as input and uses learned local operations to produce a\nsegmentation that approximates fine-grained part instances. We endow SHRED with\nthree decomposition operations: splitting regions, fixing the boundaries\nbetween regions, and merging regions together. Modules are trained\nindependently and locally, allowing SHRED to generate high-quality\nsegmentations for categories not seen during training. We train and evaluate\nSHRED with fine-grained segmentations from PartNet; using its merge-threshold\nhyperparameter, we show that SHRED produces segmentations that better respect\nground-truth annotations compared with baseline methods, at any desired\ndecomposition granularity. Finally, we demonstrate that SHRED is useful for\ndownstream applications, out-performing all baselines on zero-shot fine-grained\npart instance segmentation and few-shot fine-grained semantic segmentation when\ncombined with methods that learn to label shape regions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jones_R/0/1/0/all/0/1\">R. Kenny Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Habib_A/0/1/0/all/0/1\">Aalia Habib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ritchie_D/0/1/0/all/0/1\">Daniel Ritchie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detection Hub: Unifying Object Detection Datasets via Query Adaptation on Language Embedding. (arXiv:2206.03484v1 [cs.CV])","link":"http://arxiv.org/abs/2206.03484","description":"<p>Leveraging large-scale data can introduce performance gains on many computer\nvision tasks. Unfortunately, this does not happen in object detection when\ntraining a single model under multiple datasets together. We observe two main\nobstacles: taxonomy difference and bounding box annotation inconsistency, which\nintroduces domain gaps in different datasets that prevents us from joint\ntraining. In this paper, we show that these two challenges can be effectively\naddressed by simply adapting object queries on language embedding of categories\nper dataset. We design a detection hub to dynamically adapt queries on category\nembedding based on the different distributions of datasets. Unlike previous\nmethods attempted to learn a joint embedding for all datasets, our adaptation\nmethod can utilize the language embedding as semantic centers for common\ncategories, while learning the semantic bias towards specific categories\nbelonging to different datasets to handle annotation differences and make up\nthe domain gaps. These novel improvements enable us to end-to-end train a\nsingle detector on multiple datasets simultaneously to fully take their\nadvantages. Further experiments on joint training on multiple datasets\ndemonstrate the significant performance gains over separate individual\nfine-tuned detectors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meng_L/0/1/0/all/0/1\">Lingchen Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1\">Xiyang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yinpeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pengchuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dongdong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Mengchen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianfeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zuxuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yu-Gang Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attending Category Disentangled Global Context for Image Classification. (arXiv:1812.06663v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1812.06663","description":"<p>In this paper, we propose a general framework for image classification using\nthe attention mechanism and global context, which could incorporate with\nvarious network architectures to improve their performance. To investigate the\ncapability of the global context, we compare four mathematical models and\nobserve the global context encoded in the category disentangled conditional\ngenerative model could give more guidance as \"know what is task irrelevant will\nalso know what is relevant\". Based on this observation, we define a novel\nCategory Disentangled Global Context (CDGC) and devise a deep network to obtain\nit. By attending CDGC, the baseline networks could identify the objects of\ninterest more accurately, thus improving the performance. We apply the\nframework to many different network architectures and compare with the\nstate-of-the-art on four publicly available datasets. Extensive results\nvalidate the effectiveness and superiority of our approach. Code will be made\npublic upon paper acceptance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_K/0/1/0/all/0/1\">Keke Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_G/0/1/0/all/0/1\">Guodong Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1\">Runnan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jie Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Z/0/1/0/all/0/1\">Zhaoquan Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenping Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stratified Rule-Aware Network for Abstract Visual Reasoning. (arXiv:2002.06838v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2002.06838","description":"<p>Abstract reasoning refers to the ability to analyze information, discover\nrules at an intangible level, and solve problems in innovative ways. Raven's\nProgressive Matrices (RPM) test is typically used to examine the capability of\nabstract reasoning. The subject is asked to identify the correct choice from\nthe answer set to fill the missing panel at the bottom right of RPM (e.g., a\n3$\\times$3 matrix), following the underlying rules inside the matrix. Recent\nstudies, taking advantage of Convolutional Neural Networks (CNNs), have\nachieved encouraging progress to accomplish the RPM test. However, they partly\nignore necessary inductive biases of RPM solver, such as order sensitivity\nwithin each row/column and incremental rule induction. To address this problem,\nin this paper we propose a Stratified Rule-Aware Network (SRAN) to generate the\nrule embeddings for two input sequences. Our SRAN learns multiple granularity\nrule embeddings at different levels, and incrementally integrates the\nstratified embedding flows through a gated fusion module. With the help of\nembeddings, a rule similarity metric is applied to guarantee that SRAN can not\nonly be trained using a tuplet loss but also infer the best answer efficiently.\nWe further point out the severe defects existing in the popular RAVEN dataset\nfor RPM test, which prevent from the fair evaluation of the abstract reasoning\nability. To fix the defects, we propose an answer set generation algorithm\ncalled Attribute Bisection Tree (ABT), forming an improved dataset named\nImpartial-RAVEN (I-RAVEN for short). Extensive experiments are conducted on\nboth PGM and I-RAVEN datasets, showing that our SRAN outperforms the\nstate-of-the-art models by a considerable margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Sheng Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yuqing Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xianglong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yanlu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1\">Shihao Bai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Segment Human Body Parts with Synthetically Trained Deep Convolutional Networks. (arXiv:2102.01460v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.01460","description":"<p>This paper presents a new framework for human body part segmentation based on\nDeep Convolutional Neural Networks trained using only synthetic data. The\nproposed approach achieves cutting-edge results without the need of training\nthe models with real annotated data of human body parts. Our contributions\ninclude a data generation pipeline, that exploits a game engine for the\ncreation of the synthetic data used for training the network, and a novel\npre-processing module, that combines edge response maps and adaptive histogram\nequalization to guide the network to learn the shape of the human body parts\nensuring robustness to changes in the illumination conditions. For selecting\nthe best candidate architecture, we perform exhaustive tests on manually\nannotated images of real human body limbs. We further compare our method\nagainst several high-end commercial segmentation tools on the body parts\nsegmentation task. The results show that our method outperforms the other\nmodels by a significant margin. Finally, we present an ablation study to\nvalidate our pre-processing module. With this paper, we release an\nimplementation of the proposed approach along with the acquired datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saviolo_A/0/1/0/all/0/1\">Alessandro Saviolo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bonotto_M/0/1/0/all/0/1\">Matteo Bonotto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Evangelista_D/0/1/0/all/0/1\">Daniele Evangelista</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Imperoli_M/0/1/0/all/0/1\">Marco Imperoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lazzaro_J/0/1/0/all/0/1\">Jacopo Lazzaro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menegatti_E/0/1/0/all/0/1\">Emanuele Menegatti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pretto_A/0/1/0/all/0/1\">Alberto Pretto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Look, Cast and Mold: Learning 3D Shape Manifold from Single-view Synthetic Data. (arXiv:2103.04789v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.04789","description":"<p>Inferring the stereo structure of objects in the real world is a challenging\nyet practical task. To equip deep models with this ability usually requires\nabundant 3D supervision which is hard to acquire. It is promising that we can\nsimply benefit from synthetic data, where pairwise ground-truth is easy to\naccess. Nevertheless, the domain gaps are nontrivial considering the variant\ntexture, shape and context. To overcome these difficulties, we propose a\nVisio-Perceptual Adaptive Network for single-view 3D reconstruction, dubbed\nVPAN. To generalize the model towards a real scenario, we propose to fulfill\nseveral aspects: (1) Look: visually incorporate spatial structure from the\nsingle view to enhance the expressiveness of representation; (2) Cast:\nperceptually align the 2D image features to the 3D shape priors with\ncross-modal semantic contrastive mapping; (3) Mold: reconstruct stereo-shape of\ntarget by transforming embeddings into the desired manifold. Extensive\nexperiments on several benchmarks demonstrate the effectiveness and robustness\nof the proposed method in learning the 3D shape manifold from synthetic data\nvia a single-view. The proposed method outperforms state-of-the-arts on Pix3D\ndataset with IoU 0.292 and CD 0.108, and reaches IoU 0.329 and CD 0.104 on\nPascal 3D+.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_Q/0/1/0/all/0/1\">Qianyu Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yawei Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_K/0/1/0/all/0/1\">Keyang Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deformable Capsules for Object Detection. (arXiv:2104.05031v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.05031","description":"<p>In this study, we introduce a new family of capsule networks, deformable\ncapsules (DeformCaps), to address a very important problem in computer vision:\nobject detection. We propose two new algorithms associated with our DeformCaps:\na novel capsule structure (SplitCaps), and a novel dynamic routing algorithm\n(SE-Routing), which balance computational efficiency with the need for modeling\na large number of objects and classes, which have never been achieved with\ncapsule networks before. We demonstrate that the proposed methods allow\ncapsules to efficiently scale-up to large-scale computer vision tasks for the\nfirst time, and create the first-ever capsule network for object detection in\nthe literature. Our proposed architecture is a one-stage detection framework\nand obtains results on MS COCO which are on-par with state-of-the-art one-stage\nCNN-based methods, while producing fewer false positive detections,\ngeneralizing to unusual poses/viewpoints of objects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lalonde_R/0/1/0/all/0/1\">Rodney Lalonde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khosravan_N/0/1/0/all/0/1\">Naji Khosravan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bagci_U/0/1/0/all/0/1\">Ulas Bagci</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Common Limitations of Image Processing Metrics: A Picture Story. (arXiv:2104.05642v5 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2104.05642","description":"<p>While the importance of automatic image analysis is continuously increasing,\nrecent meta-research revealed major flaws with respect to algorithm validation.\nPerformance metrics are particularly key for meaningful, objective, and\ntransparent performance assessment and validation of the used automatic\nalgorithms, but relatively little attention has been given to the practical\npitfalls when using specific metrics for a given image analysis task. These are\ntypically related to (1) the disregard of inherent metric properties, such as\nthe behaviour in the presence of class imbalance or small target structures,\n(2) the disregard of inherent data set properties, such as the non-independence\nof the test cases, and (3) the disregard of the actual biomedical domain\ninterest that the metrics should reflect. This living dynamically document has\nthe purpose to illustrate important limitations of performance metrics commonly\napplied in the field of image analysis. In this context, it focuses on\nbiomedical image analysis problems that can be phrased as image-level\nclassification, semantic segmentation, instance segmentation, or object\ndetection task. The current version is based on a Delphi process on metrics\nconducted by an international consortium of image analysis experts from more\nthan 60 institutions worldwide.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Reinke_A/0/1/0/all/0/1\">Annika Reinke</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tizabi_M/0/1/0/all/0/1\">Minu D. Tizabi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sudre_C/0/1/0/all/0/1\">Carole H. Sudre</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Eisenmann_M/0/1/0/all/0/1\">Matthias Eisenmann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Radsch_T/0/1/0/all/0/1\">Tim R&#xe4;dsch</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Baumgartner_M/0/1/0/all/0/1\">Michael Baumgartner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Acion_L/0/1/0/all/0/1\">Laura Acion</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Antonelli_M/0/1/0/all/0/1\">Michela Antonelli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arbel_T/0/1/0/all/0/1\">Tal Arbel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bakas_S/0/1/0/all/0/1\">Spyridon Bakas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bankhead_P/0/1/0/all/0/1\">Peter Bankhead</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Benis_A/0/1/0/all/0/1\">Arriel Benis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cardoso_M/0/1/0/all/0/1\">M. Jorge Cardoso</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheplygina_V/0/1/0/all/0/1\">Veronika Cheplygina</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Christodoulou_E/0/1/0/all/0/1\">Evangelia Christodoulou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cimini_B/0/1/0/all/0/1\">Beth Cimini</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Collins_G/0/1/0/all/0/1\">Gary S. Collins</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Farahani_K/0/1/0/all/0/1\">Keyvan Farahani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ginneken_B/0/1/0/all/0/1\">Bram van Ginneken</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Glocker_B/0/1/0/all/0/1\">Ben Glocker</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Godau_P/0/1/0/all/0/1\">Patrick Godau</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hamprecht_F/0/1/0/all/0/1\">Fred Hamprecht</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hashimoto_D/0/1/0/all/0/1\">Daniel A. Hashimoto</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Heckmann_Notzel_D/0/1/0/all/0/1\">Doreen Heckmann-N&#xf6;tzel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hoffmann_M/0/1/0/all/0/1\">Michael M. Hoffmann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huisman_M/0/1/0/all/0/1\">Merel Huisman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Isensee_F/0/1/0/all/0/1\">Fabian Isensee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jannin_P/0/1/0/all/0/1\">Pierre Jannin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kahn_C/0/1/0/all/0/1\">Charles E. Kahn</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Karargyris_A/0/1/0/all/0/1\">Alexandros Karargyris</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Karthikesalingam_A/0/1/0/all/0/1\">Alan Karthikesalingam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kainz_B/0/1/0/all/0/1\">Bernhard Kainz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kavur_E/0/1/0/all/0/1\">Emre Kavur</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kenngott_H/0/1/0/all/0/1\">Hannes Kenngott</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kleesiek_J/0/1/0/all/0/1\">Jens Kleesiek</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kooi_T/0/1/0/all/0/1\">Thijs Kooi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kozubek_M/0/1/0/all/0/1\">Michal Kozubek</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kreshuk_A/0/1/0/all/0/1\">Anna Kreshuk</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kurc_T/0/1/0/all/0/1\">Tahsin Kurc</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Landman_B/0/1/0/all/0/1\">Bennett A. Landman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Litjens_G/0/1/0/all/0/1\">Geert Litjens</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Madani_A/0/1/0/all/0/1\">Amin Madani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maier_Hein_K/0/1/0/all/0/1\">Klaus Maier-Hein</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Martel_A/0/1/0/all/0/1\">Anne L. Martel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mattson_P/0/1/0/all/0/1\">Peter Mattson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meijering_E/0/1/0/all/0/1\">Erik Meijering</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Menze_B/0/1/0/all/0/1\">Bjoern Menze</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Moher_D/0/1/0/all/0/1\">David Moher</a>, et al. (28 additional authors not shown)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Consistency Regularization for Variational Auto-Encoders. (arXiv:2105.14859v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2105.14859","description":"<p>Variational auto-encoders (VAEs) are a powerful approach to unsupervised\nlearning. They enable scalable approximate posterior inference in\nlatent-variable models using variational inference (VI). A VAE posits a\nvariational family parameterized by a deep neural network called an encoder\nthat takes data as input. This encoder is shared across all the observations,\nwhich amortizes the cost of inference. However the encoder of a VAE has the\nundesirable property that it maps a given observation and a\nsemantics-preserving transformation of it to different latent representations.\nThis \"inconsistency\" of the encoder lowers the quality of the learned\nrepresentations, especially for downstream tasks, and also negatively affects\ngeneralization. In this paper, we propose a regularization method to enforce\nconsistency in VAEs. The idea is to minimize the Kullback-Leibler (KL)\ndivergence between the variational distribution when conditioning on the\nobservation and the variational distribution when conditioning on a random\nsemantic-preserving transformation of this observation. This regularization is\napplicable to any VAE. In our experiments we apply it to four different VAE\nvariants on several benchmark datasets and found it always improves the quality\nof the learned representations but also leads to better generalization. In\nparticular, when applied to the Nouveau Variational Auto-Encoder (NVAE), our\nregularization method yields state-of-the-art performance on MNIST and\nCIFAR-10. We also applied our method to 3D data and found it learns\nrepresentations of superior quality as measured by accuracy on a downstream\nclassification task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sinha_S/0/1/0/all/0/1\">Samarth Sinha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dieng_A/0/1/0/all/0/1\">Adji B. Dieng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tikhonov Regularization of Circle-Valued Signals. (arXiv:2108.02602v3 [math.OC] UPDATED)","link":"http://arxiv.org/abs/2108.02602","description":"<p>It is common to have to process signals or images whose values are cyclic and\ncan be represented as points on the complex circle, like wrapped phases,\nangles, orientations, or color hues. We consider a Tikhonov-type regularization\nmodel to smoothen or interpolate circle-valued signals defined on arbitrary\ngraphs. We propose a convex relaxation of this nonconvex problem as a\nsemidefinite program, and an efficient algorithm to solve it.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/math/1/au:+Condat_L/0/1/0/all/0/1\">Laurent Condat</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepMTS: Deep Multi-task Learning for Survival Prediction in Patients with Advanced Nasopharyngeal Carcinoma using Pretreatment PET/CT. (arXiv:2109.07711v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2109.07711","description":"<p>Nasopharyngeal Carcinoma (NPC) is a malignant epithelial cancer arising from\nthe nasopharynx. Survival prediction is a major concern for NPC patients, as it\nprovides early prognostic information to plan treatments. Recently, deep\nsurvival models based on deep learning have demonstrated the potential to\noutperform traditional radiomics-based survival prediction models. Deep\nsurvival models usually use image patches covering the whole target regions\n(e.g., nasopharynx for NPC) or containing only segmented tumor regions as the\ninput. However, the models using the whole target regions will also include\nnon-relevant background information, while the models using segmented tumor\nregions will disregard potentially prognostic information existing out of\nprimary tumors (e.g., local lymph node metastasis and adjacent tissue\ninvasion). In this study, we propose a 3D end-to-end Deep Multi-Task Survival\nmodel (DeepMTS) for joint survival prediction and tumor segmentation in\nadvanced NPC from pretreatment PET/CT. Our novelty is the introduction of a\nhard-sharing segmentation backbone to guide the extraction of local features\nrelated to the primary tumors, which reduces the interference from non-relevant\nbackground information. In addition, we also introduce a cascaded survival\nnetwork to capture the prognostic information existing out of primary tumors\nand further leverage the global tumor information (e.g., tumor size, shape, and\nlocations) derived from the segmentation backbone. Our experiments with two\nclinical datasets demonstrate that our DeepMTS can consistently outperform\ntraditional radiomics-based survival prediction models and existing deep\nsurvival models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Meng_M/0/1/0/all/0/1\">Mingyuan Meng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gu_B/0/1/0/all/0/1\">Bingxin Gu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bi_L/0/1/0/all/0/1\">Lei Bi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Song_S/0/1/0/all/0/1\">Shaoli Song</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Feng_D/0/1/0/all/0/1\">David Dagan Feng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_J/0/1/0/all/0/1\">Jinman Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learn to Ignore: Domain Adaptation for Multi-Site MRI Analysis. (arXiv:2110.06803v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.06803","description":"<p>The limited availability of large image datasets, mainly due to data privacy\nand differences in acquisition protocols or hardware, is a significant issue in\nthe development of accurate and generalizable machine learning methods in\nmedicine. This is especially the case for Magnetic Resonance (MR) images, where\ndifferent MR scanners introduce a bias that limits the performance of a machine\nlearning model. We present a novel method that learns to ignore the\nscanner-related features present in MR images, by introducing specific\nadditional constraints on the latent space. We focus on a real-world\nclassification scenario, where only a small dataset provides images of all\nclasses. Our method \\textit{Learn to Ignore (L2I)} outperforms state-of-the-art\ndomain adaptation methods on a multi-site MR dataset for a classification task\nbetween multiple sclerosis patients and healthy controls.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wolleb_J/0/1/0/all/0/1\">Julia Wolleb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sandkuhler_R/0/1/0/all/0/1\">Robin Sandk&#xfc;hler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bieder_F/0/1/0/all/0/1\">Florentin Bieder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barakovic_M/0/1/0/all/0/1\">Muhamed Barakovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hadjikhani_N/0/1/0/all/0/1\">Nouchine Hadjikhani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papadopoulou_A/0/1/0/all/0/1\">Athina Papadopoulou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yaldizli_O/0/1/0/all/0/1\">&#xd6;zg&#xfc;r Yaldizli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuhle_J/0/1/0/all/0/1\">Jens Kuhle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Granziera_C/0/1/0/all/0/1\">Cristina Granziera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cattin_P/0/1/0/all/0/1\">Philippe C. Cattin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncertainty-Guided Lung Nodule Segmentation with Feature-Aware Attention. (arXiv:2110.12372v4 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2110.12372","description":"<p>Since radiologists have different training and clinical experiences, they may\nprovide various segmentation annotations for a lung nodule. Conventional\nstudies choose a single annotation as the learning target by default, but they\nwaste valuable information of consensus or disagreements ingrained in the\nmultiple annotations. This paper proposes an Uncertainty-Guided Segmentation\nNetwork (UGS-Net), which learns the rich visual features from the regions that\nmay cause segmentation uncertainty and contributes to a better segmentation\nresult. With an Uncertainty-Aware Module, this network can provide a\nMulti-Confidence Mask (MCM), pointing out regions with different segmentation\nuncertainty levels. Moreover, this paper introduces a Feature-Aware Attention\nModule to enhance the learning of the nodule boundary and density differences.\nExperimental results show that our method can predict the nodule regions with\ndifferent uncertainty levels and achieve superior performance in LIDC-IDRI\ndataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yang_H/0/1/0/all/0/1\">Han Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shen_L/0/1/0/all/0/1\">Lu Shen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_M/0/1/0/all/0/1\">Mengke Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Q/0/1/0/all/0/1\">Qiuli Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated skin lesion segmentation using multi-scale feature extraction scheme and dual-attention mechanism. (arXiv:2111.08708v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2111.08708","description":"<p>Segmenting skin lesions from dermoscopic images is essential for diagnosing\nskin cancer. But the automatic segmentation of these lesions is complicated due\nto the poor contrast between the background and the lesion, image artifacts,\nand unclear lesion boundaries. In this work, we present a deep learning model\nfor the segmentation of skin lesions from dermoscopic images. To deal with the\nchallenges of skin lesion characteristics, we designed a multi-scale feature\nextraction module for extracting the discriminative features. Further in this\nwork, two attention mechanisms are developed to refine the post-upsampled\nfeatures and the features extracted by the encoder. This model is evaluated\nusing the ISIC2018 and ISBI2017 datasets. The proposed model outperformed all\nthe existing works and the top-ranked models in two competitions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chowdary_G/0/1/0/all/0/1\">G Jignesh Chowdary</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yathisha_G/0/1/0/all/0/1\">G V S N Durga Yathisha</a>, <a href=\"http://arxiv.org/find/eess/1/au:+G_S/0/1/0/all/0/1\">Suganya G</a>, <a href=\"http://arxiv.org/find/eess/1/au:+M_P/0/1/0/all/0/1\">Premalatha M</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical Graph-Convolutional Variational AutoEncoding for Generative Modelling of Human Motion. (arXiv:2111.12602v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.12602","description":"<p>Models of human motion commonly focus either on trajectory prediction or\naction classification but rarely both. The marked heterogeneity and intricate\ncompositionality of human motion render each task vulnerable to the data\ndegradation and distributional shift common to real-world scenarios. A\nsufficiently expressive generative model of action could in theory enable data\nconditioning and distributional resilience within a unified framework\napplicable to both tasks. Here we propose a novel architecture based on\nhierarchical variational autoencoders and deep graph convolutional neural\nnetworks for generating a holistic model of action over multiple time-scales.\nWe show this Hierarchical Graph-convolutional Variational Autoencoder (HG-VAE)\nto be capable of generating coherent actions, detecting out-of-distribution\ndata, and imputing missing data by gradient ascent on the model's posterior.\nTrained and evaluated on H3.6M and the largest collection of open source human\nmotion data, AMASS, we show HG-VAE can facilitate downstream discriminative\nlearning better than baseline models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bourached_A/0/1/0/all/0/1\">Anthony Bourached</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gray_R/0/1/0/all/0/1\">Robert Gray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_X/0/1/0/all/0/1\">Xiaodong Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Griffiths_R/0/1/0/all/0/1\">Ryan-Rhys Griffiths</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jha_A/0/1/0/all/0/1\">Ashwani Jha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nachev_P/0/1/0/all/0/1\">Parashkev Nachev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MAE-DET: Revisiting Maximum Entropy Principle in Zero-Shot NAS for Efficient Object Detection. (arXiv:2111.13336v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.13336","description":"<p>In object detection, the detection backbone consumes more than half of the\noverall inference cost. Recent researches attempt to reduce this cost by\noptimizing the backbone architecture with the help of Neural Architecture\nSearch (NAS). However, existing NAS methods for object detection require\nhundreds to thousands of GPU hours of searching, making them impractical in\nfast-paced research and development. In this work, we propose a novel zero-shot\nNAS method to address this issue. The proposed method, named MAE-DET,\nautomatically designs efficient detection backbones via the Maximum Entropy\nPrinciple without training network parameters, reducing the architecture design\ncost to nearly zero yet delivering the state-of-the-art (SOTA) performance.\nUnder the hood, MAE-DET maximizes the differential entropy of detection\nbackbones, leading to a better feature extractor for object detection under the\nsame computational budgets. After merely one GPU day of fully automatic design,\nMAE-DET innovates SOTA detection backbones on multiple detection benchmark\ndatasets with little human intervention. Comparing to ResNet-50 backbone,\nMAE-DET is $+2.0\\%$ better in mAP when using the same amount of\nFLOPs/parameters, and is $1.54$ times faster on NVIDIA V100 at the same mAP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zhenhong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Ming Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiuyu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1\">Zhiyu Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1\">Rong Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Point Light Fields. (arXiv:2112.01473v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.01473","description":"<p>We introduce Neural Point Light Fields that represent scenes implicitly with\na light field living on a sparse point cloud. Combining differentiable volume\nrendering with learned implicit density representations has made it possible to\nsynthesize photo-realistic images for novel views of small scenes. As neural\nvolumetric rendering methods require dense sampling of the underlying\nfunctional scene representation, at hundreds of samples along a ray cast\nthrough the volume, they are fundamentally limited to small scenes with the\nsame objects projected to hundreds of training views. Promoting sparse point\nclouds to neural implicit light fields allows us to represent large scenes\neffectively with only a single radiance evaluation per ray. These point light\nfields are a function of the ray direction, and local point feature\nneighborhood, allowing us to interpolate the light field conditioned training\nimages without dense object coverage and parallax. We assess the proposed\nmethod for novel view synthesis on large driving scenarios, where we synthesize\nrealistic unseen views that existing implicit approaches fail to represent. We\nvalidate that Neural Point Light Fields make it possible to predict videos\nalong unseen trajectories previously only feasible to generate by explicitly\nmodeling the scene.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ost_J/0/1/0/all/0/1\">Julian Ost</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laradji_I/0/1/0/all/0/1\">Issam Laradji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Newell_A/0/1/0/all/0/1\">Alejandro Newell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahat_Y/0/1/0/all/0/1\">Yuval Bahat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heide_F/0/1/0/all/0/1\">Felix Heide</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hybrid Instance-aware Temporal Fusion for Online Video Instance Segmentation. (arXiv:2112.01695v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.01695","description":"<p>Recently, transformer-based image segmentation methods have achieved notable\nsuccess against previous solutions. While for video domains, how to effectively\nmodel temporal context with the attention of object instances across frames\nremains an open problem. In this paper, we propose an online video instance\nsegmentation framework with a novel instance-aware temporal fusion method. We\nfirst leverages the representation, i.e., a latent code in the global context\n(instance code) and CNN feature maps to represent instance- and pixel-level\nfeatures. Based on this representation, we introduce a cropping-free temporal\nfusion approach to model the temporal consistency between video frames.\nSpecifically, we encode global instance-specific information in the instance\ncode and build up inter-frame contextual fusion with hybrid attentions between\nthe instance codes and CNN feature maps. Inter-frame consistency between the\ninstance codes are further enforced with order constraints. By leveraging the\nlearned hybrid temporal consistency, we are able to directly retrieve and\nmaintain instance identities across frames, eliminating the complicated\nframe-wise instance matching in prior methods. Extensive experiments have been\nconducted on popular VIS datasets, i.e. Youtube-VIS-19/21. Our model achieves\nthe best performance among all online VIS methods. Notably, our model also\neclipses all offline methods when using the ResNet-50 backbone.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jinglu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yan Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GradMax: Growing Neural Networks using Gradient Information. (arXiv:2201.05125v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2201.05125","description":"<p>The architecture and the parameters of neural networks are often optimized\nindependently, which requires costly retraining of the parameters whenever the\narchitecture is modified. In this work we instead focus on growing the\narchitecture without requiring costly retraining. We present a method that adds\nnew neurons during training without impacting what is already learned, while\nimproving the training dynamics. We achieve the latter by maximizing the\ngradients of the new weights and find the optimal initialization efficiently by\nmeans of the singular value decomposition (SVD). We call this technique\nGradient Maximizing Growth (GradMax) and demonstrate its effectiveness in\nvariety of vision tasks and architectures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Evci_U/0/1/0/all/0/1\">Utku Evci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merrienboer_B/0/1/0/all/0/1\">Bart van Merri&#xeb;nboer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Unterthiner_T/0/1/0/all/0/1\">Thomas Unterthiner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vladymyrov_M/0/1/0/all/0/1\">Max Vladymyrov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pedregosa_F/0/1/0/all/0/1\">Fabian Pedregosa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vertical Federated Edge Learning with Distributed Integrated Sensing and Communication. (arXiv:2201.08512v2 [eess.SP] UPDATED)","link":"http://arxiv.org/abs/2201.08512","description":"<p>This letter studies a vertical federated edge learning (FEEL) system for\ncollaborative objects/human motion recognition by exploiting the distributed\nintegrated sensing and communication (ISAC). In this system, distributed edge\ndevices first send wireless signals to sense targeted objects/human, and then\nexchange intermediate computed vectors (instead of raw sensing data) for\ncollaborative recognition while preserving data privacy. To boost the spectrum\nand hardware utilization efficiency for FEEL, we exploit ISAC for both target\nsensing and data exchange, by employing dedicated frequency-modulated\ncontinuous-wave (FMCW) signals at each edge device. Under this setup, we\npropose a vertical FEEL framework for realizing the recognition based on the\ncollected multi-view wireless sensing data. In this framework, each edge device\nowns an individual local L-model to transform its sensing data into an\nintermediate vector with relatively low dimensions, which is then transmitted\nto a coordinating edge device for final output via a common downstream S-model.\nBy considering a human motion recognition task, experimental results show that\nour vertical FEEL based approach achieves recognition accuracy up to 98\\% with\nan improvement up to 8\\% compared to the benchmarks, including on-device\ntraining and horizontal FEEL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Liu_P/0/1/0/all/0/1\">Peixi Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_G/0/1/0/all/0/1\">Guangxu Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jiang_W/0/1/0/all/0/1\">Wei Jiang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Luo_W/0/1/0/all/0/1\">Wu Luo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_J/0/1/0/all/0/1\">Jie Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cui_S/0/1/0/all/0/1\">Shuguang Cui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Plug & Play Attacks: Towards Robust and Flexible Model Inversion Attacks. (arXiv:2201.12179v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2201.12179","description":"<p>Model inversion attacks (MIAs) aim to create synthetic images that reflect\nthe class-wise characteristics from a target classifier's private training data\nby exploiting the model's learned knowledge. Previous research has developed\ngenerative MIAs that use generative adversarial networks (GANs) as image priors\ntailored to a specific target model. This makes the attacks time- and\nresource-consuming, inflexible, and susceptible to distributional shifts\nbetween datasets. To overcome these drawbacks, we present Plug &amp; Play Attacks,\nwhich relax the dependency between the target model and image prior, and enable\nthe use of a single GAN to attack a wide range of targets, requiring only minor\nadjustments to the attack. Moreover, we show that powerful MIAs are possible\neven with publicly available pre-trained GANs and under strong distributional\nshifts, for which previous approaches fail to produce meaningful results. Our\nextensive evaluation confirms the improved robustness and flexibility of Plug &amp;\nPlay Attacks and their ability to create high-quality images revealing\nsensitive class characteristics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Struppek_L/0/1/0/all/0/1\">Lukas Struppek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hintersdorf_D/0/1/0/all/0/1\">Dominik Hintersdorf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Correia_A/0/1/0/all/0/1\">Antonio De Almeida Correia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adler_A/0/1/0/all/0/1\">Antonia Adler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1\">Kristian Kersting</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Debiased Self-Training for Semi-Supervised Learning. (arXiv:2202.07136v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2202.07136","description":"<p>Deep neural networks achieve remarkable performances on a wide range of tasks\nwith the aid of large-scale labeled datasets. Yet these datasets are\ntime-consuming and labor-exhaustive to obtain on realistic tasks. To mitigate\nthe requirement for labeled data, self-training is widely used in\nsemi-supervised learning by iteratively assigning pseudo labels to unlabeled\nsamples. Despite its popularity, self-training is well-believed to be\nunreliable and often leads to training instability. Our experimental studies\nfurther reveal that the bias in semi-supervised learning arises from both the\nproblem itself and the inappropriate training with potentially incorrect pseudo\nlabels, which accumulates the error in the iterative self-training process. To\nreduce the above bias, we propose Debiased Self-Training (DST). First, the\ngeneration and utilization of pseudo labels are decoupled by two\nparameter-independent classifier heads to avoid direct error accumulation.\nSecond, we estimate the worst case of self-training bias, where the pseudo\nlabeling function is accurate on labeled samples, yet makes as many mistakes as\npossible on unlabeled samples. We then adversarially optimize the\nrepresentations to improve the quality of pseudo labels by avoiding the worst\ncase. Extensive experiments justify that DST achieves an average improvement of\n6.3% against state-of-the-art methods on standard semi-supervised learning\nbenchmark datasets and 18.9%$ against FixMatch on 13 diverse tasks.\nFurthermore, DST can be seamlessly adapted to other self-training methods and\nhelp stabilize their training and balance performance across classes in both\ncases of training from scratch and finetuning from pre-trained models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Baixu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Junguang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Ximei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_P/0/1/0/all/0/1\">Pengfei Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianmin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_M/0/1/0/all/0/1\">Mingsheng Long</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deconstructing Distributions: A Pointwise Framework of Learning. (arXiv:2202.09931v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2202.09931","description":"<p>In machine learning, we traditionally evaluate the performance of a single\nmodel, averaged over a collection of test inputs. In this work, we propose a\nnew approach: we measure the performance of a collection of models when\nevaluated on a $\\textit{single input point}$. Specifically, we study a point's\n$\\textit{profile}$: the relationship between models' average performance on the\ntest distribution and their pointwise performance on this individual point. We\nfind that profiles can yield new insights into the structure of both models and\ndata -- in and out-of-distribution. For example, we empirically show that real\ndata distributions consist of points with qualitatively different profiles. On\none hand, there are \"compatible\" points with strong correlation between the\npointwise and average performance. On the other hand, there are points with\nweak and even $\\textit{negative}$ correlation: cases where improving overall\nmodel accuracy actually $\\textit{hurts}$ performance on these inputs. We prove\nthat these experimental observations are inconsistent with the predictions of\nseveral simplified models of learning proposed in prior work. As an\napplication, we use profiles to construct a dataset we call CIFAR-10-NEG: a\nsubset of CINIC-10 such that for standard models, accuracy on CIFAR-10-NEG is\n$\\textit{negatively correlated}$ with accuracy on CIFAR-10 test. This\nillustrates, for the first time, an OOD dataset that completely inverts\n\"accuracy-on-the-line\" (Miller, Taori, Raghunathan, Sagawa, Koh, Shankar,\nLiang, Carmon, and Schmidt 2021)\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kaplun_G/0/1/0/all/0/1\">Gal Kaplun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_N/0/1/0/all/0/1\">Nikhil Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1\">Saurabh Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barak_B/0/1/0/all/0/1\">Boaz Barak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakkiran_P/0/1/0/all/0/1\">Preetum Nakkiran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Syntax-Aware Network for Handwritten Mathematical Expression Recognition. (arXiv:2203.01601v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.01601","description":"<p>Handwritten mathematical expression recognition (HMER) is a challenging task\nthat has many potential applications. Recent methods for HMER have achieved\noutstanding performance with an encoder-decoder architecture. However, these\nmethods adhere to the paradigm that the prediction is made \"from one character\nto another\", which inevitably yields prediction errors due to the complicated\nstructures of mathematical expressions or crabbed handwritings. In this paper,\nwe propose a simple and efficient method for HMER, which is the first to\nincorporate syntax information into an encoder-decoder network. Specifically,\nwe present a set of grammar rules for converting the LaTeX markup sequence of\neach expression into a parsing tree; then, we model the markup sequence\nprediction as a tree traverse process with a deep neural network. In this way,\nthe proposed method can effectively describe the syntax context of expressions,\nalleviating the structure prediction errors of HMER. Experiments on three\nbenchmark datasets demonstrate that our method achieves better recognition\nperformance than prior arts. To further validate the effectiveness of our\nmethod, we create a large-scale dataset consisting of 100k handwritten\nmathematical expression images acquired from ten thousand writers. The source\ncode, new dataset, and pre-trained models of this work will be publicly\navailable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Ye Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dikubab_W/0/1/0/all/0/1\">Wondimu Dikubab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Z/0/1/0/all/0/1\">Zhilong Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhongqin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xiang Bai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting Click-based Interactive Video Object Segmentation. (arXiv:2203.01784v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.01784","description":"<p>While current methods for interactive Video Object Segmentation (iVOS) rely\non scribble-based interactions to generate precise object masks, we propose a\nClick-based interactive Video Object Segmentation (CiVOS) framework to simplify\nthe required user workload as much as possible. CiVOS builds on de-coupled\nmodules reflecting user interaction and mask propagation. The interaction\nmodule converts click-based interactions into an object mask, which is then\ninferred to the remaining frames by the propagation module. Additional user\ninteractions allow for a refinement of the object mask. The approach is\nextensively evaluated on the popular interactive~DAVIS dataset, but with an\ninevitable adaptation of scribble-based interactions with click-based\ncounterparts. We consider several strategies for generating clicks during our\nevaluation to reflect various user inputs and adjust the DAVIS performance\nmetric to perform a hardware-independent comparison. The presented CiVOS\npipeline achieves competitive results, although requiring a lower user\nworkload.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vujasinovic_S/0/1/0/all/0/1\">Stephane Vujasinovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bullinger_S/0/1/0/all/0/1\">Sebastian Bullinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Becker_S/0/1/0/all/0/1\">Stefan Becker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scherer_Negenborn_N/0/1/0/all/0/1\">Norbert Scherer-Negenborn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arens_M/0/1/0/all/0/1\">Michael Arens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stiefelhagen_R/0/1/0/all/0/1\">Rainer Stiefelhagen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TransFusion: Multi-view Divergent Fusion for Medical Image Segmentation with Transformers. (arXiv:2203.10726v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2203.10726","description":"<p>Combining information from multi-view images is crucial to improve the\nperformance and robustness of automated methods for disease diagnosis. However,\ndue to the non-alignment characteristics of multi-view images, building\ncorrelation and data fusion across views largely remain an open problem. In\nthis study, we present TransFusion, a Transformer-based architecture to merge\ndivergent multi-view imaging information using convolutional layers and\npowerful attention mechanisms. In particular, the Divergent Fusion Attention\n(DiFA) module is proposed for rich cross-view context modeling and semantic\ndependency mining, addressing the critical issue of capturing long-range\ncorrelations between unaligned data from different image views. We further\npropose the Multi-Scale Attention (MSA) to collect global correspondence of\nmulti-scale feature representations. We evaluate TransFusion on the\nMulti-Disease, Multi-View \\&amp; Multi-Center Right Ventricular Segmentation in\nCardiac MRI (M\\&amp;Ms-2) challenge cohort. TransFusion demonstrates leading\nperformance against the state-of-the-art methods and opens up new perspectives\nfor multi-view imaging integration towards robust medical image segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Liu_D/0/1/0/all/0/1\">Di Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gao_Y/0/1/0/all/0/1\">Yunhe Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhangli_Q/0/1/0/all/0/1\">Qilong Zhangli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Han_L/0/1/0/all/0/1\">Ligong Han</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xia_Z/0/1/0/all/0/1\">Zhaoyang Xia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+He_X/0/1/0/all/0/1\">Xiaoxiao He</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wen_S/0/1/0/all/0/1\">Song Wen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yan_Z/0/1/0/all/0/1\">Zhennan Yan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_M/0/1/0/all/0/1\">Mu Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Metaxas_D/0/1/0/all/0/1\">Dimitris Metaxas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cell segmentation from telecentric bright-field transmitted light microscopy images using a Residual Attention U-Net: a case study on HeLa line. (arXiv:2203.12290v2 [q-bio.QM] UPDATED)","link":"http://arxiv.org/abs/2203.12290","description":"<p>Living cell segmentation from bright-field light microscopy images is\nchallenging due to the image complexity and temporal changes in the living\ncells. Recently developed deep learning (DL)-based methods became popular in\nmedical and microscopy image segmentation tasks due to their success and\npromising outcomes. The main objective of this paper is to develop a deep\nlearning, U-Net-based method to segment the living cells of the HeLa line in\nbright-field transmitted light microscopy. To find the most suitable\narchitecture for our datasets, a residual attention U-Net was proposed and\ncompared with an attention and a simple U-Net architecture.\n</p>\n<p>The attention mechanism highlights the remarkable features and suppresses\nactivations in the irrelevant image regions. The residual mechanism overcomes\nwith vanishing gradient problem. The Mean-IoU score for our datasets reaches\n0.9505, 0.9524, and 0.9530 for the simple, attention, and residual attention\nU-Net, respectively. The most accurate semantic segmentation results was\nachieved in the Mean-IoU and Dice metrics by applying the residual and\nattention mechanisms together. The watershed method applied to this best --\nResidual Attention -- semantic segmentation result gave the segmentation with\nthe specific information for each cell.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Ghaznavi_A/0/1/0/all/0/1\">Ali Ghaznavi</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Rychtarikova_R/0/1/0/all/0/1\">Renata Rychtarikova</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Saberioon_M/0/1/0/all/0/1\">Mohammadmehdi Saberioon</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Stys_D/0/1/0/all/0/1\">Dalibor Stys</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Eventor: An Efficient Event-Based Monocular Multi-View Stereo Accelerator on FPGA Platform. (arXiv:2203.15439v2 [cs.AR] UPDATED)","link":"http://arxiv.org/abs/2203.15439","description":"<p>Event cameras are bio-inspired vision sensors that asynchronously represent\npixel-level brightness changes as event streams. Event-based monocular\nmulti-view stereo (EMVS) is a technique that exploits the event streams to\nestimate semi-dense 3D structure with known trajectory. It is a critical task\nfor event-based monocular SLAM. However, the required intensive computation\nworkloads make it challenging for real-time deployment on embedded platforms.\nIn this paper, Eventor is proposed as a fast and efficient EMVS accelerator by\nrealizing the most critical and time-consuming stages including event\nback-projection and volumetric ray-counting on FPGA. Highly paralleled and\nfully pipelined processing elements are specially designed via FPGA and\nintegrated with the embedded ARM as a heterogeneous system to improve the\nthroughput and reduce the memory footprint. Meanwhile, the EMVS algorithm is\nreformulated to a more hardware-friendly manner by rescheduling, approximate\ncomputing and hybrid data quantization. Evaluation results on DAVIS dataset\nshow that Eventor achieves up to $24\\times$ improvement in energy efficiency\ncompared with Intel i5 CPU platform.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mingjun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianlei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1\">Yingjie Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_M/0/1/0/all/0/1\">Meng Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yuhao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Runze Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_W/0/1/0/all/0/1\">Weitao Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Bei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Weisheng Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Vehicle Detection in Satellite Video. (arXiv:2204.06828v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.06828","description":"<p>This work presents a deep learning approach for vehicle detection in\nsatellite video. Vehicle detection is perhaps impossible in single EO satellite\nimages due to the tininess of vehicles (4-10 pixel) and their similarity to the\nbackground. Instead, we consider satellite video which overcomes the lack of\nspatial information by temporal consistency of vehicle movement. A new\nspatiotemporal model of a compact $3 \\times 3$ convolutional, neural network is\nproposed which neglects pooling layers and uses leaky ReLUs. Then we use a\nreformulation of the output heatmap including Non-Maximum-Suppression (NMS) for\nthe final segmentation. Empirical results on two new annotated satellite videos\nreconfirm the applicability of this approach for vehicle detection. They more\nimportantly indicate that pre-training on WAMI data and then fine-tuning on few\nannotated video frames for a new video is sufficient. In our experiment only\nfive annotated images yield a $F_1$ score of 0.81 on a new video showing more\ncomplex traffic patterns than the Las Vegas video. Our best result on Las Vegas\nis a $F_1$ score of 0.87 which makes the proposed approach a leading method for\nthis benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pflugfelder_R/0/1/0/all/0/1\">Roman Pflugfelder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weissenfeld_A/0/1/0/all/0/1\">Axel Weissenfeld</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wagner_J/0/1/0/all/0/1\">Julian Wagner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Differentiable Zooming for Multiple Instance Learning on Whole-Slide Images. (arXiv:2204.12454v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.12454","description":"<p>Multiple Instance Learning (MIL) methods have become increasingly popular for\nclassifying giga-pixel sized Whole-Slide Images (WSIs) in digital pathology.\nMost MIL methods operate at a single WSI magnification, by processing all the\ntissue patches. Such a formulation induces high computational requirements, and\nconstrains the contextualization of the WSI-level representation to a single\nscale. A few MIL methods extend to multiple scales, but are computationally\nmore demanding. In this paper, inspired by the pathological diagnostic process,\nwe propose ZoomMIL, a method that learns to perform multi-level zooming in an\nend-to-end manner. ZoomMIL builds WSI representations by aggregating\ntissue-context information from multiple magnifications. The proposed method\noutperforms the state-of-the-art MIL methods in WSI classification on two large\ndatasets, while significantly reducing the computational demands with regard to\nFloating-Point Operations (FLOPs) and processing time by up to 40x.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thandiackal_K/0/1/0/all/0/1\">Kevin Thandiackal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Boqi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pati_P/0/1/0/all/0/1\">Pushpak Pati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaume_G/0/1/0/all/0/1\">Guillaume Jaume</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williamson_D/0/1/0/all/0/1\">Drew F. K. Williamson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gabrani_M/0/1/0/all/0/1\">Maria Gabrani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goksel_O/0/1/0/all/0/1\">Orcun Goksel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero and R2D2: A Large-scale Chinese Cross-modal Benchmark and A Vision-Language Framework. (arXiv:2205.03860v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.03860","description":"<p>Vision-language pre-training (VLP) on large-scale datasets has shown premier\nperformance on various downstream tasks. A complete and fair benchmark (i.e.,\nincluding large-scale pre-training datasets and diverse downstream tasks) is\nessential for VLP. While there are plenty of benchmarks with English corpus,\nbuilding a rich benchmark for VLP with other languages, such as Chinese,\nremains a critical problem. To this end, we build a large-scale Chinese\ncross-modal benchmark called Zero for the research community to fairly compare\nVLP models. We release two pre-training datasets and five fine-tuning datasets\nfor downstream tasks. Alongside, we propose a novel pre-training framework of\npre-Ranking + Ranking for cross-modal learning. Specifically, we apply global\ncontrastive pre-ranking to learn the individual representations of images and\ntexts, respectively. We then fuse the representations in a fine-grained ranking\nmanner via an image-text cross encoder and a text-image cross encoder. To\nfurther enhance the capability of the model, we propose a two-way distillation\nstrategy consisting of target-guided Distillation and feature-guided\nDistillation. For brevity, we name our model R2D2. We achieve state-of-the-art\nperformance on four public cross-modal datasets and the proposed five\ndownstream datasets. When conducting zero-shot tasks on Flickr30k-CN, COCO-CN,\nand MUGE, R2D2 pre-trained on a 250 million dataset achieves significant\nimprovements of 4.7%, 5.4%, and 6.3% in mean recall compared to the\nstate-of-the-art. The datasets, models, and codes are available at\nhttps://github.com/yuxie11/R2D2\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1\">Chunyu Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1\">Heng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jianfei Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jincheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_F/0/1/0/all/0/1\">Fanjing Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiaoyu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morimitsu_H/0/1/0/all/0/1\">Henrique Morimitsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_L/0/1/0/all/0/1\">Lin Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dexin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leng_D/0/1/0/all/0/1\">Dawei Leng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1\">Xiangyang Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yafeng Deng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Effective Transformer-based Solution for RSNA Intracranial Hemorrhage Detection Competition. (arXiv:2205.07556v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.07556","description":"<p>We present an effective method for Intracranial Hemorrhage Detection (IHD)\nwhich exceeds the performance of the winner solution in RSNA-IHD competition\n(2019). Meanwhile, our model only takes quarter parameters and ten percent\nFLOPs compared to the winner's solution. The IHD task needs to predict the\nhemorrhage category of each slice for the input brain CT. We review the top-5\nsolutions for the IHD competition held by the Radiological Society of North\nAmerica(RSNA) in 2019. Nearly all the top solutions rely on 2D convolutional\nnetworks and sequential models (Bidirectional GRU or LSTM) to extract\nintra-slice and inter-slice features, respectively. All the top solutions\nenhance the performance by leveraging the model ensemble, and the model number\nvaries from 7 to 31. In the past years, since much progress has been made in\nthe computer vision regime especially Transformer-based models, we introduce\nthe Transformer-based techniques to extract the features in both intra-slice\nand inter-slice views for IHD tasks. Additionally, a semi-supervised method is\nembedded into our workflow to further improve the performance. The code is\navailable in the manuscript.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shang_F/0/1/0/all/0/1\">Fangxin Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Siqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaorong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yehui Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ColonFormer: An Efficient Transformer based Method for Colon Polyp Segmentation. (arXiv:2205.08473v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.08473","description":"<p>Identifying polyps is challenging for automatic analysis of endoscopic images\nin computer-aided clinical support systems. Models based on convolutional\nnetworks (CNN), transformers, and their combinations have been proposed to\nsegment polyps with promising results. However, those approaches have\nlimitations either in modeling the local appearance of the polyps only or lack\nof multi-level features for spatial dependency in the decoding process. This\npaper proposes a novel network, namely ColonFormer, to address these\nlimitations. ColonFormer is an encoder-decoder architecture capable of modeling\nlong-range semantic information at both encoder and decoder branches. The\nencoder is a lightweight architecture based on transformers for modeling global\nsemantic relations at multi scales. The decoder is a hierarchical network\nstructure designed for learning multi-level features to enrich feature\nrepresentation. Besides, a refinement module is added with a new skip\nconnection technique to refine the boundary of polyp objects in the global map\nfor accurate segmentation. Extensive experiments have been conducted on five\npopular benchmark datasets for polyp segmentation, including Kvasir, CVC-Clinic\nDB, CVC-ColonDB, CVC-T, and ETIS-Larib. Experimental results show that our\nColonFormer outperforms other state-of-the-art methods on all benchmark\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Duc_N/0/1/0/all/0/1\">Nguyen Thanh Duc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oanh_N/0/1/0/all/0/1\">Nguyen Thi Oanh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thuy_N/0/1/0/all/0/1\">Nguyen Thi Thuy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Triet_T/0/1/0/all/0/1\">Tran Minh Triet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sang_D/0/1/0/all/0/1\">Dinh Viet Sang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BabyNet: Residual Transformer Module for Birth Weight Prediction on Fetal Ultrasound Video. (arXiv:2205.09382v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2205.09382","description":"<p>Predicting fetal weight at birth is an important aspect of perinatal care,\nparticularly in the context of antenatal management, which includes the planned\ntiming and the mode of delivery. Accurate prediction of weight using prenatal\nultrasound is challenging as it requires images of specific fetal body parts\nduring advanced pregnancy which is difficult to capture due to poor quality of\nimages caused by the lack of amniotic fluid. As a consequence, predictions\nwhich rely on standard methods often suffer from significant errors. In this\npaper we propose the Residual Transformer Module which extends a 3D\nResNet-based network for analysis of 2D+t spatio-temporal ultrasound video\nscans. Our end-to-end method, called BabyNet, automatically predicts fetal\nbirth weight based on fetal ultrasound video scans. We evaluate BabyNet using a\ndedicated clinical set comprising 225 2D fetal ultrasound videos of pregnancies\nfrom 75 patients performed one day prior to delivery. Experimental results show\nthat BabyNet outperforms several state-of-the-art methods and estimates the\nweight at birth with accuracy comparable to human experts. Furthermore,\ncombining estimates provided by human experts with those computed by BabyNet\nyields the best results, outperforming either of other methods by a significant\nmargin. The source code of BabyNet is available at\nhttps://github.com/SanoScience/BabyNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Plotka_S/0/1/0/all/0/1\">Szymon P&#x142;otka</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Grzeszczyk_M/0/1/0/all/0/1\">Michal K. Grzeszczyk</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Brawura_Biskupski_Samaha_R/0/1/0/all/0/1\">Robert Brawura-Biskupski-Samaha</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gutaj_P/0/1/0/all/0/1\">Pawe&#x142; Gutaj</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lipa_M/0/1/0/all/0/1\">Micha&#x142; Lipa</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Trzcinski_T/0/1/0/all/0/1\">Tomasz Trzci&#x144;ski</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sitek_A/0/1/0/all/0/1\">Arkadiusz Sitek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SelfReformer: Self-Refined Network with Transformer for Salient Object Detection. (arXiv:2205.11283v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.11283","description":"<p>The global and local contexts significantly contribute to the integrity of\npredictions in Salient Object Detection (SOD). Unfortunately, existing methods\nstill struggle to generate complete predictions with fine details. There are\ntwo major problems in conventional approaches: first, for global context,\nhigh-level CNN-based encoder features cannot effectively catch long-range\ndependencies, resulting in incomplete predictions. Second, downsampling the\nground truth to fit the size of predictions will introduce inaccuracy as the\nground truth details are lost during interpolation or pooling. Thus, in this\nwork, we developed a Transformer-based network and framed a supervised task for\na branch to learn the global context information explicitly. Besides, we adopt\nPixel Shuffle from Super-Resolution (SR) to reshape the predictions back to the\nsize of ground truth instead of the reverse. Thus details in the ground truth\nare untouched. In addition, we developed a two-stage Context Refinement Module\n(CRM) to fuse global context and automatically locate and refine the local\ndetails in the predictions. The proposed network can guide and correct itself\nbased on the global and local context generated, thus is named, Self-Refined\nTransformer (SelfReformer). Extensive experiments and evaluation results on\nfive benchmark datasets demonstrate the outstanding performance of the network,\nand we achieved the state-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yun_Y/0/1/0/all/0/1\">Yi Ke Yun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Weisi Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepRM: Deep Recurrent Matching for 6D Pose Refinement. (arXiv:2205.14474v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.14474","description":"<p>Precise 6D pose estimation of rigid objects from RGB images is a critical but\nchallenging task in robotics and augmented reality. To address this problem, we\npropose DeepRM, a novel recurrent network architecture for 6D pose refinement.\nDeepRM leverages initial coarse pose estimates to render synthetic images of\ntarget objects. The rendered images are then matched with the observed images\nto predict a rigid transform for updating the previous pose estimate. This\nprocess is repeated to incrementally refine the estimate at each iteration.\nLSTM units are used to propagate information through each refinement step,\nsignificantly improving overall performance. In contrast to many 2-stage\nPerspective-n-Point based solutions, DeepRM is trained end-to-end, and uses a\nscalable backbone that can be tuned via a single parameter for accuracy and\nefficiency. During training, a multi-scale optical flow head is added to\npredict the optical flow between the observed and synthetic images. Optical\nflow prediction stabilizes the training process, and enforces the learning of\nfeatures that are relevant to the task of pose estimation. Our results\ndemonstrate that DeepRM achieves state-of-the-art performance on two widely\naccepted challenging datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Avery_A/0/1/0/all/0/1\">Alexander Avery</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savakis_A/0/1/0/all/0/1\">Andreas Savakis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Harnessing spectral representations for subgraph alignment. (arXiv:2205.14938v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2205.14938","description":"<p>With the rise and advent of graph learning techniques, graph data has become\nubiquitous. However, while several efforts are being devoted to the design of\nnew convolutional architectures, pooling or positional encoding schemes, less\neffort is being spent on problems involving maps between (possibly very large)\ngraphs, such as signal transfer, graph isomorphism and subgraph correspondence.\nWith this paper, we anticipate the need for a convenient framework to deal with\nsuch problems, and focus in particular on the challenging subgraph alignment\nscenario. We claim that, first and foremost, the representation of a map plays\na central role on how these problems should be modeled. Taking the hint from\nrecent work in geometry processing, we propose the adoption of a spectral\nrepresentation for maps that is compact, easy to compute, robust to topological\nchanges, easy to plug into existing pipelines, and is especially effective for\nsubgraph alignment problems. We report for the first time a surprising\nphenomenon where the partiality arising in the subgraph alignment task is\nmanifested as a special structure of the map coefficients, even in the absence\nof exact subgraph isomorphism, and which is consistently observed over\ndifferent families of graphs up to several thousand nodes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pegoraro_M/0/1/0/all/0/1\">Marco Pegoraro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marin_R/0/1/0/all/0/1\">Riccardo Marin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rampini_A/0/1/0/all/0/1\">Arianna Rampini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melzi_S/0/1/0/all/0/1\">Simone Melzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cosmo_L/0/1/0/all/0/1\">Luca Cosmo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodola_E/0/1/0/all/0/1\">Emanuele Rodol&#xe0;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Pre-training of Vision Transformers for Dense Prediction Tasks. (arXiv:2205.15173v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.15173","description":"<p>We present a new self-supervised pre-training of Vision Transformers for\ndense prediction tasks. It is based on a contrastive loss across views that\ncompares pixel-level representations to global image representations. This\nstrategy produces better local features suitable for dense prediction tasks as\nopposed to contrastive pre-training based on global image representation only.\nFurthermore, our approach does not suffer from a reduced batch size since the\nnumber of negative examples needed in the contrastive loss is in the order of\nthe number of local features. We demonstrate the effectiveness of our\npre-training strategy on two dense prediction tasks: semantic segmentation and\nmonocular depth estimation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rabarisoa_J/0/1/0/all/0/1\">Jaonary Rabarisoa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belissen_V/0/1/0/all/0/1\">Valentin Belissen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chabot_F/0/1/0/all/0/1\">Florian Chabot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pham_Q/0/1/0/all/0/1\">Quoc-Cuong Pham</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is Mapping Necessary for Realistic PointGoal Navigation?. (arXiv:2206.00997v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.00997","description":"<p>Can an autonomous agent navigate in a new environment without building an\nexplicit map?\n</p>\n<p>For the task of PointGoal navigation ('Go to $\\Delta x$, $\\Delta y$') under\nidealized settings (no RGB-D and actuation noise, perfect GPS+Compass), the\nanswer is a clear 'yes' - map-less neural models composed of task-agnostic\ncomponents (CNNs and RNNs) trained with large-scale reinforcement learning\nachieve 100% Success on a standard dataset (Gibson). However, for PointNav in a\nrealistic setting (RGB-D and actuation noise, no GPS+Compass), this is an open\nquestion; one we tackle in this paper. The strongest published result for this\ntask is 71.7% Success.\n</p>\n<p>First, we identify the main (perhaps, only) cause of the drop in performance:\nthe absence of GPS+Compass. An agent with perfect GPS+Compass faced with RGB-D\nsensing and actuation noise achieves 99.8% Success (Gibson-v2 val). This\nsuggests that (to paraphrase a meme) robust visual odometry is all we need for\nrealistic PointNav; if we can achieve that, we can ignore the sensing and\nactuation noise.\n</p>\n<p>With that as our operating hypothesis, we scale the dataset and model size,\nand develop human-annotation-free data-augmentation techniques to train models\nfor visual odometry. We advance the state of art on the Habitat Realistic\nPointNav Challenge from 71% to 94% Success (+23, 31% relative) and 53% to 74%\nSPL (+21, 40% relative). While our approach does not saturate or 'solve' this\ndataset, this strong improvement combined with promising zero-shot sim2real\ntransfer (to a LoCoBot) provides evidence consistent with the hypothesis that\nexplicit mapping may not be necessary for navigation, even in a realistic\nsetting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Partsey_R/0/1/0/all/0/1\">Ruslan Partsey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wijmans_E/0/1/0/all/0/1\">Erik Wijmans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yokoyama_N/0/1/0/all/0/1\">Naoki Yokoyama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dobosevych_O/0/1/0/all/0/1\">Oles Dobosevych</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batra_D/0/1/0/all/0/1\">Dhruv Batra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maksymets_O/0/1/0/all/0/1\">Oleksandr Maksymets</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Spike Gating Flow: A Hierarchical Structure Based Spiking Neural Network for Online Gesture Recognition. (arXiv:2206.01910v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.01910","description":"<p>Action recognition is an exciting research avenue for artificial intelligence\nsince it may be a game changer in the emerging industrial fields such as\nrobotic visions and automobiles. However, current deep learning faces major\nchallenges for such applications because of the huge computational cost and the\ninefficient learning. Hence, we develop a novel brain-inspired Spiking Neural\nNetwork (SNN) based system titled Spiking Gating Flow (SGF) for online action\nlearning. The developed system consists of multiple SGF units which assembled\nin a hierarchical manner. A single SGF unit involves three layers: a feature\nextraction layer, an event-driven layer and a histogram-based training layer.\nTo demonstrate the developed system capabilities, we employ a standard Dynamic\nVision Sensor (DVS) gesture classification as a benchmark. The results indicate\nthat we can achieve 87.5% accuracy which is comparable with Deep Learning (DL),\nbut at smaller training/inference data number ratio 1.5:1. And only a single\ntraining epoch is required during the learning process. Meanwhile, to the best\nof our knowledge, this is the highest accuracy among the non-backpropagation\nalgorithm based SNNs. At last, we conclude the few-shot learning paradigm of\nthe developed network: 1) a hierarchical structure-based network design\ninvolves human prior knowledge; 2) SNNs for content based global dynamic\nfeature detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zihao Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanhong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Q/0/1/0/all/0/1\">Qiaosha Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Tie Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_F/0/1/0/all/0/1\">Fangbo Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiansong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1\">C.-J. Richard Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Junwen Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yuan Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CAINNFlow: Convolutional block Attention modules and Invertible Neural Networks Flow for anomaly detection and localization tasks. (arXiv:2206.01992v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.01992","description":"<p>Detection of object anomalies is crucial in industrial processes, but\nunsupervised anomaly detection and localization is particularly important due\nto the difficulty of obtaining a large number of defective samples and the\nunpredictable types of anomalies in real life. Among the existing unsupervised\nanomaly detection and localization methods, the NF-based scheme has achieved\nbetter results. However, the two subnets (complex functions) $s_{i}(u_{i})$ and\n$t_{i}(u_{i})$ in NF are usually multilayer perceptrons, which need to squeeze\nthe input visual features from 2D flattening to 1D, destroying the spatial\nlocation relationship in the feature map and losing the spatial structure\ninformation. In order to retain and effectively extract spatial structure\ninformation, we design in this study a complex function model with alternating\nCBAM embedded in a stacked $3\\times3$ full convolution, which is able to retain\nand effectively extract spatial structure information in the normalized flow\nmodel. Extensive experimental results on the MVTec AD dataset show that\nCAINNFlow achieves advanced levels of accuracy and inference efficiency based\non CNN and Transformer backbone networks as feature extractors, and CAINNFlow\nachieves a pixel-level AUC of $98.64\\%$ for anomaly detection in MVTec AD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1\">Ruiqing Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Fan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Mengyuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1\">Dongyu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinfeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jingrong Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1\">Qianjin Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Linghan Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Annotation and Learning for 3D Hand Pose Estimation: A Survey. (arXiv:2206.02257v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.02257","description":"<p>In this survey, we present comprehensive analysis of 3D hand pose estimation\nfrom the perspective of efficient annotation and learning. In particular, we\nstudy recent approaches for 3D hand pose annotation and learning methods with\nlimited annotated data. In 3D hand pose estimation, collecting 3D hand pose\nannotation is a key step in developing hand pose estimators and their\napplications, such as video understanding, AR/VR, and robotics. However,\nacquiring annotated 3D hand poses is cumbersome, e.g., due to the difficulty of\naccessing 3D information and occlusion. Motivated by elucidating how recent\nworks address the annotation issue, we investigated annotation methods\nclassified as manual, synthetic-model-based, hand-sensor-based, and\ncomputational approaches. Since these annotation methods are not always\navailable on a large scale, we examined methods of learning 3D hand poses when\nwe do not have enough annotated data, namely self-supervised pre-training,\nsemi-supervised learning, and domain adaptation. Based on the analysis of these\nefficient annotation and learning, we further discuss limitations and possible\nfuture directions of this field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ohkawa_T/0/1/0/all/0/1\">Takehiko Ohkawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Furuta_R/0/1/0/all/0/1\">Ryosuke Furuta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sato_Y/0/1/0/all/0/1\">Yoichi Sato</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SealID: Saimaa ringed seal re-identification dataset. (arXiv:2206.02260v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.02260","description":"<p>Wildlife camera traps and crowd-sourced image material provide novel\npossibilities to monitor endangered animal species. However, massive image\nvolumes that these methods produce are overwhelming for researchers to go\nthrough manually which calls for automatic systems to perform the analysis. The\nanalysis task that has gained the most attention is the re-identification of\nindividuals, as it allows, for example, to study animal migration or to\nestimate the population size. The Saimaa ringed seal (Pusa hispida saimensis)\nis an endangered subspecies only found in the Lake Saimaa, Finland, and is one\nof the few existing freshwater seal species. Ringed seals have permanent pelage\npatterns that are unique to each individual which can be used for the\nidentification of individuals. Large variation in poses further exacerbated by\nthe deformable nature of seals together with varying appearance and low\ncontrast between the ring pattern and the rest of the pelage makes the Saimaa\nringed seal re-identification task very challenging, providing a good benchmark\nto evaluate state-of-the-art re-identification methods. Therefore, we make our\nSaimaa ringed seal image (SealID) dataset (N=57) publicly available for\nresearch purposes. In this paper, the dataset is described, the evaluation\nprotocol for re-identification methods is proposed, and the results for two\nbaseline methods HotSpotter and NORPPA are provided. The SealID dataset has\nbeen made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nepovinnykh_E/0/1/0/all/0/1\">Ekaterina Nepovinnykh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eerola_T/0/1/0/all/0/1\">Tuomas Eerola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biard_V/0/1/0/all/0/1\">Vincent Biard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mutka_P/0/1/0/all/0/1\">Piia Mutka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niemi_M/0/1/0/all/0/1\">Marja Niemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalviainen_H/0/1/0/all/0/1\">Heikki K&#xe4;lvi&#xe4;inen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kunnasranta_M/0/1/0/all/0/1\">Mervi Kunnasranta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NORPPA: NOvel Ringed seal re-identification by Pelage Pattern Aggregation. (arXiv:2206.02498v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.02498","description":"<p>We propose a method for Saimaa ringed seal (Pusa hispida saimensis)\nre-identification. Access to large image volumes through camera trapping and\ncrowdsourcing provides novel possibilities for animal monitoring and\nconservation and calls for automatic methods for analysis, in particular, when\nre-identifying individual animals from the images. The proposed method NOvel\nRinged seal re-identification by Pelage Pattern Aggregation (NORPPA) utilizes\nthe permanent and unique pelage pattern of Saimaa ringed seals and\ncontent-based image retrieval techniques. First, the query image is\npreprocessed, and each seal instance is segmented. Next, the seal's pelage\npattern is extracted using a U-net encoder-decoder based method. Then,\nCNN-based affine invariant features are embedded and aggregated into Fisher\nVectors. Finally, the cosine distance between the Fisher Vectors is used to\nfind the best match from a database of known individuals. We perform extensive\nexperiments of various modifications of the method on a new challenging Saimaa\nringed seals re-identification dataset. The proposed method is shown to produce\nthe best re-identification accuracy on our dataset in comparisons with\nalternative approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nepovinnykh_E/0/1/0/all/0/1\">Ekaterina Nepovinnykh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chelak_I/0/1/0/all/0/1\">Ilia Chelak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eerola_T/0/1/0/all/0/1\">Tuomas Eerola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalviainen_H/0/1/0/all/0/1\">Heikki K&#xe4;lvi&#xe4;inen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dual Decomposition of Convex Optimization Layers for Consistent Attention in Medical Images. (arXiv:2206.02761v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.02761","description":"<p>A key concern in integrating machine learning models in medicine is the\nability to interpret their reasoning. Popular explainability methods have\ndemonstrated satisfactory results in natural image recognition, yet in medical\nimage analysis, many of these approaches provide partial and noisy\nexplanations. Recently, attention mechanisms have shown compelling results both\nin their predictive performance and in their interpretable qualities. A\nfundamental trait of attention is that it leverages salient parts of the input\nwhich contribute to the model's prediction. To this end, our work focuses on\nthe explanatory value of attention weight distributions. We propose a\nmulti-layer attention mechanism that enforces consistent interpretations\nbetween attended convolutional layers using convex optimization. We apply\nduality to decompose the consistency constraints between the layers by\nreparameterizing their attention probability distributions. We further suggest\nlearning the dual witness by optimizing with respect to our objective; thus,\nour implementation uses standard back-propagation, hence it is highly\nefficient. While preserving predictive performance, our proposed method\nleverages weakly annotated medical imaging data and provides complete and\nfaithful explanations to the model's prediction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ron_T/0/1/0/all/0/1\">Tom Ron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weiler_Sagie_M/0/1/0/all/0/1\">Michal Weiler-Sagie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hazan_T/0/1/0/all/0/1\">Tamir Hazan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Low Power Neuromorphic EMG Gesture Classification. (arXiv:2206.02061v1 [eess.SP] CROSS LISTED)","link":"http://arxiv.org/abs/2206.02061","description":"<p>EMG (Electromyograph) signal based gesture recognition can prove vital for\napplications such as smart wearables and bio-medical neuro-prosthetic control.\nSpiking Neural Networks (SNNs) are promising for low-power, real-time EMG\ngesture recognition, owing to their inherent spike/event driven spatio-temporal\ndynamics. In literature, there are limited demonstrations of neuromorphic\nhardware implementation (at full chip/board/system scale) for EMG gesture\nclassification. Moreover, most literature attempts exploit primitive SNNs based\non LIF (Leaky Integrate and Fire) neurons. In this work, we address the\naforementioned gaps with following key contributions: (1) Low-power, high\naccuracy demonstration of EMG-signal based gesture recognition using\nneuromorphic Recurrent Spiking Neural Networks (RSNN). In particular, we\npropose a multi-time scale recurrent neuromorphic system based on special\ndouble-exponential adaptive threshold (DEXAT) neurons. Our network achieves\nstate-of-the-art classification accuracy (90%) while using ~53% lesser neurons\nthan best reported prior art on Roshambo EMG dataset. (2) A new multi-channel\nspike encoder scheme for efficient processing of real-valued EMG data on\nneuromorphic systems. (3) Unique multi-compartment methodology to implement\ncomplex adaptive neurons on Intel's dedicated neuromorphic Loihi chip is shown.\n(4) RSNN implementation on Loihi (Nahuku 32) achieves significant\nenergy/latency benefits of ~983X/19X compared to GPU for batch size as 50.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bezugam_S/0/1/0/all/0/1\">Sai Sukruth Bezugam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shaban_A/0/1/0/all/0/1\">Ahmed Shaban</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Suri_M/0/1/0/all/0/1\">Manan Suri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-06-07T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","dc":"http://purl.org/dc/elements/1.1/","content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/"}}]}]}