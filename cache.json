{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-02-22T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Synthetic Disinformation Attacks on Automated Fact Verification Systems. (arXiv:2202.09381v1 [cs.CL])","link":"http://arxiv.org/abs/2202.09381","description":"<p>Automated fact-checking is a needed technology to curtail the spread of\nonline misinformation. One current framework for such solutions proposes to\nverify claims by retrieving supporting or refuting evidence from related\ntextual sources. However, the realistic use cases for fact-checkers will\nrequire verifying claims against evidence sources that could be affected by the\nsame misinformation. Furthermore, the development of modern NLP tools that can\nproduce coherent, fabricated content would allow malicious actors to\nsystematically generate adversarial disinformation for fact-checkers.\n</p>\n<p>In this work, we explore the sensitivity of automated fact-checkers to\nsynthetic adversarial evidence in two simulated settings: AdversarialAddition,\nwhere we fabricate documents and add them to the evidence repository available\nto the fact-checking system, and AdversarialModification, where existing\nevidence source documents in the repository are automatically altered. Our\nstudy across multiple models on three benchmarks demonstrates that these\nsystems suffer significant performance drops against these attacks. Finally, we\ndiscuss the growing threat of modern NLG systems as generators of\ndisinformation in the context of the challenges they pose to automated\nfact-checkers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yibing Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bosselut_A/0/1/0/all/0/1\">Antoine Bosselut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manning_C/0/1/0/all/0/1\">Christopher D. Manning</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Identifying the Adoption or Rejection of Misinformation Targeting COVID-19 Vaccines in Twitter Discourse. (arXiv:2202.09445v1 [cs.CL])","link":"http://arxiv.org/abs/2202.09445","description":"<p>Although billions of COVID-19 vaccines have been administered, too many\npeople remain hesitant. Misinformation about the COVID-19 vaccines, propagating\non social media, is believed to drive hesitancy towards vaccination. However,\nexposure to misinformation does not necessarily indicate misinformation\nadoption. In this paper we describe a novel framework for identifying the\nstance towards misinformation, relying on attitude consistency and its\nproperties. The interactions between attitude consistency, adoption or\nrejection of misinformation and the content of microblogs are exploited in a\nnovel neural architecture, where the stance towards misinformation is organized\nin a knowledge graph. This new neural framework is enabling the identification\nof stance towards misinformation about COVID-19 vaccines with state-of-the-art\nresults. The experiments are performed on a new dataset of misinformation\ntowards COVID-19 vaccines, called CoVaxLies, collected from recent Twitter\ndiscourse. Because CoVaxLies provides a taxonomy of the misinformation about\nCOVID-19 vaccines, we are able to show which type of misinformation is mostly\nadopted and which is mostly rejected.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Weinzierl_M/0/1/0/all/0/1\">Maxwell Weinzierl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harabagiu_S/0/1/0/all/0/1\">Sanda Harabagiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VaccineLies: A Natural Language Resource for Learning to Recognize Misinformation about the COVID-19 and HPV Vaccines. (arXiv:2202.09449v1 [cs.CL])","link":"http://arxiv.org/abs/2202.09449","description":"<p>Billions of COVID-19 vaccines have been administered, but many remain\nhesitant. Misinformation about the COVID-19 vaccines and other vaccines,\npropagating on social media, is believed to drive hesitancy towards\nvaccination. The ability to automatically recognize misinformation targeting\nvaccines on Twitter depends on the availability of data resources. In this\npaper we present VaccineLies, a large collection of tweets propagating\nmisinformation about two vaccines: the COVID-19 vaccines and the Human\nPapillomavirus (HPV) vaccines. Misinformation targets are organized in\nvaccine-specific taxonomies, which reveal the misinformation themes and\nconcerns. The ontological commitments of the Misinformation taxonomies provide\nan understanding of which misinformation themes and concerns dominate the\ndiscourse about the two vaccines covered in VaccineLies. The organization into\ntraining, testing and development sets of VaccineLies invites the development\nof novel supervised methods for detecting misinformation on Twitter and\nidentifying the stance towards it. Furthermore, VaccineLies can be a stepping\nstone for the development of datasets focusing on misinformation targeting\nadditional vaccines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Weinzierl_M/0/1/0/all/0/1\">Maxwell Weinzierl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harabagiu_S/0/1/0/all/0/1\">Sanda Harabagiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From FreEM to D'AlemBERT: a Large Corpus and a Language Model for Early Modern French. (arXiv:2202.09452v1 [cs.CL])","link":"http://arxiv.org/abs/2202.09452","description":"<p>Language models for historical states of language are becoming increasingly\nimportant to allow the optimal digitisation and analysis of old textual\nsources. Because these historical states are at the same time more complex to\nprocess and more scarce in the corpora available, specific efforts are\nnecessary to train natural language processing (NLP) tools adapted to the data.\nIn this paper, we present our efforts to develop NLP tools for Early Modern\nFrench (historical French from the 16$^\\text{th}$ to the 18$^\\text{th}$\ncenturies). We present the $\\text{FreEM}_{\\text{max}}$ corpus of Early Modern\nFrench and D'AlemBERT, a RoBERTa-based language model trained on\n$\\text{FreEM}_{\\text{max}}$. We evaluate the usefulness of D'AlemBERT by\nfine-tuning it on a part-of-speech tagging task, outperforming previous work on\nthe test set. Importantly, we find evidence for the transfer learning capacity\nof the language model, since its performance on lesser-resourced time periods\nappears to have been boosted by the more resourced ones. We release D'AlemBERT\nand the open-sourced subpart of the $\\text{FreEM}_{\\text{max}}$ corpus.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gabay_S/0/1/0/all/0/1\">Simon Gabay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suarez_P/0/1/0/all/0/1\">Pedro Ortiz Suarez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bartz_A/0/1/0/all/0/1\">Alexandre Bartz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chague_A/0/1/0/all/0/1\">Alix Chagu&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bawden_R/0/1/0/all/0/1\">Rachel Bawden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gambette_P/0/1/0/all/0/1\">Philippe Gambette</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sagot_B/0/1/0/all/0/1\">Beno&#xee;t Sagot</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Hesitancy Framings to Vaccine Hesitancy Profiles: A Journey of Stance, Ontological Commitments and Moral Foundations. (arXiv:2202.09456v1 [cs.CL])","link":"http://arxiv.org/abs/2202.09456","description":"<p>While billions of COVID-19 vaccines have been administered, too many people\nremain hesitant. Twitter, with its substantial reach and daily exposure, is an\nexcellent resource for examining how people frame their vaccine hesitancy and\nto uncover vaccine hesitancy profiles. In this paper we expose our processing\njourney from identifying Vaccine Hesitancy Framings in a collection of\n9,133,471 original tweets discussing the COVID-19 vaccines, establishing their\nontological commitments, annotating the Moral Foundations they imply to the\nautomatic recognition of the stance of the tweet authors toward any of the\nCoVaxFrames that we have identified. When we found that 805,336 Twitter users\nhad a stance towards some CoVaxFrames in either the 9,133,471 original tweets\nor their 17,346,664 retweets, we were able to derive nine different Vaccine\nHesitancy Profiles of these users and to interpret these profiles based on the\nontological commitments of the frames they evoked in their tweets and on value\nof their stance towards the evoked frames.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Weinzierl_M/0/1/0/all/0/1\">Maxwell Weinzierl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harabagiu_S/0/1/0/all/0/1\">Sanda Harabagiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data-Driven Mitigation of Adversarial Text Perturbation. (arXiv:2202.09483v1 [cs.CL])","link":"http://arxiv.org/abs/2202.09483","description":"<p>Social networks have become an indispensable part of our lives, with billions\nof people producing ever-increasing amounts of text. At such scales, content\npolicies and their enforcement become paramount. To automate moderation,\nquestionable content is detected by Natural Language Processing (NLP)\nclassifiers. However, high-performance classifiers are hampered by misspellings\nand adversarial text perturbations. In this paper, we classify intentional and\nunintentional adversarial text perturbation into ten types and propose a\ndeobfuscation pipeline to make NLP models robust to such perturbations. We\npropose Continuous Word2Vec (CW2V), our data-driven method to learn word\nembeddings that ensures that perturbations of words have embeddings similar to\nthose of the original words. We show that CW2V embeddings are generally more\nrobust to text perturbations than embeddings based on character ngrams. Our\nrobust classification pipeline combines deobfuscation and classification, using\nproposed defense methods and word embeddings to classify whether Facebook posts\nare requesting engagement such as likes. Our pipeline results in engagement\nbait classification that goes from 0.70 to 0.67 AUC with adversarial text\nperturbation, while character ngram-based word embedding methods result in\ndownstream classification that goes from 0.76 to 0.64.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhalerao_R/0/1/0/all/0/1\">Rasika Bhalerao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Rubaie_M/0/1/0/all/0/1\">Mohammad Al-Rubaie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhaskar_A/0/1/0/all/0/1\">Anand Bhaskar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Markov_I/0/1/0/all/0/1\">Igor Markov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PETCI: A Parallel English Translation Dataset of Chinese Idioms. (arXiv:2202.09509v1 [cs.CL])","link":"http://arxiv.org/abs/2202.09509","description":"<p>Idioms are an important language phenomenon in Chinese, but idiom translation\nis notoriously hard. Current machine translation models perform poorly on idiom\ntranslation, while idioms are sparse in many translation datasets. We present\nPETCI, a parallel English translation dataset of Chinese idioms, aiming to\nimprove idiom translation by both human and machine. The dataset is built by\nleveraging human and machine effort. Baseline generation models show\nunsatisfactory abilities to improve translation, but structure-aware\nclassification models show good performance on distinguishing good\ntranslations. Furthermore, the size of PETCI can be easily increased without\nexpertise. Overall, PETCI can be helpful to language learners and machine\ntranslation systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_K/0/1/0/all/0/1\">Kenan Tang</a> (The University of Chicago)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning for Hate Speech Detection: A Comparative Study. (arXiv:2202.09517v1 [cs.CL])","link":"http://arxiv.org/abs/2202.09517","description":"<p>Automated hate speech detection is an important tool in combating the spread\nof hate speech, particularly in social media. Numerous methods have been\ndeveloped for the task, including a recent proliferation of deep-learning based\napproaches. A variety of datasets have also been developed, exemplifying\nvarious manifestations of the hate-speech detection problem. We present here a\nlarge-scale empirical comparison of deep and shallow hate-speech detection\nmethods, mediated through the three most commonly used datasets. Our goal is to\nilluminate progress in the area, and identify strengths and weaknesses in the\ncurrent state-of-the-art. We particularly focus our analysis on measures of\npractical performance, including detection accuracy, computational efficiency,\ncapability in using pre-trained models, and domain generalization. In doing so\nwe aim to provide guidance as to the use of hate-speech detection in practice,\nquantify the state-of-the-art, and identify future research directions. Code\nand dataset are available at\nhttps://github.com/jmjmalik22/Hate-Speech-Detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Malik_J/0/1/0/all/0/1\">Jitendra Singh Malik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_G/0/1/0/all/0/1\">Guansong Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hengel_A/0/1/0/all/0/1\">Anton van den Hengel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Models and Datasets for Cross-Lingual Summarisation. (arXiv:2202.09583v1 [cs.CL])","link":"http://arxiv.org/abs/2202.09583","description":"<p>We present a cross-lingual summarisation corpus with long documents in a\nsource language associated with multi-sentence summaries in a target language.\nThe corpus covers twelve language pairs and directions for four European\nlanguages, namely Czech, English, French and German, and the methodology for\nits creation can be applied to several other languages. We derive cross-lingual\ndocument-summary instances from Wikipedia by combining lead paragraphs and\narticles' bodies from language aligned Wikipedia titles. We analyse the\nproposed cross-lingual summarisation task with automatic metrics and validate\nit with a human study. To illustrate the utility of our dataset we report\nexperiments with multi-lingual pre-trained models in supervised, zero- and\nfew-shot, and out-of-domain scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Perez_Beltrachini_L/0/1/0/all/0/1\">Laura Perez-Beltrachini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lapata_M/0/1/0/all/0/1\">Mirella Lapata</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CALCS 2021 Shared Task: Machine Translation for Code-Switched Data. (arXiv:2202.09625v1 [cs.CL])","link":"http://arxiv.org/abs/2202.09625","description":"<p>To date, efforts in the code-switching literature have focused for the most\npart on language identification, POS, NER, and syntactic parsing. In this\npaper, we address machine translation for code-switched social media data. We\ncreate a community shared task. We provide two modalities for participation:\nsupervised and unsupervised. For the supervised setting, participants are\nchallenged to translate English into Hindi-English (Eng-Hinglish) in a single\ndirection. For the unsupervised setting, we provide the following language\npairs: English and Spanish-English (Eng-Spanglish), and English and Modern\nStandard Arabic-Egyptian Arabic (Eng-MSAEA) in both directions. We share\ninsights and challenges in curating the \"into\" code-switching language\nevaluation data. Further, we provide baselines for all language pairs in the\nshared task. The leaderboard for the shared task comprises 12 individual system\nsubmissions corresponding to 5 different teams. The best performance achieved\nis 12.67% BLEU score for English to Hinglish and 25.72% BLEU score for MSAEA to\nEnglish.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shuguang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aguilar_G/0/1/0/all/0/1\">Gustavo Aguilar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_A/0/1/0/all/0/1\">Anirudh Srinivasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diab_M/0/1/0/all/0/1\">Mona Diab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Solorio_T/0/1/0/all/0/1\">Thamar Solorio</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reward Modeling for Mitigating Toxicity in Transformer-based Language Models. (arXiv:2202.09662v1 [cs.CL])","link":"http://arxiv.org/abs/2202.09662","description":"<p>Transformer-based language models are able to generate fluent text and be\nefficiently adapted across various natural language generation tasks. However,\nlanguage models that are pretrained on large unlabeled web text corpora have\nbeen shown to suffer from degenerating toxic content and social bias behaviors,\nconsequently hindering their safe deployment. Various detoxification methods\nwere proposed to mitigate the language model's toxicity; however, these methods\nstruggled to detoxify language models when conditioned on prompts that contain\nspecific social identities related to gender, race, or religion. In this study,\nwe propose Reinforce-Detoxify; A reinforcement learning-based method for\nmitigating toxicity in language models. We address the challenge of safety in\nlanguage models and propose a new reward model that is able to detect toxic\ncontent and mitigate unintended bias towards social identities in toxicity\nprediction. The experiments demonstrate that the Reinforce-Detoxify method for\nlanguage model detoxification outperforms existing detoxification approaches in\nautomatic evaluation metrics, indicating the ability of our approach in\nlanguage model detoxification and less prone to unintended bias toward social\nidentities in generated content.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Faal_F/0/1/0/all/0/1\">Farshid Faal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jia Yuan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmitt_K/0/1/0/all/0/1\">Ketra Schmitt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is there an aesthetic component of language?. (arXiv:2202.09689v1 [cs.CL])","link":"http://arxiv.org/abs/2202.09689","description":"<p>Speakers of all human languages make use of grammatical devices to express\nattributional qualities, feelings, and opinions as well as to provide\nmeta-commentary on topics in discourse. In general, linguists refer to this\ncategory as 'expressives'in spite of the fact that defining exactly what\n'expressives' are remains elusive. The elusiveness of expressives has given\nrise to considerable speculation about the nature of expressivity as a\nlinguistic principle. Specifically, several scholars have pointed out the\n'special' or 'unusual' nature of expressives vis-a-vis 'normal' or 'natural'\nmorpho-syntax.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Parmar_H/0/1/0/all/0/1\">Harshit Parmar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_J/0/1/0/all/0/1\">Jeffrey P. Williams</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MACRONYM: A Large-Scale Dataset for Multilingual and Multi-Domain Acronym Extraction. (arXiv:2202.09694v1 [cs.CL])","link":"http://arxiv.org/abs/2202.09694","description":"<p>Acronym extraction is the task of identifying acronyms and their expanded\nforms in texts that is necessary for various NLP applications. Despite major\nprogress for this task in recent years, one limitation of existing AE research\nis that they are limited to the English language and certain domains (i.e.,\nscientific and biomedical). As such, challenges of AE in other languages and\ndomains is mainly unexplored. Lacking annotated datasets in multiple languages\nand domains has been a major issue to hinder research in this area. To address\nthis limitation, we propose a new dataset for multilingual multi-domain AE.\nSpecifically, 27,200 sentences in 6 typologically different languages and 2\ndomains, i.e., Legal and Scientific, is manually annotated for AE. Our\nextensive experiments on the proposed dataset show that AE in different\nlanguages and different learning settings has unique challenges, emphasizing\nthe necessity of further research on multilingual and multi-domain AE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Veyseh_A/0/1/0/all/0/1\">Amir Pouran Ben Veyseh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meister_N/0/1/0/all/0/1\">Nicole Meister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Seunghyun Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_R/0/1/0/all/0/1\">Rajiv Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dernoncourt_F/0/1/0/all/0/1\">Franck Dernoncourt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Thien Huu Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Punctuation Restoration. (arXiv:2202.09695v1 [cs.CL])","link":"http://arxiv.org/abs/2202.09695","description":"<p>Given the increasing number of livestreaming videos, automatic speech\nrecognition and post-processing for livestreaming video transcripts are crucial\nfor efficient data management as well as knowledge mining. A key step in this\nprocess is punctuation restoration which restores fundamental text structures\nsuch as phrase and sentence boundaries from the video transcripts. This work\npresents a new human-annotated corpus, called BehancePR, for punctuation\nrestoration in livestreaming video transcripts. Our experiments on BehancePR\ndemonstrate the challenges of punctuation restoration for this domain.\nFurthermore, we show that popular natural language processing toolkits are\nincapable of detecting sentence boundary on non-punctuated transcripts of\nlivestreaming videos, calling for more research effort to develop robust models\nfor this area.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lai_V/0/1/0/all/0/1\">Viet Dac Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Veyseh_A/0/1/0/all/0/1\">Amir Pouran Ben Veyseh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dernoncourt_F/0/1/0/all/0/1\">Franck Dernoncourt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Thien Huu Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Graph-based Extractive Explainer for Recommendations. (arXiv:2202.09730v1 [cs.IR])","link":"http://arxiv.org/abs/2202.09730","description":"<p>Explanations in a recommender system assist users in making informed\ndecisions among a set of recommended items. Great research attention has been\ndevoted to generating natural language explanations to depict how the\nrecommendations are generated and why the users should pay attention to them.\nHowever, due to different limitations of those solutions, e.g., template-based\nor generation-based, it is hard to make the explanations easily perceivable,\nreliable and personalized at the same time.\n</p>\n<p>In this work, we develop a graph attentive neural network model that\nseamlessly integrates user, item, attributes, and sentences for\nextraction-based explanation. The attributes of items are selected as the\nintermediary to facilitate message passing for user-item specific evaluation of\nsentence relevance. And to balance individual sentence relevance, overall\nattribute coverage, and content redundancy, we solve an integer linear\nprogramming problem to make the final selection of sentences. Extensive\nempirical evaluations against a set of state-of-the-art baseline methods on two\nbenchmark review datasets demonstrated the generation quality of the proposed\nsolution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_R/0/1/0/all/0/1\">Renqin Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongning Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contextual Semantic Embeddings for Ontology Subsumption Prediction. (arXiv:2202.09791v1 [cs.AI])","link":"http://arxiv.org/abs/2202.09791","description":"<p>Automating ontology curation is a crucial task in knowledge engineering.\nPrediction by machine learning techniques such as semantic embedding is a\npromising direction, but the relevant research is still preliminary. In this\npaper, we present a class subsumption prediction method named BERTSubs, which\nuses the pre-trained language model BERT to compute contextual embeddings of\nthe class labels and customized input templates to incorporate contexts of\nsurrounding classes. The evaluation on two large-scale real-world ontologies\nhas shown its better performance than the state-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiaoyan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yuan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jimenez_Ruiz_E/0/1/0/all/0/1\">Ernesto Jimenez-Ruiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Hang Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horrocks_I/0/1/0/all/0/1\">Ian Horrocks</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical Interpretation of Neural Text Classification. (arXiv:2202.09792v1 [cs.CL])","link":"http://arxiv.org/abs/2202.09792","description":"<p>Recent years have witnessed increasing interests in developing interpretable\nmodels in Natural Language Processing (NLP). Most existing models aim at\nidentifying input features such as words or phrases important for model\npredictions. Neural models developed in NLP however often compose word\nsemantics in a hierarchical manner. Interpretation by words or phrases only\nthus cannot faithfully explain model decisions. This paper proposes a novel\nHierarchical INTerpretable neural text classifier, called Hint, which can\nautomatically generate explanations of model predictions in the form of\nlabel-associated topics in a hierarchical manner. Model interpretation is no\nlonger at the word level, but built on topics as the basic semantic unit.\nExperimental results on both review datasets and news datasets show that our\nproposed approach achieves text classification results on par with existing\nstate-of-the-art text classifiers, and generates interpretations more faithful\nto model predictions and better understood by humans than other interpretable\nneural text classifiers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1\">Hanqi Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gui_L/0/1/0/all/0/1\">Lin Gui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yulan He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"$\\mathcal{Y}$-Tuning: An Efficient Tuning Paradigm for Large-Scale Pre-Trained Models via Label Representation Learning. (arXiv:2202.09817v1 [cs.CL])","link":"http://arxiv.org/abs/2202.09817","description":"<p>With the success of large-scale pre-trained models (PTMs), how efficiently\nadapting PTMs to downstream tasks has attracted tremendous attention,\nespecially for PTMs with billions of parameters. Although some\nparameter-efficient tuning paradigms have been proposed to address this\nproblem, they still require large resources to compute the gradients in the\ntraining phase. In this paper, we propose $\\mathcal{Y}$-Tuning, an efficient\nyet effective paradigm to adapt frozen large-scale PTMs to specific downstream\ntasks. $\\mathcal{Y}$-tuning learns dense representations for labels\n$\\mathcal{Y}$ defined in a given task and aligns them to fixed feature\nrepresentation. Without tuning the features of input text and model parameters,\n$\\mathcal{Y}$-tuning is both parameter-efficient and training-efficient. For\n$\\text{DeBERTa}_\\text{XXL}$ with 1.6 billion parameters, $\\mathcal{Y}$-tuning\nachieves performance more than $96\\%$ of full fine-tuning on GLUE Benchmark\nwith only $2\\%$ tunable parameters and much fewer training costs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yitao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_C/0/1/0/all/0/1\">Chenxin An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Meta-embedding-based Ensemble Approach for ICD Coding Prediction. (arXiv:2102.13622v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2102.13622","description":"<p>International Classification of Diseases (ICD) are the de facto codes used\nglobally for clinical coding. These codes enable healthcare providers to claim\nreimbursement and facilitate efficient storage and retrieval of diagnostic\ninformation. The problem of automatically assigning ICD codes has been\napproached in literature as a multilabel classification, using neural models on\nunstructured data. Our proposed approach enhances the performance of neural\nmodels by effectively training word vectors using routine medical data as well\nas external knowledge from scientific articles. Furthermore, we exploit the\ngeometric properties of the two sets of word vectors and combine them into a\ncommon dimensional space, using meta-embedding techniques. We demonstrate the\nefficacy of this approach for a multimodal setting, using unstructured and\nstructured information. We empirically show that our approach improves the\ncurrent state-of-the-art deep learning architectures and benefits ensemble\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rajendran_P/0/1/0/all/0/1\">Pavithra Rajendran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zenonos_A/0/1/0/all/0/1\">Alexandros Zenonos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spear_J/0/1/0/all/0/1\">Josh Spear</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pope_R/0/1/0/all/0/1\">Rebecca Pope</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Natural Language Understanding for Argumentative Dialogue Systems in the Opinion Building Domain. (arXiv:2103.02691v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.02691","description":"<p>This paper introduces a natural language understanding (NLU) framework for\nargumentative dialogue systems in the information-seeking and opinion building\ndomain. The proposed framework consists of two sub-models, namely intent\nclassifier and argument similarity. Intent classifier model stacks BiLSTM with\nattention mechanism on top of the pre-trained BERT model and fine-tune the\nmodel for recognizing the user intent, whereas the argument similarity model\nemploys BERT+BiLSTM for identifying system arguments the user refers to in his\nor her natural language utterances. Our model is evaluated in an argumentative\ndialogue system that engages the user to inform him-/herself about a\ncontroversial topic by exploring pro and con arguments and build his/her\nopinion towards the topic. In order to evaluate the proposed approach, we\ncollect user utterances for the interaction with the respective system labeling\nintent and referenced argument in an extensive online study. The data\ncollection includes multiple topics and two different user types (native\nEnglish speakers from the UK and non-native English speakers from China).\nAdditionally, we evaluate the proposed intent classifier and argument\nsimilarity models separately on the publicly available Banking77 and STS\nbenchmark datasets. The evaluation indicates a clear advantage of the utilized\ntechniques over baseline approaches on several datasets, as well as the\nrobustness of the proposed approach against new topics and different language\nproficiency as well as the cultural background of the user. Furthermore,\nresults show that our intent classifier model outperforms DIET, DistillBERT,\nand BERT fine-tuned models in few-shot setups (i.e., with 10, 20, or 30 labeled\nexamples per intent) and full data setup.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abro_W/0/1/0/all/0/1\">Waheed Ahmed Abro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aicher_A/0/1/0/all/0/1\">Annalena Aicher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rach_N/0/1/0/all/0/1\">Niklas Rach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ultes_S/0/1/0/all/0/1\">Stefan Ultes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minker_W/0/1/0/all/0/1\">Wolfgang Minker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_G/0/1/0/all/0/1\">Guilin Qi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Retrieve Fast, Rerank Smart: Cooperative and Joint Approaches for Improved Cross-Modal Retrieval. (arXiv:2103.11920v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.11920","description":"<p>Current state-of-the-art approaches to cross-modal retrieval process text and\nvisual input jointly, relying on Transformer-based architectures with\ncross-attention mechanisms that attend over all words and objects in an image.\nWhile offering unmatched retrieval performance, such models: 1) are typically\npretrained from scratch and thus less scalable, 2) suffer from huge retrieval\nlatency and inefficiency issues, which makes them impractical in realistic\napplications. To address these crucial gaps towards both improved and efficient\ncross-modal retrieval, we propose a novel fine-tuning framework that turns any\npretrained text-image multi-modal model into an efficient retrieval model. The\nframework is based on a cooperative retrieve-and-rerank approach which\ncombines: 1) twin networks (i.e., a bi-encoder) to separately encode all items\nof a corpus, enabling efficient initial retrieval, and 2) a cross-encoder\ncomponent for a more nuanced (i.e., smarter) ranking of the retrieved small set\nof items. We also propose to jointly fine-tune the two components with shared\nweights, yielding a more parameter-efficient model. Our experiments on a series\nof standard cross-modal retrieval benchmarks in monolingual, multilingual, and\nzero-shot setups, demonstrate improved accuracy and huge efficiency benefits\nover the state-of-the-art cross-encoders.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Geigle_G/0/1/0/all/0/1\">Gregor Geigle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfeiffer_J/0/1/0/all/0/1\">Jonas Pfeiffer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reimers_N/0/1/0/all/0/1\">Nils Reimers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vulic_I/0/1/0/all/0/1\">Ivan Vuli&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quality at a Glance: An Audit of Web-Crawled Multilingual Datasets. (arXiv:2103.12028v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.12028","description":"<p>With the success of large-scale pre-training and multilingual modeling in\nNatural Language Processing (NLP), recent years have seen a proliferation of\nlarge, web-mined text datasets covering hundreds of languages. We manually\naudit the quality of 205 language-specific corpora released with five major\npublic datasets (CCAligned, ParaCrawl, WikiMatrix, OSCAR, mC4). Lower-resource\ncorpora have systematic issues: At least 15 corpora have no usable text, and a\nsignificant fraction contains less than 50% sentences of acceptable quality. In\naddition, many are mislabeled or use nonstandard/ambiguous language codes. We\ndemonstrate that these issues are easy to detect even for non-proficient\nspeakers, and supplement the human audit with automatic analyses. Finally, we\nrecommend techniques to evaluate and improve multilingual corpora and discuss\npotential risks that come with low-quality data releases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kreutzer_J/0/1/0/all/0/1\">Julia Kreutzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caswell_I/0/1/0/all/0/1\">Isaac Caswell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lisa Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wahab_A/0/1/0/all/0/1\">Ahsan Wahab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Esch_D/0/1/0/all/0/1\">Daan van Esch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ulzii_Orshikh_N/0/1/0/all/0/1\">Nasanbayar Ulzii-Orshikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tapo_A/0/1/0/all/0/1\">Allahsera Tapo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subramani_N/0/1/0/all/0/1\">Nishant Subramani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sokolov_A/0/1/0/all/0/1\">Artem Sokolov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sikasote_C/0/1/0/all/0/1\">Claytone Sikasote</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Setyawan_M/0/1/0/all/0/1\">Monang Setyawan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarin_S/0/1/0/all/0/1\">Supheakmungkol Sarin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samb_S/0/1/0/all/0/1\">Sokhar Samb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sagot_B/0/1/0/all/0/1\">Beno&#xee;t Sagot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rivera_C/0/1/0/all/0/1\">Clara Rivera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rios_A/0/1/0/all/0/1\">Annette Rios</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papadimitriou_I/0/1/0/all/0/1\">Isabel Papadimitriou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osei_S/0/1/0/all/0/1\">Salomey Osei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suarez_P/0/1/0/all/0/1\">Pedro Ortiz Suarez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orife_I/0/1/0/all/0/1\">Iroro Orife</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ogueji_K/0/1/0/all/0/1\">Kelechi Ogueji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rubungo_A/0/1/0/all/0/1\">Andre Niyongabo Rubungo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Toan Q. Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_M/0/1/0/all/0/1\">Mathias M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_A/0/1/0/all/0/1\">Andr&#xe9; M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muhammad_S/0/1/0/all/0/1\">Shamsuddeen Hassan Muhammad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muhammad_N/0/1/0/all/0/1\">Nanda Muhammad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mnyakeni_A/0/1/0/all/0/1\">Ayanda Mnyakeni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirzakhalov_J/0/1/0/all/0/1\">Jamshidbek Mirzakhalov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matangira_T/0/1/0/all/0/1\">Tapiwanashe Matangira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leong_C/0/1/0/all/0/1\">Colin Leong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lawson_N/0/1/0/all/0/1\">Nze Lawson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kudugunta_S/0/1/0/all/0/1\">Sneha Kudugunta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jernite_Y/0/1/0/all/0/1\">Yacine Jernite</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jenny_M/0/1/0/all/0/1\">Mathias Jenny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Firat_O/0/1/0/all/0/1\">Orhan Firat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dossou_B/0/1/0/all/0/1\">Bonaventure F. P. Dossou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dlamini_S/0/1/0/all/0/1\">Sakhile Dlamini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_N/0/1/0/all/0/1\">Nisansa de Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balli_S/0/1/0/all/0/1\">Sakine &#xc7;abuk Ball&#x131;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biderman_S/0/1/0/all/0/1\">Stella Biderman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Battisti_A/0/1/0/all/0/1\">Alessia Battisti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baruwa_A/0/1/0/all/0/1\">Ahmed Baruwa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bapna_A/0/1/0/all/0/1\">Ankur Bapna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baljekar_P/0/1/0/all/0/1\">Pallavi Baljekar</a>, et al. (7 additional authors not shown)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recommending Metamodel Concepts during Modeling Activities with Pre-Trained Language Models. (arXiv:2104.01642v3 [cs.SE] UPDATED)","link":"http://arxiv.org/abs/2104.01642","description":"<p>The design of conceptually sound metamodels that embody proper semantics in\nrelation to the application domain is particularly tedious in Model-Driven\nEngineering. As metamodels define complex relationships between domain\nconcepts, it is crucial for a modeler to define these concepts thoroughly while\nbeing consistent with respect to the application domain. We propose an approach\nto assist a modeler in the design of a metamodel by recommending relevant\ndomain concepts in several modeling scenarios. Our approach does not require to\nextract knowledge from the domain or to hand-design completion rules. Instead,\nwe design a fully data-driven approach using a deep learning model that is able\nto abstract domain concepts by learning from both structural and lexical\nmetamodel properties in a corpus of thousands of independent metamodels. We\nevaluate our approach on a test set containing 166 metamodels, unseen during\nthe model training, with more than 5000 test samples. Our preliminary results\nshow that the trained model is able to provide accurate top-$5$ lists of\nrelevant recommendations for concept renaming scenarios. Although promising,\nthe results are less compelling for the scenario of the iterative construction\nof the metamodel, in part because of the conservative strategy we use to\nevaluate the recommendations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Weyssow_M/0/1/0/all/0/1\">Martin Weyssow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahraoui_H/0/1/0/all/0/1\">Houari Sahraoui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Syriani_E/0/1/0/all/0/1\">Eugene Syriani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On migration to Perpetual Enterprise System. (arXiv:2104.04844v3 [cs.SE] UPDATED)","link":"http://arxiv.org/abs/2104.04844","description":"<p>This document describes a pragmatic approach on how to migrate an\norganisation computer system towards a new system that could evolve forever,\naddresses the whole organisation and it is integrated.\n</p>\n<p>Governance aspects are as important, if not more, than purely technical IT\naspects: human resources, call for tenders, and similar. Migration implies that\none is not starting from a green field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Benitez_M/0/1/0/all/0/1\">Manuel Tomas Carrasco Benitez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Differentiable Prompt Makes Pre-trained Language Models Better Few-shot Learners. (arXiv:2108.13161v6 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.13161","description":"<p>Large-scale pre-trained language models have contributed significantly to\nnatural language processing by demonstrating remarkable abilities as few-shot\nlearners. However, their effectiveness depends mainly on scaling the model\nparameters and prompt design, hindering their implementation in most real-world\napplications. This study proposes a novel pluggable, extensible, and efficient\napproach named DifferentiAble pRompT (DART), which can convert small language\nmodels into better few-shot learners without any prompt engineering. The main\nprinciple behind this approach involves reformulating potential natural\nlanguage processing tasks into the task of a pre-trained language model and\ndifferentially optimizing the prompt template as well as the target label with\nbackpropagation. Furthermore, the proposed approach can be: (i) Plugged to any\npre-trained language models; (ii) Extended to widespread classification tasks.\nA comprehensive evaluation of standard NLP tasks demonstrates that the proposed\napproach achieves a better few-shot performance. Code is available in\nhttps://github.com/zjunlp/DART.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Luoqiu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_Z/0/1/0/all/0/1\">Zhen Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ArchivalQA: A Large-scale Benchmark Dataset for Open Domain Question Answering over Archival News Collections. (arXiv:2109.03438v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.03438","description":"<p>In the last few years, open-domain question answering (ODQA) has advanced\nrapidly due to the development of deep learning techniques and the availability\nof large-scale QA datasets. However, the current datasets are essentially\ndesigned for synchronic document collections (e.g., Wikipedia). Temporal news\ncollections such as long-term news archives spanning several decades, are\nrarely used in training the models despite they are quite valuable for our\nsociety. To foster the research in the field of ODQA on such historical\ncollections, we present ArchivalQA, a large question answering dataset\nconsisting of 532,444 question-answer pairs which is designed for temporal news\nQA. We divide our dataset into four subparts based on the question difficulty\nlevels and the containment of temporal expressions, which we believe are useful\nfor training and testing ODQA systems characterized by different strengths and\nabilities. The novel QA dataset-constructing framework that we introduce can be\nalso applied to generate non-ambiguous questions of good quality over other\ntypes of temporal document collections.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiexin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jatowt_A/0/1/0/all/0/1\">Adam Jatowt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoshikawa_M/0/1/0/all/0/1\">Masatoshi Yoshikawa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TopiOCQA: Open-domain Conversational Question Answering with Topic Switching. (arXiv:2110.00768v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.00768","description":"<p>In a conversational question answering scenario, a questioner seeks to\nextract information about a topic through a series of interdependent questions\nand answers. As the conversation progresses, they may switch to related topics,\na phenomenon commonly observed in information-seeking search sessions. However,\ncurrent datasets for conversational question answering are limiting in two\nways: 1) they do not contain topic switches; and 2) they assume the reference\ntext for the conversation is given, i.e., the setting is not open-domain. We\nintroduce TopiOCQA (pronounced Tapioca), an open-domain conversational dataset\nwith topic switches on Wikipedia. TopiOCQA contains 3,920 conversations with\ninformation-seeking questions and free-form answers. On average, a conversation\nin our dataset spans 13 question-answer turns and involves four topics\n(documents). TopiOCQA poses a challenging test-bed for models, where efficient\nretrieval is required on multiple turns of the same conversation, in\nconjunction with constructing valid responses using conversational history. We\nevaluate several baselines, by combining state-of-the-art document retrieval\nmethods with neural reader models. Our best model achieves F1 of 55.8, falling\nshort of human performance by 14.2 points, indicating the difficulty of our\ndataset. Our dataset and code is available at\nhttps://mcgill-nlp.github.io/topiocqa\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Adlakha_V/0/1/0/all/0/1\">Vaibhav Adlakha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhuliawala_S/0/1/0/all/0/1\">Shehzaad Dhuliawala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suleman_K/0/1/0/all/0/1\">Kaheer Suleman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vries_H/0/1/0/all/0/1\">Harm de Vries</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1\">Siva Reddy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Internal Language Model Adaptation with Text-Only Data for End-to-End Speech Recognition. (arXiv:2110.05354v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.05354","description":"<p>Text-only adaptation of an end-to-end (E2E) model remains a challenging task\nfor automatic speech recognition (ASR). Language model (LM) fusion-based\napproaches require an additional external LM during inference, significantly\nincreasing the computation cost. To overcome this, we propose an internal LM\nadaptation (ILMA) of the E2E model using text-only data. Trained with\naudio-transcript pairs, an E2E model implicitly learns an internal LM that\ncharacterizes the token sequence probability which is approximated by the E2E\nmodel output after zeroing out the encoder contribution. During ILMA, we\nfine-tune the internal LM, i.e., the E2E components excluding the encoder, to\nminimize a cross-entropy loss. To make ILMA effective, it is essential to train\nthe E2E model with an internal LM loss besides the standard E2E loss.\nFurthermore, we propose to regularize ILMA by minimizing the Kullback-Leibler\ndivergence between the output distributions of the adapted and unadapted\ninternal LMs. ILMA is the most effective when we update only the last linear\nlayer of the joint network. ILMA enables a fast text-only adaptation of the E2E\nmodel without increasing the run-time computational cost. Experimented with\n30K-hour trained transformer transducer models, ILMA achieves up to 34.9%\nrelative word error rate reduction from the unadapted baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1\">Zhong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaur_Y/0/1/0/all/0/1\">Yashesh Gaur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanda_N/0/1/0/all/0/1\">Naoyuki Kanda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yifan Gong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IconQA: A New Benchmark for Abstract Diagram Understanding and Visual Language Reasoning. (arXiv:2110.13214v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.13214","description":"<p>Current visual question answering (VQA) tasks mainly consider answering\nhuman-annotated questions for natural images. However, aside from natural\nimages, abstract diagrams with semantic richness are still understudied in\nvisual understanding and reasoning research. In this work, we introduce a new\nchallenge of Icon Question Answering (IconQA) with the goal of answering a\nquestion in an icon image context. We release IconQA, a large-scale dataset\nthat consists of 107,439 questions and three sub-tasks: multi-image-choice,\nmulti-text-choice, and filling-in-the-blank. The IconQA dataset is inspired by\nreal-world diagram word problems that highlight the importance of abstract\ndiagram understanding and comprehensive cognitive reasoning. Thus, IconQA\nrequires not only perception skills like object recognition and text\nunderstanding, but also diverse cognitive reasoning skills, such as geometric\nreasoning, commonsense reasoning, and arithmetic reasoning. To facilitate\npotential IconQA models to learn semantic representations for icon images, we\nfurther release an icon dataset Icon645 which contains 645,687 colored icons on\n377 classes. We conduct extensive user studies and blind experiments and\nreproduce a wide range of advanced VQA methods to benchmark the IconQA task.\nAlso, we develop a strong IconQA baseline Patch-TRM that applies a pyramid\ncross-modal Transformer with input diagram embeddings pre-trained on the icon\ndataset. IconQA and Icon645 are available at https://iconqa.github.io.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1\">Pan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_L/0/1/0/all/0/1\">Liang Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiaqi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_T/0/1/0/all/0/1\">Tony Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yizhou Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Song-Chun Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Fine-Grained Reasoning for Fake News Detection. (arXiv:2110.15064v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.15064","description":"<p>The detection of fake news often requires sophisticated reasoning skills,\nsuch as logically combining information by considering word-level subtle clues.\nIn this paper, we move towards fine-grained reasoning for fake news detection\nby better reflecting the logical processes of human thinking and enabling the\nmodeling of subtle clues. In particular, we propose a fine-grained reasoning\nframework by following the human information-processing model, introduce a\nmutual-reinforcement-based method for incorporating human knowledge about which\nevidence is more important, and design a prior-aware bi-channel kernel graph\nnetwork to model subtle differences between pieces of evidence. Extensive\nexperiments show that our model outperforms the state-of-the-art methods and\ndemonstrate the explainability of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yiqiao Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiting Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1\">Ruichao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yizhou Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_H/0/1/0/all/0/1\">Hao Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xing Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Low-Cost Algorithmic Recourse for Users With Uncertain Cost Functions. (arXiv:2111.01235v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2111.01235","description":"<p>People affected by machine learning model decisions may benefit greatly from\naccess to recourses, i.e. suggestions about what features they could change to\nreceive a more favorable decision from the model. Current approaches try to\noptimize for the cost incurred by users when adopting a recourse, but they\nassume that all users share the same cost function. This is an unrealistic\nassumption because users might have diverse preferences about their willingness\nto change certain features. In this work, we introduce a new method for\nidentifying recourse sets for users which does not assume that users'\npreferences are known in advance. We propose an objective function, Expected\nMinimum Cost (EMC), based on two key ideas: (1) when presenting a set of\noptions to a user, there only needs to be one low-cost solution that the user\ncould adopt; (2) when we do not know the user's true cost function, we can\napproximately optimize for user satisfaction by first sampling plausible cost\nfunctions from a distribution, then finding a recourse set that achieves a good\ncost for these samples. We optimize EMC with a novel discrete optimization\nalgorithm, Cost Optimized Local Search (COLS), which is guaranteed to improve\nthe recourse set quality over iterations. Experimental evaluation on popular\nreal-world datasets with simulated users demonstrates that our method satisfies\nup to 25.89 percentage points more users compared to strong baseline methods,\nwhile, the human evaluation shows that our recourses are preferred more than\ntwice as often as the strongest baseline recourses. Finally, using standard\nfairness metrics we show that our method can provide more fair solutions across\ndemographic groups than baselines. We provide our source code at:\nhttps://github.com/prateeky2806/EMC-COLS-recourse\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yadav_P/0/1/0/all/0/1\">Prateek Yadav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hase_P/0/1/0/all/0/1\">Peter Hase</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conversational speech recognition leveraging effective fusion methods for cross-utterance language modeling. (arXiv:2111.03333v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.03333","description":"<p>Conversational speech normally is embodied with loose syntactic structures at\nthe utterance level but simultaneously exhibits topical coherence relations\nacross consecutive utterances. Prior work has shown that capturing longer\ncontext information with a recurrent neural network or long short-term memory\nlanguage model (LM) may suffer from the recent bias while excluding the\nlong-range context. In order to capture the long-term semantic interactions\namong words and across utterances, we put forward disparate conversation\nhistory fusion methods for language modeling in automatic speech recognition\n(ASR) of conversational speech. Furthermore, a novel audio-fusion mechanism is\nintroduced, which manages to fuse and utilize the acoustic embeddings of a\ncurrent utterance and the semantic content of its corresponding conversation\nhistory in a cooperative way. To flesh out our ideas, we frame the ASR N-best\nhypothesis rescoring task as a prediction problem, leveraging BERT, an iconic\npre-trained LM, as the ingredient vehicle to facilitate selection of the oracle\nhypothesis from a given N-best hypothesis list. Empirical experiments conducted\non the AMI benchmark dataset seem to demonstrate the feasibility and efficacy\nof our methods in relation to some current top-of-line methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1\">Bi-Cheng Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hsin-Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiu_S/0/1/0/all/0/1\">Shih-Hsuan Chiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiu_H/0/1/0/all/0/1\">Hsuan-Sheng Chiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Berlin Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conformer-based Hybrid ASR System for Switchboard Dataset. (arXiv:2111.03442v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.03442","description":"<p>The recently proposed conformer architecture has been successfully used for\nend-to-end automatic speech recognition (ASR) architectures achieving\nstate-of-the-art performance on different datasets. To our best knowledge, the\nimpact of using conformer acoustic model for hybrid ASR is not investigated. In\nthis paper, we present and evaluate a competitive conformer-based hybrid model\ntraining recipe. We study different training aspects and methods to improve\nword-error-rate as well as to increase training speed. We apply time\ndownsampling methods for efficient training and use transposed convolutions to\nupsample the output sequence again. We conduct experiments on Switchboard 300h\ndataset and our conformer-based hybrid model achieves competitive results\ncompared to other architectures. It generalizes very well on Hub5'01 test set\nand outperforms the BLSTM-based hybrid model significantly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeineldeen_M/0/1/0/all/0/1\">Mohammad Zeineldeen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jingjing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luscher_C/0/1/0/all/0/1\">Christoph L&#xfc;scher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michel_W/0/1/0/all/0/1\">Wilfried Michel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gerstenberger_A/0/1/0/all/0/1\">Alexander Gerstenberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schluter_R/0/1/0/all/0/1\">Ralf Schl&#xfc;ter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ney_H/0/1/0/all/0/1\">Hermann Ney</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Grained Vision Language Pre-Training: Aligning Texts with Visual Concepts. (arXiv:2111.08276v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.08276","description":"<p>Most existing methods in vision language pre-training rely on object-centric\nfeatures extracted through object detection, and make fine-grained alignments\nbetween the extracted features and texts. We argue that object detection may\nnot be necessary for vision language pre-training. To this end, we propose a\nnew method called X-VLM to perform `multi-grained vision language\npre-training.' The key of learning multi-grained alignments is to locate visual\nconcepts in the image given the associated texts, and in the meantime align the\ntexts with the visual concepts, where the alignments are in multi-granularity.\nExperimental results show that X-VLM effectively leverages the learned\nalignments to many downstream vision language tasks and consistently\noutperforms state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1\">Yan Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinsong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Why KDAC? A general activation function for knowledge discovery. (arXiv:2111.13858v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2111.13858","description":"<p>Named entity recognition based on deep learning (DNER) can effectively mine\nexpected knowledge from large-scale unstructured and semi-structured text, and\nhas gradually become the paradigm of knowledge discovery. Currently, Tanh, ReLU\nand Sigmoid dominate DNER, however, these activation functions failed to treat\ngradient vanishing, no negative output or non-differentiable existence, which\nmay impede DNER's exploration of knowledge caused by the omission and the\nincomplete representation of latent semantics. To surmount the non-negligible\nobstacle, we present a novel and general activation function termed KDAC.\nDetailly, KDAC is a thought that can aggregate and inherit the merits of Tanh\nand ReLU since they are widely leveraged in various knowledge domains. The\npositive region corresponds to an adaptive linear design encouraged by ReLU.\nThe negative region considers the interaction between exponent and linearity to\nsurmount the obstacle of gradient vanishing and no negative value. Crucially,\nthe non-differentiable points are alerted and eliminated by a smooth\napproximation. We perform experiments based on BERT-BiLSTM-CNN-CRF model on six\nbenchmark datasets containing different domain knowledge, such as Weibo,\nClinical, E-commerce, Resume, HAZOP and People's daily. The experimental\nresults show that KDAC is advanced and effective, and can provide more\ngeneralized activation to stimulate the performance of DNER. We hope that KDAC\ncan be exploited as a promising alternative activation function in DNER to\ndevote itself to the construction of knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhenhua Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_D/0/1/0/all/0/1\">Dong Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Haozhe Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fanglin Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AutoDistil: Few-shot Task-agnostic Neural Architecture Search for Distilling Large Language Models. (arXiv:2201.12507v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.12507","description":"<p>Knowledge distillation (KD) methods compress large models into smaller\nstudents with manually-designed student architectures given pre-specified\ncomputational cost. This requires several trials to find a viable student, and\nfurther repeating the process for each student or computational budget change.\nWe use Neural Architecture Search (NAS) to automatically distill several\ncompressed students with variable cost from a large model. Current works train\na single SuperLM consisting of millions of subnetworks with weight-sharing,\nresulting in interference between subnetworks of different sizes. Our framework\nAutoDistil addresses above challenges with the following steps: (a)\nIncorporates inductive bias and heuristics to partition Transformer search\nspace into K compact sub-spaces (K=3 for typical student sizes of base, small\nand tiny); (b) Trains one SuperLM for each sub-space using task-agnostic\nobjective (e.g., self-attention distillation) with weight-sharing of students;\n(c) Lightweight search for the optimal student without re-training. Fully\ntask-agnostic training and search allow students to be reused for fine-tuning\non any downstream task. Experiments on GLUE benchmark against state-of-the-art\nKD and NAS methods demonstrate AutoDistil to outperform leading compression\ntechniques with upto 2.7x reduction in computational cost and negligible loss\nin task performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Dongkuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_S/0/1/0/all/0/1\">Subhabrata Mukherjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaodong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dey_D/0/1/0/all/0/1\">Debadeepta Dey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenhui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1\">Ahmed Hassan Awadallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A multi-task semi-supervised framework for Text2Graph & Graph2Text. (arXiv:2202.06041v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.06041","description":"<p>The Artificial Intelligence industry regularly develops applications that\nmostly rely on Knowledge Bases, a data repository about specific, or general,\ndomains, usually represented in a graph shape. Similar to other databases, they\nface two main challenges: information ingestion and information retrieval. We\napproach these challenges by jointly learning graph extraction from text and\ntext generation from graphs. The proposed solution, a T5 architecture, is\ntrained in a multi-task semi-supervised environment, with our collected\nnon-parallel data, following a cycle training regime. Experiments on WebNLG\ndataset show that our approach surpasses unsupervised state-of-the-art results\nin text-to-graph and graph-to-text. More relevantly, our framework is more\nconsistent across seen and unseen domains than supervised models. The resulting\nmodel can be easily trained in any new domain with non-parallel data, by simply\nadding text and graphs about it, in our cycle framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Domingo_O/0/1/0/all/0/1\">Oriol Domingo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Costa_jussa_M/0/1/0/all/0/1\">Marta R. Costa-juss&#xe0;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Escolano_C/0/1/0/all/0/1\">Carlos Escolano</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distribution augmentation for low-resource expressive text-to-speech. (arXiv:2202.06409v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2202.06409","description":"<p>This paper presents a novel data augmentation technique for text-to-speech\n(TTS), that allows to generate new (text, audio) training examples without\nrequiring any additional data. Our goal is to increase diversity of text\nconditionings available during training. This helps to reduce overfitting,\nespecially in low-resource settings. Our method relies on substituting text and\naudio fragments in a way that preserves syntactical correctness. We take\nadditional measures to ensure that synthesized speech does not contain\nartifacts caused by combining inconsistent audio samples. The perceptual\nevaluations show that our method improves speech quality over a number of\ndatasets, speakers, and TTS architectures. We also demonstrate that it greatly\nimproves robustness of attention-based TTS models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lajszczak_M/0/1/0/all/0/1\">Mateusz Lajszczak</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Prasad_A/0/1/0/all/0/1\">Animesh Prasad</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Korlaar_A/0/1/0/all/0/1\">Arent van Korlaar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bollepalli_B/0/1/0/all/0/1\">Bajibabu Bollepalli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bonafonte_A/0/1/0/all/0/1\">Antonio Bonafonte</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Joly_A/0/1/0/all/0/1\">Arnaud Joly</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nicolis_M/0/1/0/all/0/1\">Marco Nicolis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Moinet_A/0/1/0/all/0/1\">Alexis Moinet</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Drugman_T/0/1/0/all/0/1\">Thomas Drugman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wood_T/0/1/0/all/0/1\">Trevor Wood</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sokolova_E/0/1/0/all/0/1\">Elena Sokolova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"When Did It Happen? Duration-informed Temporal Localization of Narrated Actions in Vlogs. (arXiv:2202.08138v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.08138","description":"<p>We consider the task of temporal human action localization in lifestyle\nvlogs. We introduce a novel dataset consisting of manual annotations of\ntemporal localization for 13,000 narrated actions in 1,200 video clips. We\npresent an extensive analysis of this data, which allows us to better\nunderstand how the language and visual modalities interact throughout the\nvideos. We propose a simple yet effective method to localize the narrated\nactions based on their expected duration. Through several experiments and\nanalyses, we show that our method brings complementary information with respect\nto previous methods, and leads to improvements over previous work for the task\nof temporal action localization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ignat_O/0/1/0/all/0/1\">Oana Ignat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castro_S/0/1/0/all/0/1\">Santiago Castro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuhang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Jiajun Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_D/0/1/0/all/0/1\">Dandan Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1\">Rada Mihalcea</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SGPT: GPT Sentence Embeddings for Semantic Search. (arXiv:2202.08904v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.08904","description":"<p>GPT transformers are the largest language models available, yet semantic\nsearch is dominated by BERT transformers. We present SGPT-BE and SGPT-CE for\napplying GPT models as Bi-Encoders or Cross-Encoders to symmetric or asymmetric\nsearch.\n</p>\n<p>SGPT-BE produces semantically meaningful sentence embeddings by contrastive\nfine-tuning of only bias tensors and a novel pooling method. A 5.8 billion\nparameter SGPT-BE outperforms the best available sentence embeddings by 6%\nsetting a new state-of-the-art on BEIR. It outperforms the concurrently\nproposed OpenAI Embeddings of the 175B Davinci endpoint, which fine-tunes\n250,000 times more parameters.\n</p>\n<p>SGPT-CE uses log probabilities from GPT models without any fine-tuning. A 6.1\nbillion parameter SGPT-CE sets an unsupervised state-of-the-art on BEIR. It\nbeats the supervised state-of-the-art on 7 datasets, but significantly loses on\nother datasets. We show how this can be alleviated by adapting the prompt.\n</p>\n<p>SGPT-BE and SGPT-CE performance scales with model size. Yet, increased\nlatency, storage and compute costs should be considered. Code, models and\nresult files are freely available at https://github.com/Muennighoff/sgpt.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Muennighoff_N/0/1/0/all/0/1\">Niklas Muennighoff</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VLP: A Survey on Vision-Language Pre-training. (arXiv:2202.09061v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.09061","description":"<p>In the past few years, the emergence of pre-training models has brought\nuni-modal fields such as computer vision (CV) and natural language processing\n(NLP) to a new era. Substantial works have shown they are beneficial for\ndownstream uni-modal tasks and avoid training a new model from scratch. So can\nsuch pre-trained models be applied to multi-modal tasks? Researchers have\nexplored this problem and made significant progress. This paper surveys recent\nadvances and new frontiers in vision-language pre-training (VLP), including\nimage-text and video-text pre-training. To give readers a better overall grasp\nof VLP, we first review its recent advances from five aspects: feature\nextraction, model architecture, pre-training objectives, pre-training datasets,\nand downstream tasks. Then, we summarize the specific VLP models in detail.\nFinally, we discuss the new frontiers in VLP. To the best of our knowledge,\nthis is the first survey on VLP. We hope that this survey can shed light on\nfuture research in the VLP field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Feilong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Duzhen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_M/0/1/0/all/0/1\">Minglun Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiuyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jing Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shuang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Bo Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-02-21T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/","dc":"http://purl.org/dc/elements/1.1/","content":"http://purl.org/rss/1.0/modules/content/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Snowflake Point Deconvolution for Point Cloud Completion and Generation with Skip-Transformer. (arXiv:2202.09367v1 [cs.CV])","link":"http://arxiv.org/abs/2202.09367","description":"<p>Most existing point cloud completion methods suffered from discrete nature of\npoint clouds and unstructured prediction of points in local regions, which\nmakes it hard to reveal fine local geometric details. To resolve this issue, we\npropose SnowflakeNet with Snowflake Point Deconvolution (SPD) to generate the\ncomplete point clouds. SPD models the generation of complete point clouds as\nthe snowflake-like growth of points, where the child points are progressively\ngenerated by splitting their parent points after each SPD. Our insight of\nrevealing detailed geometry is to introduce skip-transformer in SPD to learn\npoint splitting patterns which can fit local regions the best. Skip-transformer\nleverages attention mechanism to summarize the splitting patterns used in\nprevious SPD layer to produce the splitting in current SPD layer. The locally\ncompact and structured point clouds generated by SPD precisely reveal the\nstructure characteristic of 3D shape in local patches, which enables us to\npredict highly detailed geometries. Moreover, since SPD is a general operation,\nwhich is not limited to completion, we further explore the applications of SPD\non other generative tasks, including point cloud auto-encoding, generation,\nsingle image reconstruction and upsampling. Our experimental results outperform\nthe state-of-the-art methods under widely used benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiang_P/0/1/0/all/0/1\">Peng Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_X/0/1/0/all/0/1\">Xin Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yu-Shen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yan-Pei Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_P/0/1/0/all/0/1\">Pengfei Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1\">Wen Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1\">Zhizhong Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Molecular Prior Distribution for Bayesian Inference Based on Wilson Statistics. (arXiv:2202.09388v1 [q-bio.QM])","link":"http://arxiv.org/abs/2202.09388","description":"<p>Background and Objective: Wilson statistics describe well the power spectrum\nof proteins at high frequencies. Therefore, it has found several applications\nin structural biology, e.g., it is the basis for sharpening steps used in\ncryogenic electron microscopy (cryo-EM). A recent paper gave the first rigorous\nproof of Wilson statistics based on a formalism of Wilson's original argument.\nThis new analysis also leads to statistical estimates of the scattering\npotential of proteins that reveal a correlation between neighboring Fourier\ncoefficients. Here we exploit these estimates to craft a novel prior that can\nbe used for Bayesian inference of molecular structures. Methods: We describe\nthe properties of the prior and the computation of its hyperparameters. We then\nevaluate the prior on two synthetic linear inverse problems, and compare\nagainst a popular prior in cryo-EM reconstruction at a range of SNRs. Results:\nWe show that the new prior effectively suppresses noise and fills-in low SNR\nregions in the spectral domain. Furthermore, it improves the resolution of\nestimates on the problems considered for a wide range of SNR and produces\nFourier Shell Correlation curves that are insensitive to masking effects.\nConclusions: We analyze the assumptions in the model, discuss relations to\nother regularization strategies, and postulate on potential implications for\nstructure determination in cryo-EM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Gilles_M/0/1/0/all/0/1\">Marc Aur&#xe8;le Gilles</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Singer_A/0/1/0/all/0/1\">Amit Singer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Representations Robust to Group Shifts and Adversarial Examples. (arXiv:2202.09446v1 [cs.LG])","link":"http://arxiv.org/abs/2202.09446","description":"<p>Despite the high performance achieved by deep neural networks on various\ntasks, extensive studies have demonstrated that small tweaks in the input could\nfail the model predictions. This issue of deep neural networks has led to a\nnumber of methods to improve model robustness, including adversarial training\nand distributionally robust optimization. Though both of these two methods are\ngeared towards learning robust models, they have essentially different\nmotivations: adversarial training attempts to train deep neural networks\nagainst perturbations, while distributional robust optimization aims at\nimproving model performance on the most difficult \"uncertain distributions\". In\nthis work, we propose an algorithm that combines adversarial training and group\ndistribution robust optimization to improve robust representation learning.\nExperiments on three image benchmark datasets illustrate that the proposed\nmethod achieves superior results on robust metrics without sacrificing much of\nthe standard measures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chiu_M/0/1/0/all/0/1\">Ming-Chang Chiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xuezhe Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modern Augmented Reality: Applications, Trends, and Future Directions. (arXiv:2202.09450v1 [cs.CV])","link":"http://arxiv.org/abs/2202.09450","description":"<p>Augmented reality (AR) is one of the relatively old, yet trending areas in\nthe intersection of computer vision and computer graphics with numerous\napplications in several areas, from gaming and entertainment, to education and\nhealthcare. Although it has been around for nearly fifty years, it has seen a\nlot of interest by the research community in the recent years, mainly because\nof the huge success of deep learning models for various computer vision and AR\napplications, which made creating new generations of AR technologies possible.\nThis work tries to provide an overview of modern augmented reality, from both\napplication-level and technical perspective. We first give an overview of main\nAR applications, grouped into more than ten categories. We then give an\noverview of around 100 recent promising machine learning based works developed\nfor AR systems, such as deep learning works for AR shopping (clothing, makeup),\nAR based image filters (such as Snapchat's lenses), AR animations, and more. In\nthe end we discuss about some of the current challenges in AR domain, and the\nfuture directions in this area.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Minaee_S/0/1/0/all/0/1\">Shervin Minaee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Shuicheng Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SAGE: SLAM with Appearance and Geometry Prior for Endoscopy. (arXiv:2202.09487v1 [cs.CV])","link":"http://arxiv.org/abs/2202.09487","description":"<p>In endoscopy, many applications (e.g., surgical navigation) would benefit\nfrom a real-time method that can simultaneously track the endoscope and\nreconstruct the dense 3D geometry of the observed anatomy from a monocular\nendoscopic video. To this end, we develop a Simultaneous Localization and\nMapping system by combining the learning-based appearance and optimizable\ngeometry priors and factor graph optimization. The appearance and geometry\npriors are explicitly learned in an end-to-end differentiable training pipeline\nto master the task of pair-wise image alignment, one of the core components of\nthe SLAM system. In our experiments, the proposed SLAM system is shown to\nrobustly handle the challenges of texture scarceness and illumination variation\nthat are commonly seen in endoscopy. The system generalizes well to unseen\nendoscopes and subjects and performs favorably compared with a state-of-the-art\nfeature-based SLAM system. The code repository is available at\nhttps://github.com/lppllppl920/SAGE-SLAM.git.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xingtong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhaoshuo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ishii_M/0/1/0/all/0/1\">Masaru Ishii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hager_G/0/1/0/all/0/1\">Gregory D. Hager</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taylor_R/0/1/0/all/0/1\">Russell H. Taylor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Unberath_M/0/1/0/all/0/1\">Mathias Unberath</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Highlighting Object Category Immunity for the Generalization of Human-Object Interaction Detection. (arXiv:2202.09492v1 [cs.CV])","link":"http://arxiv.org/abs/2202.09492","description":"<p>Human-Object Interaction (HOI) detection plays a core role in activity\nunderstanding. As a compositional learning problem (human-verb-object),\nstudying its generalization matters. However, widely-used metric mean average\nprecision (mAP) fails to model the compositional generalization well. Thus, we\npropose a novel metric, mPD (mean Performance Degradation), as a complementary\nof mAP to evaluate the performance gap among compositions of different objects\nand the same verb. Surprisingly, mPD reveals that previous methods usually\ngeneralize poorly. With mPD as a cue, we propose Object Category (OC) Immunity\nto boost HOI generalization. The idea is to prevent model from learning\nspurious object-verb correlations as a short-cut to over-fit the train set. To\nachieve OC-immunity, we propose an OC-immune network that decouples the inputs\nfrom OC, extracts OC-immune representations, and leverages uncertainty\nquantification to generalize to unseen objects. In both conventional and\nzero-shot experiments, our method achieves decent improvements. To fully\nevaluate the generalization, we design a new and more difficult benchmark, on\nwhich we present significant advantage. The code is available at\nhttps://github.com/Foruck/OC-Immunity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xinpeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yong-Lu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Cewu Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PMP-Net++: Point Cloud Completion by Transformer-Enhanced Multi-step Point Moving Paths. (arXiv:2202.09507v1 [cs.CV])","link":"http://arxiv.org/abs/2202.09507","description":"<p>Point cloud completion concerns to predict missing part for incomplete 3D\nshapes. A common strategy is to generate complete shape according to incomplete\ninput. However, unordered nature of point clouds will degrade generation of\nhigh-quality 3D shapes, as detailed topology and structure of unordered points\nare hard to be captured during the generative process using an extracted latent\ncode. We address this problem by formulating completion as point cloud\ndeformation process. Specifically, we design a novel neural network, named\nPMP-Net++, to mimic behavior of an earth mover. It moves each point of\nincomplete input to obtain a complete point cloud, where total distance of\npoint moving paths (PMPs) should be the shortest. Therefore, PMP-Net++ predicts\nunique PMP for each point according to constraint of point moving distances.\nThe network learns a strict and unique correspondence on point-level, and thus\nimproves quality of predicted complete shape. Moreover, since moving points\nheavily relies on per-point features learned by network, we further introduce a\ntransformer-enhanced representation learning network, which significantly\nimproves completion performance of PMP-Net++. We conduct comprehensive\nexperiments in shape completion, and further explore application on point cloud\nup-sampling, which demonstrate non-trivial improvement of PMP-Net++ over\nstate-of-the-art point cloud completion/up-sampling methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wen_X/0/1/0/all/0/1\">Xin Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_P/0/1/0/all/0/1\">Peng Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yan-Pei Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_P/0/1/0/all/0/1\">Pengfei Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1\">Wen Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yu-Shen Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SPNet: A novel deep neural network for retinal vessel segmentation based on shared decoder and pyramid-like loss. (arXiv:2202.09515v1 [eess.IV])","link":"http://arxiv.org/abs/2202.09515","description":"<p>Segmentation of retinal vessel images is critical to the diagnosis of\nretinopathy. Recently, convolutional neural networks have shown significant\nability to extract the blood vessel structure. However, it remains challenging\nto refined segmentation for the capillaries and the edges of retinal vessels\ndue to thickness inconsistencies and blurry boundaries. In this paper, we\npropose a novel deep neural network for retinal vessel segmentation based on\nshared decoder and pyramid-like loss (SPNet) to address the above problems.\nSpecifically, we introduce a decoder-sharing mechanism to capture multi-scale\nsemantic information, where feature maps at diverse scales are decoded through\na sequence of weight-sharing decoder modules. Also, to strengthen\ncharacterization on the capillaries and the edges of blood vessels, we define a\nresidual pyramid architecture which decomposes the spatial information in the\ndecoding phase. A pyramid-like loss function is designed to compensate possible\nsegmentation errors progressively. Experimental results on public benchmarks\nshow that the proposed method outperforms the backbone network and the\nstate-of-the-art methods, especially in the regions of the capillaries and the\nvessel contours. In addition, performances on cross-datasets verify that SPNet\nshows stronger generalization ability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Xu_G/0/1/0/all/0/1\">Geng-Xin Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ren_C/0/1/0/all/0/1\">Chuan-Xian Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"C2N: Practical Generative Noise Modeling for Real-World Denoising. (arXiv:2202.09533v1 [cs.CV])","link":"http://arxiv.org/abs/2202.09533","description":"<p>Learning-based image denoising methods have been bounded to situations where\nwell-aligned noisy and clean images are given, or samples are synthesized from\npredetermined noise models, e.g., Gaussian. While recent generative noise\nmodeling methods aim to simulate the unknown distribution of real-world noise,\nseveral limitations still exist. In a practical scenario, a noise generator\nshould learn to simulate the general and complex noise distribution without\nusing paired noisy and clean images. However, since existing methods are\nconstructed on the unrealistic assumption of real-world noise, they tend to\ngenerate implausible patterns and cannot express complicated noise maps.\nTherefore, we introduce a Clean-to-Noisy image generation framework, namely\nC2N, to imitate complex real-world noise without using any paired examples. We\nconstruct the noise generator in C2N accordingly with each component of\nreal-world noise characteristics to express a wide range of noise accurately.\nCombined with our C2N, conventional denoising CNNs can be trained to outperform\nexisting unsupervised methods on challenging real-world benchmarks by a large\nmargin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jang_G/0/1/0/all/0/1\">Geonwoon Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_W/0/1/0/all/0/1\">Wooseok Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Son_S/0/1/0/all/0/1\">Sanghyun Son</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kyoung Mu Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BP-Triplet Net for Unsupervised Domain Adaptation: A Bayesian Perspective. (arXiv:2202.09541v1 [cs.CV])","link":"http://arxiv.org/abs/2202.09541","description":"<p>Triplet loss, one of the deep metric learning (DML) methods, is to learn the\nembeddings where examples from the same class are closer than examples from\ndifferent classes. Motivated by DML, we propose an effective BP-Triplet Loss\nfor unsupervised domain adaption (UDA) from the perspective of Bayesian\nlearning and we name the model as BP-Triplet Net. In previous metric learning\nbased methods for UDA, sample pairs across domains are treated equally, which\nis not appropriate due to the domain bias. In our work, considering the\ndifferent importance of pair-wise samples for both feature learning and domain\nalignment, we deduce our BP-Triplet loss for effective UDA from the perspective\nof Bayesian learning. Our BP-Triplet loss adjusts the weights of pair-wise\nsamples in intra domain and inter domain. Especially, it can self attend to the\nhard pairs (including hard positive pair and hard negative pair). Together with\nthe commonly used adversarial loss for domain alignment, the quality of target\npseudo labels is progressively improved. Our method achieved low joint error of\nthe ideal source and target hypothesis. The expected target error can then be\nupper bounded following Ben-David s theorem. Comprehensive evaluations on five\nbenchmark datasets, handwritten digits, Office31, ImageCLEF-DA, Office-Home and\nVisDA-2017 demonstrate the effectiveness of the proposed approach for UDA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shanshan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pichao Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Going Deeper into Recognizing Actions in Dark Environments: A Comprehensive Benchmark Study. (arXiv:2202.09545v1 [cs.CV])","link":"http://arxiv.org/abs/2202.09545","description":"<p>While action recognition (AR) has gained large improvements with the\nintroduction of large-scale video datasets and the development of deep neural\nnetworks, AR models robust to challenging environments in real-world scenarios\nare still under-explored. We focus on the task of action recognition in dark\nenvironments, which can be applied to fields such as surveillance and\nautonomous driving at night. Intuitively, current deep networks along with\nvisual enhancement techniques should be able to handle AR in dark environments,\nhowever, it is observed that this is not always the case in practice. To dive\ndeeper into exploring solutions for AR in dark environments, we launched the\nUG2+ Challenge Track 2 (UG2-2) in IEEE CVPR 2021, with a goal of evaluating and\nadvancing the robustness of AR models in dark environments. The challenge\nbuilds and expands on top of a novel ARID dataset, the first dataset for the\ntask of dark video AR, and guides models to tackle such a task in both fully\nand semi-supervised manners. Baseline results utilizing current AR models and\nenhancement methods are reported, justifying the challenging nature of this\ntask with substantial room for improvements. Thanks to the active participation\nfrom the research community, notable advances have been made in participants'\nsolutions, while analysis of these solutions helped better identify possible\ndirections to tackle the challenge of AR in dark environments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yuecong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianfei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_H/0/1/0/all/0/1\">Haozhi Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1\">Jianxiong Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhenghua Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoli Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhengguo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qianwen Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"student dangerous behavior detection in school. (arXiv:2202.09550v1 [cs.CV])","link":"http://arxiv.org/abs/2202.09550","description":"<p>Video surveillance systems have been installed to ensure the student safety\nin schools. However, discovering dangerous behaviors, such as fighting and\nfalling down, usually depends on untimely human observations. In this paper, we\nfocus on detecting dangerous behaviors of students automatically, which faces\nnumerous challenges, such as insufficient datasets, confusing postures,\nkeyframes detection and prompt response. To address these challenges, we first\nbuild a danger behavior dataset with locations and labels from surveillance\nvideos, and transform action recognition of long videos to an object detection\ntask that avoids keyframes detection. Then, we propose a novel end-to-end\ndangerous behavior detection method, named DangerDet, that combines multi-scale\nbody features and keypoints-based pose features. We could improve the accuracy\nof behavior classification due to the highly correlation between pose and\nbehavior. On our dataset, DangerDet achieves 71.0\\% mAP with about 11 FPS. It\nkeeps a better balance between the accuracy and time cost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Huayi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_F/0/1/0/all/0/1\">Fei Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Hongtao Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Holistic Attention-Fusion Adversarial Network for Single Image Defogging. (arXiv:2202.09553v1 [cs.CV])","link":"http://arxiv.org/abs/2202.09553","description":"<p>Adversarial learning-based image defogging methods have been extensively\nstudied in computer vision due to their remarkable performance. However, most\nexisting methods have limited defogging capabilities for real cases because\nthey are trained on the paired clear and synthesized foggy images of the same\nscenes. In addition, they have limitations in preserving vivid color and rich\ntextual details in defogging. To address these issues, we develop a novel\ngenerative adversarial network, called holistic attention-fusion adversarial\nnetwork (HAAN), for single image defogging. HAAN consists of a Fog2Fogfree\nblock and a Fogfree2Fog block. In each block, there are three learning-based\nmodules, namely, fog removal, color-texture recovery, and fog synthetic, that\nare constrained each other to generate high quality images. HAAN is designed to\nexploit the self-similarity of texture and structure information by learning\nthe holistic channel-spatial feature correlations between the foggy image with\nits several derived images. Moreover, in the fog synthetic module, we utilize\nthe atmospheric scattering model to guide it to improve the generative quality\nby focusing on an atmospheric light optimization with a novel sky segmentation\nnetwork. Extensive experiments on both synthetic and real-world datasets show\nthat HAAN outperforms state-of-the-art defogging methods in terms of\nquantitative accuracy and subjective visual quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Cheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_R/0/1/0/all/0/1\">Rui Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1\">Tao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Z/0/1/0/all/0/1\">Zixiang Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SODA: Site Object Detection dAtaset for Deep Learning in Construction. (arXiv:2202.09554v1 [cs.CV])","link":"http://arxiv.org/abs/2202.09554","description":"<p>Computer vision-based deep learning object detection algorithms have been\ndeveloped sufficiently powerful to support the ability to recognize various\nobjects. Although there are currently general datasets for object detection,\nthere is still a lack of large-scale, open-source dataset for the construction\nindustry, which limits the developments of object detection algorithms as they\ntend to be data-hungry. Therefore, this paper develops a new large-scale image\ndataset specifically collected and annotated for the construction site, called\nSite Object Detection dAtaset (SODA), which contains 15 kinds of object classes\ncategorized by workers, materials, machines, and layout. Firstly, more than\n20,000 images were collected from multiple construction sites in different site\nconditions, weather conditions, and construction phases, which covered\ndifferent angles and perspectives. After careful screening and processing,\n19,846 images including 286,201 objects were then obtained and annotated with\nlabels in accordance with predefined categories. Statistical analysis shows\nthat the developed dataset is advantageous in terms of diversity and volume.\nFurther evaluation with two widely-adopted object detection algorithms based on\ndeep learning (YOLO v3/ YOLO v4) also illustrates the feasibility of the\ndataset for typical construction scenarios, achieving a maximum mAP of 81.47%.\nIn this manner, this research contributes a large-scale image dataset for the\ndevelopment of deep learning-based object detection methods in the construction\nindustry and sets up a performance benchmark for further evaluation of\ncorresponding algorithms in this area.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Duan_R/0/1/0/all/0/1\">Rui Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_H/0/1/0/all/0/1\">Hui Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_M/0/1/0/all/0/1\">Mao Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yichuan Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jiarui Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HDAM: Heuristic Difference Attention Module for Convolutional Neural Networks. (arXiv:2202.09556v1 [cs.CV])","link":"http://arxiv.org/abs/2202.09556","description":"<p>The attention mechanism is one of the most important priori knowledge to\nenhance convolutional neural networks. Most attention mechanisms are bound to\nthe convolutional layer and use local or global contextual information to\nrecalibrate the input. This is a popular attention strategy design method.\nGlobal contextual information helps the network to consider the overall\ndistribution, while local contextual information is more general. The\ncontextual information makes the network pay attention to the mean or maximum\nvalue of a particular receptive field. Different from the most attention\nmechanism, this article proposes a novel attention mechanism with the heuristic\ndifference attention module, HDAM. HDAM's input recalibration is based on the\ndifference between the local and global contextual information instead of the\nmean and maximum values. At the same time, to make different layers have a more\nsuitable local receptive field size and increase the exibility of the local\nreceptive field design, we use genetic algorithm to heuristically produce local\nreceptive fields. First, HDAM extracts the mean value of the global and local\nreceptive fields as the corresponding contextual information. Then the\ndifference between the global and local contextual information is calculated.\nFinally HDAM uses this difference to recalibrate the input. In addition, we use\nthe heuristic ability of genetic algorithm to search for the local receptive\nfield size of each layer. Our experiments on CIFAR-10 and CIFAR-100 show that\nHDAM can use fewer parameters than other attention mechanisms to achieve higher\naccuracy. We implement HDAM with the Python library, Pytorch, and the code and\nmodels will be publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xue_Y/0/1/0/all/0/1\">Yu Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Ziming Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Priming Cross-Session Motor Imagery Classification with A Universal Deep Domain Adaptation Framework. (arXiv:2202.09559v1 [cs.CV])","link":"http://arxiv.org/abs/2202.09559","description":"<p>Motor imagery (MI) is a common brain computer interface (BCI) paradigm. EEG\nis non-stationary with low signal-to-noise, classifying motor imagery tasks of\nthe same participant from different EEG recording sessions is generally\nchallenging, as EEG data distribution may vary tremendously among different\nacquisition sessions. Although it is intuitive to consider the cross-session MI\nclassification as a domain adaptation problem, the rationale and feasible\napproach is not elucidated. In this paper, we propose a Siamese deep domain\nadaptation (SDDA) framework for cross-session MI classification based on\nmathematical models in domain adaptation theory. The proposed framework can be\neasily applied to most existing artificial neural networks without altering the\nnetwork structure, which facilitates our method with great flexibility and\ntransferability. In the proposed framework, domain invariants were firstly\nconstructed jointly with channel normalization and Euclidean alignment. Then,\nembedding features from source and target domain were mapped into the\nReproducing Kernel Hilbert Space (RKHS) and aligned accordingly. A cosine-based\ncenter loss was also integrated into the framework to improve the\ngeneralizability of the SDDA. The proposed framework was validated with two\nclassic and popular convolutional neural networks from BCI research field\n(EEGNet and ConvNet) in two MI-EEG public datasets (BCI Competition IV IIA,\nIIB). Compared to the vanilla EEGNet and ConvNet, the proposed SDDA framework\nwas able to boost the MI classification accuracy by 15.2%, 10.2% respectively\nin IIA dataset, and 5.5%, 4.2% in IIB dataset. The final MI classification\naccuracy reached 82.01% in IIA dataset and 87.52% in IIB, which outperformed\nthe state-of-the-art methods in the literature.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Miao_Z/0/1/0/all/0/1\">Zhengqing Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menon_C/0/1/0/all/0/1\">Carlo Menon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yelong Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1\">Meirong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ming_D/0/1/0/all/0/1\">Dong Ming</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bit-wise Training of Neural Network Weights. (arXiv:2202.09571v1 [cs.LG])","link":"http://arxiv.org/abs/2202.09571","description":"<p>We introduce an algorithm where the individual bits representing the weights\nof a neural network are learned. This method allows training weights with\ninteger values on arbitrary bit-depths and naturally uncovers sparse networks,\nwithout additional constraints or regularization techniques. We show better\nresults than the standard training technique with fully connected networks and\nsimilar performance as compared to standard training for convolutional and\nresidual networks. By training bits in a selective manner we found that the\nbiggest contribution to achieving high accuracy is given by the first three\nmost significant bits, while the rest provide an intrinsic regularization. As a\nconsequence more than 90\\% of a network can be used to store arbitrary codes\nwithout affecting its accuracy. These codes may be random noise, binary files\nor even the weights of previously trained networks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ivan_C/0/1/0/all/0/1\">Cristian Ivan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diversity aware image generation. (arXiv:2202.09573v1 [cs.CV])","link":"http://arxiv.org/abs/2202.09573","description":"<p>The machine learning generative algorithms such as GAN and VAE show\nimpressive results in practice when constructing images similar to those in a\ntraining set. However, the generation of new images builds mainly on the\nunderstanding of the hidden structure of the training database followed by a\nmere sampling from a multi-dimensional normal variable. In particular each\nsample is independent from the other ones and can repeatedly propose same type\nof images. To cure this drawback we propose a kernel-based measure\nrepresentation method that can produce new objects from a given target measure\nby approximating the measure as a whole and even staying away from objects\nalready drawn from that distribution. This ensures a better variety of the\nproduced images. The method is tested on some classic machine learning\nbenchmarks.\\end{abstract}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Turinici_G/0/1/0/all/0/1\">Gabriel Turinici</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tripartite: Tackle Noisy Labels by a More Precise Partition. (arXiv:2202.09579v1 [cs.CV])","link":"http://arxiv.org/abs/2202.09579","description":"<p>Samples in large-scale datasets may be mislabeled due to various reasons, and\nDeep Neural Networks can easily over-fit to the noisy label data. To tackle\nthis problem, the key point is to alleviate the harm of these noisy labels.\nMany existing methods try to divide training data into clean and noisy subsets\nin terms of loss values, and then process the noisy label data varied. One of\nthe reasons hindering a better performance is the hard samples. As hard samples\nalways have relatively large losses whether their labels are clean or noisy,\nthese methods could not divide them precisely. Instead, we propose a Tripartite\nsolution to partition training data more precisely into three subsets: hard,\nnoisy, and clean. The partition criteria are based on the inconsistent\npredictions of two networks, and the inconsistency between the prediction of a\nnetwork and the given label. To minimize the harm of noisy labels but maximize\nthe value of noisy label data, we apply a low-weight learning on hard data and\na self-supervised learning on noisy label data without using the given labels.\nExtensive experiments demonstrate that Tripartite can filter out noisy label\ndata more precisely, and outperforms most state-of-the-art methods on five\nbenchmark datasets, especially on real-world datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xuefeng Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_L/0/1/0/all/0/1\">Longshan Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xingyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Ying Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image-to-Graph Transformers for Chemical Structure Recognition. (arXiv:2202.09580v1 [cs.CV])","link":"http://arxiv.org/abs/2202.09580","description":"<p>For several decades, chemical knowledge has been published in written text,\nand there have been many attempts to make it accessible, for example, by\ntransforming such natural language text to a structured format. Although the\ndiscovered chemical itself commonly represented in an image is the most\nimportant part, the correct recognition of the molecular structure from the\nimage in literature still remains a hard problem since they are often\nabbreviated to reduce the complexity and drawn in many different styles. In\nthis paper, we present a deep learning model to extract molecular structures\nfrom images. The proposed model is designed to transform the molecular image\ndirectly into the corresponding graph, which makes it capable of handling\nnon-atomic symbols for abbreviations. Also, by end-to-end learning approach it\ncan fully utilize many open image-molecule pair data from various sources, and\nhence it is more robust to image style variation than other tools. The\nexperimental results show that the proposed model outperforms the existing\nmodels with 17.1 % and 12.8 % relative improvement for well-known benchmark\ndatasets and large molecular images that we collected from literature,\nrespectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yoo_S/0/1/0/all/0/1\">Sanghyun Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwon_O/0/1/0/all/0/1\">Ohyun Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hoshik Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Lightweight Dual-Domain Attention Framework for Sparse-View CT Reconstruction. (arXiv:2202.09609v1 [eess.IV])","link":"http://arxiv.org/abs/2202.09609","description":"<p>Computed Tomography (CT) plays an essential role in clinical diagnosis. Due\nto the adverse effects of radiation on patients, the radiation dose is expected\nto be reduced as low as possible. Sparse sampling is an effective way, but it\nwill lead to severe artifacts on the reconstructed CT image, thus sparse-view\nCT image reconstruction has been a prevailing and challenging research area.\nWith the popularity of mobile devices, the requirements for lightweight and\nreal-time networks are increasing rapidly. In this paper, we design a novel\nlightweight network called CAGAN, and propose a dual-domain reconstruction\npipeline for parallel beam sparse-view CT. CAGAN is an adversarial\nauto-encoder, combining the Coordinate Attention unit, which preserves the\nspatial information of features. Also, the application of Shuffle Blocks\nreduces the parameters by a quarter without sacrificing its performance. In the\nRadon domain, the CAGAN learns the mapping between the interpolated data and\nfringe-free projection data. After the restored Radon data is reconstructed to\nan image, the image is sent into the second CAGAN trained for recovering the\ndetails, so that a high-quality image is obtained. Experiments indicate that\nthe CAGAN strikes an excellent balance between model complexity and\nperformance, and our pipeline outperforms the DD-Net and the DuDoNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Sun_C/0/1/0/all/0/1\">Chang Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Deng_K/0/1/0/all/0/1\">Ken Deng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yitong Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_H/0/1/0/all/0/1\">Hongwen Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Unsupervised Attentive-Adversarial Learning Framework for Single Image Deraining. (arXiv:2202.09635v1 [cs.CV])","link":"http://arxiv.org/abs/2202.09635","description":"<p>Single image deraining has been an important topic in low-level computer\nvision tasks. The atmospheric veiling effect (which is generated by rain\naccumulation, similar to fog) usually appears with the rain. Most deep\nlearning-based single image deraining methods mainly focus on rain streak\nremoval by disregarding this effect, which leads to low-quality deraining\nperformance. In addition, these methods are trained only on synthetic data,\nhence they do not take into account real-world rainy images. To address the\nabove issues, we propose a novel unsupervised attentive-adversarial learning\nframework (UALF) for single image deraining that trains on both synthetic and\nreal rainy images while simultaneously capturing both rain streaks and rain\naccumulation features. UALF consists of a Rain-fog2Clean (R2C) transformation\nblock and a Clean2Rain-fog (C2R) transformation block. In R2C, to better\ncharacterize the rain-fog fusion feature and to achieve high-quality deraining\nperformance, we employ an attention rain-fog feature extraction network (ARFE)\nto exploit the self-similarity of global and local rain-fog information by\nlearning the spatial feature correlations. Moreover, to improve the\ntransformation ability of C2R, we design a rain-fog feature decoupling and\nreorganization network (RFDR) by embedding a rainy image degradation model and\na mixed discriminator to preserve richer texture details. Extensive experiments\non benchmark rain-fog and rain datasets show that UALF outperforms\nstate-of-the-art deraining methods. We also conduct defogging performance\nevaluation experiments to further demonstrate the effectiveness of UALF\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_R/0/1/0/all/0/1\">Rui Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Cheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1\">Tao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Z/0/1/0/all/0/1\">Zixiang Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Echofilter: A Deep Learning Segmentation Model Improves the Automation, Standardization, and Timeliness for Post-Processing Echosounder Data in Tidal Energy Streams. (arXiv:2202.09648v1 [cs.LG])","link":"http://arxiv.org/abs/2202.09648","description":"<p>Understanding the abundance and distribution of fish in tidal energy streams\nis important for assessing the risk presented by the introduction of tidal\nenergy devices into the habitat. However, the impressive tidal currents that\nmake sites favorable for tidal energy development are often highly turbulent\nand entrain air into the water, complicating the interpretation of echosounder\ndata. The portion of the water column contaminated by returns from entrained\nair must be excluded from data used for biological analyses. Application of a\nsingle algorithm to identify the depth-of-penetration of entrained-air is\ninsufficient for a boundary that is discontinuous, depth-dynamic, porous, and\nwidely variable across the tidal flow speeds which can range from 0 to 5m/s.\nUsing a case study at a tidal energy demonstration site in the Bay of Fundy, we\ndescribe the development and application of deep learning models that produce a\npronounced, consistent, substantial, and measurable improvement of the\nautomated detection of the extent to which entrained-air has penetrated the\nwater column.\n</p>\n<p>Our model, Echofilter, was highly responsive to the dynamic range of\nturbulence conditions and sensitive to the fine-scale nuances in the boundary\nposition, producing an entrained-air boundary line with an average error of\n0.32m on mobile downfacing and 0.5-1.0m on stationary upfacing data. The\nmodel's annotations had a high level of agreement with the human segmentation\n(mobile downfacing Jaccard index: 98.8%; stationary upfacing: 93-95%). This\nresulted in a 50% reduction in the time required for manual edits compared to\nthe time required to manually edit the line placed by currently available\nalgorithms. Because of the improved initial automated placement, the\nimplementation of the models generated a marked increase in the standardization\nand repeatability of line placement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lowe_S/0/1/0/all/0/1\">Scott C. Lowe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McGarry_L/0/1/0/all/0/1\">Louise P. McGarry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Douglas_J/0/1/0/all/0/1\">Jessica Douglas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Newport_J/0/1/0/all/0/1\">Jason Newport</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oore_S/0/1/0/all/0/1\">Sageev Oore</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Whidden_C/0/1/0/all/0/1\">Christopher Whidden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasselman_D/0/1/0/all/0/1\">Daniel J. Hasselman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Region-Based Semantic Factorization in GANs. (arXiv:2202.09649v1 [cs.CV])","link":"http://arxiv.org/abs/2202.09649","description":"<p>Despite the rapid advancement of semantic discovery in the latent space of\nGenerative Adversarial Networks (GANs), existing approaches either are limited\nto finding global attributes or rely on a number of segmentation masks to\nidentify local attributes. In this work, we present a highly efficient\nalgorithm to factorize the latent semantics learned by GANs concerning an\narbitrary image region. Concretely, we revisit the task of local manipulation\nwith pre-trained GANs and formulate region-based semantic discovery as a dual\noptimization problem. Through an appropriately defined generalized Rayleigh\nquotient, we manage to solve such a problem without any annotations or\ntraining. Experimental results on various state-of-the-art GAN models\ndemonstrate the effectiveness of our approach, as well as its superiority over\nprior arts regarding precise control, region robustness, speed of\nimplementation, and simplicity of use.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jiapeng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yujun Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yinghao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Deli Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qifeng Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MSSNet: Multi-Scale-Stage Network for Single Image Deblurring. (arXiv:2202.09652v1 [cs.CV])","link":"http://arxiv.org/abs/2202.09652","description":"<p>Most of traditional single image deblurring methods before deep learning\nadopt a coarse-to-fine scheme that estimates a sharp image at a coarse scale\nand progressively refines it at finer scales. While this scheme has also been\nadopted to several deep learning-based approaches, recently a number of\nsingle-scale approaches have been introduced showing superior performance to\nprevious coarse-to-fine approaches both in quality and computation time, making\nthe traditional coarse-to-fine scheme seemingly obsolete. In this paper, we\nrevisit the coarse-to-fine scheme, and analyze defects of previous\ncoarse-to-fine approaches that degrade their performance. Based on the\nanalysis, we propose Multi-Scale-Stage Network (MSSNet), a novel deep\nlearning-based approach to single image deblurring that adopts our remedies to\nthe defects. Specifically, MSSNet adopts three novel technical components:\nstage configuration reflecting blur scales, an inter-scale information\npropagation scheme, and a pixel-shuffle-based multi-scale scheme. Our\nexperiments show that MSSNet achieves the state-of-the-art performance in terms\nof quality, network size, and computation time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kiyeon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seungyong Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1\">Sunghyun Cho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Punctuation Restoration. (arXiv:2202.09695v1 [cs.CL])","link":"http://arxiv.org/abs/2202.09695","description":"<p>Given the increasing number of livestreaming videos, automatic speech\nrecognition and post-processing for livestreaming video transcripts are crucial\nfor efficient data management as well as knowledge mining. A key step in this\nprocess is punctuation restoration which restores fundamental text structures\nsuch as phrase and sentence boundaries from the video transcripts. This work\npresents a new human-annotated corpus, called BehancePR, for punctuation\nrestoration in livestreaming video transcripts. Our experiments on BehancePR\ndemonstrate the challenges of punctuation restoration for this domain.\nFurthermore, we show that popular natural language processing toolkits are\nincapable of detecting sentence boundary on non-punctuated transcripts of\nlivestreaming videos, calling for more research effort to develop robust models\nfor this area.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lai_V/0/1/0/all/0/1\">Viet Dac Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Veyseh_A/0/1/0/all/0/1\">Amir Pouran Ben Veyseh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dernoncourt_F/0/1/0/all/0/1\">Franck Dernoncourt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Thien Huu Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MANet: Improving Video Denoising with a Multi-Alignment Network. (arXiv:2202.09704v1 [cs.CV])","link":"http://arxiv.org/abs/2202.09704","description":"<p>In video denoising, the adjacent frames often provide very useful\ninformation, but accurate alignment is needed before such information can be\nharnassed. In this work, we present a multi-alignment network, which generates\nmultiple flow proposals followed by attention-based averaging. It serves to\nmimics the non-local mechanism, suppressing noise by averaging multiple\nobservations. Our approach can be applied to various state-of-the-art models\nthat are based on flow estimation. Experiments on a large-scale video dataset\ndemonstrate that our method improves the denoising baseline model by 0.2dB, and\nfurther reduces the parameters by 47% with model distillation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yaping Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Haitian Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhongrui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jiebo Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_E/0/1/0/all/0/1\">Edmund Y. Lam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Statistical and Topological Summaries Aid Disease Detection for Segmented Retinal Vascular Images. (arXiv:2202.09708v1 [q-bio.QM])","link":"http://arxiv.org/abs/2202.09708","description":"<p>Disease complications can alter vascular network morphology and disrupt\ntissue functioning. Diabetic retinopathy, for example, is a complication of\ntype 1 and 2 diabetus mellitus that can cause blindness. Microvascular diseases\nare assessed by visual inspection of retinal images, but this can be\nchallenging when diseases exhibit silent symptoms or patients cannot attend\nin-person meetings. We examine the performance of machine learning algorithms\nin detecting microvascular disease when trained on either statistical or\ntopological summaries of segmented retinal vascular images. We apply our\nmethods to four publicly-available datasets and find that the fractal dimension\nperforms best for high resolution images. By contrast, we find that topological\ndescriptor vectors quantifying the number of loops in the data achieve the\nhighest accuracy for low resolution images. Further analysis, using the\ntopological approach, reveals that microvascular disease may alter morphology\nby reducing the number of loops in the retinal vasculature. Our work provides\npreliminary guidelines on which methods are most appropriate for assessing\ndisease in high and low resolution images. In the longer term, these methods\ncould be incorporated into automated disease assessment tools.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Nardini_J/0/1/0/all/0/1\">John T. Nardini</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Pugh_C/0/1/0/all/0/1\">Charles W. J. Pugh</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Byrne_H/0/1/0/all/0/1\">Helen M. Byrne</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ARM3D: Attention-based relation module for indoor 3D object detection. (arXiv:2202.09715v1 [cs.CV])","link":"http://arxiv.org/abs/2202.09715","description":"<p>Relation context has been proved to be useful for many challenging vision\ntasks. In the field of 3D object detection, previous methods have been taking\nthe advantage of context encoding, graph embedding, or explicit relation\nreasoning to extract relation context. However, there exists inevitably\nredundant relation context due to noisy or low-quality proposals. In fact,\ninvalid relation context usually indicates underlying scene misunderstanding\nand ambiguity, which may, on the contrary, reduce the performance in complex\nscenes. Inspired by recent attention mechanism like Transformer, we propose a\nnovel 3D attention-based relation module (ARM3D). It encompasses object-aware\nrelation reasoning to extract pair-wise relation contexts among qualified\nproposals and an attention module to distribute attention weights towards\ndifferent relation contexts. In this way, ARM3D can take full advantage of the\nuseful relation context and filter those less relevant or even confusing\ncontexts, which mitigates the ambiguity in detection. We have evaluated the\neffectiveness of ARM3D by plugging it into several state-of-the-art 3D object\ndetectors and showing more accurate and robust detection results. Extensive\nexperiments show the capability and generalization of ARM3D on 3D object\ndetection. Our source code is available at https://github.com/lanlan96/ARM3D.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1\">Yuqing Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_Y/0/1/0/all/0/1\">Yao Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chenyi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenyang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1\">Yueshan Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hui Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kai Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards a Unified Approach to Homography Estimation Using Image Features and Pixel Intensities. (arXiv:2202.09716v1 [cs.CV])","link":"http://arxiv.org/abs/2202.09716","description":"<p>The homography matrix is a key component in various vision-based robotic\ntasks. Traditionally, homography estimation algorithms are classified into\nfeature- or intensity-based. The main advantages of the latter are their\nversatility, accuracy, and robustness to arbitrary illumination changes. On the\nother hand, they have a smaller domain of convergence than the feature-based\nsolutions. Their combination is hence promising, but existing techniques only\napply them sequentially. This paper proposes a new hybrid method that unifies\nboth classes into a single nonlinear optimization procedure, applies the same\nminimization method, and uses the same homography parametrization and warping\nfunction. Experimental validation using a classical testing framework shows\nthat the proposed unified approach has improved convergence properties compared\nto each individual class. These are also demonstrated in a visual tracking\napplication. As a final contribution, our ready-to-use implementation of the\nalgorithm is made publicly available to the research community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nogueira_L/0/1/0/all/0/1\">Lucas Nogueira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paiva_E/0/1/0/all/0/1\">Ely C. de Paiva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silvera_G/0/1/0/all/0/1\">Geraldo Silvera</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3DRM:Pair-wise relation module for 3D object detection. (arXiv:2202.09721v1 [cs.CV])","link":"http://arxiv.org/abs/2202.09721","description":"<p>Context has proven to be one of the most important factors in object layout\nreasoning for 3D scene understanding. Existing deep contextual models either\nlearn holistic features for context encoding or rely on pre-defined scene\ntemplates for context modeling. We argue that scene understanding benefits from\nobject relation reasoning, which is capable of mitigating the ambiguity of 3D\nobject detections and thus helps locate and classify the 3D objects more\naccurately and robustly. To achieve this, we propose a novel 3D relation module\n(3DRM) which reasons about object relations at pair-wise levels. The 3DRM\npredicts the semantic and spatial relationships between objects and extracts\nthe object-wise relation features. We demonstrate the effects of 3DRM by\nplugging it into proposal-based and voting-based 3D object detection pipelines,\nrespectively. Extensive evaluations show the effectiveness and generalization\nof 3DRM on 3D object detection. Our source code is available at\nhttps://github.com/lanlan96/3DRM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1\">Yuqing Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_Y/0/1/0/all/0/1\">Yao Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yifei Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hui Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kai Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Overparametrization improves robustness against adversarial attacks: A replication study. (arXiv:2202.09735v1 [cs.LG])","link":"http://arxiv.org/abs/2202.09735","description":"<p>Overparametrization has become a de facto standard in machine learning.\nDespite numerous efforts, our understanding of how and where\noverparametrization helps model accuracy and robustness is still limited. To\nthis end, here we conduct an empirical investigation to systemically study and\nreplicate previous findings in this area, in particular the study by Madry et\nal. Together with this study, our findings support the \"universal law of\nrobustness\" recently proposed by Bubeck et al. We argue that while critical for\nrobust perception, overparametrization may not be enough to achieve full\nrobustness and smarter architectures e.g. the ones implemented by the human\nvisual cortex) seem inevitable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Borji_A/0/1/0/all/0/1\">Ali Borji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Loop Game: Quality Assessment and Optimization for Low-Light Image Enhancement. (arXiv:2202.09738v1 [eess.IV])","link":"http://arxiv.org/abs/2202.09738","description":"<p>There is an increasing consensus that the design and optimization of low\nlight image enhancement methods need to be fully driven by perceptual quality.\nWith numerous approaches proposed to enhance low-light images, much less work\nhas been dedicated to quality assessment and quality optimization of low-light\nenhancement. In this paper, to close the gap between enhancement and\nassessment, we propose a loop enhancement framework that produces a clear\npicture of how the enhancement of low-light images could be optimized towards\nbetter visual quality. In particular, we create a large-scale database for\nQUality assessment Of The Enhanced LOw-Light Image (QUOTE-LOL), which serves as\nthe foundation in studying and developing objective quality assessment\nmeasures. The objective quality assessment measure plays a critical bridging\nrole between visual quality and enhancement and is further incorporated in the\noptimization in learning the enhancement model towards perceptual optimally.\nFinally, we iteratively perform the enhancement and optimization tasks,\nenhancing the low-light images continuously. The superiority of the proposed\nscheme is validated based on various low-light scenes. The database as well as\nthe code will be available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chen_B/0/1/0/all/0/1\">Baoliang Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_L/0/1/0/all/0/1\">Lingyu Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_H/0/1/0/all/0/1\">Hanwei Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_W/0/1/0/all/0/1\">Wenhan Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_F/0/1/0/all/0/1\">Fangbo Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1\">Shiqi Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Attention Network. (arXiv:2202.09741v1 [cs.CV])","link":"http://arxiv.org/abs/2202.09741","description":"<p>While originally designed for natural language processing (NLP) tasks, the\nself-attention mechanism has recently taken various computer vision areas by\nstorm. However, the 2D nature of images brings three challenges for applying\nself-attention in computer vision. (1) Treating images as 1D sequences neglects\ntheir 2D structures. (2) The quadratic complexity is too expensive for\nhigh-resolution images. (3) It only captures spatial adaptability but ignores\nchannel adaptability. In this paper, we propose a novel large kernel attention\n(LKA) module to enable self-adaptive and long-range correlations in\nself-attention while avoiding the above issues. We further introduce a novel\nneural network based on LKA, namely Visual Attention Network (VAN). While\nextremely simple and efficient, VAN outperforms the state-of-the-art vision\ntransformers and convolutional neural networks with a large margin in extensive\nexperiments, including image classification, object detection, semantic\nsegmentation, instance segmentation, etc. Code is available at\nhttps://github.com/Visual-Attention-Network.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1\">Meng-Hao Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Cheng-Ze Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zheng-Ning Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1\">Ming-Ming Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shi-Min Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RDP-Net: Region Detail Preserving Network for Change Detection. (arXiv:2202.09745v1 [eess.IV])","link":"http://arxiv.org/abs/2202.09745","description":"<p>Change detection (CD) is an essential earth observation technique. It\ncaptures the dynamic information of land objects. With the rise of deep\nlearning, neural networks (NN) have shown great potential in CD. However,\ncurrent NN models introduce backbone architectures that lose the detail\ninformation during learning. Moreover, current NN models are heavy in\nparameters, which prevents their deployment on edge devices such as drones. In\nthis work, we tackle this issue by proposing RDP-Net: a region detail\npreserving network for CD. We propose an efficient training strategy that\nquantifies the importance of individual samples during the warmup period of NN\ntraining. Then, we perform non-uniform sampling based on the importance score\nso that the NN could learn detail information from easy to hard. Next, we\npropose an effective edge loss that improves the network's attention on details\nsuch as boundaries and small regions. As a result, we provide a NN model that\nachieves the state-of-the-art empirical performance in CD with only 1.70M\nparameters. We hope our RDP-Net would benefit the practical CD applications on\ncompact devices and could inspire more people to bring change detection to a\nnew level with the efficient training strategy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chen_H/0/1/0/all/0/1\">Hongjia Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pu_F/0/1/0/all/0/1\">Fangling Pu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_R/0/1/0/all/0/1\">Rui Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tang_R/0/1/0/all/0/1\">Rui Tang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_X/0/1/0/all/0/1\">Xin Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Spatial Propagation Network for Depth Completion. (arXiv:2202.09769v1 [cs.CV])","link":"http://arxiv.org/abs/2202.09769","description":"<p>Image-guided depth completion aims to generate dense depth maps with sparse\ndepth measurements and corresponding RGB images. Currently, spatial propagation\nnetworks (SPNs) are the most popular affinity-based methods in depth\ncompletion, but they still suffer from the representation limitation of the\nfixed affinity and the over smoothing during iterations. Our solution is to\nestimate independent affinity matrices in each SPN iteration, but it is\nover-parameterized and heavy calculation. This paper introduces an efficient\nmodel that learns the affinity among neighboring pixels with an\nattention-based, dynamic approach. Specifically, the Dynamic Spatial\nPropagation Network (DySPN) we proposed makes use of a non-linear propagation\nmodel (NLPM). It decouples the neighborhood into parts regarding to different\ndistances and recursively generates independent attention maps to refine these\nparts into adaptive affinity matrices. Furthermore, we adopt a diffusion\nsuppression (DS) operation so that the model converges at an early stage to\nprevent over-smoothing of dense depth. Finally, in order to decrease the\ncomputational cost required, we also introduce three variations that reduce the\namount of neighbors and attentions needed while still retaining similar\naccuracy. In practice, our method requires less iteration to match the\nperformance of other SPNs and yields better results overall. DySPN outperforms\nother state-of-the-art (SoTA) methods on KITTI Depth Completion (DC) evaluation\nby the time of submission and is able to yield SoTA performance in NYU Depth v2\ndataset as well.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yuankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_T/0/1/0/all/0/1\">Tao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Q/0/1/0/all/0/1\">Qi Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wending Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hua Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pseudo Numerical Methods for Diffusion Models on Manifolds. (arXiv:2202.09778v1 [cs.CV])","link":"http://arxiv.org/abs/2202.09778","description":"<p>Denoising Diffusion Probabilistic Models (DDPMs) can generate high-quality\nsamples such as image and audio samples. However, DDPMs require hundreds to\nthousands of iterations to produce final samples. Several prior works have\nsuccessfully accelerated DDPMs through adjusting the variance schedule (e.g.,\nImproved Denoising Diffusion Probabilistic Models) or the denoising equation\n(e.g., Denoising Diffusion Implicit Models (DDIMs)). However, these\nacceleration methods cannot maintain the quality of samples and even introduce\nnew noise at a high speedup rate, which limit their practicability. To\naccelerate the inference process while keeping the sample quality, we provide a\nfresh perspective that DDPMs should be treated as solving differential\nequations on manifolds. Under such a perspective, we propose pseudo numerical\nmethods for diffusion models (PNDMs). Specifically, we figure out how to solve\ndifferential equations on manifolds and show that DDIMs are simple cases of\npseudo numerical methods. We change several classical numerical methods to\ncorresponding pseudo numerical methods and find that the pseudo linear\nmulti-step method is the best in most situations. According to our experiments,\nby directly using pre-trained models on Cifar10, CelebA and LSUN, PNDMs can\ngenerate higher quality synthetic images with only 50 steps compared with\n1000-step DDIMs (20x speedup), significantly outperform DDIMs with 250 steps\n(by around 0.4 in FID) and have good generalization on different variance\nschedules. Our implementation is available at\nhttps://github.com/luping-liu/PNDM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Luping Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Yi Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhijie Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhou Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Clustering by the Probability Distributions from Extreme Value Theory. (arXiv:2202.09784v1 [cs.LG])","link":"http://arxiv.org/abs/2202.09784","description":"<p>Clustering is an essential task to unsupervised learning. It tries to\nautomatically separate instances into coherent subsets. As one of the most\nwell-known clustering algorithms, k-means assigns sample points at the boundary\nto a unique cluster, while it does not utilize the information of sample\ndistribution or density. Comparably, it would potentially be more beneficial to\nconsider the probability of each sample in a possible cluster. To this end,\nthis paper generalizes k-means to model the distribution of clusters. Our novel\nclustering algorithm thus models the distributions of distances to centroids\nover a threshold by Generalized Pareto Distribution (GPD) in Extreme Value\nTheory (EVT). Notably, we propose the concept of centroid margin distance, use\nGPD to establish a probability model for each cluster, and perform a clustering\nalgorithm based on the covering probability function derived from GPD. Such a\nGPD k-means thus enables the clustering algorithm from the probabilistic\nperspective. Correspondingly, we also introduce a naive baseline, dubbed as\nGeneralized Extreme Value (GEV) k-means. GEV fits the distribution of the block\nmaxima. In contrast, the GPD fits the distribution of distance to the centroid\nexceeding a sufficiently large threshold, leading to a more stable performance\nof GPD k-means. Notably, GEV k-means can also estimate cluster structure and\nthus perform reasonably well over classical k-means. Thus, extensive\nexperiments on synthetic datasets and real datasets demonstrate that GPD\nk-means outperforms competitors. The github codes are released in\nhttps://github.com/sixiaozheng/EVT-K-means.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Sixiao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_K/0/1/0/all/0/1\">Ke Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yanxi Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jianfeng Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yanwei Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image quality assessment by overlapping task-specific and task-agnostic measures: application to prostate multiparametric MR images for cancer segmentation. (arXiv:2202.09798v1 [eess.IV])","link":"http://arxiv.org/abs/2202.09798","description":"<p>Image quality assessment (IQA) in medical imaging can be used to ensure that\ndownstream clinical tasks can be reliably performed. Quantifying the impact of\nan image on the specific target tasks, also named as task amenability, is\nneeded. A task-specific IQA has recently been proposed to learn an\nimage-amenability-predicting controller simultaneously with a target task\npredictor. This allows for the trained IQA controller to measure the impact an\nimage has on the target task performance, when this task is performed using the\npredictor, e.g. segmentation and classification neural networks in modern\nclinical applications. In this work, we propose an extension to this\ntask-specific IQA approach, by adding a task-agnostic IQA based on\nauto-encoding as the target task. Analysing the intersection between\nlow-quality images, deemed by both the task-specific and task-agnostic IQA, may\nhelp to differentiate the underpinning factors that caused the poor target task\nperformance. For example, common imaging artefacts may not adversely affect the\ntarget task, which would lead to a low task-agnostic quality and a high\ntask-specific quality, whilst individual cases considered clinically\nchallenging, which can not be improved by better imaging equipment or\nprotocols, is likely to result in a high task-agnostic quality but a low\ntask-specific quality. We first describe a flexible reward shaping strategy\nwhich allows for the adjustment of weighting between task-agnostic and\ntask-specific quality scoring. Furthermore, we evaluate the proposed algorithm\nusing a clinically challenging target task of prostate tumour segmentation on\nmultiparametric magnetic resonance (mpMR) images, from 850 patients. The\nproposed reward shaping strategy, with appropriately weighted task-specific and\ntask-agnostic qualities, successfully identified samples that need\nre-acquisition due to defected imaging process.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Saeed_S/0/1/0/all/0/1\">Shaheer U. Saeed</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yan_W/0/1/0/all/0/1\">Wen Yan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fu_Y/0/1/0/all/0/1\">Yunguan Fu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Giganti_F/0/1/0/all/0/1\">Francesco Giganti</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_Q/0/1/0/all/0/1\">Qianye Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Baum_Z/0/1/0/all/0/1\">Zachary M. C. Baum</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rusu_M/0/1/0/all/0/1\">Mirabela Rusu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fan_R/0/1/0/all/0/1\">Richard E. Fan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sonn_G/0/1/0/all/0/1\">Geoffrey A. Sonn</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Emberton_M/0/1/0/all/0/1\">Mark Emberton</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Barratt_D/0/1/0/all/0/1\">Dean C. Barratt</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_Y/0/1/0/all/0/1\">Yipeng Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distortion-Aware Loop Filtering of Intra 360^o Video Coding with Equirectangular Projection. (arXiv:2202.09802v1 [cs.CV])","link":"http://arxiv.org/abs/2202.09802","description":"<p>In this paper, we propose a distortion-aware loop filtering model to improve\nthe performance of intra coding for 360$^o$ videos projected via\nequirectangular projection (ERP) format. To enable the awareness of distortion,\nour proposed module analyzes content characteristics based on a coding unit\n(CU) partition mask and processes them through partial convolution to activate\nthe specified area. The feature recalibration module, which leverages cascaded\nresidual channel-wise attention blocks (RCABs) to adjust the inter-channel and\nintra-channel features automatically, is capable of adapting with different\nquality levels. The perceptual geometry optimization combining with weighted\nmean squared error (WMSE) and the perceptual loss guarantees both the local\nfield of view (FoV) and global image reconstruction with high quality.\nExtensive experimental results show that our proposed scheme achieves\nsignificant bitrate savings compared with the anchor (HM + 360Lib), leading to\n8.9%, 9.0%, 7.1% and 7.4% on average bit rate reductions in terms of PSNR,\nWPSNR, and PSNR of two viewports for luminance component of 360^o videos,\nrespectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pingping Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Linwei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shiqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwong_S/0/1/0/all/0/1\">Sam Kwong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Alternative design of DeepPDNet in the context of image restoration. (arXiv:2202.09810v1 [eess.IV])","link":"http://arxiv.org/abs/2202.09810","description":"<p>This work designs an image restoration deep network relying on unfolded\nChambolle-Pock primal-dual iterations. Each layer of our network is built from\nChambolle-Pock iterations when specified for minimizing a sum of a\n$\\ell_2$-norm data-term and an analysis sparse prior. The parameters of our\nnetwork are the step-sizes of the Chambolle-Pock scheme and the linear operator\ninvolved in sparsity-based penalization, including implicitly the\nregularization parameter. A backpropagation procedure is fully described.\nPreliminary experiments illustrate the good behavior of such a deep primal-dual\nnetwork in the context of image restoration on BSD68 database.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Jiu_M/0/1/0/all/0/1\">Mingyuan Jiu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pustelnik_N/0/1/0/all/0/1\">Nelly Pustelnik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparsity Winning Twice: Better Robust Generaliztion from More Efficient Training. (arXiv:2202.09844v1 [cs.CV])","link":"http://arxiv.org/abs/2202.09844","description":"<p>Recent studies demonstrate that deep networks, even robustified by the\nstate-of-the-art adversarial training (AT), still suffer from large robust\ngeneralization gaps, in addition to the much more expensive training costs than\nstandard training. In this paper, we investigate this intriguing problem from a\nnew perspective, i.e., injecting appropriate forms of sparsity during\nadversarial training. We introduce two alternatives for sparse adversarial\ntraining: (i) static sparsity, by leveraging recent results from the lottery\nticket hypothesis to identify critical sparse subnetworks arising from the\nearly training; (ii) dynamic sparsity, by allowing the sparse subnetwork to\nadaptively adjust its connectivity pattern (while sticking to the same sparsity\nratio) throughout training. We find both static and dynamic sparse methods to\nyield win-win: substantially shrinking the robust generalization gap and\nalleviating the robust overfitting, meanwhile significantly saving training and\ninference FLOPs. Extensive experiments validate our proposals with multiple\nnetwork architectures on diverse datasets, including CIFAR-10/100 and\nTiny-ImageNet. For example, our methods reduce robust generalization gap and\noverfitting by 34.44% and 4.02%, with comparable robust/standard accuracy\nboosts and 87.83%/87.82% training/inference FLOPs savings on CIFAR-100 with\nResNet-18. Besides, our approaches can be organically combined with existing\nregularizers, establishing new state-of-the-art results in AT. Codes are\navailable in https://github.com/VITA-Group/Sparsity-Win-Robust-Generalization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianlong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhenyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pengjun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balachandra_S/0/1/0/all/0/1\">Santosh Balachandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1\">Haoyu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zehao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Novel Framework for Brain Tumor Detection Based on Convolutional Variational Generative Models. (arXiv:2202.09850v1 [eess.IV])","link":"http://arxiv.org/abs/2202.09850","description":"<p>Brain tumor detection can make the difference between life and death.\nRecently, deep learning-based brain tumor detection techniques have gained\nattention due to their higher performance. However, obtaining the expected\nperformance of such deep learning-based systems requires large amounts of\nclassified images to train the deep models. Obtaining such data is usually\nboring, time-consuming, and can easily be exposed to human mistakes which\nhinder the utilization of such deep learning approaches. This paper introduces\na novel framework for brain tumor detection and classification. The basic idea\nis to generate a large synthetic MRI images dataset that reflects the typical\npattern of the brain MRI images from a small class-unbalanced collected\ndataset. The resulted dataset is then used for training a deep model for\ndetection and classification. Specifically, we employ two types of deep models.\nThe first model is a generative model to capture the distribution of the\nimportant features in a set of small class-unbalanced brain MRI images. Then by\nusing this distribution, the generative model can synthesize any number of\nbrain MRI images for each class. Hence, the system can automatically convert a\nsmall unbalanced dataset to a larger balanced one. The second model is the\nclassifier that is trained using the large balanced dataset to detect brain\ntumors in MRI images. The proposed framework acquires an overall detection\naccuracy of 96.88% which highlights the promise of the proposed framework as an\naccurate low-overhead brain tumor detection system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Salama_W/0/1/0/all/0/1\">Wessam M. Salama</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shokry_A/0/1/0/all/0/1\">Ahmed Shokry</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Non-Deterministic Face Mask Removal Based On 3D Priors. (arXiv:2202.09856v1 [cs.CV])","link":"http://arxiv.org/abs/2202.09856","description":"<p>This paper presents a novel image inpainting framework for face mask removal.\nAlthough current methods have demonstrated their impressive ability in\nrecovering damaged face images, they suffer from two main problems: the\ndependence on manually labeled missing regions and the deterministic result\ncorresponding to each input. The proposed approach tackles these problems by\nintegrating a multi-task 3D face reconstruction module with a face inpainting\nmodule. Given a masked face image, the former predicts a 3DMM-based\nreconstructed face together with a binary occlusion map, providing dense\ngeometrical and textural priors that greatly facilitate the inpainting task of\nthe latter. By gradually controlling the 3D shape parameters, our method\ngenerates high-quality dynamic inpainting results with different expressions\nand mouth movements. Qualitative and quantitative experiments verify the\neffectiveness of the proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1\">Xiangnan Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liming Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SRL-SOA: Self-Representation Learning with Sparse 1D-Operational Autoencoder for Hyperspectral Image Band Selection. (arXiv:2202.09918v1 [cs.CV])","link":"http://arxiv.org/abs/2202.09918","description":"<p>The band selection in the hyperspectral image (HSI) data processing is an\nimportant task considering its effect on the computational complexity and\naccuracy. In this work, we propose a novel framework for the band selection\nproblem: Self-Representation Learning (SRL) with Sparse 1D-Operational\nAutoencoder (SOA). The proposed SLR-SOA approach introduces a novel autoencoder\nmodel, SOA, that is designed to learn a representation domain where the data\nare sparsely represented. Moreover, the network composes of 1D-operational\nlayers with the non-linear neuron model. Hence, the learning capability of\nneurons (filters) is greatly improved with shallow architectures. Using compact\narchitectures is especially crucial in autoencoders as they tend to overfit\neasily because of their identity mapping objective. Overall, we show that the\nproposed SRL-SOA band selection approach outperforms the competing methods over\ntwo HSI data including Indian Pines and Salinas-A considering the achieved land\ncover classification accuracies. The software implementation of the SRL-SOA\napproach is shared publicly at https://github.com/meteahishali/SRL-SOA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ahishali_M/0/1/0/all/0/1\">Mete Ahishali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiranyaz_S/0/1/0/all/0/1\">Serkan Kiranyaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_I/0/1/0/all/0/1\">Iftikhar Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gabbouj_M/0/1/0/all/0/1\">Moncef Gabbouj</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deconstructing Distributions: A Pointwise Framework of Learning. (arXiv:2202.09931v1 [cs.LG])","link":"http://arxiv.org/abs/2202.09931","description":"<p>In machine learning, we traditionally evaluate the performance of a single\nmodel, averaged over a collection of test inputs. In this work, we propose a\nnew approach: we measure the performance of a collection of models when\nevaluated on a $\\textit{single input point}$. Specifically, we study a point's\n$\\textit{profile}$: the relationship between models' average performance on the\ntest distribution and their pointwise performance on this individual point. We\nfind that profiles can yield new insights into the structure of both models and\ndata -- in and out-of-distribution. For example, we empirically show that real\ndata distributions consist of points with qualitatively different profiles. On\none hand, there are \"compatible\" points with strong correlation between the\npointwise and average performance. On the other hand, there are points with\nweak and even $\\textit{negative}$ correlation: cases where improving overall\nmodel accuracy actually $\\textit{hurts}$ performance on these inputs. We prove\nthat these experimental observations are inconsistent with the predictions of\nseveral simplified models of learning proposed in prior work. As an\napplication, we use profiles to construct a dataset we call CIFAR-10-NEG: a\nsubset of CINIC-10 such that for standard models, accuracy on CIFAR-10-NEG is\n$\\textit{negatively correlated}$ with accuracy on CIFAR-10 test. This\nillustrates, for the first time, an OOD dataset that completely inverts\n\"accuracy-on-the-line\" (Miller, Taori, Raghunathan, Sagawa, Koh, Shankar,\nLiang, Carmon, and Schmidt 2021)\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kaplun_G/0/1/0/all/0/1\">Gal Kaplun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_N/0/1/0/all/0/1\">Nikhil Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1\">Saurabh Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barak_B/0/1/0/all/0/1\">Boaz Barak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakkiran_P/0/1/0/all/0/1\">Preetum Nakkiran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Imbalanced Malware Images Classification: a CNN based Approach. (arXiv:1708.08042v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1708.08042","description":"<p>Deep convolutional neural networks (CNNs) can be applied to malware binary\ndetection via image classification. The performance, however, is degraded due\nto the imbalance of malware families (classes). To mitigate this issue, we\npropose a simple yet effective weighted softmax loss which can be employed as\nthe final layer of deep CNNs. The original softmax loss is weighted, and the\nweight value can be determined according to class size. A scaling parameter is\nalso included in computing the weight. Proper selection of this parameter is\nstudied and an empirical option is suggested. The weighted loss aims at\nalleviating the impact of data imbalance in an end-to-end learning fashion. To\nvalidate the efficacy, we deploy the proposed weighted loss in a pre-trained\ndeep CNN model and fine-tune it to achieve promising results on malware images\nclassification. Extensive experiments also demonstrate that the new loss\nfunction can well fit other typical CNNs, yielding an improved classification\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yue_S/0/1/0/all/0/1\">Songqing Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianyang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pyramid Convolutional RNN for MRI Image Reconstruction. (arXiv:1912.00543v6 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/1912.00543","description":"<p>Fast and accurate MRI image reconstruction from undersampled data is crucial\nin clinical practice. Deep learning based reconstruction methods have shown\npromising advances in recent years. However, recovering fine details from\nundersampled data is still challenging. In this paper, we introduce a novel\ndeep learning based method, Pyramid Convolutional RNN (PC-RNN), to reconstruct\nimages from multiple scales. Based on the formulation of MRI reconstruction as\nan inverse problem, we design the PC-RNN model with three convolutional RNN\n(ConvRNN) modules to iteratively learn the features in multiple scales. Each\nConvRNN module reconstructs images at different scales and the reconstructed\nimages are combined by a final CNN module in a pyramid fashion. The multi-scale\nConvRNN modules learn a coarse-to-fine image reconstruction. Unlike other\ncommon reconstruction methods for parallel imaging, PC-RNN does not employ coil\nsensitive maps for multi-coil data and directly model the multiple coils as\nmulti-channel inputs. The coil compression technique is applied to standardize\ndata with various coil numbers, leading to more efficient training. We evaluate\nour model on the fastMRI knee and brain datasets and the results show that the\nproposed model outperforms other methods and can recover more details. The\nproposed method is one of the winner solutions in the 2019 fastMRI competition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chen_E/0/1/0/all/0/1\">Eric Z. Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_P/0/1/0/all/0/1\">Puyang Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1\">Xiao Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_T/0/1/0/all/0/1\">Terrence Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_S/0/1/0/all/0/1\">Shanhui Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Causality-Aware Inferring: A Sequential Discriminative Approach for Medical Diagnosis. (arXiv:2003.06534v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2003.06534","description":"<p>Medical diagnosis assistant (MDA) aims to build an interactive diagnostic\nagent to sequentially inquire about symptoms for discriminating diseases.\nHowever, since the dialogue records used to build a patient simulator are\ncollected passively, the data might be deteriorated by some task-unrelated\nbiases, such as the preference of the collectors. These biases might hinder the\ndiagnostic agent to capture transportable knowledge from the simulator. This\nwork attempts to address these critical issues in MDA by taking advantage of\nthe causal diagram to identify and resolve two representative non-causal\nbiases, i.e., (i) default-answer bias and (ii) distributional inquiry bias.\nSpecifically, Bias (i) originates from the patient simulator which tries to\nanswer the unrecorded inquiries with some biased default answers. Consequently,\nthe diagnostic agents cannot fully demonstrate their advantages due to the\nbiased answers. To eliminate this bias and inspired by the propensity score\nmatching technique with causal diagram, we propose a propensity-based patient\nsimulator to effectively answer unrecorded inquiry by drawing knowledge from\nthe other records; Bias (ii) inherently comes along with the passively\ncollected data, and is one of the key obstacles for training the agent towards\n\"learning how\" rather than \"remembering what\". For example, within the\ndistribution of training data, if a symptom is highly coupled with a certain\ndisease, the agent might learn to only inquire about that symptom to\ndiscriminate that disease, thus might not generalize to the out-of-distribution\ncases. To this end, we propose a progressive assurance agent, which includes\nthe dual processes accounting for symptom inquiry and disease diagnosis\nrespectively. The inquiry process is driven by the diagnosis process in a\ntop-down manner to inquire about symptoms for enhancing diagnostic confidence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Junfan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Keze Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Ziliang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Liang Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Isotropic multichannel total variation framework for joint reconstruction of multicontrast parallel MRI. (arXiv:2006.04128v5 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2006.04128","description":"<p>Purpose: To develop a synergistic image reconstruction framework that\nexploits multicontrast (MC), multicoil, and compressed sensing (CS)\nredundancies in magnetic resonance imaging (MRI).\n</p>\n<p>Approach: CS, MC acquisition, and parallel imaging (PI) have been\nindividually well developed, but the combination of the three has not been\nequally well studied, much less the potential benefits of isotropy within such\na setting. Inspired by total variation theory, we introduce an isotropic MC\nimage regularizer and attain its full potential by integrating it into\ncompressed MC multicoil MRI. A convex optimization problem is posed to model\nthe new variational framework and a first-order algorithm is developed to solve\nthe problem.\n</p>\n<p>Results: It turns out that the proposed isotropic regularizer outperforms\nmany of the state-of-the-art reconstruction methods not only in terms of\nrotation-invariance preservation of symmetrical features, but also in\nsuppressing noise or streaking artifacts, which are normally encountered in PI\nmethods at aggressive undersampling rates. Moreover, the new framework\nsignificantly prevents intercontrast leakage of contrast-specific details,\nwhich seems to be a difficult situation to handle for some variational and\nlow-rank MC reconstruction approaches.\n</p>\n<p>Conclusions: The new framework is a viable option for image reconstruction in\nfast protocols of MC parallel MRI, potentially reducing patient discomfort in\notherwise long and time-consuming scans.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Esfahani_E/0/1/0/all/0/1\">Erfan Ebrahim Esfahani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Post-hoc Calibration of Neural Networks by g-Layers. (arXiv:2006.12807v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2006.12807","description":"<p>Calibration of neural networks is a critical aspect to consider when\nincorporating machine learning models in real-world decision-making systems\nwhere the confidence of decisions are equally important as the decisions\nthemselves. In recent years, there is a surge of research on neural network\ncalibration and the majority of the works can be categorized into post-hoc\ncalibration methods, defined as methods that learn an additional function to\ncalibrate an already trained base network. In this work, we intend to\nunderstand the post-hoc calibration methods from a theoretical point of view.\nEspecially, it is known that minimizing Negative Log-Likelihood (NLL) will lead\nto a calibrated network on the training set if the global optimum is attained\n(Bishop, 1994). Nevertheless, it is not clear learning an additional function\nin a post-hoc manner would lead to calibration in the theoretical sense. To\nthis end, we prove that even though the base network ($f$) does not lead to the\nglobal optimum of NLL, by adding additional layers ($g$) and minimizing NLL by\noptimizing the parameters of $g$ one can obtain a calibrated network $g \\circ\nf$. This not only provides a less stringent condition to obtain a calibrated\nnetwork but also provides a theoretical justification of post-hoc calibration\nmethods. Our experiments on various image classification benchmarks confirm the\ntheory.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rahimi_A/0/1/0/all/0/1\">Amir Rahimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mensink_T/0/1/0/all/0/1\">Thomas Mensink</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_K/0/1/0/all/0/1\">Kartik Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ajanthan_T/0/1/0/all/0/1\">Thalaiyasingam Ajanthan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sminchisescu_C/0/1/0/all/0/1\">Cristian Sminchisescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hartley_R/0/1/0/all/0/1\">Richard Hartley</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spatio-Temporal EEG Representation Learning on Riemannian Manifold and Euclidean Space. (arXiv:2008.08633v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2008.08633","description":"<p>We present a novel deep neural architecture for learning Electroencephalogram\n(EEG). To learn the spatial information, our model first obtains the Riemannian\nmean and distance from Spatial Covariance Matrices (SCMs) on the Riemannian\nmanifold. We then project the spatial information onto the Euclidean space via\ntangent space learning. Following, two fully connected layers are used to learn\nthe spatial information embeddings. Moreover, our proposed method learns the\ntemporal information via differential entropy and logarithm power spectrum\ndensity features extracted from EEG signals in Euclidean space using a deep\nlong short-term memory network with a soft attention mechanism. To combine the\nspatial and temporal information, we use an effective fusion strategy, which\nlearns attention weights applied to embedding-specific features for decision\nmaking. We evaluate our proposed framework on four public datasets across three\npopular EEG-related tasks, notably emotion recognition, vigilance estimation,\nand motor imagery classification, containing various types of tasks such as\nbinary classification, multi-class classification, and regression. Our proposed\narchitecture approaches the state-of-the-art on one dataset (SEED) and\noutperforms other methods on the other three datasets (SEED-VIG, BCI-IV 2A, and\nBCI-IV 2B), setting new state-of-the-art values and showing the robustness of\nour framework in EEG representation learning. The source code of our paper is\npublicly available at https://github.com/guangyizhangbci/EEG_Riemannian.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guangyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Etemad_A/0/1/0/all/0/1\">Ali Etemad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Attack with Fewer Pixels: A Probabilistic Post-hoc Framework for Refining Arbitrary Dense Adversarial Attacks. (arXiv:2010.06131v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2010.06131","description":"<p>Deep neural network image classifiers are reported to be susceptible to\nadversarial evasion attacks, which use carefully crafted images created to\nmislead a classifier. Many adversarial attacks belong to the category of dense\nattacks, which generate adversarial examples by perturbing all the pixels of a\nnatural image. To generate sparse perturbations, sparse attacks have been\nrecently developed, which are usually independent attacks derived by modifying\na dense attack's algorithm with sparsity regularisations, resulting in reduced\nattack efficiency. In this paper, we aim to tackle this task from a different\nperspective. We select the most effective perturbations from the ones generated\nfrom a dense attack, based on the fact we find that a considerable amount of\nthe perturbations on an image generated by dense attacks may contribute little\nto attacking a classifier. Accordingly, we propose a probabilistic post-hoc\nframework that refines given dense attacks by significantly reducing the number\nof perturbed pixels but keeping their attack power, trained with mutual\ninformation maximisation. Given an arbitrary dense attack, the proposed model\nenjoys appealing compatibility for making its adversarial images more realistic\nand less detectable with fewer perturbations. Moreover, our framework performs\nadversarial attacks much faster than existing sparse attacks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">He Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Thanh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1\">Trung Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Montague_P/0/1/0/all/0/1\">Paul Montague</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vel_O/0/1/0/all/0/1\">Olivier De Vel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abraham_T/0/1/0/all/0/1\">Tamas Abraham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phung_D/0/1/0/all/0/1\">Dinh Phung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stochastic sparse adversarial attacks. (arXiv:2011.12423v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2011.12423","description":"<p>This paper introduces stochastic sparse adversarial attacks (SSAA), standing\nas simple, fast and purely noise-based targeted and untargeted attacks of\nneural network classifiers (NNC). SSAA offer new examples of sparse (or $L_0$)\nattacks for which only few methods have been proposed previously. These attacks\nare devised by exploiting a small-time expansion idea widely used for Markov\nprocesses. Experiments on small and large datasets (CIFAR-10 and ImageNet)\nillustrate several advantages of SSAA in comparison with the-state-of-the-art\nmethods. For instance, in the untargeted case, our method called Voting Folded\nGaussian Attack (VFGA) scales efficiently to ImageNet and achieves a\nsignificantly lower $L_0$ score than SparseFool (up to $\\frac{2}{5}$) while\nbeing faster. Moreover, VFGA achieves better $L_0$ scores on ImageNet than\nSparse-RS when both attacks are fully successful on a large number of samples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cesaire_M/0/1/0/all/0/1\">Manon C&#xe9;saire</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schott_L/0/1/0/all/0/1\">Lucas Schott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajri_H/0/1/0/all/0/1\">Hatem Hajri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lamprier_S/0/1/0/all/0/1\">Sylvain Lamprier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gallinari_P/0/1/0/all/0/1\">Patrick Gallinari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video Reenactment as Inductive Bias for Content-Motion Disentanglement. (arXiv:2102.00324v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.00324","description":"<p>Independent components within low-dimensional representations are essential\ninputs in several downstream tasks, and provide explanations over the observed\ndata. Video-based disentangled factors of variation provide low-dimensional\nrepresentations that can be identified and used to feed task-specific models.\nWe introduce MTC-VAE, a self-supervised motion-transfer VAE model to\ndisentangle motion and content from videos. Unlike previous work on video\ncontent-motion disentanglement, we adopt a chunk-wise modeling approach and\ntake advantage of the motion information contained in spatiotemporal\nneighborhoods. Our model yields independent per-chunk representations that\npreserve temporal consistency. Hence, we reconstruct whole videos in a single\nforward-pass. We extend the ELBO's log-likelihood term and include a Blind\nReenactment Loss as an inductive bias to leverage motion disentanglement, under\nthe assumption that swapping motion features yields reenactment between two\nvideos. We evaluate our model with recently-proposed disentanglement metrics\nand show that it outperforms a variety of methods for video motion-content\ndisentanglement. Experiments on video reenactment show the effectiveness of our\ndisentanglement in the input space where our model outperforms the baselines in\nreconstruction quality and motion alignment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Albarracin_J/0/1/0/all/0/1\">Juan F. Hern&#xe1;ndez Albarrac&#xed;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rivera_A/0/1/0/all/0/1\">Ad&#xed;n Ram&#xed;rez Rivera</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Child-Computer Interaction: Recent Works, New Dataset, and Age Detection. (arXiv:2102.01405v2 [cs.HC] UPDATED)","link":"http://arxiv.org/abs/2102.01405","description":"<p>This article provides an overview of recent research in Child-Computer\nInteraction with mobile devices and describe our framework ChildCI intended\nfor: i) overcoming the lack of large-scale publicly available databases in the\narea, ii) generating a better understanding of the cognitive and neuromotor\ndevelopment of children along time, contrary to most previous studies in the\nliterature focused on a single-session acquisition, and iii) enabling new\napplications in e-Learning and e-Health through the acquisition of additional\ninformation such as the school grades and children's disorders, among others.\nOur framework includes a new mobile application, specific data acquisition\nprotocols, and a first release of the ChildCI dataset (ChildCIdb v1), which is\nplanned to be extended yearly to enable longitudinal studies.\n</p>\n<p>In our framework children interact with a tablet device, using both a pen\nstylus and the finger, performing different tasks that require different levels\nof neuromotor and cognitive skills. ChildCIdb is the first database in the\nliterature that comprises more than 400 children from 18 months to 8 years old,\nconsidering therefore the first three development stages of the Piaget's\ntheory. In addition, and as a demonstration of the potential of the ChildCI\nframework, we include experimental results for one of the many applications\nenabled by ChildCIdb: children age detection based on device interaction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tolosana_R/0/1/0/all/0/1\">Ruben Tolosana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruiz_Garcia_J/0/1/0/all/0/1\">Juan Carlos Ruiz-Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vera_Rodriguez_R/0/1/0/all/0/1\">Ruben Vera-Rodriguez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herreros_Rodriguez_J/0/1/0/all/0/1\">Jaime Herreros-Rodriguez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romero_Tapiador_S/0/1/0/all/0/1\">Sergio Romero-Tapiador</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morales_A/0/1/0/all/0/1\">Aythami Morales</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fierrez_J/0/1/0/all/0/1\">Julian Fierrez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PURSUhInT: In Search of Informative Hint Points Based on Layer Clustering for Knowledge Distillation. (arXiv:2103.00053v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2103.00053","description":"<p>We propose a novel knowledge distillation methodology for compressing deep\nneural networks. One of the most efficient methods for knowledge distillation\nis hint distillation, where the student model is injected with information\n(hints) from several different layers of the teacher model. Although the\nselection of hint points can drastically alter the compression performance,\nconventional distillation approaches overlook this fact. Therefore, we propose\na clustering based hint selection methodology, where the layers of teacher\nmodel are clustered with respect to several metrics and the cluster centers are\nused as the hint points. Our method is applicable for any student network, once\nit is applied on a chosen teacher network. The proposed approach is validated\nin CIFAR-100 and ImageNet datasets, using various teacher-student pairs and\nnumerous hint distillation methods. Our results show that hint points selected\nby our algorithm results in superior compression performance with respect to\nstate-of-the-art knowledge distillation algorithms on the same student models\nand datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Keser_R/0/1/0/all/0/1\">Reyhan Kevser Keser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayanzadeh_A/0/1/0/all/0/1\">Aydin Ayanzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aghdam_O/0/1/0/all/0/1\">Omid Abdollahi Aghdam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kilcioglu_C/0/1/0/all/0/1\">Caglar Kilcioglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toreyin_B/0/1/0/all/0/1\">Behcet Ugur Toreyin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ure_N/0/1/0/all/0/1\">Nazim Kemal Ure</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recent Advances on Neural Network Pruning at Initialization. (arXiv:2103.06460v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2103.06460","description":"<p>Neural network pruning typically removes connections or neurons from a\npretrained converged model; while a new pruning paradigm, pruning at\ninitialization (PaI), attempts to prune a randomly initialized network. This\npaper offers the first survey concentrated on this emerging pruning fashion. We\nfirst introduce a generic formulation of neural network pruning, followed by\nthe major classic pruning topics. Then, as the main body of this paper, a\nthorough and structured literature review of PaI methods is presented,\nconsisting of two major tracks (sparse training and sparse selection). Finally,\nwe summarize the surge of PaI compared to traditional pruning and discuss the\nopen problems. Apart from the dedicated paper review, this paper also offers a\ncode base for easy sanity-checking and benchmarking of different PaI methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_C/0/1/0/all/0/1\">Can Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yue Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yulun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yun Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Retrieve Fast, Rerank Smart: Cooperative and Joint Approaches for Improved Cross-Modal Retrieval. (arXiv:2103.11920v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.11920","description":"<p>Current state-of-the-art approaches to cross-modal retrieval process text and\nvisual input jointly, relying on Transformer-based architectures with\ncross-attention mechanisms that attend over all words and objects in an image.\nWhile offering unmatched retrieval performance, such models: 1) are typically\npretrained from scratch and thus less scalable, 2) suffer from huge retrieval\nlatency and inefficiency issues, which makes them impractical in realistic\napplications. To address these crucial gaps towards both improved and efficient\ncross-modal retrieval, we propose a novel fine-tuning framework that turns any\npretrained text-image multi-modal model into an efficient retrieval model. The\nframework is based on a cooperative retrieve-and-rerank approach which\ncombines: 1) twin networks (i.e., a bi-encoder) to separately encode all items\nof a corpus, enabling efficient initial retrieval, and 2) a cross-encoder\ncomponent for a more nuanced (i.e., smarter) ranking of the retrieved small set\nof items. We also propose to jointly fine-tune the two components with shared\nweights, yielding a more parameter-efficient model. Our experiments on a series\nof standard cross-modal retrieval benchmarks in monolingual, multilingual, and\nzero-shot setups, demonstrate improved accuracy and huge efficiency benefits\nover the state-of-the-art cross-encoders.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Geigle_G/0/1/0/all/0/1\">Gregor Geigle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfeiffer_J/0/1/0/all/0/1\">Jonas Pfeiffer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reimers_N/0/1/0/all/0/1\">Nils Reimers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vulic_I/0/1/0/all/0/1\">Ivan Vuli&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"M6-UFC: Unifying Multi-Modal Controls for Conditional Image Synthesis via Non-Autoregressive Generative Transformers. (arXiv:2105.14211v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.14211","description":"<p>Conditional image synthesis aims to create an image according to some\nmulti-modal guidance in the forms of textual descriptions, reference images,\nand image blocks to preserve, as well as their combinations. In this paper,\ninstead of investigating these control signals separately, we propose a new\ntwo-stage architecture, M6-UFC, to unify any number of multi-modal controls. In\nM6-UFC, both the diverse control signals and the synthesized image are\nuniformly represented as a sequence of discrete tokens to be processed by\nTransformer. Different from existing two-stage autoregressive approaches such\nas DALL-E and VQGAN, M6-UFC adopts non-autoregressive generation (NAR) at the\nsecond stage to enhance the holistic consistency of the synthesized image, to\nsupport preserving specified image blocks, and to improve the synthesis speed.\nFurther, we design a progressive algorithm that iteratively improves the\nnon-autoregressively generated image, with the help of two estimators developed\nfor evaluating the compliance with the controls and evaluating the fidelity of\nthe synthesized image, respectively. Extensive experiments on a newly collected\nlarge-scale clothing dataset M2C-Fashion and a facial dataset Multi-Modal\nCelebA-HQ verify that M6-UFC can synthesize high-fidelity images that comply\nwith flexible multi-modal controls.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jianxin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Men_R/0/1/0/all/0/1\">Rui Men</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhikang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1\">Ming Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jie Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jingren Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hongxia Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Z2P: Instant Visualization of Point Clouds. (arXiv:2105.14548v2 [cs.GR] UPDATED)","link":"http://arxiv.org/abs/2105.14548","description":"<p>We present a technique for visualizing point clouds using a neural network.\nOur technique allows for an instant preview of any point cloud, and bypasses\nthe notoriously difficult surface reconstruction problem or the need to\nestimate oriented normals for splat-based rendering. We cast the preview\nproblem as a conditional image-to-image translation task, and design a neural\nnetwork that translates point depth-map directly into an image, where the point\ncloud is visualized as though a surface was reconstructed from it. Furthermore,\nthe resulting appearance of the visualized point cloud can be, optionally,\nconditioned on simple control variables (e.g., color and light). We demonstrate\nthat our technique instantly produces plausible images, and can, on-the-fly\neffectively handle noise, non-uniform sampling, and thin surfaces sheets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Metzer_G/0/1/0/all/0/1\">Gal Metzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanocka_R/0/1/0/all/0/1\">Rana Hanocka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giryes_R/0/1/0/all/0/1\">Raja Giryes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitra_N/0/1/0/all/0/1\">Niloy J. Mitra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_Or_D/0/1/0/all/0/1\">Daniel Cohen-Or</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The 2021 Image Similarity Dataset and Challenge. (arXiv:2106.09672v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.09672","description":"<p>This paper introduces a new benchmark for large-scale image similarity\ndetection. This benchmark is used for the Image Similarity Challenge at\nNeurIPS'21 (ISC2021). The goal is to determine whether a query image is a\nmodified copy of any image in a reference corpus of size 1~million. The\nbenchmark features a variety of image transformations such as automated\ntransformations, hand-crafted image edits and machine-learning based\nmanipulations. This mimics real-life cases appearing in social media, for\nexample for integrity-related problems dealing with misinformation and\nobjectionable content. The strength of the image manipulations, and therefore\nthe difficulty of the benchmark, is calibrated according to the performance of\na set of baseline approaches. Both the query and reference set contain a\nmajority of \"distractor\" images that do not match, which corresponds to a\nreal-life needle-in-haystack setting, and the evaluation metric reflects that.\nWe expect the DISC21 benchmark to promote image copy detection as an important\nand challenging computer vision task and refresh the state of the art. Code and\ndata are available at https://github.com/facebookresearch/isc2021\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Douze_M/0/1/0/all/0/1\">Matthijs Douze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tolias_G/0/1/0/all/0/1\">Giorgos Tolias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pizzi_E/0/1/0/all/0/1\">Ed Pizzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papakipos_Z/0/1/0/all/0/1\">Zo&#xeb; Papakipos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chanussot_L/0/1/0/all/0/1\">Lowik Chanussot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radenovic_F/0/1/0/all/0/1\">Filip Radenovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jenicek_T/0/1/0/all/0/1\">Tomas Jenicek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maximov_M/0/1/0/all/0/1\">Maxim Maximov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leal_Taixe_L/0/1/0/all/0/1\">Laura Leal-Taix&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elezi_I/0/1/0/all/0/1\">Ismail Elezi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chum_O/0/1/0/all/0/1\">Ond&#x159;ej Chum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrer_C/0/1/0/all/0/1\">Cristian Canton Ferrer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning for Technical Document Classification. (arXiv:2106.14269v5 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.14269","description":"<p>In large technology companies, the requirements for managing and organizing\ntechnical documents created by engineers and managers have increased\ndramatically in recent years, which has led to a higher demand for more\nscalable, accurate, and automated document classification. Prior studies have\nonly focused on processing text for classification, whereas technical documents\noften contain multimodal information. To leverage multimodal information for\ndocument classification to improve the model performance, this paper presents a\nnovel multimodal deep learning architecture, TechDoc, which utilizes three\ntypes of information, including natural language texts and descriptive images\nwithin documents and the associations among the documents. The architecture\nsynthesizes the convolutional neural network, recurrent neural network, and\ngraph neural network through an integrated training process. We applied the\narchitecture to a large multimodal technical document database and trained the\nmodel for classifying documents based on the hierarchical International Patent\nClassification system. Our results show that TechDoc presents a greater\nclassification accuracy than the unimodal methods and other state-of-the-art\nbenchmarks. The trained model can potentially be scaled to millions of\nreal-world multimodal technical documents, which is useful for data and\nknowledge management in large technology companies and organizations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Shuo Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jie Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Magee_C/0/1/0/all/0/1\">Christopher L. Magee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jianxi Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Application of artificial intelligence techniques for automated detection of myocardial infarction: A review. (arXiv:2107.06179v2 [eess.SP] UPDATED)","link":"http://arxiv.org/abs/2107.06179","description":"<p>Myocardial infarction (MI) results in heart muscle injury due to receiving\ninsufficient blood flow. MI is the most common cause of mortality in\nmiddle-aged and elderly individuals around the world. To diagnose MI,\nclinicians need to interpret electrocardiography (ECG) signals, which requires\nexpertise and is subject to observer bias. Artificial intelligence-based\nmethods can be utilized to screen for or diagnose MI automatically using ECG\nsignals. In this work, we conducted a comprehensive assessment of artificial\nintelligence-based approaches for MI detection based on ECG as well as other\nbiophysical signals, including machine learning (ML) and deep learning (DL)\nmodels. The performance of traditional ML methods relies on handcrafted\nfeatures and manual selection of ECG signals, whereas DL models can automate\nthese tasks. The review observed that deep convolutional neural networks\n(DCNNs) yielded excellent classification performance for MI diagnosis, which\nexplains why they have become prevalent in recent years. To our knowledge, this\nis the first comprehensive survey of artificial intelligence techniques\nemployed for MI diagnosis using ECG and other biophysical signals.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Joloudari_J/0/1/0/all/0/1\">Javad Hassannataj Joloudari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mojrian_S/0/1/0/all/0/1\">Sanaz Mojrian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nodehi_I/0/1/0/all/0/1\">Issa Nodehi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mashmool_A/0/1/0/all/0/1\">Amir Mashmool</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zadegan_Z/0/1/0/all/0/1\">Zeynab Kiani Zadegan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shirkharkolaie_S/0/1/0/all/0/1\">Sahar Khanjani Shirkharkolaie</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Alizadehsani_R/0/1/0/all/0/1\">Roohallah Alizadehsani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tamadon_T/0/1/0/all/0/1\">Tahereh Tamadon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khosravi_S/0/1/0/all/0/1\">Samiyeh Khosravi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kohnehshari_M/0/1/0/all/0/1\">Mitra Akbari Kohnehshari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hassannatajjeloudari_E/0/1/0/all/0/1\">Edris Hassannatajjeloudari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sharifrazi_D/0/1/0/all/0/1\">Danial Sharifrazi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mosavi_A/0/1/0/all/0/1\">Amir Mosavi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Loh_H/0/1/0/all/0/1\">Hui Wen Loh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tan_R/0/1/0/all/0/1\">Ru-San Tan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Acharya_U/0/1/0/all/0/1\">U Rajendra Acharya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VILENS: Visual, Inertial, Lidar, and Leg Odometry for All-Terrain Legged Robots. (arXiv:2107.07243v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2107.07243","description":"<p>We present VILENS (Visual Inertial Lidar Legged Navigation System), an\nodometry system for legged robots based on factor graphs. The key novelty is\nthe tight fusion of four different sensor modalities to achieve reliable\noperation when the individual sensors would otherwise produce degenerate\nestimation. To minimize leg odometry drift, we extend the robot's state with a\nlinear velocity bias term which is estimated online. This bias is observable\nbecause of the tight fusion of this preintegrated velocity factor with vision,\nlidar, and IMU factors. Extensive experimental validation on different ANYmal\nquadruped robots is presented, for a total duration of 2 h and 1.8 km traveled.\nThe experiments involved dynamic locomotion over loose rocks, slopes, and mud\nwhich caused challenges like slippage and terrain deformation. Perceptual\nchallenges included dark and dusty underground caverns, and open and\nfeature-deprived areas. We show an average improvement of 62% translational and\n51% rotational errors compared to a state-of-the-art loosely coupled approach.\nTo demonstrate its robustness, VILENS was also integrated with a perceptive\ncontroller and a local path planner.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wisth_D/0/1/0/all/0/1\">David Wisth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camurri_M/0/1/0/all/0/1\">Marco Camurri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fallon_M/0/1/0/all/0/1\">Maurice Fallon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GANmapper: geographical data translation. (arXiv:2108.04232v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.04232","description":"<p>We present a new method to create spatial data using a generative adversarial\nnetwork (GAN). Our contribution uses coarse and widely available geospatial\ndata to create maps of less available features at the finer scale in the built\nenvironment, bypassing their traditional acquisition techniques (e.g. satellite\nimagery or land surveying). In the work, we employ land use data and road\nnetworks as input to generate building footprints and conduct experiments in 9\ncities around the world. The method, which we implement in a tool we release\nopenly, enables the translation of one geospatial dataset to another with high\nfidelity and morphological accuracy. It may be especially useful in locations\nmissing detailed and high-resolution data and those that are mapped with\nuncertain or heterogeneous quality, such as much of OpenStreetMap. The quality\nof the results is influenced by the urban form and scale. In most cases, the\nexperiments suggest promising performance as the method tends to truthfully\nindicate the locations, amount, and shape of buildings. The work has the\npotential to support several applications, such as energy, climate, and urban\nmorphology studies in areas previously lacking required data or inpainting\ngeospatial data in regions with incomplete data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_A/0/1/0/all/0/1\">Abraham Noah Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biljecki_F/0/1/0/all/0/1\">Filip Biljecki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DKM: Differentiable K-Means Clustering Layer for Neural Network Compression. (arXiv:2108.12659v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2108.12659","description":"<p>Deep neural network (DNN) model compression for efficient on-device inference\nis becoming increasingly important to reduce memory requirements and keep user\ndata on-device. To this end, we propose a novel differentiable k-means\nclustering layer (DKM) and its application to train-time weight\nclustering-based DNN model compression. DKM casts k-means clustering as an\nattention problem and enables joint optimization of the DNN parameters and\nclustering centroids. Unlike prior works that rely on additional regularizers\nand parameters, DKM-based compression keeps the original loss function and\nmodel architecture fixed. We evaluated DKM-based compression on various DNN\nmodels for computer vision and natural language processing (NLP) tasks. Our\nresults demonstrate that DKM delivers superior compression and accuracy\ntrade-off on ImageNet1k and GLUE benchmarks. For example, DKM-based compression\ncan offer 74.5% top-1 ImageNet1k accuracy on ResNet50 DNN model with 3.3MB\nmodel size (29.4x model compression factor). For MobileNet-v1, which is a\nchallenging DNN to compress, DKM delivers 63.9% top-1 ImageNet1k accuracy with\n0.72 MB model size (22.4x model compression factor). This result is 6.8% higher\ntop-1accuracy and 33% relatively smaller model size than the current\nstate-of-the-art DNN compression algorithms. Additionally, DKM enables\ncompression of DistilBERT model by 11.8x with minimal (1.1%) accuracy loss on\nGLUE NLP benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cho_M/0/1/0/all/0/1\">Minsik Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vahid_K/0/1/0/all/0/1\">Keivan A. Vahid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adya_S/0/1/0/all/0/1\">Saurabh Adya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rastegari_M/0/1/0/all/0/1\">Mohammad Rastegari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fair Representation: Guaranteeing Approximate Multiple Group Fairness for Unknown Tasks. (arXiv:2109.00545v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2109.00545","description":"<p>Motivated by scenarios where data is used for diverse prediction tasks, we\nstudy whether fair representation can be used to guarantee fairness for unknown\ntasks and for multiple fairness notions simultaneously. We consider seven group\nfairness notions that cover the concepts of independence, separation, and\ncalibration. Against the backdrop of the fairness impossibility results, we\nexplore approximate fairness. We prove that, although fair representation might\nnot guarantee fairness for all prediction tasks, it does guarantee fairness for\nan important subset of tasks -- the tasks for which the representation is\ndiscriminative. Specifically, all seven group fairness notions are linearly\ncontrolled by fairness and discriminativeness of the representation. When an\nincompatibility exists between different fairness notions, fair and\ndiscriminative representation hits the sweet spot that approximately satisfies\nall notions. Motivated by our theoretical findings, we propose to learn both\nfair and discriminative representations using pretext loss which\nself-supervises learning, and Maximum Mean Discrepancy as a fair regularizer.\nExperiments on tabular, image, and face datasets show that using the learned\nrepresentation, downstream predictions that we are unaware of when learning the\nrepresentation indeed become fairer for seven group fairness notions, and the\nfairness guarantees computed from our theoretical results are all valid.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xudong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_Y/0/1/0/all/0/1\">Yongkang Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kankanhalli_M/0/1/0/all/0/1\">Mohan Kankanhalli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fair Conformal Predictors for Applications in Medical Imaging. (arXiv:2109.04392v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2109.04392","description":"<p>Deep learning has the potential to augment several clinically useful aspects\nof the radiologist's workflow such as medical imaging interpretation. However,\nthe translation of deep learning algorithms into clinical practice has been\nhindered by relative lack of transparency in these algorithms compared to more\ntraditional statistical methods. Specifically, common deep learning models lack\nintuitive and rigorous methods of conveying prediction confidence in a\ncalibrated manner, which ultimately restricts widespread use of these \"black\nbox\" systems for critical decision-making. Furthermore, numerous demonstrations\nof algorithmic bias in clinical machine learning have caused considerable\nhesitancy towards the deployment of these models for clinical application. To\nthis end, we explore how conformal predictions can complement existing deep\nlearning approaches by providing an intuitive way of expressing model\nuncertainty to facilitate greater transparency to clinical users. In this\npaper, we conduct field interviews with radiologists to assess potential\nuse-cases of conformal predictors. Using insights collected from these\ninterviews, we devise two use-cases and empirically evaluate several conformal\nmethods on a dermatology photography dataset for skin lesion classification.\nAdditionally, we show how group conformal predictors are more adaptive to\ndifferences between patient skin tones for malignant skin lesions. We find our\nconformal predictors to be a promising and generally applicable approach to\nincreasing clinical usability and trustworthiness -- hopefully facilitating\nbetter modes of collaboration between medical AI tools and their clinical\nusers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lu_C/0/1/0/all/0/1\">Charles Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lemay_A/0/1/0/all/0/1\">Andreanne Lemay</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chang_K/0/1/0/all/0/1\">Ken Chang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hoebel_K/0/1/0/all/0/1\">Katharina Hoebel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kalpathy_Cramer_J/0/1/0/all/0/1\">Jayashree Kalpathy-Cramer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning a Metacognition for Object Detection. (arXiv:2110.03105v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2110.03105","description":"<p>In contrast to object recognition models, humans do not blindly trust their\nperception when building representations of the world, instead recruiting\nmetacognition to detect percepts that are unreliable or false, such as when we\nrealize that we mistook one object for another. We propose METAGEN, an\nunsupervised model that enhances object recognition models through a\nmetacognition. Given noisy output from an object-detection model, METAGEN\nlearns a meta-representation of how its perceptual system works and uses it to\ninfer the objects in the world responsible for the detections. METAGEN achieves\nthis by conditioning its inference on basic principles of objects that even\nhuman infants understand (known as Spelke principles: object permanence,\ncohesion, and spatiotemporal continuity). We test METAGEN on a variety of\nstate-of-the-art object detection neural networks. We find that METAGEN quickly\nlearns an accurate metacognitive representation of the neural network, and that\nthis improves detection accuracy by filling in objects that the detection model\nmissed and removing hallucinated objects. This approach enables generalization\nto out-of-sample data and outperforms comparison models that lack a\nmetacognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Berke_M/0/1/0/all/0/1\">Marlene Berke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belledonne_M/0/1/0/all/0/1\">Mario Belledonne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azerbayev_Z/0/1/0/all/0/1\">Zhangir Azerbayev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jara_Ettinger_J/0/1/0/all/0/1\">Julian Jara-Ettinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Subspace Regularizers for Few-Shot Class Incremental Learning. (arXiv:2110.07059v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.07059","description":"<p>Few-shot class incremental learning -- the problem of updating a trained\nclassifier to discriminate among an expanded set of classes with limited\nlabeled data -- is a key challenge for machine learning systems deployed in\nnon-stationary environments. Existing approaches to the problem rely on complex\nmodel architectures and training procedures that are difficult to tune and\nre-use. In this paper, we present an extremely simple approach that enables the\nuse of ordinary logistic regression classifiers for few-shot incremental\nlearning. The key to this approach is a new family of subspace regularization\nschemes that encourage weight vectors for new classes to lie close to the\nsubspace spanned by the weights of existing classes. When combined with\npretrained convolutional feature extractors, logistic regression models trained\nwith subspace regularization outperform specialized, state-of-the-art\napproaches to few-shot incremental image classification by up to 22% on the\nminiImageNet dataset. Because of its simplicity, subspace regularization can be\nstraightforwardly extended to incorporate additional background information\nabout the new classes (including class names and descriptions specified in\nnatural language); these further improve accuracy by up to 2%. Our results show\nthat simple geometric regularization of class representations offers an\neffective tool for continual learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Akyurek_A/0/1/0/all/0/1\">Afra Feyza Aky&#xfc;rek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akyurek_E/0/1/0/all/0/1\">Ekin Aky&#xfc;rek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wijaya_D/0/1/0/all/0/1\">Derry Tanti Wijaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andreas_J/0/1/0/all/0/1\">Jacob Andreas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep multi-modal aggregation network for MR image reconstruction with auxiliary modality. (arXiv:2110.08080v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2110.08080","description":"<p>Magnetic resonance (MR) imaging produces detailed images of organs and\ntissues with better contrast, but it suffers from a long acquisition time,\nwhich makes the image quality vulnerable to say motion artifacts. Recently,\nmany approaches have been developed to reconstruct full-sampled images from\npartially observed measurements to accelerate MR imaging. However, most\napproaches focused on reconstruction over a single modality, neglecting the\ndiscovery of correlation knowledge between the different modalities. Here we\npropose a Multi-modal Aggregation network for mR Image recOnstruction with\nauxiliary modality (MARIO), which is capable of discovering complementary\nrepresentations from a fully sampled auxiliary modality, with which to\nhierarchically guide the reconstruction of a given target modality. This\nimplies that our method can selectively aggregate multi-modal representations\nfor better reconstruction, yielding comprehensive, multi-scale, multi-modal\nfeature fusion. Extensive experiments on IXI and fastMRI datasets demonstrate\nthe superiority of the proposed approach over state-of-the-art MR image\nreconstruction methods in removing artifacts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Feng_C/0/1/0/all/0/1\">Chun-Mei Feng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fu_H/0/1/0/all/0/1\">Huazhu Fu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_T/0/1/0/all/0/1\">Tianfei Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Y/0/1/0/all/0/1\">Yong Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_D/0/1/0/all/0/1\">David Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncertainty-Aware Lung Nodule Segmentation with Multiple Annotations. (arXiv:2110.12372v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2110.12372","description":"<p>Since radiologists have different training and clinical experience, they may\nprovide various segmentation maps for a lung nodule. As a result, for a\nspecific lung nodule, some regions have a higher chance of causing segmentation\nuncertainty, which brings difficulty for lung nodule segmentation with multiple\nannotations. To address this problem, this paper proposes an Uncertainty-Aware\nSegmentation Network (UAS-Net) based on multi-branch U-Net, which can learn the\nvaluable visual features from the regions that may cause segmentation\nuncertainty and contribute to a better segmentation result. Meanwhile, this\nnetwork can provide a Multi-Confidence Mask (MCM) simultaneously, pointing out\nregions with different segmentation uncertainty levels. We introduce a\nFeature-Aware Concatenation structure for different learning targets and let\neach branch have a specific learning preference. Moreover, a joint adversarial\nlearning process is also adopted to help learn discriminative features of\ncomplex structures. Experimental results show that our method can predict the\nreasonable regions with higher uncertainty and improve lung nodule segmentation\nperformance in LIDC-IDRI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_Q/0/1/0/all/0/1\">Qiuli Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_H/0/1/0/all/0/1\">Han Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shen_L/0/1/0/all/0/1\">Lu Shen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_M/0/1/0/all/0/1\">Mengke Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IconQA: A New Benchmark for Abstract Diagram Understanding and Visual Language Reasoning. (arXiv:2110.13214v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.13214","description":"<p>Current visual question answering (VQA) tasks mainly consider answering\nhuman-annotated questions for natural images. However, aside from natural\nimages, abstract diagrams with semantic richness are still understudied in\nvisual understanding and reasoning research. In this work, we introduce a new\nchallenge of Icon Question Answering (IconQA) with the goal of answering a\nquestion in an icon image context. We release IconQA, a large-scale dataset\nthat consists of 107,439 questions and three sub-tasks: multi-image-choice,\nmulti-text-choice, and filling-in-the-blank. The IconQA dataset is inspired by\nreal-world diagram word problems that highlight the importance of abstract\ndiagram understanding and comprehensive cognitive reasoning. Thus, IconQA\nrequires not only perception skills like object recognition and text\nunderstanding, but also diverse cognitive reasoning skills, such as geometric\nreasoning, commonsense reasoning, and arithmetic reasoning. To facilitate\npotential IconQA models to learn semantic representations for icon images, we\nfurther release an icon dataset Icon645 which contains 645,687 colored icons on\n377 classes. We conduct extensive user studies and blind experiments and\nreproduce a wide range of advanced VQA methods to benchmark the IconQA task.\nAlso, we develop a strong IconQA baseline Patch-TRM that applies a pyramid\ncross-modal Transformer with input diagram embeddings pre-trained on the icon\ndataset. IconQA and Icon645 are available at https://iconqa.github.io.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1\">Pan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_L/0/1/0/all/0/1\">Liang Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiaqi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_T/0/1/0/all/0/1\">Tony Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yizhou Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Song-Chun Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RRNet: Relational Reasoning Network with Parallel Multi-scale Attention for Salient Object Detection in Optical Remote Sensing Images. (arXiv:2110.14223v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.14223","description":"<p>Salient object detection (SOD) for optical remote sensing images (RSIs) aims\nat locating and extracting visually distinctive objects/regions from the\noptical RSIs. Despite some saliency models were proposed to solve the intrinsic\nproblem of optical RSIs (such as complex background and scale-variant objects),\nthe accuracy and completeness are still unsatisfactory. To this end, we propose\na relational reasoning network with parallel multi-scale attention for SOD in\noptical RSIs in this paper. The relational reasoning module that integrates the\nspatial and the channel dimensions is designed to infer the semantic\nrelationship by utilizing high-level encoder features, thereby promoting the\ngeneration of more complete detection results. The parallel multi-scale\nattention module is proposed to effectively restore the detail information and\naddress the scale variation of salient objects by using the low-level features\nrefined by multi-scale attention. Extensive experiments on two datasets\ndemonstrate that our proposed RRNet outperforms the existing state-of-the-art\nSOD competitors both qualitatively and quantitatively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cong_R/0/1/0/all/0/1\">Runmin Cong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yumo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_L/0/1/0/all/0/1\">Leyuan Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yao Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwong_S/0/1/0/all/0/1\">Sam Kwong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Grained Vision Language Pre-Training: Aligning Texts with Visual Concepts. (arXiv:2111.08276v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.08276","description":"<p>Most existing methods in vision language pre-training rely on object-centric\nfeatures extracted through object detection, and make fine-grained alignments\nbetween the extracted features and texts. We argue that object detection may\nnot be necessary for vision language pre-training. To this end, we propose a\nnew method called X-VLM to perform `multi-grained vision language\npre-training.' The key of learning multi-grained alignments is to locate visual\nconcepts in the image given the associated texts, and in the meantime align the\ntexts with the visual concepts, where the alignments are in multi-granularity.\nExperimental results show that X-VLM effectively leverages the learned\nalignments to many downstream vision language tasks and consistently\noutperforms state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1\">Yan Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinsong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Modified Indicator Functions for Surface Reconstruction. (arXiv:2111.09526v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.09526","description":"<p>Surface reconstruction is a fundamental problem in 3D graphics. In this\npaper, we propose a learning-based approach for implicit surface reconstruction\nfrom raw point clouds without normals. Our method is inspired by Gauss Lemma in\npotential energy theory, which gives an explicit integral formula for the\nindicator functions. We design a novel deep neural network to perform surface\nintegral and learn the modified indicator functions from un-oriented and noisy\npoint clouds. We concatenate features with different scales for accurate\npoint-wise contributions to the integral. Moreover, we propose a novel Surface\nElement Feature Extractor to learn local shape properties. Experiments show\nthat our method generates smooth surfaces with high normal consistency from\npoint clouds with different noise scales and achieves state-of-the-art\nreconstruction performance compared with current data-driven and\nnon-data-driven approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_D/0/1/0/all/0/1\">Dong Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Siyou Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Zuoqiang Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bin Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spatial-context-aware deep neural network for multi-class image classification. (arXiv:2111.12296v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.12296","description":"<p>Multi-label image classification is a fundamental but challenging task in\ncomputer vision. Over the past few decades, solutions exploring relationships\nbetween semantic labels have made great progress. However, the underlying\nspatial-contextual information of labels is under-exploited. To tackle this\nproblem, a spatial-context-aware deep neural network is proposed to predict\nlabels taking into account both semantic and spatial information. This proposed\nframework is evaluated on Microsoft COCO and PASCAL VOC, two widely used\nbenchmark datasets for image multi-labelling. The results show that the\nproposed approach is superior to the state-of-the-art solutions on dealing with\nthe multi-label image classification problem.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jialu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Jianfeng Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yitian Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Human Pose Manipulation and Novel View Synthesis using Differentiable Rendering. (arXiv:2111.12731v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.12731","description":"<p>We present a new approach for synthesizing novel views of people in new\nposes. Our novel differentiable renderer enables the synthesis of highly\nrealistic images from any viewpoint. Rather than operating over mesh-based\nstructures, our renderer makes use of diffuse Gaussian primitives that directly\nrepresent the underlying skeletal structure of a human. Rendering these\nprimitives gives results in a high-dimensional latent image, which is then\ntransformed into an RGB image by a decoder network. The formulation gives rise\nto a fully differentiable framework that can be trained end-to-end. We\ndemonstrate the effectiveness of our approach to image reconstruction on both\nthe Human3.6M and Panoptic Studio datasets. We show how our approach can be\nused for motion transfer between individuals; novel view synthesis of\nindividuals captured from just a single camera; to synthesize individuals from\nany virtual viewpoint; and to re-render people in novel poses. Code and video\nresults are available at\nhttps://github.com/GuillaumeRochette/HumanViewSynthesis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rochette_G/0/1/0/all/0/1\">Guillaume Rochette</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Russell_C/0/1/0/all/0/1\">Chris Russell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bowden_R/0/1/0/all/0/1\">Richard Bowden</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploiting full Resolution Feature Context for Liver Tumor and Vessel Segmentation via Fusion Encoder: Application to Liver Tumor and Vessel 3D reconstruction. (arXiv:2111.13299v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2111.13299","description":"<p>Liver cancer is one of the most common malignant diseases in the world.\nSegmentation and labeling of liver tumors and blood vessels in CT images can\nprovide convenience for doctors in liver tumor diagnosis and surgical\nintervention. In the past decades, automatic CT segmentation methods based on\ndeep learning have received widespread attention in the medical field. Many\nstate-of-the-art segmentation algorithms appeared during this period. Yet, most\nof the existing segmentation methods only care about the local feature context\nand have a perception defect in the global relevance of medical images, which\nsignificantly affects the segmentation effect of liver tumors and blood\nvessels. We introduce a multi-scale feature context fusion network called\nTransFusionNet based on Transformer and SEBottleNet. This network can\naccurately detect and identify the details of the region of interest of the\nliver vessel, meanwhile it can improve the recognition of morphologic margins\nof liver tumors by exploiting the global information of CT images. Experiments\nshow that TransFusionNet is better than the state-of-the-art method on both the\npublic dataset LITS and 3Dircadb and our clinical dataset. Finally, we propose\nan automatic 3D reconstruction algorithm based on the trained model. The\nalgorithm can complete the reconstruction quickly and accurately in 1 second.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Meng_X/0/1/0/all/0/1\">Xiangyu Meng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1\">Xudong Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_G/0/1/0/all/0/1\">Gan Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Ying Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shi_X/0/1/0/all/0/1\">Xin Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dai_H/0/1/0/all/0/1\">Huanhuan Dai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1\">Zixuan Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1\">Xun Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PTTR: Relational 3D Point Cloud Object Tracking with Transformer. (arXiv:2112.02857v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.02857","description":"<p>In a point cloud sequence, 3D object tracking aims to predict the location\nand orientation of an object in the current search point cloud given a template\npoint cloud. Motivated by the success of transformers, we propose Point\nTracking TRansformer (PTTR), which efficiently predicts high-quality 3D\ntracking results in a coarse-to-fine manner with the help of transformer\noperations. PTTR consists of three novel designs. 1) Instead of random\nsampling, we design Relation-Aware Sampling to preserve relevant points to\ngiven templates during subsampling. 2) Furthermore, we propose a Point Relation\nTransformer (PRT) consisting of a self-attention and a cross-attention module.\nThe global self-attention operation captures long-range dependencies to enhance\nencoded point features for the search area and the template, respectively.\nSubsequently, we generate the coarse tracking results by matching the two sets\nof point features via cross-attention. 3) Based on the coarse tracking results,\nwe employ a novel Prediction Refinement Module to obtain the final refined\nprediction. In addition, we create a large-scale point cloud single object\ntracking benchmark based on the Waymo Open Dataset. Extensive experiments show\nthat PTTR achieves superior point cloud tracking in both accuracy and\nefficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Changqing Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Zhipeng Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yueru Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianrui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1\">Liang Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1\">Zhongang Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Haiyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shijian Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Annotation-efficient cancer detection with report-guided lesion annotation for deep learning-based prostate cancer detection in bpMRI. (arXiv:2112.05151v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2112.05151","description":"<p>Deep learning-based diagnostic performance increases with more annotated\ndata, but large-scale manual annotations are expensive and labour-intensive.\nExperts evaluate diagnostic images during clinical routine, and write their\nfindings in reports. Leveraging unlabelled exams paired with clinical reports\ncould overcome the manual labelling bottleneck. We hypothesise that detection\nmodels can be trained semi-supervised with automatic annotations generated\nusing model predictions, guided by sparse information from clinical reports. To\ndemonstrate efficacy, we train clinically significant prostate cancer (csPCa)\nsegmentation models, where automatic annotations are guided by the number of\nclinically significant findings in the radiology reports. We included 7,756\nprostate MRI examinations, of which 3,050 were manually annotated. We evaluated\nprostate cancer detection performance on 300 exams from an external centre with\nhistopathology-confirmed ground truth. Semi-supervised training improved\npatient-based diagnostic area under the receiver operating characteristic curve\nfrom $87.2 \\pm 0.8\\%$ to $89.4 \\pm 1.0\\%$ ($P&lt;10^{-4}$) and improved\nlesion-based sensitivity at one false positive per case from $76.4 \\pm 3.8\\%$\nto $83.6 \\pm 2.3\\%$ ($P&lt;10^{-4}$). Semi-supervised training was 14$\\times$ more\nannotation-efficient for case-based performance and 6$\\times$ more\nannotation-efficient for lesion-based performance. This improved performance\ndemonstrates the feasibility of our training procedure. Source code is publicly\navailable at github.com/DIAGNijmegen/Report-Guided-Annotation. Best csPCa\ndetection algorithm is available at\ngrand-challenge.org/algorithms/bpmri-cspca-detection-report-guided-annotations/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bosma_J/0/1/0/all/0/1\">Joeran S. Bosma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Saha_A/0/1/0/all/0/1\">Anindo Saha</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hosseinzadeh_M/0/1/0/all/0/1\">Matin Hosseinzadeh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Slootweg_I/0/1/0/all/0/1\">Ilse Slootweg</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rooij_M/0/1/0/all/0/1\">Maarten de Rooij</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huisman_H/0/1/0/all/0/1\">Henkjan Huisman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Homography Decomposition Networks for Planar Object Tracking. (arXiv:2112.07909v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.07909","description":"<p>Planar object tracking plays an important role in AI applications, such as\nrobotics, visual servoing, and visual SLAM. Although the previous planar\ntrackers work well in most scenarios, it is still a challenging task due to the\nrapid motion and large transformation between two consecutive frames. The\nessential reason behind this problem is that the condition number of such a\nnon-linear system changes unstably when the searching range of the homography\nparameter space becomes larger. To this end, we propose a novel Homography\nDecomposition Networks(HDN) approach that drastically reduces and stabilizes\nthe condition number by decomposing the homography transformation into two\ngroups. Specifically, a similarity transformation estimator is designed to\npredict the first group robustly by a deep convolution equivariant network. By\ntaking advantage of the scale and rotation estimation with high confidence, a\nresidual transformation is estimated by a simple regression model. Furthermore,\nthe proposed end-to-end network is trained in a semi-supervised fashion.\nExtensive experiments show that our proposed approach outperforms the\nstate-of-the-art planar tracking methods at a large margin on the challenging\nPOT, UCSB and POIC datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhan_X/0/1/0/all/0/1\">Xinrui Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yueran Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jianke Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision Transformer with Deformable Attention. (arXiv:2201.00520v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.00520","description":"<p>Transformers have recently shown superior performances on various vision\ntasks. The large, sometimes even global, receptive field endows Transformer\nmodels with higher representation power over their CNN counterparts.\nNevertheless, simply enlarging receptive field also gives rise to several\nconcerns. On the one hand, using dense attention e.g., in ViT, leads to\nexcessive memory and computational cost, and features can be influenced by\nirrelevant parts which are beyond the region of interests. On the other hand,\nthe sparse attention adopted in PVT or Swin Transformer is data agnostic and\nmay limit the ability to model long range relations. To mitigate these issues,\nwe propose a novel deformable self-attention module, where the positions of key\nand value pairs in self-attention are selected in a data-dependent way. This\nflexible scheme enables the self-attention module to focus on relevant regions\nand capture more informative features. On this basis, we present Deformable\nAttention Transformer, a general backbone model with deformable attention for\nboth image classification and dense prediction tasks. Extensive experiments\nshow that our models achieve consistently improved results on comprehensive\nbenchmarks. Code is available at https://github.com/LeapLabTHU/DAT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xia_Z/0/1/0/all/0/1\">Zhuofan Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1\">Xuran Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Shiji Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Li Erran Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Gao Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UniFormer: Unifying Convolution and Self-attention for Visual Recognition. (arXiv:2201.09450v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.09450","description":"<p>It is a challenging task to learn discriminative representation from images\nand videos, due to large local redundancy and complex global dependency in\nthese visual data. Convolution neural networks (CNNs) and vision transformers\n(ViTs) have been two dominant frameworks in the past few years. Though CNNs can\nefficiently decrease local redundancy by convolution within a small\nneighborhood, the limited receptive field makes it hard to capture global\ndependency. Alternatively, ViTs can effectively capture long-range dependency\nvia self-attention, while blind similarity comparisons among all the tokens\nlead to high redundancy. To resolve these problems, we propose a novel Unified\ntransFormer (UniFormer), which can seamlessly integrate the merits of\nconvolution and self-attention in a concise transformer format. Different from\nthe typical transformer blocks, the relation aggregators in our UniFormer block\nare equipped with local and global token affinity respectively in shallow and\ndeep layers, allowing to tackle both redundancy and dependency for efficient\nand effective representation learning. Finally, we flexibly stack our UniFormer\nblocks into a new powerful backbone, and adopt it for various vision tasks from\nimage to video domain, from classification to dense prediction. Without any\nextra training data, our UniFormer achieves 86.3 top-1 accuracy on ImageNet-1K\nclassification. With only ImageNet-1K pre-training, it can simply achieve\nstate-of-the-art performance in a broad range of downstream tasks, e.g., it\nobtains 82.9/84.8 top-1 accuracy on Kinetics-400/600, 60.9/71.2 top-1 accuracy\non Something-Something V1/V2 video classification tasks, 53.8 box AP and 46.4\nmask AP on COCO object detection task, 50.8 mIoU on ADE20K semantic\nsegmentation task, and 77.4 AP on COCO pose estimation task. Code is available\nat https://github.com/Sense-X/UniFormer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kunchang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yali Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Junhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Peng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_G/0/1/0/all/0/1\">Guanglu Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Proximal denoiser for convergent plug-and-play optimization with nonconvex regularization. (arXiv:2201.13256v2 [math.OC] UPDATED)","link":"http://arxiv.org/abs/2201.13256","description":"<p>Plug-and-Play (PnP) methods solve ill-posed inverse problems through\niterative proximal algorithms by replacing a proximal operator by a denoising\noperation. When applied with deep neural network denoisers, these methods have\nshown state-of-the-art visual performance for image restoration problems.\nHowever, their theoretical convergence analysis is still incomplete. Most of\nthe existing convergence results consider nonexpansive denoisers, which is\nnon-realistic, or limit their analysis to strongly convex data-fidelity terms\nin the inverse problem to solve. Recently, it was proposed to train the\ndenoiser as a gradient descent step on a functional parameterized by a deep\nneural network. Using such a denoiser guarantees the convergence of the PnP\nversion of the Half-Quadratic-Splitting (PnP-HQS) iterative algorithm. In this\npaper, we show that this gradient denoiser can actually correspond to the\nproximal operator of another scalar function. Given this new result, we exploit\nthe convergence theory of proximal algorithms in the nonconvex setting to\nobtain convergence results for PnP-PGD (Proximal Gradient Descent) and PnP-ADMM\n(Alternating Direction Method of Multipliers). When built on top of a smooth\ngradient denoiser, we show that PnP-PGD and PnP-ADMM are convergent and target\nstationary points of an explicit functional. These convergence results are\nconfirmed with numerical experiments on deblurring, super-resolution and\ninpainting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/math/1/au:+Hurault_S/0/1/0/all/0/1\">Samuel Hurault</a>, <a href=\"http://arxiv.org/find/math/1/au:+Leclaire_A/0/1/0/all/0/1\">Arthur Leclaire</a>, <a href=\"http://arxiv.org/find/math/1/au:+Papadakis_N/0/1/0/all/0/1\">Nicolas Papadakis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Coding Framework and Benchmark towards Compressed Video Understanding. (arXiv:2202.02813v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2202.02813","description":"<p>Most video understanding methods are learned on high-quality videos. However,\nin real-world scenarios, the videos are first compressed before the\ntransportation and then decompressed for understanding. The decompressed videos\nmay have lost the critical information to the downstream tasks. To address this\nissue, we propose the first coding framework for compressed video\nunderstanding, where another learnable analytic bitstream is simultaneously\ntransported with the original video bitstream. With the dedicatedly designed\nself-supervised optimization target and dynamic network architectures, this new\nstream largely boosts the downstream tasks yet with a small bit cost. By only\none-time training, our framework can be deployed for multiple downstream tasks.\nOur framework also enjoys the best of both two worlds, (1) high efficiency of\nindustrial video codec and (2) flexible coding capability of neural networks\n(NNs). Finally, we build a rigorous benchmark for compressed video\nunderstanding on three popular tasks over seven large-scale datasets and four\ndifferent compression levels. The proposed Understanding oriented Video Coding\nframework UVC consistently demonstrates significantly stronger performances\nthan the baseline industrial codec.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Tian_Y/0/1/0/all/0/1\">Yuan Tian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_G/0/1/0/all/0/1\">Guo Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yan_Y/0/1/0/all/0/1\">Yichao Yan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhai_G/0/1/0/all/0/1\">Guangtao Zhai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_L/0/1/0/all/0/1\">Li Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gao_Z/0/1/0/all/0/1\">Zhiyong Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Patch-Based Stochastic Attention for Image Editing. (arXiv:2202.03163v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.03163","description":"<p>Attention mechanisms have become of crucial importance in deep learning in\nrecent years. These non-local operations, which are similar to traditional\npatch-based methods in image processing, complement local convolutions.\nHowever, computing the full attention matrix is an expensive step with a heavy\nmemory and computational load. These limitations curb network architectures and\nperformances, in particular for the case of high resolution images. We propose\nan efficient attention layer based on the stochastic algorithm PatchMatch,\nwhich is used for determining approximate nearest neighbors. We refer to our\nproposed layer as a \"Patch-based Stochastic Attention Layer\" (PSAL).\nFurthermore, we propose different approaches, based on patch aggregation, to\nensure the differentiability of PSAL, thus allowing end-to-end training of any\nnetwork containing our layer. PSAL has a small memory footprint and can\ntherefore scale to high resolution images. It maintains this footprint without\nsacrificing spatial precision and globality of the nearest neighbours, which\nmeans that it can be easily inserted in any level of a deep architecture, even\nin shallower levels. We demonstrate the usefulness of PSAL on several image\nediting tasks, such as image inpainting and image colorization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cherel_N/0/1/0/all/0/1\">Nicolas Cherel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Almansa_A/0/1/0/all/0/1\">Andr&#xe9;s Almansa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gousseau_Y/0/1/0/all/0/1\">Yann Gousseau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Newson_A/0/1/0/all/0/1\">Alasdair Newson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Paced Imbalance Rectification for Class Incremental Learning. (arXiv:2202.03703v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.03703","description":"<p>Exemplar-based class-incremental learning is to recognize new classes while\nnot forgetting old ones, whose samples can only be saved in limited memory. The\nratio fluctuation of new samples to old exemplars, which is caused by the\nvariation of memory capacity at different environments, will bring challenges\nto stabilize the incremental optimization process. To address this problem, we\npropose a novel self-paced imbalance rectification scheme, which dynamically\nmaintains the incremental balance during the representation learning phase.\nSpecifically, our proposed scheme consists of a frequency compensation strategy\nthat adjusts the logits margin between old and new classes with the\ncorresponding number ratio to strengthen the expression ability of the old\nclasses, and an inheritance transfer strategy to reduce the representation\nconfusion by estimating the similarity of different classes in the old\nembedding space. Furthermore, a chronological attenuation mechanism is proposed\nto mitigate the repetitive optimization of the older classes at multiple\nstep-wise increments. Extensive experiments on three benchmarks demonstrate\nstable incremental performance, significantly outperforming the\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1\">Kai Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yang Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Face2PPG: An unsupervised pipeline for blood volume pulse extraction from faces. (arXiv:2202.04101v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.04101","description":"<p>Photoplethysmography (PPG) signals have become a key technology in many\nfields such as medicine, well-being, or sports. Our work proposes a set of\npipelines to extract remote PPG signals (rPPG) from the face, robustly,\nreliably, and in a configurable manner. We identify and evaluate the possible\nchoices in the critical steps of unsupervised rPPG methodologies. We evaluate a\nstate-of-the-art processing pipeline in six different datasets, incorporating\nimportant corrections in the methodology that ensure reproducible and fair\ncomparisons. In addition, we extend the pipeline by proposing three novel\nideas; 1) a new method to stabilize the detected face based on a rigid mesh\nnormalization; 2) a new method to dynamically select the different regions in\nthe face that provide the best raw signals, and 3) a new RGB to rPPG\ntransformation method called Orthogonal Matrix Image Transformation (OMIT)\nbased on QR decomposition, that increases robustness against compression\nartifacts. We show that all three changes introduce noticeable improvements in\nretrieving rPPG signals from faces, obtaining state-of-the-art results compared\nwith unsupervised, non-learning-based methodologies, and in some databases,\nvery close to supervised, learning-based methods. We perform a comparative\nstudy to quantify the contribution of each proposed idea. In addition, we\ndepict a series of observations that could help in future implementations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Casado_C/0/1/0/all/0/1\">Constantino &#xc1;lvarez Casado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lopez_M/0/1/0/all/0/1\">Miguel Bordallo L&#xf3;pez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Motion Sickness Modeling with Visual Vertical Estimation and Its Application to Autonomous Personal Mobility Vehicles. (arXiv:2202.06299v2 [cs.HC] UPDATED)","link":"http://arxiv.org/abs/2202.06299","description":"<p>Passengers (drivers) of level 3-5 autonomous personal mobility vehicles\n(APMV) and cars can perform non-driving tasks, such as reading books and\nsmartphones, while driving. It has been pointed out that such activities may\nincrease motion sickness. Many studies have been conducted to build\ncountermeasures, of which various computational motion sickness models have\nbeen developed. Many of these are based on subjective vertical conflict (SVC)\ntheory, which describes vertical changes in direction sensed by human sensory\norgans vs. those expected by the central nervous system. Such models are\nexpected to be applied to autonomous driving scenarios. However, no current\ncomputational model can integrate visual vertical information with vestibular\nsensations.\n</p>\n<p>We proposed a 6 DoF SVC-VV model which add a visually perceived vertical\nblock into a conventional six-degrees-of-freedom SVC model to predict VV\ndirections from image data simulating the visual input of a human. Hence, a\nsimple image-based VV estimation method is proposed.\n</p>\n<p>As the validation of the proposed model, this paper focuses on describing the\nfact that the motion sickness increases as a passenger reads a book while using\nan AMPV, assuming that visual vertical (VV) plays an important role. In the\nstatic experiment, it is demonstrated that the estimated VV by the proposed\nmethod accurately described the gravitational acceleration direction with a low\nmean absolute deviation. In addition, the results of the driving experiment\nusing an APMV demonstrated that the proposed 6 DoF SVC-VV model could describe\nthat the increased motion sickness experienced when the VV and gravitational\nacceleration directions were different.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hailong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inoue_S/0/1/0/all/0/1\">Shota Inoue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wada_T/0/1/0/all/0/1\">Takahiro Wada</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A precortical module for robust CNNs to light variations. (arXiv:2202.07432v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.07432","description":"<p>We present a simple mathematical model for the mammalian low visual pathway,\ntaking into account its key elements: retina, lateral geniculate nucleus (LGN),\nprimary visual cortex (V1). The analogies between the cortical level of the\nvisual system and the structure of popular CNNs, used in image classification\ntasks, suggests the introduction of an additional preliminary convolutional\nmodule inspired to precortical neuronal circuits to improve robustness with\nrespect to global light intensity and contrast variations in the input images.\nWe validate our hypothesis on the popular databases MNIST, FashionMNIST and\nSVHN, obtaining significantly more robust CNNs with respect to these\nvariations, once such extra module is added.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fioresi_R/0/1/0/all/0/1\">R. Fioresi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petkovic_J/0/1/0/all/0/1\">J. Petkovic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continuously Learning to Detect People on the Fly: A Bio-inspired Visual System for Drones. (arXiv:2202.08023v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.08023","description":"<p>This paper demonstrates for the first time that a biologically-plausible\nspiking neural network (SNN) equipped with Spike-Timing-Dependent Plasticity\n(STDP) can continuously learn to detect walking people on the fly using\nretina-inspired, event-based cameras. Our pipeline works as follows. First, a\nshort sequence of event data ($&lt;2$ minutes), capturing a walking human by a\nflying drone, is forwarded to a convolutional SNNSTDP system which also\nreceives teacher spiking signals from a readout (forming a semi-supervised\nsystem). Then, STDP adaptation is stopped and the learned system is assessed on\ntesting sequences. We conduct several experiments to study the effect of key\nparameters in our system and to compare it against conventionally-trained CNNs.\nWe show that our system reaches a higher peak $F_1$ score (+19%) compared to\nCNNs with event-based camera frames, while enabling on-line adaptation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Safa_A/0/1/0/all/0/1\">Ali Safa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ocket_I/0/1/0/all/0/1\">Ilja Ocket</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bourdoux_A/0/1/0/all/0/1\">Andr&#xe9; Bourdoux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahli_H/0/1/0/all/0/1\">Hichem Sahli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Catthoor_F/0/1/0/all/0/1\">Francky Catthoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gielen_G/0/1/0/all/0/1\">Georges Gielen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"When Did It Happen? Duration-informed Temporal Localization of Narrated Actions in Vlogs. (arXiv:2202.08138v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.08138","description":"<p>We consider the task of temporal human action localization in lifestyle\nvlogs. We introduce a novel dataset consisting of manual annotations of\ntemporal localization for 13,000 narrated actions in 1,200 video clips. We\npresent an extensive analysis of this data, which allows us to better\nunderstand how the language and visual modalities interact throughout the\nvideos. We propose a simple yet effective method to localize the narrated\nactions based on their expected duration. Through several experiments and\nanalyses, we show that our method brings complementary information with respect\nto previous methods, and leads to improvements over previous work for the task\nof temporal action localization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ignat_O/0/1/0/all/0/1\">Oana Ignat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castro_S/0/1/0/all/0/1\">Santiago Castro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuhang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Jiajun Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_D/0/1/0/all/0/1\">Dandan Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1\">Rada Mihalcea</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contextualize differential privacy in image database: a lightweight image differential privacy approach based on principle component analysis inverse. (arXiv:2202.08309v2 [cs.CR] UPDATED)","link":"http://arxiv.org/abs/2202.08309","description":"<p>Differential privacy (DP) has been the de-facto standard to preserve\nprivacy-sensitive information in database. Nevertheless, there lacks a clear\nand convincing contextualization of DP in image database, where individual\nimages' indistinguishable contribution to a certain analysis can be achieved\nand observed when DP is exerted. As a result, the privacy-accuracy trade-off\ndue to integrating DP is insufficiently demonstrated in the context of\ndifferentially-private image database. This work aims at contextualizing DP in\nimage database by an explicit and intuitive demonstration of integrating\nconceptional differential privacy with images. To this end, we design a\nlightweight approach dedicating to privatizing image database as a whole and\npreserving the statistical semantics of the image database to an adjustable\nlevel, while making individual images' contribution to such statistics\nindistinguishable. The designed approach leverages principle component analysis\n(PCA) to reduce the raw image with large amount of attributes to a lower\ndimensional space whereby DP is performed, so as to decrease the DP load of\ncalculating sensitivity attribute-by-attribute. The DP-exerted image data,\nwhich is not visible in its privatized format, is visualized through PCA\ninverse such that both a human and machine inspector can evaluate the\nprivatization and quantify the privacy-accuracy trade-off in an analysis on the\nprivatized image database. Using the devised approach, we demonstrate the\ncontextualization of DP in images by two use cases based on deep learning\nmodels, where we show the indistinguishability of individual images induced by\nDP and the privatized images' retention of statistical semantics in deep\nlearning tasks, which is elaborated by quantitative analyses on the\nprivacy-accuracy trade-off under different privatization settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shiliang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xuehui Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_H/0/1/0/all/0/1\">Hui Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tengyuan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yajie Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhuzhu Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Colonoscopy polyp detection with massive endoscopic images. (arXiv:2202.08730v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.08730","description":"<p>We improved an existing end-to-end polyp detection model with better average\nprecision validated by different data sets with trivial cost on detection\nspeed. Our previous work on detecting polyps within colonoscopy provided an\nefficient end-to-end solution to alleviate doctor's examination overhead.\nHowever, our later experiments found this framework is not as robust as before\nas the condition of polyp capturing varies. In this work, we conducted several\nstudies on data set, identifying main issues that causes low precision rate in\nthe task of polyp detection. We used an optimized anchor generation methods to\nget better anchor box shape and more boxes are used for detection as we believe\nthis is necessary for small object detection. A alternative backbone is used to\ncompensate the heavy time cost introduced by dense anchor box regression. With\nuse of the attention gate module, our model can achieve state-of-the-art polyp\ndetection performance while still maintain real-time detection speed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jialin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huogen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Ming Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VLP: A Survey on Vision-Language Pre-training. (arXiv:2202.09061v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.09061","description":"<p>In the past few years, the emergence of pre-training models has brought\nuni-modal fields such as computer vision (CV) and natural language processing\n(NLP) to a new era. Substantial works have shown they are beneficial for\ndownstream uni-modal tasks and avoid training a new model from scratch. So can\nsuch pre-trained models be applied to multi-modal tasks? Researchers have\nexplored this problem and made significant progress. This paper surveys recent\nadvances and new frontiers in vision-language pre-training (VLP), including\nimage-text and video-text pre-training. To give readers a better overall grasp\nof VLP, we first review its recent advances from five aspects: feature\nextraction, model architecture, pre-training objectives, pre-training datasets,\nand downstream tasks. Then, we summarize the specific VLP models in detail.\nFinally, we discuss the new frontiers in VLP. To the best of our knowledge,\nthis is the first survey on VLP. We hope that this survey can shed light on\nfuture research in the VLP field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Feilong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Duzhen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_M/0/1/0/all/0/1\">Minglun Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiuyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jing Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shuang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Bo Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Multiple-Object Tracking with a Dynamical Variational Autoencoder. (arXiv:2202.09315v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2202.09315","description":"<p>In this paper, we present an unsupervised probabilistic model and associated\nestimation algorithm for multi-object tracking (MOT) based on a dynamical\nvariational autoencoder (DVAE), called DVAE-UMOT. The DVAE is a latent-variable\ndeep generative model that can be seen as an extension of the variational\nautoencoder for the modeling of temporal sequences. It is included in DVAE-UMOT\nto model the objects' dynamics, after being pre-trained on an unlabeled\nsynthetic dataset of single-object trajectories. Then the distributions and\nparameters of DVAE-UMOT are estimated on each multi-object sequence to track\nusing the principles of variational inference: Definition of an approximate\nposterior distribution of the latent variables and maximization of the\ncorresponding evidence lower bound of the data likehood function. DVAE-UMOT is\nshown experimentally to compete well with and even surpass the performance of\ntwo state-of-the-art probabilistic MOT models. Code and data are publicly\navailable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xiaoyu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Girin_L/0/1/0/all/0/1\">Laurent Girin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alameda_Pineda_X/0/1/0/all/0/1\">Xavier Alameda-Pineda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-02-21T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}}]}]}