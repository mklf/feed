{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-07-15T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Modeling Long-term Dependencies and Short-term Correlations in Patient Journey Data with Temporal Attention Networks for Health Prediction. (arXiv:2207.06414v1 [cs.LG])","link":"http://arxiv.org/abs/2207.06414","description":"<p>Building models for health prediction based on Electronic Health Records\n(EHR) has become an active research area. EHR patient journey data consists of\npatient time-ordered clinical events/visits from patients. Most existing\nstudies focus on modeling long-term dependencies between visits, without\nexplicitly taking short-term correlations between consecutive visits into\naccount, where irregular time intervals, incorporated as auxiliary information,\nare fed into health prediction models to capture latent progressive patterns of\npatient journeys. We present a novel deep neural network with four modules to\ntake into account the contributions of various variables for health prediction:\ni) the Stacked Attention module strengthens the deep semantics in clinical\nevents within each patient journey and generates visit embeddings, ii) the\nShort-Term Temporal Attention module models short-term correlations between\nconsecutive visit embeddings while capturing the impact of time intervals\nwithin those visit embeddings, iii) the Long-Term Temporal Attention module\nmodels long-term dependencies between visit embeddings while capturing the\nimpact of time intervals within those visit embeddings, iv) and finally, the\nCoupled Attention module adaptively aggregates the outputs of Short-Term\nTemporal Attention and Long-Term Temporal Attention modules to make health\npredictions. Experimental results on MIMIC-III demonstrate superior predictive\naccuracy of our model compared to existing state-of-the-art methods, as well as\nthe interpretability and robustness of this approach. Furthermore, we found\nthat modeling short-term correlations contributes to local priors generation,\nleading to improved predictive modeling of patient journeys.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuxi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhenhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yepes_A/0/1/0/all/0/1\">Antonio Jimeno Yepes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salim_F/0/1/0/all/0/1\">Flora D. Salim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Robustly Optimized Long Text to Math Models for Numerical Reasoning On FinQA. (arXiv:2207.06490v1 [cs.CL])","link":"http://arxiv.org/abs/2207.06490","description":"<p>Numerical reasoning is required when solving most problems in our life, but\nit has been neglected in previous artificial intelligence researches. FinQA\nchallenge has been organized to strengthen the study on numerical reasoning\nwhere the participants are asked to predict the numerical reasoning program to\nsolve financial question. The result of FinQA will be evaluated by both\nexecution accuracy and program accuracy. In this paper, we present our approach\nto tackle the task objective by developing models with different specialized\ncapabilities and fusing their strength. Overall, our approach achieves the 1st\nplace in FinQA challenge, with 71.93% execution accuracy and 67.03% program\naccuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Renhui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Youwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yao Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A tool to overcome technical barriers for bias assessment in human language technologies. (arXiv:2207.06591v1 [cs.CL])","link":"http://arxiv.org/abs/2207.06591","description":"<p>Automatic processing of language is becoming pervasive in our lives, often\ntaking central roles in our decision making, like choosing the wording for our\nmessages and mails, translating our readings, or even having full conversations\nwith us. Word embeddings are a key component of modern natural language\nprocessing systems. They provide a representation of words that has boosted the\nperformance of many applications, working as a semblance of meaning. Word\nembeddings seem to capture a semblance of the meaning of words from raw text,\nbut, at the same time, they also distill stereotypes and societal biases which\nare subsequently relayed to the final applications. Such biases can be\ndiscriminatory. It is very important to detect and mitigate those biases, to\nprevent discriminatory behaviors of automated processes, which can be much more\nharmful than in the case of humans because their of their scale. There are\ncurrently many tools and techniques to detect and mitigate biases in word\nembeddings, but they present many barriers for the engagement of people without\ntechnical skills. As it happens, most of the experts in bias, either social\nscientists or people with deep knowledge of the context where bias is harmful,\ndo not have such skills, and they cannot engage in the processes of bias\ndetection because of the technical barriers. We have studied the barriers in\nexisting tools and have explored their possibilities and limitations with\ndifferent kinds of users. With this exploration, we propose to develop a tool\nthat is specially aimed to lower the technical barriers and provide the\nexploration power to address the requirements of experts, scientists and people\nin general who are willing to audit these technologies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alemany_L/0/1/0/all/0/1\">Laura Alonso Alemany</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benotti_L/0/1/0/all/0/1\">Luciana Benotti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_L/0/1/0/all/0/1\">Luc&#xed;a Gonz&#xe1;lez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_J/0/1/0/all/0/1\">Jorge S&#xe1;nchez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Busaniche_B/0/1/0/all/0/1\">Beatriz Busaniche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Halvorsen_A/0/1/0/all/0/1\">Alexia Halvorsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bordone_M/0/1/0/all/0/1\">Mat&#xed;as Bordone</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Two-Pass Low Latency End-to-End Spoken Language Understanding. (arXiv:2207.06670v1 [cs.CL])","link":"http://arxiv.org/abs/2207.06670","description":"<p>End-to-end (E2E) models are becoming increasingly popular for spoken language\nunderstanding (SLU) systems and are beginning to achieve competitive\nperformance to pipeline-based approaches. However, recent work has shown that\nthese models struggle to generalize to new phrasings for the same intent\nindicating that models cannot understand the semantic content of the given\nutterance. In this work, we incorporated language models pre-trained on\nunlabeled text data inside E2E-SLU frameworks to build strong semantic\nrepresentations. Incorporating both semantic and acoustic information can\nincrease the inference time, leading to high latency when deployed for\napplications like voice assistants. We developed a 2-pass SLU system that makes\nlow latency prediction using acoustic information from the few seconds of the\naudio in the first pass and makes higher quality prediction in the second pass\nby combining semantic and acoustic representations. We take inspiration from\nprior work on 2-pass end-to-end speech recognition systems that attends on both\naudio and first-pass hypothesis using a deliberation network. The proposed\n2-pass SLU system outperforms the acoustic-based SLU model on the Fluent Speech\nCommands Challenge Set and SLURP dataset and reduces latency, thus improving\nuser experience. Our code and models are publicly available as part of the\nESPnet-SLU toolkit.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arora_S/0/1/0/all/0/1\">Siddhant Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dalmia_S/0/1/0/all/0/1\">Siddharth Dalmia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xuankai Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1\">Brian Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Black_A/0/1/0/all/0/1\">Alan Black</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Overview of Abusive and Threatening Language Detection in Urdu at FIRE 2021. (arXiv:2207.06710v1 [cs.CL])","link":"http://arxiv.org/abs/2207.06710","description":"<p>With the growth of social media platform influence, the effect of their\nmisuse becomes more and more impactful. The importance of automatic detection\nof threatening and abusive language can not be overestimated. However, most of\nthe existing studies and state-of-the-art methods focus on English as the\ntarget language, with limited work on low- and medium-resource languages. In\nthis paper, we present two shared tasks of abusive and threatening language\ndetection for the Urdu language which has more than 170 million speakers\nworldwide. Both are posed as binary classification tasks where participating\nsystems are required to classify tweets in Urdu into two classes, namely: (i)\nAbusive and Non-Abusive for the first task, and (ii) Threatening and\nNon-Threatening for the second. We present two manually annotated datasets\ncontaining tweets labelled as (i) Abusive and Non-Abusive, and (ii) Threatening\nand Non-Threatening. The abusive dataset contains 2400 annotated tweets in the\ntrain part and 1100 annotated tweets in the test part. The threatening dataset\ncontains 6000 annotated tweets in the train part and 3950 annotated tweets in\nthe test part. We also provide logistic regression and BERT-based baseline\nclassifiers for both tasks. In this shared task, 21 teams from six countries\nregistered for participation (India, Pakistan, China, Malaysia, United Arab\nEmirates, and Taiwan), 10 teams submitted their runs for Subtask A, which is\nAbusive Language Detection and 9 teams submitted their runs for Subtask B,\nwhich is Threatening Language detection, and seven teams submitted their\ntechnical reports. The best performing system achieved an F1-score value of\n0.880 for Subtask A and 0.545 for Subtask B. For both subtasks, m-Bert based\ntransformer model showed the best performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Amjad_M/0/1/0/all/0/1\">Maaz Amjad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhila_A/0/1/0/all/0/1\">Alisa Zhila</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sidorov_G/0/1/0/all/0/1\">Grigori Sidorov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labunets_A/0/1/0/all/0/1\">Andrey Labunets</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Butta_S/0/1/0/all/0/1\">Sabur Butta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amjad_H/0/1/0/all/0/1\">Hamza Imam Amjad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vitman_O/0/1/0/all/0/1\">Oxana Vitman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gelbukh_A/0/1/0/all/0/1\">Alexander Gelbukh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Layout-Aware Information Extraction for Document-Grounded Dialogue: Dataset, Method and Demonstration. (arXiv:2207.06717v1 [cs.CL])","link":"http://arxiv.org/abs/2207.06717","description":"<p>Building document-grounded dialogue systems have received growing interest as\ndocuments convey a wealth of human knowledge and commonly exist in enterprises.\nWherein, how to comprehend and retrieve information from documents is a\nchallenging research problem. Previous work ignores the visual property of\ndocuments and treats them as plain text, resulting in incomplete modality. In\nthis paper, we propose a Layout-aware document-level Information Extraction\ndataset, LIE, to facilitate the study of extracting both structural and\nsemantic knowledge from visually rich documents (VRDs), so as to generate\naccurate responses in dialogue systems. LIE contains 62k annotations of three\nextraction tasks from 4,061 pages in product and official documents, becoming\nthe largest VRD-based information extraction dataset to the best of our\nknowledge. We also develop benchmark methods that extend the token-based\nlanguage model to consider layout features like humans. Empirical results show\nthat layout is critical for VRD-based extraction, and system demonstration also\nverifies that the extracted knowledge can help locate the answers that users\ncare about.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhenyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Bowen Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Haiyang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tingwen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1\">Cheng Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jingyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1\">Chengguang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yongbin Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Open Terminology Management and Sharing Toolkit for Federation of Terminology Databases. (arXiv:2207.06729v1 [cs.CL])","link":"http://arxiv.org/abs/2207.06729","description":"<p>Consolidated access to current and reliable terms from different subject\nfields and languages is necessary for content creators and translators.\nTerminology is also needed in AI applications such as machine translation,\nspeech recognition, information extraction, and other natural language\nprocessing tools. In this work, we facilitate standards-based sharing and\nmanagement of terminology resources by providing an open terminology management\nsolution - the EuroTermBank Toolkit. It allows organisations to manage and\nsearch their terms, create term collections, and share them within and outside\nthe organisation by participating in the network of federated databases. The\ndata curated in the federated databases are automatically shared with\nEuroTermBank, the largest multilingual terminology resource in Europe, allowing\ntranslators and language service providers as well as researchers and students\nto access terminology resources in their most current version.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lagzdins_A/0/1/0/all/0/1\">Andis Lagzdi&#x146;&#x161;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silins_U/0/1/0/all/0/1\">Uldis Sili&#x146;&#x161;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pinnis_M/0/1/0/all/0/1\">M&#x101;rcis Pinnis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bergmanis_T/0/1/0/all/0/1\">Toms Bergmanis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasilevskis_A/0/1/0/all/0/1\">Art&#x16b;rs Vasi&#x13c;evskis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasiljevs_A/0/1/0/all/0/1\">Andrejs Vasi&#x13c;jevs</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FFTc: An MLIR Dialect for Developing HPC Fast Fourier Transform Libraries. (arXiv:2207.06803v1 [cs.MS])","link":"http://arxiv.org/abs/2207.06803","description":"<p>Discrete Fourier Transform (DFT) libraries are one of the most critical\nsoftware components for scientific computing. Inspired by FFTW, a widely used\nlibrary for DFT HPC calculations, we apply compiler technologies for the\ndevelopment of HPC Fourier transform libraries. In this work, we introduce\nFFTc, a domain-specific language, based on Multi-Level Intermediate\nRepresentation (MLIR), for expressing Fourier Transform algorithms. We present\nthe initial design, implementation, and preliminary results of FFTc.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yifei He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Podobas_A/0/1/0/all/0/1\">Artur Podobas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andersson_M/0/1/0/all/0/1\">M&#xe5;ns I. Andersson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Markidis_S/0/1/0/all/0/1\">Stefano Markidis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BERTIN: Efficient Pre-Training of a Spanish Language Model using Perplexity Sampling. (arXiv:2207.06814v1 [cs.CL])","link":"http://arxiv.org/abs/2207.06814","description":"<p>The pre-training of large language models usually requires massive amounts of\nresources, both in terms of computation and data. Frequently used web sources\nsuch as Common Crawl might contain enough noise to make this pre-training\nsub-optimal. In this work, we experiment with different sampling methods from\nthe Spanish version of mC4, and present a novel data-centric technique which we\nname $\\textit{perplexity sampling}$ that enables the pre-training of language\nmodels in roughly half the amount of steps and using one fifth of the data. The\nresulting models are comparable to the current state-of-the-art, and even\nachieve better results for certain tasks. Our work is proof of the versatility\nof Transformers, and paves the way for small teams to train their models on a\nlimited budget. Our models are available at this\n$\\href{https://huggingface.co/bertin-project}{URL}$.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rosa_J/0/1/0/all/0/1\">Javier de la Rosa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ponferrada_E/0/1/0/all/0/1\">Eduardo G. Ponferrada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Villegas_P/0/1/0/all/0/1\">Paulo Villegas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salas_P/0/1/0/all/0/1\">Pablo Gonzalez de Prado Salas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romero_M/0/1/0/all/0/1\">Manu Romero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grandury_M/0/1/0/all/0/1\">Mar&#x131;a Grandury</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Data-to-Text Generation Based on Small Datasets: Comparing the Added Value of Two Semi-Supervised Learning Approaches on Top of a Large Language Model. (arXiv:2207.06839v1 [cs.CL])","link":"http://arxiv.org/abs/2207.06839","description":"<p>This study discusses the effect of semi-supervised learning in combination\nwith pretrained language models for data-to-text generation. It is not known\nwhether semi-supervised learning is still helpful when a large-scale language\nmodel is also supplemented. This study aims to answer this question by\ncomparing a data-to-text system only supplemented with a language model, to two\ndata-to-text systems that are additionally enriched by a data augmentation or a\npseudo-labeling semi-supervised learning approach.\n</p>\n<p>Results show that semi-supervised learning results in higher scores on\ndiversity metrics. In terms of output quality, extending the training set of a\ndata-to-text system with a language model using the pseudo-labeling approach\ndid increase text quality scores, but the data augmentation approach yielded\nsimilar scores to the system without training set extension. These results\nindicate that semi-supervised learning approaches can bolster output quality\nand diversity, even when a language model is also present.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chris van der Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferreira_T/0/1/0/all/0/1\">Thiago Castro Ferreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Emmery_C/0/1/0/all/0/1\">Chris Emmery</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiltshire_T/0/1/0/all/0/1\">Travis Wiltshire</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krahmer_E/0/1/0/all/0/1\">Emiel Krahmer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep versus Wide: An Analysis of Student Architectures for Task-Agnostic Knowledge Distillation of Self-Supervised Speech Models. (arXiv:2207.06867v1 [cs.CL])","link":"http://arxiv.org/abs/2207.06867","description":"<p>Self-supervised learning (SSL) is seen as a very promising approach with high\nperformance for several speech downstream tasks. Since the parameters of SSL\nmodels are generally so large that training and inference require a lot of\nmemory and computational cost, it is desirable to produce compact SSL models\nwithout a significant performance degradation by applying compression methods\nsuch as knowledge distillation (KD). Although the KD approach is able to shrink\nthe depth and/or width of SSL model structures, there has been little research\non how varying the depth and width impacts the internal representation of the\nsmall-footprint model. This paper provides an empirical study that addresses\nthe question. We investigate the performance on SUPERB while varying the\nstructure and KD methods so as to keep the number of parameters constant; this\nallows us to analyze the contribution of the representation introduced by\nvarying the model architecture. Experiments demonstrate that a certain depth is\nessential for solving content-oriented tasks (e.g. automatic speech\nrecognition) accurately, whereas a certain width is necessary for achieving\nhigh performance on several speaker-oriented tasks (e.g. speaker\nidentification). Based on these observations, we identify, for SUPERB, a more\ncompressed model with better performance than previous studies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ashihara_T/0/1/0/all/0/1\">Takanori Ashihara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moriya_T/0/1/0/all/0/1\">Takafumi Moriya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsuura_K/0/1/0/all/0/1\">Kohei Matsuura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanaka_T/0/1/0/all/0/1\">Tomohiro Tanaka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data Augmentation for Low-Resource Quechua ASR Improvement. (arXiv:2207.06872v1 [cs.SD])","link":"http://arxiv.org/abs/2207.06872","description":"<p>Automatic Speech Recognition (ASR) is a key element in new services that\nhelps users to interact with an automated system. Deep learning methods have\nmade it possible to deploy systems with word error rates below 5% for ASR of\nEnglish. However, the use of these methods is only available for languages with\nhundreds or thousands of hours of audio and their corresponding transcriptions.\nFor the so-called low-resource languages to speed up the availability of\nresources that can improve the performance of their ASR systems, methods of\ncreating new resources on the basis of existing ones are being investigated. In\nthis paper we describe our data augmentation approach to improve the results of\nASR models for low-resource and agglutinative languages. We carry out\nexperiments developing an ASR for Quechua using the wav2letter++ model. We\nreduced WER by 8.73% through our approach to the base model. The resulting ASR\nmodel obtained 22.75% WER and was trained with 99 hours of original resources\nand 99 hours of synthetic data obtained with a combination of text augmentation\nand synthetic speech generati\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zevallos_R/0/1/0/all/0/1\">Rodolfo Zevallos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bel_N/0/1/0/all/0/1\">Nuria Bel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cambara_G/0/1/0/all/0/1\">Guillermo C&#xe1;mbara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farrus_M/0/1/0/all/0/1\">Mireia Farr&#xfa;s</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luque_J/0/1/0/all/0/1\">Jordi Luque</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recurrent Memory Transformer. (arXiv:2207.06881v1 [cs.CL])","link":"http://arxiv.org/abs/2207.06881","description":"<p>Transformer-based models show their effectiveness across multiple domains and\ntasks. The self-attention allows to combine information from all sequence\nelements into context-aware representations. However, global and local\ninformation has to be stored mostly in the same element-wise representations.\nMoreover, the length of an input sequence is limited by quadratic computational\ncomplexity of self-attention.\n</p>\n<p>In this work, we propose and study a memory-augmented segment-level recurrent\nTransformer (Recurrent Memory Transformer). Memory allows to store and process\nlocal and global information as well as to pass information between segments of\nthe long sequence with the help of recurrence. We implement a memory mechanism\nwith no changes to Transformer model by adding special memory tokens to the\ninput or output sequence. Then Transformer is trained to control both memory\noperations and sequence representations processing.\n</p>\n<p>Results of experiments show that our model performs on par with the\nTransformer-XL on language modeling for smaller memory sizes and outperforms it\nfor tasks that require longer sequence processing. We show that adding memory\ntokens to Tr-XL is able to improve it performance. This makes Recurrent Memory\nTransformer a promising architecture for applications that require learning of\nlong-term dependencies and general purpose in memory processing, such as\nalgorithmic tasks and reasoning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bulatov_A/0/1/0/all/0/1\">Aydar Bulatov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuratov_Y/0/1/0/all/0/1\">Yuri Kuratov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burtsev_M/0/1/0/all/0/1\">Mikhail S. Burtsev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilinguals at SemEval-2022 Task 11: Complex NER in Semantically Ambiguous Settings for Low Resource Languages. (arXiv:2207.06882v1 [cs.CL])","link":"http://arxiv.org/abs/2207.06882","description":"<p>We leverage pre-trained language models to solve the task of complex NER for\ntwo low-resource languages: Chinese and Spanish. We use the technique of Whole\nWord Masking(WWM) to boost the performance of masked language modeling\nobjective on large and unsupervised corpora. We experiment with multiple neural\nnetwork architectures, incorporating CRF, BiLSTMs, and Linear Classifiers on\ntop of a fine-tuned BERT layer. All our models outperform the baseline by a\nsignificant margin and our best performing model obtains a competitive position\non the evaluation leaderboard for the blind test set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pandey_A/0/1/0/all/0/1\">Amit Pandey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daw_S/0/1/0/all/0/1\">Swayatta Daw</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Unnam_N/0/1/0/all/0/1\">Narendra Babu Unnam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pudi_V/0/1/0/all/0/1\">Vikram Pudi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beware the Rationalization Trap! When Language Model Explainability Diverges from our Mental Models of Language. (arXiv:2207.06897v1 [cs.CL])","link":"http://arxiv.org/abs/2207.06897","description":"<p>Language models learn and represent language differently than humans; they\nlearn the form and not the meaning. Thus, to assess the success of language\nmodel explainability, we need to consider the impact of its divergence from a\nuser's mental model of language. In this position paper, we argue that in order\nto avoid harmful rationalization and achieve truthful understanding of language\nmodels, explanation processes must satisfy three main conditions: (1)\nexplanations have to truthfully represent the model behavior, i.e., have a high\nfidelity; (2) explanations must be complete, as missing information distorts\nthe truth; and (3) explanations have to take the user's mental model into\naccount, progressively verifying a person's knowledge and adapting their\nunderstanding. We introduce a decision tree model to showcase potential reasons\nwhy current explanations fail to reach their objectives. We further emphasize\nthe need for human-centered design to explain the model from multiple\nperspectives, progressively adapting explanations to changing user\nexpectations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sevastjanova_R/0/1/0/all/0/1\">Rita Sevastjanova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Assady_M/0/1/0/all/0/1\">Mennatallah El-Assady</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Forming Trees with Treeformers. (arXiv:2207.06960v1 [cs.CL])","link":"http://arxiv.org/abs/2207.06960","description":"<p>Popular models such as Transformers and LSTMs use tokens as its unit of\ninformation. That is, each token is encoded into a vector representation, and\nthose vectors are used directly in a computation. However, humans frequently\nconsider spans of tokens (i.e., phrases) instead of their constituent tokens.\nIn this paper we introduce Treeformer, an architecture inspired by the CKY\nalgorithm and Transformer which learns a composition operator and pooling\nfunction in order to construct hierarchical encodings for phrases and\nsentences. Our extensive experiments demonstrate the benefits of incorporating\na hierarchical structure into the Transformer, and show significant\nimprovements compared to a baseline Transformer in machine translation,\nabstractive summarization, and various natural language understanding tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Patel_N/0/1/0/all/0/1\">Nilay Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flanigan_J/0/1/0/all/0/1\">Jeffrey Flanigan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scene Text Recognition with Permuted Autoregressive Sequence Models. (arXiv:2207.06966v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06966","description":"<p>Context-aware STR methods typically use internal autoregressive (AR) language\nmodels (LM). Inherent limitations of AR models motivated two-stage methods\nwhich employ an external LM. The conditional independence of the external LM on\nthe input image may cause it to erroneously rectify correct predictions,\nleading to significant inefficiencies. Our method, PARSeq, learns an ensemble\nof internal AR LMs with shared weights using Permutation Language Modeling. It\nunifies context-free non-AR and context-aware AR inference, and iterative\nrefinement using bidirectional context. Using synthetic training data, PARSeq\nachieves state-of-the-art (SOTA) results in STR benchmarks (91.9% accuracy) and\nmore challenging datasets. It establishes new SOTA results (96.0% accuracy)\nwhen trained on real data. PARSeq is optimal on accuracy vs parameter count,\nFLOPS, and latency because of its simple, unified structure and parallel token\nprocessing. Due to its extensive use of attention, it is robust on\narbitrarily-oriented text which is common in real-world images. Code,\npretrained weights, and data are available at: https://github.com/baudm/parseq.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bautista_D/0/1/0/all/0/1\">Darwin Bautista</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atienza_R/0/1/0/all/0/1\">Rowel Atienza</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Modelling with Pixels. (arXiv:2207.06991v1 [cs.CL])","link":"http://arxiv.org/abs/2207.06991","description":"<p>Language models are defined over a finite set of inputs, which creates a\nvocabulary bottleneck when we attempt to scale the number of supported\nlanguages. Tackling this bottleneck results in a trade-off between what can be\nrepresented in the embedding matrix and computational issues in the output\nlayer. This paper introduces PIXEL, the Pixel-based Encoder of Language, which\nsuffers from neither of these issues. PIXEL is a pretrained language model that\nrenders text as images, making it possible to transfer representations across\nlanguages based on orthographic similarity or the co-activation of pixels.\nPIXEL is trained to reconstruct the pixels of masked patches, instead of\npredicting a distribution over tokens. We pretrain the 86M parameter PIXEL\nmodel on the same English data as BERT and evaluate on syntactic and semantic\ntasks in typologically diverse languages, including various non-Latin scripts.\nWe find that PIXEL substantially outperforms BERT on syntactic and semantic\nprocessing tasks on scripts that are not found in the pretraining data, but\nPIXEL is slightly weaker than BERT when working with Latin scripts.\nFurthermore, we find that PIXEL is more robust to noisy text inputs than BERT,\nfurther confirming the benefits of modelling language with pixels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rust_P/0/1/0/all/0/1\">Phillip Rust</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lotz_J/0/1/0/all/0/1\">Jonas F. Lotz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bugliarello_E/0/1/0/all/0/1\">Emanuele Bugliarello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salesky_E/0/1/0/all/0/1\">Elizabeth Salesky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lhoneux_M/0/1/0/all/0/1\">Miryam de Lhoneux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elliott_D/0/1/0/all/0/1\">Desmond Elliott</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to translate by learning to communicate. (arXiv:2207.07025v1 [cs.CL])","link":"http://arxiv.org/abs/2207.07025","description":"<p>We formulate and test a technique to use Emergent Communication (EC) with a\npretrained multilingual model to improve on modern Unsupervised NMT systems,\nespecially for low-resource languages. It has been argued that the currently\ndominant paradigm in NLP of pretraining on text-only corpora will not yield\nrobust natural language understanding systems, and the need for grounded,\ngoal-oriented, and interactive language learning has been highlighted. In our\napproach, we embed a modern multilingual model (mBART, Liu et. al. 2020) into\nan EC image-reference game, in which the model is incentivized to use\nmultilingual generations to accomplish a vision-grounded task, with the\nhypothesis that this will align multiple languages to a shared task space. We\npresent two variants of EC Fine-Tuning (Steinert-Threlkeld et. al. 2022), one\nof which outperforms a backtranslation-based baseline in 6/8 translation\nsettings, and proves especially beneficial for the very low-resource languages\nof Nepali and Sinhala.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Downey_C/0/1/0/all/0/1\">C.M. Downey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Leo Z. Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xuhui Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steinert_Threlkeld_S/0/1/0/all/0/1\">Shane Steinert-Threlkeld</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Single Self-Supervised Model for Many Speech Modalities Enables Zero-Shot Modality Transfer. (arXiv:2207.07036v1 [cs.CL])","link":"http://arxiv.org/abs/2207.07036","description":"<p>While audio-visual speech models can yield superior performance and\nrobustness compared to audio-only models, their development and adoption are\nhindered by the lack of labeled and unlabeled audio-visual data and the cost to\ndeploy one model per modality. In this paper, we present u-HuBERT, a\nself-supervised pre-training framework that can leverage both multimodal and\nunimodal speech with a unified masked cluster prediction objective. By\nutilizing modality dropout during pre-training, we demonstrate that a single\nfine-tuned model can achieve performance on par or better than the\nstate-of-the-art modality-specific models. Moreover, our model fine-tuned only\non audio can perform well with audio-visual and visual speech input, achieving\nzero-shot modality generalization for speech recognition and speaker\nverification. In particular, our single model yields 1.2%/1.4%/27.2% speech\nrecognition word error rate on LRS3 with audio-visual/audio/visual input.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Wei-Ning Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1\">Bowen Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language models show human-like content effects on reasoning. (arXiv:2207.07051v1 [cs.CL])","link":"http://arxiv.org/abs/2207.07051","description":"<p>Abstract reasoning is a key ability for an intelligent system. Large language\nmodels achieve above-chance performance on abstract reasoning tasks, but\nexhibit many imperfections. However, human abstract reasoning is also\nimperfect, and depends on our knowledge and beliefs about the content of the\nreasoning problem. For example, humans reason much more reliably about logical\nrules that are grounded in everyday situations than arbitrary rules about\nabstract attributes. The training experiences of language models similarly\nendow them with prior expectations that reflect human knowledge and beliefs. We\ntherefore hypothesized that language models would show human-like content\neffects on abstract reasoning problems. We explored this hypothesis across\nthree logical reasoning tasks: natural language inference, judging the logical\nvalidity of syllogisms, and the Wason selection task (Wason, 1968). We find\nthat state of the art large language models (with 7 or 70 billion parameters;\nHoffman et al., 2022) reflect many of the same patterns observed in humans\nacross these tasks -- like humans, models reason more effectively about\nbelievable situations than unrealistic or abstract ones. Our findings have\nimplications for understanding both these cognitive effects, and the factors\nthat contribute to language model performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dasgupta_I/0/1/0/all/0/1\">Ishita Dasgupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lampinen_A/0/1/0/all/0/1\">Andrew K. Lampinen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_S/0/1/0/all/0/1\">Stephanie C. Y. Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Creswell_A/0/1/0/all/0/1\">Antonia Creswell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumaran_D/0/1/0/all/0/1\">Dharshan Kumaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McClelland_J/0/1/0/all/0/1\">James L. McClelland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hill_F/0/1/0/all/0/1\">Felix Hill</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Confident Adaptive Language Modeling. (arXiv:2207.07061v1 [cs.CL])","link":"http://arxiv.org/abs/2207.07061","description":"<p>Recent advances in Transformer-based large language models (LLMs) have led to\nsignificant performance improvements across many tasks. These gains come with a\ndrastic increase in the models' size, potentially leading to slow and costly\nuse at inference time. In practice, however, the series of generations made by\nLLMs is composed of varying levels of difficulty. While certain predictions\ntruly benefit from the models' full capacity, other continuations are more\ntrivial and can be solved with reduced compute. In this work, we introduce\nConfident Adaptive Language Modeling (CALM), a framework for dynamically\nallocating different amounts of compute per input and generation timestep.\nEarly exit decoding involves several challenges that we address here, such as:\n(1) what confidence measure to use; (2) connecting sequence-level constraints\nto local per-token exit decisions; and (3) attending back to missing hidden\nrepresentations due to early exits in previous tokens. Through theoretical\nanalysis and empirical experiments on three diverse text generation tasks, we\ndemonstrate the efficacy of our framework in reducing compute -- potential\nspeedup of up to $\\times 3$ -- while provably maintaining high performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schuster_T/0/1/0/all/0/1\">Tal Schuster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fisch_A/0/1/0/all/0/1\">Adam Fisch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_J/0/1/0/all/0/1\">Jai Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1\">Mostafa Dehghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahri_D/0/1/0/all/0/1\">Dara Bahri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_V/0/1/0/all/0/1\">Vinh Q. Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1\">Donald Metzler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Parameter-Efficient Prompt Tuning Makes Generalized and Calibrated Neural Text Retrievers. (arXiv:2207.07087v1 [cs.CL])","link":"http://arxiv.org/abs/2207.07087","description":"<p>Prompt tuning attempts to update few task-specific parameters in pre-trained\nmodels. It has achieved comparable performance to fine-tuning of the full\nparameter set on both language understanding and generation tasks. In this\nwork, we study the problem of prompt tuning for neural text retrievers. We\nintroduce parameter-efficient prompt tuning for text retrieval across\nin-domain, cross-domain, and cross-topic settings. Through an extensive\nanalysis, we show that the strategy can mitigate the two issues --\nparameter-inefficiency and weak generalizability -- faced by fine-tuning based\nretrieval methods. Notably, it can significantly improve the out-of-domain\nzero-shot generalization of the retrieval models. By updating only 0.1% of the\nmodel parameters, the prompt tuning strategy can help retrieval models achieve\nbetter generalization performance than traditional methods in which all\nparameters are updated. Finally, to facilitate research on retrievers'\ncross-topic generalizability, we curate and release an academic retrieval\ndataset with 18K query-results pairs in 87 topics, making it the largest\ntopic-specific one to date.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tam_W/0/1/0/all/0/1\">Weng Lam Tam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_K/0/1/0/all/0/1\">Kaixuan Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_L/0/1/0/all/0/1\">Lilong Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xingjian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yuxiao Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiahua Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_M/0/1/0/all/0/1\">Maodi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jie Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A comparison of latent semantic analysis and correspondence analysis of document-term matrices. (arXiv:2108.06197v3 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2108.06197","description":"<p>Latent semantic analysis (LSA) and correspondence analysis (CA) are two\ntechniques that use a singular value decomposition (SVD) for dimensionality\nreduction. LSA has been extensively used to obtain low-dimensional\nrepresentations that capture relationships among documents and terms. In this\narticle, we present a theoretical analysis and comparison of the two techniques\nin the context of document-term matrices. We show that CA has some attractive\nproperties as compared to LSA, for instance that effects of margins arising\nfrom differing document-lengths and term-frequencies are effectively\neliminated, so that the CA solution is optimally suited to focus on\nrelationships among documents and terms. A unifying framework is proposed that\nincludes both CA and LSA as special cases. We empirically compare CA to various\nLSA based methods on text categorization in English and authorship attribution\non historical Dutch texts, and find that CA performs significantly better. We\nalso apply CA to a long-standing question regarding the authorship of the Dutch\nnational anthem Wilhelmus and provide further support that it can be attributed\nto the author Datheen, amongst several contenders.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qi_Q/0/1/0/all/0/1\">Qianqian Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hessen_D/0/1/0/all/0/1\">David J. Hessen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deoskar_T/0/1/0/all/0/1\">Tejaswini Deoskar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heijden_P/0/1/0/all/0/1\">Peter G. M. van der Heijden</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"2020 U.S. presidential election in swing states: Gender differences in Twitter conversations. (arXiv:2108.09416v2 [cs.SI] UPDATED)","link":"http://arxiv.org/abs/2108.09416","description":"<p>Social media is commonly used by the public during election campaigns to\nexpress their opinions regarding different issues. Among various social media\nchannels, Twitter provides an efficient platform for researchers and\npoliticians to explore public opinion regarding a wide range of topics such as\nthe economy and foreign policy. Current literature mainly focuses on analyzing\nthe content of tweets without considering the gender of users. This research\ncollects and analyzes a large number of tweets and uses computational, human\ncoding, and statistical analyses to identify topics in more than 300,000 tweets\nposted during the 2020 U.S. presidential election and to compare female and\nmale users regarding the average weight of the discussed topics. Our findings\nare based upon a wide range of topics, such as tax, climate change, and the\nCOVID-19 pandemic. Out of the topics, there exists a significant difference\nbetween female and male users for more than 70% of topics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karami_A/0/1/0/all/0/1\">Amir Karami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_S/0/1/0/all/0/1\">Spring B. Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mackenzie_A/0/1/0/all/0/1\">Anderson Mackenzie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dorathea Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1\">Michael Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boyajieff_H/0/1/0/all/0/1\">Hannah R. Boyajieff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldschmidt_B/0/1/0/all/0/1\">Bailey Goldschmidt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Security Analysis Based on Random Geometry Theory for Satellite-Terrestrial-Vehicle Network. (arXiv:2112.14192v2 [cs.IT] UPDATED)","link":"http://arxiv.org/abs/2112.14192","description":"<p>Driven by B5G and 6G technologies, multi-network fusion is an indispensable\ntendency for future communications. In this paper, we focus on and analyze the\n\\emph{security performance} (SP) of the \\emph{satellite-terrestrial downlink\ntransmission} (STDT). Here, the STDT is composed of a satellite network and a\nvehicular network with a legitimate mobile receiver and an mobile eavesdropper\ndistributing. To theoretically analyze the SP of this system from the\nperspective of mobile terminals better, the random geometry theory is adopted,\nwhich assumes that both terrestrial vehicles are distributed stochastically in\none beam of the satellite. Furthermore, based on this theory, the closed-form\nanalytical expressions for two crucial and specific indicators in the STDT are\nderived, respectively, the secrecy outage probability and the ergodic secrecy\ncapacity. Additionally, several related variables restricting the SP of the\nSTDT are discussed, and specific schemes are presented to enhance the SP. Then,\nthe asymptotic property is investigated in the high signal-to-noise ratio\nscenario, and accurate and asymptotic closed-form expressions are given.\nFinally, simulation results show that, under the precondition of guaranteeing\nthe reliability of the STDT, the asymptotic solutions outperform the\ncorresponding accurate results significantly in the effectiveness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xudong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1\">Ye Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_R/0/1/0/all/0/1\">Rugui Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_N/0/1/0/all/0/1\">Nan Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_X/0/1/0/all/0/1\">Xiaoya Zuo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ToxiGen: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection. (arXiv:2203.09509v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.09509","description":"<p>Toxic language detection systems often falsely flag text that contains\nminority group mentions as toxic, as those groups are often the targets of\nonline hate. Such over-reliance on spurious correlations also causes systems to\nstruggle with detecting implicitly toxic language. To help mitigate these\nissues, we create ToxiGen, a new large-scale and machine-generated dataset of\n274k toxic and benign statements about 13 minority groups. We develop a\ndemonstration-based prompting framework and an adversarial\nclassifier-in-the-loop decoding method to generate subtly toxic and benign text\nwith a massive pretrained language model. Controlling machine generation in\nthis way allows ToxiGen to cover implicitly toxic text at a larger scale, and\nabout more demographic groups, than previous resources of human-written text.\nWe conduct a human evaluation on a challenging subset of ToxiGen and find that\nannotators struggle to distinguish machine-generated text from human-written\nlanguage. We also find that 94.5% of toxic examples are labeled as hate speech\nby human annotators. Using three publicly-available datasets, we show that\nfinetuning a toxicity classifier on our data improves its performance on\nhuman-written data substantially. We also demonstrate that ToxiGen can be used\nto fight machine-generated toxicity as finetuning improves the classifier\nsignificantly on our evaluation subset. Our code and data can be found at\nhttps://github.com/microsoft/ToxiGen.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hartvigsen_T/0/1/0/all/0/1\">Thomas Hartvigsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gabriel_S/0/1/0/all/0/1\">Saadia Gabriel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palangi_H/0/1/0/all/0/1\">Hamid Palangi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sap_M/0/1/0/all/0/1\">Maarten Sap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ray_D/0/1/0/all/0/1\">Dipankar Ray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamar_E/0/1/0/all/0/1\">Ece Kamar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Meta-learning for Low-resource Text Classification and Generation via Memory Imitation. (arXiv:2203.11670v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.11670","description":"<p>Building models of natural language processing (NLP) is challenging in\nlow-resource scenarios where only limited data are available.\nOptimization-based meta-learning algorithms achieve promising results in\nlow-resource scenarios by adapting a well-generalized model initialization to\nhandle new tasks. Nonetheless, these approaches suffer from the memorization\noverfitting issue, where the model tends to memorize the meta-training tasks\nwhile ignoring support sets when adapting to new tasks. To address this issue,\nwe propose a memory imitation meta-learning (MemIML) method that enhances the\nmodel's reliance on support sets for task adaptation. Specifically, we\nintroduce a task-specific memory module to store support set information and\nconstruct an imitation module to force query sets to imitate the behaviors of\nsome representative support-set samples stored in the memory. A theoretical\nanalysis is provided to prove the effectiveness of our method, and empirical\nresults also demonstrate that our method outperforms competitive baselines on\nboth text classification and generation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yingxiu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1\">Zhiliang Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1\">Huaxiu Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yinhe Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dongkyu Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yiping Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Nevin L. Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Speech-enhanced and Noise-aware Networks for Robust Speech Recognition. (arXiv:2203.13696v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2203.13696","description":"<p>Compensation for channel mismatch and noise interference is essential for\nrobust automatic speech recognition. Enhanced speech has been introduced into\nthe multi-condition training of acoustic models to improve their generalization\nability. In this paper, a noise-aware training framework based on two cascaded\nneural structures is proposed to jointly optimize speech enhancement and speech\nrecognition. The feature enhancement module is composed of a multi-task\nautoencoder, where noisy speech is decomposed into clean speech and noise. By\nconcatenating its enhanced, noise-aware, and noisy features for each frame, the\nacoustic-modeling module maps each feature-augmented frame into a triphone\nstate by optimizing the lattice-free maximum mutual information and cross\nentropy between the predicted and actual state sequences. On top of the\nfactorized time delay neural network (TDNN-F) and its convolutional variant\n(CNN-TDNNF), both with SpecAug, the two proposed systems achieve word error\nrate (WER) of 3.90% and 3.55%, respectively, on the Aurora-4 task. Compared\nwith the best existing systems that use bigram and trigram language models for\ndecoding, the proposed CNN-TDNNF-based system achieves a relative WER reduction\nof 15.20% and 33.53%, respectively. In addition, the proposed CNN-TDNNF-based\nsystem also outperforms the baseline CNN-TDNNF system on the AMI task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-Shin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Pin-Yuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yao-Fei Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsao_Y/0/1/0/all/0/1\">Yu Tsao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hsin-Min Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Parallelize in a Shared-Memory Environment with Transformers. (arXiv:2204.12835v4 [cs.DC] UPDATED)","link":"http://arxiv.org/abs/2204.12835","description":"<p>In past years, the world has switched to many-core and multi-core shared\nmemory architectures. As a result, there is a growing need to utilize these\narchitectures by introducing shared memory parallelization schemes to software\napplications. OpenMP is the most comprehensive API that implements such\nschemes, characterized by a readable interface. Nevertheless, introducing\nOpenMP into code is challenging due to pervasive pitfalls in management of\nparallel shared memory. To facilitate the performance of this task, many\nsource-to-source (S2S) compilers have been created over the years, tasked with\ninserting OpenMP directives into code automatically. In addition to having\nlimited robustness to their input format, these compilers still do not achieve\nsatisfactory coverage and precision in locating parallelizable code and\ngenerating appropriate directives. In this work, we propose leveraging recent\nadvances in ML techniques, specifically in natural language processing (NLP),\nto replace S2S compilers altogether. We create a database (corpus), Open-OMP,\nspecifically for this goal. Open-OMP contains over 28,000 code snippets, half\nof which contain OpenMP directives while the other half do not need\nparallelization at all with high probability. We use the corpus to train\nsystems to automatically classify code segments in need of parallelization, as\nwell as suggest individual OpenMP clauses. We train several transformer models,\nnamed PragFormer, for these tasks, and show that they outperform\nstatistically-trained baselines and automatic S2S parallelization compilers in\nboth classifying the overall need for an OpenMP directive and the introduction\nof private and reduction clauses.\n</p>\n<p>Our source code and database are available at:\nhttps://github.com/Scientific-Computing-Lab-NRCN/PragFormer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Harel_R/0/1/0/all/0/1\">Re&#x27;em Harel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pinter_Y/0/1/0/all/0/1\">Yuval Pinter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oren_G/0/1/0/all/0/1\">Gal Oren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Non-Parametric Domain Adaptation for End-to-End Speech Translation. (arXiv:2205.11211v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.11211","description":"<p>End-to-End Speech Translation (E2E-ST) has received increasing attention due\nto the potential of its less error propagation, lower latency, and fewer\nparameters. However, the effectiveness of neural-based approaches to this task\nis severely limited by the available training corpus, especially for domain\nadaptation where in-domain triplet training data is scarce or nonexistent. In\nthis paper, we propose a novel non-parametric method that leverages\ndomain-specific text translation corpus to achieve domain adaptation for the\nE2E-ST system. To this end, we first incorporate an additional encoder into the\npre-trained E2E-ST model to realize text translation modelling, and then unify\nthe decoder's output representation for text and speech translation tasks by\nreducing the correspondent representation mismatch in available triplet\ntraining data. During domain adaptation, a k-nearest-neighbor (kNN) classifier\nis introduced to produce the final translation distribution using the external\ndatastore built by the domain-specific text translation corpus, while the\nuniversal output representation is adopted to perform a similarity search.\nExperiments on the Europarl-ST benchmark demonstrate that when in-domain text\ntranslation data is involved only, our proposed approach significantly improves\nbaseline by 12.82 BLEU on average in all translation directions, even\noutperforming the strong in-domain fine-tuning method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yichao Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weizhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhirui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Boxing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Tong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jun Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1\">Enhong Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Medical Scientific Table-to-Text Generation with Human-in-the-Loop under the Data Sparsity Constraint. (arXiv:2205.12368v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.12368","description":"<p>Structured (tabular) data in the preclinical and clinical domains contains\nvaluable information about individuals and an efficient table-to-text\nsummarization system can drastically reduce manual efforts to condense this\ndata into reports. However, in practice, the problem is heavily impeded by the\ndata paucity, data sparsity and inability of the state-of-the-art natural\nlanguage generation models (including T5, PEGASUS and GPT-Neo) to produce\naccurate and reliable outputs. In this paper, we propose a novel table-to-text\napproach and tackle these problems with a novel two-step architecture which is\nenhanced by auto-correction, copy mechanism and synthetic data augmentation.\nThe study shows that the proposed approach selects salient biomedical entities\nand values from structured data with improved precision (up to 0.13 absolute\nincrease) of copying the tabular values to generate coherent and accurate text\nfor assay validation reports and toxicology reports. Moreover, we also\ndemonstrate a light-weight adaptation of the proposed system to new datasets by\nfine-tuning with as little as 40\\% training examples. The outputs of our model\nare validated by human experts in the Human-in-the-Loop scenario.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Heng-Yi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jingqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ive_J/0/1/0/all/0/1\">Julia Ive</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_V/0/1/0/all/0/1\">Vibhor Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bingyuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yike Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language with Vision: a Study on Grounded Word and Sentence Embeddings. (arXiv:2206.08823v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.08823","description":"<p>Language grounding to vision is an active field of research aiming to enrich\ntext-based representations of word meanings by leveraging perceptual knowledge\nfrom vision. Despite many attempts at language grounding, it is still unclear\nhow to effectively inject visual knowledge into the word embeddings of a\nlanguage in such a way that a proper balance of textual and visual knowledge is\nmaintained. Some common concerns are the following. Is visual grounding\nbeneficial for abstract words or is its contribution only limited to concrete\nwords? What is the optimal way of bridging the gap between text and vision? How\nmuch do we gain by visually grounding textual embeddings? The present study\naddresses these questions by proposing a simple yet very effective grounding\napproach for pre-trained word embeddings. Our model aligns textual embeddings\nwith vision while largely preserving the distributional statistics that\ncharacterize word use in text corpora. By applying a learned alignment, we are\nable to generate visually grounded embeddings for unseen words, including\nabstract words. A series of evaluations on word similarity benchmarks shows\nthat visual grounding is beneficial not only for concrete words, but also for\nabstract words. We also show that our method for visual grounding offers\nadvantages for contextualized embeddings, but only when these are trained on\ncorpora of relatively modest size. Code and grounded embeddings for English are\navailable at https://github.com/Hazel1994/Visually_Grounded_Word_Embeddings_2.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shahmohammadi_H/0/1/0/all/0/1\">Hassan Shahmohammadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heitmeier_M/0/1/0/all/0/1\">Maria Heitmeier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shafaei_Bajestan_E/0/1/0/all/0/1\">Elnaz Shafaei-Bajestan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lensch_H/0/1/0/all/0/1\">Hendrik P. A. Lensch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baayen_H/0/1/0/all/0/1\">Harald Baayen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Models (Mostly) Know What They Know. (arXiv:2207.05221v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2207.05221","description":"<p>We study whether language models can evaluate the validity of their own\nclaims and predict which questions they will be able to answer correctly. We\nfirst show that larger models are well-calibrated on diverse multiple choice\nand true/false questions when they are provided in the right format. Thus we\ncan approach self-evaluation on open-ended sampling tasks by asking models to\nfirst propose answers, and then to evaluate the probability \"P(True)\" that\ntheir answers are correct. We find encouraging performance, calibration, and\nscaling for P(True) on a diverse array of tasks. Performance at self-evaluation\nfurther improves when we allow models to consider many of their own samples\nbefore predicting the validity of one specific possibility. Next, we\ninvestigate whether models can be trained to predict \"P(IK)\", the probability\nthat \"I know\" the answer to a question, without reference to any particular\nproposed answer. Models perform well at predicting P(IK) and partially\ngeneralize across tasks, though they struggle with calibration of P(IK) on new\ntasks. The predicted P(IK) probabilities also increase appropriately in the\npresence of relevant source materials in the context, and in the presence of\nhints towards the solution of mathematical word problems. We hope these\nobservations lay the groundwork for training more honest models, and for\ninvestigating how honesty generalizes to cases where models are trained on\nobjectives other than the imitation of human writing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kadavath_S/0/1/0/all/0/1\">Saurav Kadavath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Conerly_T/0/1/0/all/0/1\">Tom Conerly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Askell_A/0/1/0/all/0/1\">Amanda Askell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henighan_T/0/1/0/all/0/1\">Tom Henighan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drain_D/0/1/0/all/0/1\">Dawn Drain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_E/0/1/0/all/0/1\">Ethan Perez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schiefer_N/0/1/0/all/0/1\">Nicholas Schiefer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dodds_Z/0/1/0/all/0/1\">Zac Hatfield Dodds</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DasSarma_N/0/1/0/all/0/1\">Nova DasSarma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_Johnson_E/0/1/0/all/0/1\">Eli Tran-Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnston_S/0/1/0/all/0/1\">Scott Johnston</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Showk_S/0/1/0/all/0/1\">Sheer El-Showk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jones_A/0/1/0/all/0/1\">Andy Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elhage_N/0/1/0/all/0/1\">Nelson Elhage</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hume_T/0/1/0/all/0/1\">Tristan Hume</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1\">Anna Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yuntao Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bowman_S/0/1/0/all/0/1\">Sam Bowman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fort_S/0/1/0/all/0/1\">Stanislav Fort</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganguli_D/0/1/0/all/0/1\">Deep Ganguli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hernandez_D/0/1/0/all/0/1\">Danny Hernandez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacobson_J/0/1/0/all/0/1\">Josh Jacobson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kernion_J/0/1/0/all/0/1\">Jackson Kernion</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kravec_S/0/1/0/all/0/1\">Shauna Kravec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lovitt_L/0/1/0/all/0/1\">Liane Lovitt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ndousse_K/0/1/0/all/0/1\">Kamal Ndousse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olsson_C/0/1/0/all/0/1\">Catherine Olsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ringer_S/0/1/0/all/0/1\">Sam Ringer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amodei_D/0/1/0/all/0/1\">Dario Amodei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_T/0/1/0/all/0/1\">Tom Brown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_J/0/1/0/all/0/1\">Jack Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joseph_N/0/1/0/all/0/1\">Nicholas Joseph</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mann_B/0/1/0/all/0/1\">Ben Mann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCandlish_S/0/1/0/all/0/1\">Sam McCandlish</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olah_C/0/1/0/all/0/1\">Chris Olah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaplan_J/0/1/0/all/0/1\">Jared Kaplan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-07-14T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Open High-Resolution Satellite Imagery: The WorldStrat Dataset -- With Application to Super-Resolution. (arXiv:2207.06418v1 [eess.IV])","link":"http://arxiv.org/abs/2207.06418","description":"<p>Analyzing the planet at scale with satellite imagery and machine learning is\na dream that has been constantly hindered by the cost of difficult-to-access\nhighly-representative high-resolution imagery. To remediate this, we introduce\nhere the WorldStrat dataset. The largest and most varied such publicly\navailable dataset, at Airbus SPOT 6/7 satellites' high resolution of up to 1.5\nm/pixel, empowered by European Space Agency's Phi-Lab as part of the ESA-funded\nQueryPlanet project, we curate nearly 10,000 sqkm of unique locations to ensure\nstratified representation of all types of land-use across the world: from\nagriculture to ice caps, from forests to multiple urbanization densities. We\nalso enrich those with locations typically under-represented in ML datasets:\nsites of humanitarian interest, illegal mining sites, and settlements of\npersons at risk. We temporally-match each high-resolution image with multiple\nlow-resolution images from the freely accessible lower-resolution Sentinel-2\nsatellites at 10 m/pixel. We accompany this dataset with an open-source Python\npackage to: rebuild or extend the WorldStrat dataset, train and infer baseline\nalgorithms, and learn with abundant tutorials, all compatible with the popular\nEO-learn toolbox. We hereby hope to foster broad-spectrum applications of ML to\nsatellite imagery, and possibly develop from free public low-resolution\nSentinel2 imagery the same power of analysis allowed by costly private\nhigh-resolution imagery. We illustrate this specific point by training and\nreleasing several highly compute-efficient baselines on the task of Multi-Frame\nSuper-Resolution. High-resolution Airbus imagery is CC BY-NC, while the labels\nand Sentinel2 imagery are CC BY, and the source code and pre-trained models\nunder BSD. The dataset is available at https://zenodo.org/record/6810792 and\nthe software package at https://github.com/worldstrat/worldstrat .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Cornebise_J/0/1/0/all/0/1\">Julien Cornebise</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Orsolic_I/0/1/0/all/0/1\">Ivan Or&#x161;oli&#x107;</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kalaitzis_F/0/1/0/all/0/1\">Freddie Kalaitzis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Graph CNN for Moving Object Detection in Complex Environments from Unseen Videos. (arXiv:2207.06440v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06440","description":"<p>Moving Object Detection (MOD) is a fundamental step for many computer vision\napplications. MOD becomes very challenging when a video sequence captured from\na static or moving camera suffers from the challenges: camouflage, shadow,\ndynamic backgrounds, and lighting variations, to name a few. Deep learning\nmethods have been successfully applied to address MOD with competitive\nperformance. However, in order to handle the overfitting problem, deep learning\nmethods require a large amount of labeled data which is a laborious task as\nexhaustive annotations are always not available. Moreover, some MOD deep\nlearning methods show performance degradation in the presence of unseen video\nsequences because the testing and training splits of the same sequences are\ninvolved during the network learning process. In this work, we pose the problem\nof MOD as a node classification problem using Graph Convolutional Neural\nNetworks (GCNNs). Our algorithm, dubbed as GraphMOD-Net, encompasses instance\nsegmentation, background initialization, feature extraction, and graph\nconstruction. GraphMOD-Net is tested on unseen videos and outperforms\nstate-of-the-art methods in unsupervised, semi-supervised, and supervised\nlearning in several challenges of the Change Detection 2014 (CDNet2014) and\nUCSD background subtraction datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Giraldo_J/0/1/0/all/0/1\">Jhony H. Giraldo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Javed_S/0/1/0/all/0/1\">Sajid Javed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Werghi_N/0/1/0/all/0/1\">Naoufel Werghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouwmans_T/0/1/0/all/0/1\">Thierry Bouwmans</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Imaging through the Atmosphere using Turbulence Mitigation Transformer. (arXiv:2207.06465v1 [eess.IV])","link":"http://arxiv.org/abs/2207.06465","description":"<p>Restoring images distorted by atmospheric turbulence is a long-standing\nproblem due to the spatially varying nature of the distortion, nonlinearity of\nthe image formation process, and scarcity of training and testing data.\nExisting methods often have strong statistical assumptions on the distortion\nmodel which in many cases will lead to a limited performance in real-world\nscenarios as they do not generalize. To overcome the challenge, this paper\npresents an end-to-end physics-driven approach that is efficient and can\ngeneralize to real-world turbulence. On the data synthesis front, we\nsignificantly increase the image resolution that can be handled by the SOTA\nturbulence simulator by approximating the random field via wide-sense\nstationarity. The new data synthesis process enables the generation of\nlarge-scale multi-level turbulence and ground truth pairs for training. On the\nnetwork design front, we propose the turbulence mitigation transformer (TMT), a\ntwo stage U-Net shaped multi-frame restoration network which has a noval\nefficient self-attention mechanism named temporal channel joint attention\n(TCJA). We also introduce a new training scheme that is enabled by the new\nsimulator, and we design new transformer units to reduce the memory\nconsumption. Experimental results on both static and dynamic scenes are\npromising, including various real turbulence scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1\">Xingguang Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mao_Z/0/1/0/all/0/1\">Zhiyuan Mao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chimitt_N/0/1/0/all/0/1\">Nicholas Chimitt</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chan_S/0/1/0/all/0/1\">Stanley H. Chan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analysis of face detection, face landmarking, and face recognition performance with masked face images. (arXiv:2207.06478v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06478","description":"<p>Face recognition has become an essential task in our lives. However, the\ncurrent COVID-19 pandemic has led to the widespread use of face masks. The\neffect of wearing face masks is currently an understudied issue. The aim of\nthis paper is to analyze face detection, face landmarking, and face recognition\nperformance with masked face images. HOG and CNN face detectors are used for\nface detection in combination with 5-point and 68-point face landmark\npredictors and VGG16 face recognition model is used for face recognition on\nmasked and unmasked images. We found that the performance of face detection,\nface landmarking, and face recognition is negatively impacted by face masks\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Golob_O/0/1/0/all/0/1\">O&#x17e;bej Golob</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Study on Image Filtering -- Techniques, Algorithm and Applications. (arXiv:2207.06481v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06481","description":"<p>Image processing is one of the most immerging and widely growing techniques\nmaking it a lively research field. Image processing is converting an image to a\ndigital format and then doing different operations on it, such as improving the\nimage or extracting various valuable data. Image filtering is one of the\nfascinating applications of image processing. Image filtering is a technique\nfor altering the size, shape, color, depth, smoothness, and other image\nproperties. It alters the pixels of the image to transform it into the desired\nform using different types of graphical editing methods through graphic design\nand editing software. This paper introduces various image filtering techniques\nand their wide applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Desai_B/0/1/0/all/0/1\">Bhishman Desai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paliwal_M/0/1/0/all/0/1\">Manish Paliwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagwanshi_K/0/1/0/all/0/1\">Kapil Kumar Nagwanshi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Data-Efficient Deep Learning Framework for Segmentation and Classification of Histopathology Images. (arXiv:2207.06489v1 [eess.IV])","link":"http://arxiv.org/abs/2207.06489","description":"<p>The current study of cell architecture of inflammation in histopathology\nimages commonly performed for diagnosis and research purposes excludes a lot of\ninformation available on the biopsy slide. In autoimmune diseases, major\noutstanding research questions remain regarding which cell types participate in\ninflammation at the tissue level,and how they interact with each other. While\nthese questions can be partially answered using traditional methods, artificial\nintelligence approaches for segmentation and classification provide a much more\nefficient method to understand the architecture of inflammation in autoimmune\ndisease, holding a great promise for novel insights. In this paper, we\nempirically develop deep learning approaches that uses dermatomyositis biopsies\nof human tissue to detect and identify inflammatory cells. Our approach\nimproves classification performance by 26% and segmentation performance by 5%.\nWe also propose a novel post-processing autoencoder architecture that improves\nsegmentation performance by an additional 3%. We have open-sourced our approach\nand architecture at https://github.com/pranavsinghps1/DEDL\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Singh_P/0/1/0/all/0/1\">Pranav Singh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cirrone_J/0/1/0/all/0/1\">Jacopo Cirrone</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"One Model to Unite Them All: Personalized Federated Learning of Multi-Contrast MRI Synthesis. (arXiv:2207.06509v1 [eess.IV])","link":"http://arxiv.org/abs/2207.06509","description":"<p>Learning-based MRI translation involves a synthesis model that maps a\nsource-contrast onto a target-contrast image. Multi-institutional\ncollaborations are key to training synthesis models across broad datasets, yet\ncentralized training involves privacy risks. Federated learning (FL) is a\ncollaboration framework that instead adopts decentralized training to avoid\nsharing imaging data and mitigate privacy concerns. However, FL-trained models\ncan be impaired by the inherent heterogeneity in the distribution of imaging\ndata. On the one hand, implicit shifts in image distribution are evident across\nsites, even for a common translation task with fixed source-target\nconfiguration. Conversely, explicit shifts arise within and across sites when\ndiverse translation tasks with varying source-target configurations are\nprescribed. To improve reliability against domain shifts, here we introduce the\nfirst personalized FL method for MRI Synthesis (pFLSynth). pFLSynth is based on\nan adversarial model equipped with a mapper that produces latents specific to\nindividual sites and source-target contrasts. It leverages novel\npersonalization blocks that adaptively tune the statistics and weighting of\nfeature maps across the generator based on these latents. To further promote\nsite-specificity, partial model aggregation is employed over downstream layers\nof the generator while upstream layers are retained locally. As such, pFLSynth\nenables training of a unified synthesis model that can reliably generalize\nacross multiple sites and translation tasks. Comprehensive experiments on\nmulti-site datasets clearly demonstrate the enhanced performance of pFLSynth\nagainst prior federated methods in multi-contrast MRI synthesis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Dalmaz_O/0/1/0/all/0/1\">Onat Dalmaz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mirza_U/0/1/0/all/0/1\">Usama Mirza</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Elmas_G/0/1/0/all/0/1\">G&#xf6;kberk Elmas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ozbey_M/0/1/0/all/0/1\">Muzaffer &#xd6;zbey</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dar_S/0/1/0/all/0/1\">Salman UH Dar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ceyani_E/0/1/0/all/0/1\">Emir Ceyani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Avestimehr_S/0/1/0/all/0/1\">Salman Avestimehr</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cukur_T/0/1/0/all/0/1\">Tolga &#xc7;ukur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lipschitz Continuity Retained Binary Neural Network. (arXiv:2207.06540v1 [cs.LG])","link":"http://arxiv.org/abs/2207.06540","description":"<p>Relying on the premise that the performance of a binary neural network can be\nlargely restored with eliminated quantization error between full-precision\nweight vectors and their corresponding binary vectors, existing works of\nnetwork binarization frequently adopt the idea of model robustness to reach the\naforementioned objective. However, robustness remains to be an ill-defined\nconcept without solid theoretical support. In this work, we introduce the\nLipschitz continuity, a well-defined functional property, as the rigorous\ncriteria to define the model robustness for BNN. We then propose to retain the\nLipschitz continuity as a regularization term to improve the model robustness.\nParticularly, while the popular Lipschitz-involved regularization methods often\ncollapse in BNN due to its extreme sparsity, we design the Retention Matrices\nto approximate spectral norms of the targeted weight matrices, which can be\ndeployed as the approximation for the Lipschitz constant of BNNs without the\nexact Lipschitz constant computation (NP-hard). Our experiments prove that our\nBNN-specific regularization method can effectively strengthen the robustness of\nBNN (testified on ImageNet-C), achieving state-of-the-art performance on CIFAR\nand ImageNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shang_Y/0/1/0/all/0/1\">Yuzhang Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Dan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_B/0/1/0/all/0/1\">Bin Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zong_Z/0/1/0/all/0/1\">Ziliang Zong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_L/0/1/0/all/0/1\">Liqiang Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yan Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Body Composition Assessment with Limited Field-of-view Computed Tomography: A Semantic Image Extension Perspective. (arXiv:2207.06551v1 [eess.IV])","link":"http://arxiv.org/abs/2207.06551","description":"<p>Field-of-view (FOV) tissue truncation beyond the lungs is common in routine\nlung screening computed tomography (CT). This poses limitations for\nopportunistic CT- based body composition (BC) assessment as key anatomical\nstructures are missing. Traditionally, extending the FOV of CT is considered as\na CT reconstruction problem using limited data. However, this approach relies\non the projection domain data which might not be available in application. In\nthis work, we formulate the problem from the semantic image extension\nperspective which only requires image data as inputs. The proposed two-stage\nmethod identifies a new FOV border based on the estimated extent of the\ncomplete body and imputes missing tissues in the truncated region. The training\nsamples are simulated using CT slices with complete body in FOV, making the\nmodel development self-supervised. We evaluate the validity of the proposed\nmethod in automatic BC assessment using lung screening CT with limited FOV. The\nproposed method effectively restores the missing tissues and reduces BC\nassessment error introduced by FOV tissue truncation. In the BC assessment for\na large-scale lung screening CT dataset, this correction improves both the\nintra-subject consistency and the correlation with anthropometric\napproximations. The developed method is available at\nhttps://github.com/MASILab/S-EFOV.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Xu_K/0/1/0/all/0/1\">Kaiwen Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_T/0/1/0/all/0/1\">Thomas Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khan_M/0/1/0/all/0/1\">Mirza S. Khan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gao_R/0/1/0/all/0/1\">Riqiang Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Antic_S/0/1/0/all/0/1\">Sanja L. Antic</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huo_Y/0/1/0/all/0/1\">Yuankai Huo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sandler_K/0/1/0/all/0/1\">Kim L. Sandler</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maldonado_F/0/1/0/all/0/1\">Fabien Maldonado</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Landman_B/0/1/0/all/0/1\">Bennett A. Landman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"QML for Argoverse 2 Motion Forecasting Challenge. (arXiv:2207.06553v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06553","description":"<p>To safely navigate in various complex traffic scenarios, autonomous driving\nsystems are generally equipped with a motion forecasting module to provide\nvital information for the downstream planning module. For the real-world\nonboard applications, both accuracy and latency of a motion forecasting model\nare essential. In this report, we present an effective and efficient solution,\nwhich ranks the 3rd place in the Argoverse 2 Motion Forecasting Challenge 2022.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_T/0/1/0/all/0/1\">Tong Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xishun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaodong Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Supervised Attribute Information Removal and Reconstruction for Image Manipulation. (arXiv:2207.06555v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06555","description":"<p>The goal of attribute manipulation is to control specified attribute(s) in\ngiven images. Prior work approaches this problem by learning disentangled\nrepresentations for each attribute that enables it to manipulate the encoded\nsource attributes to the target attributes. However, encoded attributes are\noften correlated with relevant image content. Thus, the source attribute\ninformation can often be hidden in the disentangled features, leading to\nunwanted image editing effects. In this paper, we propose an Attribute\nInformation Removal and Reconstruction (AIRR) network that prevents such\ninformation hiding by learning how to remove the attribute information\nentirely, creating attribute excluded features, and then learns to directly\ninject the desired attributes in a reconstructed image. We evaluate our\napproach on four diverse datasets with a variety of attributes including\nDeepFashion Synthesis, DeepFashion Fine-grained Attribute, CelebA and\nCelebA-HQ, where our model improves attribute manipulation accuracy and top-k\nretrieval rate by 10% on average over prior work. A user study also reports\nthat AIRR manipulated images are preferred over prior work in up to 76% of\ncases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_N/0/1/0/all/0/1\">Nannan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plummer_B/0/1/0/all/0/1\">Bryan A. Plummer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving the diagnosis of breast cancer based on biophysical ultrasound features utilizing machine learning. (arXiv:2207.06560v1 [eess.IV])","link":"http://arxiv.org/abs/2207.06560","description":"<p>The improved diagnostic accuracy of ultrasound breast examinations remains an\nimportant goal. In this study, we propose a biophysical feature based machine\nlearning method for breast cancer detection to improve the performance beyond a\nbenchmark deep learning algorithm and to furthermore provide a color overlay\nvisual map of the probability of malignancy within a lesion. This overall\nframework is termed disease specific imaging. Previously, 150 breast lesions\nwere segmented and classified utilizing a modified fully convolutional network\nand a modified GoogLeNet, respectively. In this study multiparametric analysis\nwas performed within the contoured lesions. Features were extracted from\nultrasound radiofrequency, envelope, and log compressed data based on\nbiophysical and morphological models. The support vector machine with a\nGaussian kernel constructed a nonlinear hyperplane, and we calculated the\ndistance between the hyperplane and data point of each feature in\nmultiparametric space. The distance can quantitatively assess a lesion, and\nsuggest the probability of malignancy that is color coded and overlaid onto B\nmode images. Training and evaluation were performed on in vivo patient data.\nThe overall accuracy for the most common types and sizes of breast lesions in\nour study exceeded 98.0% for classification and 0.98 for an area under the\nreceiver operating characteristic curve, which is more precise than the\nperformance of radiologists and a deep learning system. Further, the\ncorrelation between the probability and BI RADS enables a quantitative\nguideline to predict breast cancer. Therefore, we anticipate that the proposed\nframework can help radiologists achieve more accurate and convenient breast\ncancer classification and detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Baek_J/0/1/0/all/0/1\">Jihye Baek</a>, <a href=\"http://arxiv.org/find/eess/1/au:+OConnell_A/0/1/0/all/0/1\">Avice M. O&#x27;Connell</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Parker_K/0/1/0/all/0/1\">Kevin J. Parker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Benign, Tempered, or Catastrophic: A Taxonomy of Overfitting. (arXiv:2207.06569v1 [cs.LG])","link":"http://arxiv.org/abs/2207.06569","description":"<p>The practical success of overparameterized neural networks has motivated the\nrecent scientific study of interpolating methods, which perfectly fit their\ntraining data. Certain interpolating methods, including neural networks, can\nfit noisy training data without catastrophically bad test performance, in\ndefiance of standard intuitions from statistical learning theory. Aiming to\nexplain this, a body of recent work has studied $\\textit{benign overfitting}$,\na phenomenon where some interpolating methods approach Bayes optimality, even\nin the presence of noise. In this work we argue that while benign overfitting\nhas been instructive and fruitful to study, many real interpolating methods\nlike neural networks $\\textit{do not fit benignly}$: modest noise in the\ntraining set causes nonzero (but non-infinite) excess risk at test time,\nimplying these models are neither benign nor catastrophic but rather fall in an\nintermediate regime. We call this intermediate regime $\\textit{tempered\noverfitting}$, and we initiate its systematic study. We first explore this\nphenomenon in the context of kernel (ridge) regression (KR) by obtaining\nconditions on the ridge parameter and kernel eigenspectrum under which KR\nexhibits each of the three behaviors. We find that kernels with powerlaw\nspectra, including Laplace kernels and ReLU neural tangent kernels, exhibit\ntempered overfitting. We then empirically study deep neural networks through\nthe lens of our taxonomy, and find that those trained to interpolation are\ntempered, while those stopped early are benign. We hope our work leads to a\nmore refined understanding of overfitting in modern learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mallinar_N/0/1/0/all/0/1\">Neil Mallinar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simon_J/0/1/0/all/0/1\">James B. Simon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abedsoltan_A/0/1/0/all/0/1\">Amirhesam Abedsoltan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandit_P/0/1/0/all/0/1\">Parthe Pandit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belkin_M/0/1/0/all/0/1\">Mikhail Belkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakkiran_P/0/1/0/all/0/1\">Preetum Nakkiran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Virtual stain transfer in histology via cascaded deep neural networks. (arXiv:2207.06578v1 [physics.med-ph])","link":"http://arxiv.org/abs/2207.06578","description":"<p>Pathological diagnosis relies on the visual inspection of histologically\nstained thin tissue specimens, where different types of stains are applied to\nbring contrast to and highlight various desired histological features. However,\nthe destructive histochemical staining procedures are usually irreversible,\nmaking it very difficult to obtain multiple stains on the same tissue section.\nHere, we demonstrate a virtual stain transfer framework via a cascaded deep\nneural network (C-DNN) to digitally transform hematoxylin and eosin (H&amp;E)\nstained tissue images into other types of histological stains. Unlike a single\nneural network structure which only takes one stain type as input to digitally\noutput images of another stain type, C-DNN first uses virtual staining to\ntransform autofluorescence microscopy images into H&amp;E and then performs stain\ntransfer from H&amp;E to the domain of the other stain in a cascaded manner. This\ncascaded structure in the training phase allows the model to directly exploit\nhistochemically stained image data on both H&amp;E and the target special stain of\ninterest. This advantage alleviates the challenge of paired data acquisition\nand improves the image quality and color accuracy of the virtual stain transfer\nfrom H&amp;E to another stain. We validated the superior performance of this C-DNN\napproach using kidney needle core biopsy tissue sections and successfully\ntransferred the H&amp;E-stained tissue images into virtual PAS (periodic\nacid-Schiff) stain. This method provides high-quality virtual images of special\nstains using existing, histochemically stained slides and creates new\nopportunities in digital pathology by performing highly accurate stain-to-stain\ntransformations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Yang_X/0/1/0/all/0/1\">Xilin Yang</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Bai_B/0/1/0/all/0/1\">Bijie Bai</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Zhang_Y/0/1/0/all/0/1\">Yijie Zhang</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Li_Y/0/1/0/all/0/1\">Yuzhu Li</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Haan_K/0/1/0/all/0/1\">Kevin de Haan</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Liu_T/0/1/0/all/0/1\">Tairan Liu</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Ozcan_A/0/1/0/all/0/1\">Aydogan Ozcan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Temporal Action Detection with Global Segmentation Mask Learning. (arXiv:2207.06580v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06580","description":"<p>Existing temporal action detection (TAD) methods rely on generating an\noverwhelmingly large number of proposals per video. This leads to complex model\ndesigns due to proposal generation and/or per-proposal action instance\nevaluation and the resultant high computational cost. In this work, for the\nfirst time, we propose a proposal-free Temporal Action detection model with\nGlobal Segmentation mask (TAGS). Our core idea is to learn a global\nsegmentation mask of each action instance jointly at the full video length. The\nTAGS model differs significantly from the conventional proposal-based methods\nby focusing on global temporal representation learning to directly detect local\nstart and end points of action instances without proposals. Further, by\nmodeling TAD holistically rather than locally at the individual proposal level,\nTAGS needs a much simpler model architecture with lower computational cost.\nExtensive experiments show that despite its simpler design, TAGS outperforms\nexisting TAD methods, achieving new state-of-the-art performance on two\nbenchmarks. Importantly, it is ~ 20x faster to train and ~1.6x more efficient\nfor inference. Our PyTorch implementation of TAGS is available at\nhttps://github.com/sauradip/TAGS .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nag_S/0/1/0/all/0/1\">Sauradip Nag</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiatian Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yi-Zhe Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1\">Tao Xiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformer-based Context Condensation for Boosting Feature Pyramids in Object Detection. (arXiv:2207.06603v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06603","description":"<p>Current object detectors typically have a feature pyramid (FP) module for\nmulti-level feature fusion (MFF) which aims to mitigate the gap between\nfeatures from different levels and form a comprehensive object representation\nto achieve better detection performance. However, they usually require heavy\ncross-level connections or iterative refinement to obtain better MFF result,\nmaking them complicated in structure and inefficient in computation. To address\nthese issues, we propose a novel and efficient context modeling mechanism that\ncan help existing FPs deliver better MFF results while reducing the\ncomputational costs effectively. In particular, we introduce a novel insight\nthat comprehensive contexts can be decomposed and condensed into two types of\nrepresentations for higher efficiency. The two representations include a\nlocally concentrated representation and a globally summarized representation,\nwhere the former focuses on extracting context cues from nearby areas while the\nlatter extracts key representations of the whole image scene as global context\ncues. By collecting the condensed contexts, we employ a Transformer decoder to\ninvestigate the relations between them and each local feature from the FP and\nthen refine the MFF results accordingly. As a result, we obtain a simple and\nlight-weight Transformer-based Context Condensation (TCC) module, which can\nboost various FPs and lower their computational costs simultaneously. Extensive\nexperimental results on the challenging MS COCO dataset show that TCC is\ncompatible to four representative FPs and consistently improves their detection\naccuracy by up to 7.8 % in terms of average precision and reduce their\ncomplexities by up to around 20% in terms of GFLOPs, helping them achieve\nstate-of-the-art performance more efficiently. Code will be released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhe Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yufei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Super-Resolution as Text-Guided Details Generation. (arXiv:2207.06604v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06604","description":"<p>Deep neural networks have greatly promoted the performance of single image\nsuper-resolution (SISR). Conventional methods still resort to restoring the\nsingle high-resolution (HR) solution only based on the input of image modality.\nHowever, the image-level information is insufficient to predict adequate\ndetails and photo-realistic visual quality facing large upscaling factors (x8,\nx16). In this paper, we propose a new perspective that regards the SISR as a\nsemantic image detail enhancement problem to generate semantically reasonable\nHR image that are faithful to the ground truth. To enhance the semantic\naccuracy and the visual quality of the reconstructed image, we explore the\nmulti-modal fusion learning in SISR by proposing a Text-Guided Super-Resolution\n(TGSR) framework, which can effectively utilize the information from the text\nand image modalities. Different from existing methods, the proposed TGSR could\ngenerate HR image details that match the text descriptions through a\ncoarse-to-fine process. Extensive experiments and ablation studies demonstrate\nthe effect of the TGSR, which exploits the text reference to recover realistic\nimages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Chenxi Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1\">Bo Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Q/0/1/0/all/0/1\">Qing Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_W/0/1/0/all/0/1\">Weimin Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Siming Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deepfake Video Detection with Spatiotemporal Dropout Transformer. (arXiv:2207.06612v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06612","description":"<p>While the abuse of deepfake technology has caused serious concerns recently,\nhow to detect deepfake videos is still a challenge due to the high\nphoto-realistic synthesis of each frame. Existing image-level approaches often\nfocus on single frame and ignore the spatiotemporal cues hidden in deepfake\nvideos, resulting in poor generalization and robustness. The key of a\nvideo-level detector is to fully exploit the spatiotemporal inconsistency\ndistributed in local facial regions across different frames in deepfake videos.\nInspired by that, this paper proposes a simple yet effective patch-level\napproach to facilitate deepfake video detection via spatiotemporal dropout\ntransformer. The approach reorganizes each input video into bag of patches that\nis then fed into a vision transformer to achieve robust representation.\nSpecifically, a spatiotemporal dropout operation is proposed to fully explore\npatch-level spatiotemporal cues and serve as effective data augmentation to\nfurther enhance model's robustness and generalization ability. The operation is\nflexible and can be easily plugged into existing vision transformers. Extensive\nexperiments demonstrate the effectiveness of our approach against 25\nstate-of-the-arts with impressive robustness, generalizability, and\nrepresentation ability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Daichi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_F/0/1/0/all/0/1\">Fanzhao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_Y/0/1/0/all/0/1\">Yingying Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pengju Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_D/0/1/0/all/0/1\">Dan Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1\">Shiming Ge</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"T-RECX: Tiny-Resource Efficient Convolutional Neural Networks with Early-Exit. (arXiv:2207.06613v1 [cs.LG])","link":"http://arxiv.org/abs/2207.06613","description":"<p>Deploying Machine learning (ML) on the milliwatt-scale edge devices (tinyML)\nis gaining popularity due to recent breakthroughs in ML and IoT. However, the\ncapabilities of tinyML are restricted by strict power and compute constraints.\nThe majority of the contemporary research in tinyML focuses on model\ncompression techniques such as model pruning and quantization to fit ML models\non low-end devices. Nevertheless, the improvements in energy consumption and\ninference time obtained by existing techniques are limited because aggressive\ncompression quickly shrinks model capacity and accuracy. Another approach to\nimprove inference time and/or reduce power while preserving its model capacity\nis through early-exit networks. These networks place intermediate classifiers\nalong a baseline neural network that facilitate early exit from neural network\ncomputation if an intermediate classifier exhibits sufficient confidence in its\nprediction. Previous work on early-exit networks have focused on large\nnetworks, beyond what would typically be used for tinyML applications. In this\npaper, we discuss the challenges of adding early-exits to state-of-the-art\ntiny-CNNs and devise an early-exit architecture, T-RECX, that addresses these\nchallenges. In addition, we develop a method to alleviate the effect of network\noverthinking at the final exit by leveraging the high-level representations\nlearned by the early-exit. We evaluate T-RECX on three CNNs from the MLPerf\ntiny benchmark suite for image classification, keyword spotting and visual wake\nword detection tasks. Our results demonstrate that T-RECX improves the accuracy\nof baseline network and significantly reduces the average inference time of\ntiny-CNNs. T-RECX achieves 32.58% average reduction in FLOPS in exchange for 1%\naccuracy across all evaluated models. Also, our techniques increase the\naccuracy of baseline network in two out of three models we evaluate\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghanathe_N/0/1/0/all/0/1\">Nikhil P Ghanathe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilton_S/0/1/0/all/0/1\">Steve Wilton</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Perception-Oriented Stereo Image Super-Resolution. (arXiv:2207.06617v1 [eess.IV])","link":"http://arxiv.org/abs/2207.06617","description":"<p>Recent studies of deep learning based stereo image super-resolution\n(StereoSR) have promoted the development of StereoSR. However, existing\nStereoSR models mainly concentrate on improving quantitative evaluation metrics\nand neglect the visual quality of super-resolved stereo images. To improve the\nperceptual performance, this paper proposes the first perception-oriented\nstereo image super-resolution approach by exploiting the feedback, provided by\nthe evaluation on the perceptual quality of StereoSR results. To provide\naccurate guidance for the StereoSR model, we develop the first special stereo\nimage super-resolution quality assessment (StereoSRQA) model, and further\nconstruct a StereoSRQA database. Extensive experiments demonstrate that our\nStereoSR approach significantly improves the perceptual quality and enhances\nthe reliability of stereo images for disparity estimation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ma_C/0/1/0/all/0/1\">Chenxi Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yan_B/0/1/0/all/0/1\">Bo Yan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tan_W/0/1/0/all/0/1\">Weimin Tan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jiang_X/0/1/0/all/0/1\">Xuhao Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continuous Facial Motion Deblurring. (arXiv:2207.06626v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06626","description":"<p>We introduce a novel framework for continuous facial motion deblurring that\nrestores the continuous sharp moment latent in a single motion-blurred face\nimage via a moment control factor. Although a motion-blurred image is the\naccumulated signal of continuous sharp moments during the exposure time, most\nexisting single image deblurring approaches aim to restore a fixed number of\nframes using multiple networks and training stages. To address this problem, we\npropose a continuous facial motion deblurring network based on GAN (CFMD-GAN),\nwhich is a novel framework for restoring the continuous moment latent in a\nsingle motion-blurred face image with a single network and a single training\nstage. To stabilize the network training, we train the generator to restore\ncontinuous moments in the order determined by our facial motion-based\nreordering process (FMR) utilizing domain-specific knowledge of the face.\nMoreover, we propose an auxiliary regressor that helps our generator produce\nmore accurate images by estimating continuous sharp moments. Furthermore, we\nintroduce a control-adaptive (ContAda) block that performs spatially deformable\nconvolution and channel-wise attention as a function of the control factor.\nExtensive experiments on the 300VW datasets demonstrate that the proposed\nframework generates a various number of continuous output frames by varying the\nmoment control factor. Compared with the recent single-to-single image\ndeblurring networks trained with the same 300VW training set, the proposed\nmethod show the superior performance in restoring the central sharp frame in\nterms of perceptual metrics, including LPIPS, FID and Arcface identity\ndistance. The proposed method outperforms the existing single-to-video\ndeblurring method for both qualitative and quantitative comparisons.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_T/0/1/0/all/0/1\">Tae Bok Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Sujy Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heo_Y/0/1/0/all/0/1\">Yong Seok Heo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EGSDE: Unpaired Image-to-Image Translation via Energy-Guided Stochastic Differential Equations. (arXiv:2207.06635v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06635","description":"<p>Score-based diffusion generative models (SDGMs) have achieved the SOTA FID\nresults in unpaired image-to-image translation (I2I). However, we notice that\nexisting methods totally ignore the training data in the source domain, leading\nto sub-optimal solutions for unpaired I2I. To this end, we propose\nenergy-guided stochastic differential equations (EGSDE) that employs an energy\nfunction pretrained on both the source and target domains to guide the\ninference process of a pretrained SDE for realistic and faithful unpaired I2I.\nBuilding upon two feature extractors, we carefully design the energy function\nsuch that it encourages the transferred image to preserve the\ndomain-independent features and discard domainspecific ones. Further, we\nprovide an alternative explanation of the EGSDE as a product of experts, where\neach of the three experts (corresponding to the SDE and two feature extractors)\nsolely contributes to faithfulness or realism. Empirically, we compare EGSDE to\na large family of baselines on three widely-adopted unpaired I2I tasks under\nfour metrics. EGSDE not only consistently outperforms existing SDGMs-based\nmethods in almost all settings but also achieves the SOTA realism results\n(e.g., FID of 65.82 in Cat to Dog and FID of 59.75 in Wild to Dog on AFHQ)\nwithout harming the faithful performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1\">Min Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_F/0/1/0/all/0/1\">Fan Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chongxuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jun Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Source-Free Domain Adaptation for Real-world Image Dehazing. (arXiv:2207.06644v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06644","description":"<p>Deep learning-based source dehazing methods trained on synthetic datasets\nhave achieved remarkable performance but suffer from dramatic performance\ndegradation on real hazy images due to domain shift. Although certain Domain\nAdaptation (DA) dehazing methods have been presented, they inevitably require\naccess to the source dataset to reduce the gap between the source synthetic and\ntarget real domains. To address these issues, we present a novel Source-Free\nUnsupervised Domain Adaptation (SFUDA) image dehazing paradigm, in which only a\nwell-trained source model and an unlabeled target real hazy dataset are\navailable. Specifically, we devise the Domain Representation Normalization\n(DRN) module to make the representation of real hazy domain features match that\nof the synthetic domain to bridge the gaps. With our plug-and-play DRN module,\nunlabeled real hazy images can adapt existing well-trained source networks.\nBesides, the unsupervised losses are applied to guide the learning of the DRN\nmodule, which consists of frequency losses and physical prior losses. Frequency\nlosses provide structure and style constraints, while the prior loss explores\nthe inherent statistic property of haze-free images. Equipped with our DRN\nmodule and unsupervised loss, existing source dehazing models are able to\ndehaze unlabeled real hazy images. Extensive experiments on multiple baselines\ndemonstrate the validity and superiority of our method visually and\nquantitatively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hu Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yajing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1\">Qi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Man Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_F/0/1/0/all/0/1\">Feng Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prototypical Contrast Adaptation for Domain Adaptive Semantic Segmentation. (arXiv:2207.06654v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06654","description":"<p>Unsupervised Domain Adaptation (UDA) aims to adapt the model trained on the\nlabeled source domain to an unlabeled target domain. In this paper, we present\nPrototypical Contrast Adaptation (ProCA), a simple and efficient contrastive\nlearning method for unsupervised domain adaptive semantic segmentation.\nPrevious domain adaptation methods merely consider the alignment of the\nintra-class representational distributions across various domains, while the\ninter-class structural relationship is insufficiently explored, resulting in\nthe aligned representations on the target domain might not be as easily\ndiscriminated as done on the source domain anymore. Instead, ProCA incorporates\ninter-class information into class-wise prototypes, and adopts the\nclass-centered distribution alignment for adaptation. By considering the same\nclass prototypes as positives and other class prototypes as negatives to\nachieve class-centered distribution alignment, ProCA achieves state-of-the-art\nperformance on classical domain adaptation tasks, {\\em i.e., GTA5 $\\to$\nCityscapes \\text{and} SYNTHIA $\\to$ Cityscapes}. Code is available at\n\\href{https://github.com/jiangzhengkai/ProCA}{ProCA}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zhengkai Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuxi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Ceyuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Peng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yabiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1\">Ying Tai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengjie Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploration of an End-to-End Automatic Number-plate Recognition neural network for Indian datasets. (arXiv:2207.06657v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06657","description":"<p>Indian vehicle number plates have wide variety in terms of size, font, script\nand shape. Development of Automatic Number Plate Recognition (ANPR) solutions\nis therefore challenging, necessitating a diverse dataset to serve as a\ncollection of examples. However, a comprehensive dataset of Indian scenario is\nmissing, thereby, hampering the progress towards publicly available and\nreproducible ANPR solutions. Many countries have invested efforts to develop\ncomprehensive ANPR datasets like Chinese City Parking Dataset (CCPD) for China\nand Application-oriented License Plate (AOLP) dataset for US. In this work, we\nrelease an expanding dataset presently consisting of 1.5k images and a scalable\nand reproducible procedure of enhancing this dataset towards development of\nANPR solution for Indian conditions. We have leveraged this dataset to explore\nan End-to-End (E2E) ANPR architecture for Indian scenario which was originally\nproposed for Chinese Vehicle number-plate recognition based on the CCPD\ndataset. As we customized the architecture for our dataset, we came across\ninsights, which we have discussed in this paper. We report the hindrances in\ndirect reusability of the model provided by the authors of CCPD because of the\nextreme diversity in Indian number plates and differences in distribution with\nrespect to the CCPD dataset. An improvement of 42.86% was observed in LP\ndetection after aligning the characteristics of Indian dataset with Chinese\ndataset. In this work, we have also compared the performance of the E2E\nnumber-plate detection model with YOLOv5 model, pre-trained on COCO dataset and\nfine-tuned on Indian vehicle images. Given that the number Indian vehicle\nimages used for fine-tuning the detection module and yolov5 were same, we\nconcluded that it is more sample efficient to develop an ANPR solution for\nIndian conditions based on COCO dataset rather than CCPD dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nadiminti_S/0/1/0/all/0/1\">Sai Sirisha Nadiminti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaur_P/0/1/0/all/0/1\">Pranav Kant Gaur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhardwaj_A/0/1/0/all/0/1\">Abhilash Bhardwaj</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Universal Adaptive Data Augmentation. (arXiv:2207.06658v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06658","description":"<p>Existing automatic data augmentation (DA) methods either ignore updating DA's\nparameters according to the target model's state during training or adopt\nupdate strategies that are not effective enough. In this work, we design a\nnovel data augmentation strategy called \"Universal Adaptive Data Augmentation\"\n(UADA). Different from existing methods, UADA would adaptively update DA's\nparameters according to the target model's gradient information during\ntraining: given a pre-defined set of DA operations, we randomly decide types\nand magnitudes of DA operations for every data batch during training, and\nadaptively update DA's parameters along the gradient direction of the loss\nconcerning DA's parameters. In this way, UADA can increase the training loss of\nthe target networks, and the target networks would learn features from harder\nsamples to improve the generalization. Moreover, UADA is very general and can\nbe utilized in numerous tasks, e.g., image classification, semantic\nsegmentation and object detection. Extensive experiments with various models\nare conducted on CIFAR-10, CIFAR-100, ImageNet, tiny-ImageNet, Cityscapes, and\nVOC07+12 to prove the significant performance improvements brought by our\nproposed adaptive augmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiaogang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hengshuang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip Torr</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Forcing the Whole Video as Background: An Adversarial Learning Strategy for Weakly Temporal Action Localization. (arXiv:2207.06659v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06659","description":"<p>With video-level labels, weakly supervised temporal action localization\n(WTAL) applies a localization-by-classification paradigm to detect and classify\nthe action in untrimmed videos. Due to the characteristic of classification,\nclass-specific background snippets are inevitably mis-activated to improve the\ndiscriminability of the classifier in WTAL. To alleviate the disturbance of\nbackground, existing methods try to enlarge the discrepancy between action and\nbackground through modeling background snippets with pseudo-snippet-level\nannotations, which largely rely on artificial hypotheticals. Distinct from the\nprevious works, we present an adversarial learning strategy to break the\nlimitation of mining pseudo background snippets. Concretely, the background\nclassification loss forces the whole video to be regarded as the background by\na background gradient reinforcement strategy, confusing the recognition model.\nReversely, the foreground(action) loss guides the model to focus on action\nsnippets under such conditions. As a result, competition between the two\nclassification losses drives the model to boost its ability for action\nmodeling. Simultaneously, a novel temporal enhancement network is designed to\nfacilitate the model to construct temporal relation of affinity snippets based\non the proposed strategy, for further improving the performance of action\nlocalization. Finally, extensive experiments conducted on THUMOS14 and\nActivityNet1.2 demonstrate the effectiveness of the proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Ziqiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yongxin Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jiaruo Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhongming Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Point-to-Plane Registration by Efficient Backpropagation for Error Minimizing Function. (arXiv:2207.06661v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06661","description":"<p>Traditional algorithms of point set registration minimizing point-to-plane\ndistances often achieve a better estimation of rigid transformation than those\nminimizing point-to-point distances. Nevertheless, recent deep-learning-based\nmethods minimize the point-to-point distances. In contrast to these methods,\nthis paper proposes the first deep-learning-based approach to point-to-plane\nregistration. A challenging part of this problem is that a typical solution for\npoint-to-plane registration requires an iterative process of accumulating small\ntransformations obtained by minimizing a linearized energy function. The\niteration significantly increases the size of the computation graph needed for\nbackpropagation and can slow down both forward and backward network\nevaluations. To solve this problem, we consider the estimated rigid\ntransformation as a function of input point clouds and derive its analytic\ngradients using the implicit function theorem. The analytic gradient that we\nintroduce is independent of how the error minimizing function (i.e., the rigid\ntransformation) is obtained, thus allowing us to calculate both the rigid\ntransformation and its gradient efficiently. We implement the proposed\npoint-to-plane registration module over several previous methods that minimize\npoint-to-point distances and demonstrate that the extensions outperform the\nbase methods even with point clouds with noise and low-quality point normals\nestimated with local point distributions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yatagawa_T/0/1/0/all/0/1\">Tatsuya Yatagawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ohtake_Y/0/1/0/all/0/1\">Yutaka Ohtake</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suzuki_H/0/1/0/all/0/1\">Hiromasa Suzuki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Volunteer Cotton Plants in a Corn Field with Deep Learning on UAV Remote-Sensing Imagery. (arXiv:2207.06673v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06673","description":"<p>The cotton boll weevil, Anthonomus grandis Boheman is a serious pest to the\nU.S. cotton industry that has cost more than 16 billion USD in damages since it\nentered the United States from Mexico in the late 1800s. This pest has been\nnearly eradicated; however, southern part of Texas still faces this issue and\nis always prone to the pest reinfestation each year due to its sub-tropical\nclimate where cotton plants can grow year-round. Volunteer cotton (VC) plants\ngrowing in the fields of inter-seasonal crops, like corn, can serve as hosts to\nthese pests once they reach pin-head square stage (5-6 leaf stage) and\ntherefore need to be detected, located, and destroyed or sprayed . In this\npaper, we present a study to detect VC plants in a corn field using YOLOv3 on\nthree band aerial images collected by unmanned aircraft system (UAS). The\ntwo-fold objectives of this paper were : (i) to determine whether YOLOv3 can be\nused for VC detection in a corn field using RGB (red, green, and blue) aerial\nimages collected by UAS and (ii) to investigate the behavior of YOLOv3 on\nimages at three different scales (320 x 320, S1; 416 x 416, S2; and 512 x 512,\nS3 pixels) based on average precision (AP), mean average precision (mAP) and\nF1-score at 95% confidence level. No significant differences existed for mAP\namong the three scales, while a significant difference was found for AP between\nS1 and S3 (p = 0.04) and S2 and S3 (p = 0.02). A significant difference was\nalso found for F1-score between S2 and S3 (p = 0.02). The lack of significant\ndifferences of mAP at all the three scales indicated that the trained YOLOv3\nmodel can be used on a computer vision-based remotely piloted aerial\napplication system (RPAAS) for VC detection and spray application in near\nreal-time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yadav_P/0/1/0/all/0/1\">Pappu Kumar Yadav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomasson_J/0/1/0/all/0/1\">J. Alex Thomasson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hardin_R/0/1/0/all/0/1\">Robert Hardin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Searcy_S/0/1/0/all/0/1\">Stephen W. Searcy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Braga_Neto_U/0/1/0/all/0/1\">Ulisses Braga-Neto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Popescu_S/0/1/0/all/0/1\">Sorin C. Popescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_D/0/1/0/all/0/1\">Daniel E. Martin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_R/0/1/0/all/0/1\">Roberto Rodriguez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meza_K/0/1/0/all/0/1\">Karem Meza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Enciso_J/0/1/0/all/0/1\">Juan Enciso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diaz_J/0/1/0/all/0/1\">Jorge Solorzano Diaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianyi Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Subgraph Frequency Distribution Estimation using Graph Neural Networks. (arXiv:2207.06684v1 [cs.LG])","link":"http://arxiv.org/abs/2207.06684","description":"<p>Small subgraphs (graphlets) are important features to describe fundamental\nunits of a large network. The calculation of the subgraph frequency\ndistributions has a wide application in multiple domains including biology and\nengineering. Unfortunately due to the inherent complexity of this task, most of\nthe existing methods are computationally intensive and inefficient. In this\nwork, we propose GNNS, a novel representational learning framework that\nutilizes graph neural networks to sample subgraphs efficiently for estimating\ntheir frequency distribution. Our framework includes an inference model and a\ngenerative model that learns hierarchical embeddings of nodes, subgraphs, and\ngraph types. With the learned model and embeddings, subgraphs are sampled in a\nhighly scalable and parallel way and the frequency distribution estimation is\nthen performed based on these sampled subgraphs. Eventually, our methods\nachieve comparable accuracy and a significant speedup by three orders of\nmagnitude compared to existing methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhongren Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xinyue Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Shengyi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mi_L/0/1/0/all/0/1\">Lu Mi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Low-Resolution Distillation for Cost-Efficient End-to-End Text Spotting. (arXiv:2207.06694v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06694","description":"<p>End-to-end text spotting has attached great attention recently due to its\nbenefits on global optimization and high maintainability for real applications.\nHowever, the input scale has always been a tough trade-off since recognizing a\nsmall text instance usually requires enlarging the whole image, which brings\nhigh computational costs. In this paper, to address this problem, we propose a\nnovel cost-efficient Dynamic Low-resolution Distillation (DLD) text spotting\nframework, which aims to infer images in different small but recognizable\nresolutions and achieve a better balance between accuracy and efficiency.\nConcretely, we adopt a resolution selector to dynamically decide the input\nresolutions for different images, which is constraint by both inference\naccuracy and computational cost. Another sequential knowledge distillation\nstrategy is conducted on the text recognition branch, making the low-res input\nobtains comparable performance to a high-res image. The proposed method can be\noptimized end-to-end and adopted in any current text spotting framework to\nimprove the practicability. Extensive experiments on several text spotting\nbenchmarks show that the proposed method vastly improves the usability of\nlow-res models. The code is available at\nhttps://github.com/hikopensource/DAVAR-Lab-OCR/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Ying Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao1_L/0/1/0/all/0/1\">Liang Qiao1</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zhanzhan Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_S/0/1/0/all/0/1\">Shiliang Pu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_Y/0/1/0/all/0/1\">Yi Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xi Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DavarOCR: A Toolbox for OCR and Multi-Modal Document Understanding. (arXiv:2207.06695v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06695","description":"<p>This paper presents DavarOCR, an open-source toolbox for OCR and document\nunderstanding tasks. DavarOCR currently implements 19 advanced algorithms,\ncovering 9 different task forms. DavarOCR provides detailed usage instructions\nand the trained models for each algorithm. Compared with the previous\nopensource OCR toolbox, DavarOCR has relatively more complete support for the\nsub-tasks of the cutting-edge technology of document understanding. In order to\npromote the development and application of OCR technology in academia and\nindustry, we pay more attention to the use of modules that different\nsub-domains of technology can share. DavarOCR is publicly released at\nhttps://github.com/hikopensource/Davar-Lab-OCR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiao_L/0/1/0/all/0/1\">Liang Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Hui Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Ying Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Can Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Pengfei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zaisheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_B/0/1/0/all/0/1\">Baorui Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_D/0/1/0/all/0/1\">Dashan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yingda Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yunlu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zhanzhan Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_Y/0/1/0/all/0/1\">Yi Niu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SHREC 2022 Track on Online Detection of Heterogeneous Gestures. (arXiv:2207.06706v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06706","description":"<p>This paper presents the outcomes of a contest organized to evaluate methods\nfor the online recognition of heterogeneous gestures from sequences of 3D hand\nposes. The task is the detection of gestures belonging to a dictionary of 16\nclasses characterized by different pose and motion features. The dataset\nfeatures continuous sequences of hand tracking data where the gestures are\ninterleaved with non-significant motions. The data have been captured using the\nHololens 2 finger tracking system in a realistic use-case of mixed reality\ninteraction. The evaluation is based not only on the detection performances but\nalso on the latency and the false positives, making it possible to understand\nthe feasibility of practical interaction tools based on the algorithms\nproposed. The outcomes of the contest's evaluation demonstrate the necessity of\nfurther research to reduce recognition errors, while the computational cost of\nthe algorithms proposed is sufficiently low.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Caputo_A/0/1/0/all/0/1\">Ariel Caputo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Emporio_M/0/1/0/all/0/1\">Marco Emporio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giachetti_A/0/1/0/all/0/1\">Andrea Giachetti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cristani_M/0/1/0/all/0/1\">Marco Cristani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borghi_G/0/1/0/all/0/1\">Guido Borghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DEusanio_A/0/1/0/all/0/1\">Andrea D&#x27;Eusanio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_M/0/1/0/all/0/1\">Minh-Quan Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Hai-Dang Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_M/0/1/0/all/0/1\">Minh-Triet Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ambellan_F/0/1/0/all/0/1\">F. Ambellan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanik_M/0/1/0/all/0/1\">M. Hanik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nava_Yazdani_E/0/1/0/all/0/1\">E. Nava-Yazdani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tycowicz_C/0/1/0/all/0/1\">C. von Tycowicz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Octuplet Loss: Make Face Recognition Robust to Image Resolution. (arXiv:2207.06726v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06726","description":"<p>Image resolution, or in general, image quality, plays an essential role in\nthe performance of today's face recognition systems. To address this problem,\nwe propose a novel combination of the popular triplet loss to improve\nrobustness against image resolution via fine-tuning of existing face\nrecognition models. With octuplet loss, we leverage the relationship between\nhigh-resolution images and their synthetically down-sampled variants jointly\nwith their identity labels. Fine-tuning several state-of-the-art approaches\nwith our method proves that we can significantly boost performance for\ncross-resolution (high-to-low resolution) face verification on various datasets\nwithout meaningfully exacerbating the performance on high-to-high resolution\nimages. Our method applied on the FaceTransformer network achieves 95.12% face\nverification accuracy on the challenging XQLFW dataset while reaching 99.73% on\nthe LFW database. Moreover, the low-to-low face verification accuracy benefits\nfrom our method. We release our code to allow seamless integration of the\noctuplet loss into existing frameworks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Knoche_M/0/1/0/all/0/1\">Martin Knoche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elkadeem_M/0/1/0/all/0/1\">Mohamed Elkadeem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hormann_S/0/1/0/all/0/1\">Stefan H&#xf6;rmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rigoll_G/0/1/0/all/0/1\">Gerhard Rigoll</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ConCL: Concept Contrastive Learning for Dense Prediction Pre-training in Pathology Images. (arXiv:2207.06733v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06733","description":"<p>Detectingandsegmentingobjectswithinwholeslideimagesis essential in\ncomputational pathology workflow. Self-supervised learning (SSL) is appealing\nto such annotation-heavy tasks. Despite the extensive benchmarks in natural\nimages for dense tasks, such studies are, unfortunately, absent in current\nworks for pathology. Our paper intends to narrow this gap. We first benchmark\nrepresentative SSL methods for dense prediction tasks in pathology images.\nThen, we propose concept contrastive learning (ConCL), an SSL framework for\ndense pre-training. We explore how ConCL performs with concepts provided by\ndifferent sources and end up with proposing a simple dependency-free concept\ngenerating method that does not rely on external segmentation algorithms or\nsaliency detection models. Extensive experiments demonstrate the superiority of\nConCL over previous state-of-the-art SSL methods across different settings.\nAlong our exploration, we distll several important and intriguing components\ncontributing to the success of dense pre-training for pathology images. We hope\nthis work could provide useful data points and encourage the community to\nconduct ConCL pre-training for problems of interest. Code is available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jiawei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hanbo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yuan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Junzhou Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Lei He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1\">Jianhua Yao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-supervised Vector-Quantization in Visual SLAM using HGCN. (arXiv:2207.06738v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06738","description":"<p>In this paper, two semi-supervised appearance based loop closure detection\ntechnique, HGCN-FABMAP and HGCN-BoW are introduced. Furthermore an extension to\nthe current state of the art localization SLAM algorithm, ORB-SLAM, is\npresented. The proposed HGCN-FABMAP method is implemented in an off-line manner\nincorporating Bayesian probabilistic schema for loop detection decision making.\nSpecifically, we let a Hyperbolic Graph Convolutional Neural Network (HGCN) to\noperate over the SURF features graph space, and perform vector quantization\npart of the SLAM procedure. This part previously was performed in an\nunsupervised manner using algorithms like HKmeans, kmeans++,..etc. The main\nAdvantage of using HGCN, is that it scales linearly in number of graph edges.\nExperimental results shows that HGCN-FABMAP algorithm needs far more cluster\ncentroids than HGCN-ORB, otherwise it fails to detect loop closures. Therefore\nwe consider HGCN-ORB to be more efficient in terms of memory consumption, also\nwe conclude the superiority of HGCN-BoW and HGCN-FABMAP with respect to other\nalgorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zarringhalam_A/0/1/0/all/0/1\">Amir Zarringhalam</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Ghidary_S/0/1/0/all/0/1\">Saeed Shiry Ghidary</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Khorasani_A/0/1/0/all/0/1\">Ali Mohades Khorasani</a> (3) ((1),(2) and (3), Amirkabir University of Technology)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TRIE++: Towards End-to-End Information Extraction from Visually Rich Documents. (arXiv:2207.06744v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06744","description":"<p>Recently, automatically extracting information from visually rich documents\n(e.g., tickets and resumes) has become a hot and vital research topic due to\nits widespread commercial value. Most existing methods divide this task into\ntwo subparts: the text reading part for obtaining the plain text from the\noriginal document images and the information extraction part for extracting key\ncontents. These methods mainly focus on improving the second, while neglecting\nthat the two parts are highly correlated. This paper proposes a unified\nend-to-end information extraction framework from visually rich documents, where\ntext reading and information extraction can reinforce each other via a\nwell-designed multi-modal context block. Specifically, the text reading part\nprovides multi-modal features like visual, textual and layout features. The\nmulti-modal context block is developed to fuse the generated multi-modal\nfeatures and even the prior knowledge from the pre-trained language model for\nbetter semantic representation. The information extraction part is responsible\nfor generating key contents with the fused context features. The framework can\nbe trained in an end-to-end trainable manner, achieving global optimization.\nWhat is more, we define and group visually rich documents into four categories\nacross two dimensions, the layout and text type. For each document category, we\nprovide or recommend the corresponding benchmarks, experimental settings and\nstrong baselines for remedying the problem that this research area lacks the\nuniform evaluation standard. Extensive experiments on four kinds of benchmarks\n(from fixed layout to variable layout, from full-structured text to\nsemi-unstructured text) are reported, demonstrating the proposed method's\neffectiveness. Data, source code and models are available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zhanzhan Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Peng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Can Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Q/0/1/0/all/0/1\">Qiao Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yunlu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Pengfei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_S/0/1/0/all/0/1\">Shiliang Pu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_Y/0/1/0/all/0/1\">Yi Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fei Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Single-Pixel Image Reconstruction Based on Block Compressive Sensing and Deep Learning. (arXiv:2207.06746v1 [eess.IV])","link":"http://arxiv.org/abs/2207.06746","description":"<p>Single-pixel imaging (SPI) is a novel imaging technique whose working\nprinciple is based on the compressive sensing (CS) theory. In SPI, data is\nobtained through a series of compressive measurements and the corresponding\nimage is reconstructed. Typically, the reconstruction algorithm such as basis\npursuit relies on the sparsity assumption in images. However, recent advances\nin deep learning have found its uses in reconstructing CS images. Despite\nshowing a promising result in simulations, it is often unclear how such an\nalgorithm can be implemented in an actual SPI setup. In this paper, we\ndemonstrate the use of deep learning on the reconstruction of SPI images in\nconjunction with block compressive sensing (BCS). We also proposed a novel\nreconstruction model based on convolutional neural networks that outperforms\nother competitive CS reconstruction algorithms. Besides, by incorporating BCS\nin our deep learning model, we were able to reconstruct images of any size\nabove a certain smallest image size. In addition, we show that our model is\ncapable of reconstructing images obtained from an SPI setup while being priorly\ntrained on natural images, which can be vastly different from the SPI images.\nThis opens up opportunity for the feasibility of pretrained deep learning\nmodels for CS reconstructions of images from various domain areas.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lau_S/0/1/0/all/0/1\">Stephen L. H. Lau</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chong_E/0/1/0/all/0/1\">Edwin K. P. Chong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"E2-AEN: End-to-End Incremental Learning with Adaptively Expandable Network. (arXiv:2207.06754v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06754","description":"<p>Expandable networks have demonstrated their advantages in dealing with\ncatastrophic forgetting problem in incremental learning. Considering that\ndifferent tasks may need different structures, recent methods design dynamic\nstructures adapted to different tasks via sophisticated skills. Their routine\nis to search expandable structures first and then train on the new tasks,\nwhich, however, breaks tasks into multiple training stages, leading to\nsuboptimal or overmuch computational cost. In this paper, we propose an\nend-to-end trainable adaptively expandable network named E2-AEN, which\ndynamically generates lightweight structures for new tasks without any accuracy\ndrop in previous tasks. Specifically, the network contains a serial of powerful\nfeature adapters for augmenting the previously learned representations to new\ntasks, and avoiding task interference. These adapters are controlled via an\nadaptive gate-based pruning strategy which decides whether the expanded\nstructures can be pruned, making the network structure dynamically changeable\naccording to the complexity of the new tasks. Moreover, we introduce a novel\nsparsity-activation regularization to encourage the model to learn\ndiscriminative features with limited parameters. E2-AEN reduces cost and can be\nbuilt upon any feed-forward architectures in an end-to-end manner. Extensive\nexperiments on both classification (i.e., CIFAR and VDD) and detection (i.e.,\nCOCO, VOC and ICCV2021 SSLAD challenge) benchmarks demonstrate the\neffectiveness of the proposed method, which achieves the new remarkable\nresults.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_G/0/1/0/all/0/1\">Guimei Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zhanzhan Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yunlu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Duo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_S/0/1/0/all/0/1\">Shiliang Pu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_Y/0/1/0/all/0/1\">Yi Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fei Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neighbor Correspondence Matching for Flow-based Video Frame Synthesis. (arXiv:2207.06763v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06763","description":"<p>Video frame synthesis, which consists of interpolation and extrapolation, is\nan essential video processing technique that can be applied to various\nscenarios. However, most existing methods cannot handle small objects or large\nmotion well, especially in high-resolution videos such as 4K videos. To\neliminate such limitations, we introduce a neighbor correspondence matching\n(NCM) algorithm for flow-based frame synthesis. Since the current frame is not\navailable in video frame synthesis, NCM is performed in a\ncurrent-frame-agnostic fashion to establish multi-scale correspondences in the\nspatial-temporal neighborhoods of each pixel. Based on the powerful motion\nrepresentation capability of NCM, we further propose to estimate intermediate\nflows for frame synthesis in a heterogeneous coarse-to-fine scheme.\nSpecifically, the coarse-scale module is designed to leverage neighbor\ncorrespondences to capture large motion, while the fine-scale module is more\ncomputationally efficient to speed up the estimation process. Both modules are\ntrained progressively to eliminate the resolution gap between training dataset\nand real-world videos. Experimental results show that NCM achieves\nstate-of-the-art performance on several benchmarks. In addition, NCM can be\napplied to various practical scenarios such as video compression to achieve\nbetter performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jia_Z/0/1/0/all/0/1\">Zhaoyang Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Houqiang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GeoSegNet: Point Cloud Semantic Segmentation via Geometric Encoder-Decoder Modeling. (arXiv:2207.06766v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06766","description":"<p>Semantic segmentation of point clouds, aiming to assign each point a semantic\ncategory, is critical to 3D scene understanding.Despite of significant advances\nin recent years, most of existing methods still suffer from either the\nobject-level misclassification or the boundary-level ambiguity. In this paper,\nwe present a robust semantic segmentation network by deeply exploring the\ngeometry of point clouds, dubbed GeoSegNet. Our GeoSegNet consists of a\nmulti-geometry based encoder and a boundary-guided decoder. In the encoder, we\ndevelop a new residual geometry module from multi-geometry perspectives to\nextract object-level features. In the decoder, we introduce a contrastive\nboundary learning module to enhance the geometric representation of boundary\npoints. Benefiting from the geometric encoder-decoder modeling, our GeoSegNet\ncan infer the segmentation of objects effectively while making the\nintersections (boundaries) of two or more objects clear. Experiments show\nobvious improvements of our method over its competitors in terms of the overall\nsegmentation accuracy and object boundary clearness. Code is available at\nhttps://github.com/Chen-yuiyui/GeoSegNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yisen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Honghua Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1\">Xuefeng Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_D/0/1/0/all/0/1\">Dayong Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yanwen Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1\">Haoran Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fu Lee Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_M/0/1/0/all/0/1\">Mingqiang Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Empirical Evaluation of Four Off-the-Shelf Proprietary Visual-Inertial Odometry Systems. (arXiv:2207.06780v1 [cs.RO])","link":"http://arxiv.org/abs/2207.06780","description":"<p>Commercial visual-inertial odometry (VIO) systems have been gaining attention\nas cost-effective, off-the-shelf six degrees of freedom (6-DoF) ego-motion\ntracking methods for estimating accurate and consistent camera pose data, in\naddition to their ability to operate without external localization from motion\ncapture or global positioning systems. It is unclear from existing results,\nhowever, which commercial VIO platforms are the most stable, consistent, and\naccurate in terms of state estimation for indoor and outdoor robotic\napplications. We assess four popular proprietary VIO systems (Apple ARKit,\nGoogle ARCore, Intel RealSense T265, and Stereolabs ZED 2) through a series of\nboth indoor and outdoor experiments where we show their positioning stability,\nconsistency, and accuracy. We present our complete results as a benchmark\ncomparison for the research community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jungha Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1\">Minkyeong Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yeoeun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_M/0/1/0/all/0/1\">Moonkyeong Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_P/0/1/0/all/0/1\">Pyojin Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inertial Hallucinations -- When Wearable Inertial Devices Start Seeing Things. (arXiv:2207.06789v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06789","description":"<p>We propose a novel approach to multimodal sensor fusion for Ambient Assisted\nLiving (AAL) which takes advantage of learning using privileged information\n(LUPI). We address two major shortcomings of standard multimodal approaches,\nlimited area coverage and reduced reliability. Our new framework fuses the\nconcept of modality hallucination with triplet learning to train a model with\ndifferent modalities to handle missing sensors at inference time. We evaluate\nthe proposed model on inertial data from a wearable accelerometer device, using\nRGB videos and skeletons as privileged modalities, and show an improvement of\naccuracy of an average 6.6% on the UTD-MHAD dataset and an average 5.5% on the\nBerkeley MHAD dataset, reaching a new state-of-the-art for inertial-only\nclassification accuracy on these datasets. We validate our framework through\nseveral ablation studies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Masullo_A/0/1/0/all/0/1\">Alessandro Masullo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perrett_T/0/1/0/all/0/1\">Toby Perrett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burghardt_T/0/1/0/all/0/1\">Tilo Burghardt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Craddock_I/0/1/0/all/0/1\">Ian Craddock</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Damen_D/0/1/0/all/0/1\">Dima Damen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirmehdi_M/0/1/0/all/0/1\">Majid Mirmehdi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural apparent BRDF fields for multiview photometric stereo. (arXiv:2207.06793v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06793","description":"<p>We propose to tackle the multiview photometric stereo problem using an\nextension of Neural Radiance Fields (NeRFs), conditioned on light source\ndirection. The geometric part of our neural representation predicts surface\nnormal direction, allowing us to reason about local surface reflectance. The\nappearance part of our neural representation is decomposed into a neural\nbidirectional reflectance function (BRDF), learnt as part of the fitting\nprocess, and a shadow prediction network (conditioned on light source\ndirection) allowing us to model the apparent BRDF. This balance of learnt\ncomponents with inductive biases based on physical image formation models\nallows us to extrapolate far from the light source and viewer directions\nobserved during training. We demonstrate our approach on a multiview\nphotometric stereo benchmark and show that competitive performance can be\nobtained with the neural density representation of a NeRF.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Asthana_M/0/1/0/all/0/1\">Meghna Asthana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_W/0/1/0/all/0/1\">William A. P. Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huber_P/0/1/0/all/0/1\">Patrik Huber</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Multi-Modality Ovarian Tumor Ultrasound Image Dataset for Unsupervised Cross-Domain Semantic Segmentation. (arXiv:2207.06799v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06799","description":"<p>Ovarian cancer is one of the most harmful gynecological diseases. Detecting\novarian tumors in early stage with computer-aided techniques can efficiently\ndecrease the mortality rate. With the improvement of medical treatment\nstandard, ultrasound images are widely applied in clinical treatment. However,\nrecent notable methods mainly focus on single-modality ultrasound ovarian tumor\nsegmentation or recognition, which means there still lacks of researches on\nexploring the representation capability of multi-modality ultrasound ovarian\ntumor images. To solve this problem, we propose a Multi-Modality Ovarian Tumor\nUltrasound (MMOTU) image dataset containing 1469 2d ultrasound images and 170\ncontrast enhanced ultrasonography (CEUS) images with pixel-wise and global-wise\nannotations. Based on MMOTU, we mainly focus on unsupervised cross-domain\nsemantic segmentation task. To solve the domain shift problem, we propose a\nfeature alignment based architecture named Dual-Scheme Domain-Selected Network\n(DS$^2$Net). Specifically, we first design source-encoder and target-encoder to\nextract two-style features of source and target images. Then, we propose\nDomain-Distinct Selected Module (DDSM) and Domain-Universal Selected Module\n(DUSM) to extract the distinct and universal features in two styles\n(source-style or target-style). Finally, we fuse these two kinds of features\nand feed them into the source-decoder and target-decoder to generate final\npredictions. Extensive comparison experiments and analysis on MMOTU image\ndataset show that DS$^2$Net can boost the segmentation performance for\nbidirectional cross-domain adaptation of 2d ultrasound images and CEUS images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1\">Qi Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1\">Shuchang Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_W/0/1/0/all/0/1\">Wenpei Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_L/0/1/0/all/0/1\">Linghan Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Binghao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Meijing Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sang_X/0/1/0/all/0/1\">Xiubo Sang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Min Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lijiang Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pseudo-Labeling Based Practical Semi-Supervised Meta-Training for Few-Shot Learning. (arXiv:2207.06817v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06817","description":"<p>Most existing few-shot learning (FSL) methods require a large amount of\nlabeled data in meta-training, which is a major limit. To reduce the\nrequirement of labels, a semi-supervised meta-training setting has been\nproposed for FSL, which includes only a few labeled samples and numbers of\nunlabeled samples in base classes. However, existing methods under this setting\nrequire class-aware sample selection from the unlabeled set, which violates the\nassumption of unlabeled set. In this paper, we propose a practical\nsemi-supervised meta-training setting with truly unlabeled data. Under the new\nsetting, the performance of existing methods drops notably. To better utilize\nboth the labeled and truly unlabeled data, we propose a simple and effective\nmeta-training framework, called pseudo-labeling based on meta-learning (PLML).\nFirstly, we train a classifier via common semi-supervised learning (SSL) and\nuse it to obtain the pseudo-labels of unlabeled data. Then we build few-shot\ntasks from labeled and pseudo-labeled data and run meta-learning over the\nconstructed tasks to learn the FSL model. Surprisingly, through extensive\nexperiments across two FSL datasets, we find that this simple meta-training\nframework effectively prevents the performance degradation of FSL under limited\nlabeled data. Besides, benefiting from meta-training, the proposed method\nimproves the classifiers learned by two representative SSL algorithms as well.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xingping Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_S/0/1/0/all/0/1\">Shengcai Liao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DEXTER: An end-to-end system to extract table contents from electronic medical health documents. (arXiv:2207.06823v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06823","description":"<p>In this paper, we propose DEXTER, an end to end system to extract information\nfrom tables present in medical health documents, such as electronic health\nrecords (EHR) and explanation of benefits (EOB). DEXTER consists of four\nsub-system stages: i) table detection ii) table type classification iii) cell\ndetection; and iv) cell content extraction. We propose a two-stage transfer\nlearning-based approach using CDeC-Net architecture along with Non-Maximal\nsuppression for table detection. We design a conventional computer vision-based\napproach for table type classification and cell detection using parameterized\nkernels based on image size for detecting rows and columns. Finally, we extract\nthe text from the detected cells using pre-existing OCR engine Tessaract. To\nevaluate our system, we manually annotated a sample of the real-world medical\ndataset (referred to as Meddata) consisting of wide variations of documents (in\nterms of appearance) covering different table structures, such as bordered,\npartially bordered, borderless, or coloured tables. We experimentally show that\nDEXTER outperforms the commercially available Amazon Textract and Microsoft\nAzure Form Recognizer systems on the annotated real-world medical dataset\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+PR_N/0/1/0/all/0/1\">Nandhinee PR</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnamoorthy_H/0/1/0/all/0/1\">Harinath Krishnamoorthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_A/0/1/0/all/0/1\">Anil Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santhiappan_S/0/1/0/all/0/1\">Sudarsun Santhiappan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Refign: Align and Refine for Adaptation of Semantic Segmentation to Adverse Conditions. (arXiv:2207.06825v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06825","description":"<p>Due to the scarcity of dense pixel-level semantic annotations for images\nrecorded in adverse visual conditions, there has been a keen interest in\nunsupervised domain adaptation (UDA) for the semantic segmentation of such\nimages. UDA adapts models trained on normal conditions to the target\nadverse-condition domains. Meanwhile, multiple datasets with driving scenes\nprovide corresponding images of the same scenes across multiple conditions,\nwhich can serve as a form of weak supervision for domain adaptation. We propose\nRefign, a generic extension to self-training-based UDA methods which leverages\nthese cross-domain correspondences. Refign consists of two steps: (1) aligning\nthe normal-condition image to the corresponding adverse-condition image using\nan uncertainty-aware dense matching network, and (2) refining the adverse\nprediction with the normal prediction using an adaptive label correction\nmechanism. We design custom modules to streamline both steps and set the new\nstate of the art for domain-adaptive semantic segmentation on several\nadverse-condition benchmarks, including ACDC and Dark Zurich. The approach\nintroduces no extra training parameters, minimal computational overhead --\nduring training only -- and can be used as a drop-in extension to improve any\ngiven self-training-based UDA method. Code is available at\nhttps://github.com/brdav/refign.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bruggemann_D/0/1/0/all/0/1\">David Bruggemann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sakaridis_C/0/1/0/all/0/1\">Christos Sakaridis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Truong_P/0/1/0/all/0/1\">Prune Truong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Point-to-Box Network for Accurate Object Detection via Single Point Supervision. (arXiv:2207.06827v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06827","description":"<p>Object detection using single point supervision has received increasing\nattention over the years. In this paper, we attribute such a large performance\ngap to the failure of generating high-quality proposal bags which are crucial\nfor multiple instance learning (MIL). To address this problem, we introduce a\nlightweight alternative to the off-the-shelf proposal (OTSP) method and thereby\ncreate the Point-to-Box Network (P2BNet), which can construct an inter-objects\nbalanced proposal bag by generating proposals in an anchor-like way. By fully\ninvestigating the accurate position information, P2BNet further constructs an\ninstance-level bag, avoiding the mixture of multiple objects. Finally, a\ncoarse-to-fine policy in a cascade fashion is utilized to improve the IoU\nbetween proposals and ground-truth (GT). Benefiting from these strategies,\nP2BNet is able to produce high-quality instance-level bags for object\ndetection. P2BNet improves the mean average precision (AP) by more than 50%\nrelative to the previous best PSOD method on the MS COCO dataset. It also\ndemonstrates the great potential to bridge the performance gap between point\nsupervised and bounding-box supervised detectors. The code will be released at\ngithub.com/ucas-vg/P2BNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Pengfei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xuehui Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xumeng Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassan_N/0/1/0/all/0/1\">Najmul Hassan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiachen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jian Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Humphrey Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1\">Zhenjun Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1\">Qixiang Ye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pose-based Tremor Classification for Parkinson's Disease Diagnosis from Video. (arXiv:2207.06828v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06828","description":"<p>Parkinson's disease (PD) is a progressive neurodegenerative disorder that\nresults in a variety of motor dysfunction symptoms, including tremors,\nbradykinesia, rigidity and postural instability. The diagnosis of PD mainly\nrelies on clinical experience rather than a definite medical test, and the\ndiagnostic accuracy is only about 73-84% since it is challenged by the\nsubjective opinions or experiences of different medical experts. Therefore, an\nefficient and interpretable automatic PD diagnosis system is valuable for\nsupporting clinicians with more robust diagnostic decision-making. To this end,\nwe propose to classify Parkinson's tremor since it is one of the most\npredominant symptoms of PD with strong generalizability. Different from other\ncomputer-aided time and resource-consuming Parkinson's Tremor (PT)\nclassification systems that rely on wearable sensors, we propose SPAPNet, which\nonly requires consumer-grade non-intrusive video recording of camera-facing\nhuman movements as input to provide undiagnosed patients with low-cost PT\nclassification results as a PD warning sign. For the first time, we propose to\nuse a novel attention module with a lightweight pyramidal\nchannel-squeezing-fusion architecture to extract relevant PT information and\nfilter the noise efficiently. This design aids in improving both classification\nperformance and system interpretability. Experimental results show that our\nsystem outperforms state-of-the-arts by achieving a balanced accuracy of 90.9%\nand an F1-score of 90.6% in classifying PT with the non-PT class.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haozheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_E/0/1/0/all/0/1\">Edmond S.L. Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiatian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shum_H/0/1/0/all/0/1\">Hubert P.H. Shum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"iColoriT: Towards Propagating Local Hint to the Right Region in Interactive Colorization by Leveraging Vision Transformer. (arXiv:2207.06831v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06831","description":"<p>Point-interactive image colorization aims to colorize grayscale images when a\nuser provides the colors for specific locations. It is essential for\npoint-interactive colorization methods to appropriately propagate user-provided\ncolors (i.e., user hints) in the entire image to obtain a reasonably colorized\nimage with minimal user effort. However, existing approaches often produce\npartially colorized results due to the inefficient design of stacking\nconvolutional layers to propagate hints to distant relevant regions. To address\nthis problem, we present iColoriT, a novel point-interactive colorization\nVision Transformer capable of propagating user hints to relevant regions,\nleveraging the global receptive field of Transformers. The self-attention\nmechanism of Transformers enables iColoriT to selectively colorize relevant\nregions with only a few local hints. Our approach colorizes images in real-time\nby utilizing pixel shuffling, an efficient upsampling technique that replaces\nthe decoder architecture. Also, in order to mitigate the artifacts caused by\npixel shuffling with large upsampling ratios, we present the local stabilizing\nlayer. Extensive quantitative and qualitative results demonstrate that our\napproach highly outperforms existing methods for point-interactive\ncolorization, producing accurately colorized images with a user's minimal\neffort.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sanghyeon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yun_J/0/1/0/all/0/1\">Jooyeol Yun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_M/0/1/0/all/0/1\">Minho Park</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enforcing connectivity of 3D linear structures using their 2D projections. (arXiv:2207.06832v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06832","description":"<p>Many biological and medical tasks require the delineation of 3D curvilinear\nstructures such as blood vessels and neurites from image volumes. This is\ntypically done using neural networks trained by minimizing voxel-wise loss\nfunctions that do not capture the topological properties of these structures.\nAs a result, the connectivity of the recovered structures is often wrong, which\nlessens their usefulness. In this paper, we propose to improve the 3D\nconnectivity of our results by minimizing a sum of topology-aware losses on\ntheir 2D projections. This suffices to increase the accuracy and to reduce the\nannotation effort required to provide the required annotated training data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Oner_D/0/1/0/all/0/1\">Doruk Oner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osman_H/0/1/0/all/0/1\">Hussein Osman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kozinski_M/0/1/0/all/0/1\">Mateusz Kozinski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fua_P/0/1/0/all/0/1\">Pascal Fua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Dictionary Learning with An Intra-class Constraint. (arXiv:2207.06841v1 [cs.LG])","link":"http://arxiv.org/abs/2207.06841","description":"<p>In recent years, deep dictionary learning (DDL)has attracted a great amount\nof attention due to its effectiveness for representation learning and visual\nrecognition.~However, most existing methods focus on unsupervised deep\ndictionary learning, failing to further explore the category information.~To\nmake full use of the category information of different samples, we propose a\nnovel deep dictionary learning model with an intra-class constraint (DDLIC) for\nvisual classification. Specifically, we design the intra-class compactness\nconstraint on the intermediate representation at different levels to encourage\nthe intra-class representations to be closer to each other, and eventually the\nlearned representation becomes more discriminative.~Unlike the traditional DDL\nmethods, during the classification stage, our DDLIC performs a layer-wise\ngreedy optimization in a similar way to the training stage. Experimental\nresults on four image datasets show that our method is superior to the\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xia Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gou_J/0/1/0/all/0/1\">Jianping Gou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Baosheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jiali Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_Z/0/1/0/all/0/1\">Zhang Yi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AIParsing: Anchor-free Instance-level Human Parsing. (arXiv:2207.06854v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06854","description":"<p>Most state-of-the-art instance-level human parsing models adopt two-stage\nanchor-based detectors and, therefore, cannot avoid the heuristic anchor box\ndesign and the lack of analysis on a pixel level. To address these two issues,\nwe have designed an instance-level human parsing network which is anchor-free\nand solvable on a pixel level. It consists of two simple sub-networks: an\nanchor-free detection head for bounding box predictions and an edge-guided\nparsing head for human segmentation. The anchor-free detector head inherits the\npixel-like merits and effectively avoids the sensitivity of hyper-parameters as\nproved in object detection applications. By introducing the part-aware boundary\nclue, the edge-guided parsing head is capable to distinguish adjacent human\nparts from among each other up to 58 parts in a single human instance, even\noverlapping instances. Meanwhile, a refinement head integrating box-level score\nand part-level parsing quality is exploited to improve the quality of the\nparsing results. Experiments on two multiple human parsing datasets (i.e., CIHP\nand LV-MHP-v2.0) and one video instance-level human parsing dataset (i.e., VIP)\nshow that our method achieves the best global-level and instance-level\nperformance over state-of-the-art one-stage top-down alternatives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Sanyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1\">Xiaochun Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_G/0/1/0/all/0/1\">Guo-Jun Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1\">Zhanjie Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Immunofluorescence Capillary Imaging Segmentation: Cases Study. (arXiv:2207.06861v1 [eess.IV])","link":"http://arxiv.org/abs/2207.06861","description":"<p>Nonunion is one of the challenges faced by orthopedics clinics for the\ntechnical difficulties and high costs in photographing interosseous\ncapillaries. Segmenting vessels and filling capillaries are critical in\nunderstanding the obstacles encountered in capillary growth. However, existing\ndatasets for blood vessel segmentation mainly focus on the large blood vessels\nof the body, and the lack of labeled capillary image datasets greatly limits\nthe methodological development and applications of vessel segmentation and\ncapillary filling. Here, we present a benchmark dataset, named IFCIS-155,\nconsisting of 155 2D capillary images with segmentation boundaries and vessel\nfillings annotated by biomedical experts, and 19 large-scale, high-resolution\n3D capillary images. To obtain better images of interosseous capillaries, we\nleverage state-of-the-art immunofluorescence imaging techniques to highlight\nthe rich vascular morphology of interosseous capillaries. We conduct\ncomprehensive experiments to verify the effectiveness of the dataset and the\nbenchmarking deep learning models (\\eg UNet/UNet++ and the modified\nUNet/UNet++). Our work offers a benchmark dataset for training deep learning\nmodels for capillary image segmentation and provides a potential tool for\nfuture capillary research. The IFCIS-155 dataset and code are all publicly\navailable at \\url{https://github.com/ncclabsustech/IFCIS-55}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hou_R/0/1/0/all/0/1\">Runpeng Hou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ye_Z/0/1/0/all/0/1\">Ziyuan Ye</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_C/0/1/0/all/0/1\">Chengyu Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fu_L/0/1/0/all/0/1\">Linhao Fu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_C/0/1/0/all/0/1\">Chao Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Q/0/1/0/all/0/1\">Quanying Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BayesCap: Bayesian Identity Cap for Calibrated Uncertainty in Frozen Neural Networks. (arXiv:2207.06873v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06873","description":"<p>High-quality calibrated uncertainty estimates are crucial for numerous\nreal-world applications, especially for deep learning-based deployed ML\nsystems. While Bayesian deep learning techniques allow uncertainty estimation,\ntraining them with large-scale datasets is an expensive process that does not\nalways yield models competitive with non-Bayesian counterparts. Moreover, many\nof the high-performing deep learning models that are already trained and\ndeployed are non-Bayesian in nature and do not provide uncertainty estimates.\nTo address these issues, we propose BayesCap that learns a Bayesian identity\nmapping for the frozen model, allowing uncertainty estimation. BayesCap is a\nmemory-efficient method that can be trained on a small fraction of the original\ndataset, enhancing pretrained non-Bayesian computer vision models by providing\ncalibrated uncertainty estimates for the predictions without (i) hampering the\nperformance of the model and (ii) the need for expensive retraining the model\nfrom scratch. The proposed method is agnostic to various architectures and\ntasks. We show the efficacy of our method on a wide variety of tasks with a\ndiverse set of architectures, including image super-resolution, deblurring,\ninpainting, and crucial application such as medical image translation.\nMoreover, we apply the derived uncertainty estimates to detect\nout-of-distribution samples in critical scenarios like depth estimation in\nautonomous driving. Code is available at\nhttps://github.com/ExplainableML/BayesCap.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Upadhyay_U/0/1/0/all/0/1\">Uddeshya Upadhyay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karthik_S/0/1/0/all/0/1\">Shyamgopal Karthik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yanbei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mancini_M/0/1/0/all/0/1\">Massimiliano Mancini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akata_Z/0/1/0/all/0/1\">Zeynep Akata</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"E2FIF: Push the limit of Binarized Deep Imagery Super-resolution using End-to-end Full-precision Information Flow. (arXiv:2207.06893v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06893","description":"<p>Binary neural network (BNN) provides a promising solution to deploy\nparameter-intensive deep single image super-resolution (SISR) models onto real\ndevices with limited storage and computational resources. To achieve comparable\nperformance with the full-precision counterpart, most existing BNNs for SISR\nmainly focus on compensating the information loss incurred by binarizing\nweights and activations in the network through better approximations to the\nbinarized convolution. In this study, we revisit the difference between BNNs\nand their full-precision counterparts and argue that the key for good\ngeneralization performance of BNNs lies on preserving a complete full-precision\ninformation flow as well as an accurate gradient flow passing through each\nbinarized convolution layer. Inspired by this, we propose to introduce a\nfull-precision skip connection or its variant over each binarized convolution\nlayer across the entire network, which can increase the forward expressive\ncapability and the accuracy of back-propagated gradient, thus enhancing the\ngeneralization performance. More importantly, such a scheme is applicable to\nany existing BNN backbones for SISR without introducing any additional\ncomputation cost. To testify its efficacy, we evaluate it using four different\nbackbones for SISR on four benchmark datasets and report obviously superior\nperformance over existing BNNs and even some 4-bit competitors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lang_Z/0/1/0/all/0/1\">Zhiqiang Lang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1\">Wei Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Factorized and Controllable Neural Re-Rendering of Outdoor Scene for Photo Extrapolation. (arXiv:2207.06899v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06899","description":"<p>Expanding an existing tourist photo from a partially captured scene to a full\nscene is one of the desired experiences for photography applications. Although\nphoto extrapolation has been well studied, it is much more challenging to\nextrapolate a photo (i.e., selfie) from a narrow field of view to a wider one\nwhile maintaining a similar visual style. In this paper, we propose a\nfactorized neural re-rendering model to produce photorealistic novel views from\ncluttered outdoor Internet photo collections, which enables the applications\nincluding controllable scene re-rendering, photo extrapolation and even\nextrapolated 3D photo generation. Specifically, we first develop a novel\nfactorized re-rendering pipeline to handle the ambiguity in the decomposition\nof geometry, appearance and illumination. We also propose a composited training\nstrategy to tackle the unexpected occlusion in Internet images. Moreover, to\nenhance photo-realism when extrapolating tourist photographs, we propose a\nnovel realism augmentation process to complement appearance details, which\nautomatically propagates the texture details from a narrow captured photo to\nthe extrapolated neural rendered image. The experiments and photo editing\nexamples on outdoor scenes demonstrate the superior performance of our proposed\nmethod in both photo-realism and downstream applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1\">Boming Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Bangbang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zuoyue Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guofeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jiashu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1\">Dawei Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1\">Zhaopeng Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1\">Hujun Bao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Real-time Streaming Video Denoising with Bidirectional Buffers. (arXiv:2207.06937v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06937","description":"<p>Video streams are delivered continuously to save the cost of storage and\ndevice memory. Real-time denoising algorithms are typically adopted on the user\ndevice to remove the noise involved during the shooting and transmission of\nvideo streams. However, sliding-window-based methods feed multiple input frames\nfor a single output and lack computation efficiency. Recent multi-output\ninference works propagate the bidirectional temporal feature with a parallel or\nrecurrent framework, which either suffers from performance drops on the\ntemporal edges of clips or can not achieve online inference. In this paper, we\npropose a Bidirectional Streaming Video Denoising (BSVD) framework, to achieve\nhigh-fidelity real-time denoising for streaming videos with both past and\nfuture temporal receptive fields. The bidirectional temporal fusion for online\ninference is considered not applicable in the MoViNet. However, we introduce a\nnovel Bidirectional Buffer Block as the core module of our BSVD, which makes it\npossible during our pipeline-style inference. In addition, our method is\nconcise and flexible to be utilized in both non-blind and blind video\ndenoising. We compare our model with various state-of-the-art video denoising\nmodels qualitatively and quantitatively on synthetic and real noise. Our method\noutperforms previous methods in terms of restoration fidelity and runtime. Our\nsource code is publicly available at https://github.com/ChenyangQiQi/BSVD\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qi_C/0/1/0/all/0/1\">Chenyang Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Junming Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qifeng Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Insurgency as Complex Network: Image Co-Appearance and Hierarchy in the PKK. (arXiv:2207.06946v1 [cs.SI])","link":"http://arxiv.org/abs/2207.06946","description":"<p>Despite a growing recognition of the importance of insurgent group structure\non conflict outcomes, there is very little empirical research thereon. Though\nthis problem is rooted in the inaccessibility of data on militant group\nstructure, insurgents frequently publish large volumes of image data on the\ninternet. In this paper, I develop a new methodology that leverages this\nabundant but underutilized source of data by automating the creation of a\nsocial network graph based on co-appearance in photographs using deep learning.\nUsing a trove of 19,115 obituary images published online by the PKK, a Kurdish\nmilitant group in Turkey, I demonstrate that an individual's centrality in the\nresulting co-appearance network is closely correlated with their rank in the\ninsurgent group.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ballinger_O/0/1/0/all/0/1\">Ollie Ballinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tackling Background Distraction in Video Object Segmentation. (arXiv:2207.06953v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06953","description":"<p>Semi-supervised video object segmentation (VOS) aims to densely track certain\ndesignated objects in videos. One of the main challenges in this task is the\nexistence of background distractors that appear similar to the target objects.\nWe propose three novel strategies to suppress such distractors: 1) a\nspatio-temporally diversified template construction scheme to obtain\ngeneralized properties of the target objects; 2) a learnable distance-scoring\nfunction to exclude spatially-distant distractors by exploiting the temporal\nconsistency between two consecutive frames; 3) swap-and-attach augmentation to\nforce each object to have unique features by providing training samples\ncontaining entangled objects. On all public benchmark datasets, our model\nachieves a comparable performance to contemporary state-of-the-art approaches,\neven with real-time performance. Qualitative results also demonstrate the\nsuperiority of our approach over existing methods. We believe our approach will\nbe widely used for future VOS research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1\">Suhwan Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Heansung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Minhyeok Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1\">Chaewon Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_S/0/1/0/all/0/1\">Sungjun Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Minjung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sangyoun Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Implicit Templates for Point-Based Clothed Human Modeling. (arXiv:2207.06955v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06955","description":"<p>We present FITE, a First-Implicit-Then-Explicit framework for modeling human\navatars in clothing. Our framework first learns implicit surface templates\nrepresenting the coarse clothing topology, and then employs the templates to\nguide the generation of point sets which further capture pose-dependent\nclothing deformations such as wrinkles. Our pipeline incorporates the merits of\nboth implicit and explicit representations, namely, the ability to handle\nvarying topology and the ability to efficiently capture fine details. We also\npropose diffused skinning to facilitate template training especially for loose\nclothing, and projection-based pose-encoding to extract pose information from\nmesh templates without predefined UV map or connectivity. Our code is publicly\navailable at https://github.com/jsnln/fite.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Siyou Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongwen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zerong Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_R/0/1/0/all/0/1\">Ruizhi Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yebin Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AutoMerge: A Framework for Map Assembling and Smoothing in City-scale Environments. (arXiv:2207.06965v1 [cs.RO])","link":"http://arxiv.org/abs/2207.06965","description":"<p>We present AutoMerge, a LiDAR data processing framework for assembling a\nlarge number of map segments into a complete map. Traditional large-scale map\nmerging methods are fragile to incorrect data associations, and are primarily\nlimited to working only offline. AutoMerge utilizes multi-perspective fusion\nand adaptive loop closure detection for accurate data associations, and it uses\nincremental merging to assemble large maps from individual trajectory segments\ngiven in random order and with no initial estimations. Furthermore, after\nassembling the segments, AutoMerge performs fine matching and pose-graph\noptimization to globally smooth the merged map. We demonstrate AutoMerge on\nboth city-scale merging (120km) and campus-scale repeated merging (4.5km x 8).\nThe experiments show that AutoMerge (i) surpasses the second- and third- best\nmethods by 14% and 24% recall in segment retrieval, (ii) achieves comparable 3D\nmapping accuracy for 120 km large-scale map assembly, (iii) and it is robust to\ntemporally-spaced revisits. To the best of our knowledge, AutoMerge is the\nfirst mapping approach that can merge hundreds of kilometers of individual\nsegments without the aid of GPS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yin_P/0/1/0/all/0/1\">Peng Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_H/0/1/0/all/0/1\">Haowen Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shiqi Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_R/0/1/0/all/0/1\">Ruijie Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cisneros_I/0/1/0/all/0/1\">Ivan Cisneros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_R/0/1/0/all/0/1\">Ruohai Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Ji Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choset_H/0/1/0/all/0/1\">Howie Choset</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scherer_S/0/1/0/all/0/1\">Sebastian Scherer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scene Text Recognition with Permuted Autoregressive Sequence Models. (arXiv:2207.06966v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06966","description":"<p>Context-aware STR methods typically use internal autoregressive (AR) language\nmodels (LM). Inherent limitations of AR models motivated two-stage methods\nwhich employ an external LM. The conditional independence of the external LM on\nthe input image may cause it to erroneously rectify correct predictions,\nleading to significant inefficiencies. Our method, PARSeq, learns an ensemble\nof internal AR LMs with shared weights using Permutation Language Modeling. It\nunifies context-free non-AR and context-aware AR inference, and iterative\nrefinement using bidirectional context. Using synthetic training data, PARSeq\nachieves state-of-the-art (SOTA) results in STR benchmarks (91.9% accuracy) and\nmore challenging datasets. It establishes new SOTA results (96.0% accuracy)\nwhen trained on real data. PARSeq is optimal on accuracy vs parameter count,\nFLOPS, and latency because of its simple, unified structure and parallel token\nprocessing. Due to its extensive use of attention, it is robust on\narbitrarily-oriented text which is common in real-world images. Code,\npretrained weights, and data are available at: https://github.com/baudm/parseq.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bautista_D/0/1/0/all/0/1\">Darwin Bautista</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atienza_R/0/1/0/all/0/1\">Rowel Atienza</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PR-DARTS: Pruning-Based Differentiable Architecture Search. (arXiv:2207.06968v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06968","description":"<p>The deployment of Convolutional Neural Networks (CNNs) on edge devices is\nhindered by the substantial gap between performance requirements and available\nprocessing power. While recent research has made large strides in developing\nnetwork pruning methods for reducing the computing overhead of CNNs, there\nremains considerable accuracy loss, especially at high pruning ratios.\nQuestioning that the architectures designed for non-pruned networks might not\nbe effective for pruned networks, we propose to search architectures for\npruning methods by defining a new search space and a novel search objective. To\nimprove the generalization of the pruned networks, we propose two novel\nPrunedConv and PrunedLinear operations. Specifically, these operations mitigate\nthe problem of unstable gradients by regularizing the objective function of the\npruned networks. The proposed search objective enables us to train architecture\nparameters regarding the pruned weight elements. Quantitative analyses\ndemonstrate that our searched architectures outperform those used in the\nstate-of-the-art pruning networks on CIFAR-10 and ImageNet. In terms of\nhardware effectiveness, PR-DARTS increases MobileNet-v2's accuracy from 73.44%\nto 81.35% (+7.91% improvement) and runs 3.87$\\times$ faster.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mousavi_H/0/1/0/all/0/1\">Hamid Mousavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loni_M/0/1/0/all/0/1\">Mohammad Loni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alibeigi_M/0/1/0/all/0/1\">Mina Alibeigi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daneshtalab_M/0/1/0/all/0/1\">Masoud Daneshtalab</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Discriminative Representation via Metric Learning for Imbalanced Medical Image Classification. (arXiv:2207.06975v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06975","description":"<p>Data imbalance between common and rare diseases during model training often\ncauses intelligent diagnosis systems to have biased predictions towards common\ndiseases. The state-of-the-art approaches apply a two-stage learning framework\nto alleviate the class-imbalance issue, where the first stage focuses on\ntraining of a general feature extractor and the second stage focuses on\nfine-tuning the classifier head for class rebalancing. However, existing\ntwo-stage approaches do not consider the fine-grained property between\ndifferent diseases, often causing the first stage less effective for medical\nimage classification than for natural image classification tasks. In this\nstudy, we propose embedding metric learning into the first stage of the\ntwo-stage framework specially to help the feature extractor learn to extract\nmore discriminative feature representations. Extensive experiments mainly on\nthree medical image datasets show that the proposed approach consistently\noutperforms existing onestage and two-stage approaches, suggesting that metric\nlearning can be used as an effective plug-in component in the two-stage\nframework for fine-grained class-imbalanced image classification tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_C/0/1/0/all/0/1\">Chenghua Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Huijuan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kanghao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ruixuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1\">Wei-Shi Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ObjectBox: From Centers to Boxes for Anchor-Free Object Detection. (arXiv:2207.06985v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06985","description":"<p>We present ObjectBox, a novel single-stage anchor-free and highly\ngeneralizable object detection approach. As opposed to both existing\nanchor-based and anchor-free detectors, which are more biased toward specific\nobject scales in their label assignments, we use only object center locations\nas positive samples and treat all objects equally in different feature levels\nregardless of the objects' sizes or shapes. Specifically, our label assignment\nstrategy considers the object center locations as shape- and size-agnostic\nanchors in an anchor-free fashion, and allows learning to occur at all scales\nfor every object. To support this, we define new regression targets as the\ndistances from two corners of the center cell location to the four sides of the\nbounding box. Moreover, to handle scale-variant objects, we propose a tailored\nIoU loss to deal with boxes with different sizes. As a result, our proposed\nobject detector does not need any dataset-dependent hyperparameters to be tuned\nacross datasets. We evaluate our method on MS-COCO 2017 and PASCAL VOC 2012\ndatasets, and compare our results to state-of-the-art methods. We observe that\nObjectBox performs favorably in comparison to prior works. Furthermore, we\nperform rigorous ablation experiments to evaluate different components of our\nmethod. Our code is available at: https://github.com/MohsenZand/ObjectBox.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zand_M/0/1/0/all/0/1\">Mohsen Zand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Etemad_A/0/1/0/all/0/1\">Ali Etemad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Greenspan_M/0/1/0/all/0/1\">Michael Greenspan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tree Structure-Aware Few-Shot Image Classification via Hierarchical Aggregation. (arXiv:2207.06989v1 [cs.CV])","link":"http://arxiv.org/abs/2207.06989","description":"<p>In this paper, we mainly focus on the problem of how to learn additional\nfeature representations for few-shot image classification through pretext tasks\n(e.g., rotation or color permutation and so on). This additional knowledge\ngenerated by pretext tasks can further improve the performance of few-shot\nlearning (FSL) as it differs from human-annotated supervision (i.e., class\nlabels of FSL tasks). To solve this problem, we present a plug-in Hierarchical\nTree Structure-aware (HTS) method, which not only learns the relationship of\nFSL and pretext tasks, but more importantly, can adaptively select and\naggregate feature representations generated by pretext tasks to maximize the\nperformance of FSL tasks. A hierarchical tree constructing component and a\ngated selection aggregating component is introduced to construct the tree\nstructure and find richer transferable knowledge that can rapidly adapt to\nnovel classes with a few labeled images. Extensive experiments show that our\nHTS can significantly enhance multiple few-shot methods to achieve new\nstate-of-the-art performance on four benchmark datasets. The code is available\nat: https://github.com/remiMZ/HTS-ECCV22.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Siteng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenbin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Donglin Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Modelling with Pixels. (arXiv:2207.06991v1 [cs.CL])","link":"http://arxiv.org/abs/2207.06991","description":"<p>Language models are defined over a finite set of inputs, which creates a\nvocabulary bottleneck when we attempt to scale the number of supported\nlanguages. Tackling this bottleneck results in a trade-off between what can be\nrepresented in the embedding matrix and computational issues in the output\nlayer. This paper introduces PIXEL, the Pixel-based Encoder of Language, which\nsuffers from neither of these issues. PIXEL is a pretrained language model that\nrenders text as images, making it possible to transfer representations across\nlanguages based on orthographic similarity or the co-activation of pixels.\nPIXEL is trained to reconstruct the pixels of masked patches, instead of\npredicting a distribution over tokens. We pretrain the 86M parameter PIXEL\nmodel on the same English data as BERT and evaluate on syntactic and semantic\ntasks in typologically diverse languages, including various non-Latin scripts.\nWe find that PIXEL substantially outperforms BERT on syntactic and semantic\nprocessing tasks on scripts that are not found in the pretraining data, but\nPIXEL is slightly weaker than BERT when working with Latin scripts.\nFurthermore, we find that PIXEL is more robust to noisy text inputs than BERT,\nfurther confirming the benefits of modelling language with pixels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rust_P/0/1/0/all/0/1\">Phillip Rust</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lotz_J/0/1/0/all/0/1\">Jonas F. Lotz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bugliarello_E/0/1/0/all/0/1\">Emanuele Bugliarello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salesky_E/0/1/0/all/0/1\">Elizabeth Salesky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lhoneux_M/0/1/0/all/0/1\">Miryam de Lhoneux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elliott_D/0/1/0/all/0/1\">Desmond Elliott</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Accurate Ground-Truth Depth Image Generation via Overfit Training of Point Cloud Registration using Local Frame Sets. (arXiv:2207.07016v1 [cs.CV])","link":"http://arxiv.org/abs/2207.07016","description":"<p>Accurate three-dimensional perception is a fundamental task in several\ncomputer vision applications. Recently, commercial RGB-depth (RGB-D) cameras\nhave been widely adopted as single-view depth-sensing devices owing to their\nefficient depth-sensing abilities. However, the depth quality of most RGB-D\nsensors remains insufficient owing to the inherent noise from a single-view\nenvironment. Recently, several studies have focused on the single-view depth\nenhancement of RGB-D cameras. Recent research has proposed deep-learning-based\napproaches that typically train networks using high-quality supervised depth\ndatasets, which indicates that the quality of the ground-truth (GT) depth\ndataset is a top-most important factor for accurate system; however, such\nhigh-quality GT datasets are difficult to obtain. In this study, we developed a\nnovel method for high-quality GT depth generation based on an RGB-D stream\ndataset. First, we defined consecutive depth frames in a local spatial region\nas a local frame set. Then, the depth frames were aligned to a certain frame in\nthe local frame set using an unsupervised point cloud registration scheme. The\nregistration parameters were trained based on an overfit-training scheme, which\nwas primarily used to construct a single GT depth image for each frame set. The\nfinal GT depth dataset was constructed using several local frame sets, and each\nlocal frame set was trained independently. The primary advantage of this study\nis that a high-quality GT depth dataset can be constructed under various\nscanning environments using only the RGB-D stream dataset. Moreover, our\nproposed method can be used as a new benchmark GT dataset for accurate\nperformance evaluations. We evaluated our GT dataset on previously benchmarked\nGT depth datasets and demonstrated that our method is superior to\nstate-of-the-art depth enhancement frameworks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jiwan Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Minchang Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_Y/0/1/0/all/0/1\">Yeong-Gil Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_M/0/1/0/all/0/1\">Minyoung Chung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MedFuse: Multi-modal fusion with clinical time-series data and chest X-ray images. (arXiv:2207.07027v1 [eess.IV])","link":"http://arxiv.org/abs/2207.07027","description":"<p>Multi-modal fusion approaches aim to integrate information from different\ndata sources. Unlike natural datasets, such as in audio-visual applications,\nwhere samples consist of \"paired\" modalities, data in healthcare is often\ncollected asynchronously. Hence, requiring the presence of all modalities for a\ngiven sample is not realistic for clinical tasks and significantly limits the\nsize of the dataset during training. In this paper, we propose MedFuse, a\nconceptually simple yet promising LSTM-based fusion module that can accommodate\nuni-modal as well as multi-modal input. We evaluate the fusion method and\nintroduce new benchmark results for in-hospital mortality prediction and\nphenotype classification, using clinical time-series data in the MIMIC-IV\ndataset and corresponding chest X-ray images in MIMIC-CXR. Compared to more\ncomplex multi-modal fusion strategies, MedFuse provides a performance\nimprovement by a large margin on the fully paired test set. It also remains\nrobust across the partially paired test set containing samples with missing\nchest X-ray images. We release our code for reproducibility and to enable the\nevaluation of competing models in the future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hayat_N/0/1/0/all/0/1\">Nasir Hayat</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Geras_K/0/1/0/all/0/1\">Krzysztof J. Geras</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shamout_F/0/1/0/all/0/1\">Farah E. Shamout</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Attacks on Monocular Pose Estimation. (arXiv:2207.07032v1 [cs.CV])","link":"http://arxiv.org/abs/2207.07032","description":"<p>Advances in deep learning have resulted in steady progress in computer vision\nwith improved accuracy on tasks such as object detection and semantic\nsegmentation. Nevertheless, deep neural networks are vulnerable to adversarial\nattacks, thus presenting a challenge in reliable deployment. Two of the\nprominent tasks in 3D scene-understanding for robotics and advanced drive\nassistance systems are monocular depth and pose estimation, often learned\ntogether in an unsupervised manner. While studies evaluating the impact of\nadversarial attacks on monocular depth estimation exist, a systematic\ndemonstration and analysis of adversarial perturbations against pose estimation\nare lacking. We show how additive imperceptible perturbations can not only\nchange predictions to increase the trajectory drift but also catastrophically\nalter its geometry. We also study the relation between adversarial\nperturbations targeting monocular depth and pose estimation networks, as well\nas the transferability of perturbations to other networks with different\narchitectures and losses. Our experiments show how the generated perturbations\nlead to notable errors in relative rotation and translation predictions and\nelucidate vulnerabilities of the networks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chawla_H/0/1/0/all/0/1\">Hemang Chawla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varma_A/0/1/0/all/0/1\">Arnav Varma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arani_E/0/1/0/all/0/1\">Elahe Arani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zonooz_B/0/1/0/all/0/1\">Bahram Zonooz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Single Self-Supervised Model for Many Speech Modalities Enables Zero-Shot Modality Transfer. (arXiv:2207.07036v1 [cs.CL])","link":"http://arxiv.org/abs/2207.07036","description":"<p>While audio-visual speech models can yield superior performance and\nrobustness compared to audio-only models, their development and adoption are\nhindered by the lack of labeled and unlabeled audio-visual data and the cost to\ndeploy one model per modality. In this paper, we present u-HuBERT, a\nself-supervised pre-training framework that can leverage both multimodal and\nunimodal speech with a unified masked cluster prediction objective. By\nutilizing modality dropout during pre-training, we demonstrate that a single\nfine-tuned model can achieve performance on par or better than the\nstate-of-the-art modality-specific models. Moreover, our model fine-tuned only\non audio can perform well with audio-visual and visual speech input, achieving\nzero-shot modality generalization for speech recognition and speaker\nverification. In particular, our single model yields 1.2%/1.4%/27.2% speech\nrecognition word error rate on LRS3 with audio-visual/audio/visual input.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Wei-Ning Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1\">Bowen Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Convolutional Bypasses Are Better Vision Transformer Adapters. (arXiv:2207.07039v1 [cs.CV])","link":"http://arxiv.org/abs/2207.07039","description":"<p>The pretrain-then-finetune paradigm has been widely adopted in computer\nvision. But as the size of Vision Transformer (ViT) grows exponentially, the\nfull finetuning becomes prohibitive in view of the heavier storage overhead.\nMotivated by parameter-efficient transfer learning (PETL) on language\ntransformers, recent studies attempt to insert lightweight adaptation modules\n(e.g., adapter layers or prompt tokens) to pretrained ViT and only finetune\nthese modules while the pretrained weights are frozen. However, these modules\nwere originally proposed to finetune language models. Although ported well to\nViT, their design lacks prior knowledge for visual tasks. In this paper, we\npropose to construct Convolutional Bypasses (Convpass) in ViT as adaptation\nmodules, introducing only a small amount (less than 0.5% of model parameters)\nof trainable parameters to adapt the large ViT. Different from other PETL\nmethods, Convpass benefits from the hard-coded inductive bias of convolutional\nlayers and thus is more suitable for visual tasks, especially in the low-data\nregime. Experimental results on VTAB-1k benchmark and few-shot learning\ndatasets demonstrate that Convpass outperforms current language-oriented\nadaptation modules, demonstrating the necessity to tailor vision-oriented\nadaptation modules for vision models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jie_S/0/1/0/all/0/1\">Shibo Jie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1\">Zhi-Hong Deng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-Supervised Temporal Action Detection with Proposal-Free Masking. (arXiv:2207.07059v1 [cs.CV])","link":"http://arxiv.org/abs/2207.07059","description":"<p>Existing temporal action detection (TAD) methods rely on a large number of\ntraining data with segment-level annotations. Collecting and annotating such a\ntraining set is thus highly expensive and unscalable. Semi-supervised TAD\n(SS-TAD) alleviates this problem by leveraging unlabeled videos freely\navailable at scale. However, SS-TAD is also a much more challenging problem\nthan supervised TAD, and consequently much under-studied. Prior SS-TAD methods\ndirectly combine an existing proposal-based TAD method and a SSL method. Due to\ntheir sequential localization (e.g, proposal generation) and classification\ndesign, they are prone to proposal error propagation. To overcome this\nlimitation, in this work we propose a novel Semi-supervised Temporal action\ndetection model based on PropOsal-free Temporal mask (SPOT) with a parallel\nlocalization (mask generation) and classification architecture. Such a novel\ndesign effectively eliminates the dependence between localization and\nclassification by cutting off the route for error propagation in-between. We\nfurther introduce an interaction mechanism between classification and\nlocalization for prediction refinement, and a new pretext task for\nself-supervised model pre-training. Extensive experiments on two standard\nbenchmarks show that our SPOT outperforms state-of-the-art alternatives, often\nby a large margin. The PyTorch implementation of SPOT is available at\nhttps://github.com/sauradip/SPOT\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nag_S/0/1/0/all/0/1\">Sauradip Nag</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiatian Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yi-Zhe Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1\">Tao Xiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Egocentric Scene Understanding via Multimodal Spatial Rectifier. (arXiv:2207.07077v1 [cs.CV])","link":"http://arxiv.org/abs/2207.07077","description":"<p>In this paper, we study a problem of egocentric scene understanding, i.e.,\npredicting depths and surface normals from an egocentric image. Egocentric\nscene understanding poses unprecedented challenges: (1) due to large head\nmovements, the images are taken from non-canonical viewpoints (i.e., tilted\nimages) where existing models of geometry prediction do not apply; (2) dynamic\nforeground objects including hands constitute a large proportion of visual\nscenes. These challenges limit the performance of the existing models learned\nfrom large indoor datasets, such as ScanNet and NYUv2, which comprise\npredominantly upright images of static scenes. We present a multimodal spatial\nrectifier that stabilizes the egocentric images to a set of reference\ndirections, which allows learning a coherent visual representation. Unlike\nunimodal spatial rectifier that often produces excessive perspective warp for\negocentric images, the multimodal spatial rectifier learns from multiple\ndirections that can minimize the impact of the perspective warp. To learn\nvisual representations of the dynamic foreground objects, we present a new\ndataset called EDINA (Egocentric Depth on everyday INdoor Activities) that\ncomprises more than 500K synchronized RGBD frames and gravity directions.\nEquipped with the multimodal spatial rectifier and the EDINA dataset, our\nproposed method on single-view depth and surface normal estimation\nsignificantly outperforms the baselines not only on our EDINA dataset, but also\non other popular egocentric datasets, such as First Person Hand Action (FPHA)\nand EPIC-KITCHENS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Do_T/0/1/0/all/0/1\">Tien Do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vuong_K/0/1/0/all/0/1\">Khiem Vuong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_H/0/1/0/all/0/1\">Hyun Soo Park</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Grand Unification of Object Tracking. (arXiv:2207.07078v1 [cs.CV])","link":"http://arxiv.org/abs/2207.07078","description":"<p>We present a unified method, termed Unicorn, that can simultaneously solve\nfour tracking problems (SOT, MOT, VOS, MOTS) with a single network using the\nsame model parameters. Due to the fragmented definitions of the object tracking\nproblem itself, most existing trackers are developed to address a single or\npart of tasks and overspecialize on the characteristics of specific tasks. By\ncontrast, Unicorn provides a unified solution, adopting the same input,\nbackbone, embedding, and head across all tracking tasks. For the first time, we\naccomplish the great unification of the tracking network architecture and\nlearning paradigm. Unicorn performs on-par or better than its task-specific\ncounterparts in 8 tracking datasets, including LaSOT, TrackingNet, MOT17,\nBDD100K, DAVIS16-17, MOTS20, and BDD100K MOTS. We believe that Unicorn will\nserve as a solid step towards the general vision model. Code is available at\nhttps://github.com/MasterBin-IIAU/Unicorn.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1\">Bin Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_P/0/1/0/all/0/1\">Peize Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zehuan Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Huchuan Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Asymmetric Contrastive Loss for Handling Imbalanced Datasets. (arXiv:2207.07080v1 [cs.LG])","link":"http://arxiv.org/abs/2207.07080","description":"<p>Contrastive learning is a representation learning method performed by\ncontrasting a sample to other similar samples so that they are brought closely\ntogether, forming clusters in the feature space. The learning process is\ntypically conducted using a two-stage training architecture, and it utilizes\nthe contrastive loss (CL) for its feature learning. Contrastive learning has\nbeen shown to be quite successful in handling imbalanced datasets, in which\nsome classes are overrepresented while some others are underrepresented.\nHowever, previous studies have not specifically modified CL for imbalanced\ndatasets. In this work, we introduce an asymmetric version of CL, referred to\nas ACL, in order to directly address the problem of class imbalance. In\naddition, we propose the asymmetric focal contrastive loss (AFCL) as a further\ngeneralization of both ACL and focal contrastive loss (FCL). Results on the\nFMNIST and ISIC 2018 imbalanced datasets show that AFCL is capable of\noutperforming CL and FCL in terms of both weighted and unweighted\nclassification accuracies. In the appendix, we provide a full axiomatic\ntreatment on entropy, along with complete proofs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vito_V/0/1/0/all/0/1\">Valentino Vito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stefanus_L/0/1/0/all/0/1\">Lim Yohanes Stefanus</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Personalized Zero-Shot ECG Arrhythmia Monitoring System: From Sparse Representation Based Domain Adaption to Energy Efficient Abnormal Beat Detection for Practical ECG Surveillance. (arXiv:2207.07089v1 [cs.LG])","link":"http://arxiv.org/abs/2207.07089","description":"<p>This paper proposes a low-cost and highly accurate ECG-monitoring system\nintended for personalized early arrhythmia detection for wearable mobile\nsensors. Earlier supervised approaches for personalized ECG monitoring require\nboth abnormal and normal heartbeats for the training of the dedicated\nclassifier. However, in a real-world scenario where the personalized algorithm\nis embedded in a wearable device, such training data is not available for\nhealthy people with no cardiac disorder history. In this study, (i) we propose\na null space analysis on the healthy signal space obtained via sparse\ndictionary learning, and investigate how a simple null space projection or\nalternatively regularized least squares-based classification methods can reduce\nthe computational complexity, without sacrificing the detection accuracy, when\ncompared to sparse representation-based classification. (ii) Then we introduce\na sparse representation-based domain adaptation technique in order to project\nother existing users' abnormal and normal signals onto the new user's signal\nspace, enabling us to train the dedicated classifier without having any\nabnormal heartbeat of the new user. Therefore, zero-shot learning can be\nachieved without the need for synthetic abnormal heartbeat generation. An\nextensive set of experiments performed on the benchmark MIT-BIH ECG dataset\nshows that when this domain adaptation-based training data generator is used\nwith a simple 1-D CNN classifier, the method outperforms the prior work by a\nsignificant margin. (iii) Then, by combining (i) and (ii), we propose an\nensemble classifier that further improves the performance. This approach for\nzero-shot arrhythmia detection achieves an average accuracy level of 98.2% and\nan F1-Score of 92.8%. Finally, a personalized energy-efficient ECG monitoring\nscheme is proposed using the above-mentioned innovations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yamac_M/0/1/0/all/0/1\">Mehmet Yama&#xe7;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duman_M/0/1/0/all/0/1\">Mert Duman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adalioglu_I/0/1/0/all/0/1\">&#x130;lke Adal&#x131;o&#x11f;lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiranyaz_S/0/1/0/all/0/1\">Serkan Kiranyaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gabbouj_M/0/1/0/all/0/1\">Moncef Gabbouj</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explaining Image Enhancement Black-Box Methods through a Path Planning Based Algorithm. (arXiv:2207.07092v1 [cs.CV])","link":"http://arxiv.org/abs/2207.07092","description":"<p>Nowadays, image-to-image translation methods, are the state of the art for\nthe enhancement of natural images. Even if they usually show high performance\nin terms of accuracy, they often suffer from several limitations such as the\ngeneration of artifacts and the scalability to high resolutions. Moreover,\ntheir main drawback is the completely black-box approach that does not allow to\nprovide the final user with any insight about the enhancement processes\napplied. In this paper we present a path planning algorithm which provides a\nstep-by-step explanation of the output produced by state of the art enhancement\nmethods, overcoming black-box limitation. This algorithm, called eXIE, uses a\nvariant of the A* algorithm to emulate the enhancement process of another\nmethod through the application of an equivalent sequence of enhancing\noperators. We applied eXIE to explain the output of several state-of-the-art\nmodels trained on the Five-K dataset, obtaining sequences of enhancing\noperators able to produce very similar results in terms of performance and\novercoming the huge limitation of poor interpretability of the best performing\nalgorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cotogni_M/0/1/0/all/0/1\">Marco Cotogni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cusano_C/0/1/0/all/0/1\">Claudio Cusano</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ReAct: Temporal Action Detection with Relational Queries. (arXiv:2207.07097v1 [cs.CV])","link":"http://arxiv.org/abs/2207.07097","description":"<p>This work aims at advancing temporal action detection (TAD) using an\nencoder-decoder framework with action queries, similar to DETR, which has shown\ngreat success in object detection. However, the framework suffers from several\nproblems if directly applied to TAD: the insufficient exploration of\ninter-query relation in the decoder, the inadequate classification training due\nto a limited number of training samples, and the unreliable classification\nscores at inference. To this end, we first propose a relational attention\nmechanism in the decoder, which guides the attention among queries based on\ntheir relations. Moreover, we propose two losses to facilitate and stabilize\nthe training of action classification. Lastly, we propose to predict the\nlocalization quality of each action query at inference in order to distinguish\nhigh-quality queries. The proposed method, named ReAct, achieves the\nstate-of-the-art performance on THUMOS14, with much lower computational costs\nthan previous methods. Besides, extensive ablation studies are conducted to\nverify the effectiveness of each proposed component. The code is available at\nhttps://github.com/sssste/React.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_D/0/1/0/all/0/1\">Dingfeng Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1\">Yujie Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Q/0/1/0/all/0/1\">Qiong Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Relighting4D: Neural Relightable Human from Videos. (arXiv:2207.07104v1 [cs.CV])","link":"http://arxiv.org/abs/2207.07104","description":"<p>Human relighting is a highly desirable yet challenging task. Existing works\neither require expensive one-light-at-a-time (OLAT) captured data using light\nstage or cannot freely change the viewpoints of the rendered body. In this\nwork, we propose a principled framework, Relighting4D, that enables\nfree-viewpoints relighting from only human videos under unknown illuminations.\nOur key insight is that the space-time varying geometry and reflectance of the\nhuman body can be decomposed as a set of neural fields of normal, occlusion,\ndiffuse, and specular maps. These neural fields are further integrated into\nreflectance-aware physically based rendering, where each vertex in the neural\nfield absorbs and reflects the light from the environment. The whole framework\ncan be learned from videos in a self-supervised manner, with physically\ninformed priors designed for regularization. Extensive experiments on both real\nand synthetic datasets demonstrate that our framework is capable of relighting\ndynamic human actors with free-viewpoints.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhaoxi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Benchmarking Omni-Vision Representation through the Lens of Visual Realms. (arXiv:2207.07106v1 [cs.CV])","link":"http://arxiv.org/abs/2207.07106","description":"<p>Though impressive performance has been achieved in specific visual realms\n(e.g. faces, dogs, and places), an omni-vision representation generalizing to\nmany natural visual domains is highly desirable. But, existing benchmarks are\nbiased and inefficient to evaluate the omni-vision representation -- these\nbenchmarks either only include several specific realms, or cover most realms at\nthe expense of subsuming numerous datasets that have extensive realm\noverlapping. In this paper, we propose Omni-Realm Benchmark (OmniBenchmark). It\nincludes 21 realm-wise datasets with 7,372 concepts and 1,074,346 images.\nWithout semantic overlapping, these datasets cover most visual realms\ncomprehensively and meanwhile efficiently. In addition, we propose a new\nsupervised contrastive learning framework, namely Relational Contrastive\nlearning (ReCo), for a better omni-vision representation. Beyond pulling two\ninstances from the same concept closer -- the typical supervised contrastive\nlearning framework -- ReCo also pulls two instances from the same semantic\nrealm closer, encoding the semantic relation between concepts, and facilitating\nomni-vision representation learning. We benchmark ReCo and other advances in\nomni-vision representation studies that are different in architectures (from\nCNNs to transformers) and in learning paradigms (from supervised learning to\nself-supervised learning) on OmniBenchmark. We illustrate the superior of ReCo\nto other supervised contrastive learning methods and reveal multiple practical\nobservations to facilitate future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuanhan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1\">Zhenfei Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1\">Jing Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-grained Few-shot Recognition by Deep Object Parsing. (arXiv:2207.07110v1 [cs.CV])","link":"http://arxiv.org/abs/2207.07110","description":"<p>In our framework, an object is made up of K distinct parts or units, and we\nparse a test instance by inferring the K parts, where each part occupies a\ndistinct location in the feature space, and the instance features at this\nlocation, manifest as an active subset of part templates shared across all\ninstances. We recognize test instances by comparing its active templates and\nthe relative geometry of its part locations against those of the presented\nfew-shot instances. We propose an end-to-end training method to learn part\ntemplates on-top of a convolutional backbone. To combat visual distortions such\nas orientation, pose and size, we learn multi-scale templates, and at test-time\nparse and match instances across these scales. We show that our method is\ncompetitive with the state-of-the-art, and by virtue of parsing enjoys\ninterpretability as well.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_P/0/1/0/all/0/1\">Pengkai Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1\">Ruizhao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Samarth Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saligrama_V/0/1/0/all/0/1\">Venkatesh Saligrama</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"XMem: Long-Term Video Object Segmentation with an Atkinson-Shiffrin Memory Model. (arXiv:2207.07115v1 [cs.CV])","link":"http://arxiv.org/abs/2207.07115","description":"<p>We present XMem, a video object segmentation architecture for long videos\nwith unified feature memory stores inspired by the Atkinson-Shiffrin memory\nmodel. Prior work on video object segmentation typically only uses one type of\nfeature memory. For videos longer than a minute, a single feature memory model\ntightly links memory consumption and accuracy. In contrast, following the\nAtkinson-Shiffrin model, we develop an architecture that incorporates multiple\nindependent yet deeply-connected feature memory stores: a rapidly updated\nsensory memory, a high-resolution working memory, and a compact thus sustained\nlong-term memory. Crucially, we develop a memory potentiation algorithm that\nroutinely consolidates actively used working memory elements into the long-term\nmemory, which avoids memory explosion and minimizes performance decay for\nlong-term prediction. Combined with a new memory reading mechanism, XMem\ngreatly exceeds state-of-the-art performance on long-video datasets while being\non par with state-of-the-art methods (that do not work on long videos) on\nshort-video datasets. Code is available at https://hkchengrex.github.io/XMem\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Ho Kei Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwing_A/0/1/0/all/0/1\">Alexander G. Schwing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bootstrapped Masked Autoencoders for Vision BERT Pretraining. (arXiv:2207.07116v1 [cs.CV])","link":"http://arxiv.org/abs/2207.07116","description":"<p>We propose bootstrapped masked autoencoders (BootMAE), a new approach for\nvision BERT pretraining. BootMAE improves the original masked autoencoders\n(MAE) with two core designs: 1) momentum encoder that provides online feature\nas extra BERT prediction targets; 2) target-aware decoder that tries to reduce\nthe pressure on the encoder to memorize target-specific information in BERT\npretraining. The first design is motivated by the observation that using a\npretrained MAE to extract the features as the BERT prediction target for masked\ntokens can achieve better pretraining performance. Therefore, we add a momentum\nencoder in parallel with the original MAE encoder, which bootstraps the\npretraining performance by using its own representation as the BERT prediction\ntarget. In the second design, we introduce target-specific information (e.g.,\npixel values of unmasked patches) from the encoder directly to the decoder to\nreduce the pressure on the encoder of memorizing the target-specific\ninformation. Thus, the encoder focuses on semantic modeling, which is the goal\nof BERT pretraining, and does not need to waste its capacity in memorizing the\ninformation of unmasked tokens related to the prediction target. Through\nextensive experiments, our BootMAE achieves $84.2\\%$ Top-1 accuracy on\nImageNet-1K with ViT-B backbone, outperforming MAE by $+0.8\\%$ under the same\npre-training epochs. BootMAE also gets $+1.0$ mIoU improvements on semantic\nsegmentation on ADE20K and $+1.3$ box AP, $+1.4$ mask AP improvement on object\ndetection and segmentation on COCO dataset. Code is released at\nhttps://github.com/LightDXY/BootMAE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xiaoyi Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Jianmin Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Ting Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dongdong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weiming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_F/0/1/0/all/0/1\">Fang Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_N/0/1/0/all/0/1\">Nenghai Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Novel Implementation of Machine Learning for the Efficient, Explainable Diagnosis of COVID-19 from Chest CT. (arXiv:2207.07117v1 [eess.IV])","link":"http://arxiv.org/abs/2207.07117","description":"<p>In a worldwide health crisis as exigent as COVID-19, there has become a\npressing need for rapid, reliable diagnostics. Currently, popular testing\nmethods such as reverse transcription polymerase chain reaction (RT-PCR) can\nhave high false negative rates. Consequently, COVID-19 patients are not\naccurately identified nor treated quickly enough to prevent transmission of the\nvirus. However, the recent rise of medical CT data has presented promising\navenues, since CT manifestations contain key characteristics indicative of\nCOVID-19. This study aimed to take a novel approach in the machine\nlearning-based detection of COVID-19 from chest CT scans. First, the dataset\nutilized in this study was derived from three major sources, comprising a total\nof 17,698 chest CT slices across 923 patient cases. Image preprocessing\nalgorithms were then developed to reduce noise by excluding irrelevant\nfeatures. Transfer learning was also implemented with the EfficientNetB7\npre-trained model to provide a backbone architecture and save computational\nresources. Lastly, several explainability techniques were leveraged to\nqualitatively validate model performance by localizing infected regions and\nhighlighting fine-grained pixel details. The proposed model attained an overall\naccuracy of 0.927 and a sensitivity of 0.958. Explainability measures showed\nthat the model correctly distinguished between relevant, critical features\npertaining to COVID-19 chest CT images and normal controls. Deep learning\nframeworks provide efficient, human-interpretable COVID-19 diagnostics that\ncould complement radiologist decisions or serve as an alternative screening\ntool. Future endeavors may provide insight into infection severity, patient\nrisk stratification, and prognosis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Justin Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Deep Epipolar Flow for Stationary or Dynamic Scenes. (arXiv:1904.03848v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1904.03848","description":"<p>Unsupervised deep learning for optical flow computation has achieved\npromising results. Most existing deep-net based methods rely on image\nbrightness consistency and local smoothness constraint to train the networks.\nTheir performance degrades at regions where repetitive textures or occlusions\noccur. In this paper, we propose Deep Epipolar Flow, an unsupervised optical\nflow method which incorporates global geometric constraints into network\nlearning. In particular, we investigate multiple ways of enforcing the epipolar\nconstraint in flow estimation. To alleviate a \"chicken-and-egg\" type of problem\nencountered in dynamic scenes where multiple motions may be present, we propose\na low-rank constraint as well as a union-of-subspaces constraint for training.\nExperimental results on various benchmarking datasets show that our method\nachieves competitive performance compared with supervised methods and\noutperforms state-of-the-art unsupervised deep-learning methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1\">Yiran Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_P/0/1/0/all/0/1\">Pan Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1\">Yuchao Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongdong Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Surface Normal Estimation of Tilted Images via Spatial Rectifier. (arXiv:2007.09264v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2007.09264","description":"<p>In this paper, we present a spatial rectifier to estimate surface normals of\ntilted images. Tilted images are of particular interest as more visual data are\ncaptured by arbitrarily oriented sensors such as body-/robot-mounted cameras.\nExisting approaches exhibit bounded performance on predicting surface normals\nbecause they were trained using gravity-aligned images. Our two main hypotheses\nare: (1) visual scene layout is indicative of the gravity direction; and (2)\nnot all surfaces are equally represented by a learned estimator due to the\nstructured distribution of the training data, thus, there exists a\ntransformation for each tilted image that is more responsive to the learned\nestimator than others. We design a spatial rectifier that is learned to\ntransform the surface normal distribution of a tilted image to the rectified\none that matches the gravity-aligned training data distribution. Along with the\nspatial rectifier, we propose a novel truncated angular loss that offers a\nstronger gradient at smaller angular errors and robustness to outliers. The\nresulting estimator outperforms the state-of-the-art methods including data\naugmentation baselines not only on ScanNet and NYUv2 but also on a new dataset\ncalled Tilt-RGBD that includes considerable roll and pitch camera motion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Do_T/0/1/0/all/0/1\">Tien Do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vuong_K/0/1/0/all/0/1\">Khiem Vuong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roumeliotis_S/0/1/0/all/0/1\">Stergios I. Roumeliotis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_H/0/1/0/all/0/1\">Hyun Soo Park</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RGB-D Salient Object Detection: A Survey. (arXiv:2008.00230v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2008.00230","description":"<p>Salient object detection (SOD), which simulates the human visual perception\nsystem to locate the most attractive object(s) in a scene, has been widely\napplied to various computer vision tasks. Now, with the advent of depth\nsensors, depth maps with affluent spatial information that can be beneficial in\nboosting the performance of SOD, can easily be captured. Although various RGB-D\nbased SOD models with promising performance have been proposed over the past\nseveral years, an in-depth understanding of these models and challenges in this\ntopic remains lacking. In this paper, we provide a comprehensive survey of\nRGB-D based SOD models from various perspectives, and review related benchmark\ndatasets in detail. Further, considering that the light field can also provide\ndepth maps, we review SOD models and popular benchmark datasets from this\ndomain as well. Moreover, to investigate the SOD ability of existing models, we\ncarry out a comprehensive evaluation, as well as attribute-based evaluation of\nseveral representative RGB-D based SOD models. Finally, we discuss several\nchallenges and open directions of RGB-D based SOD for future research. All\ncollected models, benchmark datasets, source code links, datasets constructed\nfor attribute-based evaluation, and codes for evaluation will be made publicly\navailable at https://github.com/taozh2017/RGBDSODsurvey\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1\">Tao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1\">Deng-Ping Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1\">Ming-Ming Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jianbing Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BANet: Blur-aware Attention Networks for Dynamic Scene Deblurring. (arXiv:2101.07518v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.07518","description":"<p>Image motion blur usually results from moving objects or camera shakes. Such\nblur is generally directional and non-uniform. Previous research efforts\nattempt to solve non-uniform blur by using self-recurrent multi-scale or\nmulti-patch architectures accompanying with self-attention. However, using\nself-recurrent frameworks typically leads to a longer inference time, while\ninter-pixel or inter-channel self-attention may cause excessive memory usage.\nThis paper proposes blur-aware attention networks (BANet) that accomplish\naccurate and efficient deblurring via a single forward pass. Our BANet utilizes\nregion-based self-attention with multi-kernel strip pooling to disentangle blur\npatterns of different degrees and with cascaded parallel dilated convolution to\naggregate multi-scale content features. Extensive experimental results on the\nGoPro and HIDE benchmarks demonstrate that the proposed BANet performs\nfavorably against the state-of-the-art in blurred image restoration and can\nprovide deblurred results in real-time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tsai_F/0/1/0/all/0/1\">Fu-Jen Tsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yan-Tsung Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yen-Yu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsai_C/0/1/0/all/0/1\">Chung-Chi Tsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chia-Wen Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Blurs Behave Like Ensembles: Spatial Smoothings to Improve Accuracy, Uncertainty, and Robustness. (arXiv:2105.12639v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2105.12639","description":"<p>Neural network ensembles, such as Bayesian neural networks (BNNs), have shown\nsuccess in the areas of uncertainty estimation and robustness. However, a\ncrucial challenge prohibits their use in practice. BNNs require a large number\nof predictions to produce reliable results, leading to a significant increase\nin computational cost. To alleviate this issue, we propose spatial smoothing, a\nmethod that spatially ensembles neighboring feature map points of convolutional\nneural networks. By simply adding a few blur layers to the models, we\nempirically show that spatial smoothing improves accuracy, uncertainty\nestimation, and robustness of BNNs across a whole range of ensemble sizes. In\nparticular, BNNs incorporating spatial smoothing achieve high predictive\nperformance merely with a handful of ensembles. Moreover, this method also can\nbe applied to canonical deterministic neural networks to improve the\nperformances. A number of evidences suggest that the improvements can be\nattributed to the stabilized feature maps and the smoothing of the loss\nlandscape. In addition, we provide a fundamental explanation for prior works -\nnamely, global average pooling, pre-activation, and ReLU6 - by addressing them\nas special cases of spatial smoothing. These not only enhance accuracy, but\nalso improve uncertainty estimation and robustness by making the loss landscape\nsmoother in the same manner as spatial smoothing. The code is available at\nhttps://github.com/xxxnell/spatial-smoothing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_N/0/1/0/all/0/1\">Namuk Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Songkuk Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continual Contrastive Learning for Image Classification. (arXiv:2107.01776v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.01776","description":"<p>For artificial learning systems, continual learning over time from a stream\nof data is essential. The burgeoning studies on supervised continual learning\nhave achieved great progress, while the study of catastrophic forgetting in\nunsupervised learning is still blank. Among unsupervised learning methods,\nself-supervise learning method shows tremendous potential on visual\nrepresentation without any labeled data at scale. To improve the visual\nrepresentation of self-supervised learning, larger and more varied data is\nneeded. In the real world, unlabeled data is generated at all times. This\ncircumstance provides a huge advantage for the learning of the self-supervised\nmethod. However, in the current paradigm, packing previous data and current\ndata together and training it again is a waste of time and resources. Thus, a\ncontinual self-supervised learning method is badly needed. In this paper, we\nmake the first attempt to implement the continual contrastive self-supervised\nlearning by proposing a rehearsal method, which keeps a few exemplars from the\nprevious data. Instead of directly combining saved exemplars with the current\ndata set for training, we leverage self-supervised knowledge distillation to\ntransfer contrastive information among previous data to the current network by\nmimicking similarity score distribution inferred by the old network over a set\nof saved exemplars. Moreover, we build an extra sample queue to assist the\nnetwork to distinguish between previous and current data and prevent mutual\ninterference while learning their own feature representation. Experimental\nresults show that our method performs well on CIFAR100 and ImageNet-Sub.\nCompared with the baselines, which learning tasks without taking any technique,\nwe improve the image classification top-1 accuracy by 1.60% on CIFAR100, 2.86%\non ImageNet-Sub and 1.29% on ImageNet-Full under 10 incremental steps setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhiwei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongtao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hongxiang Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FOCUS: Familiar Objects in Common and Uncommon Settings. (arXiv:2110.03804v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.03804","description":"<p>Standard training datasets for deep learning often contain objects in common\nsettings (e.g., \"a horse on grass\" or \"a ship in water\") since they are usually\ncollected by randomly scraping the web. Uncommon and rare settings (e.g., \"a\nplane on water\", \"a car in snowy weather\") are thus severely under-represented\nin the training data. This can lead to an undesirable bias in model predictions\ntowards common settings and create a false sense of accuracy. In this paper, we\nintroduce FOCUS (Familiar Objects in Common and Uncommon Settings), a dataset\nfor stress-testing the generalization power of deep image classifiers. By\nleveraging the power of modern search engines, we deliberately gather data\ncontaining objects in common and uncommon settings in a wide range of\nlocations, weather conditions, and time of day. We present a detailed analysis\nof the performance of various popular image classifiers on our dataset and\ndemonstrate a clear drop in performance when classifying images in uncommon\nsettings. By analyzing deep features of these models, we show that such errors\ncan be due to the use of spurious features in model predictions. We believe\nthat our dataset will aid researchers in understanding the inability of deep\nmodels to generalize well to uncommon settings and drive future work on\nimproving their distributional robustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kattakinda_P/0/1/0/all/0/1\">Priyatham Kattakinda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feizi_S/0/1/0/all/0/1\">Soheil Feizi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Wide Neural Networks Forget Less Catastrophically. (arXiv:2110.11526v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.11526","description":"<p>A primary focus area in continual learning research is alleviating the\n\"catastrophic forgetting\" problem in neural networks by designing new\nalgorithms that are more robust to the distribution shifts. While the recent\nprogress in continual learning literature is encouraging, our understanding of\nwhat properties of neural networks contribute to catastrophic forgetting is\nstill limited. To address this, instead of focusing on continual learning\nalgorithms, in this work, we focus on the model itself and study the impact of\n\"width\" of the neural network architecture on catastrophic forgetting, and show\nthat width has a surprisingly significant effect on forgetting. To explain this\neffect, we study the learning dynamics of the network from various perspectives\nsuch as gradient orthogonality, sparsity, and lazy training regime. We provide\npotential explanations that are consistent with the empirical results across\ndifferent architectures and continual learning benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mirzadeh_S/0/1/0/all/0/1\">Seyed Iman Mirzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhry_A/0/1/0/all/0/1\">Arslan Chaudhry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1\">Dong Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Huiyi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pascanu_R/0/1/0/all/0/1\">Razvan Pascanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gorur_D/0/1/0/all/0/1\">Dilan Gorur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farajtabar_M/0/1/0/all/0/1\">Mehrdad Farajtabar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Class-agnostic Object Detection with Multi-modal Transformer. (arXiv:2111.11430v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.11430","description":"<p>What constitutes an object? This has been a long-standing question in\ncomputer vision. Towards this goal, numerous learning-free and learning-based\napproaches have been developed to score objectness. However, they generally do\nnot scale well across new domains and novel objects. In this paper, we advocate\nthat existing methods lack a top-down supervision signal governed by\nhuman-understandable semantics. For the first time in literature, we\ndemonstrate that Multi-modal Vision Transformers (MViT) trained with aligned\nimage-text pairs can effectively bridge this gap. Our extensive experiments\nacross various domains and novel objects show the state-of-the-art performance\nof MViTs to localize generic objects in images. Based on the observation that\nexisting MViTs do not include multi-scale feature processing and usually\nrequire longer training schedules, we develop an efficient MViT architecture\nusing multi-scale deformable attention and late vision-language fusion. We show\nthe significance of MViT proposals in a diverse range of applications including\nopen-world object detection, salient and camouflage object detection,\nsupervised and self-supervised detection tasks. Further, MViTs can adaptively\ngenerate proposals given a specific language query and thus offer enhanced\ninteractability. Code: \\url{https://git.io/J1HPY}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maaz_M/0/1/0/all/0/1\">Muhammad Maaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rasheed_H/0/1/0/all/0/1\">Hanoona Rasheed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1\">Salman Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1\">Fahad Shahbaz Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anwer_R/0/1/0/all/0/1\">Rao Muhammad Anwer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Ming-Hsuan Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Graph Modularity: Towards Understanding the Cross-Layer Transition of Feature Representations in Deep Neural Networks. (arXiv:2111.12485v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.12485","description":"<p>There are good arguments to support the claim that deep neural networks\n(DNNs) capture better feature representations than the previous hand-crafted\nfeature engineering, which leads to a significant performance improvement. In\nthis paper, we move a tiny step towards understanding the dynamics of feature\nrepresentations over layers. Specifically, we model the process of class\nseparation of intermediate representations in pre-trained DNNs as the evolution\nof communities in dynamic graphs. Then, we introduce modularity, a generic\nmetric in graph theory, to quantify the evolution of communities. In the\npreliminary experiment, we find that modularity roughly tends to increase as\nthe layer goes deeper and the degradation and plateau arise when the model\ncomplexity is great relative to the dataset. Through an asymptotic analysis, we\nprove that modularity can be broadly used for different applications. For\nexample, modularity provides new insights to quantify the difference between\nfeature representations. More crucially, we demonstrate that the degradation\nand plateau in modularity curves represent redundant layers in DNNs and can be\npruned with minimal impact on performance, which provides theoretical guidance\nfor layer pruning. Our code is available at\nhttps://github.com/yaolu-zjut/Dynamic-Graphs-Construction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yunzhe Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zuohui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jinyin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xuan_Q/0/1/0/all/0/1\">Qi Xuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaoniu Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lightweight Attentional Feature Fusion: A New Baseline for Text-to-Video Retrieval. (arXiv:2112.01832v2 [cs.MM] UPDATED)","link":"http://arxiv.org/abs/2112.01832","description":"<p>In this paper we revisit \\emph{feature fusion}, an old-fashioned topic, in\nthe new context of text-to-video retrieval. Different from previous research\nthat considers feature fusion only at one end, let it be video or text, we aim\nfor feature fusion for both ends within a unified framework. We hypothesize\nthat optimizing the convex combination of the features is preferred to modeling\ntheir correlations by computationally heavy multi-head self attention. We\npropose Lightweight Attentional Feature Fusion (LAFF). LAFF performs feature\nfusion at both early and late stages and at both video and text ends, making it\na powerful method for exploiting diverse (off-the-shelf) features. The\ninterpretability of LAFF can be used for feature selection. Extensive\nexperiments on five public benchmark sets (MSR-VTT, MSVD, TGIF, VATEX and\nTRECVID AVS 2016-2020) justify LAFF as a new baseline for text-to-video\nretrieval.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_F/0/1/0/all/0/1\">Fan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1\">Aozhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Ziyue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_F/0/1/0/all/0/1\">Fangming Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Jianfeng Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xirong Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data-Free Neural Architecture Search via Recursive Label Calibration. (arXiv:2112.02086v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2112.02086","description":"<p>This paper aims to explore the feasibility of neural architecture search\n(NAS) given only a pre-trained model without using any original training data.\nThis is an important circumstance for privacy protection, bias avoidance, etc.,\nin real-world scenarios. To achieve this, we start by synthesizing usable data\nthrough recovering the knowledge from a pre-trained deep neural network. Then\nwe use the synthesized data and their predicted soft-labels to guide neural\narchitecture search. We identify that the NAS task requires the synthesized\ndata (we target at image domain here) with enough semantics, diversity, and a\nminimal domain gap from the natural images. For semantics, we propose recursive\nlabel calibration to produce more informative outputs. For diversity, we\npropose a regional update strategy to generate more diverse and\nsemantically-enriched synthetic data. For minimal domain gap, we use input and\nfeature-level regularization to mimic the original data distribution in latent\nspace. We instantiate our proposed framework with three popular NAS algorithms:\nDARTS, ProxylessNAS and SPOS. Surprisingly, our results demonstrate that the\narchitectures discovered by searching with our synthetic data achieve accuracy\nthat is comparable to, or even higher than, architectures discovered by\nsearching from the original ones, for the first time, deriving the conclusion\nthat NAS can be done effectively with no need of access to the original or\ncalled natural data if the synthesis method is well designed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zechun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zhiqiang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_Y/0/1/0/all/0/1\">Yun Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1\">Eric Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_K/0/1/0/all/0/1\">Kwang-Ting Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leichner_C/0/1/0/all/0/1\">Chas Leichner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Feature Interpolation for Low-Shot Image Generation. (arXiv:2112.02450v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.02450","description":"<p>Training of generative models especially Generative Adversarial Networks can\neasily diverge in low-data setting. To mitigate this issue, we propose a novel\nimplicit data augmentation approach which facilitates stable training and\nsynthesize high-quality samples without need of label information.\nSpecifically, we view the discriminator as a metric embedding of the real data\nmanifold, which offers proper distances between real data points. We then\nutilize information in the feature space to develop a fully unsupervised and\ndata-driven augmentation method. Experiments on few-shot generation tasks show\nthe proposed method significantly improve results from strong baselines with\nhundreds of training samples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dai_M/0/1/0/all/0/1\">Mengyu Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hang_H/0/1/0/all/0/1\">Haibin Hang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xiaoyang Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pose2Room: Understanding 3D Scenes from Human Activities. (arXiv:2112.03030v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2112.03030","description":"<p>With wearable IMU sensors, one can estimate human poses from wearable devices\nwithout requiring visual input~\\cite{von2017sparse}. In this work, we pose the\nquestion: Can we reason about object structure in real-world environments\nsolely from human trajectory information? Crucially, we observe that human\nmotion and interactions tend to give strong information about the objects in a\nscene -- for instance a person sitting indicates the likely presence of a chair\nor sofa. To this end, we propose P2R-Net to learn a probabilistic 3D model of\nthe objects in a scene characterized by their class categories and oriented 3D\nbounding boxes, based on an input observed human trajectory in the environment.\nP2R-Net models the probability distribution of object class as well as a deep\nGaussian mixture model for object boxes, enabling sampling of multiple,\ndiverse, likely modes of object configurations from an observed human\ntrajectory. In our experiments we show that P2R-Net can effectively learn\nmulti-modal distributions of likely objects for human motions, and produce a\nvariety of plausible object structures of the environment, even without any\nvisual information. The results demonstrate that P2R-Net consistently\noutperforms the baselines on the PROX dataset and the VirtualHome platform.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nie_Y/0/1/0/all/0/1\">Yinyu Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1\">Angela Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xiaoguang Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niessner_M/0/1/0/all/0/1\">Matthias Nie&#xdf;ner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep learning for brain metastasis detection and segmentation in longitudinal MRI data. (arXiv:2112.11833v4 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2112.11833","description":"<p>Brain metastases occur frequently in patients with metastatic cancer. Early\nand accurate detection of brain metastases is very essential for treatment\nplanning and prognosis in radiation therapy. To improve brain metastasis\ndetection performance with deep learning, a custom detection loss called\nvolume-level sensitivity-specificity (VSS) is proposed, which rates individual\nmetastasis detection sensitivity and specificity in (sub-)volume levels. As\nsensitivity and precision are always a trade-off in a metastasis level, either\na high sensitivity or a high precision can be achieved by adjusting the weights\nin the VSS loss without decline in dice score coefficient for segmented\nmetastases. To reduce metastasis-like structures being detected as false\npositive metastases, a temporal prior volume is proposed as an additional input\nof DeepMedic. The modified network is called DeepMedic+ for distinction. Our\nproposed VSS loss improves the sensitivity of brain metastasis detection for\nDeepMedic, increasing the sensitivity from 85.3% to 97.5%. Alternatively, it\nimproves the precision from 69.1% to 98.7%. Comparing DeepMedic+ with DeepMedic\nwith the same VSS loss, 44.4% of the false positive metastases are reduced in\nthe high sensitivity model and the precision reaches 99.6% for the high\nspecificity model. The mean dice coefficient for all metastases is about 0.81.\nWith the ensemble of the high sensitivity and high specificity models, on\naverage only 1.5 false positive metastases per patient needs further check,\nwhile the majority of true positive metastases are confirmed. The ensemble\nlearning is able to distinguish high confidence true positive metastases from\nmetastases candidates that require special expert review or further follow-up,\nbeing particularly well-fit to the requirements of expert support in real\nclinical practice.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1\">Yixing Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bert_C/0/1/0/all/0/1\">Christoph Bert</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sommer_P/0/1/0/all/0/1\">Philipp Sommer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Frey_B/0/1/0/all/0/1\">Benjamin Frey</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gaipl_U/0/1/0/all/0/1\">Udo Gaipl</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Distel_L/0/1/0/all/0/1\">Luitpold V. Distel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Weissmann_T/0/1/0/all/0/1\">Thomas Weissmann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Uder_M/0/1/0/all/0/1\">Michael Uder</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schmidt_M/0/1/0/all/0/1\">Manuel A. Schmidt</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dorfler_A/0/1/0/all/0/1\">Arnd D&#xf6;rfler</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maier_A/0/1/0/all/0/1\">Andreas Maier</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fietkau_R/0/1/0/all/0/1\">Rainer Fietkau</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Putz_F/0/1/0/all/0/1\">Florian Putz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving the Behaviour of Vision Transformers with Token-consistent Stochastic Layers. (arXiv:2112.15111v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.15111","description":"<p>We introduce token-consistent stochastic layers in vision transformers,\nwithout causing any severe drop in performance. The added stochasticity\nimproves network calibration, robustness and strengthens privacy. We use linear\nlayers with token-consistent stochastic parameters inside the multilayer\nperceptron blocks, without altering the architecture of the transformer. The\nstochastic parameters are sampled from the uniform distribution, both during\ntraining and inference. The applied linear operations preserve the topological\nstructure, formed by the set of tokens passing through the shared multilayer\nperceptron. This operation encourages the learning of the recognition task to\nrely on the topological structures of the tokens, instead of their values,\nwhich in turn offers the desired robustness and privacy of the visual features.\nThe effectiveness of the token-consistent stochasticity is demonstrated on\nthree different applications, namely, network calibration, adversarial\nrobustness, and feature privacy, by boosting the performance of the respective\nestablished baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Popovic_N/0/1/0/all/0/1\">Nikola Popovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paudel_D/0/1/0/all/0/1\">Danda Pani Paudel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Probst_T/0/1/0/all/0/1\">Thomas Probst</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Iterative training of robust k-space interpolation networks for improved image reconstruction with limited scan specific training samples. (arXiv:2201.03560v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2201.03560","description":"<p>Purpose: To evaluate an iterative learning approach for enhanced performance\nof Robust Artificial-neural-networks for K-space Interpolation (RAKI), when\nonly a limited amount of training data (auto-calibration signals, ACS) are\navailable for accelerated standard 2D imaging. Methods: In a first step, the\nRAKI model was optimized for the case of strongly limited training data amount.\nIn the iterative learning approach (termed iterative RAKI), the optimized RAKI\nmodel is initially trained using original and augmented ACS obtained from a\nlinear parallel imaging reconstruction. Subsequently, the RAKI convolution\nfilters are refined iteratively using original and augmented ACS extracted from\nthe previous RAKI reconstruction. Evaluation was carried out on 200\nretrospectively undersampled in-vivo datasets from the fastMRI neuro database\nwith different contrast settings. Results: For limited training data (18 and 22\nACS lines for R=4 and R=5, respectively), iterative RAKI outperforms standard\nRAKI by reducing residual artefacts and yields strong noise suppression when\ncompared to standard parallel imaging, underlined by quantitative\nreconstruction quality metrics. In combination with a phase constraint, further\nreconstruction improvements can be achieved. Additionally, iterative RAKI shows\nbetter performance than both GRAPPA and RAKI in case of pre-scan calibration\nwith varying contrast between training- and undersampled data. Conclusion: The\niterative learning approach with RAKI benefits from standard RAKIs well known\nnoise suppression feature but requires less original training data for the\naccurate reconstruction of standard 2D images thereby improving net\nacceleration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Dawood_P/0/1/0/all/0/1\">Peter Dawood</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Breuer_F/0/1/0/all/0/1\">Felix Breuer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Burd_P/0/1/0/all/0/1\">Paul R. Burd</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Homolya_I/0/1/0/all/0/1\">Istv&#xe1;n Homolya</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Oberberger_J/0/1/0/all/0/1\">Johannes Oberberger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jakob_P/0/1/0/all/0/1\">Peter M. Jakob</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Blaimer_M/0/1/0/all/0/1\">Martin Blaimer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HyperTransformer: Model Generation for Supervised and Semi-Supervised Few-Shot Learning. (arXiv:2201.04182v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2201.04182","description":"<p>In this work we propose a HyperTransformer, a Transformer-based model for\nsupervised and semi-supervised few-shot learning that generates weights of a\nconvolutional neural network (CNN) directly from support samples. Since the\ndependence of a small generated CNN model on a specific task is encoded by a\nhigh-capacity Transformer model, we effectively decouple the complexity of the\nlarge task space from the complexity of individual tasks. Our method is\nparticularly effective for small target CNN architectures where learning a\nfixed universal task-independent embedding is not optimal and better\nperformance is attained when the information about the task can modulate all\nmodel parameters. For larger models we discover that generating the last layer\nalone allows us to produce competitive or better results than those obtained\nwith state-of-the-art methods while being end-to-end differentiable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhmoginov_A/0/1/0/all/0/1\">Andrey Zhmoginov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sandler_M/0/1/0/all/0/1\">Mark Sandler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vladymyrov_M/0/1/0/all/0/1\">Max Vladymyrov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Pretraining for Echocardiography Segmentation with Limited Data. (arXiv:2201.07219v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2201.07219","description":"<p>Contrastive learning has proven useful in many applications where access to\nlabelled data is limited. The lack of annotated data is particularly\nproblematic in medical image segmentation as it is difficult to have clinical\nexperts manually annotate large volumes of data such as cardiac structures in\nultrasound images of the heart. In this paper, We propose a self supervised\ncontrastive learning method to segment the left ventricle from echocardiography\nwhen limited annotated images exist. Furthermore, we study the effect of\ncontrastive pretraining on two well-known segmentation networks, UNet and\nDeepLabV3. Our results show that contrastive pretraining helps improve the\nperformance on left ventricle segmentation, particularly when annotated data is\nscarce. We show how to achieve comparable results to state-of-the-art fully\nsupervised algorithms when we train our models in a self-supervised fashion\nfollowed by fine-tuning on just 5\\% of the data. We show that our solution\noutperforms what is currently published on a large public dataset\n(EchoNet-Dynamic) achieving a Dice score of 0.9252. We also compare the\nperformance of our solution on another smaller dataset (CAMUS) to demonstrate\nthe generalizability of our proposed solution. The code is available at\n(https://github.com/BioMedIA-MBZUAI/contrastive-echo).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Saeed_M/0/1/0/all/0/1\">Mohamed Saeed</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Muhtaseb_R/0/1/0/all/0/1\">Rand Muhtaseb</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yaqub_M/0/1/0/all/0/1\">Mohammad Yaqub</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing variational generation through self-decomposition. (arXiv:2202.02738v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.02738","description":"<p>In this article we introduce the notion of Split Variational Autoencoder\n(SVAE), whose output $\\hat{x}$ is obtained as a weighted sum $\\sigma \\odot\n\\hat{x_1} + (1-\\sigma) \\odot \\hat{x_2}$ of two generated images\n$\\hat{x_1},\\hat{x_2}$, and $\\sigma$ is a {\\em learned} compositional map. The\ncomposing images $\\hat{x_1},\\hat{x_2}$, as well as the $\\sigma$-map are\nautomatically synthesized by the model. The network is trained as a usual\nVariational Autoencoder with a negative loglikelihood loss between training and\nreconstructed images. No additional loss is required for $\\hat{x_1},\\hat{x_2}$\nor $\\sigma$, neither any form of human tuning. The decomposition is\nnondeterministic, but follows two main schemes, that we may roughly categorize\nas either \\say{syntactic} or \\say{semantic}. In the first case, the map tends\nto exploit the strong correlation between adjacent pixels, splitting the image\nin two complementary high frequency sub-images. In the second case, the map\ntypically focuses on the contours of objects, splitting the image in\ninteresting variations of its content, with more marked and distinctive\nfeatures. In this case, according to empirical observations, the Fr\\'echet\nInception Distance (FID) of $\\hat{x_1}$ and $\\hat{x_2}$ is usually lower (hence\nbetter) than that of $\\hat{x}$, that clearly suffers from being the average of\nthe former. In a sense, a SVAE forces the Variational Autoencoder to make\nchoices, in contrast with its intrinsic tendency to {\\em average} between\nalternatives with the aim to minimize the reconstruction loss towards a\nspecific sample. According to the FID metric, our technique, tested on typical\ndatasets such as Mnist, Cifar10 and CelebA, allows us to outperform all\nprevious purely variational architectures (not relying on normalization flows).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Asperti_A/0/1/0/all/0/1\">Andrea Asperti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bugo_L/0/1/0/all/0/1\">Laura Bugo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Filippini_D/0/1/0/all/0/1\">Daniele Filippini</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Machines Help Us Answering Question 16 in Datasheets, and In Turn Reflecting on Inappropriate Content?. (arXiv:2202.06675v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2202.06675","description":"<p>Large datasets underlying much of current machine learning raise serious\nissues concerning inappropriate content such as offensive, insulting,\nthreatening, or might otherwise cause anxiety. This calls for increased dataset\ndocumentation, e.g., using datasheets. They, among other topics, encourage to\nreflect on the composition of the datasets. So far, this documentation,\nhowever, is done manually and therefore can be tedious and error-prone,\nespecially for large image datasets. Here we ask the arguably \"circular\"\nquestion of whether a machine can help us reflect on inappropriate content,\nanswering Question 16 in Datasheets. To this end, we propose to use the\ninformation stored in pre-trained transformer models to assist us in the\ndocumentation process. Specifically, prompt-tuning based on a dataset of\nsocio-moral values steers CLIP to identify potentially inappropriate content,\ntherefore reducing human labor. We then document the inappropriate images found\nusing word clouds, based on captions generated using a vision-language model.\nThe documentations of two popular, large-scale computer vision datasets --\nImageNet and OpenImages -- produced this way suggest that machines can indeed\nhelp dataset creators to answer Question 16 on inappropriate image content.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schramowski_P/0/1/0/all/0/1\">Patrick Schramowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tauchmann_C/0/1/0/all/0/1\">Christopher Tauchmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1\">Kristian Kersting</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CAR: Class-aware Regularizations for Semantic Segmentation. (arXiv:2203.07160v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.07160","description":"<p>Recent segmentation methods, such as OCR and CPNet, utilizing \"class level\"\ninformation in addition to pixel features, have achieved notable success for\nboosting the accuracy of existing network modules. However, the extracted\nclass-level information was simply concatenated to pixel features, without\nexplicitly being exploited for better pixel representation learning. Moreover,\nthese approaches learn soft class centers based on coarse mask prediction,\nwhich is prone to error accumulation. In this paper, aiming to use class level\ninformation more effectively, we propose a universal Class-Aware Regularization\n(CAR) approach to optimize the intra-class variance and inter-class distance\nduring feature learning, motivated by the fact that humans can recognize an\nobject by itself no matter which other objects it appears with. Three novel\nloss functions are proposed. The first loss function encourages more compact\nclass representations within each class, the second directly maximizes the\ndistance between different class centers, and the third further pushes the\ndistance between inter-class centers and pixels. Furthermore, the class center\nin our approach is directly generated from ground truth instead of from the\nerror-prone coarse prediction. Our method can be easily applied to most\nexisting segmentation models during training, including OCR and CPNet, and can\nlargely improve their accuracy at no additional inference overhead. Extensive\nexperiments and ablation studies conducted on multiple benchmark datasets\ndemonstrate that the proposed CAR can boost the accuracy of all baseline models\nby up to 2.23% mIOU with superior generalization ability. The complete code is\navailable at https://github.com/edwardyehuang/CAR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Ye Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_D/0/1/0/all/0/1\">Di Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhe_X/0/1/0/all/0/1\">Xuefei Zhe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_W/0/1/0/all/0/1\">Wenjing Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiangjian He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_L/0/1/0/all/0/1\">Linchao Bao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Animatable Implicit Neural Representations for Creating Realistic Avatars from Videos. (arXiv:2203.08133v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.08133","description":"<p>This paper addresses the challenge of reconstructing an animatable human\nmodel from a multi-view video. Some recent works have proposed to decompose a\nnon-rigidly deforming scene into a canonical neural radiance field and a set of\ndeformation fields that map observation-space points to the canonical space,\nthereby enabling them to learn the dynamic scene from images. However, they\nrepresent the deformation field as translational vector field or SE(3) field,\nwhich makes the optimization highly under-constrained. Moreover, these\nrepresentations cannot be explicitly controlled by input motions. Instead, we\nintroduce a pose-driven deformation field based on the linear blend skinning\nalgorithm, which combines the blend weight field and the 3D human skeleton to\nproduce observation-to-canonical correspondences. Since 3D human skeletons are\nmore observable, they can regularize the learning of the deformation field.\nMoreover, the pose-driven deformation field can be controlled by input skeletal\nmotions to generate new deformation fields to animate the canonical human\nmodel. Experiments show that our approach significantly outperforms recent\nhuman modeling methods. The code is available at\nhttps://zju3dv.github.io/animatable_nerf/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1\">Sida Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Junting Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qianqian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shangzhan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shuai_Q/0/1/0/all/0/1\">Qing Shuai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1\">Hujun Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xiaowei Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain Adaptive Hand Keypoint and Pixel Localization in the Wild. (arXiv:2203.08344v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.08344","description":"<p>We aim to improve the performance of regressing hand keypoints and segmenting\npixel-level hand masks under new imaging conditions (e.g., outdoors) when we\nonly have labeled images taken under very different conditions (e.g., indoors).\nIn the real world, it is important that the model trained for both tasks works\nunder various imaging conditions. However, their variation covered by existing\nlabeled hand datasets is limited. Thus, it is necessary to adapt the model\ntrained on the labeled images (source) to unlabeled images (target) with unseen\nimaging conditions. While self-training domain adaptation methods (i.e.,\nlearning from the unlabeled target images in a self-supervised manner) have\nbeen developed for both tasks, their training may degrade performance when the\npredictions on the target images are noisy. To avoid this, it is crucial to\nassign a low importance (confidence) weight to the noisy predictions during\nself-training. In this paper, we propose to utilize the divergence of two\npredictions to estimate the confidence of the target image for both tasks.\nThese predictions are given from two separate networks, and their divergence\nhelps identify the noisy predictions. To integrate our proposed confidence\nestimation into self-training, we propose a teacher-student framework where the\ntwo networks (teachers) provide supervision to a network (student) for\nself-training, and the teachers are learned from the student by knowledge\ndistillation. Our experiments show its superiority over state-of-the-art\nmethods in adaptation settings with different lighting, grasping objects,\nbackgrounds, and camera viewpoints. Our method improves by 4% the multi-task\nscore on HO3D compared to the latest adversarial adaptation method. We also\nvalidate our method on Ego4D, egocentric videos with rapid changes in imaging\nconditions outdoors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ohkawa_T/0/1/0/all/0/1\">Takehiko Ohkawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yu-Jhe Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Q/0/1/0/all/0/1\">Qichen Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Furuta_R/0/1/0/all/0/1\">Ryosuke Furuta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitani_K/0/1/0/all/0/1\">Kris M. Kitani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sato_Y/0/1/0/all/0/1\">Yoichi Sato</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End Video Text Spotting with Transformer. (arXiv:2203.10539v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.10539","description":"<p>Recent video text spotting methods usually require the three-staged pipeline,\ni.e., detecting text in individual images, recognizing localized text, tracking\ntext streams with post-processing to generate final results. These methods\ntypically follow the tracking-by-match paradigm and develop sophisticated\npipelines. In this paper, rooted in Transformer sequence modeling, we propose a\nsimple, but effective end-to-end video text DEtection, Tracking, and\nRecognition framework (TransDETR). TransDETR mainly includes two advantages: 1)\nDifferent from the explicit match paradigm in the adjacent frame, TransDETR\ntracks and recognizes each text implicitly by the different query termed text\nquery over long-range temporal sequence (more than 7 frames). 2) TransDETR is\nthe first end-to-end trainable video text spotting framework, which\nsimultaneously addresses the three sub-tasks (e.g., text detection, tracking,\nrecognition). Extensive experiments in four video text datasets (i.e.,ICDAR2013\nVideo, ICDAR2015 Video, Minetto, and YouTube Video Text) are conducted to\ndemonstrate that TransDETR achieves state-of-the-art performance with up to\naround 8.0% improvements on video text spotting tasks. The code of TransDETR\ncan be found at https://github.com/weijiawu/TransDETR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Weijia Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1\">Yuanqiang Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Debing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Ying Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spatially Multi-conditional Image Generation. (arXiv:2203.13812v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.13812","description":"<p>In most scenarios, conditional image generation can be thought of as an\ninversion of the image understanding process. Since generic image understanding\ninvolves solving multiple tasks, it is natural to aim at generating images via\nmulti-conditioning. However, multi-conditional image generation is a very\nchallenging problem due to the heterogeneity and the sparsity of the (in\npractice) available conditioning labels. In this work, we propose a novel\nneural architecture to address the problem of heterogeneity and sparsity of the\nspatially multi-conditional labels. Our choice of spatial conditioning, such as\nby semantics and depth, is driven by the promise it holds for better control of\nthe image generation process. The proposed method uses a transformer-like\narchitecture operating pixel-wise, which receives the available labels as input\ntokens to merge them in a learned homogeneous space of labels. The merged\nlabels are then used for image generation via conditional generative\nadversarial training. In this process, the sparsity of the labels is handled by\nsimply dropping the input tokens corresponding to the missing labels at the\ndesired locations, thanks to the proposed pixel-wise operating architecture.\nOur experiments on three benchmark datasets demonstrate the clear superiority\nof our method over the state-of-the-art and compared baselines. The source code\nwill be made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_R/0/1/0/all/0/1\">Ritika Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Popovic_N/0/1/0/all/0/1\">Nikola Popovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paudel_D/0/1/0/all/0/1\">Danda Pani Paudel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Probst_T/0/1/0/all/0/1\">Thomas Probst</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Feature robustness and sex differences in medical imaging: a case study in MRI-based Alzheimer's disease detection. (arXiv:2204.01737v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2204.01737","description":"<p>Convolutional neural networks have enabled significant improvements in\nmedical image-based diagnosis. It is, however, increasingly clear that these\nmodels are susceptible to performance degradation when facing spurious\ncorrelations and dataset shift, leading, e.g., to underperformance on\nunderrepresented patient groups. In this paper, we compare two classification\nschemes on the ADNI MRI dataset: a simple logistic regression model using\nmanually selected volumetric features, and a convolutional neural network\ntrained on 3D MRI data. We assess the robustness of the trained models in the\nface of varying dataset splits, training set sex composition, and stage of\ndisease. In contrast to earlier work in other imaging modalities, we do not\nobserve a clear pattern of improved model performance for the majority group in\nthe training dataset. Instead, while logistic regression is fully robust to\ndataset composition, we find that CNN performance is generally improved for\nboth male and female subjects when including more female subjects in the\ntraining dataset. We hypothesize that this might be due to inherent differences\nin the pathology of the two sexes. Moreover, in our analysis, the logistic\nregression model outperforms the 3D CNN, emphasizing the utility of manual\nfeature specification based on prior knowledge, and the need for more robust\nautomatic feature selection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Petersen_E/0/1/0/all/0/1\">Eike Petersen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Feragen_A/0/1/0/all/0/1\">Aasa Feragen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zemsch_M/0/1/0/all/0/1\">Maria Luise da Costa Zemsch</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Henriksen_A/0/1/0/all/0/1\">Anders Henriksen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Christensen_O/0/1/0/all/0/1\">Oskar Eiler Wiese Christensen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ganz_M/0/1/0/all/0/1\">Melanie Ganz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Unlearning via Randomized Conditionally Independent Hessians. (arXiv:2204.07655v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.07655","description":"<p>Recent legislation has led to interest in machine unlearning, i.e., removing\nspecific training samples from a predictive model as if they never existed in\nthe training dataset. Unlearning may also be required due to\ncorrupted/adversarial data or simply a user's updated privacy requirement. For\nmodels which require no training (k-NN), simply deleting the closest original\nsample can be effective. But this idea is inapplicable to models which learn\nricher representations. Recent ideas leveraging optimization-based updates\nscale poorly with the model dimension d, due to inverting the Hessian of the\nloss function. We use a variant of a new conditional independence coefficient,\nL-CODEC, to identify a subset of the model parameters with the most semantic\noverlap on an individual sample level. Our approach completely avoids the need\nto invert a (possibly) huge matrix. By utilizing a Markov blanket selection, we\npremise that L-CODEC is also suitable for deep unlearning, as well as other\napplications in vision. Compared to alternatives, L-CODEC makes approximate\nunlearning possible in settings that would otherwise be infeasible, including\nvision models used for face recognition, person re-identification and NLP\nmodels that may require unlearning samples identified for exclusion. Code can\nbe found at https://github.com/vsingh-group/LCODEC-deep-unlearning/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mehta_R/0/1/0/all/0/1\">Ronak Mehta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pal_S/0/1/0/all/0/1\">Sourav Pal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_V/0/1/0/all/0/1\">Vikas Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravi_S/0/1/0/all/0/1\">Sathya N. Ravi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MMRotate: A Rotated Object Detection Benchmark using Pytorch. (arXiv:2204.13317v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.13317","description":"<p>We present an open-source toolbox, named MMRotate, which provides a coherent\nalgorithm framework of training, inferring, and evaluation for the popular\nrotated object detection algorithm based on deep learning. MMRotate implements\n18 state-of-the-art algorithms and supports the three most frequently used\nangle definition methods. To facilitate future research and industrial\napplications of rotated object detection-related problems, we also provide a\nlarge number of trained models and detailed benchmarks to give insights into\nthe performance of rotated object detection. MMRotate is publicly released at\nhttps://github.com/open-mmlab/mmrotate.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yue Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xue Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Gefan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiabao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yanyi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Liping Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xue Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xingzhao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Junchi Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_C/0/1/0/all/0/1\">Chengqi Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kai Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Privacy Preserving Image Registration. (arXiv:2205.10120v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.10120","description":"<p>Image registration is a key task in medical imaging applications, allowing to\nrepresent medical images in a common spatial reference frame. Current\nliterature on image registration is generally based on the assumption that\nimages are usually accessible to the researcher, from which the spatial\ntransformation is subsequently estimated. This common assumption may not be met\nin current practical applications, since the sensitive nature of medical images\nmay ultimately require their analysis under privacy constraints, preventing to\nshare the image content in clear form. In this work, we formulate the problem\nof image registration under a privacy preserving regime, where images are\nassumed to be confidential and cannot be disclosed in clear. We derive our\nprivacy preserving image registration framework by extending classical\nregistration paradigms to account for advanced cryptographic tools, such as\nsecure multi-party computation and homomorphic encryption, that enable the\nexecution of operations without leaking the underlying data. To overcome the\nproblem of performance and scalability of cryptographic tools in high\ndimensions, we first propose to optimize the underlying image registration\noperations using gradient approximations. We further revisit the use of\nhomomorphic encryption and use a packing method to allow the encryption and\nmultiplication of large matrices more efficiently. We demonstrate our privacy\npreserving framework in linear and non-linear registration problems, evaluating\nits accuracy and scalability with respect to standard image registration. Our\nresults show that privacy preserving image registration is feasible and can be\nadopted in sensitive medical imaging applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Taiello_R/0/1/0/all/0/1\">Riccardo Taiello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Onen_M/0/1/0/all/0/1\">Melek &#xd6;nen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Humbert_O/0/1/0/all/0/1\">Olivier Humbert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lorenzi_M/0/1/0/all/0/1\">Marco Lorenzi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Superclass Adversarial Attack. (arXiv:2205.14629v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.14629","description":"<p>Adversarial attacks have only focused on changing the predictions of the\nclassifier, but their danger greatly depends on how the class is mistaken. For\nexample, when an automatic driving system mistakes a Persian cat for a Siamese\ncat, it is hardly a problem. However, if it mistakes a cat for a 120km/h\nminimum speed sign, serious problems can arise. As a stepping stone to more\nthreatening adversarial attacks, we consider the superclass adversarial attack,\nwhich causes misclassification of not only fine classes, but also superclasses.\nWe conducted the first comprehensive analysis of superclass adversarial attacks\n(an existing and 19 new methods) in terms of accuracy, speed, and stability,\nand identified several strategies to achieve better performance. Although this\nstudy is aimed at superclass misclassification, the findings can be applied to\nother problem settings involving multiple classes, such as top-k and\nmulti-label classification attacks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumano_S/0/1/0/all/0/1\">Soichiro Kumano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kera_H/0/1/0/all/0/1\">Hiroshi Kera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamasaki_T/0/1/0/all/0/1\">Toshihiko Yamasaki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Strongly Augmented Contrastive Clustering. (arXiv:2206.00380v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2206.00380","description":"<p>Deep clustering has attracted increasing attention in recent years due to its\ncapability of joint representation learning and clustering via deep neural\nnetworks. In its latest developments, the contrastive learning has emerged as\nan effective technique to substantially enhance the deep clustering\nperformance. However, the existing contrastive learning based deep clustering\nalgorithms mostly focus on some carefully-designed augmentations (often with\nlimited transformations to preserve the structure), referred to as weak\naugmentations, but cannot go beyond the weak augmentations to explore the more\nopportunities in stronger augmentations (with more aggressive transformations\nor even severe distortions). In this paper, we present an end-to-end deep\nclustering approach termed Strongly Augmented Contrastive Clustering (SACC),\nwhich extends the conventional two-augmentation-view paradigm to multiple views\nand jointly leverages strong and weak augmentations for strengthened deep\nclustering. Particularly, we utilize a backbone network with triply-shared\nweights, where a strongly augmented view and two weakly augmented views are\nincorporated. Based on the representations produced by the backbone, the\nweak-weak view pair and the strong-weak view pairs are simultaneously exploited\nfor the instance-level contrastive learning (via an instance projector) and the\ncluster-level contrastive learning (via a cluster projector), which, together\nwith the backbone, can be jointly optimized in a purely unsupervised manner.\nExperimental results on five challenging image datasets have shown the\nsuperiority of our SACC approach over the state-of-the-art. The code is\navailable at https://github.com/dengxiaozhi/SACC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_X/0/1/0/all/0/1\">Xiaozhi Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">Dong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Ding-Hua Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chang-Dong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_J/0/1/0/all/0/1\">Jian-Huang Lai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Modal Transformer GAN: A Brain Structure-Function Deep Fusing Framework for Alzheimer's Disease. (arXiv:2206.13393v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2206.13393","description":"<p>Cross-modal fusion of different types of neuroimaging data has shown great\npromise for predicting the progression of Alzheimer's Disease(AD). However,\nmost existing methods applied in neuroimaging can not efficiently fuse the\nfunctional and structural information from multi-modal neuroimages. In this\nwork, a novel cross-modal transformer generative adversarial network(CT-GAN) is\nproposed to fuse functional information contained in resting-state functional\nmagnetic resonance imaging (rs-fMRI) and structural information contained in\nDiffusion Tensor Imaging (DTI). The developed bi-attention mechanism can match\nfunctional information to structural information efficiently and maximize the\ncapability of extracting complementary information from rs-fMRI and DTI. By\ncapturing the deep complementary information between structural features and\nfunctional features, the proposed CT-GAN can detect the AD-related brain\nconnectivity, which could be used as a bio-marker of AD. Experimental results\nshow that the proposed model can not only improve classification performance\nbut also detect the AD-related brain connectivity effectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Pan_J/0/1/0/all/0/1\">Junren Pan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1\">Shuqiang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On-Device Training Under 256KB Memory. (arXiv:2206.15472v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.15472","description":"<p>On-device training enables the model to adapt to new data collected from the\nsensors by fine-tuning a pre-trained model. However, the training memory\nconsumption is prohibitive for IoT devices that have tiny memory resources. We\npropose an algorithm-system co-design framework to make on-device training\npossible with only 256KB of memory. On-device training faces two unique\nchallenges: (1) the quantized graphs of neural networks are hard to optimize\ndue to mixed bit-precision and the lack of normalization; (2) the limited\nhardware resource (memory and computation) does not allow full backward\ncomputation. To cope with the optimization difficulty, we propose\nQuantization-Aware Scaling to calibrate the gradient scales and stabilize\nquantized training. To reduce the memory footprint, we propose Sparse Update to\nskip the gradient computation of less important layers and sub-tensors. The\nalgorithm innovation is implemented by a lightweight training system, Tiny\nTraining Engine, which prunes the backward computation graph to support sparse\nupdates and offloads the runtime auto-differentiation to compile time. Our\nframework is the first practical solution for on-device transfer learning of\nvisual recognition on tiny IoT devices (e.g., a microcontroller with only 256KB\nSRAM), using less than 1/100 of the memory of existing frameworks while\nmatching the accuracy of cloud training+edge deployment for the tinyML\napplication VWW. Our study enables IoT devices to not only perform inference\nbut also continuously adapt to new data for on-device lifelong learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Ji Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Ligeng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wei-Ming Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei-Chen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1\">Chuang Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Song Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detection of ADHD based on Eye Movements during Natural Viewing. (arXiv:2207.01377v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.01377","description":"<p>Attention-deficit/hyperactivity disorder (ADHD) is a neurodevelopmental\ndisorder that is highly prevalent and requires clinical specialists to\ndiagnose. It is known that an individual's viewing behavior, reflected in their\neye movements, is directly related to attentional mechanisms and higher-order\ncognitive processes. We therefore explore whether ADHD can be detected based on\nrecorded eye movements together with information about the video stimulus in a\nfree-viewing task. To this end, we develop an end-to-end deep learning-based\nsequence model which we pre-train on a related task for which more data are\navailable. We find that the method is in fact able to detect ADHD and\noutperforms relevant baselines. We investigate the relevance of the input\nfeatures in an ablation study. Interestingly, we find that the model's\nperformance is closely related to the content of the video, which provides\ninsights for future experimental designs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shuwen Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prasse_P/0/1/0/all/0/1\">Paul Prasse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reich_D/0/1/0/all/0/1\">David R. Reich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dziemian_S/0/1/0/all/0/1\">Sabine Dziemian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stegenwallner_Schutz_M/0/1/0/all/0/1\">Maja Stegenwallner-Sch&#xfc;tz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krakowczyk_D/0/1/0/all/0/1\">Daniel Krakowczyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makowski_S/0/1/0/all/0/1\">Silvia Makowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Langer_N/0/1/0/all/0/1\">Nicolas Langer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scheffer_T/0/1/0/all/0/1\">Tobias Scheffer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jager_L/0/1/0/all/0/1\">Lena A. J&#xe4;ger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Patch-wise Deep Metric Learning for Unsupervised Low-Dose CT Denoising. (arXiv:2207.02377v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2207.02377","description":"<p>The acquisition conditions for low-dose and high-dose CT images are usually\ndifferent, so that the shifts in the CT numbers often occur. Accordingly,\nunsupervised deep learning-based approaches, which learn the target image\ndistribution, often introduce CT number distortions and result in detrimental\neffects in diagnostic performance. To address this, here we propose a novel\nunsupervised learning approach for lowdose CT reconstruction using patch-wise\ndeep metric learning. The key idea is to learn embedding space by pulling the\npositive pairs of image patches which shares the same anatomical structure, and\npushing the negative pairs which have same noise level each other. Thereby, the\nnetwork is trained to suppress the noise level, while retaining the original\nglobal CT number distributions even after the image translation. Experimental\nresults confirm that our deep metric learning plays a critical role in\nproducing high quality denoised images without CT number shift.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Jung_C/0/1/0/all/0/1\">Chanyong Jung</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_J/0/1/0/all/0/1\">Joonhyung Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+You_S/0/1/0/all/0/1\">Sunkyoung You</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ye_J/0/1/0/all/0/1\">Jong Chul Ye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What Makes for Automatic Reconstruction of Pulmonary Segments. (arXiv:2207.03078v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2207.03078","description":"<p>3D reconstruction of pulmonary segments plays an important role in surgical\ntreatment planning of lung cancer, which facilitates preservation of pulmonary\nfunction and helps ensure low recurrence rates. However, automatic\nreconstruction of pulmonary segments remains unexplored in the era of deep\nlearning. In this paper, we investigate what makes for automatic reconstruction\nof pulmonary segments. First and foremost, we formulate, clinically and\ngeometrically, the anatomical definitions of pulmonary segments, and propose\nevaluation metrics adhering to these definitions. Second, we propose ImPulSe\n(Implicit Pulmonary Segment), a deep implicit surface model designed for\npulmonary segment reconstruction. The automatic reconstruction of pulmonary\nsegments by ImPulSe is accurate in metrics and visually appealing. Compared\nwith canonical segmentation methods, ImPulSe outputs continuous predictions of\narbitrary resolutions with higher training efficiency and fewer parameters.\nLastly, we experiment with different network inputs to analyze what matters in\nthe task of pulmonary segment reconstruction. Our code is available at\nhttps://github.com/M3DV/ImPulSe.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kuang_K/0/1/0/all/0/1\">Kaiming Kuang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Jingyu Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1\">Hongwei Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_J/0/1/0/all/0/1\">Jiajun Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Du_B/0/1/0/all/0/1\">Bo Du</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_J/0/1/0/all/0/1\">Jiancheng Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"D2HNet: Joint Denoising and Deblurring with Hierarchical Network for Robust Night Image Restoration. (arXiv:2207.03294v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.03294","description":"<p>Night imaging with modern smartphone cameras is troublesome due to low photon\ncount and unavoidable noise in the imaging system. Directly adjusting exposure\ntime and ISO ratings cannot obtain sharp and noise-free images at the same time\nin low-light conditions. Though many methods have been proposed to enhance\nnoisy or blurry night images, their performances on real-world night photos are\nstill unsatisfactory due to two main reasons: 1) Limited information in a\nsingle image and 2) Domain gap between synthetic training images and real-world\nphotos (e.g., differences in blur area and resolution). To exploit the\ninformation from successive long- and short-exposure images, we propose a\nlearning-based pipeline to fuse them. A D2HNet framework is developed to\nrecover a high-quality image by deblurring and enhancing a long-exposure image\nunder the guidance of a short-exposure image. To shrink the domain gap, we\nleverage a two-phase DeblurNet-EnhanceNet architecture, which performs accurate\nblur removal on a fixed low resolution so that it is able to handle large\nranges of blur in different resolution inputs. In addition, we synthesize a\nD2-Dataset from HD videos and experiment on it. The results on the validation\nset and real photos demonstrate our methods achieve better visual quality and\nstate-of-the-art quantitative scores. The D2HNet codes and D2-Dataset can be\nfound at https://github.com/zhaoyuzhi/D2HNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yuzhi Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yongzhe Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Q/0/1/0/all/0/1\">Qiong Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Dingdong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuehui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Po_L/0/1/0/all/0/1\">Lai-Man Po</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A simple normalization technique using window statistics to improve the out-of-distribution generalization on medical images. (arXiv:2207.03366v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.03366","description":"<p>Since data scarcity and data heterogeneity are prevailing for medical images,\nwell-trained Convolutional Neural Networks (CNNs) using previous normalization\nmethods may perform poorly when deployed to a new site. However, a reliable\nmodel for real-world clinical applications should be able to generalize well\nboth on in-distribution (IND) and out-of-distribution (OOD) data (e.g., the new\nsite data). In this study, we present a novel normalization technique called\nwindow normalization (WIN) to improve the model generalization on heterogeneous\nmedical images, which is a simple yet effective alternative to existing\nnormalization methods. Specifically, WIN perturbs the normalizing statistics\nwith the local statistics computed on the window of features. This\nfeature-level augmentation technique regularizes the models well and improves\ntheir OOD generalization significantly. Taking its advantage, we propose a\nnovel self-distillation method called WIN-WIN for classification tasks. WIN-WIN\nis easily implemented with twice forward passes and a consistency constraint,\nwhich can be a simple extension for existing methods. Extensive experimental\nresults on various tasks (6 tasks) and datasets (24 datasets) demonstrate the\ngenerality and effectiveness of our methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chengfeng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Songchang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chenming Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Feng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Juan Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hefeng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_D/0/1/0/all/0/1\">Dahong Qian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Direct Handheld Burst Imaging to Simulated Defocus. (arXiv:2207.04175v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.04175","description":"<p>A shallow depth-of-field image keeps the subject in focus, and the foreground\nand background contexts blurred. This effect requires much larger lens\napertures than those of smartphone cameras. Conventional methods acquire RGB-D\nimages and blur image regions based on their depth. However, this approach is\nnot suitable for reflective or transparent surfaces, or finely detailed object\nsilhouettes, where the depth value is inaccurate or ambiguous.\n</p>\n<p>We present a learning-based method to synthesize the defocus blur in shallow\ndepth-of-field images from handheld bursts acquired with a single small\naperture lens. Our deep learning model directly produces the shallow\ndepth-of-field image, avoiding explicit depth-based blurring. The simulated\naperture diameter equals the camera translation during burst acquisition. Our\nmethod does not suffer from artifacts due to inaccurate or ambiguous depth\nestimation, and it is well-suited to portrait photography.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Meng-Lin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dayana_V/0/1/0/all/0/1\">Venkata Ravi Kiran Dayana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_H/0/1/0/all/0/1\">Hau Hwang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Radiomics-Guided Global-Local Transformer for Weakly Supervised Pathology Localization in Chest X-Rays. (arXiv:2207.04394v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.04394","description":"<p>Before the recent success of deep learning methods for automated medical\nimage analysis, practitioners used handcrafted radiomic features to\nquantitatively describe local patches of medical images. However, extracting\ndiscriminative radiomic features relies on accurate pathology localization,\nwhich is difficult to acquire in real-world settings. Despite advances in\ndisease classification and localization from chest X-rays, many approaches fail\nto incorporate clinically-informed domain knowledge. For these reasons, we\npropose a Radiomics-Guided Transformer (RGT) that fuses \\textit{global} image\ninformation with \\textit{local} knowledge-guided radiomics information to\nprovide accurate cardiopulmonary pathology localization and classification\n\\textit{without any bounding box annotations}. RGT consists of an image\nTransformer branch, a radiomics Transformer branch, and fusion layers that\naggregate image and radiomic information. Using the learned self-attention of\nits image branch, RGT extracts a bounding box for which to compute radiomic\nfeatures, which are further processed by the radiomics branch; learned image\nand radiomic features are then fused and mutually interact via cross-attention\nlayers. Thus, RGT utilizes a novel end-to-end feedback loop that can bootstrap\naccurate pathology localization only using image-level disease labels.\nExperiments on the NIH ChestXRay dataset demonstrate that RGT outperforms prior\nworks in weakly supervised disease localization (by an average margin of 3.6\\%\nover various intersection-over-union thresholds) and classification (by 1.1\\%\nin average area under the receiver operating characteristic curve). We publicly\nrelease our codes and pre-trained models at\n\\url{https://github.com/VITA-Group/chext}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1\">Yan Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holste_G/0/1/0/all/0/1\">Gregory Holste</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Ying Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tewfik_A/0/1/0/all/0/1\">Ahmed Tewfik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yifan Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Waste Copper Granules Rating System Based on Machine Vision. (arXiv:2207.04575v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.04575","description":"<p>In the field of waste copper granules recycling, engineers should be able to\nidentify all different sorts of impurities in waste copper granules and\nestimate their mass proportion relying on experience before rating. This manual\nrating method is costly, lacking in objectivity and comprehensiveness. To\ntackle this problem, we propose a waste copper granules rating system based on\nmachine vision and deep learning. We firstly formulate the rating task into a\n2D image recognition and purity regression task. Then we design a two-stage\nconvolutional rating network to compute the mass purity and rating level of\nwaste copper granules. Our rating network includes a segmentation network and a\npurity regression network, which respectively calculate the semantic\nsegmentation heatmaps and purity results of the waste copper granules. After\ntraining the rating network on the augmented datasets, experiments on real\nwaste copper granules demonstrate the effectiveness and superiority of the\nproposed network. Specifically, our system is superior to the manual method in\nterms of accuracy, effectiveness, robustness, and objectivity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_K/0/1/0/all/0/1\">Kaikai Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Yajie Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhaoxiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_S/0/1/0/all/0/1\">Shiguo Lian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Susceptibility of Continual Learning Against Adversarial Attacks. (arXiv:2207.05225v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2207.05225","description":"<p>The recent advances in continual (incremental or lifelong) learning have\nconcentrated on the prevention of forgetting that can lead to catastrophic\nconsequences, but there are two outstanding challenges that must be addressed.\nThe first is the evaluation of the robustness of the proposed methods. The\nsecond is ensuring the security of learned tasks remains largely unexplored.\nThis paper presents a comprehensive study of the susceptibility of the\ncontinually learned tasks (including both current and previously learned tasks)\nthat are vulnerable to forgetting. Such vulnerability of tasks against\nadversarial attacks raises profound issues in data integrity and privacy. We\nconsider all three scenarios (i.e, task-incremental leaning, domain-incremental\nlearning and class-incremental learning) of continual learning and explore\nthree regularization-based experiments, three replay-based experiments, and one\nhybrid technique based on the reply and exemplar approach. We examine the\nrobustness of these methods. In particular, we consider cases where we\ndemonstrate that any class belonging to the current or previously learned tasks\nis prone to misclassification. Our observations, we identify potential\nlimitations in continual learning approaches against adversarial attacks. Our\nempirical study recommends that the research community consider the robustness\nof the proposed continual learning approaches and invest extensive efforts in\nmitigating catastrophic forgetting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khan_H/0/1/0/all/0/1\">Hikmat Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_P/0/1/0/all/0/1\">Pir Masoom Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaidi_S/0/1/0/all/0/1\">Syed Farhan Alam Zaidi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_S/0/1/0/all/0/1\">Saif ul Islam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Fairness of Visual Attribute Predictors. (arXiv:2207.05727v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.05727","description":"<p>The performance of deep neural networks for image recognition tasks such as\npredicting a smiling face is known to degrade with under-represented classes of\nsensitive attributes. We address this problem by introducing fairness-aware\nregularization losses based on batch estimates of Demographic Parity, Equalized\nOdds, and a novel Intersection-over-Union measure. The experiments performed on\nfacial and medical images from CelebA, UTKFace, and the SIIM-ISIC melanoma\nclassification challenge show the effectiveness of our proposed fairness losses\nfor bias mitigation as they improve model fairness while maintaining high\nclassification performance. To the best of our knowledge, our work is the first\nattempt to incorporate these types of losses in an end-to-end training scheme\nfor mitigating biases of visual attribute predictors. Our code is available at\nhttps://github.com/nish03/FVAP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hanel_T/0/1/0/all/0/1\">Tobias H&#xe4;nel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_N/0/1/0/all/0/1\">Nishant Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schlesinger_D/0/1/0/all/0/1\">Dmitrij Schlesinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mengze Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Unal_E/0/1/0/all/0/1\">Erdem &#xdc;nal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eslami_A/0/1/0/all/0/1\">Abouzar Eslami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gumhold_S/0/1/0/all/0/1\">Stefan Gumhold</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Collaborative Quantization Embeddings for Intra-Subject Prostate MR Image Registration. (arXiv:2207.06189v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2207.06189","description":"<p>Image registration is useful for quantifying morphological changes in\nlongitudinal MR images from prostate cancer patients. This paper describes a\ndevelopment in improving the learning-based registration algorithms, for this\nchallenging clinical application often with highly variable yet limited\ntraining data. First, we report that the latent space can be clustered into a\nmuch lower dimensional space than that commonly found as bottleneck features at\nthe deep layer of a trained registration network. Based on this observation, we\npropose a hierarchical quantization method, discretizing the learned feature\nvectors using a jointly-trained dictionary with a constrained size, in order to\nimprove the generalisation of the registration networks. Furthermore, a novel\ncollaborative dictionary is independently optimised to incorporate additional\nprior information, such as the segmentation of the gland or other regions of\ninterest, in the latent quantized space. Based on 216 real clinical images from\n86 prostate cancer patients, we show the efficacy of both the designed\ncomponents. Improved registration accuracy was obtained with statistical\nsignificance, in terms of both Dice on gland and target registration error on\ncorresponding landmarks, the latter of which achieved 5.46 mm, an improvement\nof 28.7\\% from the baseline without quantization. Experimental results also\nshow that the difference in performance was indeed minimised between training\nand testing data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Shen_Z/0/1/0/all/0/1\">Ziyi Shen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_Q/0/1/0/all/0/1\">Qianye Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shen_Y/0/1/0/all/0/1\">Yuming Shen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Giganti_F/0/1/0/all/0/1\">Francesco Giganti</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Stavrinides_V/0/1/0/all/0/1\">Vasilis Stavrinides</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fan_R/0/1/0/all/0/1\">Richard Fan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Moore_C/0/1/0/all/0/1\">Caroline Moore</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rusu_M/0/1/0/all/0/1\">Mirabela Rusu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sonn_G/0/1/0/all/0/1\">Geoffrey Sonn</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Torr_P/0/1/0/all/0/1\">Philip Torr</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Barratt_D/0/1/0/all/0/1\">Dean Barratt</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_Y/0/1/0/all/0/1\">Yipeng Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarially-Aware Robust Object Detector. (arXiv:2207.06202v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.06202","description":"<p>Object detection, as a fundamental computer vision task, has achieved a\nremarkable progress with the emergence of deep neural networks. Nevertheless,\nfew works explore the adversarial robustness of object detectors to resist\nadversarial attacks for practical applications in various real-world scenarios.\nDetectors have been greatly challenged by unnoticeable perturbation, with sharp\nperformance drop on clean images and extremely poor performance on adversarial\nimages. In this work, we empirically explore the model training for adversarial\nrobustness in object detection, which greatly attributes to the conflict\nbetween learning clean images and adversarial images. To mitigate this issue,\nwe propose a Robust Detector (RobustDet) based on adversarially-aware\nconvolution to disentangle gradients for model learning on clean and\nadversarial images. RobustDet also employs the Adversarial Image Discriminator\n(AID) and Consistent Features with Reconstruction (CFR) to ensure a reliable\nrobustness. Extensive experiments on PASCAL VOC and MS-COCO demonstrate that\nour model effectively disentangles gradients and significantly enhances the\ndetection robustness with maintaining the detection ability on clean images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1\">Ziyi Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_P/0/1/0/all/0/1\">Pengxu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Liang Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Entry-Flipped Transformer for Inference and Prediction of Participant Behavior. (arXiv:2207.06235v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.06235","description":"<p>Some group activities, such as team sports and choreographed dances, involve\nclosely coupled interaction between participants. Here we investigate the tasks\nof inferring and predicting participant behavior, in terms of motion paths and\nactions, under such conditions. We narrow the problem to that of estimating how\na set target participants react to the behavior of other observed participants.\nOur key idea is to model the spatio-temporal relations among participants in a\nmanner that is robust to error accumulation during frame-wise inference and\nprediction. We propose a novel Entry-Flipped Transformer (EF-Transformer),\nwhich models the relations of participants by attention mechanisms on both\nspatial and temporal domains. Unlike typical transformers, we tackle the\nproblem of error accumulation by flipping the order of query, key, and value\nentries, to increase the importance and fidelity of observed features in the\ncurrent frame. Comparative experiments show that our EF-Transformer achieves\nthe best performance on a newly-collected tennis doubles dataset, a Ceilidh\ndance dataset, and two pedestrian datasets. Furthermore, it is also\ndemonstrated that our EF-Transformer is better at limiting accumulated errors\nand recovering from wrong estimations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1\">Bo Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cham_T/0/1/0/all/0/1\">Tat-Jen Cham</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Organic Priors in Non-Rigid Structure from Motion. (arXiv:2207.06262v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.06262","description":"<p>This paper advocates the use of organic priors in classical non-rigid\nstructure from motion (NRSfM). By organic priors, we mean invaluable\nintermediate prior information intrinsic to the NRSfM matrix factorization\ntheory. It is shown that such priors reside in the factorized matrices, and\nquite surprisingly, existing methods generally disregard them. The paper's main\ncontribution is to put forward a simple, methodical, and practical method that\ncan effectively exploit such organic priors to solve NRSfM. The proposed method\ndoes not make assumptions other than the popular one on the low-rank shape and\noffers a reliable solution to NRSfM under orthographic projection. Our work\nreveals that the accessibility of organic priors is independent of the camera\nmotion and shape deformation type. Besides that, the paper provides insights\ninto the NRSfM factorization -- both in terms of shape and motion -- and is the\nfirst approach to show the benefit of single rotation averaging for NRSfM.\nFurthermore, we outline how to effectively recover motion and non-rigid 3D\nshape using the proposed organic prior based approach and demonstrate results\nthat outperform prior-free NRSfM performance by a significant margin. Finally,\nwe present the benefits of our method via extensive experiments and evaluations\non several benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Suryansh Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Adaptive Unknown Authentication for Universal Domain Adaptation by Classifier Paradox. (arXiv:2207.04494v1 [cs.CV] CROSS LISTED)","link":"http://arxiv.org/abs/2207.04494","description":"<p>Universal domain adaptation (UniDA) is a general unsupervised domain\nadaptation setting, which addresses both domain and label shifts in adaptation.\nIts main challenge lies in how to identify target samples in unshared or\nunknown classes. Previous methods commonly strive to depict sample \"confidence\"\nalong with a threshold for rejecting unknowns, and align feature distributions\nof shared classes across domains. However, it is still hard to pre-specify a\n\"confidence\" criterion and threshold which are adaptive to various real tasks,\nand a mis-prediction of unknowns further incurs misalignment of features in\nshared classes. In this paper, we propose a new UniDA method with adaptive\nUnknown Authentication by Classifier Paradox (UACP), considering that samples\nwith paradoxical predictions are probably unknowns belonging to none of the\nsource classes. In UACP, a composite classifier is jointly designed with two\ntypes of predictors. That is, a multi-class (MC) predictor classifies samples\nto one of the multiple source classes, while a binary one-vs-all (OVA)\npredictor further verifies the prediction by MC predictor. Samples with\nverification failure or paradox are identified as unknowns. Further, instead of\nfeature alignment for shared classes, implicit domain alignment is conducted in\noutput space such that samples across domains share the same decision boundary,\nthough with feature discrepancy. Empirical results validate UACP under both\nopen-set and universal UDA settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunyun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Songcan Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-07-14T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}