{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.3","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-09-27T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"CSAGN: Conversational Structure Aware Graph Network for Conversational Semantic Role Labeling. (arXiv:2109.11541v1 [cs.CL])","link":"http://arxiv.org/abs/2109.11541","description":"<p>Conversational semantic role labeling (CSRL) is believed to be a crucial step\ntowards dialogue understanding. However, it remains a major challenge for\nexisting CSRL parser to handle conversational structural information. In this\npaper, we present a simple and effective architecture for CSRL which aims to\naddress this problem. Our model is based on a conversational structure-aware\ngraph network which explicitly encodes the speaker dependent information. We\nalso propose a multi-task learning method to further improve the model.\nExperimental results on benchmark datasets show that our model with our\nproposed training objectives significantly outperforms previous baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Han Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kun Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1\">Linqi Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Document Automation Architectures and Technologies: A Survey. (arXiv:2109.11603v1 [cs.CL])","link":"http://arxiv.org/abs/2109.11603","description":"<p>This paper surveys the current state of the art in document automation (DA).\nThe objective of DA is to reduce the manual effort during the generation of\ndocuments by automatically integrating input from different sources and\nassembling documents conforming to defined templates. There have been reviews\nof commercial solutions of DA, particularly in the legal domain, but to date\nthere has been no comprehensive review of the academic research on DA\narchitectures and technologies. The current survey of DA reviews the academic\nliterature and provides a clearer definition and characterization of DA and its\nfeatures, identifies state-of-the-art DA architectures and technologies in\nacademic research, and provides ideas that can lead to new research\nopportunities within the DA field in light of recent advances in artificial\nintelligence and deep neural networks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Achachlouei_M/0/1/0/all/0/1\">Mohammad Ahmadi Achachlouei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patil_O/0/1/0/all/0/1\">Omkar Patil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_T/0/1/0/all/0/1\">Tarun Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nair_V/0/1/0/all/0/1\">Vijayan N. Nair</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"iFacetSum: Coreference-based Interactive Faceted Summarization for Multi-Document Exploration. (arXiv:2109.11621v1 [cs.CL])","link":"http://arxiv.org/abs/2109.11621","description":"<p>We introduce iFacetSum, a web application for exploring topical document\nsets. iFacetSum integrates interactive summarization together with faceted\nsearch, by providing a novel faceted navigation scheme that yields abstractive\nsummaries for the user's selections. This approach offers both a comprehensive\noverview as well as concise details regarding subtopics of choice. Fine-grained\nfacets are automatically produced based on cross-document coreference\npipelines, rendering generic concepts, entities and statements surfacing in the\nsource texts. We analyze the effectiveness of our application through\nsmall-scale user studies, which suggest the usefulness of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hirsch_E/0/1/0/all/0/1\">Eran Hirsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eirew_A/0/1/0/all/0/1\">Alon Eirew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shapira_O/0/1/0/all/0/1\">Ori Shapira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caciularu_A/0/1/0/all/0/1\">Avi Caciularu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cattan_A/0/1/0/all/0/1\">Arie Cattan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ernst_O/0/1/0/all/0/1\">Ori Ernst</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasunuru_R/0/1/0/all/0/1\">Ramakanth Pasunuru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ronen_H/0/1/0/all/0/1\">Hadar Ronen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dagan_I/0/1/0/all/0/1\">Ido Dagan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting the Uniform Information Density Hypothesis. (arXiv:2109.11635v1 [cs.CL])","link":"http://arxiv.org/abs/2109.11635","description":"<p>The uniform information density (UID) hypothesis posits a preference among\nlanguage users for utterances structured such that information is distributed\nuniformly across a signal. While its implications on language production have\nbeen well explored, the hypothesis potentially makes predictions about language\ncomprehension and linguistic acceptability as well. Further, it is unclear how\nuniformity in a linguistic signal -- or lack thereof -- should be measured, and\nover which linguistic unit, e.g., the sentence or language level, this\nuniformity should hold. Here we investigate these facets of the UID hypothesis\nusing reading time and acceptability data. While our reading time results are\ngenerally consistent with previous work, they are also consistent with a weakly\nsuper-linear effect of surprisal, which would be compatible with UID's\npredictions. For acceptability judgments, we find clearer evidence that\nnon-uniformity in information density is predictive of lower acceptability. We\nthen explore multiple operationalizations of UID, motivated by different\ninterpretations of the original hypothesis, and analyze the scope over which\nthe pressure towards uniformity is exerted. The explanatory power of a subset\nof the proposed operationalizations suggests that the strongest trend may be a\nregression towards a mean surprisal across the language, rather than the\nphrase, sentence, or document -- a finding that supports a typical\ninterpretation of UID, namely that it is the byproduct of language users\nmaximizing the use of a (hypothetical) communication channel.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meister_C/0/1/0/all/0/1\">Clara Meister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pimentel_T/0/1/0/all/0/1\">Tiago Pimentel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haller_P/0/1/0/all/0/1\">Patrick Haller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jager_L/0/1/0/all/0/1\">Lena J&#xe4;ger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_R/0/1/0/all/0/1\">Roger Levy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simple and Effective Zero-shot Cross-lingual Phoneme Recognition. (arXiv:2109.11680v1 [cs.CL])","link":"http://arxiv.org/abs/2109.11680","description":"<p>Recent progress in self-training, self-supervised pretraining and\nunsupervised learning enabled well performing speech recognition systems\nwithout any labeled data. However, in many cases there is labeled data\navailable for related languages which is not utilized by these methods. This\npaper extends previous work on zero-shot cross-lingual transfer learning by\nfine-tuning a multilingually pretrained wav2vec 2.0 model to transcribe unseen\nlanguages. This is done by mapping phonemes of the training languages to the\ntarget language using articulatory features. Experiments show that this simple\nmethod significantly outperforms prior work which introduced task-specific\narchitectures and used only part of a monolingually pretrained model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qiantong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baevski_A/0/1/0/all/0/1\">Alexei Baevski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Auli_M/0/1/0/all/0/1\">Michael Auli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detect and Perturb: Neutral Rewriting of Biased and Sensitive Text via Gradient-based Decoding. (arXiv:2109.11708v1 [cs.CL])","link":"http://arxiv.org/abs/2109.11708","description":"<p>Written language carries explicit and implicit biases that can distract from\nmeaningful signals. For example, letters of reference may describe male and\nfemale candidates differently, or their writing style may indirectly reveal\ndemographic characteristics. At best, such biases distract from the meaningful\ncontent of the text; at worst they can lead to unfair outcomes. We investigate\nthe challenge of re-generating input sentences to 'neutralize' sensitive\nattributes while maintaining the semantic meaning of the original text (e.g. is\nthe candidate qualified?). We propose a gradient-based rewriting framework,\nDetect and Perturb to Neutralize (DEPEN), that first detects sensitive\ncomponents and masks them for regeneration, then perturbs the generation model\nat decoding time under a neutralizing constraint that pushes the (predicted)\ndistribution of sensitive attributes towards a uniform distribution. Our\nexperiments in two different scenarios show that DEPEN can regenerate fluent\nalternatives that are neutral in the sensitive attribute while maintaining the\nsemantics of other attributes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zexue He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majumder_B/0/1/0/all/0/1\">Bodhisattwa Prasad Majumder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1\">Julian McAuley</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AES Are Both Overstable And Oversensitive: Explaining Why And Proposing Defenses. (arXiv:2109.11728v1 [cs.CL])","link":"http://arxiv.org/abs/2109.11728","description":"<p>Deep-learning based Automatic Essay Scoring (AES) systems are being actively\nused by states and language testing agencies alike to evaluate millions of\ncandidates for life-changing decisions ranging from college applications to\nvisa approvals. However, little research has been put to understand and\ninterpret the black-box nature of deep-learning based scoring algorithms.\nPrevious studies indicate that scoring models can be easily fooled. In this\npaper, we explore the reason behind their surprising adversarial brittleness.\nWe utilize recent advances in interpretability to find the extent to which\nfeatures such as coherence, content, vocabulary, and relevance are important\nfor automated scoring mechanisms. We use this to investigate the\noversensitivity i.e., large change in output score with a little change in\ninput essay content) and overstability i.e., little change in output scores\nwith large changes in input essay content) of AES. Our results indicate that\nautoscoring models, despite getting trained as \"end-to-end\" models with rich\ncontextual embeddings such as BERT, behave like bag-of-words models. A few\nwords determine the essay score without the requirement of any context making\nthe model largely overstable. This is in stark contrast to recent probing\nstudies on pre-trained representation learning models, which show that rich\nlinguistic features such as parts-of-speech and morphology are encoded by them.\nFurther, we also find that the models have learnt dataset biases, making them\noversensitive. To deal with these issues, we propose detection-based protection\nmodels that can detect oversensitivity and overstability causing samples with\nhigh accuracies. We find that our proposed models are able to detect unusual\nattribution patterns and flag adversarial samples successfully.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singla_Y/0/1/0/all/0/1\">Yaman Kumar Singla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parekh_S/0/1/0/all/0/1\">Swapnil Parekh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Somesh Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Jessy Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1\">Rajiv Ratn Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Changyou Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DACT-BERT: Differentiable Adaptive Computation Time for an Efficient BERT Inference. (arXiv:2109.11745v1 [cs.CL])","link":"http://arxiv.org/abs/2109.11745","description":"<p>Large-scale pre-trained language models have shown remarkable results in\ndiverse NLP applications. Unfortunately, these performance gains have been\naccompanied by a significant increase in computation time and model size,\nstressing the need to develop new or complementary strategies to increase the\nefficiency of these models. In this paper we propose DACT-BERT, a\ndifferentiable adaptive computation time strategy for BERT-like models.\nDACT-BERT adds an adaptive computational mechanism to BERT's regular processing\npipeline, which controls the number of Transformer blocks that need to be\nexecuted at inference time. By doing this, the model learns to combine the most\nappropriate intermediate representations for the task at hand. Our experiments\ndemonstrate that our approach, when compared to the baselines, excels on a\nreduced computational regime and is competitive in other less restrictive ones.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Eyzaguirre_C/0/1/0/all/0/1\">Crist&#xf3;bal Eyzaguirre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rio_F/0/1/0/all/0/1\">Felipe del R&#xed;o</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Araujo_V/0/1/0/all/0/1\">Vladimir Araujo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soto_A/0/1/0/all/0/1\">&#xc1;lvaro Soto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lacking the embedding of a word? Look it up into a traditional dictionary. (arXiv:2109.11763v1 [cs.CL])","link":"http://arxiv.org/abs/2109.11763","description":"<p>Word embeddings are powerful dictionaries, which may easily capture language\nvariations. However, these dictionaries fail to give sense to rare words, which\nare surprisingly often covered by traditional dictionaries. In this paper, we\npropose to use definitions retrieved in traditional dictionaries to produce\nword embeddings for rare words. For this purpose, we introduce two methods:\nDefinition Neural Network (DefiNNet) and Define BERT (DefBERT). In our\nexperiments, DefiNNet and DefBERT significantly outperform state-of-the-art as\nwell as baseline methods devised for producing embeddings of unknown words. In\nfact, DefiNNet significantly outperforms FastText, which implements a method\nfor the same task-based on n-grams, and DefBERT significantly outperforms the\nBERT method for OOV words. Then, definitions in traditional dictionaries are\nuseful to build word embeddings for rare words.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ruzzetti_E/0/1/0/all/0/1\">Elena Sofia Ruzzetti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranaldi_L/0/1/0/all/0/1\">Leonardo Ranaldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mastromattei_M/0/1/0/all/0/1\">Michele Mastromattei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fallucchi_F/0/1/0/all/0/1\">Francesca Fallucchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zanzotto_F/0/1/0/all/0/1\">Fabio Massimo Zanzotto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dense Contrastive Visual-Linguistic Pretraining. (arXiv:2109.11778v1 [cs.CV])","link":"http://arxiv.org/abs/2109.11778","description":"<p>Inspired by the success of BERT, several multimodal representation learning\napproaches have been proposed that jointly represent image and text. These\napproaches achieve superior performance by capturing high-level semantic\ninformation from large-scale multimodal pretraining. In particular, LXMERT and\nUNITER adopt visual region feature regression and label classification as\npretext tasks. However, they tend to suffer from the problems of noisy labels\nand sparse semantic annotations, based on the visual features having been\npretrained on a crowdsourced dataset with limited and inconsistent semantic\nlabeling. To overcome these issues, we propose unbiased Dense Contrastive\nVisual-Linguistic Pretraining (DCVLP), which replaces the region regression and\nclassification with cross-modality region contrastive learning that requires no\nannotations. Two data augmentation strategies (Mask Perturbation and\nIntra-/Inter-Adversarial Perturbation) are developed to improve the quality of\nnegative samples used in contrastive learning. Overall, DCVLP allows\ncross-modality dense region contrastive learning in a self-supervised setting\nindependent of any object annotations. We compare our method against prior\nvisual-linguistic pretraining frameworks to validate the superiority of dense\ncontrastive learning on multimodal representation learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_L/0/1/0/all/0/1\">Lei Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shuang_K/0/1/0/all/0/1\">Kai Shuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_S/0/1/0/all/0/1\">Shijie Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Peng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1\">Zuohui Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melo_G/0/1/0/all/0/1\">Gerard de Melo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yunpeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_S/0/1/0/all/0/1\">Sen Su</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CPT: Colorful Prompt Tuning for Pre-trained Vision-Language Models. (arXiv:2109.11797v1 [cs.CV])","link":"http://arxiv.org/abs/2109.11797","description":"<p>Pre-Trained Vision-Language Models (VL-PTMs) have shown promising\ncapabilities in grounding natural language in image data, facilitating a broad\nvariety of cross-modal tasks. However, we note that there exists a significant\ngap between the objective forms of model pre-training and fine-tuning,\nresulting in a need for quantities of labeled data to stimulate the visual\ngrounding capability of VL-PTMs for downstream tasks. To address the challenge,\nwe present Cross-modal Prompt Tuning (CPT, alternatively, Colorful Prompt\nTuning), a novel paradigm for tuning VL-PTMs, which reformulates visual\ngrounding into a fill-in-the-blank problem with color-based co-referential\nmarkers in image and text, maximally mitigating the gap. In this way, our\nprompt tuning approach enables strong few-shot and even zero-shot visual\ngrounding capabilities of VL-PTMs. Comprehensive experimental results show that\nprompt tuned VL-PTMs outperform their fine-tuned counterparts by a large margin\n(e.g., 17.3% absolute accuracy improvement, and 73.8% relative standard\ndeviation reduction on average with one shot in RefCOCO evaluation). All the\ndata and code will be available to facilitate future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yuan Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Ao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhengyan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1\">Tat-Seng Chua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Does Knowledge Graph Embedding Extrapolate to Unseen Data: a Semantic Evidence View. (arXiv:2109.11800v1 [cs.CL])","link":"http://arxiv.org/abs/2109.11800","description":"<p>Knowledge Graph Embedding (KGE) aims to learn representations for entities\nand relations. Most KGE models have gained great success, especially on\nextrapolation scenarios. Specifically, given an unseen triple (h, r, t), a\ntrained model can still correctly predict t from (h, r, ?), or h from (?, r,\nt), such extrapolation ability is impressive. However, most existing KGE works\nfocus on the design of delicate triple modeling function, which mainly tell us\nhow to measure the plausibility of observed triples, but we have limited\nunderstanding of why the methods can extrapolate to unseen data, and what are\nthe important factors to help KGE extrapolate. Therefore in this work, we\nattempt to, from a data relevant view, study KGE extrapolation of two problems:\n1. How does KGE extrapolate to unseen data? 2. How to design the KGE model with\nbetter extrapolation ability? For the problem 1, we first discuss the impact\nfactors for extrapolation and from relation, entity and triple level\nrespectively, propose three Semantic Evidences (SEs), which can be observed\nfrom training set and provide important semantic information for extrapolation\nto unseen data. Then we verify the effectiveness of SEs through extensive\nexperiments on several typical KGE methods, and demonstrate that SEs serve as\nan important role for understanding the extrapolation ability of KGE. For the\nproblem 2, to make better use of the SE information for more extrapolative\nknowledge representation, we propose a novel GNN-based KGE model, called\nSemantic Evidence aware Graph Neural Network (SE-GNN). Finally, through\nextensive experiments on FB15k-237 and WN18RR datasets, we show that SE-GNN\nachieves state-of-the-art performance on Knowledge Graph Completion task and\nperform a better extrapolation ability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ren Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yanan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1\">Qiannan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_G/0/1/0/all/0/1\">Guanqun Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_F/0/1/0/all/0/1\">Fang Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qian Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Diversity-Enhanced and Constraints-Relaxed Augmentation for Low-Resource Classification. (arXiv:2109.11834v1 [cs.CL])","link":"http://arxiv.org/abs/2109.11834","description":"<p>Data augmentation (DA) aims to generate constrained and diversified data to\nimprove classifiers in Low-Resource Classification (LRC). Previous studies\nmostly use a fine-tuned Language Model (LM) to strengthen the constraints but\nignore the fact that the potential of diversity could improve the effectiveness\nof generated data. In LRC, strong constraints but weak diversity in DA result\nin the poor generalization ability of classifiers. To address this dilemma, we\npropose a {D}iversity-{E}nhanced and {C}onstraints-\\{R}elaxed {A}ugmentation\n(DECRA). Our DECRA has two essential components on top of a transformer-based\nbackbone model. 1) A k-beta augmentation, an essential component of DECRA, is\nproposed to enhance the diversity in generating constrained data. It expands\nthe changing scope and improves the degree of complexity of the generated data.\n2) A masked language model loss, instead of fine-tuning, is used as a\nregularization. It relaxes constraints so that the classifier can be trained\nwith more scattered generated data. The combination of these two components\ngenerates data that can reach or approach category boundaries and hence help\nthe classifier generalize better. We evaluate our DECRA on three public\nbenchmark datasets under low-resource settings. Extensive experiments\ndemonstrate that our DECRA outperforms state-of-the-art approaches by 3.8% in\nthe overall score.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Guang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hailong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1\">Yuzhao Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1\">Weiguo Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jianping Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robustness and Sensitivity of BERT Models Predicting Alzheimer's Disease from Text. (arXiv:2109.11888v1 [cs.CL])","link":"http://arxiv.org/abs/2109.11888","description":"<p>Understanding robustness and sensitivity of BERT models predicting\nAlzheimer's disease from text is important for both developing better\nclassification models and for understanding their capabilities and limitations.\nIn this paper, we analyze how a controlled amount of desired and undesired text\nalterations impacts performance of BERT. We show that BERT is robust to natural\nlinguistic variations in text. On the other hand, we show that BERT is not\nsensitive to removing clinically important information from text.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Novikova_J/0/1/0/all/0/1\">Jekaterina Novikova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Crowd Sourcing for Semantic Similarity. (arXiv:2109.11969v1 [cs.CL])","link":"http://arxiv.org/abs/2109.11969","description":"<p>Estimation of semantic similarity is crucial for a variety of natural\nlanguage processing (NLP) tasks. In the absence of a general theory of semantic\ninformation, many papers rely on human annotators as the source of ground truth\nfor semantic similarity estimation. This paper investigates the ambiguities\ninherent in crowd-sourced semantic labeling. It shows that annotators that\ntreat semantic similarity as a binary category (two sentences are either\nsimilar or not similar and there is no middle ground) play the most important\nrole in the labeling. The paper offers heuristics to filter out unreliable\nannotators and stimulates further discussions on human perception of semantic\nsimilarity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Solomon_S/0/1/0/all/0/1\">Shaul Solomon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohn_A/0/1/0/all/0/1\">Adam Cohn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosenblum_H/0/1/0/all/0/1\">Hernan Rosenblum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hershkovitz_C/0/1/0/all/0/1\">Chezi Hershkovitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamshchikov_I/0/1/0/all/0/1\">Ivan P. Yamshchikov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Separating Retention from Extraction in the Evaluation of End-to-end Relation Extraction. (arXiv:2109.12008v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12008","description":"<p>State-of-the-art NLP models can adopt shallow heuristics that limit their\ngeneralization capability (McCoy et al., 2019). Such heuristics include lexical\noverlap with the training set in Named-Entity Recognition (Taill\\'e et al.,\n2020) and Event or Type heuristics in Relation Extraction (Rosenman et al.,\n2020). In the more realistic end-to-end RE setting, we can expect yet another\nheuristic: the mere retention of training relation triples. In this paper, we\npropose several experiments confirming that retention of known facts is a key\nfactor of performance on standard benchmarks. Furthermore, one experiment\nsuggests that a pipeline model able to use intermediate type representations is\nless prone to over-rely on retention.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Taille_B/0/1/0/all/0/1\">Bruno Taill&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guigue_V/0/1/0/all/0/1\">Vincent Guigue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scoutheeten_G/0/1/0/all/0/1\">Geoffrey Scoutheeten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gallinari_P/0/1/0/all/0/1\">Patrick Gallinari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Translation of German--Lower Sorbian: Exploring Training and Novel Transfer Methods on a Low-Resource Language. (arXiv:2109.12012v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12012","description":"<p>This paper describes the methods behind the systems submitted by the\nUniversity of Groningen for the WMT 2021 Unsupervised Machine Translation task\nfor German--Lower Sorbian (DE--DSB): a high-resource language to a low-resource\none. Our system uses a transformer encoder-decoder architecture in which we\nmake three changes to the standard training procedure. First, our training\nfocuses on two languages at a time, contrasting with a wealth of research on\nmultilingual systems. Second, we introduce a novel method for initializing the\nvocabulary of an unseen language, achieving improvements of 3.2 BLEU for\nDE$\\rightarrow$DSB and 4.0 BLEU for DSB$\\rightarrow$DE. Lastly, we experiment\nwith the order in which offline and online back-translation are used to train\nan unsupervised system, finding that using online back-translation first works\nbetter for DE$\\rightarrow$DSB by 2.76 BLEU. Our submissions ranked first (tied\nwith another team) for DSB$\\rightarrow$DE and third for DE$\\rightarrow$DSB.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Edman_L/0/1/0/all/0/1\">Lukas Edman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ustun_A/0/1/0/all/0/1\">Ahmet &#xdc;st&#xfc;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toral_A/0/1/0/all/0/1\">Antonio Toral</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noord_G/0/1/0/all/0/1\">Gertjan van Noord</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Indirectly Supervised English Sentence Break Prediction Using Paragraph Break Probability Estimates. (arXiv:2109.12023v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12023","description":"<p>This report explores the use of paragraph break probability estimates to help\npredict the location of sentence breaks in English natural language text. We\nshow that a sentence break predictor based almost solely on paragraph break\nprobability estimates can achieve high accuracy on this task. This sentence\nbreak predictor is trained almost entirely on a large amount of naturally\noccurring text without sentence break annotations, with only a small amount of\nannotated data needed to tune two hyperparameters. We also show that even\nbetter results can be achieved across in-domain and out-of-domain test data, if\nparagraph break probability signals are combined with a support vector machine\nclassifier trained on a somewhat larger amount of sentence-break-annotated\ndata. Numerous related issues are addressed along the way.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moore_R/0/1/0/all/0/1\">Robert C. Moore</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating Post-pretraining Representation Alignment for Cross-Lingual Question Answering. (arXiv:2109.12028v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12028","description":"<p>Human knowledge is collectively encoded in the roughly 6500 languages spoken\naround the world, but it is not distributed equally across languages. Hence,\nfor information-seeking question answering (QA) systems to adequately serve\nspeakers of all languages, they need to operate cross-lingually. In this work\nwe investigate the capabilities of multilingually pre-trained language models\non cross-lingual QA. We find that explicitly aligning the representations\nacross languages with a post-hoc fine-tuning step generally leads to improved\nperformance. We additionally investigate the effect of data size as well as the\nlanguage choice in this fine-tuning step, also releasing a dataset for\nevaluating cross-lingual QA systems. Code and dataset are publicly available\nhere: https://github.com/ffaisal93/aligned_qa\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Faisal_F/0/1/0/all/0/1\">Fahim Faisal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anastasopoulos_A/0/1/0/all/0/1\">Antonios Anastasopoulos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformers Generalize Linearly. (arXiv:2109.12036v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12036","description":"<p>Natural language exhibits patterns of hierarchically governed dependencies,\nin which relations between words are sensitive to syntactic structure rather\nthan linear ordering. While re-current network models often fail to generalize\nin a hierarchically sensitive way (McCoy et al.,2020) when trained on ambiguous\ndata, the improvement in performance of newer Trans-former language models\n(Vaswani et al., 2017)on a range of syntactic benchmarks trained on large data\nsets (Goldberg, 2019; Warstadtet al., 2019) opens the question of whether these\nmodels might exhibit hierarchical generalization in the face of impoverished\ndata.In this paper we examine patterns of structural generalization for\nTransformer sequence-to-sequence models and find that not only do Transformers\nfail to generalize hierarchically across a wide variety of grammatical mapping\ntasks, but they exhibit an even stronger preference for linear generalization\nthan comparable recurrent networks\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Petty_J/0/1/0/all/0/1\">Jackson Petty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frank_R/0/1/0/all/0/1\">Robert Frank</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Monolingual and Cross-Lingual Acceptability Judgments with the Italian CoLA corpus. (arXiv:2109.12053v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12053","description":"<p>The development of automated approaches to linguistic acceptability has been\ngreatly fostered by the availability of the English CoLA corpus, which has also\nbeen included in the widely used GLUE benchmark. However, this kind of research\nfor languages other than English, as well as the analysis of cross-lingual\napproaches, has been hindered by the lack of resources with a comparable size\nin other languages. We have therefore developed the ItaCoLA corpus, containing\nalmost 10,000 sentences with acceptability judgments, which has been created\nfollowing the same approach and the same steps as the English one. In this\npaper we describe the corpus creation, we detail its content, and we present\nthe first experiments on this new resource. We compare in-domain and\nout-of-domain classification, and perform a specific evaluation of nine\nlinguistic phenomena. We also present the first cross-lingual experiments,\naimed at assessing whether multilingual transformerbased approaches can benefit\nfrom using sentences in two languages during fine-tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Trotta_D/0/1/0/all/0/1\">Daniela Trotta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guarasci_R/0/1/0/all/0/1\">Raffaele Guarasci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leonardelli_E/0/1/0/all/0/1\">Elisa Leonardelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tonelli_S/0/1/0/all/0/1\">Sara Tonelli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AraT5: Text-to-Text Transformers for Arabic Language Understanding and Generation. (arXiv:2109.12068v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12068","description":"<p>Transfer learning with a unified Transformer framework (T5) that converts all\nlanguage problems into a text-to-text format has recently been proposed as a\nsimple, yet effective, transfer learning approach. Although a multilingual\nversion of the T5 model (mT5) has been introduced, it is not clear how well it\ncan fare on non-English tasks involving diverse data. To investigate this\nquestion, we apply mT5 on a language with a wide variety of dialects--Arabic.\nFor evaluation, we use an existing benchmark for Arabic language understanding\nand introduce a new benchmark for Arabic language generation (ARGEN). We also\npre-train three powerful Arabic-specific text-to-text Transformer based models\nand evaluate them on the two benchmarks. Our new models perform significantly\nbetter than mT5 and exceed MARBERT, the current state-of-the-art Arabic\nBERT-based model, on Arabic language understanding. The models also set new\nSOTA on the generation benchmark. Our new models and are publicly released at\nhttps://github.com/UBC-NLP/araT5 and ARLGE will be released through the same\nrepository.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nagoudi_E/0/1/0/all/0/1\">El Moatez Billah Nagoudi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdul_Mageed_M/0/1/0/all/0/1\">Muhammad Abdul-Mageed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elmadany_A/0/1/0/all/0/1\">AbdelRahim Elmadany</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SD-QA: Spoken Dialectal Question Answering for the Real World. (arXiv:2109.12072v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12072","description":"<p>Question answering (QA) systems are now available through numerous commercial\napplications for a wide variety of domains, serving millions of users that\ninteract with them via speech interfaces. However, current benchmarks in QA\nresearch do not account for the errors that speech recognition models might\nintroduce, nor do they consider the language variations (dialects) of the\nusers. To address this gap, we augment an existing QA dataset to construct a\nmulti-dialect, spoken QA benchmark on five languages (Arabic, Bengali, English,\nKiswahili, Korean) with more than 68k audio prompts in 24 dialects from 255\nspeakers. We provide baseline results showcasing the real-world performance of\nQA systems and analyze the effect of language variety and other sensitive\nspeaker attributes on downstream performance. Last, we study the fairness of\nthe ASR and QA models with respect to the underlying user populations. The\ndataset, model outputs, and code for reproducing all our experiments are\navailable: https://github.com/ffaisal93/SD-QA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Faisal_F/0/1/0/all/0/1\">Fahim Faisal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keshava_S/0/1/0/all/0/1\">Sharlina Keshava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alam_M/0/1/0/all/0/1\">Md Mahfuz ibn Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anastasopoulos_A/0/1/0/all/0/1\">Antonios Anastasopoulos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Progressive Adversarial Learning for Bootstrapping: A Case Study on Entity Set Expansion. (arXiv:2109.12082v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12082","description":"<p>Bootstrapping has become the mainstream method for entity set expansion.\nConventional bootstrapping methods mostly define the expansion boundary using\nseed-based distance metrics, which heavily depend on the quality of selected\nseeds and are hard to be adjusted due to the extremely sparse supervision. In\nthis paper, we propose BootstrapGAN, a new learning method for bootstrapping\nwhich jointly models the bootstrapping process and the boundary learning\nprocess in a GAN framework. Specifically, the expansion boundaries of different\nbootstrapping iterations are learned via different discriminator networks; the\nbootstrapping network is the generator to generate new positive entities, and\nthe discriminator networks identify the expansion boundaries by trying to\ndistinguish the generated entities from known positive entities. By iteratively\nperforming the above adversarial learning, the generator and the discriminators\ncan reinforce each other and be progressively refined along the whole\nbootstrapping process. Experiments show that BootstrapGAN achieves the new\nstate-of-the-art entity set expansion performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_L/0/1/0/all/0/1\">Lingyong Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xianpei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Le Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text-based NP Enrichment. (arXiv:2109.12085v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12085","description":"<p>Understanding the relations between entities denoted by NPs in text is a\ncritical part of human-like natural language understanding. However, only a\nfraction of such relations is covered by NLP tasks and models nowadays. In this\nwork, we establish the task of text-based NP enrichment (TNE), that is,\nenriching each NP with all the preposition-mediated relations that hold between\nthis and the other NPs in the text. The relations are represented as triplets,\neach denoting two NPs linked via a preposition. Humans recover such relations\nseamlessly, while current state-of-the-art models struggle with them due to the\nimplicit nature of the problem. We build the first large-scale dataset for the\nproblem, provide the formal framing and scope of annotation, analyze the data,\nand report the result of fine-tuned neural language models on the task,\ndemonstrating the challenge it poses to current technology. We created a\nwebpage with the data, data-exploration UI, code, models, and demo to foster\nfurther research into this challenging text understanding problem at\nyanaiela.github.io/TNE/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Elazar_Y/0/1/0/all/0/1\">Yanai Elazar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basmov_V/0/1/0/all/0/1\">Victoria Basmov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsarfaty_R/0/1/0/all/0/1\">Reut Tsarfaty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SAIS: Supervising and Augmenting Intermediate Steps for Document-Level Relation Extraction. (arXiv:2109.12093v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12093","description":"<p>Stepping from sentence-level to document-level relation extraction, the\nresearch community confronts increasing text length and more complicated entity\ninteractions. Consequently, it is more challenging to encode the key sources of\ninformation--relevant contexts and entity types. However, existing methods only\nimplicitly learn to model these critical information sources while being\ntrained for relation extraction. As a result, they suffer the problems of\nineffective supervision and uninterpretable model predictions. In contrast, we\npropose to explicitly teach the model to capture relevant contexts and entity\ntypes by supervising and augmenting intermediate steps (SAIS) for relation\nextraction. Based on a broad spectrum of carefully designed tasks, our proposed\nSAIS method not only extracts relations of better quality due to more effective\nsupervision, but also retrieves the corresponding supporting evidence more\naccurately so as to enhance interpretability. By assessing model uncertainty,\nSAIS further boosts the performance via evidence-based data augmentation and\nensemble inference while reducing the computational cost. Eventually, SAIS\ndelivers state-of-the-art relation extraction results on three benchmarks\n(DocRED, CDR, and GDA) and achieves 5.04% relative gains in F1 score compared\nto the runner-up in evidence retrieval on DocRED.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yuxin Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zecheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1\">Yuning Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Carl Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLIPort: What and Where Pathways for Robotic Manipulation. (arXiv:2109.12098v1 [cs.RO])","link":"http://arxiv.org/abs/2109.12098","description":"<p>How can we imbue robots with the ability to manipulate objects precisely but\nalso to reason about them in terms of abstract concepts? Recent works in\nmanipulation have shown that end-to-end networks can learn dexterous skills\nthat require precise spatial reasoning, but these methods often fail to\ngeneralize to new goals or quickly learn transferable concepts across tasks. In\nparallel, there has been great progress in learning generalizable semantic\nrepresentations for vision and language by training on large-scale internet\ndata, however these representations lack the spatial understanding necessary\nfor fine-grained manipulation. To this end, we propose a framework that\ncombines the best of both worlds: a two-stream architecture with semantic and\nspatial pathways for vision-based manipulation. Specifically, we present\nCLIPort, a language-conditioned imitation-learning agent that combines the\nbroad semantic understanding (what) of CLIP [1] with the spatial precision\n(where) of Transporter [2]. Our end-to-end framework is capable of solving a\nvariety of language-specified tabletop tasks from packing unseen objects to\nfolding cloths, all without any explicit representations of object poses,\ninstance segmentations, memory, symbolic states, or syntactic structures.\nExperiments in simulated and real-world settings show that our approach is data\nefficient in few-shot settings and generalizes effectively to seen and unseen\nsemantic concepts. We even learn one multi-task policy for 10 simulated and 9\nreal-world tasks that is better or comparable to single-task policies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shridhar_M/0/1/0/all/0/1\">Mohit Shridhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manuelli_L/0/1/0/all/0/1\">Lucas Manuelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fox_D/0/1/0/all/0/1\">Dieter Fox</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GERNERMED -- An Open German Medical NER Model. (arXiv:2109.12104v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12104","description":"<p>The current state of adoption of well-structured electronic health records\nand integration of digital methods for storing medical patient data in\nstructured formats can often considered as inferior compared to the use of\ntraditional, unstructured text based patient data documentation. Data mining in\nthe field of medical data analysis often needs to rely solely on processing of\nunstructured data to retrieve relevant data. In natural language processing\n(NLP), statistical models have been shown successful in various tasks like\npart-of-speech tagging, relation extraction (RE) and named entity recognition\n(NER). In this work, we present GERNERMED, the first open, neural NLP model for\nNER tasks dedicated to detect medical entity types in German text data. Here,\nwe avoid the conflicting goals of protection of sensitive patient data from\ntraining data extraction and the publication of the statistical model weights\nby training our model on a custom dataset that was translated from publicly\navailable datasets in foreign language by a pretrained neural machine\ntranslation model. The sample code and the statistical model is available at:\nhttps://github.com/frankkramer-lab/GERNERMED\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Frei_J/0/1/0/all/0/1\">Johann Frei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kramer_F/0/1/0/all/0/1\">Frank Kramer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Faithful Target Attribute Prediction in Neural Machine Translation. (arXiv:2109.12105v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12105","description":"<p>The training data used in NMT is rarely controlled with respect to specific\nattributes, such as word casing or gender, which can cause errors in\ntranslations. We argue that predicting the target word and attributes\nsimultaneously is an effective way to ensure that translations are more\nfaithful to the training data distribution with respect to these attributes.\nExperimental results on two tasks, uppercased input translation and gender\nprediction, show that this strategy helps mirror the training data distribution\nin testing. It also facilitates data augmentation on the task of uppercased\ninput translation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Niu_X/0/1/0/all/0/1\">Xing Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dinu_G/0/1/0/all/0/1\">Georgiana Dinu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathur_P/0/1/0/all/0/1\">Prashant Mathur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Currey_A/0/1/0/all/0/1\">Anna Currey</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Source Code Search: A Study of the Past and a Glimpse at the Future. (arXiv:1908.06738v2 [cs.SE] UPDATED)","link":"http://arxiv.org/abs/1908.06738","description":"<p>With the recent explosion in the size and complexity of source codebases and\nsoftware projects, the need for efficient source code search engines has\nincreased dramatically. Unfortunately, existing information retrieval-based\nmethods fail to capture the query semantics and perform well only when the\nquery contains syntax-based keywords. Consequently, such methods will perform\npoorly when given high-level natural language queries. In this paper, we review\nexisting methods for building code search engines. We also outline the open\nresearch directions and the various obstacles that stand in the way of having a\nuniversal source code search engine.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khalifa_M/0/1/0/all/0/1\">Muhammad Khalifa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SOLID: A Large-Scale Semi-Supervised Dataset for Offensive Language Identification. (arXiv:2004.14454v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2004.14454","description":"<p>The widespread use of offensive content in social media has led to an\nabundance of research in detecting language such as hate speech, cyberbullying,\nand cyber-aggression. Recent work presented the OLID dataset, which follows a\ntaxonomy for offensive language identification that provides meaningful\ninformation for understanding the type and the target of offensive messages.\nHowever, it is limited in size and it might be biased towards offensive\nlanguage as it was collected using keywords. In this work, we present SOLID, an\nexpanded dataset, where the tweets were collected in a more principled manner.\nSOLID contains over nine million English tweets labeled in a semi-supervised\nfashion. We demonstrate that using SOLID along with OLID yields sizable\nperformance gains on the OLID test set for two different models, especially for\nthe lower levels of the taxonomy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rosenthal_S/0/1/0/all/0/1\">Sara Rosenthal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atanasova_P/0/1/0/all/0/1\">Pepa Atanasova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karadzhov_G/0/1/0/all/0/1\">Georgi Karadzhov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zampieri_M/0/1/0/all/0/1\">Marcos Zampieri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ValNorm Quantifies Semantics to Reveal Consistent Valence Biases Across Languages and Over Centuries. (arXiv:2006.03950v4 [cs.CY] UPDATED)","link":"http://arxiv.org/abs/2006.03950","description":"<p>Word embeddings learn implicit biases from linguistic regularities captured\nby word co-occurrence statistics. By extending methods that quantify human-like\nbiases in word embeddings, we introduceValNorm, a novel intrinsic evaluation\ntask and method to quantify the valence dimension of affect in human-rated word\nsets from social psychology. We apply ValNorm on static word embeddings from\nseven languages (Chinese, English, German, Polish, Portuguese, Spanish, and\nTurkish) and from historical English text spanning 200 years. ValNorm achieves\nconsistently high accuracy in quantifying the valence of non-discriminatory,\nnon-social group word sets. Specifically, ValNorm achieves a Pearson\ncorrelation of r=0.88 for human judgment scores of valence for 399 words\ncollected to establish pleasantness norms in English. In contrast, we measure\ngender stereotypes using the same set of word embeddings and find that social\nbiases vary across languages. Our results indicate that valence associations of\nnon-discriminatory, non-social group words represent widely-shared\nassociations, in seven languages and over 200 years.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Toney_Wails_A/0/1/0/all/0/1\">Autumn Toney-Wails</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caliskan_A/0/1/0/all/0/1\">Aylin Caliskan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Calling Out Bluff: Evaluation Toolkit For Robustness Testing Of Automatic Essay Scoring Systems. (arXiv:2007.06796v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2007.06796","description":"<p>Automatic scoring engines have been used for scoring approximately fifteen\nmillion test-takers in just the last three years. This number is increasing\nfurther due to COVID-19 and the associated automation of education and testing.\nDespite such wide usage, the AI-based testing literature of these \"intelligent\"\nmodels is highly lacking. Most of the papers proposing new models rely only on\nquadratic weighted kappa (QWK) based agreement with human raters for showing\nmodel efficacy. However, this effectively ignores the highly multi-feature\nnature of essay scoring. Essay scoring depends on features like coherence,\ngrammar, relevance, sufficiency and, vocabulary. To date, there has been no\nstudy testing Automated Essay Scoring: AES systems holistically on all these\nfeatures. With this motivation, we propose a model agnostic adversarial\nevaluation scheme and associated metrics for AES systems to test their natural\nlanguage understanding capabilities and overall robustness. We evaluate the\ncurrent state-of-the-art AES models using the proposed scheme and report the\nresults on five recent models. These models range from\nfeature-engineering-based approaches to the latest deep learning algorithms. We\nfind that AES models are highly overstable. Even heavy modifications(as much as\n25%) with content unrelated to the topic of the questions do not decrease the\nscore produced by the models. On the other hand, irrelevant content, on\naverage, increases the scores, thus showing that the model evaluation strategy\nand rubrics should be reconsidered. We also ask 200 human raters to score both\nan original and adversarial response to seeing if humans can detect differences\nbetween the two and whether they agree with the scores assigned by auto scores.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kabra_A/0/1/0/all/0/1\">Anubha Kabra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatia_M/0/1/0/all/0/1\">Mehar Bhatia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_Y/0/1/0/all/0/1\">Yaman Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jessy Junyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_D/0/1/0/all/0/1\">Di Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1\">Rajiv Ratn Shah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OpenAttack: An Open-source Textual Adversarial Attack Toolkit. (arXiv:2009.09191v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2009.09191","description":"<p>Textual adversarial attacking has received wide and increasing attention in\nrecent years. Various attack models have been proposed, which are enormously\ndistinct and implemented with different programming frameworks and settings.\nThese facts hinder quick utilization and fair comparison of attack models. In\nthis paper, we present an open-source textual adversarial attack toolkit named\nOpenAttack to solve these issues. Compared with existing other textual\nadversarial attack toolkits, OpenAttack has its unique strengths in support for\nall attack types, multilinguality, and parallel processing. Currently,\nOpenAttack includes 15 typical attack models that cover all attack types. Its\nhighly inclusive modular design not only supports quick utilization of existing\nattack models, but also enables great flexibility and extensibility. OpenAttack\nhas broad uses including comparing and evaluating attack models, measuring\nrobustness of a model, assisting in developing new attack models, and\nadversarial training. Source code and documentation can be obtained at\nhttps://github.com/thunlp/OpenAttack.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_G/0/1/0/all/0/1\">Guoyang Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_F/0/1/0/all/0/1\">Fanchao Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qianrui Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tingji Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zixian Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_B/0/1/0/all/0/1\">Bairu Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zang_Y/0/1/0/all/0/1\">Yuan Zang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Char2Subword: Extending the Subword Embedding Space Using Robust Character Compositionality. (arXiv:2010.12730v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.12730","description":"<p>Byte-pair encoding (BPE) is a ubiquitous algorithm in the subword\ntokenization process of language models as it provides multiple benefits.\nHowever, this process is solely based on pre-training data statistics, making\nit hard for the tokenizer to handle infrequent spellings. On the other hand,\nthough robust to misspellings, pure character-level models often lead to\nunreasonably long sequences and make it harder for the model to learn\nmeaningful words. To alleviate these challenges, we propose a character-based\nsubword module (char2subword) that learns the subword embedding table in\npre-trained models like BERT. Our char2subword module builds representations\nfrom characters out of the subword vocabulary, and it can be used as a drop-in\nreplacement of the subword embedding table. The module is robust to\ncharacter-level alterations such as misspellings, word inflection, casing, and\npunctuation. We integrate it further with BERT through pre-training while\nkeeping BERT transformer parameters fixed--and thus, providing a practical\nmethod. Finally, we show that incorporating our module to mBERT significantly\nimproves the performance on the social media linguistic code-switching\nevaluation (LinCE) benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aguilar_G/0/1/0/all/0/1\">Gustavo Aguilar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCann_B/0/1/0/all/0/1\">Bryan McCann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_T/0/1/0/all/0/1\">Tong Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajani_N/0/1/0/all/0/1\">Nazneen Rajani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keskar_N/0/1/0/all/0/1\">Nitish Keskar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Solorio_T/0/1/0/all/0/1\">Thamar Solorio</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Sequence-to-Sequence Pre-training via Sequence Span Rewriting. (arXiv:2101.00416v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2101.00416","description":"<p>In this paper, we generalize text infilling (e.g., masked language models) by\nproposing Sequence Span Rewriting (SSR) as a self-supervised\nsequence-to-sequence (seq2seq) pre-training objective. SSR provides more\nfine-grained learning signals for text representations by supervising the model\nto rewrite imperfect spans to ground truth, and it is more consistent than text\ninfilling with many downstream seq2seq tasks that rewrite a source sentences\ninto a target sentence. Our experiments with T5 models on various seq2seq tasks\nshow that SSR can substantially improve seq2seq pre-training. Moreover, we\nobserve SSR is especially helpful to improve pre-training a small-size seq2seq\nmodel with a powerful imperfect span generator, which indicates a new\nperspective of transferring knowledge from a large model to a smaller model for\nseq2seq pre-training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wangchunshu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_T/0/1/0/all/0/1\">Tao Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Canwen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Ke Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Advances and Challenges in Conversational Recommender Systems: A Survey. (arXiv:2101.09459v7 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2101.09459","description":"<p>Recommender systems exploit interaction history to estimate user preference,\nhaving been heavily used in a wide range of industry applications. However,\nstatic recommendation models are difficult to answer two important questions\nwell due to inherent shortcomings: (a) What exactly does a user like? (b) Why\ndoes a user like an item? The shortcomings are due to the way that static\nmodels learn user preference, i.e., without explicit instructions and active\nfeedback from users. The recent rise of conversational recommender systems\n(CRSs) changes this situation fundamentally. In a CRS, users and the system can\ndynamically communicate through natural language interactions, which provide\nunprecedented opportunities to explicitly obtain the exact preference of users.\n</p>\n<p>Considerable efforts, spread across disparate settings and applications, have\nbeen put into developing CRSs. Existing models, technologies, and evaluation\nmethods for CRSs are far from mature. In this paper, we provide a systematic\nreview of the techniques used in current CRSs. We summarize the key challenges\nof developing CRSs in five directions: (1) Question-based user preference\nelicitation. (2) Multi-turn conversational recommendation strategies. (3)\nDialogue understanding and generation. (4) Exploitation-exploration trade-offs.\n(5) Evaluation and user simulation. These research directions involve multiple\nresearch fields like information retrieval (IR), natural language processing\n(NLP), and human-computer interaction (HCI). Based on these research\ndirections, we discuss some future challenges and opportunities. We provide a\nroad map for researchers from multiple communities to get started in this area.\nWe hope this survey can help to identify and address challenges in CRSs and\ninspire future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Chongming Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_W/0/1/0/all/0/1\">Wenqiang Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiangnan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rijke_M/0/1/0/all/0/1\">Maarten de Rijke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1\">Tat-Seng Chua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Statistically significant detection of semantic shifts using contextual word embeddings. (arXiv:2104.03776v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.03776","description":"<p>Detecting lexical semantic change in smaller data sets, e.g. in historical\nlinguistics and digital humanities, is challenging due to a lack of statistical\npower. This issue is exacerbated by non-contextual embedding models that\nproduce one embedding per word and, therefore, mask the variability present in\nthe data. In this article, we propose an approach to estimate semantic shift by\ncombining contextual word embeddings with permutation-based statistical tests.\nWe use the false discovery rate procedure to address the large number of\nhypothesis tests being conducted simultaneously. We demonstrate the performance\nof this approach in simulation where it achieves consistently high precision by\nsuppressing false positives. We additionally analyze real-world data from\nSemEval-2020 Task 1 and the Liverpool FC subreddit corpus. We show that by\ntaking sample variation into account, we can improve the robustness of\nindividual semantic shift estimates without degrading overall performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Medlar_A/0/1/0/all/0/1\">Alan Medlar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glowacka_D/0/1/0/all/0/1\">Dorota Glowacka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bilingual Alignment Pre-Training for Zero-Shot Cross-Lingual Transfer. (arXiv:2106.01732v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.01732","description":"<p>Multilingual pre-trained models have achieved remarkable performance on\ncross-lingual transfer learning. Some multilingual models such as mBERT, have\nbeen pre-trained on unlabeled corpora, therefore the embeddings of different\nlanguages in the models may not be aligned very well. In this paper, we aim to\nimprove the zero-shot cross-lingual transfer performance by proposing a\npre-training task named Word-Exchange Aligning Model (WEAM), which uses the\nstatistical alignment information as the prior knowledge to guide cross-lingual\nword prediction. We evaluate our model on multilingual machine reading\ncomprehension task MLQA and natural language interface task XNLI. The results\nshow that WEAM can significantly improve the zero-shot performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Ziqing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1\">Wentao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Yiming Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jiani Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_W/0/1/0/all/0/1\">Wanxiang Che</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shijin Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learned Token Pruning for Transformers. (arXiv:2107.00910v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.00910","description":"<p>Deploying transformer models in practice is challenging due to their\ninference cost, which scales quadratically with input sequence length. To\naddress this, we present a novel Learned Token Pruning (LTP) method which\nadaptively removes unimportant tokens as an input sequence passes through\ntransformer layers. In particular, LTP prunes tokens with an attention score\nbelow a threshold value which is learned for each layer during training. Our\nthreshold-based method allows the length of the pruned sequence to vary\nadaptively based on the input sequence, and avoids algorithmically expensive\noperations such as top-k token selection. We extensively test the performance\nof LTP on GLUE tasks and show that our method outperforms the prior\nstate-of-the-art token pruning methods by up to ~2.5% higher accuracy with the\nsame amount of FLOPs. In particular, LTP achieves up to 2.1x FLOPs reduction\nwith less than 1% accuracy drop, which results in up to 1.9x and 2.0x\nthroughput improvement on Intel Haswell CPUs and NVIDIA V100 GPUs,\nrespectively. Furthermore, we demonstrate that LTP is more robust than prior\nmethods to variations on input sentence lengths. Our code has been developed in\nPyTorch and has been open-sourced.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sehoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1\">Sheng Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thorsley_D/0/1/0/all/0/1\">David Thorsley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gholami_A/0/1/0/all/0/1\">Amir Gholami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwon_W/0/1/0/all/0/1\">Woosuk Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassoun_J/0/1/0/all/0/1\">Joseph Hassoun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1\">Kurt Keutzer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End Natural Language Understanding Pipeline for Bangla Conversational Agent. (arXiv:2107.05541v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.05541","description":"<p>Chatbots are intelligent software built to be used as a replacement for human\ninteraction. Existing studies typically do not provide enough support for\nlow-resource languages like Bangla. Due to the increasing popularity of social\nmedia, we can also see the rise of interactions in Bangla transliteration\n(mostly in English) among the native Bangla speakers. In this paper, we propose\na novel approach to build a Bangla chatbot aimed to be used as a business\nassistant which can communicate in Bangla and Bangla Transliteration in English\nwith high confidence consistently. Since annotated data was not available for\nthis purpose, we had to work on the whole machine learning life cycle (data\npreparation, machine learning modeling, and model deployment) using Rasa Open\nSource Framework, fastText embeddings, Polyglot embeddings, Flask, and other\nsystems as building blocks. While working with the skewed annotated dataset, we\ntry out different setups and pipelines to evaluate which works best and provide\npossible reasoning behind the observed results. Finally, we present a pipeline\nfor intent classification and entity extraction which achieves reasonable\nperformance (accuracy: 83.02%, precision: 80.82%, recall: 83.02%, F1-score:\n80%).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1\">Fahim Shahriar Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mushabbir_M/0/1/0/all/0/1\">Mueeze Al Mushabbir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Irbaz_M/0/1/0/all/0/1\">Mohammad Sabik Irbaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasim_M/0/1/0/all/0/1\">MD Abdullah Al Nasim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Summary Explorer: Visualizing the State of the Art in Text Summarization. (arXiv:2108.01879v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.01879","description":"<p>This paper introduces Summary Explorer, a new tool to support the manual\ninspection of text summarization systems by compiling the outputs of\n55~state-of-the-art single document summarization approaches on three benchmark\ndatasets, and visually exploring them during a qualitative assessment. The\nunderlying design of the tool considers three well-known summary quality\ncriteria (coverage, faithfulness, and position bias), encapsulated in a guided\nassessment based on tailored visualizations. The tool complements existing\napproaches for locally debugging summarization models and improves upon them.\nThe tool is available at https://tldr.webis.de/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Syed_S/0/1/0/all/0/1\">Shahbaz Syed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yousef_T/0/1/0/all/0/1\">Tariq Yousef</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Khatib_K/0/1/0/all/0/1\">Khalid Al-Khatib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Janicke_S/0/1/0/all/0/1\">Stefan J&#xe4;nicke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potthast_M/0/1/0/all/0/1\">Martin Potthast</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sequence Level Contrastive Learning for Text Summarization. (arXiv:2109.03481v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.03481","description":"<p>Contrastive learning models have achieved great success in unsupervised\nvisual representation learning, which maximize the similarities between feature\nrepresentations of different views of the same image, while minimize the\nsimilarities between feature representations of views of different images. In\ntext summarization, the output summary is a shorter form of the input document\nand they have similar meanings. In this paper, we propose a contrastive\nlearning model for supervised abstractive text summarization, where we view a\ndocument, its gold summary and its model generated summaries as different views\nof the same mean representation and maximize the similarities between them\nduring training. We improve over a strong sequence-to-sequence text generation\nmodel (i.e., BART) on three different summarization datasets. Human evaluation\nalso shows that our model achieves better faithfulness ratings compared to its\ncounterpart without contrastive objectives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shusheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xingxing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Forget me not: A Gentle Reminder to Mind the Simple Multi-Layer Perceptron Baseline for Text Classification. (arXiv:2109.03777v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.03777","description":"<p>Graph neural networks have triggered a resurgence of graph-based text\nclassification. We show that already a simple MLP baseline achieves comparable\nperformance on benchmark datasets, questioning the importance of synthetic\ngraph structures. When considering an inductive scenario, i. e., when adding\nnew documents to a corpus, a simple MLP even outperforms the recent graph-based\nmodels TextGCN and HeteGCN and is comparable with HyperGAT. We further\nfine-tune DistilBERT and find that it outperforms all state-of-the-art models.\nWe suggest that future studies use at least an MLP baseline to contextualize\nthe results. We provide recommendations for the design and training of such a\nbaseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Galke_L/0/1/0/all/0/1\">Lukas Galke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scherp_A/0/1/0/all/0/1\">Ansgar Scherp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NADE: A Benchmark for Robust Adverse Drug Events Extraction in Face of Negations. (arXiv:2109.10080v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.10080","description":"<p>Adverse Drug Event (ADE) extraction models can rapidly examine large\ncollections of social media texts, detecting mentions of drug-related adverse\nreactions and trigger medical investigations. However, despite the recent\nadvances in NLP, it is currently unknown if such models are robust in face of\nnegation, which is pervasive across language varieties.\n</p>\n<p>In this paper we evaluate three state-of-the-art systems, showing their\nfragility against negation, and then we introduce two possible strategies to\nincrease the robustness of these models: a pipeline approach, relying on a\nspecific component for negation detection; an augmentation of an ADE extraction\ndataset to artificially create negated samples and further train the models.\n</p>\n<p>We show that both strategies bring significant increases in performance,\nlowering the number of spurious entities predicted by the models. Our dataset\nand code will be publicly released to encourage research on the topic.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Scaboro_S/0/1/0/all/0/1\">Simone Scaboro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Portelli_B/0/1/0/all/0/1\">Beatrice Portelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chersoni_E/0/1/0/all/0/1\">Emmanuele Chersoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santus_E/0/1/0/all/0/1\">Enrico Santus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Serra_G/0/1/0/all/0/1\">Giuseppe Serra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diarisation using location tracking with agglomerative clustering. (arXiv:2109.10598v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2109.10598","description":"<p>Previous works have shown that spatial location information can be\ncomplementary to speaker embeddings for a speaker diarisation task. However,\nthe models used often assume that speakers are fairly stationary throughout a\nmeeting. This paper proposes to relax this assumption, by explicitly modelling\nthe movements of speakers within an Agglomerative Hierarchical Clustering (AHC)\ndiarisation framework. Kalman filters, which track the locations of speakers,\nare used to compute log-likelihood ratios that contribute to the cluster\naffinity computations for the AHC merging and stopping decisions. Experiments\nshow that the proposed approach is able to yield improvements on a Microsoft\nrich meeting transcription task, compared to methods that do not use location\ninformation or that make stationarity assumptions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wong_J/0/1/0/all/0/1\">Jeremy H. M. Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abramovski_I/0/1/0/all/0/1\">Igor Abramovski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1\">Xiong Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yifan Gong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Alzheimers Dementia Detection using Acoustic & Linguistic features and Pre-Trained BERT. (arXiv:2109.11010v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.11010","description":"<p>Alzheimers disease is a fatal progressive brain disorder that worsens with\ntime. It is high time we have inexpensive and quick clinical diagnostic\ntechniques for early detection and care. In previous studies, various Machine\nLearning techniques and Pre-trained Deep Learning models have been used in\nconjunction with the extraction of various acoustic and linguistic features.\nOur study focuses on three models for the classification task in the ADReSS\n(The Alzheimers Dementia Recognition through Spontaneous Speech) 2021\nChallenge. We use the well-balanced dataset provided by the ADReSS Challenge\nfor training and validating our models. Model 1 uses various acoustic features\nfrom the eGeMAPs feature-set, Model 2 uses various linguistic features that we\ngenerated from auto-generated transcripts and Model 3 uses the auto-generated\ntranscripts directly to extract features using a Pre-trained BERT and TF-IDF.\nThese models are described in detail in the models section.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Valsaraj_A/0/1/0/all/0/1\">Akshay Valsaraj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madala_I/0/1/0/all/0/1\">Ithihas Madala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_N/0/1/0/all/0/1\">Nikhil Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baths_V/0/1/0/all/0/1\">Veeky Baths</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Volctrans GLAT System: Non-autoregressive Translation Meets WMT21. (arXiv:2109.11247v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.11247","description":"<p>This paper describes the Volctrans' submission to the WMT21 news translation\nshared task for German-&gt;English translation. We build a parallel (i.e.,\nnon-autoregressive) translation system using the Glancing Transformer, which\nenables fast and accurate parallel decoding in contrast to the currently\nprevailing autoregressive models. To the best of our knowledge, this is the\nfirst parallel translation system that can be scaled to such a practical\nscenario like WMT competition. More importantly, our parallel translation\nsystem achieves the best BLEU score (35.0) on German-&gt;English translation task,\noutperforming all strong autoregressive counterparts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qian_L/0/1/0/all/0/1\">Lihua Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zaixiang Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yaoming Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zehui Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jiangtao Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1\">Shanbo Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mingxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hao Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-09-26T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Lifelong 3D Object Recognition and Grasp Synthesis Using Dual Memory Recurrent Self-Organization Networks. (arXiv:2109.11544v1 [cs.RO])","link":"http://arxiv.org/abs/2109.11544","description":"<p>Humans learn to recognize and manipulate new objects in lifelong settings\nwithout forgetting the previously gained knowledge under non-stationary and\nsequential conditions. In autonomous systems, the agents also need to mitigate\nsimilar behavior to continually learn the new object categories and adapt to\nnew environments. In most conventional deep neural networks, this is not\npossible due to the problem of catastrophic forgetting, where the newly gained\nknowledge overwrites existing representations. Furthermore, most\nstate-of-the-art models excel either in recognizing the objects or in grasp\nprediction, while both tasks use visual input. The combined architecture to\ntackle both tasks is very limited. In this paper, we proposed a hybrid model\narchitecture consists of a dynamically growing dual-memory recurrent neural\nnetwork (GDM) and an autoencoder to tackle object recognition and grasping\nsimultaneously. The autoencoder network is responsible to extract a compact\nrepresentation for a given object, which serves as input for the GDM learning,\nand is responsible to predict pixel-wise antipodal grasp configurations. The\nGDM part is designed to recognize the object in both instances and categories\nlevels. We address the problem of catastrophic forgetting using the intrinsic\nmemory replay, where the episodic memory periodically replays the neural\nactivation trajectories in the absence of external sensory information. To\nextensively evaluate the proposed model in a lifelong setting, we generate a\nsynthetic dataset due to lack of sequential 3D objects dataset. Experiment\nresults demonstrated that the proposed model can learn both object\nrepresentation and grasping simultaneously in continual learning scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Santhakumar_K/0/1/0/all/0/1\">Krishnakumar Santhakumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasaei_H/0/1/0/all/0/1\">Hamidreza Kasaei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SAME: Deformable Image Registration based on Self-supervised Anatomical Embeddings. (arXiv:2109.11572v1 [eess.IV])","link":"http://arxiv.org/abs/2109.11572","description":"<p>In this work, we introduce a fast and accurate method for unsupervised 3D\nmedical image registration. This work is built on top of a recent algorithm\nSAM, which is capable of computing dense anatomical/semantic correspondences\nbetween two images at the pixel level. Our method is named SAME, which breaks\ndown image registration into three steps: affine transformation, coarse\ndeformation, and deep deformable registration. Using SAM embeddings, we enhance\nthese steps by finding more coherent correspondences, and providing features\nand a loss function with better semantic guidance. We collect a multi-phase\nchest computed tomography dataset with 35 annotated organs for each patient and\nconduct inter-subject registration for quantitative evaluation. Results show\nthat SAME outperforms widely-used traditional registration techniques (Elastix\nFFD, ANTs SyN) and learning based VoxelMorph method by at least 4.7% and 2.7%\nin Dice scores for two separate tasks of within-contrast-phase and\nacross-contrast-phase registration, respectively. SAME achieves the comparable\nperformance to the best traditional registration method, DEEDS (from our\nevaluation), while being orders of magnitude faster (from 45 seconds to 1.2\nseconds).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Liu_F/0/1/0/all/0/1\">Fengze Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yan_K/0/1/0/all/0/1\">Ke Yan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Harrison_A/0/1/0/all/0/1\">Adam Harrison</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guo_D/0/1/0/all/0/1\">Dazhou Guo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_L/0/1/0/all/0/1\">Le Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yuille_A/0/1/0/all/0/1\">Alan Yuille</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_L/0/1/0/all/0/1\">Lingyun Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xie_G/0/1/0/all/0/1\">Guotong Xie</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiao_J/0/1/0/all/0/1\">Jing Xiao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ye_X/0/1/0/all/0/1\">Xianghua Ye</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jin_D/0/1/0/all/0/1\">Dakai Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly-Supervised Monocular Depth Estimationwith Resolution-Mismatched Data. (arXiv:2109.11573v1 [cs.CV])","link":"http://arxiv.org/abs/2109.11573","description":"<p>Depth estimation from a single image is an active research topic in computer\nvision. The most accurate approaches are based on fully supervised learning\nmodels, which rely on a large amount of dense and high-resolution (HR)\nground-truth depth maps. However, in practice, color images are usually\ncaptured with much higher resolution than depth maps, leading to the\nresolution-mismatched effect. In this paper, we propose a novel\nweakly-supervised framework to train a monocular depth estimation network to\ngenerate HR depth maps with resolution-mismatched supervision, i.e., the inputs\nare HR color images and the ground-truth are low-resolution (LR) depth maps.\nThe proposed weakly supervised framework is composed of a sharing weight\nmonocular depth estimation network and a depth reconstruction network for\ndistillation. Specifically, for the monocular depth estimation network the\ninput color image is first downsampled to obtain its LR version with the same\nresolution as the ground-truth depth. Then, both HR and LR color images are fed\ninto the proposed monocular depth estimation network to obtain the\ncorresponding estimated depth maps. We introduce three losses to train the\nnetwork: 1) reconstruction loss between the estimated LR depth and the\nground-truth LR depth; 2) reconstruction loss between the downsampled estimated\nHR depth and the ground-truth LR depth; 3) consistency loss between the\nestimated LR depth and the downsampled estimated HR depth. In addition, we\ndesign a depth reconstruction network from depth to depth. Through distillation\nloss, features between two networks maintain the structural consistency in\naffinity space, and finally improving the estimation network performance.\nExperimental results demonstrate that our method achieves superior performance\nthan unsupervised and semi-supervised learning based schemes, and is\ncompetitive or even better compared to supervised ones.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jialei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yuanchao Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xianming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Junjun Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1\">Xiangyang Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Long Short View Feature Decomposition via Contrastive Video Representation Learning. (arXiv:2109.11593v1 [cs.CV])","link":"http://arxiv.org/abs/2109.11593","description":"<p>Self-supervised video representation methods typically focus on the\nrepresentation of temporal attributes in videos. However, the role of\nstationary versus non-stationary attributes is less explored: Stationary\nfeatures, which remain similar throughout the video, enable the prediction of\nvideo-level action classes. Non-stationary features, which represent temporally\nvarying attributes, are more beneficial for downstream tasks involving more\nfine-grained temporal understanding, such as action segmentation. We argue that\na single representation to capture both types of features is sub-optimal, and\npropose to decompose the representation space into stationary and\nnon-stationary features via contrastive learning from long and short views,\ni.e. long video sequences and their shorter sub-sequences. Stationary features\nare shared between the short and long views, while non-stationary features\naggregate the short views to match the corresponding long view. To empirically\nverify our approach, we demonstrate that our stationary features work\nparticularly well on an action recognition downstream task, while our\nnon-stationary features perform better on action segmentation. Furthermore, we\nanalyse the learned representations and find that stationary features capture\nmore temporally stable, static attributes, while non-stationary features\nencompass more temporally varying ones.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Behrmann_N/0/1/0/all/0/1\">Nadine Behrmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fayyaz_M/0/1/0/all/0/1\">Mohsen Fayyaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gall_J/0/1/0/all/0/1\">Juergen Gall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noroozi_M/0/1/0/all/0/1\">Mehdi Noroozi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SPNet: Multi-Shell Kernel Convolution for Point Cloud Semantic Segmentation. (arXiv:2109.11610v1 [cs.CV])","link":"http://arxiv.org/abs/2109.11610","description":"<p>Feature encoding is essential for point cloud analysis. In this paper, we\npropose a novel point convolution operator named Shell Point Convolution\n(SPConv) for shape encoding and local context learning. Specifically, SPConv\nsplits 3D neighborhood space into shells, aggregates local features on manually\ndesigned kernel points, and performs convolution on the shells. Moreover,\nSPConv incorporates a simple yet effective attention module that enhances local\nfeature aggregation. Based upon SPConv, a deep neural network named SPNet is\nconstructed to process large-scale point clouds. Poisson disk sampling and\nfeature propagation are incorporated in SPNet for better efficiency and\naccuracy. We provided details of the shell design and conducted extensive\nexperiments on challenging large-scale point cloud datasets. Experimental\nresults show that SPConv is effective in local shape encoding, and our SPNet is\nable to achieve top-ranking performances in semantic segmentation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuyan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1\">Chuanmao Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_Y/0/1/0/all/0/1\">Ye Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast Point Voxel Convolution Neural Network with Selective Feature Fusion for Point Cloud Semantic Segmentation. (arXiv:2109.11614v1 [cs.CV])","link":"http://arxiv.org/abs/2109.11614","description":"<p>We present a novel lightweight convolutional neural network for point cloud\nanalysis. In contrast to many current CNNs which increase receptive field by\ndownsampling point cloud, our method directly operates on the entire point sets\nwithout sampling and achieves good performances efficiently. Our network\nconsists of point voxel convolution (PVC) layer as building block. Each layer\nhas two parallel branches, namely the voxel branch and the point branch. For\nthe voxel branch specifically, we aggregate local features on non-empty voxel\ncenters to reduce geometric information loss caused by voxelization, then apply\nvolumetric convolutions to enhance local neighborhood geometry encoding. For\nthe point branch, we use Multi-Layer Perceptron (MLP) to extract fine-detailed\npoint-wise features. Outputs from these two branches are adaptively fused via a\nfeature selection module. Moreover, we supervise the output from every PVC\nlayer to learn different levels of semantic information. The final prediction\nis made by averaging all intermediate predictions. We demonstrate empirically\nthat our method is able to achieve comparable results while being fast and\nmemory efficient. We evaluate our method on popular point cloud datasets for\nobject classification and semantic segmentation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuyan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_Y/0/1/0/all/0/1\">Ye Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Keypoints-Based Deep Feature Fusion for Cooperative Vehicle Detection of Autonomous Driving. (arXiv:2109.11615v1 [cs.CV])","link":"http://arxiv.org/abs/2109.11615","description":"<p>Sharing collective perception messages (CPM) between vehicles is investigated\nto decrease occlusions, so as to improve perception accuracy and safety of\nautonomous driving. However, highly accurate data sharing and low communication\noverhead is a big challenge for collective perception, especially when\nreal-time communication is required among connected and automated vehicles. In\nthis paper, we propose an efficient and effective keypoints-based deep feature\nfusion framework, called FPV-RCNN, for collective perception, which is built on\ntop of the 3D object detector PV-RCNN. We introduce a bounding box proposal\nmatching module and a keypoints selection strategy to compress the CPM size and\nsolve the multi-vehicle data fusion problem. Compared to a bird's-eye view\n(BEV) keypoints feature fusion, FPV-RCNN achieves improved detection accuracy\nby about 14% at a high evaluation criterion (IoU 0.7) on a synthetic dataset\nCOMAP dedicated to collective perception. Also, its performance is comparable\nto two raw data fusion baselines that have no data loss in sharing. Moreover,\nour method also significantly decreases the CPM size to less than 0.3KB, which\nis about 50 times smaller than the BEV feature map sharing used in previous\nworks. Even with a further decreased number of CPM feature channels, i.e., from\n128 to 32, the detection performance only drops about 1%. The code of our\nmethod is available at https://github.com/YuanYunshuang/FPV_RCNN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yunshuang Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Hao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sester_M/0/1/0/all/0/1\">Monika Sester</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Learned Stereo Depth System for Robotic Manipulation in Homes. (arXiv:2109.11644v1 [cs.RO])","link":"http://arxiv.org/abs/2109.11644","description":"<p>We present a passive stereo depth system that produces dense and accurate\npoint clouds optimized for human environments, including dark, textureless,\nthin, reflective and specular surfaces and objects, at 2560x2048 resolution,\nwith 384 disparities, in 30 ms. The system consists of an algorithm combining\nlearned stereo matching with engineered filtering, a training and data-mixing\nmethodology, and a sensor hardware design. Our architecture is 15x faster than\napproaches that perform similarly on the Middlebury and Flying Things Stereo\nBenchmarks. To effectively supervise the training of this model, we combine\nreal data labelled using off-the-shelf depth sensors, as well as a number of\ndifferent rendered, simulated labeled datasets. We demonstrate the efficacy of\nour system by presenting a large number of qualitative results in the form of\ndepth maps and point-clouds, experiments validating the metric accuracy of our\nsystem and comparisons to other sensors on challenging objects and scenes. We\nalso show the competitiveness of our algorithm compared to state-of-the-art\nlearned models using the Middlebury and FlyingThings datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shankar_K/0/1/0/all/0/1\">Krishna Shankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tjersland_M/0/1/0/all/0/1\">Mark Tjersland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jeremy Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stone_K/0/1/0/all/0/1\">Kevin Stone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bajracharya_M/0/1/0/all/0/1\">Max Bajracharya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Paint4Poem: A Dataset for Artistic Visualization of Classical Chinese Poems. (arXiv:2109.11682v1 [cs.CV])","link":"http://arxiv.org/abs/2109.11682","description":"<p>In this work we propose a new task: artistic visualization of classical\nChinese poems, where the goal is to generatepaintings of a certain artistic\nstyle for classical Chinese poems. For this purpose, we construct a new dataset\ncalled Paint4Poem. Thefirst part of Paint4Poem consists of 301 high-quality\npoem-painting pairs collected manually from an influential modern Chinese\nartistFeng Zikai. As its small scale poses challenges for effectively training\npoem-to-painting generation models, we introduce the secondpart of Paint4Poem,\nwhich consists of 3,648 caption-painting pairs collected manually from Feng\nZikai's paintings and 89,204 poem-painting pairs collected automatically from\nthe web. We expect the former to help learning the artist painting style as it\ncontainshis most paintings, and the latter to help learning the semantic\nrelevance between poems and paintings. Further, we analyze Paint4Poem regarding\npoem diversity, painting style, and the semantic relevance between poems and\npaintings. We create abenchmark for Paint4Poem: we train two representative\ntext-to-image generation models: AttnGAN and MirrorGAN, and evaluate\ntheirperformance regarding painting pictorial quality, painting stylistic\nrelevance, and semantic relevance between poems and paintings.The results\nindicate that the models are able to generate paintings that have good\npictorial quality and mimic Feng Zikai's style, but thereflection of poem\nsemantics is limited. The dataset also poses many interesting research\ndirections on this task, including transferlearning, few-shot learning,\ntext-to-image generation for low-resource data etc. The dataset is publicly\navailable.(https://github.com/paint4poem/paint4poem)\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1\">Jie Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_C/0/1/0/all/0/1\">Chang Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nieuwburg_E/0/1/0/all/0/1\">Elisha Nieuwburg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1\">Fengyuan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanoulas_E/0/1/0/all/0/1\">Evangelos Kanoulas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Feasibility study of urban flood mapping using traffic signs for route optimization. (arXiv:2109.11712v1 [cs.CV])","link":"http://arxiv.org/abs/2109.11712","description":"<p>Water events are the most frequent and costliest climate disasters around the\nworld. In the U.S., an estimated 127 million people who live in coastal areas\nare at risk of substantial home damage from hurricanes or flooding. In flood\nemergency management, timely and effective spatial decision-making and\nintelligent routing depend on flood depth information at a fine spatiotemporal\nscale. In this paper, crowdsourcing is utilized to collect photos of submerged\nstop signs, and pair each photo with a pre-flood photo taken at the same\nlocation. Each photo pair is then analyzed using deep neural network and image\nprocessing to estimate the depth of floodwater in the location of the photo.\nGenerated point-by-point depth data is converted to a flood inundation map and\nused by an A* search algorithm to determine an optimal flood-free path\nconnecting points of interest. Results provide crucial information to rescue\nteams and evacuees by enabling effective wayfinding during flooding events.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alizadeh_B/0/1/0/all/0/1\">Bahareh Alizadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Diya Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhe Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Behzadan_A/0/1/0/all/0/1\">Amir H. Behzadan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Training Automatic View Planner for Cardiac MR Imaging via Self-Supervision by Spatial Relationship between Views. (arXiv:2109.11715v1 [eess.IV])","link":"http://arxiv.org/abs/2109.11715","description":"<p>View planning for the acquisition of cardiac magnetic resonance imaging (CMR)\nrequires acquaintance with the cardiac anatomy and remains a challenging task\nin clinical practice. Existing approaches to its automation relied either on an\nadditional volumetric image not typically acquired in clinic routine, or on\nlaborious manual annotations of cardiac structural landmarks. This work\npresents a clinic-compatible and annotation-free system for automatic CMR view\nplanning. The system mines the spatial relationship -- more specifically,\nlocates and exploits the intersecting lines -- between the source and target\nviews, and trains deep networks to regress heatmaps defined by these\nintersecting lines. As the spatial relationship is self-contained in properly\nstored data, e.g., in the DICOM format, the need for manual annotation is\neliminated. Then, a multi-view planning strategy is proposed to aggregate\ninformation from the predicted heatmaps for all the source views of a target\nview, for a globally optimal prescription. The multi-view aggregation mimics\nthe similar strategy practiced by skilled human prescribers. Experimental\nresults on 181 clinical CMR exams show that our system achieves superior\naccuracy to existing approaches including conventional atlas-based and newer\ndeep learning based ones, in prescribing four standard CMR views. The mean\nangle difference and point-to-plane distance evaluated against the ground truth\nplanes are 5.98 degrees and 3.48 mm, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wei_D/0/1/0/all/0/1\">Dong Wei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ma_K/0/1/0/all/0/1\">Kai Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_Y/0/1/0/all/0/1\">Yefeng Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A 3D Mesh-based Lifting-and-Projection Network for Human Pose Transfer. (arXiv:2109.11719v1 [cs.CV])","link":"http://arxiv.org/abs/2109.11719","description":"<p>Human pose transfer has typically been modeled as a 2D image-to-image\ntranslation problem. This formulation ignores the human body shape prior in 3D\nspace and inevitably causes implausible artifacts, especially when facing\nocclusion. To address this issue, we propose a lifting-and-projection framework\nto perform pose transfer in the 3D mesh space. The core of our framework is a\nforeground generation module, that consists of two novel networks: a\nlifting-and-projection network (LPNet) and an appearance detail compensating\nnetwork (ADCNet). To leverage the human body shape prior, LPNet exploits the\ntopological information of the body mesh to learn an expressive visual\nrepresentation for the target person in the 3D mesh space. To preserve texture\ndetails, ADCNet is further introduced to enhance the feature produced by LPNet\nwith the source foreground image. Such design of the foreground generation\nmodule enables the model to better handle difficult cases such as those with\nocclusions. Experiments on the iPER and Fashion datasets empirically\ndemonstrate that the proposed lifting-and-projection framework is effective and\noutperforms the existing image-to-image-based and mesh-based methods on human\npose transfer task in both self-transfer and cross-transfer settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jinxiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yangheng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Siheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ya Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Holistic Semi-Supervised Approaches for EEG Representation Learning. (arXiv:2109.11732v1 [cs.LG])","link":"http://arxiv.org/abs/2109.11732","description":"<p>Recently, supervised methods, which often require substantial amounts of\nclass labels, have achieved promising results for EEG representation learning.\nHowever, labeling EEG data is a challenging task. More recently, holistic\nsemi-supervised learning approaches, which only require few output labels, have\nshown promising results in the field of computer vision. These methods,\nhowever, have not yet been adapted for EEG learning. In this paper, we adapt\nthree state-of-the-art holistic semi-supervised approaches, namely MixMatch,\nFixMatch, and AdaMatch, as well as five classical semi-supervised methods for\nEEG learning. We perform rigorous experiments with all 8 methods on two public\nEEG-based emotion recognition datasets, namely SEED and SEED-IV. The\nexperiments with different amounts of limited labeled samples show that the\nholistic approaches achieve strong results even when only 1 labeled sample is\nused per class. Further experiments show that in most cases, AdaMatch is the\nmost effective method, followed by MixMatch and FixMatch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guangyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Etemad_A/0/1/0/all/0/1\">Ali Etemad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unaligned Image-to-Image Translation by Learning to Reweight. (arXiv:2109.11736v1 [cs.CV])","link":"http://arxiv.org/abs/2109.11736","description":"<p>Unsupervised image-to-image translation aims at learning the mapping from the\nsource to target domain without using paired images for training. An essential\nyet restrictive assumption for unsupervised image translation is that the two\ndomains are aligned, e.g., for the selfie2anime task, the anime (selfie) domain\nmust contain only anime (selfie) face images that can be translated to some\nimages in the other domain. Collecting aligned domains can be laborious and\nneeds lots of attention. In this paper, we consider the task of image\ntranslation between two unaligned domains, which may arise for various possible\nreasons. To solve this problem, we propose to select images based on importance\nreweighting and develop a method to learn the weights and perform translation\nsimultaneously and automatically. We compare the proposed method with\nstate-of-the-art image translation approaches and present qualitative and\nquantitative results on different tasks with unaligned domains. Extensive\nempirical evidence demonstrates the usefulness of the proposed problem\nformulation and the superiority of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1\">Shaoan Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1\">Mingming Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yanwu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kun Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-View Video-Based 3D Hand Pose Estimation. (arXiv:2109.11747v1 [cs.CV])","link":"http://arxiv.org/abs/2109.11747","description":"<p>Hand pose estimation (HPE) can be used for a variety of human-computer\ninteraction applications such as gesture-based control for physical or\nvirtual/augmented reality devices. Recent works have shown that videos or\nmulti-view images carry rich information regarding the hand, allowing for the\ndevelopment of more robust HPE systems. In this paper, we present the\nMulti-View Video-Based 3D Hand (MuViHand) dataset, consisting of multi-view\nvideos of the hand along with ground-truth 3D pose labels. Our dataset includes\nmore than 402,000 synthetic hand images available in 4,560 videos. The videos\nhave been simultaneously captured from six different angles with complex\nbackgrounds and random levels of dynamic lighting. The data has been captured\nfrom 10 distinct animated subjects using 12 cameras in a semi-circle topology\nwhere six tracking cameras only focus on the hand and the other six fixed\ncameras capture the entire body. Next, we implement MuViHandNet, a neural\npipeline consisting of image encoders for obtaining visual embeddings of the\nhand, recurrent learners to learn both temporal and angular sequential\ninformation, and graph networks with U-Net architectures to estimate the final\n3D pose information. We perform extensive experiments and show the challenging\nnature of this new dataset as well as the effectiveness of our proposed method.\nAblation studies show the added value of each component in MuViHandNet, as well\nas the benefit of having temporal and sequential information in the dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khaleghi_L/0/1/0/all/0/1\">Leyla Khaleghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moghaddam_A/0/1/0/all/0/1\">Alireza Sepas Moghaddam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marshall_J/0/1/0/all/0/1\">Joshua Marshall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Etemad_A/0/1/0/all/0/1\">Ali Etemad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-Grained Image Generation from Bangla Text Description using Attentional Generative Adversarial Network. (arXiv:2109.11749v1 [cs.CV])","link":"http://arxiv.org/abs/2109.11749","description":"<p>Generating fine-grained, realistic images from text has many applications in\nthe visual and semantic realm. Considering that, we propose Bangla Attentional\nGenerative Adversarial Network (AttnGAN) that allows intensified, multi-stage\nprocessing for high-resolution Bangla text-to-image generation. Our model can\nintegrate the most specific details at different sub-regions of the image. We\ndistinctively concentrate on the relevant words in the natural language\ndescription. This framework has achieved a better inception score on the CUB\ndataset. For the first time, a fine-grained image is generated from Bangla text\nusing attentional GAN. Bangla has achieved 7th position among 100 most spoken\nlanguages. This inspires us to explicitly focus on this language, which will\nensure the inevitable need of many people. Moreover, Bangla has a more complex\nsyntactic structure and less natural language processing resource that\nvalidates our work more.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Palash_M/0/1/0/all/0/1\">Md Aminul Haque Palash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasim_M/0/1/0/all/0/1\">Md Abdullah Al Nasim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhali_A/0/1/0/all/0/1\">Aditi Dhali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Afrin_F/0/1/0/all/0/1\">Faria Afrin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quantifying point cloud realism through adversarially learned latent representations. (arXiv:2109.11775v1 [cs.CV])","link":"http://arxiv.org/abs/2109.11775","description":"<p>Judging the quality of samples synthesized by generative models can be\ntedious and time consuming, especially for complex data structures, such as\npoint clouds. This paper presents a novel approach to quantify the realism of\nlocal regions in LiDAR point clouds. Relevant features are learned from\nreal-world and synthetic point clouds by training on a proxy classification\ntask. Inspired by fair networks, we use an adversarial technique to discourage\nthe encoding of dataset-specific information. The resulting metric can assign a\nquality score to samples without requiring any task specific annotations.\n</p>\n<p>In a series of experiments, we confirm the soundness of our metric by\napplying it in controllable task setups and on unseen data. Additional\nexperiments show reliable interpolation capabilities of the metric between data\nwith varying degree of realism. As one important application, we demonstrate\nhow the local realism score can be used for anomaly detection in point clouds.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Triess_L/0/1/0/all/0/1\">Larissa T. Triess</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peter_D/0/1/0/all/0/1\">David Peter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baur_S/0/1/0/all/0/1\">Stefan A. Baur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zollner_J/0/1/0/all/0/1\">J. Marius Z&#xf6;llner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dense Contrastive Visual-Linguistic Pretraining. (arXiv:2109.11778v1 [cs.CV])","link":"http://arxiv.org/abs/2109.11778","description":"<p>Inspired by the success of BERT, several multimodal representation learning\napproaches have been proposed that jointly represent image and text. These\napproaches achieve superior performance by capturing high-level semantic\ninformation from large-scale multimodal pretraining. In particular, LXMERT and\nUNITER adopt visual region feature regression and label classification as\npretext tasks. However, they tend to suffer from the problems of noisy labels\nand sparse semantic annotations, based on the visual features having been\npretrained on a crowdsourced dataset with limited and inconsistent semantic\nlabeling. To overcome these issues, we propose unbiased Dense Contrastive\nVisual-Linguistic Pretraining (DCVLP), which replaces the region regression and\nclassification with cross-modality region contrastive learning that requires no\nannotations. Two data augmentation strategies (Mask Perturbation and\nIntra-/Inter-Adversarial Perturbation) are developed to improve the quality of\nnegative samples used in contrastive learning. Overall, DCVLP allows\ncross-modality dense region contrastive learning in a self-supervised setting\nindependent of any object annotations. We compare our method against prior\nvisual-linguistic pretraining frameworks to validate the superiority of dense\ncontrastive learning on multimodal representation learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_L/0/1/0/all/0/1\">Lei Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shuang_K/0/1/0/all/0/1\">Kai Shuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_S/0/1/0/all/0/1\">Shijie Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Peng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1\">Zuohui Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melo_G/0/1/0/all/0/1\">Gerard de Melo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yunpeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_S/0/1/0/all/0/1\">Sen Su</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CPT: Colorful Prompt Tuning for Pre-trained Vision-Language Models. (arXiv:2109.11797v1 [cs.CV])","link":"http://arxiv.org/abs/2109.11797","description":"<p>Pre-Trained Vision-Language Models (VL-PTMs) have shown promising\ncapabilities in grounding natural language in image data, facilitating a broad\nvariety of cross-modal tasks. However, we note that there exists a significant\ngap between the objective forms of model pre-training and fine-tuning,\nresulting in a need for quantities of labeled data to stimulate the visual\ngrounding capability of VL-PTMs for downstream tasks. To address the challenge,\nwe present Cross-modal Prompt Tuning (CPT, alternatively, Colorful Prompt\nTuning), a novel paradigm for tuning VL-PTMs, which reformulates visual\ngrounding into a fill-in-the-blank problem with color-based co-referential\nmarkers in image and text, maximally mitigating the gap. In this way, our\nprompt tuning approach enables strong few-shot and even zero-shot visual\ngrounding capabilities of VL-PTMs. Comprehensive experimental results show that\nprompt tuned VL-PTMs outperform their fine-tuned counterparts by a large margin\n(e.g., 17.3% absolute accuracy improvement, and 73.8% relative standard\ndeviation reduction on average with one shot in RefCOCO evaluation). All the\ndata and code will be available to facilitate future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yuan Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Ao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhengyan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1\">Tat-Seng Chua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Domain Feature Adaptation for Bronchoscopic Depth Estimation. (arXiv:2109.11798v1 [eess.IV])","link":"http://arxiv.org/abs/2109.11798","description":"<p>Depth estimation from monocular images is an important task in localization\nand 3D reconstruction pipelines for bronchoscopic navigation. Various\nsupervised and self-supervised deep learning-based approaches have proven\nthemselves on this task for natural images. However, the lack of labeled data\nand the bronchial tissue's feature-scarce texture make the utilization of these\nmethods ineffective on bronchoscopic scenes. In this work, we propose an\nalternative domain-adaptive approach. Our novel two-step structure first trains\na depth estimation network with labeled synthetic images in a supervised\nmanner; then adopts an unsupervised adversarial domain feature adaptation\nscheme to improve the performance on real images. The results of our\nexperiments show that the proposed method improves the network's performance on\nreal images by a considerable margin and can be employed in 3D reconstruction\npipelines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Karaoglu_M/0/1/0/all/0/1\">Mert Asim Karaoglu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Brasch_N/0/1/0/all/0/1\">Nikolas Brasch</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Stollenga_M/0/1/0/all/0/1\">Marijn Stollenga</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wein_W/0/1/0/all/0/1\">Wolfgang Wein</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Navab_N/0/1/0/all/0/1\">Nassir Navab</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tombari_F/0/1/0/all/0/1\">Federico Tombari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ladikos_A/0/1/0/all/0/1\">Alexander Ladikos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SIM2REALVIZ: Visualizing the Sim2Real Gap in Robot Ego-Pose Estimation. (arXiv:2109.11801v1 [cs.RO])","link":"http://arxiv.org/abs/2109.11801","description":"<p>The Robotics community has started to heavily rely on increasingly realistic\n3D simulators for large-scale training of robots on massive amounts of data.\nBut once robots are deployed in the real world, the simulation gap, as well as\nchanges in the real world (e.g. lights, objects displacements) lead to errors.\nIn this paper, we introduce Sim2RealViz, a visual analytics tool to assist\nexperts in understanding and reducing this gap for robot ego-pose estimation\ntasks, i.e. the estimation of a robot's position using trained models.\nSim2RealViz displays details of a given model and the performance of its\ninstances in both simulation and real-world. Experts can identify environment\ndifferences that impact model predictions at a given location and explore\nthrough direct interactions with the model hypothesis to fix it. We detail the\ndesign of the tool, and case studies related to the exploit of the regression\nto the mean bias and how it can be addressed, and how models are perturbed by\nthe vanish of landmarks such as bikes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jaunet_T/0/1/0/all/0/1\">Theo Jaunet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bono_G/0/1/0/all/0/1\">Guillaume Bono</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vuillemot_R/0/1/0/all/0/1\">Romain Vuillemot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_C/0/1/0/all/0/1\">Christian Wolf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-shot Learning Based on Multi-stage Transfer and Class-Balanced Loss for Diabetic Retinopathy Grading. (arXiv:2109.11806v1 [eess.IV])","link":"http://arxiv.org/abs/2109.11806","description":"<p>Diabetic retinopathy (DR) is one of the major blindness-causing diseases\ncurrent-ly known. Automatic grading of DR using deep learning methods not only\nspeeds up the diagnosis of the disease but also reduces the rate of\nmisdiagnosis. However, problems such as insufficient samples and imbalanced\nclass distribu-tion in DR datasets have constrained the improvement of grading\nperformance. In this paper, we introduce the idea of multi-stage transfer into\nthe grading task of DR. The new transfer learning technique leverages multiple\ndatasets with differ-ent scales to enable the model to learn more feature\nrepresentation information. Meanwhile, to cope with imbalanced DR datasets, we\npresent a class-balanced loss function that performs well in natural image\nclassification tasks, and adopt a simple and easy-to-implement training method\nfor it. The experimental results show that the application of multi-stage\ntransfer and class-balanced loss function can effectively improve the grading\nperformance metrics such as accuracy and quadratic weighted kappa. In fact, our\nmethod has outperformed two state-of-the-art methods and achieved the best\nresult on the DR grading task of IDRiD Sub-Challenge 2.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Shi_L/0/1/0/all/0/1\">Lei Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1\">Junxing Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MODNet-V: Improving Portrait Video Matting via Background Restoration. (arXiv:2109.11818v1 [cs.CV])","link":"http://arxiv.org/abs/2109.11818","description":"<p>To address the challenging portrait video matting problem more precisely,\nexisting works typically apply some matting priors that require additional user\nefforts to obtain, such as annotated trimaps or background images. In this\nwork, we observe that instead of asking the user to explicitly provide a\nbackground image, we may recover it from the input video itself. To this end,\nwe first propose a novel background restoration module (BRM) to recover the\nbackground image dynamically from the input video. BRM is extremely lightweight\nand can be easily integrated into existing matting models. By combining BRM\nwith a recent image matting model, MODNet, we then present MODNet-V for\nportrait video matting. Benefited from the strong background prior provided by\nBRM, MODNet-V has only 1/3 of the parameters of MODNet but achieves comparable\nor even better performances. Our design allows MODNet-V to be trained in an\nend-to-end manner on a single NVIDIA 3090 GPU. Finally, we introduce a new\npatch refinement module (PRM) to adapt MODNet-V for high-resolution videos\nwhile keeping MODNet-V lightweight and fast.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jiayu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ke_Z/0/1/0/all/0/1\">Zhanghan Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lihe Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Huchuan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lau_R/0/1/0/all/0/1\">Rynson W.H. Lau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GSIP: Green Semantic Segmentation of Large-Scale Indoor Point Clouds. (arXiv:2109.11835v1 [cs.CV])","link":"http://arxiv.org/abs/2109.11835","description":"<p>An efficient solution to semantic segmentation of large-scale indoor scene\npoint clouds is proposed in this work. It is named GSIP (Green Segmentation of\nIndoor Point clouds) and its performance is evaluated on a representative\nlarge-scale benchmark -- the Stanford 3D Indoor Segmentation (S3DIS) dataset.\nGSIP has two novel components: 1) a room-style data pre-processing method that\nselects a proper subset of points for further processing, and 2) a new feature\nextractor which is extended from PointHop. For the former, sampled points of\neach room form an input unit. For the latter, the weaknesses of PointHop's\nfeature extraction when extending it to large-scale point clouds are identified\nand fixed with a simpler processing pipeline. As compared with PointNet, which\nis a pioneering deep-learning-based solution, GSIP is green since it has\nsignificantly lower computational complexity and a much smaller model size.\nFurthermore, experiments show that GSIP outperforms PointNet in segmentation\nperformance for the S3DIS dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadam_P/0/1/0/all/0/1\">Pranav Kadam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuo_C/0/1/0/all/0/1\">C.-C. Jay Kuo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Frequency Pooling: Shift-Equivalent and Anti-Aliasing Downsampling. (arXiv:2109.11839v1 [cs.CV])","link":"http://arxiv.org/abs/2109.11839","description":"<p>Convolution utilizes a shift-equivalent prior of images, thus leading to\ngreat success in image processing tasks. However, commonly used poolings in\nconvolutional neural networks (CNNs), such as max-pooling, average-pooling, and\nstrided-convolution, are not shift-equivalent. Thus, the shift-equivalence of\nCNNs is destroyed when convolutions and poolings are stacked. Moreover,\nanti-aliasing is another essential property of poolings from the perspective of\nsignal processing. However, recent poolings are neither shift-equivalent nor\nanti-aliasing. To address this issue, we propose a new pooling method that is\nshift-equivalent and anti-aliasing, named frequency pooling. Frequency pooling\nfirst transforms the features into the frequency domain, and then removes the\nfrequency components beyond the Nyquist frequency. Finally, it transforms the\nfeatures back to the spatial domain. We prove that frequency pooling is\nshift-equivalent and anti-aliasing based on the property of Fourier transform\nand Nyquist frequency. Experiments on image classification show that frequency\npooling improves accuracy and robustness with respect to the shifts of CNNs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhendong Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learnable Triangulation for Deep Learning-based 3D Reconstruction of Objects of Arbitrary Topology from Single RGB Images. (arXiv:2109.11844v1 [cs.CV])","link":"http://arxiv.org/abs/2109.11844","description":"<p>We propose a novel deep reinforcement learning-based approach for 3D object\nreconstruction from monocular images. Prior works that use mesh representations\nare template based. Thus, they are limited to the reconstruction of objects\nthat have the same topology as the template. Methods that use volumetric grids\nas intermediate representations are computationally expensive, which limits\ntheir application in real-time scenarios. In this paper, we propose a novel\nend-to-end method that reconstructs 3D objects of arbitrary topology from a\nmonocular image. It is composed of of (1) a Vertex Generation Network (VGN),\nwhich predicts the initial 3D locations of the object's vertices from an input\nRGB image, (2) a differentiable triangulation layer, which learns in a\nnon-supervised manner, using a novel reinforcement learning algorithm, the best\ntriangulation of the object's vertices, and finally, (3) a hierarchical mesh\nrefinement network that uses graph convolutions to refine the initial mesh. Our\nkey contribution is the learnable triangulation process, which recovers in an\nunsupervised manner the topology of the input shape. Our experiments on\nShapeNet and Pix3D benchmarks show that the proposed method outperforms the\nstate-of-the-art in terms of visual quality, reconstruction accuracy, and\ncomputational time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Charrada_T/0/1/0/all/0/1\">Tarek Ben Charrada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tabia_H/0/1/0/all/0/1\">Hedi Tabia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chetouani_A/0/1/0/all/0/1\">Aladine Chetouani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laga_H/0/1/0/all/0/1\">Hamid Laga</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How to find a good image-text embedding for remote sensing visual question answering?. (arXiv:2109.11848v1 [cs.CV])","link":"http://arxiv.org/abs/2109.11848","description":"<p>Visual question answering (VQA) has recently been introduced to remote\nsensing to make information extraction from overhead imagery more accessible to\neveryone. VQA considers a question (in natural language, therefore easy to\nformulate) about an image and aims at providing an answer through a model based\non computer vision and natural language processing methods. As such, a VQA\nmodel needs to jointly consider visual and textual features, which is\nfrequently done through a fusion step. In this work, we study three different\nfusion methodologies in the context of VQA for remote sensing and analyse the\ngains in accuracy with respect to the model complexity. Our findings indicate\nthat more complex fusion mechanisms yield an improved performance, yet that\nseeking a trade-of between model complexity and performance is worthwhile in\npractice.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chappuis_C/0/1/0/all/0/1\">Christel Chappuis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lobry_S/0/1/0/all/0/1\">Sylvain Lobry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kellenberger_B/0/1/0/all/0/1\">Benjamin Kellenberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saux_B/0/1/0/all/0/1\">Bertrand Le Saux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuia_D/0/1/0/all/0/1\">Devis Tuia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Training dataset generation for bridge game registration. (arXiv:2109.11861v1 [cs.CV])","link":"http://arxiv.org/abs/2109.11861","description":"<p>This paper presents a method for automatic generation of a training dataset\nfor a deep convolutional neural network used for playing card detection. The\nsolution allows to skip the time-consuming processes of manual image collecting\nand labelling recognised objects. The YOLOv4 network trained on the generated\ndataset achieved an efficiency of 99.8% in the cards detection task. The\nproposed method is a part of a project that aims to automate the process of\nbroadcasting duplicate bridge competitions using a vision system and neural\nnetworks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wzorek_P/0/1/0/all/0/1\">Piotr Wzorek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kryjak_T/0/1/0/all/0/1\">Tomasz Kryjak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Catadioptric Stereo on a Smartphone. (arXiv:2109.11872v1 [cs.CV])","link":"http://arxiv.org/abs/2109.11872","description":"<p>We present a 3D printed adapter with planar mirrors for stereo reconstruction\nusing front and back smartphone camera. The adapter presents a practical and\nlow-cost solution for enabling any smartphone to be used as a stereo camera,\nwhich is currently only possible using high-end phones with expensive 3D\nsensors. Using the prototype version of the adapter, we experiment with\nparameters like the angles between cameras and mirrors and the distance to each\ncamera (the stereo baseline). We find the most convenient configuration and\ncalibrate the stereo pair. Based on the presented preliminary analysis, we\nidentify possible improvements in the current design. To demonstrate the\nworking prototype, we reconstruct a 3D human pose using 2D keypoint detections\nfrom the stereo pair and evaluate extracted body lengths. The result shows that\nthe adapter can be used for anthropometric measurement of several body\nsegments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bartol_K/0/1/0/all/0/1\">Kristijan Bartol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bojanic_D/0/1/0/all/0/1\">David Bojani&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petkovic_T/0/1/0/all/0/1\">Tomislav Petkovi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pribanic_T/0/1/0/all/0/1\">Tomislav Pribani&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Localizing Infinity-shaped fishes: Sketch-guided object localization in the wild. (arXiv:2109.11874v1 [cs.CV])","link":"http://arxiv.org/abs/2109.11874","description":"<p>This work investigates the problem of sketch-guided object localization\n(SGOL), where human sketches are used as queries to conduct the object\nlocalization in natural images. In this cross-modal setting, we first\ncontribute with a tough-to-beat baseline that without any specific SGOL\ntraining is able to outperform the previous works on a fixed set of classes.\nThe baseline is useful to analyze the performance of SGOL approaches based on\navailable simple yet powerful methods. We advance prior arts by proposing a\nsketch-conditioned DETR (DEtection TRansformer) architecture which avoids a\nhard classification and alleviates the domain gap between sketches and images\nto localize object instances. Although the main goal of SGOL is focused on\nobject detection, we explored its natural extension to sketch-guided instance\nsegmentation. This novel task allows to move towards identifying the objects at\npixel level, which is of key importance in several applications. We\nexperimentally demonstrate that our model and its variants significantly\nadvance over previous state-of-the-art results. All training and testing code\nof our model will be released to facilitate future\nresearch{{https://github.com/priba/sgol_wild}}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Riba_P/0/1/0/all/0/1\">Pau Riba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dey_S/0/1/0/all/0/1\">Sounak Dey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biten_A/0/1/0/all/0/1\">Ali Furkan Biten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Llados_J/0/1/0/all/0/1\">Josep Llados</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning-based Noise Component Map Estimation for Image Denoising. (arXiv:2109.11877v1 [eess.IV])","link":"http://arxiv.org/abs/2109.11877","description":"<p>A problem of image denoising when images are corrupted by a non-stationary\nnoise is considered in this paper. Since in practice no a priori information on\nnoise is available, noise statistics should be pre-estimated for image\ndenoising. In this paper, deep convolutional neural network (CNN) based method\nfor estimation of a map of local, patch-wise, standard deviations of noise\n(so-called sigma-map) is proposed. It achieves the state-of-the-art performance\nin accuracy of estimation of sigma-map for the case of non-stationary noise, as\nwell as estimation of noise variance for the case of additive white Gaussian\nnoise. Extensive experiments on image denoising using estimated sigma-maps\ndemonstrate that our method outperforms recent CNN-based blind image denoising\nmethods by up to 6 dB in PSNR, as well as other state-of-the-art methods based\non sigma-map estimation by up to 0.5 dB, providing same time better usage\nflexibility. Comparison with the ideal case, when denoising is applied using\nground-truth sigma-map, shows that a difference of corresponding PSNR values\nfor most of noise levels is within 0.1-0.2 dB and does not exceeds 0.6 dB.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bahnemiri_S/0/1/0/all/0/1\">Sheyda Ghanbaralizadeh Bahnemiri</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ponomarenko_M/0/1/0/all/0/1\">Mykola Ponomarenko</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Egiazarian_K/0/1/0/all/0/1\">Karen Egiazarian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tackling Inter-Class Similarity and Intra-Class Variance for Microscopic Image-based Classification. (arXiv:2109.11891v1 [cs.CV])","link":"http://arxiv.org/abs/2109.11891","description":"<p>Automatic classification of aquatic microorganisms is based on the\nmorphological features extracted from individual images. The current works on\ntheir classification do not consider the inter-class similarity and intra-class\nvariance that causes misclassification. We are particularly interested in the\ncase where variance within a class occurs due to discrete visual changes in\nmicroscopic images. In this paper, we propose to account for it by partitioning\nthe classes with high variance based on the visual features. Our algorithm\nautomatically decides the optimal number of sub-classes to be created and\nconsider each of them as a separate class for training. This way, the network\nlearns finer-grained visual features. Our experiments on two databases of\nfreshwater benthic diatoms and marine plankton show that our method can\noutperform the state-of-the-art approaches for classification of these aquatic\nmicroorganisms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Venkataramanan_A/0/1/0/all/0/1\">Aishwarya Venkataramanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laviale_M/0/1/0/all/0/1\">Martin Laviale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Figus_C/0/1/0/all/0/1\">C&#xe9;cile Figus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Usseglio_Polatera_P/0/1/0/all/0/1\">Philippe Usseglio-Polatera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pradalier_C/0/1/0/all/0/1\">C&#xe9;dric Pradalier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RSDet++: Point-based Modulated Loss for More Accurate Rotated Object Detection. (arXiv:2109.11906v1 [cs.CV])","link":"http://arxiv.org/abs/2109.11906","description":"<p>We classify the discontinuity of loss in both five-param and eight-param\nrotated object detection methods as rotation sensitivity error (RSE) which will\nresult in performance degeneration. We introduce a novel modulated rotation\nloss to alleviate the problem and propose a rotation sensitivity detection\nnetwork (RSDet) which is consists of an eight-param single-stage rotated object\ndetector and the modulated rotation loss. Our proposed RSDet has several\nadvantages: 1) it reformulates the rotated object detection problem as\npredicting the corners of objects while most previous methods employ a\nfive-para-based regression method with different measurement units. 2)\nmodulated rotation loss achieves consistent improvement on both five-param and\neight-param rotated object detection methods by solving the discontinuity of\nloss. To further improve the accuracy of our method on objects smaller than 10\npixels, we introduce a novel RSDet++ which is consists of a point-based\nanchor-free rotated object detector and a modulated rotation loss. Extensive\nexperiments demonstrate the effectiveness of both RSDet and RSDet++, which\nachieve competitive results on rotated object detection in the challenging\nbenchmarks DOTA1.0, DOTA1.5, and DOTA2.0. We hope the proposed method can\nprovide a new perspective for designing algorithms to solve rotated object\ndetection and pay more attention to tiny objects. The codes and models are\navailable at: https://github.com/yangxue0827/RotationDetection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qian_W/0/1/0/all/0/1\">Wen Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xue Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1\">Silong Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Junchi Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiujuan Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Autonomous Crop-Agnostic Visual Navigation in Arable Fields. (arXiv:2109.11936v1 [cs.RO])","link":"http://arxiv.org/abs/2109.11936","description":"<p>Autonomous navigation of a robot in agricultural fields is essential for\nevery task from crop monitoring through to weed management and fertilizer\napplication. Many current approaches rely on accurate GPS, however, such\ntechnology is expensive and also prone to failure~(e.g. through lack of\ncoverage). As such, navigation through sensors that can interpret their\nenvironment (such as cameras) is important to achieve the goal of autonomy in\nagriculture. In this paper, we introduce a purely vision-based navigation\nscheme which is able to reliably guide the robot through row-crop fields.\nIndependent of any global localization or mapping, this approach is able to\naccurately follow the crop-rows and switch between the rows, only using\non-board cameras. With the help of a novel crop-row detection and a novel\ncrop-row switching technique, our navigation scheme can be deployed in a wide\nrange of fields with different canopy types in various growth stages. We have\nextensively tested our approach in five different fields under various\nillumination conditions using our agricultural robotic platform (BonnBot-I).\nAnd our evaluations show that we have achieved a navigation accuracy of 3.82cm\nover five different crop fields.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ahmadi_A/0/1/0/all/0/1\">Alireza Ahmadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Halstead_M/0/1/0/all/0/1\">Michael Halstead</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCool_C/0/1/0/all/0/1\">Chris McCool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Two-Stage Mesh Deep Learning for Automated Tooth Segmentation and Landmark Localization on 3D Intraoral Scans. (arXiv:2109.11941v1 [cs.CV])","link":"http://arxiv.org/abs/2109.11941","description":"<p>Accurately segmenting teeth and identifying the corresponding anatomical\nlandmarks on dental mesh models are essential in computer-aided orthodontic\ntreatment. Manually performing these two tasks is time-consuming, tedious, and,\nmore importantly, highly dependent on orthodontists' experiences due to the\nabnormality and large-scale variance of patients' teeth. Some machine\nlearning-based methods have been designed and applied in the orthodontic field\nto automatically segment dental meshes (e.g., intraoral scans). In contrast,\nthe number of studies on tooth landmark localization is still limited. This\npaper proposes a two-stage framework based on mesh deep learning (called\nTS-MDL) for joint tooth labeling and landmark identification on raw intraoral\nscans. Our TS-MDL first adopts an end-to-end \\emph{i}MeshSegNet method (i.e., a\nvariant of the existing MeshSegNet with both improved accuracy and efficiency)\nto label each tooth on the downsampled scan. Guided by the segmentation\noutputs, our TS-MDL further selects each tooth's region of interest (ROI) on\nthe original mesh to construct a light-weight variant of the pioneering\nPointNet (i.e., PointNet-Reg) for regressing the corresponding landmark\nheatmaps. Our TS-MDL was evaluated on a real-clinical dataset, showing\npromising segmentation and localization performance. Specifically,\n\\emph{i}MeshSegNet in the first stage of TS-MDL reached an averaged Dice\nsimilarity coefficient (DSC) at $0.953\\pm0.076$, significantly outperforming\nthe original MeshSegNet. In the second stage, PointNet-Reg achieved a mean\nabsolute error (MAE) of $0.623\\pm0.718 \\, mm$ in distances between the\nprediction and ground truth for $44$ landmarks, which is superior compared with\nother networks for landmark detection. All these results suggest the potential\nusage of our TS-MDL in clinical practices.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tai-Hsien Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_C/0/1/0/all/0/1\">Chunfeng Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sanghee Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pastewait_M/0/1/0/all/0/1\">Matthew Pastewait</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piers_C/0/1/0/all/0/1\">Christian Piers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Li Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jackson_C/0/1/0/all/0/1\">Christina Jackson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_W/0/1/0/all/0/1\">Wei-Lun Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_D/0/1/0/all/0/1\">Dinggang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_C/0/1/0/all/0/1\">Ching-Chang Ko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Scene Graphs for Audio Source Separation. (arXiv:2109.11955v1 [cs.CV])","link":"http://arxiv.org/abs/2109.11955","description":"<p>State-of-the-art approaches for visually-guided audio source separation\ntypically assume sources that have characteristic sounds, such as musical\ninstruments. These approaches often ignore the visual context of these sound\nsources or avoid modeling object interactions that may be useful to better\ncharacterize the sources, especially when the same object class may produce\nvaried sounds from distinct interactions. To address this challenging problem,\nwe propose Audio Visual Scene Graph Segmenter (AVSGS), a novel deep learning\nmodel that embeds the visual structure of the scene as a graph and segments\nthis graph into subgraphs, each subgraph being associated with a unique sound\nobtained by co-segmenting the audio spectrogram. At its core, AVSGS uses a\nrecursive neural network that emits mutually-orthogonal sub-graph embeddings of\nthe visual graph using multi-head attention. These embeddings are used for\nconditioning an audio encoder-decoder towards source separation. Our pipeline\nis trained end-to-end via a self-supervised task consisting of separating audio\nsources using the visual graph from artificially mixed sounds. In this paper,\nwe also introduce an \"in the wild'' video dataset for sound source separation\nthat contains multiple non-musical sources, which we call Audio Separation in\nthe Wild (ASIW). This dataset is adapted from the AudioCaps dataset, and\nprovides a challenging, natural, and daily-life setting for source separation.\nThorough experiments on the proposed ASIW and the standard MUSIC datasets\ndemonstrate state-of-the-art sound separation performance of our method against\nrecent prior approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chatterjee_M/0/1/0/all/0/1\">Moitreya Chatterjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roux_J/0/1/0/all/0/1\">Jonathan Le Roux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahuja_N/0/1/0/all/0/1\">Narendra Ahuja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cherian_A/0/1/0/all/0/1\">Anoop Cherian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quantitative Matching of Forensic Evidence Fragments Utilizing 3D Microscopy Analysis of Fracture Surface Replicas. (arXiv:2109.11972v1 [eess.IV])","link":"http://arxiv.org/abs/2109.11972","description":"<p>Fractured surfaces carry unique details that can provide an accurate\nquantitative comparison to support comparative forensic analysis of those\nfractured surfaces. In this study, a statistical analysis comparison protocol\nwas applied to a set of 3D topological images of fractured surface pairs and\ntheir replicas to provide confidence in the quantitative statistical comparison\nbetween fractured items and their replicas. A set of 10 fractured stainless\nsteel samples was fractured from the same metal rod under controlled conditions\nand were cast using a standard forensic casting technique. Six 3D topological\nmaps with 50% overlap were acquired for each fractured pair. Spectral analysis\nwas utilized to identify the correlation between topological surface features\nat different length scales of the surface topology. We selected two frequency\nbands over the critical wavelength (which is greater than two-grain diameters)\nfor statistical comparison. Our statistical model utilized a matrix-variate-$t$\ndistribution that accounts for the image-overlap to model the match and\nnon-match population densities. A decision rule was developed to identify the\nprobability of matched and unmatched pairs of surfaces. The proposed\nmethodology correctly classified the fractured steel surfaces and their\nreplicas with a posterior probability of match exceeding 99.96%. Moreover, the\nreplication technique shows the potential to accurately replicate fracture\nsurface topological details with a wavelength greater than 20$\\mu$m, which far\nexceeds the range for comparison of most metallic alloys of 50-200$\\mu$m. The\ndeveloped framework establishes the basis of forensic comparison of fractured\narticles and their replicas while providing a reliable quantitative statistical\nforensic comparison, utilizing fracture mechanics-based analysis of the\nfracture surface topology.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Dawood_B/0/1/0/all/0/1\">Bishoy Dawood</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Llosa_Vite_C/0/1/0/all/0/1\">Carlos Llosa-Vite</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Thompson_G/0/1/0/all/0/1\">Geoffrey Z. Thompson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lograsso_B/0/1/0/all/0/1\">Barbara K. Lograsso</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Claytor_L/0/1/0/all/0/1\">Lauren K. Claytor</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vanderkolk_J/0/1/0/all/0/1\">John Vanderkolk</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meeker_W/0/1/0/all/0/1\">William Meeker</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maitra_R/0/1/0/all/0/1\">Ranjan Maitra</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bastawros_A/0/1/0/all/0/1\">Ashraf Bastawros</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From images in the wild to video-informed image classification. (arXiv:2109.12040v1 [cs.CV])","link":"http://arxiv.org/abs/2109.12040","description":"<p>Image classifiers work effectively when applied on structured images, yet\nthey often fail when applied on images with very high visual complexity. This\npaper describes experiments applying state-of-the-art object classifiers toward\na unique set of images in the wild with high visual complexity collected on the\nisland of Bali. The text describes differences between actual images in the\nwild and images from Imagenet, and then discusses a novel approach combining\ninformational cues particular to video with an ensemble of imperfect\nclassifiers in order to improve classification results on video sourced images\nof plants in the wild.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bohlen_M/0/1/0/all/0/1\">Marc B&#xf6;hlen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandola_V/0/1/0/all/0/1\">Varun Chandola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sujarwo_W/0/1/0/all/0/1\">Wawan Sujarwo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_R/0/1/0/all/0/1\">Raunaq Jain</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepStroke: An Efficient Stroke Screening Framework for Emergency Rooms with Multimodal Adversarial Deep Learning. (arXiv:2109.12065v1 [cs.CV])","link":"http://arxiv.org/abs/2109.12065","description":"<p>In an emergency room (ER) setting, the diagnosis of stroke is a common\nchallenge. Due to excessive execution time and cost, an MRI scan is usually not\navailable in the ER. Clinical tests are commonly referred to in stroke\nscreening, but neurologists may not be immediately available. We propose a\nnovel multimodal deep learning framework, DeepStroke, to achieve computer-aided\nstroke presence assessment by recognizing the patterns of facial motion\nincoordination and speech inability for patients with suspicion of stroke in an\nacute setting. Our proposed DeepStroke takes video data for local facial\nparalysis detection and audio data for global speech disorder analysis. It\nfurther leverages a multi-modal lateral fusion to combine the low- and\nhigh-level features and provides mutual regularization for joint training. A\nnovel adversarial training loss is also introduced to obtain\nidentity-independent and stroke-discriminative features. Experiments on our\nvideo-audio dataset with actual ER patients show that the proposed approach\noutperforms state-of-the-art models and achieves better performance than ER\ndoctors, attaining a 6.60% higher sensitivity and maintaining 4.62% higher\naccuracy when specificity is aligned. Meanwhile, each assessment can be\ncompleted in less than 6 minutes, demonstrating the framework's great potential\nfor clinical implementation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_T/0/1/0/all/0/1\">Tongan Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_H/0/1/0/all/0/1\">Haomiao Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1\">Mingli Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaolei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_K/0/1/0/all/0/1\">Kelvin Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Volpi_J/0/1/0/all/0/1\">John Volpi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">James Z. Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_S/0/1/0/all/0/1\">Stephen T.C. Wong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ZSD-YOLO: Zero-Shot YOLO Detection using Vision-Language KnowledgeDistillation. (arXiv:2109.12066v1 [cs.CV])","link":"http://arxiv.org/abs/2109.12066","description":"<p>Real-world object sampling produces long-tailed distributions requiring\nexponentially more images for rare types. Zero-shot detection, which aims to\ndetect unseen objects, is one direction to address this problem. A dataset such\nas COCO is extensively annotated across many images but with a sparse number of\ncategories and annotating all object classes across a diverse domain is\nexpensive and challenging. To advance zero-shot detection, we develop a\nVision-Language distillation method that aligns both image and text embeddings\nfrom a zero-shot pre-trained model such as CLIP to a modified semantic\nprediction head from a one-stage detector like YOLOv5. With this method, we are\nable to train an object detector that achieves state-of-the-art accuracy on the\nCOCO zero-shot detection splits with fewer model parameters. During inference,\nour model can be adapted to detect any number of object classes without\nadditional training. We also find that the improvements provided by the scaling\nof our method are consistent across various YOLOv5 scales. Furthermore, we\ndevelop a self-training method that provides a significant score improvement\nwithout needing extra images nor labels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Johnathan Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Shuai Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLIPort: What and Where Pathways for Robotic Manipulation. (arXiv:2109.12098v1 [cs.RO])","link":"http://arxiv.org/abs/2109.12098","description":"<p>How can we imbue robots with the ability to manipulate objects precisely but\nalso to reason about them in terms of abstract concepts? Recent works in\nmanipulation have shown that end-to-end networks can learn dexterous skills\nthat require precise spatial reasoning, but these methods often fail to\ngeneralize to new goals or quickly learn transferable concepts across tasks. In\nparallel, there has been great progress in learning generalizable semantic\nrepresentations for vision and language by training on large-scale internet\ndata, however these representations lack the spatial understanding necessary\nfor fine-grained manipulation. To this end, we propose a framework that\ncombines the best of both worlds: a two-stream architecture with semantic and\nspatial pathways for vision-based manipulation. Specifically, we present\nCLIPort, a language-conditioned imitation-learning agent that combines the\nbroad semantic understanding (what) of CLIP [1] with the spatial precision\n(where) of Transporter [2]. Our end-to-end framework is capable of solving a\nvariety of language-specified tabletop tasks from packing unseen objects to\nfolding cloths, all without any explicit representations of object poses,\ninstance segmentations, memory, symbolic states, or syntactic structures.\nExperiments in simulated and real-world settings show that our approach is data\nefficient in few-shot settings and generalizes effectively to seen and unseen\nsemantic concepts. We even learn one multi-task policy for 10 simulated and 9\nreal-world tasks that is better or comparable to single-task policies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shridhar_M/0/1/0/all/0/1\">Mohit Shridhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manuelli_L/0/1/0/all/0/1\">Lucas Manuelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fox_D/0/1/0/all/0/1\">Dieter Fox</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ImplicitVol: Sensorless 3D Ultrasound Reconstruction with Deep Implicit Representation. (arXiv:2109.12108v1 [eess.IV])","link":"http://arxiv.org/abs/2109.12108","description":"<p>The objective of this work is to achieve sensorless reconstruction of a 3D\nvolume from a set of 2D freehand ultrasound images with deep implicit\nrepresentation. In contrast to the conventional way that represents a 3D volume\nas a discrete voxel grid, we do so by parameterizing it as the zero level-set\nof a continuous function, i.e. implicitly representing the 3D volume as a\nmapping from the spatial coordinates to the corresponding intensity values. Our\nproposed model, termed as ImplicitVol, takes a set of 2D scans and their\nestimated locations in 3D as input, jointly re?fing the estimated 3D locations\nand learning a full reconstruction of the 3D volume. When testing on real 2D\nultrasound images, novel cross-sectional views that are sampled from\nImplicitVol show significantly better visual quality than those sampled from\nexisting reconstruction approaches, outperforming them by over 30% (NCC and\nSSIM), between the output and ground-truth on the 3D volume testing data. The\ncode will be made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yeung_P/0/1/0/all/0/1\">Pak-Hei Yeung</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hesse_L/0/1/0/all/0/1\">Linde Hesse</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Aliasi_M/0/1/0/all/0/1\">Moska Aliasi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Haak_M/0/1/0/all/0/1\">Monique Haak</a>, the <a href=\"http://arxiv.org/find/eess/1/au:+Consortium_I/0/1/0/all/0/1\">INTERGROWTH-21st Consortium</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xie_W/0/1/0/all/0/1\">Weidi Xie</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Namburete_A/0/1/0/all/0/1\">Ana I.L. Namburete</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Goal-Conditioned End-to-End Visuomotor Control for Versatile Skill Primitives. (arXiv:2003.08854v3 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2003.08854","description":"<p>Visuomotor control (VMC) is an effective means of achieving basic\nmanipulation tasks such as pushing or pick-and-place from raw images.\nConditioning VMC on desired goal states is a promising way of achieving\nversatile skill primitives. However, common conditioning schemes either rely on\ntask-specific fine tuning - e.g. using one-shot imitation learning (IL) - or on\nsampling approaches using a forward model of scene dynamics i.e.\nmodel-predictive control (MPC), leaving deployability and planning horizon\nseverely limited. In this paper we propose a conditioning scheme which avoids\nthese pitfalls by learning the controller and its conditioning in an end-to-end\nmanner. Our model predicts complex action sequences based directly on a dynamic\nimage representation of the robot motion and the distance to a given target\nobservation. In contrast to related works, this enables our approach to\nefficiently perform complex manipulation tasks from raw image observations\nwithout predefined control primitives or test time demonstrations. We report\nsignificant improvements in task success over representative MPC and IL\nbaselines. We also demonstrate our model's generalisation capabilities in\nchallenging, unseen tasks featuring visual noise, cluttered scenes and unseen\nobject geometries.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Groth_O/0/1/0/all/0/1\">Oliver Groth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hung_C/0/1/0/all/0/1\">Chia-Man Hung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vedaldi_A/0/1/0/all/0/1\">Andrea Vedaldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Posner_I/0/1/0/all/0/1\">Ingmar Posner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scan-based Semantic Segmentation of LiDAR Point Clouds: An Experimental Study. (arXiv:2004.11803v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2004.11803","description":"<p>Autonomous vehicles need to have a semantic understanding of the\nthree-dimensional world around them in order to reason about their environment.\nState of the art methods use deep neural networks to predict semantic classes\nfor each point in a LiDAR scan. A powerful and efficient way to process LiDAR\nmeasurements is to use two-dimensional, image-like projections. In this work,\nwe perform a comprehensive experimental study of image-based semantic\nsegmentation architectures for LiDAR point clouds. We demonstrate various\ntechniques to boost the performance and to improve runtime as well as memory\nconstraints.\n</p>\n<p>First, we examine the effect of network size and suggest that much faster\ninference times can be achieved at a very low cost to accuracy. Next, we\nintroduce an improved point cloud projection technique that does not suffer\nfrom systematic occlusions. We use a cyclic padding mechanism that provides\ncontext at the horizontal field-of-view boundaries. In a third part, we perform\nexperiments with a soft Dice loss function that directly optimizes for the\nintersection-over-union metric. Finally, we propose a new kind of convolution\nlayer with a reduced amount of weight-sharing along one of the two spatial\ndimensions, addressing the large difference in appearance along the vertical\naxis of a LiDAR scan. We propose a final set of the above methods with which\nthe model achieves an increase of 3.2% in mIoU segmentation performance over\nthe baseline while requiring only 42% of the original inference time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Triess_L/0/1/0/all/0/1\">Larissa T. Triess</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peter_D/0/1/0/all/0/1\">David Peter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rist_C/0/1/0/all/0/1\">Christoph B. Rist</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zollner_J/0/1/0/all/0/1\">J. Marius Z&#xf6;llner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond Triplet Loss: Meta Prototypical N-tuple Loss for Person Re-identification. (arXiv:2006.04991v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2006.04991","description":"<p>Person Re-identification (ReID) aims at matching a person of interest across\nimages. In convolutional neural network (CNN) based approaches, loss design\nplays a vital role in pulling closer features of the same identity and pushing\nfar apart features of different identities. In recent years, triplet loss\nachieves superior performance and is predominant in ReID. However, triplet loss\nconsiders only three instances of two classes in per-query optimization (with\nan anchor sample as query) and it is actually equivalent to a two-class\nclassification. There is a lack of loss design which enables the joint\noptimization of multiple instances (of multiple classes) within per-query\noptimization for person ReID. In this paper, we introduce a multi-class\nclassification loss, i.e., N-tuple loss, to jointly consider multiple (N)\ninstances for per-query optimization. This in fact aligns better with the ReID\ntest/inference process, which conducts the ranking/comparisons among multiple\ninstances. Furthermore, for more efficient multi-class classification, we\npropose a new meta prototypical N-tuple loss. With the multi-class\nclassification incorporated, our model achieves the state-of-the-art\nperformance on the benchmark person ReID datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhizheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_C/0/1/0/all/0/1\">Cuiling Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1\">Wenjun Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhibo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shih-Fu Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Kronecker CP Decomposition with Fast Multiplication for Compressing RNNs. (arXiv:2008.09342v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2008.09342","description":"<p>Recurrent neural networks (RNNs) are powerful in the tasks oriented to\nsequential data, such as natural language processing and video recognition.\nHowever, since the modern RNNs, including long-short term memory (LSTM) and\ngated recurrent unit (GRU) networks, have complex topologies and expensive\nspace/computation complexity, compressing them becomes a hot and promising\ntopic in recent years. Among plenty of compression methods, tensor\ndecomposition, e.g., tensor train (TT), block term (BT), tensor ring (TR) and\nhierarchical Tucker (HT), appears to be the most amazing approach since a very\nhigh compression ratio might be obtained. Nevertheless, none of these tensor\ndecomposition formats can provide both the space and computation efficiency. In\nthis paper, we consider to compress RNNs based on a novel Kronecker\nCANDECOMP/PARAFAC (KCP) decomposition, which is derived from Kronecker tensor\n(KT) decomposition, by proposing two fast algorithms of multiplication between\nthe input and the tensor-decomposed weight. According to our experiments based\non UCF11, Youtube Celebrities Face and UCF50 datasets, it can be verified that\nthe proposed KCP-RNNs have comparable performance of accuracy with those in\nother tensor-decomposed formats, and even 278,219x compression ratio could be\nobtained by the low rank KCP. More importantly, KCP-RNNs are efficient in both\nspace and computation complexity compared with other tensor-decomposed ones\nunder similar ranks. Besides, we find KCP has the best potential for parallel\ncomputing to accelerate the calculations in neural networks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dingheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Bijiao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1\">Guangshe Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_M/0/1/0/all/0/1\">Man Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hengnu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_L/0/1/0/all/0/1\">Lei Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_T/0/1/0/all/0/1\">Tianyi Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guoqi Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AFP-SRC: Identification of Antifreeze Proteins Using Sparse Representation Classifier. (arXiv:2009.05277v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2009.05277","description":"<p>Species living in the extreme cold environment fight against the harsh\nconditions using antifreeze proteins (AFPs), that manipulates the freezing\nmechanism of water in more than one way. This amazing nature of AFP turns out\nto be extremely useful in several industrial and medical applications. The lack\nof similarity in their structure and sequence makes their prediction an arduous\ntask and identifying them experimentally in the wet-lab is time-consuming and\nexpensive. In this research, we propose a computational framework for the\nprediction of AFPs which is essentially based on a sample-specific\nclassification method using the sparse reconstruction. A linear model and an\nover-complete dictionary matrix of known AFPs are used to predict a sparse\nclass-label vector that provides a sample-association score. Delta-rule is\napplied for the reconstruction of two pseudo-samples using lower and upper\nparts of the sample-association vector and based on the minimum recovery score,\nclass labels are assigned. We compare our approach with contemporary methods on\na standard dataset and the proposed method is found to outperform in terms of\nBalanced accuracy and Youden's index. The MATLAB implementation of the proposed\nmethod is available at the author's GitHub page\n(\\{https://github.com/Shujaat123/AFP-SRC}{https://github.com/Shujaat123/AFP-SRC}).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1\">Shujaat Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Usman_M/0/1/0/all/0/1\">Muhammad Usman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wahab_A/0/1/0/all/0/1\">Abdul Wahab</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is First Person Vision Challenging for Object Tracking?. (arXiv:2011.12263v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.12263","description":"<p>Understanding human-object interactions is fundamental in First Person Vision\n(FPV). Tracking algorithms which follow the objects manipulated by the camera\nwearer can provide useful cues to effectively model such interactions. Despite\na few previous attempts to exploit trackers in FPV applications, a methodical\nanalysis of the performance of state-of-the-art visual trackers in this domain\nis still missing. In this short paper, we provide a recap of the first\nsystematic study of object tracking in FPV. Our work extensively analyses the\nperformance of recent and baseline FPV trackers with respect to different\naspects. This is achieved through TREK-150, a novel benchmark dataset composed\nof 150 densely annotated video sequences. The results suggest that more\nresearch efforts should be devoted to this problem so that tracking could\nbenefit FPV tasks. The full version of this paper is available at\n<a href=\"/abs/2108.13665\">arXiv:2108.13665</a>.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dunnhofer_M/0/1/0/all/0/1\">Matteo Dunnhofer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Furnari_A/0/1/0/all/0/1\">Antonino Furnari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farinella_G/0/1/0/all/0/1\">Giovanni Maria Farinella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Micheloni_C/0/1/0/all/0/1\">Christian Micheloni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepKoCo: Efficient latent planning with a task-relevant Koopman representation. (arXiv:2011.12690v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2011.12690","description":"<p>This paper presents DeepKoCo, a novel model-based agent that learns a latent\nKoopman representation from images. This representation allows DeepKoCo to plan\nefficiently using linear control methods, such as linear model predictive\ncontrol. Compared to traditional agents, DeepKoCo learns task-relevant\ndynamics, thanks to the use of a tailored lossy autoencoder network that allows\nDeepKoCo to learn latent dynamics that reconstruct and predict only observed\ncosts, rather than all observed dynamics. As our results show, DeepKoCo\nachieves similar final performance as traditional model-free methods on complex\ncontrol tasks while being considerably more robust to distractor dynamics,\nmaking the proposed agent more amenable for real-life applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Heijden_B/0/1/0/all/0/1\">Bas van der Heijden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferranti_L/0/1/0/all/0/1\">Laura Ferranti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kober_J/0/1/0/all/0/1\">Jens Kober</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Babuska_R/0/1/0/all/0/1\">Robert Babuska</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Faster Convergence in Deep-Predictive-Coding Networks to Learn Deeper Representations. (arXiv:2101.06848v4 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2101.06848","description":"<p>Deep-predictive-coding networks (DPCNs) are hierarchical, generative models.\nThey rely on feed-forward and feed-back connections to modulate latent feature\nrepresentations of stimuli in a dynamic and context-sensitive manner. A crucial\nelement of DPCNs is a forward-backward inference procedure to uncover sparse,\ninvariant features. However, this inference is a major computational\nbottleneck. It severely limits the network depth due to learning stagnation.\nHere, we prove why this bottleneck occurs. We then propose a new\nforward-inference strategy based on accelerated proximal gradients. This\nstrategy has faster theoretical convergence guarantees than the one used for\nDPCNs. It overcomes learning stagnation. We also demonstrate that it permits\nconstructing deep and wide predictive-coding networks. Such convolutional\nnetworks implement receptive fields that capture well the entire classes of\nobjects on which the networks are trained. This improves the feature\nrepresentations compared with our lab's previous non-convolutional and\nconvolutional DPCNs. It yields unsupervised object recognition that surpass\nconvolutional autoencoders and are on par with convolutional networks trained\nin a supervised manner.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sledge_I/0/1/0/all/0/1\">Isaac J. Sledge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Principe_J/0/1/0/all/0/1\">Jose C. Principe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MISA: Online Defense of Trojaned Models using Misattributions. (arXiv:2103.15918v2 [cs.CR] UPDATED)","link":"http://arxiv.org/abs/2103.15918","description":"<p>Recent studies have shown that neural networks are vulnerable to Trojan\nattacks, where a network is trained to respond to specially crafted trigger\npatterns in the inputs in specific and potentially malicious ways. This paper\nproposes MISA, a new online approach to detect Trojan triggers for neural\nnetworks at inference time. Our approach is based on a novel notion called\nmisattributions, which captures the anomalous manifestation of a Trojan\nactivation in the feature space. Given an input image and the corresponding\noutput prediction, our algorithm first computes the model's attribution on\ndifferent features. It then statistically analyzes these attributions to\nascertain the presence of a Trojan trigger. Across a set of benchmarks, we show\nthat our method can effectively detect Trojan triggers for a wide variety of\ntrigger patterns, including several recent ones for which there are no known\ndefenses. Our method achieves 96% AUC for detecting images that include a\nTrojan trigger without any assumptions on the trigger pattern.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kiourti_P/0/1/0/all/0/1\">Panagiota Kiourti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenchao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_A/0/1/0/all/0/1\">Anirban Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sikka_K/0/1/0/all/0/1\">Karan Sikka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1\">Susmit Jha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MRI-based Alzheimer's disease prediction via distilling the knowledge in multi-modal data. (arXiv:2104.03618v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2104.03618","description":"<p>Mild cognitive impairment (MCI) conversion prediction, i.e., identifying MCI\npatients of high risks converting to Alzheimer's disease (AD), is essential for\npreventing or slowing the progression of AD. Although previous studies have\nshown that the fusion of multi-modal data can effectively improve the\nprediction accuracy, their applications are largely restricted by the limited\navailability or high cost of multi-modal data. Building an effective prediction\nmodel using only magnetic resonance imaging (MRI) remains a challenging\nresearch topic. In this work, we propose a multi-modal multi-instance\ndistillation scheme, which aims to distill the knowledge learned from\nmulti-modal data to an MRI-based network for MCI conversion prediction. In\ncontrast to existing distillation algorithms, the proposed multi-instance\nprobabilities demonstrate a superior capability of representing the complicated\natrophy distributions, and can guide the MRI-based network to better explore\nthe input MRI. To our best knowledge, this is the first study that attempts to\nimprove an MRI-based prediction model by leveraging extra supervision distilled\nfrom multi-modal information. Experiments demonstrate the advantage of our\nframework, suggesting its potentials in the data-limited clinical settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Guan_H/0/1/0/all/0/1\">Hao Guan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_C/0/1/0/all/0/1\">Chaoyue Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Imaginative Walks: Generative Random Walk Deviation Loss for Improved Unseen Learning Representation. (arXiv:2104.09757v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.09757","description":"<p>We propose a novel loss for generative models, dubbed as GRaWD (Generative\nRandom Walk Deviation), to improve learning representations of unexplored\nvisual spaces. Quality learning representation of unseen classes (or styles) is\ncritical to facilitate novel image generation and better generative\nunderstanding of unseen visual classes, i.e., zero-shot learning (ZSL). By\ngenerating representations of unseen classes based on their semantic\ndescriptions, e.g., attributes or text, generative ZSL attempts to\ndifferentiate unseen from seen categories. The proposed GRaWD loss is defined\nby constructing a dynamic graph that includes the seen class/style centers and\ngenerated samples in the current minibatch. Our loss initiates a random walk\nprobability from each center through visual generations produced from\nhallucinated unseen classes. As a deviation signal, we encourage the random\nwalk to eventually land after t steps in a feature representation that is\ndifficult to classify as any of the seen classes. We demonstrate that the\nproposed loss can improve unseen class representation quality inductively on\ntext-based ZSL benchmarks on CUB and NABirds datasets and attribute-based ZSL\nbenchmarks on AWA2, SUN, and aPY datasets. In addition, we investigate the\nability of the proposed loss to generate meaningful novel visual art on the\nWikiArt dataset. The results of experiments and human evaluations demonstrate\nthat the proposed GRaWD loss can improve StyleGAN1 and StyleGAN2 generation\nquality and create novel art that is significantly more preferable. Our code is\nmade publicly available at https://github.com/Vision-CAIR/GRaWD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jha_D/0/1/0/all/0/1\">Divyansh Jha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_K/0/1/0/all/0/1\">Kai Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skorokhodov_I/0/1/0/all/0/1\">Ivan Skorokhodov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elhoseiny_M/0/1/0/all/0/1\">Mohamed Elhoseiny</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection. (arXiv:2104.10956v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.10956","description":"<p>Monocular 3D object detection is an important task for autonomous driving\nconsidering its advantage of low cost. It is much more challenging than\nconventional 2D cases due to its inherent ill-posed property, which is mainly\nreflected in the lack of depth information. Recent progress on 2D detection\noffers opportunities to better solving this problem. However, it is non-trivial\nto make a general adapted 2D detector work in this 3D task. In this paper, we\nstudy this problem with a practice built on a fully convolutional single-stage\ndetector and propose a general framework FCOS3D. Specifically, we first\ntransform the commonly defined 7-DoF 3D targets to the image domain and\ndecouple them as 2D and 3D attributes. Then the objects are distributed to\ndifferent feature levels with consideration of their 2D scales and assigned\nonly according to the projected 3D-center for the training procedure.\nFurthermore, the center-ness is redefined with a 2D Gaussian distribution based\non the 3D-center to fit the 3D target formulation. All of these make this\nframework simple yet effective, getting rid of any 2D detection or 2D-3D\ncorrespondence priors. Our solution achieves 1st place out of all the\nvision-only methods in the nuScenes 3D detection challenge of NeurIPS 2020.\nCode and models are released at https://github.com/open-mmlab/mmdetection3d.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xinge Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_J/0/1/0/all/0/1\">Jiangmiao Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1\">Dahua Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Boosting Light-Weight Depth Estimation Via Knowledge Distillation. (arXiv:2105.06143v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.06143","description":"<p>The advanced performance of depth estimation is achieved by the employment of\nlarge and complex neural networks. While the performance is still being\ncontinuously improved, we argue that the depth estimation has to be efficient\nas well since it is a preliminary requirement for real-world applications.\nHowever, fast depth estimation tends to lower the performance as the trade-off\nbetween the model's capacity and accuracy. In this paper, we aim to achieve\naccurate depth estimation with a light-weight network. To this end, we first\nintroduce a highly compact network that can estimate a depth map in real-time.\nWe then develop a knowledge distillation paradigm to further improve the\nperformance. We observe that many scenarios have the same scene scales in\nreal-world, yielding similar depth histograms, thus they are potentially\nvaluable and applicable to develop a better learning strategy. Therefore, we\npropose to employ auxiliary unlabeled/labeled data to improve knowledge\ndistillation. Through extensive and rigorous experiments, we show that our\nmethod can achieve comparable performance against state-of-the-of-art methods\nwith only 1% parameters, and outperforms previous light-weight methods in terms\nof inference accuracy, computational efficiency and generalizability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Junjie Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1\">Chenyou Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Hualie Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xiyue Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yuan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiangyong Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_T/0/1/0/all/0/1\">Tin Lun Lam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Orthogonal Classifier for Improving the Adversarial Robustness of Neural Networks. (arXiv:2105.09109v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.09109","description":"<p>Neural networks are susceptible to artificially designed adversarial\nperturbations. Recent efforts have shown that imposing certain modifications on\nclassification layer can improve the robustness of the neural networks. In this\npaper, we explicitly construct a dense orthogonal weight matrix whose entries\nhave the same magnitude, thereby leading to a novel robust classifier. The\nproposed classifier avoids the undesired structural redundancy issue in\nprevious work. Applying this classifier in standard training on clean data is\nsufficient to ensure the high accuracy and good robustness of the model.\nMoreover, when extra adversarial samples are used, better robustness can be\nfurther obtained with the help of a special worst-case loss. Experimental\nresults show that our method is efficient and competitive to many\nstate-of-the-art defensive approaches. Our code is available at\n\\url{https://github.com/MTandHJ/roboc}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Cong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Min Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SHD360: A Benchmark Dataset for Salient Human Detection in 360{\\deg} Videos. (arXiv:2105.11578v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.11578","description":"<p>Salient human detection (SHD) in dynamic 360{\\deg} immersive videos is of\ngreat importance for various applications such as robotics, inter-human and\nhuman-object interaction in augmented reality. However, 360{\\deg} video SHD has\nbeen seldom discussed in the computer vision community due to a lack of\ndatasets with large-scale omnidirectional videos and rich annotations. To this\nend, we propose SHD360, the first 360{\\deg} video SHD dataset which contains\nvarious real-life daily scenes. Our SHD360 provides six-level hierarchical\nannotations for 6,268 key frames uniformly sampled from 37,403 omnidirectional\nvideo frames at 4K resolution. Specifically, each collected frame is labeled\nwith a super-class, a sub-class, associated attributes (e.g., geometrical\ndistortion), bounding boxes and per-pixel object-/instance-level masks. As a\nresult, our SHD360 contains totally 16,238 salient human instances with\nmanually annotated pixel-wise ground truth. Since so far there is no method\nproposed for 360{\\deg} image/video SHD, we systematically benchmark 11\nrepresentative state-of-the-art salient object detection (SOD) approaches on\nour SHD360, and explore key issues derived from extensive experimenting\nresults. We hope our proposed dataset and benchmark could serve as a good\nstarting point for advancing human-centric researches towards 360{\\deg}\npanoramic data. Our dataset and benchmark is publicly available at\nhttps://github.com/PanoAsh/SHD360.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamidouche_W/0/1/0/all/0/1\">Wassim Hamidouche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deforges_O/0/1/0/all/0/1\">Olivier Deforges</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-StyleGAN: Towards Image-Based Simulation of Time-Lapse Live-Cell Microscopy. (arXiv:2106.08285v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.08285","description":"<p>Time-lapse fluorescent microscopy (TLFM) combined with predictive\nmathematical modelling is a powerful tool to study the inherently dynamic\nprocesses of life on the single-cell level. Such experiments are costly,\ncomplex and labour intensive. A complimentary approach and a step towards in\nsilico experimentation, is to synthesise the imagery itself. Here, we propose\nMulti-StyleGAN as a descriptive approach to simulate time-lapse fluorescence\nmicroscopy imagery of living cells, based on a past experiment. This novel\ngenerative adversarial network synthesises a multi-domain sequence of\nconsecutive timesteps. We showcase Multi-StyleGAN on imagery of multiple live\nyeast cells in microstructured environments and train on a dataset recorded in\nour laboratory. The simulation captures underlying biophysical factors and time\ndependencies, such as cell morphology, growth, physical interactions, as well\nas the intensity of a fluorescent reporter protein. An immediate application is\nto generate additional training and validation data for feature extraction\nalgorithms or to aid and expedite development of advanced experimental\ntechniques such as online monitoring or control of cells.\n</p>\n<p>Code and dataset is available at\nhttps://git.rwth-aachen.de/bcs/projects/tp/multi-stylegan.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Reich_C/0/1/0/all/0/1\">Christoph Reich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prangemeier_T/0/1/0/all/0/1\">Tim Prangemeier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wildner_C/0/1/0/all/0/1\">Christian Wildner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koeppl_H/0/1/0/all/0/1\">Heinz Koeppl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond the Hausdorff Metric in Digital Topology. (arXiv:2108.03114v2 [cs.CG] UPDATED)","link":"http://arxiv.org/abs/2108.03114","description":"<p>Two objects may be close in the Hausdorff metric, yet have very different\ngeometric and topological properties. We examine other methods of comparing\ndigital images such that objects close in each of these measures have some\nsimilar geometric or topological property. Such measures may be combined with\nthe Hausdorff metric to yield a metric in which close images are similar with\nrespect to multiple properties.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Boxer_L/0/1/0/all/0/1\">Laurence Boxer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards to Robust and Generalized Medical Image Segmentation Framework. (arXiv:2108.03823v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.03823","description":"<p>To mitigate the radiologist's workload, computer-aided diagnosis with the\ncapability to review and analyze medical images is gradually deployed. Deep\nlearning-based region of interest segmentation is among the most exciting use\ncases. However, this paradigm is restricted in real-world clinical applications\ndue to poor robustness and generalization. The issue is more sinister with a\nlack of training data. In this paper, we address the challenge from the\nrepresentation learning point of view. We investigate that the collapsed\nrepresentations, as one of the main reasons which caused poor robustness and\ngeneralization, could be avoided through transfer learning. Therefore, we\npropose a novel two-stage framework for robust generalized segmentation. In\nparticular, an unsupervised Tile-wise AutoEncoder (T-AE) pretraining\narchitecture is coined to learn meaningful representation for improving the\ngeneralization and robustness of the downstream tasks. Furthermore, the learned\nknowledge is transferred to the segmentation benchmark. Coupled with an image\nreconstruction network, the representation keeps to be decoded, encouraging the\nmodel to capture more semantic features. Experiments of lung segmentation on\nmulti chest X-ray datasets are conducted. Empirically, the related experimental\nresults demonstrate the superior generalization capability of the proposed\nframework on unseen domains in terms of high performance and robustness to\ncorruption, especially under the scenario of the limited training data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yurong Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"White blood cell subtype detection and classification. (arXiv:2108.04614v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.04614","description":"<p>Machine learning has endless applications in the health care industry. White\nblood cell classification is one of the interesting and promising area of\nresearch. The classification of the white blood cells plays an important part\nin the medical diagnosis. In practise white blood cell classification is\nperformed by the haematologist by taking a small smear of blood and careful\nexamination under the microscope. The current procedures to identify the white\nblood cell subtype is more time taking and error-prone. The computer aided\ndetection and diagnosis of the white blood cells tend to avoid the human error\nand reduce the time taken to classify the white blood cells. In the recent\nyears several deep learning approaches have been developed in the context of\nclassification of the white blood cells that are able to identify but are\nunable to localize the positions of white blood cells in the blood cell image.\nFollowing this, the present research proposes to utilize YOLOv3 object\ndetection technique to localize and classify the white blood cells with\nbounding boxes. With exhaustive experimental analysis, the proposed work is\nfound to detect the white blood cell with 99.2% accuracy and classify with 90%\naccuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Praveen_N/0/1/0/all/0/1\">Nalla Praveen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Punn_N/0/1/0/all/0/1\">Narinder Singh Punn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sonbhadra_S/0/1/0/all/0/1\">Sanjay Kumar Sonbhadra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1\">Sonali Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Syafrullah_M/0/1/0/all/0/1\">M. Syafrullah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adiyarta_K/0/1/0/all/0/1\">Krisna Adiyarta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ReGenMorph: Visibly Realistic GAN Generated Face Morphing Attacks by Attack Re-generation. (arXiv:2108.09130v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.09130","description":"<p>Face morphing attacks aim at creating face images that are verifiable to be\nthe face of multiple identities, which can lead to building faulty identity\nlinks in operations like border checks. While creating a morphed face detector\n(MFD), training on all possible attack types is essential to achieve good\ndetection performance. Therefore, investigating new methods of creating\nmorphing attacks drives the generalizability of MADs. Creating morphing attacks\nwas performed on the image level, by landmark interpolation, or on the\nlatent-space level, by manipulating latent vectors in a generative adversarial\nnetwork. The earlier results in varying blending artifacts and the latter\nresults in synthetic-like striping artifacts. This work presents the novel\nmorphing pipeline, ReGenMorph, to eliminate the LMA blending artifacts by using\na GAN-based generation, as well as, eliminate the manipulation in the latent\nspace, resulting in visibly realistic morphed images compared to previous\nworks. The generated ReGenMorph appearance is compared to recent morphing\napproaches and evaluated for face recognition vulnerability and attack\ndetectability, whether as known or unknown attacks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Damer_N/0/1/0/all/0/1\">Naser Damer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raja_K/0/1/0/all/0/1\">Kiran Raja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sussmilch_M/0/1/0/all/0/1\">Marius S&#xfc;&#xdf;milch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkatesh_S/0/1/0/all/0/1\">Sushma Venkatesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boutros_F/0/1/0/all/0/1\">Fadi Boutros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_M/0/1/0/all/0/1\">Meiling Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirchbuchner_F/0/1/0/all/0/1\">Florian Kirchbuchner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramachandra_R/0/1/0/all/0/1\">Raghavendra Ramachandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuijper_A/0/1/0/all/0/1\">Arjan Kuijper</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dual-Neighborhood Deep Fusion Network for Point Cloud Classification. (arXiv:2108.09228v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.09228","description":"<p>Recently, deep neural networks have made remarkable achievements in 3D point\ncloud classification. However, existing classification methods are mainly\nimplemented on idealized point clouds and suffer heavy degradation of\nper-formance on non-idealized scenarios. To handle this prob-lem, a feature\nrepresentation learning method, named Dual-Neighborhood Deep Fusion Network\n(DNDFN), is proposed to serve as an improved point cloud encoder for the task\nof non-idealized point cloud classification. DNDFN utilizes a trainable\nneighborhood learning method called TN-Learning to capture the global key\nneighborhood. Then, the global neighborhood is fused with the local\nneighbor-hood to help the network achieve more powerful reasoning ability.\nBesides, an Information Transfer Convolution (IT-Conv) is proposed for DNDFN to\nlearn the edge infor-mation between point-pairs and benefits the feature\ntransfer procedure. The transmission of information in IT-Conv is similar to\nthe propagation of information in the graph which makes DNDFN closer to the\nhuman reasoning mode. Extensive experiments on existing benchmarks especially\nnon-idealized datasets verify the effectiveness of DNDFN and DNDFN achieves the\nstate of the arts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guoquan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_H/0/1/0/all/0/1\">Hezhi Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yifan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_J/0/1/0/all/0/1\">Jianwei Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Ke Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jiahua Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Chao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zeyu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yanxin Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Well Googled is Half Done: Multimodal Forecasting of New Fashion Product Sales with Image-based Google Trends. (arXiv:2109.09824v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.09824","description":"<p>This paper investigates the effectiveness of systematically probing Google\nTrendsagainst textual translations of visual aspects as exogenous knowledge to\npredict the sales of brand-new fashion items, where past sales data is not\navailable, but only an image and few metadata are available. In particular, we\npropose GTM-Transformer, standing for Google Trends Multimodal Transformer,\nwhose encoder works on the representation of the exogenous time series, while\nthe decoder forecasts the sales using the Google Trends encoding, and the\navailable visual and metadata information. Our model works in a\nnon-autoregressive manner, avoiding the compounding effect of the first-step\nerrors. As a second contribution, we present the VISUELLE dataset, which is the\nfirst publicly available dataset for the task of new fashion product sales\nforecasting, containing the sales of 5577 new products sold between 2016-2019,\nderived from genuine historical data ofNunalie, an Italian fast-fashion\ncompany. Our dataset is equipped with images of products, metadata, related\nsales, and associated Google Trends. We use VISUELLE to compare our approach\nagainst state-of-the-art alternatives and numerous baselines, showing that\nGTM-Transformer is the most accurate in terms of both percentage and absolute\nerror. It is worth noting that the addition of exogenous knowledge boosts the\nforecasting accuracy by 1.5% WAPE wise, showing the importance of exploiting\nGoogle Trends. The code and dataset are both available at\nhttps://github.com/HumaticsLAB/GTM-Transformer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Skenderi_G/0/1/0/all/0/1\">Geri Skenderi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joppi_C/0/1/0/all/0/1\">Christian Joppi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denitto_M/0/1/0/all/0/1\">Matteo Denitto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cristani_M/0/1/0/all/0/1\">Marco Cristani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Live Speech Portraits: Real-Time Photorealistic Talking-Head Animation. (arXiv:2109.10595v2 [cs.GR] UPDATED)","link":"http://arxiv.org/abs/2109.10595","description":"<p>To the best of our knowledge, we first present a live system that generates\npersonalized photorealistic talking-head animation only driven by audio signals\nat over 30 fps. Our system contains three stages. The first stage is a deep\nneural network that extracts deep audio features along with a manifold\nprojection to project the features to the target person's speech space. In the\nsecond stage, we learn facial dynamics and motions from the projected audio\nfeatures. The predicted motions include head poses and upper body motions,\nwhere the former is generated by an autoregressive probabilistic model which\nmodels the head pose distribution of the target person. Upper body motions are\ndeduced from head poses. In the final stage, we generate conditional feature\nmaps from previous predictions and send them with a candidate image set to an\nimage-to-image translation network to synthesize photorealistic renderings. Our\nmethod generalizes well to wild audio and successfully synthesizes\nhigh-fidelity personalized facial details, e.g., wrinkles, teeth. Our method\nalso allows explicit control of head poses. Extensive qualitative and\nquantitative evaluations, along with user studies, demonstrate the superiority\nof our method over state-of-the-art techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yuanxun Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_J/0/1/0/all/0/1\">Jinxiang Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1\">Xun Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HybridSDF: Combining Free Form Shapes and Geometric Primitives for effective Shape Manipulation. (arXiv:2109.10767v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.10767","description":"<p>CAD modeling typically involves the use of simple geometric primitives\nwhereas recent advances in deep-learning based 3D surface modeling have opened\nnew shape design avenues. Unfortunately, these advances have not yet been\naccepted by the CAD community because they cannot be integrated into\nengineering workflows. To remedy this, we propose a novel approach to\neffectively combining geometric primitives and free-form surfaces represented\nby implicit surfaces for accurate modeling that preserves interpretability,\nenforces consistency, and enables easy manipulation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vasu_S/0/1/0/all/0/1\">Subeesh Vasu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talabot_N/0/1/0/all/0/1\">Nicolas Talabot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lukoianov_A/0/1/0/all/0/1\">Artem Lukoianov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baque_P/0/1/0/all/0/1\">Pierre Baque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Donier_J/0/1/0/all/0/1\">Jonathan Donier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fua_P/0/1/0/all/0/1\">Pascal Fua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mixed-supervised segmentation: Confidence maximization helps knowledge distillation. (arXiv:2109.10902v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2109.10902","description":"<p>Despite achieving promising results in a breadth of medical image\nsegmentation tasks, deep neural networks require large training datasets with\npixel-wise annotations. Obtaining these curated datasets is a cumbersome\nprocess which limits the application in scenarios where annotated images are\nscarce. Mixed supervision is an appealing alternative for mitigating this\nobstacle, where only a small fraction of the data contains complete pixel-wise\nannotations and other images have a weaker form of supervision. In this work,\nwe propose a dual-branch architecture, where the upper branch (teacher)\nreceives strong annotations, while the bottom one (student) is driven by\nlimited supervision and guided by the upper branch. Combined with a standard\ncross-entropy loss over the labeled pixels, our novel formulation integrates\ntwo important terms: (i) a Shannon entropy loss defined over the\nless-supervised images, which encourages confident student predictions in the\nbottom branch; and (ii) a Kullback-Leibler (KL) divergence term, which\ntransfers the knowledge of the strongly supervised branch to the\nless-supervised branch and guides the entropy (student-confidence) term to\navoid trivial solutions. We show that the synergy between the entropy and KL\ndivergence yields substantial improvements in performance. We also discuss an\ninteresting link between Shannon-entropy minimization and standard pseudo-mask\ngeneration, and argue that the former should be preferred over the latter for\nleveraging information from unlabeled pixels. Quantitative and qualitative\nresults on two publicly available datasets demonstrate that our method\nsignificantly outperforms other strategies for semantic segmentation within a\nmixed-supervision framework, as well as recent semi-supervised approaches.\nMoreover, we show that the branch trained with reduced supervision and guided\nby the top branch largely outperforms the latter.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Liu_B/0/1/0/all/0/1\">Bingyuan Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Desrosiers_C/0/1/0/all/0/1\">Christian Desrosiers</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ayed_I/0/1/0/all/0/1\">Ismail Ben Ayed</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dolz_J/0/1/0/all/0/1\">Jose Dolz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recent Advances of Continual Learning in Computer Vision: An Overview. (arXiv:2109.11369v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.11369","description":"<p>In contrast to batch learning where all training data is available at once,\ncontinual learning represents a family of methods that accumulate knowledge and\nlearn continuously with data available in sequential order. Similar to the\nhuman learning process with the ability of learning, fusing, and accumulating\nnew knowledge coming at different time steps, continual learning is considered\nto have high practical significance. Hence, continual learning has been studied\nin various artificial intelligence tasks. In this paper, we present a\ncomprehensive review of the recent progress of continual learning in computer\nvision. In particular, the works are grouped by their representative\ntechniques, including regularization, knowledge distillation, memory,\ngenerative replay, parameter isolation, and a combination of the above\ntechniques. For each category of these techniques, both its characteristics and\napplications in computer vision are presented. At the end of this overview,\nseveral subareas, where continuous knowledge accumulation is potentially\nhelpful while continual learning has not been well studied, are discussed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qu_H/0/1/0/all/0/1\">Haoxuan Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahmani_H/0/1/0/all/0/1\">Hossein Rahmani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Li Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_B/0/1/0/all/0/1\">Bryan Williams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jun Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-09-26T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/"}}]}]}