{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-01-26T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Classification Of Fake News Headline Based On Neural Networks. (arXiv:2201.09966v1 [cs.CL])","link":"http://arxiv.org/abs/2201.09966","description":"<p>Over the last few years, Text classification is one of the fundamental tasks\nin natural language processing (NLP) in which the objective is to categorize\ntext documents into one of the predefined classes. The news is full of our\nlife. Therefore, news headlines classification is a crucial task to connect\nusers with the right news. The news headline classification is a kind of text\nclassification, which can be generally divided into three mainly parts: feature\nextraction, classifier selection, and evaluations. In this article, we use the\ndataset, containing news over a period of eighteen years provided by Kaggle\nplatform to classify news headlines. We choose TF-IDF to extract features and\nneural network as the classifier, while the evaluation metrics is accuracy.\nFrom the experiment result, it is obvious that our NN model has the best\nperformance among these models in the metrics of accuracy. The higher the\naccuracy is, the better performance the model will gain. Our NN model owns the\naccuracy 0.8622, which is highest accuracy among these four models. And it is\n0.0134, 0.033, 0.080 higher than its of other models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yahan_K/0/1/0/all/0/1\">Ke Yahan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_R/0/1/0/all/0/1\">Ruyi Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiaoxia_L/0/1/0/all/0/1\">Lu Xiaoxia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HC4: A New Suite of Test Collections for Ad Hoc CLIR. (arXiv:2201.09992v1 [cs.IR])","link":"http://arxiv.org/abs/2201.09992","description":"<p>HC4 is a new suite of test collections for ad hoc Cross-Language Information\nRetrieval (CLIR), with Common Crawl News documents in Chinese, Persian, and\nRussian, topics in English and in the document languages, and graded relevance\njudgments. New test collections are needed because existing CLIR test\ncollections built using pooling of traditional CLIR runs have systematic gaps\nin their relevance judgments when used to evaluate neural CLIR methods. The HC4\ncollections contain 60 topics and about half a million documents for each of\nChinese and Persian, and 54 topics and five million documents for Russian.\nActive learning was used to determine which documents to annotate after being\nseeded using interactive search and judgment. Documents were judged on a\nthree-grade relevance scale. This paper describes the design and construction\nof the new test collections and provides baseline results for demonstrating\ntheir utility for evaluating systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lawrie_D/0/1/0/all/0/1\">Dawn Lawrie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mayfield_J/0/1/0/all/0/1\">James Mayfield</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oard_D/0/1/0/all/0/1\">Douglas Oard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_E/0/1/0/all/0/1\">Eugene Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Razmecheno: Named Entity Recognition from Digital Archive of Diaries \"Prozhito\". (arXiv:2201.09997v1 [cs.CL])","link":"http://arxiv.org/abs/2201.09997","description":"<p>The vast majority of existing datasets for Named Entity Recognition (NER) are\nbuilt primarily on news, research papers and Wikipedia with a few exceptions,\ncreated from historical and literary texts. What is more, English is the main\nsource for data for further labelling. This paper aims to fill in multiple gaps\nby creating a novel dataset \"Razmecheno\", gathered from the diary texts of the\nproject \"Prozhito\" in Russian. Our dataset is of interest for multiple research\nlines: literary studies of diary texts, transfer learning from other domains,\nlow-resource or cross-lingual named entity recognition. Razmecheno comprises\n1331 sentences and 14119 tokens, sampled from diaries, written during the\nPerestroika. The annotation schema consists of five commonly used entity tags:\nperson, characteristics, location, organisation, and facility. The labelling is\ncarried out on the crowdsourcing platfrom Yandex.Toloka in two stages. First,\nworkers selected sentences, which contain an entity of particular type. Second,\nthey marked up entity spans. As a result 1113 entities were obtained. Empirical\nevaluation of Razmecheno is carried out with off-the-shelf NER tools and by\nfine-tuning pre-trained contextualized encoders. We release the annotated\ndataset for open access.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Atnashev_T/0/1/0/all/0/1\">Timofey Atnashev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganeeva_V/0/1/0/all/0/1\">Veronika Ganeeva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kazakov_R/0/1/0/all/0/1\">Roman Kazakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matyash_D/0/1/0/all/0/1\">Daria Matyash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sonkin_M/0/1/0/all/0/1\">Michael Sonkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Voloshina_E/0/1/0/all/0/1\">Ekaterina Voloshina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Serikov_O/0/1/0/all/0/1\">Oleg Serikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artemova_E/0/1/0/all/0/1\">Ekaterina Artemova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text and Code Embeddings by Contrastive Pre-Training. (arXiv:2201.10005v1 [cs.CL])","link":"http://arxiv.org/abs/2201.10005","description":"<p>Text embeddings are useful features in many applications such as semantic\nsearch and computing text similarity. Previous work typically trains models\ncustomized for different use cases, varying in dataset choice, training\nobjective and model architecture. In this work, we show that contrastive\npre-training on unsupervised data at scale leads to high quality vector\nrepresentations of text and code. The same unsupervised text embeddings that\nachieve new state-of-the-art results in linear-probe classification also\ndisplay impressive semantic search capabilities and sometimes even perform\ncompetitively with fine-tuned models. On linear-probe classification accuracy\naveraging over 7 tasks, our best unsupervised model achieves a relative\nimprovement of 4% and 1.8% over previous best unsupervised and supervised text\nembedding models respectively. The same text embeddings when evaluated on\nlarge-scale semantic search attains a relative improvement of 23.4%, 14.7%, and\n10.6% over previous best unsupervised methods on MSMARCO, Natural Questions and\nTriviaQA benchmarks, respectively. Similarly to text embeddings, we train code\nembedding models on (text, code) pairs, obtaining a 20.8% relative improvement\nover prior best work on code search.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Neelakantan_A/0/1/0/all/0/1\">Arvind Neelakantan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Tao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Puri_R/0/1/0/all/0/1\">Raul Puri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radford_A/0/1/0/all/0/1\">Alec Radford</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jesse Michael Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tworek_J/0/1/0/all/0/1\">Jerry Tworek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Q/0/1/0/all/0/1\">Qiming Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tezak_N/0/1/0/all/0/1\">Nikolas Tezak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jong Wook Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hallacy_C/0/1/0/all/0/1\">Chris Hallacy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heidecke_J/0/1/0/all/0/1\">Johannes Heidecke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shyam_P/0/1/0/all/0/1\">Pranav Shyam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Power_B/0/1/0/all/0/1\">Boris Power</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nekoul_T/0/1/0/all/0/1\">Tyna Eloundou Nekoul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sastry_G/0/1/0/all/0/1\">Girish Sastry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krueger_G/0/1/0/all/0/1\">Gretchen Krueger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schnurr_D/0/1/0/all/0/1\">David Schnurr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Such_F/0/1/0/all/0/1\">Felipe Petroski Such</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_K/0/1/0/all/0/1\">Kenny Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thompson_M/0/1/0/all/0/1\">Madeleine Thompson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_T/0/1/0/all/0/1\">Tabarak Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sherbakov_T/0/1/0/all/0/1\">Toki Sherbakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_J/0/1/0/all/0/1\">Joanne Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Welinder_P/0/1/0/all/0/1\">Peter Welinder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weng_L/0/1/0/all/0/1\">Lilian Weng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Documenting Geographically and Contextually Diverse Data Sources: The BigScience Catalogue of Language Data and Resources. (arXiv:2201.10066v1 [cs.CL])","link":"http://arxiv.org/abs/2201.10066","description":"<p>In recent years, large-scale data collection efforts have prioritized the\namount of data collected in order to improve the modeling capabilities of large\nlanguage models. This prioritization, however, has resulted in concerns with\nrespect to the rights of data subjects represented in data collections,\nparticularly when considering the difficulty in interrogating these collections\ndue to insufficient documentation and tools for analysis. Mindful of these\npitfalls, we present our methodology for a documentation-first, human-centered\ndata collection project as part of the BigScience initiative. We identified a\ngeographically diverse set of target language groups (Arabic, Basque, Chinese,\nCatalan, English, French, Indic languages, Indonesian, Niger-Congo languages,\nPortuguese, Spanish, and Vietnamese, as well as programming languages) for\nwhich to collect metadata on potential data sources. To structure this effort,\nwe developed our online catalogue as a supporting tool for gathering metadata\nthrough organized public hackathons. We present our development process;\nanalyses of the resulting resource metadata, including distributions over\nlanguages, regions, and resource types; and our lessons learned in this\nendeavor.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+McMillan_Major_A/0/1/0/all/0/1\">Angelina McMillan-Major</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alyafeai_Z/0/1/0/all/0/1\">Zaid Alyafeai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biderman_S/0/1/0/all/0/1\">Stella Biderman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kimbo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toni_F/0/1/0/all/0/1\">Francesco De Toni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dupont_G/0/1/0/all/0/1\">G&#xe9;rard Dupont</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elsahar_H/0/1/0/all/0/1\">Hady Elsahar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Emezue_C/0/1/0/all/0/1\">Chris Emezue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aji_A/0/1/0/all/0/1\">Alham Fikri Aji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilic_S/0/1/0/all/0/1\">Suzana Ili&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khamis_N/0/1/0/all/0/1\">Nurulaqilla Khamis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leong_C/0/1/0/all/0/1\">Colin Leong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masoud_M/0/1/0/all/0/1\">Maraim Masoud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soroa_A/0/1/0/all/0/1\">Aitor Soroa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suarez_P/0/1/0/all/0/1\">Pedro Ortiz Suarez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talat_Z/0/1/0/all/0/1\">Zeerak Talat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strien_D/0/1/0/all/0/1\">Daniel van Strien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jernite_Y/0/1/0/all/0/1\">Yacine Jernite</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Two heads are better than one: Enhancing medical representations by pre-training over structured and unstructured electronic health records. (arXiv:2201.10113v1 [cs.CL])","link":"http://arxiv.org/abs/2201.10113","description":"<p>The massive context of electronic health records (EHRs) has created enormous\npotentials for improving healthcare, among which structured (coded) data and\nunstructured (text) data are two important textual modalities. They do not\nexist in isolation and can complement each other in most real-life clinical\nscenarios. Most existing researches in medical informatics, however, either\nonly focus on a particular modality or straightforwardly concatenate the\ninformation from different modalities, which ignore the interaction and\ninformation sharing between them. To address these issues, we proposed a\nunified deep learning-based medical pre-trained language model, named UMM-PLM,\nto automatically learn representative features from multimodal EHRs that\nconsist of both structured data and unstructured data. Specifically, we first\ndeveloped parallel unimodal information representation modules to capture the\nunimodal-specific characteristic, where unimodal representations were learned\nfrom each data source separately. A cross-modal module was further introduced\nto model the interactions between different modalities. We pre-trained the\nmodel on a large EHRs dataset containing both structured data and unstructured\ndata and verified the effectiveness of the model on three downstream clinical\ntasks, i.e., medication recommendation, 30-day readmission and ICD coding\nthrough extensive experiments. The results demonstrate the power of UMM-PLM\ncompared with benchmark methods and state-of-the-art baselines. Analyses show\nthat UMM-PLM can effectively concern with multimodal textual information and\nhas the potential to provide more comprehensive interpretations for clinical\ndecision making.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sicen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaolong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yongshuai Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Ge Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hui Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_Y/0/1/0/all/0/1\">Yang Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_B/0/1/0/all/0/1\">Buzhou Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SPIRAL: Self-supervised Perturbation-Invariant Representation Learning for Speech Pre-Training. (arXiv:2201.10207v1 [eess.AS])","link":"http://arxiv.org/abs/2201.10207","description":"<p>We introduce a new approach for speech pre-training named SPIRAL which works\nby learning denoising representation of perturbed data in a teacher-student\nframework. Specifically, given a speech utterance, we first feed the utterance\nto a teacher network to obtain corresponding representation. Then the same\nutterance is perturbed and fed to a student network. The student network is\ntrained to output representation resembling that of the teacher. At the same\ntime, the teacher network is updated as moving average of student's weights\nover training steps. In order to prevent representation collapse, we apply an\nin-utterance contrastive loss as pre-training objective and impose position\nrandomization on the input to the teacher. SPIRAL achieves competitive or\nbetter results compared to state-of-the-art speech pre-training method wav2vec\n2.0, with significant reduction of training cost (80% for Base model, 65% for\nLarge model). Furthermore, we address the problem of noise-robustness that is\ncritical to real-world speech applications. We propose multi-condition\npre-training by perturbing the student's input with various types of additive\nnoise. We demonstrate that multi-condition pre-trained SPIRAL models are more\nrobust to noisy speech (9.0% - 13.3% relative word error rate reduction on real\nnoisy test data), compared to applying multi-condition training solely in the\nfine-tuning stage. The code will be released after publication.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Huang_W/0/1/0/all/0/1\">Wenyong Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhenhe Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yeung_Y/0/1/0/all/0/1\">Yu Ting Yeung</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explanatory Learning: Beyond Empiricism in Neural Networks. (arXiv:2201.10222v1 [cs.LG])","link":"http://arxiv.org/abs/2201.10222","description":"<p>We introduce Explanatory Learning (EL), a framework to let machines use\nexisting knowledge buried in symbolic sequences -- e.g. explanations written in\nhieroglyphic -- by autonomously learning to interpret them. In EL, the burden\nof interpreting symbols is not left to humans or rigid human-coded compilers,\nas done in Program Synthesis. Rather, EL calls for a learned interpreter, built\nupon a limited collection of symbolic sequences paired with observations of\nseveral phenomena. This interpreter can be used to make predictions on a novel\nphenomenon given its explanation, and even to find that explanation using only\na handful of observations, like human scientists do. We formulate the EL\nproblem as a simple binary classification task, so that common end-to-end\napproaches aligned with the dominant empiricist view of machine learning could,\nin principle, solve it. To these models, we oppose Critical Rationalist\nNetworks (CRNs), which instead embrace a rationalist view on the acquisition of\nknowledge. CRNs express several desired properties by construction, they are\ntruly explainable, can adjust their processing at test-time for harder\ninferences, and can offer strong confidence guarantees on their predictions. As\na final contribution, we introduce Odeen, a basic EL environment that simulates\na small flatland-style universe full of phenomena to explain. Using Odeen as a\ntestbed, we show how CRNs outperform empiricist end-to-end approaches of\nsimilar size and architecture (Transformers) in discovering explanations for\nnovel phenomena.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Norelli_A/0/1/0/all/0/1\">Antonio Norelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mariani_G/0/1/0/all/0/1\">Giorgio Mariani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moschella_L/0/1/0/all/0/1\">Luca Moschella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santilli_A/0/1/0/all/0/1\">Andrea Santilli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parascandolo_G/0/1/0/all/0/1\">Giambattista Parascandolo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melzi_S/0/1/0/all/0/1\">Simone Melzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodola_E/0/1/0/all/0/1\">Emanuele Rodol&#xe0;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving the fusion of acoustic and text representations in RNN-T. (arXiv:2201.10240v1 [eess.AS])","link":"http://arxiv.org/abs/2201.10240","description":"<p>The recurrent neural network transducer (RNN-T) has recently become the\nmainstream end-to-end approach for streaming automatic speech recognition\n(ASR). To estimate the output distributions over subword units, RNN-T uses a\nfully connected layer as the joint network to fuse the acoustic representations\nextracted using the acoustic encoder with the text representations obtained\nusing the prediction network based on the previous subword units. In this\npaper, we propose to use gating, bilinear pooling, and a combination of them in\nthe joint network to produce more expressive representations to feed into the\noutput layer. A regularisation method is also proposed to enable better\nacoustic encoder training by reducing the gradients back-propagated into the\nprediction network at the beginning of RNN-T training. Experimental results on\na multilingual ASR setting for voice search over nine languages show that the\njoint use of the proposed methods can result in 4%--5% relative word error rate\nreductions with only a few million extra parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_Z/0/1/0/all/0/1\">Zhiyun Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sainath_T/0/1/0/all/0/1\">Tara N. Sainath</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chang_S/0/1/0/all/0/1\">Shuo-yiin Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do Transformers Encode a Foundational Ontology? Probing Abstract Classes in Natural Language. (arXiv:2201.10262v1 [cs.CL])","link":"http://arxiv.org/abs/2201.10262","description":"<p>With the methodological support of probing (or diagnostic classification),\nrecent studies have demonstrated that Transformers encode syntactic and\nsemantic information to some extent. Following this line of research, this\npaper aims at taking semantic probing to an abstraction extreme with the goal\nof answering the following research question: can contemporary\nTransformer-based models reflect an underlying Foundational Ontology? To this\nend, we present a systematic Foundational Ontology (FO) probing methodology to\ninvestigate whether Transformers-based models encode abstract semantic\ninformation. Following different pre-training and fine-tuning regimes, we\npresent an extensive evaluation of a diverse set of large-scale language models\nover three distinct and complementary FO tagging experiments. Specifically, we\npresent and discuss the following conclusions: (1) The probing results indicate\nthat Transformer-based models incidentally encode information related to\nFoundational Ontologies during the pre-training pro-cess; (2) Robust FO taggers\n(accuracy of 90 percent)can be efficiently built leveraging on this knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jullien_M/0/1/0/all/0/1\">Mael Jullien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valentino_M/0/1/0/all/0/1\">Marco Valentino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freitas_A/0/1/0/all/0/1\">Andre Freitas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-channel Attentive Graph Convolutional Network With Sentiment Fusion For Multimodal Sentiment Analysis. (arXiv:2201.10274v1 [cs.CL])","link":"http://arxiv.org/abs/2201.10274","description":"<p>Nowadays, with the explosive growth of multimodal reviews on social media\nplatforms, multimodal sentiment analysis has recently gained popularity because\nof its high relevance to these social media posts. Although most previous\nstudies design various fusion frameworks for learning an interactive\nrepresentation of multiple modalities, they fail to incorporate sentimental\nknowledge into inter-modality learning. This paper proposes a Multi-channel\nAttentive Graph Convolutional Network (MAGCN), consisting of two main\ncomponents: cross-modality interactive learning and sentimental feature fusion.\nFor cross-modality interactive learning, we exploit the self-attention\nmechanism combined with densely connected graph convolutional networks to learn\ninter-modality dynamics. For sentimental feature fusion, we utilize multi-head\nself-attention to merge sentimental knowledge into inter-modality feature\nrepresentations. Extensive experiments are conducted on three widely-used\ndatasets. The experimental results demonstrate that the proposed model achieves\ncompetitive performance on accuracy and F1 scores compared to several\nstate-of-the-art approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_L/0/1/0/all/0/1\">Luwei Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xingjiao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Liang He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-Shot Long-Form Voice Cloning with Dynamic Convolution Attention. (arXiv:2201.10375v1 [eess.AS])","link":"http://arxiv.org/abs/2201.10375","description":"<p>With recent advancements in voice cloning, the performance of speech\nsynthesis for a target speaker has been rendered similar to the human level.\nHowever, autoregressive voice cloning systems still suffer from text alignment\nfailures, resulting in an inability to synthesize long sentences. In this work,\nwe propose a variant of attention-based text-to-speech system that can\nreproduce a target voice from a few seconds of reference speech and generalize\nto very long utterances as well. The proposed system is based on three\nindependently trained components: a speaker encoder, synthesizer and universal\nvocoder. Generalization to long utterances is realized using an energy-based\nattention mechanism known as Dynamic Convolution Attention, in combination with\na set of modifications proposed for the synthesizer based on Tacotron 2.\nMoreover, effective zero-shot speaker adaptation is achieved by conditioning\nboth the synthesizer and vocoder on a speaker encoder that has been pretrained\non a large corpus of diverse data. We compare several implementations of voice\ncloning systems in terms of speech naturalness, speaker similarity, alignment\nconsistency and ability to synthesize long utterances, and conclude that the\nproposed model can produce intelligible synthetic speech for extremely long\nutterances, while preserving a high extent of naturalness and similarity for\nshort texts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Gorodetskii_A/0/1/0/all/0/1\">Artem Gorodetskii</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ozhiganov_I/0/1/0/all/0/1\">Ivan Ozhiganov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modeling Multi-level Context for Informational Bias Detection by Contrastive Learning and Sentential Graph Network. (arXiv:2201.10376v1 [cs.CL])","link":"http://arxiv.org/abs/2201.10376","description":"<p>Informational bias is widely present in news articles. It refers to providing\none-sided, selective or suggestive information of specific aspects of certain\nentity to guide a specific interpretation, thereby biasing the reader's\nopinion. Sentence-level informational bias detection is a very challenging task\nin a way that such bias can only be revealed together with the context,\nexamples include collecting information from various sources or analyzing the\nentire article in combination with the background. In this paper, we integrate\nthree levels of context to detect the sentence-level informational bias in\nEnglish news articles: adjacent sentences, whole article, and articles from\nother news outlets describing the same event. Our model, MultiCTX (Multi-level\nConTeXt), uses contrastive learning and sentence graphs together with Graph\nAttention Network (GAT) to encode these three degrees of context at different\nstages by tactically composing contrastive triplets and constructing sentence\ngraphs within events. Our experiments proved that contrastive learning together\nwith sentence graphs effectively incorporates context in varying degrees and\nsignificantly outperforms the current SOTA model sentence-wise in informational\nbias detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Shijia Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1\">Kenny Q. Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Generation for Broad-Coverage, Explainable Cognitive Systems. (arXiv:2201.10422v1 [cs.CL])","link":"http://arxiv.org/abs/2201.10422","description":"<p>This paper describes recent progress on natural language generation (NLG) for\nlanguage-endowed intelligent agents (LEIAs) developed within the OntoAgent\ncognitive architecture. The approach draws heavily from past work on natural\nlanguage understanding in this paradigm: it uses the same knowledge bases,\ntheory of computational linguistics, agent architecture, and methodology of\ndeveloping broad-coverage capabilities over time while still supporting\nnear-term applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+McShane_M/0/1/0/all/0/1\">Marjorie McShane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leon_I/0/1/0/all/0/1\">Ivan Leon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Quantitative and Qualitative Analysis of Schizophrenia Language. (arXiv:2201.10430v1 [cs.CL])","link":"http://arxiv.org/abs/2201.10430","description":"<p>Schizophrenia is one of the most disabling mental health conditions to live\nwith. Approximately one percent of the population has schizophrenia which makes\nit fairly common, and it affects many people and their families. Patients with\nschizophrenia suffer different symptoms: formal thought disorder (FTD),\ndelusions, and emotional flatness. In this paper, we quantitatively and\nqualitatively analyze the language of patients with schizophrenia measuring\nvarious linguistic features in two modalities: speech and written text. We\nexamine the following features: coherence and cohesion of thoughts, emotions,\nspecificity, level of committed belief (LCB), and personality traits. Our\nresults show that patients with schizophrenia score high in fear and\nneuroticism compared to healthy controls. In addition, they are more committed\nto their beliefs, and their writing lacks details. They score lower in most of\nthe linguistic features of cohesion with significant p-values.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alqahtani_A/0/1/0/all/0/1\">Amal Alqahtani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kay_E/0/1/0/all/0/1\">Efsun Sarioglu Kay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamidian_S/0/1/0/all/0/1\">Sardar Hamidian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Compton_M/0/1/0/all/0/1\">Michael Compton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diab_M/0/1/0/all/0/1\">Mona Diab</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distantly supervised end-to-end medical entity extraction from electronic health records with human-level quality. (arXiv:2201.10463v1 [cs.CL])","link":"http://arxiv.org/abs/2201.10463","description":"<p>Medical entity extraction (EE) is a standard procedure used as a first stage\nin medical texts processing. Usually Medical EE is a two-step process: named\nentity recognition (NER) and named entity normalization (NEN). We propose a\nnovel method of doing medical EE from electronic health records (EHR) as a\nsingle-step multi-label classification task by fine-tuning a transformer model\npretrained on a large EHR dataset. Our model is trained end-to-end in an\ndistantly supervised manner using targets automatically extracted from medical\nknowledge base. We show that our model learns to generalize for entities that\nare present frequently enough, achieving human-level classification quality for\nmost frequent entities. Our work demonstrates that medical entity extraction\ncan be done end-to-end without human supervision and with human quality given\nthe availability of a large enough amount of unlabeled EHR and a medical\nknowledge base.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nesterov_A/0/1/0/all/0/1\">Alexander Nesterov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Umerenkov_D/0/1/0/all/0/1\">Dmitry Umerenkov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Whose Language Counts as High Quality? Measuring Language Ideologies in Text Data Selection. (arXiv:2201.10474v1 [cs.CL])","link":"http://arxiv.org/abs/2201.10474","description":"<p>Language models increasingly rely on massive web dumps for diverse text data.\nHowever, these sources are rife with undesirable content. As such, resources\nlike Wikipedia, books, and newswire often serve as anchors for automatically\nselecting web text most suitable for language modeling, a process typically\nreferred to as quality filtering. Using a new dataset of U.S. high school\nnewspaper articles -- written by students from across the country -- we\ninvestigate whose language is preferred by the quality filter used for GPT-3.\nWe find that newspapers from larger schools, located in wealthier, educated,\nand urban ZIP codes are more likely to be classified as high quality. We then\ndemonstrate that the filter's measurement of quality is unaligned with other\nsensible metrics, such as factuality or literary acclaim. We argue that\nprivileging any corpus as high quality entails a language ideology, and more\ncare is needed to construct training corpora for language models, with better\ntransparency and justification for the inclusion or exclusion of various texts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gururangan_S/0/1/0/all/0/1\">Suchin Gururangan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Card_D/0/1/0/all/0/1\">Dallas Card</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drier_S/0/1/0/all/0/1\">Sarah K. Drier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gade_E/0/1/0/all/0/1\">Emily K. Gade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Leroy Z. Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zeyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Suicidal Ideation Detection on Social Media: A Review of Machine Learning Methods. (arXiv:2201.10515v1 [cs.CL])","link":"http://arxiv.org/abs/2201.10515","description":"<p>Social media platforms have transformed traditional communication methods by\nallowing users worldwide to communicate instantly, openly, and frequently.\nPeople use social media to express their opinion and share their personal\nstories and struggles. Negative feelings that express hardship, thoughts of\ndeath, and self-harm are widespread in social media, especially among young\ngenerations. Therefore, using social media to detect and identify suicidal\nideation will help provide proper intervention that will eventually dissuade\nothers from self-harming and committing suicide and prevent the spread of\nsuicidal ideations on social media. Many studies have been carried out to\nidentify suicidal ideation and behaviors in social media. This paper presents a\ncomprehensive summary of current research efforts to detect suicidal ideation\nusing machine learning algorithms on social media. This review 24 studies\ninvestigating the feasibility of social media usage for suicidal ideation\ndetection is intended to facilitate further research in the field and will be a\nbeneficial resource for researchers engaged in suicidal text classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abdulsalam_A/0/1/0/all/0/1\">Asma Abdulsalam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alhothali_A/0/1/0/all/0/1\">Areej Alhothali</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Finding Influential Instances for Distantly Supervised Relation Extraction. (arXiv:2009.09841v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2009.09841","description":"<p>Distant supervision (DS) is a strong way to expand the datasets for enhancing\nrelation extraction (RE) models but often suffers from high label noise.\nCurrent works based on attention, reinforcement learning, or GAN are black-box\nmodels so they neither provide meaningful interpretation of sample selection in\nDS nor stability on different domains. On the contrary, this work proposes a\nnovel model-agnostic instance sampling method for DS by influence function\n(IF), namely REIF. Our method identifies favorable/unfavorable instances in the\nbag based on IF, then does dynamic instance sampling. We design a fast\ninfluence sampling algorithm that reduces the computational complexity from\n$\\mathcal{O}(mn)$ to $\\mathcal{O}(1)$, with analyzing its robustness on the\nselected sampling function. Experiments show that by simply sampling the\nfavorable instances during training, REIF is able to win over a series of\nbaselines that have complicated architectures. We also demonstrate that REIF\ncan support interpretable instance selection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zifeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_R/0/1/0/all/0/1\">Rui Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shao-Lun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yefeng Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discovering and Interpreting Biased Concepts in Online Communities. (arXiv:2010.14448v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.14448","description":"<p>Language carries implicit human biases, functioning both as a reflection and\na perpetuation of stereotypes that people carry with them. Recently, ML-based\nNLP methods such as word embeddings have been shown to learn such language\nbiases with striking accuracy. This capability of word embeddings has been\nsuccessfully exploited as a tool to quantify and study human biases. However,\nprevious studies only consider a predefined set of biased concepts to attest\n(e.g., whether gender is more or less associated with particular jobs), or just\ndiscover biased words without helping to understand their meaning at the\nconceptual level. As such, these approaches can be either unable to find biased\nconcepts that have not been defined in advance, or the biases they find are\ndifficult to interpret and study. This could make existing approaches\nunsuitable to discover and interpret biases in online communities, as such\ncommunities may carry different biases than those in mainstream culture. This\npaper improves upon, extends, and evaluates our previous data-driven method to\nautomatically discover and help interpret biased concepts encoded in word\nembeddings. We apply this approach to study the biased concepts present in the\nlanguage used in online communities and experimentally show the validity and\nstability of our method\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ferrer_Aran_X/0/1/0/all/0/1\">Xavier Ferrer-Aran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nuenen_T/0/1/0/all/0/1\">Tom van Nuenen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Criado_N/0/1/0/all/0/1\">Natalia Criado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Such_J/0/1/0/all/0/1\">Jose M. Such</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Embarrassingly Simple Model for Dialogue Relation Extraction. (arXiv:2012.13873v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2012.13873","description":"<p>Dialogue relation extraction (RE) is to predict the relation type of two\nentities mentioned in a dialogue. In this paper, we propose a simple yet\neffective model named SimpleRE for the RE task. SimpleRE captures the\ninterrelations among multiple relations in a dialogue through a novel input\nformat named BERT Relation Token Sequence (BRS). In BRS, multiple [CLS] tokens\nare used to capture possible relations between different pairs of entities\nmentioned in the dialogue. A Relation Refinement Gate (RRG) is then designed to\nextract relation-specific semantic representation in an adaptive manner.\nExperiments on the DialogRE dataset show that SimpleRE achieves the best\nperformance, with much shorter training time. Further, SimpleRE outperforms all\ndirect baselines on sentence-level RE without using external resources.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xue_F/0/1/0/all/0/1\">Fuzhao Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_A/0/1/0/all/0/1\">Aixin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_J/0/1/0/all/0/1\">Jinjie Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chng_E/0/1/0/all/0/1\">Eng Siong Chng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ruddit: Norms of Offensiveness for English Reddit Comments. (arXiv:2106.05664v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.05664","description":"<p>On social media platforms, hateful and offensive language negatively impact\nthe mental well-being of users and the participation of people from diverse\nbackgrounds. Automatic methods to detect offensive language have largely relied\non datasets with categorical labels. However, comments can vary in their degree\nof offensiveness. We create the first dataset of English language Reddit\ncomments that has fine-grained, real-valued scores between -1 (maximally\nsupportive) and 1 (maximally offensive). The dataset was annotated using\nBest--Worst Scaling, a form of comparative annotation that has been shown to\nalleviate known biases of using rating scales. We show that the method produces\nhighly reliable offensiveness scores. Finally, we evaluate the ability of\nwidely-used neural models to predict offensiveness scores on this new dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hada_R/0/1/0/all/0/1\">Rishav Hada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sudhir_S/0/1/0/all/0/1\">Sohi Sudhir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_P/0/1/0/all/0/1\">Pushkar Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yannakoudakis_H/0/1/0/all/0/1\">Helen Yannakoudakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammad_S/0/1/0/all/0/1\">Saif M. Mohammad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shutova_E/0/1/0/all/0/1\">Ekaterina Shutova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Stable Classifiers by Transferring Unstable Features. (arXiv:2106.07847v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.07847","description":"<p>While unbiased machine learning models are essential for many applications,\nbias is a human-defined concept that can vary across tasks. Given only\ninput-label pairs, algorithms may lack sufficient information to distinguish\nstable (causal) features from unstable (spurious) features. However, related\ntasks often share similar biases -- an observation we may leverage to develop\nstable classifiers in the transfer setting. In this work, we explicitly inform\nthe target classifier about unstable features in the source tasks.\nSpecifically, we derive a representation that encodes the unstable features by\ncontrasting different data environments in the source task. We achieve\nrobustness by clustering data of the target task according to this\nrepresentation and minimizing the worst-case risk across these clusters. We\nevaluate our method on both text and image classifications. Empirical results\ndemonstrate that our algorithm is able to maintain robustness on the target\ntask for both synthetically generated environments and real-world environments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bao_Y/0/1/0/all/0/1\">Yujia Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shiyu Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barzilay_R/0/1/0/all/0/1\">Regina Barzilay</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Post-OCR Document Correction with large Ensembles of Character Sequence-to-Sequence Models. (arXiv:2109.06264v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.06264","description":"<p>In this paper, we propose a novel method based on character\nsequence-to-sequence models to correct documents already processed with Optical\nCharacter Recognition (OCR) systems. The main contribution of this paper is a\nset of strategies to accurately process strings much longer than the ones used\nto train the sequence model while being sample- and resource-efficient,\nsupported by thorough experimentation. The strategy with the best performance\ninvolves splitting the input document in character n-grams and combining their\nindividual corrections into the final output using a voting scheme that is\nequivalent to an ensemble of a large number of sequence models. We further\ninvestigate how to weigh the contributions from each one of the members of this\nensemble. We test our method on nine languages of the ICDAR 2019 competition on\npost-OCR text correction and achieve a new state-of-the-art performance in five\nof them. Our code for post-OCR correction is shared at\nhttps://github.com/jarobyte91/post_ocr_correction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ramirez_Orta_J/0/1/0/all/0/1\">Juan Ramirez-Orta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xamena_E/0/1/0/all/0/1\">Eduardo Xamena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maguitman_A/0/1/0/all/0/1\">Ana Maguitman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milios_E/0/1/0/all/0/1\">Evangelos Milios</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soto_A/0/1/0/all/0/1\">Axel J. Soto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Primer: Searching for Efficient Transformers for Language Modeling. (arXiv:2109.08668v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2109.08668","description":"<p>Large Transformer models have been central to recent advances in natural\nlanguage processing. The training and inference costs of these models, however,\nhave grown rapidly and become prohibitively expensive. Here we aim to reduce\nthe costs of Transformers by searching for a more efficient variant. Compared\nto previous approaches, our search is performed at a lower level, over the\nprimitives that define a Transformer TensorFlow program. We identify an\narchitecture, named Primer, that has a smaller training cost than the original\nTransformer and other variants for auto-regressive language modeling. Primer's\nimprovements can be mostly attributed to two simple modifications: squaring\nReLU activations and adding a depthwise convolution layer after each Q, K, and\nV projection in self-attention.\n</p>\n<p>Experiments show Primer's gains over Transformer increase as compute scale\ngrows and follow a power law with respect to quality at optimal model sizes. We\nalso verify empirically that Primer can be dropped into different codebases to\nsignificantly speed up training without additional tuning. For example, at a\n500M parameter size, Primer improves the original T5 architecture on C4\nauto-regressive language modeling, reducing the training cost by 4X.\nFurthermore, the reduced training cost means Primer needs much less compute to\nreach a target one-shot performance. For instance, in a 1.9B parameter\nconfiguration similar to GPT-3 XL, Primer uses 1/3 of the training compute to\nachieve the same one-shot performance as Transformer. We open source our models\nand several comparisons in T5 to help with reproducibility.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+So_D/0/1/0/all/0/1\">David R. So</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manke_W/0/1/0/all/0/1\">Wojciech Ma&#x144;ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hanxiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Z/0/1/0/all/0/1\">Zihang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shazeer_N/0/1/0/all/0/1\">Noam Shazeer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1\">Quoc V. Le</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TopiOCQA: Open-domain Conversational Question Answering with Topic Switching. (arXiv:2110.00768v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.00768","description":"<p>In a conversational question answering scenario, a questioner seeks to\nextract information about a topic through a series of interdependent questions\nand answers. As the conversation progresses, they may switch to related topics,\na phenomenon commonly observed in information-seeking search sessions. However,\ncurrent datasets for conversational question answering are limiting in two\nways: 1) they do not contain topic switches; and 2) they assume the reference\ntext for the conversation is given, i.e., the setting is not open-domain. We\nintroduce TopiOCQA (pronounced Tapioca), an open-domain conversational dataset\nwith topic switches on Wikipedia. TopiOCQA contains 3,920 conversations with\ninformation-seeking questions and free-form answers. On average, a conversation\nin our dataset spans 13 question-answer turns and involves four topics\n(documents). TopiOCQA poses a challenging test-bed for models, where efficient\nretrieval is required on multiple turns of the same conversation, in\nconjunction with constructing valid responses using conversational history. We\nevaluate several baselines, by combining state-of-the-art document retrieval\nmethods with neural reader models. Our best model achieves F1 of 51.9, falling\nshort of human performance by 18.3 points, indicating the difficulty of our\ndataset. Our dataset and code is available at\nhttps://mcgill-nlp.github.io/topiocqa\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Adlakha_V/0/1/0/all/0/1\">Vaibhav Adlakha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhuliawala_S/0/1/0/all/0/1\">Shehzaad Dhuliawala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suleman_K/0/1/0/all/0/1\">Kaheer Suleman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vries_H/0/1/0/all/0/1\">Harm de Vries</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1\">Siva Reddy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BERT Attends the Conversation: Improving Low-Resource Conversational ASR. (arXiv:2110.02267v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.02267","description":"<p>We propose new, data-efficient training tasks for BERT models that improve\nperformance of automatic speech recognition (ASR) systems on conversational\nspeech. We include past conversational context and fine-tune BERT on transcript\ndisambiguation without external data to rescore ASR candidates. Our results\nshow word error rate recoveries up to 37.2%. We test our methods in\nlow-resource data domains, both in language (Norwegian), tone (spontaneous,\nconversational), and topics (parliament proceedings and customer service phone\ncalls). These techniques are applicable to any ASR system and do not require\nany additional data, provided a pre-trained BERT model. We also show how the\nperformance of our context-augmented rescoring methods strongly depends on the\ndegree of spontaneity and nature of the conversation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ortiz_P/0/1/0/all/0/1\">Pablo Ortiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burud_S/0/1/0/all/0/1\">Simen Burud</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Speech Summarization using Restricted Self-Attention. (arXiv:2110.06263v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.06263","description":"<p>Speech summarization is typically performed by using a cascade of speech\nrecognition and text summarization models. End-to-end modeling of speech\nsummarization models is challenging due to memory and compute constraints\narising from long input audio sequences. Recent work in document summarization\nhas inspired methods to reduce the complexity of self-attentions, which enables\ntransformer models to handle long sequences. In this work, we introduce a\nsingle model optimized end-to-end for speech summarization. We apply the\nrestricted self-attention technique from text-based models to speech models to\naddress the memory and compute constraints. We demonstrate that the proposed\nmodel learns to directly summarize speech for the How-2 corpus of instructional\nvideos. The proposed end-to-end model outperforms the previously proposed\ncascaded model by 3 points absolute on ROUGE. Further, we consider the spoken\nlanguage understanding task of predicting concepts from speech inputs and show\nthat the proposed end-to-end model outperforms the cascade model by 4 points\nabsolute F-1.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_R/0/1/0/all/0/1\">Roshan Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palaskar_S/0/1/0/all/0/1\">Shruti Palaskar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Black_A/0/1/0/all/0/1\">Alan W Black</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metze_F/0/1/0/all/0/1\">Florian Metze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Time Masking for Temporal Language Models. (arXiv:2110.06366v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.06366","description":"<p>Our world is constantly evolving, and so is the content on the web.\nConsequently, our languages, often said to mirror the world, are dynamic in\nnature. However, most current contextual language models are static and cannot\nadapt to changes over time. In this work, we propose a temporal contextual\nlanguage model called TempoBERT, which uses time as an additional context of\ntexts. Our technique is based on modifying texts with temporal information and\nperforming time masking - specific masking for the supplementary time\ninformation. We leverage our approach for the tasks of semantic change\ndetection and sentence time prediction, experimenting on diverse datasets in\nterms of time, size, genre, and language. Our extensive evaluation shows that\nboth tasks benefit from exploiting time masking.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rosin_G/0/1/0/all/0/1\">Guy D. Rosin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guy_I/0/1/0/all/0/1\">Ido Guy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radinsky_K/0/1/0/all/0/1\">Kira Radinsky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SILG: The Multi-environment Symbolic Interactive Language Grounding Benchmark. (arXiv:2110.10661v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.10661","description":"<p>Existing work in language grounding typically study single environments. How\ndo we build unified models that apply across multiple environments? We propose\nthe multi-environment Symbolic Interactive Language Grounding benchmark (SILG),\nwhich unifies a collection of diverse grounded language learning environments\nunder a common interface. SILG consists of grid-world environments that require\ngeneralization to new dynamics, entities, and partially observed worlds (RTFM,\nMessenger, NetHack), as well as symbolic counterparts of visual worlds that\nrequire interpreting rich natural language with respect to complex scenes\n(ALFWorld, Touchdown). Together, these environments provide diverse grounding\nchallenges in richness of observation space, action space, language\nspecification, and plan complexity. In addition, we propose the first shared\nmodel architecture for RL on these environments, and evaluate recent advances\nsuch as egocentric local convolution, recurrent state-tracking, entity-centric\nattention, and pretrained LM using SILG. Our shared architecture achieves\ncomparable performance to environment-specific architectures. Moreover, we find\nthat many recent modelling advances do not result in significant gains on\nenvironments other than the one they were designed for. This highlights the\nneed for a multi-environment benchmark. Finally, the best models significantly\nunderperform humans on SILG, which suggests ample room for future work. We hope\nSILG enables the community to quickly identify new methodologies for language\ngrounding that generalize to a diverse set of environments and their associated\nchallenges.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_V/0/1/0/all/0/1\">Victor Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanjie_A/0/1/0/all/0/1\">Austin W. Hanjie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sida I. Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narasimhan_K/0/1/0/all/0/1\">Karthik Narasimhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CoAuthor: Designing a Human-AI Collaborative Writing Dataset for Exploring Language Model Capabilities. (arXiv:2201.06796v2 [cs.HC] UPDATED)","link":"http://arxiv.org/abs/2201.06796","description":"<p>Large language models (LMs) offer unprecedented language generation\ncapabilities and exciting opportunities for interaction design. However, their\nhighly context-dependent capabilities are difficult to grasp and are often\nsubjectively interpreted. In this paper, we argue that by curating and\nanalyzing large interaction datasets, the HCI community can foster more\nincisive examinations of LMs' generative capabilities. Exemplifying this\napproach, we present CoAuthor, a dataset designed for revealing GPT-3's\ncapabilities in assisting creative and argumentative writing. CoAuthor captures\nrich interactions between 63 writers and four instances of GPT-3 across 1445\nwriting sessions. We demonstrate that CoAuthor can address questions about\nGPT-3's language, ideation, and collaboration capabilities, and reveal its\ncontribution as a writing \"collaborator\" under various definitions of good\ncollaboration. Finally, we discuss how this work may facilitate a more\nprincipled discussion around LMs' promises and pitfalls in relation to\ninteraction design. The dataset and an interface for replaying the writing\nsessions are publicly available at https://coauthor.stanford.edu.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Mina Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Percy Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1\">Qian Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-01-25T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"AutoMC: Automated Model Compression based on Domain Knowledge and Progressive search strategy. (arXiv:2201.09884v1 [cs.LG])","link":"http://arxiv.org/abs/2201.09884","description":"<p>Model compression methods can reduce model complexity on the premise of\nmaintaining acceptable performance, and thus promote the application of deep\nneural networks under resource constrained environments. Despite their great\nsuccess, the selection of suitable compression methods and design of details of\nthe compression scheme are difficult, requiring lots of domain knowledge as\nsupport, which is not friendly to non-expert users. To make more users easily\naccess to the model compression scheme that best meet their needs, in this\npaper, we propose AutoMC, an effective automatic tool for model compression.\nAutoMC builds the domain knowledge on model compression to deeply understand\nthe characteristics and advantages of each compression method under different\nsettings. In addition, it presents a progressive search strategy to efficiently\nexplore pareto optimal compression scheme according to the learned prior\nknowledge combined with the historical evaluation information. Extensive\nexperimental results show that AutoMC can provide satisfying compression\nschemes within short time, demonstrating the effectiveness of AutoMC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chunnan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xiangyu Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Euclidean and Affine Curve Reconstruction. (arXiv:2201.09929v1 [math.DG])","link":"http://arxiv.org/abs/2201.09929","description":"<p>We consider practical aspects of reconstructing planar curves with prescribed\nEuclidean or affine curvatures. These curvatures are invariant under the\nspecial Euclidean group and the equi-affine groups, respectively, and play an\nimportant role in computer vision and shape analysis. We discuss and implement\nalgorithms for such reconstruction, and give estimates on how close\nreconstructed curves are relative to the closeness of their curvatures in\nappropriate metrics. Several illustrative examples are provided.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/math/1/au:+Agudelo_J/0/1/0/all/0/1\">Jose Agudelo</a>, <a href=\"http://arxiv.org/find/math/1/au:+Dippold_B/0/1/0/all/0/1\">Brooke Dippold</a>, <a href=\"http://arxiv.org/find/math/1/au:+Klein_I/0/1/0/all/0/1\">Ian Klein</a>, <a href=\"http://arxiv.org/find/math/1/au:+Kokot_A/0/1/0/all/0/1\">Alex Kokot</a>, <a href=\"http://arxiv.org/find/math/1/au:+Geiger_E/0/1/0/all/0/1\">Eric Geiger</a>, <a href=\"http://arxiv.org/find/math/1/au:+Kogan_I/0/1/0/all/0/1\">Irina Kogan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do Smart Glasses Dream of Sentimental Visions? Deep Emotionship Analysis for Eyewear Devices. (arXiv:2201.09933v1 [cs.CV])","link":"http://arxiv.org/abs/2201.09933","description":"<p>Emotion recognition in smart eyewear devices is highly valuable but\nchallenging. One key limitation of previous works is that the\nexpression-related information like facial or eye images is considered as the\nonly emotional evidence. However, emotional status is not isolated; it is\ntightly associated with people's visual perceptions, especially those\nsentimental ones. However, little work has examined such associations to better\nillustrate the cause of different emotions. In this paper, we study the\nemotionship analysis problem in eyewear systems, an ambitious task that\nrequires not only classifying the user's emotions but also semantically\nunderstanding the potential cause of such emotions. To this end, we devise\nEMOShip, a deep-learning-based eyewear system that can automatically detect the\nwearer's emotional status and simultaneously analyze its associations with\nsemantic-level visual perceptions. Experimental studies with 20 participants\ndemonstrate that, thanks to the emotionship awareness, EMOShip not only\nachieves superior emotion recognition accuracy over existing methods (80.2% vs.\n69.4%), but also provides a valuable understanding of the cause of emotions.\nPilot studies with 20 participants further motivate the potential use of\nEMOShip to empower emotion-aware applications, such as emotionship\nself-reflection and emotionship life-logging.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yingying Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yuhu Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yutian Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yujiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_M/0/1/0/all/0/1\">Mingzhi Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_Q/0/1/0/all/0/1\">Qin Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dick_R/0/1/0/all/0/1\">Robert P. Dick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1\">Tun Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_N/0/1/0/all/0/1\">Ning Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_L/0/1/0/all/0/1\">Li Shang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What is the cost of adding a constraint in linear least squares?. (arXiv:2201.09935v1 [cs.CV])","link":"http://arxiv.org/abs/2201.09935","description":"<p>Although the theory of constrained least squares (CLS) estimation is well\nknown, it is usually applied with the view that the constraints to be imposed\nare unavoidable. However, there are cases in which constraints are optional.\nFor example, in camera color calibration, one of several possible color\nprocessing systems is obtained if a constraint on the row sums of a desired\ncolor correction matrix is imposed; in this example, it is not clear a priori\nwhether imposing the constraint leads to better system performance. In this\npaper, we derive an exact expression connecting the constraint to the increase\nin fitting error obtained from imposing it. As another contribution, we show\nhow to determine projection matrices that separate the measured data into two\ncomponents: the first component drives up the fitting error due to imposing a\nconstraint, and the second component is unaffected by the constraint. We\ndemonstrate the use of these results in the color calibration problem.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kakarala_R/0/1/0/all/0/1\">Ramakrishna Kakarala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jun Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Deep Learning Approach for the Detection of COVID-19 from Chest X-Ray Images using Convolutional Neural Networks. (arXiv:2201.09952v1 [eess.IV])","link":"http://arxiv.org/abs/2201.09952","description":"<p>The COVID-19 (coronavirus) is an ongoing pandemic caused by severe acute\nrespiratory syndrome coronavirus 2 (SARS-CoV-2). The virus was first identified\nin mid-December 2019 in the Hubei province of Wuhan, China and by now has\nspread throughout the planet with more than 75.5 million confirmed cases and\nmore than 1.67 million deaths. With limited number of COVID-19 test kits\navailable in medical facilities, it is important to develop and implement an\nautomatic detection system as an alternative diagnosis option for COVID-19\ndetection that can used on a commercial scale. Chest X-ray is the first imaging\ntechnique that plays an important role in the diagnosis of COVID-19 disease.\nComputer vision and deep learning techniques can help in determining COVID-19\nvirus with Chest X-ray Images. Due to the high availability of large-scale\nannotated image datasets, great success has been achieved using convolutional\nneural network for image analysis and classification. In this research, we have\nproposed a deep convolutional neural network trained on five open access\ndatasets with binary output: Normal and Covid. The performance of the model is\ncompared with four pre-trained convolutional neural network-based models\n(COVID-Net, ResNet18, ResNet and MobileNet-V2) and it has been seen that the\nproposed model provides better accuracy on the validation set as compared to\nthe other four pre-trained models. This research work provides promising\nresults which can be further improvise and implement on a commercial scale.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Saxena_A/0/1/0/all/0/1\">Aditya Saxena</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Singh_S/0/1/0/all/0/1\">Shamsheer Pal Singh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attacks and Defenses for Free-Riders in Multi-Discriminator GAN. (arXiv:2201.09967v1 [cs.CV])","link":"http://arxiv.org/abs/2201.09967","description":"<p>Generative Adversarial Networks (GANs) are increasingly adopted by the\nindustry to synthesize realistic images. Due to data not being centrally\navailable, Multi-Discriminator (MD)-GANs training framework employs multiple\ndiscriminators that have direct access to the real data. Distributedly training\na joint GAN model entails the risk of free-riders, i.e., participants that aim\nto benefit from the common model while only pretending to participate in the\ntraining process. In this paper, we conduct the first characterization study of\nthe impact of free-riders on MD-GAN. Based on two production prototypes of\nMD-GAN, we find that free-riders drastically reduce the ability of MD-GANs to\nproduce images that are indistinguishable from real data, i.e., they increase\nthe FID score -- the standard measure to assess the quality of generated\nimages. To mitigate the model degradation, we propose a defense strategy\nagainst free-riders in MD-GAN, termed DFG. DFG distinguishes free-riders and\nbenign participants through periodic probing and clustering of discriminators'\nresponses based on a reference response of free-riders, which then allows the\ngenerator to exclude the detected free-riders from the training. Furthermore,\nwe extend our defense, termed DFG+, to enable discriminators to filter out\nfree-riders at the variant of MD-GAN that allows peer exchanges of\ndiscriminators networks. Extensive evaluation on various scenarios of\nfree-riders, MD-GAN architecture, and three datasets show that our defenses\neffectively detect free-riders. With 1 to 5 free-riders, DFG and DFG+ averagely\ndecreases FID by 5.22% to 11.53% for CIFAR10 and 5.79% to 13.22% for CIFAR100\nin comparison to an attack without defense. In a shell, the proposed DFG(+) can\neffectively defend against free-riders without affecting benign clients at a\nnegligible computation overhead.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zilong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiyue Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roos_S/0/1/0/all/0/1\">Stefanie Roos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lydia Y. Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ImpliCity: City Modeling from Satellite Images with Deep Implicit Occupancy Fields. (arXiv:2201.09968v1 [cs.CV])","link":"http://arxiv.org/abs/2201.09968","description":"<p>High-resolution optical satellite sensors, in combination with dense stereo\nalgorithms, have made it possible to reconstruct 3D city models from space.\nHowever, the resulting models are, in practice, rather noisy, and they tend to\nmiss small geometric features that are clearly visible in the images. We argue\nthat one reason for the limited DSM quality may be a too early, heuristic\nreduction of the triangulated 3D point cloud to an explicit height field or\nsurface mesh. To make full use of the point cloud and the underlying images, we\nintroduce ImpliCity, a neural representation of the 3D scene as an implicit,\ncontinuous occupancy field, driven by learned embeddings of the point cloud and\na stereo pair of ortho-photos. We show that this representation enables the\nextraction of high-quality DSMs: with image resolution 0.5$\\,$m, ImpliCity\nreaches a median height error of $\\approx\\,$0.7$\\,$m and outperforms competing\nmethods, especially w.r.t. building reconstruction, featuring intricate roof\ndetails, smooth surfaces, and straight, regular outlines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stucker_C/0/1/0/all/0/1\">Corinne Stucker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ke_B/0/1/0/all/0/1\">Bingxin Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_Y/0/1/0/all/0/1\">Yuanwen Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shengyu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Armeni_I/0/1/0/all/0/1\">Iro Armeni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schindler_K/0/1/0/all/0/1\">Konrad Schindler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COVID-19 Detection Using CT Image Based On YOLOv5 Network. (arXiv:2201.09972v1 [eess.IV])","link":"http://arxiv.org/abs/2201.09972","description":"<p>Computer aided diagnosis (CAD) increases diagnosis efficiency, helping\ndoctors providing a quick and confident diagnosis, it has played an important\nrole in the treatment of COVID19. In our task, we solve the problem about\nabnormality detection and classification. The dataset provided by Kaggle\nplatform and we choose YOLOv5 as our model. We introduce some methods on\nobjective detection in the related work section, the objection detection can be\ndivided into two streams: onestage and two stage. The representational model\nare Faster RCNN and YOLO series. Then we describe the YOLOv5 model in the\ndetail. Compared Experiments and results are shown in section IV. We choose\nmean average precision (mAP) as our experiments' metrics, and the higher (mean)\nmAP is, the better result the model will gain. mAP@0.5 of our YOLOv5s is 0.623\nwhich is 0.157 and 0.101 higher than Faster RCNN and EfficientDet respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Qu_R/0/1/0/all/0/1\">Ruyi Qu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yuwei Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Vehicle Trajectory Prediction Based on ResNet and EfficientNet Model. (arXiv:2201.09973v1 [cs.CV])","link":"http://arxiv.org/abs/2201.09973","description":"<p>At present, a major challenge for the application of automatic driving\ntechnology is the accurate prediction of vehicle trajectory. With the vigorous\ndevelopment of computer technology and the emergence of convolution depth\nneural network, the accuracy of prediction results has been improved. But, the\ndepth, width of the network and image resolution are still important reasons\nthat restrict the accuracy of the model and the prediction results. The main\ninnovation of this paper is the combination of RESNET network and efficient net\nnetwork, which not only greatly increases the network depth, but also\ncomprehensively changes the choice of network width and image resolution, so as\nto make the model performance better, but also save computing resources as much\nas possible. The experimental results also show that our proposed model obtains\nthe optimal prediction results. Specifically, the loss value of our method is\nseparately 4 less and 2.1 less than that of resnet and efficientnet method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qu_R/0/1/0/all/0/1\">Ruyi Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shukai Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jiexuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1\">ChenXi Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">ZhiYuan Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Manifold Clustering and Embedding. (arXiv:2201.10000v1 [cs.LG])","link":"http://arxiv.org/abs/2201.10000","description":"<p>Given a union of non-linear manifolds, non-linear subspace clustering or\nmanifold clustering aims to cluster data points based on manifold structures\nand also learn to parameterize each manifold as a linear subspace in a feature\nspace. Deep neural networks have the potential to achieve this goal under\nhighly non-linear settings given their large capacity and flexibility. We argue\nthat achieving manifold clustering with neural networks requires two essential\ningredients: a domain-specific constraint that ensures the identification of\nthe manifolds, and a learning algorithm for embedding each manifold to a linear\nsubspace in the feature space. This work shows that many constraints can be\nimplemented by data augmentation. For subspace feature learning, Maximum Coding\nRate Reduction (MCR$^2$) objective can be used. Putting them together yields\n{\\em Neural Manifold Clustering and Embedding} (NMCE), a novel method for\ngeneral purpose manifold clustering, which significantly outperforms\nautoencoder-based deep subspace clustering. Further, on more challenging\nnatural image datasets, NMCE can also outperform other algorithms specifically\ndesigned for clustering. Qualitatively, we demonstrate that NMCE learns a\nmeaningful and interpretable feature space. As the formulation of NMCE is\nclosely related to several important Self-supervised learning (SSL) methods, we\nbelieve this work can help us build a deeper understanding on SSL\nrepresentation learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zengyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yubei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+LeCun_Y/0/1/0/all/0/1\">Yann LeCun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sommer_F/0/1/0/all/0/1\">Friedrich T. Sommer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Recognition and Digital Documentation of Cultural Heritage Hemispherical Domes using Images. (arXiv:2201.10015v1 [cs.CV])","link":"http://arxiv.org/abs/2201.10015","description":"<p>Advancements in optical metrology has enabled documentation of dense 3D point\nclouds of cultural heritage sites. For large scale and continuous digital\ndocumentation, processing of dense 3D point clouds becomes computationally\ncumbersome, and often requires additional hardware for data management,\nincreasing the time cost, and complexity of projects. To this end, this\nmanuscript presents an original approach to generate fast and reliable semantic\ndigital models of heritage hemispherical domes using only two images. New\nclosed formulations were derived to establish the relationships between spheres\nand their projected ellipses onto images, which fostered the development of a\nnew automatic framework for as-built generation of spheres. The effectiveness\nof the proposed method was evaluated under both laboratory and real-world\ndatasets. The results revealed that the proposed method achieved as-built\nmodeling accuracy of around 6mm, while improving the computation time by a\nfactor of 7, when compared to established point cloud processing methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maalek_R/0/1/0/all/0/1\">Reza Maalek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maalek_S/0/1/0/all/0/1\">Shahrokh Maalek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PONI: Potential Functions for ObjectGoal Navigation with Interaction-free Learning. (arXiv:2201.10029v1 [cs.CV])","link":"http://arxiv.org/abs/2201.10029","description":"<p>State-of-the-art approaches to ObjectGoal navigation rely on reinforcement\nlearning and typically require significant computational resources and time for\nlearning. We propose Potential functions for ObjectGoal Navigation with\nInteraction-free learning (PONI), a modular approach that disentangles the\nskills of `where to look?' for an object and `how to navigate to (x, y)?'. Our\nkey insight is that `where to look?' can be treated purely as a perception\nproblem, and learned without environment interactions. To address this, we\npropose a network that predicts two complementary potential functions\nconditioned on a semantic map and uses them to decide where to look for an\nunseen object. We train the potential function network using supervised\nlearning on a passive dataset of top-down semantic maps, and integrate it into\na modular framework to perform ObjectGoal navigation. Experiments on Gibson and\nMatterport3D demonstrate that our method achieves the state-of-the-art for\nObjectGoal navigation while incurring up to 1,600x less computational cost for\ntraining.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ramakrishnan_S/0/1/0/all/0/1\">Santhosh Kumar Ramakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaplot_D/0/1/0/all/0/1\">Devendra Singh Chaplot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Halah_Z/0/1/0/all/0/1\">Ziad Al-Halah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malik_J/0/1/0/all/0/1\">Jitendra Malik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grauman_K/0/1/0/all/0/1\">Kristen Grauman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised Point Cloud Registration with Deep Versatile Descriptors. (arXiv:2201.10034v1 [cs.CV])","link":"http://arxiv.org/abs/2201.10034","description":"<p>Recent years have witnessed an increasing trend toward solving point cloud\nregistration problems with various deep learning-based algorithms. Compared to\nsupervised/semi-supervised registration methods, unsupervised methods require\nno human annotations. However, unsupervised methods mainly depend on the global\ndescriptors, which ignore the high-level representations of local geometries.\nIn this paper, we propose a self-supervised registration scheme with a novel\nDeep Versatile Descriptors (DVD), jointly considering global representations\nand local representations. The DVD is motivated by a key observation that the\nlocal distinctive geometric structures of the point cloud by two subset points\ncan be employed to enhance the representation ability of the feature extraction\nmodule. Furthermore, we utilize two additional tasks (reconstruction and normal\nestimation) to enhance the transformation awareness of the proposed DVDs.\nLastly, we conduct extensive experiments on synthetic and real-world datasets,\ndemonstrating that our method achieves state-of-the-art performance against\ncompeting methods over a wide range of experimental settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Dongrui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chuanchuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Changqing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_R/0/1/0/all/0/1\">Robert Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_L/0/1/0/all/0/1\">Lei Chu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are Commercial Face Detection Models as Biased as Academic Models?. (arXiv:2201.10047v1 [cs.CV])","link":"http://arxiv.org/abs/2201.10047","description":"<p>As facial recognition systems are deployed more widely, scholars and\nactivists have studied their biases and harms. Audits are commonly used to\naccomplish this and compare the algorithmic facial recognition systems'\nperformance against datasets with various metadata labels about the subjects of\nthe images. Seminal works have found discrepancies in performance by gender\nexpression, age, perceived race, skin type, etc. These studies and audits often\nexamine algorithms which fall into two categories: academic models or\ncommercial models. We present a detailed comparison between academic and\ncommercial face detection systems, specifically examining robustness to noise.\nWe find that state-of-the-art academic face detection models exhibit\ndemographic disparities in their noise robustness, specifically by having\nstatistically significant decreased performance on older individuals and those\nwho present their gender in a masculine manner. When we compare the size of\nthese disparities to that of commercial models, we conclude that commercial\nmodels - in contrast to their relatively larger development budget and\nindustry-level fairness commitments - are always as biased or more biased than\nan academic model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dooley_S/0/1/0/all/0/1\">Samuel Dooley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_G/0/1/0/all/0/1\">George Z. Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1\">Tom Goldstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dickerson_J/0/1/0/all/0/1\">John P. Dickerson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ViT-HGR: Vision Transformer-based Hand Gesture Recognition from High Density Surface EMG Signals. (arXiv:2201.10060v1 [cs.CV])","link":"http://arxiv.org/abs/2201.10060","description":"<p>Recently, there has been a surge of significant interest on application of\nDeep Learning (DL) models to autonomously perform hand gesture recognition\nusing surface Electromyogram (sEMG) signals. DL models are, however, mainly\ndesigned to be applied on sparse sEMG signals. Furthermore, due to their\ncomplex structure, typically, we are faced with memory constraints; require\nlarge training times and a large number of training samples, and; there is the\nneed to resort to data augmentation and/or transfer learning. In this paper,\nfor the first time (to the best of our knowledge), we investigate and design a\nVision Transformer (ViT) based architecture to perform hand gesture recognition\nfrom High Density (HD-sEMG) signals. Intuitively speaking, we capitalize on the\nrecent breakthrough role of the transformer architecture in tackling different\ncomplex problems together with its potential for employing more input\nparallelization via its attention mechanism. The proposed Vision\nTransformer-based Hand Gesture Recognition (ViT-HGR) framework can overcome the\naforementioned training time problems and can accurately classify a large\nnumber of hand gestures from scratch without any need for data augmentation\nand/or transfer learning. The efficiency of the proposed ViT-HGR framework is\nevaluated using a recently-released HD-sEMG dataset consisting of 65 isometric\nhand gestures. Our experiments with 64-sample (31.25 ms) window size yield\naverage test accuracy of 84.62 +/- 3.07%, where only 78, 210 number of\nparameters is utilized. The compact structure of the proposed ViT-based ViT-HGR\nframework (i.e., having significantly reduced number of trainable parameters)\nshows great potentials for its practical application for prosthetic control.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Montazerin_M/0/1/0/all/0/1\">Mansooreh Montazerin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zabihi_S/0/1/0/all/0/1\">Soheil Zabihi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahimian_E/0/1/0/all/0/1\">Elahe Rahimian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammadi_A/0/1/0/all/0/1\">Arash Mohammadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naderkhani_F/0/1/0/all/0/1\">Farnoosh Naderkhani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Splatting-based Synthesis for Video Frame Interpolation. (arXiv:2201.10075v1 [cs.CV])","link":"http://arxiv.org/abs/2201.10075","description":"<p>Frame interpolation is an essential video processing technique that adjusts\nthe temporal resolution of an image sequence. An effective approach to perform\nframe interpolation is based on splatting, also known as forward warping.\nSpecifically, splatting can be used to warp the input images to an arbitrary\ntemporal location based on an optical flow estimate. A synthesis network, also\nsometimes referred to as refinement network, can then be used to generate the\noutput frame from the warped images. In doing so, it is common to not only warp\nthe images but also various feature representations which provide rich\ncontextual cues to the synthesis network. However, while this approach has been\nshown to work well and enables arbitrary-time interpolation due to using\nsplatting, the involved synthesis network is prohibitively slow. In contrast,\nwe propose to solely rely on splatting to synthesize the output without any\nsubsequent refinement. This splatting-based synthesis is much faster than\nsimilar approaches, especially for multi-frame interpolation, while enabling\nnew state-of-the-art results at high resolutions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Niklaus_S/0/1/0/all/0/1\">Simon Niklaus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_P/0/1/0/all/0/1\">Ping Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiawen Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Real-time automatic polyp detection in colonoscopy using feature enhancement module and spatiotemporal similarity correlation unit. (arXiv:2201.10079v1 [cs.CV])","link":"http://arxiv.org/abs/2201.10079","description":"<p>Automatic detection of polyps is challenging because different polyps vary\ngreatly, while the changes between polyps and their analogues are small. The\nstate-of-the-art methods are based on convolutional neural networks (CNNs).\nHowever, they may fail due to lack of training data, resulting in high rates of\nmissed detection and false positives (FPs). In order to solve these problems,\nour method combines the two-dimensional (2-D) CNN-based real-time object\ndetector network with spatiotemporal information. Firstly, we use a 2-D\ndetector network to detect static images and frames, and based on the detector\nnetwork, we propose two feature enhancement modules-the FP Relearning Module\n(FPRM) to make the detector network learning more about the features of FPs for\nhigher precision, and the Image Style Transfer Module (ISTM) to enhance the\nfeatures of polyps for sensitivity improvement. In video detection, we\nintegrate spatiotemporal information, which uses Structural Similarity (SSIM)\nto measure the similarity between video frames. Finally, we propose the\nInter-frame Similarity Correlation Unit (ISCU) to combine the results obtained\nby the detector network and frame similarity to make the final decision. We\nverify our method on both private databases and publicly available databases.\nExperimental results show that these modules and units provide a performance\nimprovement compared with the baseline method. Comparison with the\nstate-of-the-art methods shows that the proposed method outperforms the\nexisting ones which can meet real-time constraints. It's demonstrated that our\nmethod provides a performance improvement in sensitivity, precision and\nspecificity, and has great potential to be applied in clinical colonoscopy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jianwei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1\">Ran Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yizhou Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qingwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_X/0/1/0/all/0/1\">Xianzhang Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Z/0/1/0/all/0/1\">Zhizheng Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_D/0/1/0/all/0/1\">Dahong Qian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting L1 Loss in Super-Resolution: A Probabilistic View and Beyond. (arXiv:2201.10084v1 [cs.CV])","link":"http://arxiv.org/abs/2201.10084","description":"<p>Super-resolution as an ill-posed problem has many high-resolution candidates\nfor a low-resolution input. However, the popular $\\ell_1$ loss used to best fit\nthe given HR image fails to consider this fundamental property of\nnon-uniqueness in image restoration. In this work, we fix the missing piece in\n$\\ell_1$ loss by formulating super-resolution with neural networks as a\nprobabilistic model. It shows that $\\ell_1$ loss is equivalent to a degraded\nlikelihood function that removes the randomness from the learning process. By\nintroducing a data-adaptive random variable, we present a new objective\nfunction that aims at minimizing the expectation of the reconstruction error\nover all plausible solutions. The experimental results show consistent\nimprovements on mainstream architectures, with no extra parameter or computing\ncost at inference time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiangyu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1\">Jian Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Classical Approach to Handcrafted Feature Extraction Techniques for Bangla Handwritten Digit Recognition. (arXiv:2201.10102v1 [cs.CV])","link":"http://arxiv.org/abs/2201.10102","description":"<p>Bangla Handwritten Digit recognition is a significant step forward in the\ndevelopment of Bangla OCR. However, intricate shape, structural likeness and\ndistinctive composition style of Bangla digits makes it relatively challenging\nto distinguish. Thus, in this paper, we benchmarked four rigorous classifiers\nto recognize Bangla Handwritten Digit: K-Nearest Neighbor (KNN), Support Vector\nMachine (SVM), Random Forest (RF), and Gradient-Boosted Decision Trees (GBDT)\nbased on three handcrafted feature extraction techniques: Histogram of Oriented\nGradients (HOG), Local Binary Pattern (LBP), and Gabor filter on four publicly\navailable Bangla handwriting digits datasets: NumtaDB, CMARTdb, Ekush and BDRW.\nHere, handcrafted feature extraction methods are used to extract features from\nthe dataset image, which are then utilized to train machine learning\nclassifiers to identify Bangla handwritten digits. We further fine-tuned the\nhyperparameters of the classification algorithms in order to acquire the finest\nBangla handwritten digits recognition performance from these algorithms, and\namong all the models we employed, the HOG features combined with SVM model\n(HOG+SVM) attained the best performance metrics across all datasets. The\nrecognition accuracy of the HOG+SVM method on the NumtaDB, CMARTdb, Ekush and\nBDRW datasets reached 93.32%, 98.08%, 95.68% and 89.68%, respectively as well\nas we compared the model performance with recent state-of-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wahid_M/0/1/0/all/0/1\">Md. Ferdous Wahid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahriar_M/0/1/0/all/0/1\">Md. Fahim Shahriar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sobuj_M/0/1/0/all/0/1\">Md. Shohanur Islam Sobuj</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ARPD: Anchor-free Rotation-aware People Detection using Topview Fisheye Camera. (arXiv:2201.10107v1 [cs.CV])","link":"http://arxiv.org/abs/2201.10107","description":"<p>People detection in top-view, fish-eye images is challenging as people in\nfish-eye images often appear in arbitrary directions and are distorted\ndifferently. Due to this unique radial geometry, axis-aligned people detectors\noften work poorly on fish-eye frames. Recent works account for this variability\nby modifying existing anchor-based detectors or relying on complex\npre/post-processing. Anchor-based methods spread a set of pre-defined bounding\nboxes on the input image, most of which are invalid. In addition to being\ninefficient, this approach could lead to a significant imbalance between the\npositive and negative anchor boxes. In this work, we propose ARPD, a\nsingle-stage anchor-free fully convolutional network to detect arbitrarily\nrotated people in fish-eye images. Our network uses keypoint estimation to find\nthe center point of each object and regress the object's other properties\ndirectly. To capture the various orientation of people in fish-eye cameras, in\naddition to the center and size, ARPD also predicts the angle of each bounding\nbox. We also propose a periodic loss function that accounts for angle\nperiodicity and relieves the difficulty of learning small-angle oscillations.\nExperimental results show that our method competes favorably with\nstate-of-the-art algorithms while running significantly faster.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Minh_Q/0/1/0/all/0/1\">Quan Nguyen Minh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Van_B/0/1/0/all/0/1\">Bang Le Van</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1\">Can Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_A/0/1/0/all/0/1\">Anh Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_V/0/1/0/all/0/1\">Viet Dung Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Hybrid Quantum-Classical Algorithm for Robust Fitting. (arXiv:2201.10110v1 [cs.CV])","link":"http://arxiv.org/abs/2201.10110","description":"<p>Fitting geometric models onto outlier contaminated data is provably\nintractable. Many computer vision systems rely on random sampling heuristics to\nsolve robust fitting, which do not provide optimality guarantees and error\nbounds. It is therefore critical to develop novel approaches that can bridge\nthe gap between exact solutions that are costly, and fast heuristics that offer\nno quality assurances. In this paper, we propose a hybrid quantum-classical\nalgorithm for robust fitting. Our core contribution is a novel robust fitting\nformulation that solves a sequence of integer programs and terminates with a\nglobal solution or an error bound. The combinatorial subproblems are amenable\nto a quantum annealer, which helps to tighten the bound efficiently. While our\nusage of quantum computing does not surmount the fundamental intractability of\nrobust fitting, by providing error bounds our algorithm is a practical\nimprovement over randomised heuristics. Moreover, our work represents a\nconcrete application of quantum computing in computer vision. We present\nresults obtained using an actual quantum computer (D-Wave Advantage) and via\nsimulation. Source code: https://github.com/dadung/HQC-robust-fitting\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Doan_A/0/1/0/all/0/1\">Anh-Dzung Doan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sasdelli_M/0/1/0/all/0/1\">Michele Sasdelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chin_T/0/1/0/all/0/1\">Tat-Jun Chin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suter_D/0/1/0/all/0/1\">David Suter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SURDS: Self-Supervised Attention-guided Reconstruction and Dual Triplet Loss for Writer Independent Offline Signature Verification. (arXiv:2201.10138v1 [cs.CV])","link":"http://arxiv.org/abs/2201.10138","description":"<p>Offline Signature Verification (OSV) is a fundamental biometric task across\nvarious forensic, commercial and legal applications. The underlying task at\nhand is to carefully model fine-grained features of the signatures to\ndistinguish between genuine and forged ones, which differ only in minute\ndeformities. This makes OSV more challenging compared to other verification\nproblems. In this work, we propose a two-stage deep learning framework that\nleverages self-supervised representation learning as well as metric learning\nfor writer-independent OSV. First, we train an image reconstruction network\nusing an encoder-decoder architecture that is augmented by a 2D spatial\nattention mechanism using signature image patches. Next, the trained encoder\nbackbone is fine-tuned with a projector head using a supervised metric learning\nframework, whose objective is to optimize a novel dual triplet loss by sampling\nnegative samples from both within the same writer class as well as from other\nwriters. The intuition behind this is to ensure that a signature sample lies\ncloser to its positive counterpart compared to negative samples from both\nintra-writer and cross-writer sets. This results in robust discriminative\nlearning of the embedding space. To the best of our knowledge, this is the\nfirst work of using self-supervised learning frameworks for OSV. The proposed\ntwo-stage framework has been evaluated on two publicly available offline\nsignature datasets and compared with various state-of-the-art methods. It is\nnoted that the proposed method provided promising results outperforming several\nexisting pieces of work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chattopadhyay_S/0/1/0/all/0/1\">Soumitri Chattopadhyay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manna_S/0/1/0/all/0/1\">Siladittya Manna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_S/0/1/0/all/0/1\">Saumik Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pal_U/0/1/0/all/0/1\">Umapada Pal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MSNet: A Deep Multi-scale Submanifold Network for Visual Classification. (arXiv:2201.10145v1 [cs.CV])","link":"http://arxiv.org/abs/2201.10145","description":"<p>The Symmetric Positive Definite (SPD) matrix has received wide attention as a\ntool for visual data representation in computer vision. Although there are many\ndifferent attempts to develop effective deep architectures for data processing\non the Riemannian manifold of SPD matrices, a very few solutions explicitly\nmine the local geometrical information in deep SPD feature representations.\nWhile CNNs have demonstrated the potential of hierarchical local pattern\nextraction even for SPD represented data, we argue that it is of utmost\nimportance to ensure the preservation of local geometric information in the SPD\nnetworks. Accordingly, in this work we propose an SPD network designed with\nthis objective in mind. In particular, we propose an architecture, referred to\nas MSNet, which fuses geometrical multi-scale information. We first analyse the\nconvolution operator commonly used for mapping the local information in\nEuclidean deep networks from the perspective of a higher level of abstraction\nafforded by the Category Theory. Based on this analysis, we postulate a\nsubmanifold selection principle to guide the design of our MSNet. In\nparticular, we use it to design a submanifold fusion block to take advantage of\nthe rich local geometry encoded in the network layers. The experiments\ninvolving multiple visual tasks show that our algorithm outperforms most\nRiemannian SOTA competitors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Ziheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiao-Jun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Tianyang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhiwu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kittler_J/0/1/0/all/0/1\">Josef Kittler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TGFuse: An Infrared and Visible Image Fusion Approach Based on Transformer and Generative Adversarial Network. (arXiv:2201.10147v1 [cs.CV])","link":"http://arxiv.org/abs/2201.10147","description":"<p>The end-to-end image fusion framework has achieved promising performance,\nwith dedicated convolutional networks aggregating the multi-modal local\nappearance. However, long-range dependencies are directly neglected in existing\nCNN fusion approaches, impeding balancing the entire image-level perception for\ncomplex scenario fusion. In this paper, therefore, we propose an infrared and\nvisible image fusion algorithm based on a lightweight transformer module and\nadversarial learning. Inspired by the global interaction power, we use the\ntransformer technique to learn the effective global fusion relations. In\nparticular, shallow features extracted by CNN are interacted in the proposed\ntransformer fusion module to refine the fusion relationship within the spatial\nscope and across channels simultaneously. Besides, adversarial learning is\ndesigned in the training process to improve the output discrimination via\nimposing competitive consistency from the inputs, reflecting the specific\ncharacteristics in infrared and visible images. The experimental performance\ndemonstrates the effectiveness of the proposed modules, with superior\nimprovement against the state-of-the-art, generalising a novel paradigm via\ntransformer and adversarial learning in the fusion task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rao_D/0/1/0/all/0/1\">Dongyu Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiao-Jun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Tianyang Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Image Fusion Method based on Feature Mutual Mapping. (arXiv:2201.10152v1 [cs.CV])","link":"http://arxiv.org/abs/2201.10152","description":"<p>Deep learning-based image fusion approaches have obtained wide attention in\nrecent years, achieving promising performance in terms of visual perception.\nHowever, the fusion module in the current deep learning-based methods suffers\nfrom two limitations, \\textit{i.e.}, manually designed fusion function, and\ninput-independent network learning. In this paper, we propose an unsupervised\nadaptive image fusion method to address the above issues. We propose a feature\nmutual mapping fusion module and dual-branch multi-scale autoencoder. More\nspecifically, we construct a global map to measure the connections of pixels\nbetween the input source images. % The found mapping relationship guides the\nimage fusion. Besides, we design a dual-branch multi-scale network through\nsampling transformation to extract discriminative image features. We further\nenrich feature representations of different scales through feature aggregation\nin the decoding process. Finally, we propose a modified loss function to train\nthe network with efficient convergence property. Through sufficient training on\ninfrared and visible image data sets, our method also shows excellent\ngeneralized performance in multi-focus and medical image fusion. Our method\nachieves superior performance in both visual perception and objective\nevaluation. Experiments prove that the performance of our proposed method on a\nvariety of image fusion tasks surpasses other state-of-the-art methods, proving\nthe effectiveness and versatility of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rao_D/0/1/0/all/0/1\">Dongyu Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiao-Jun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Tianyang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guoyang Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantically Video Coding: Instill Static-Dynamic Clues into Structured Bitstream for AI Tasks. (arXiv:2201.10162v1 [cs.CV])","link":"http://arxiv.org/abs/2201.10162","description":"<p>Traditional media coding schemes typically encode image/video into a\nsemantic-unknown binary stream, which fails to directly support downstream\nintelligent tasks at the bitstream level. Semantically Structured Image Coding\n(SSIC) framework makes the first attempt to enable decoding-free or\npartial-decoding image intelligent task analysis via a Semantically Structured\nBitstream (SSB). However, the SSIC only considers image coding and its\ngenerated SSB only contains the static object information. In this paper, we\nextend the idea of semantically structured coding from video coding perspective\nand propose an advanced Semantically Structured Video Coding (SSVC) framework\nto support heterogeneous intelligent applications. Video signals contain more\nrich dynamic motion information and exist more redundancy due to the similarity\nbetween adjacent frames. Thus, we present a reformulation of semantically\nstructured bitstream (SSB) in SSVC which contains both static object\ncharacteristics and dynamic motion clues. Specifically, we introduce optical\nflow to encode continuous motion information and reduce cross-frame redundancy\nvia a predictive coding architecture, then the optical flow and residual\ninformation are reorganized into SSB, which enables the proposed SSVC could\nbetter adaptively support video-based downstream intelligent applications.\nExtensive experiments demonstrate that the proposed SSVC framework could\ndirectly support multiple intelligent tasks just depending on a partially\ndecoded bitstream. This avoids the full bitstream decompression and thus\nsignificantly saves bitrate/bandwidth consumption for intelligent analytics. We\nverify this point on the tasks of image object detection, pose estimation,\nvideo action recognition, video object segmentation, etc.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1\">Xin Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_R/0/1/0/all/0/1\">Ruoyu Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1\">Simeng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_R/0/1/0/all/0/1\">Runsen Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1\">Tianyu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhibo Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dense Pixel-Labeling for Reverse-Transfer and Diagnostic Learning on Lung Ultrasound for COVID-19 and Pneumonia Detection. (arXiv:2201.10166v1 [eess.IV])","link":"http://arxiv.org/abs/2201.10166","description":"<p>We propose using a pre-trained segmentation model to perform diagnostic\nclassification in order to achieve better generalization and interpretability,\nterming the technique reverse-transfer learning. We present an architecture to\nconvert segmentation models to classification models. We compare and contrast\ndense vs sparse segmentation labeling and study its impact on diagnostic\nclassification. We compare the performance of U-Net trained with dense and\nsparse labels to segment A-lines, B-lines, and Pleural lines on a custom\ndataset of lung ultrasound scans from 4 patients. Our experiments show that\ndense labels help reduce false positive detection. We study the classification\ncapability of the dense and sparse trained U-Net and contrast it with a\nnon-pretrained U-Net, to detect and differentiate COVID-19 and Pneumonia on a\nlarge ultrasound dataset of about 40k curvilinear and linear probe images. Our\nsegmentation-based models perform better classification when using pretrained\nsegmentation weights, with the dense-label pretrained U-Net performing the\nbest.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Gare_G/0/1/0/all/0/1\">Gautam Rajendrakumar Gare</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schoenling_A/0/1/0/all/0/1\">Andrew Schoenling</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Philip_V/0/1/0/all/0/1\">Vipin Philip</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tran_H/0/1/0/all/0/1\">Hai V Tran</a>, <a href=\"http://arxiv.org/find/eess/1/au:+deBoisblanc_B/0/1/0/all/0/1\">Bennett P deBoisblanc</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rodriguez_R/0/1/0/all/0/1\">Ricardo Luis Rodriguez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Galeotti_J/0/1/0/all/0/1\">John Michael Galeotti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explore and Match: End-to-End Video Grounding with Transformer. (arXiv:2201.10168v1 [cs.CV])","link":"http://arxiv.org/abs/2201.10168","description":"<p>We present a new paradigm named explore-and-match for video grounding, which\naims to seamlessly unify two streams of video grounding methods: proposal-based\nand proposal-free. To achieve this goal, we formulate video grounding as a set\nprediction problem and design an end-to-end trainable Video Grounding\nTransformer (VidGTR) that can utilize the architectural strengths of rich\ncontextualization and parallel decoding for set prediction. The overall\ntraining is balanced by two key losses that play different roles, namely span\nlocalization loss and set guidance loss. These two losses force each proposal\nto regress the target timespan and identify the target query. Throughout the\ntraining, VidGTR first explores the search space to diversify the initial\nproposals and then matches the proposals to the corresponding targets to fit\nthem in a fine-grained manner. The explore-and-match scheme successfully\ncombines the strengths of two complementary methods, without encoding prior\nknowledge into the pipeline. As a result, VidGTR sets new state-of-the-art\nresults on two video grounding benchmarks with double the inference speed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Woo_S/0/1/0/all/0/1\">Sangmin Woo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jinyoung Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koo_I/0/1/0/all/0/1\">Inyong Koo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sumin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_M/0/1/0/all/0/1\">Minki Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1\">Changick Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RFMask: A Simple Baseline for Human Silhouette Segmentation with Radio Signals. (arXiv:2201.10175v1 [cs.CV])","link":"http://arxiv.org/abs/2201.10175","description":"<p>Human silhouette segmentation, which is originally defined in computer\nvision, has achieved promising results for understanding human activities.\nHowever, the physical limitation makes existing systems based on optical\ncameras suffer from severe performance degradation under low illumination,\nsmoke, and/or opaque obstruction conditions. To overcome such limitations, in\nthis paper, we propose to utilize the radio signals, which can traverse\nobstacles and are unaffected by the lighting conditions to achieve silhouette\nsegmentation. The proposed RFMask framework is composed of three modules. It\nfirst transforms RF signals captured by millimeter wave radar on two planes\ninto spatial domain and suppress interference with the signal processing\nmodule. Then, it locates human reflections on RF frames and extract features\nfrom surrounding signals with human detection module. Finally, the extracted\nfeatures from RF frames are aggregated with an attention based mask generation\nmodule. To verify our proposed framework, we collect a dataset containing\n804,760 radio frames and 402,380 camera frames with human activities under\nvarious scenes. Experimental results show that the proposed framework can\nachieve impressive human silhouette segmentation even under the challenging\nscenarios(such as low light and occlusion scenarios) where traditional\noptical-camera-based methods fail. To the best of our knowledge, this is the\nfirst investigation towards segmenting human silhouette based on millimeter\nwave signals. We hope that our work can serve as a baseline and inspire further\nresearch that perform vision tasks with radio signals. The dataset and codes\nwill be made in public.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1\">Chunyang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1\">Cong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jinbo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yan Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pre-Trained Language Transformers are Universal Image Classifiers. (arXiv:2201.10182v1 [cs.CV])","link":"http://arxiv.org/abs/2201.10182","description":"<p>Facial images disclose many hidden personal traits such as age, gender, race,\nhealth, emotion, and psychology. Understanding these traits will help to\nclassify the people in different attributes. In this paper, we have presented a\nnovel method for classifying images using a pretrained transformer model. We\napply the pretrained transformer for the binary classification of facial images\nin criminal and non-criminal classes. The pretrained transformer of GPT-2 is\ntrained to generate text and then fine-tuned to classify facial images. During\nthe finetuning process with images, most of the layers of GT-2 are frozen\nduring backpropagation and the model is frozen pretrained transformer (FPT).\nThe FPT acts as a universal image classifier, and this paper shows the\napplication of FPT on facial images. We also use our FPT on encrypted images\nfor classification. Our FPT shows high accuracy on both raw facial images and\nencrypted images. We hypothesize the meta-learning capacity FPT gained because\nof its large size and trained on a large size with theory and experiments. The\nGPT-2 trained to generate a single word token at a time, through the\nautoregressive process, forced to heavy-tail distribution. Then the FPT uses\nthe heavy-tail property as its meta-learning capacity for classifying images.\nOur work shows one way to avoid bias during the machine classification of\nimages.The FPT encodes worldly knowledge because of the pretraining of one\ntext, which it uses during the classification. The statistical error of\nclassification is reduced because of the added context gained from the text.Our\npaper shows the ethical dimension of using encrypted data for\nclassification.Criminal images are sensitive to share across the boundary but\nencrypted largely evades ethical concern.FPT showing good classification\naccuracy on encrypted images shows promise for further research on\nprivacy-preserving machine learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Goel_R/0/1/0/all/0/1\">Rahul Goel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sulaiman_M/0/1/0/all/0/1\">Modar Sulaiman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noorbakhsh_K/0/1/0/all/0/1\">Kimia Noorbakhsh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharifi_M/0/1/0/all/0/1\">Mahdi Sharifi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_R/0/1/0/all/0/1\">Rajesh Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jamshidi_P/0/1/0/all/0/1\">Pooyan Jamshidi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_K/0/1/0/all/0/1\">Kallol Roy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Estimating the Direction and Radius of Pipe from GPR Image by Ellipse Inversion Model. (arXiv:2201.10184v1 [cs.CV])","link":"http://arxiv.org/abs/2201.10184","description":"<p>Ground Penetrating Radar (GPR) is widely used as a non-destructive approach\nto estimate buried utilities. When the GPR's detecting direction is\nperpendicular to a pipeline, a hyperbolic characteristic would be formed on the\nGPR B-scan image. However, in real-world applications, the direction of\npipelines on the existing pipeline map could be inaccurate, and it is hard to\nensure the moving direction of GPR to be actually perpendicular to underground\npipelines. In this paper, a novel model is proposed to estimate the direction\nand radius of pipeline and revise the existing pipeline map from GPR B-scan\nimages. The model consists of two parts: GPR B-scan image processing and\nEllipse Iterative Inversion Algorithm (EIIA). Firstly, the GPR B-scan image is\nprocessed with downward-opening point set extracted. The obtained point set is\nthen iteratively inverted to the elliptical cross section of the buried\npipeline, which is caused by the angle between the GPR's detecting direction\nand the pipeline's direction. By minimizing the sum of the algebraic distances\nfrom the extracted point set to the inverted ellipse, the most likely\npipeline's direction and radius are determined. Experiments on real-world\ndatasets are conducted, and the results demonstrate the effectiveness of the\nmethod.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xiren Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qiuju Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1\">Shengfei Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huanhuan Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-Shot Sketch Based Image Retrieval using Graph Transformer. (arXiv:2201.10185v1 [cs.CV])","link":"http://arxiv.org/abs/2201.10185","description":"<p>The performance of a zero-shot sketch-based image retrieval (ZS-SBIR) task is\nprimarily affected by two challenges. The substantial domain gap between image\nand sketch features needs to be bridged, while at the same time the side\ninformation has to be chosen tactfully. Existing literature has shown that\nvarying the semantic side information greatly affects the performance of\nZS-SBIR. To this end, we propose a novel graph transformer based zero-shot\nsketch-based image retrieval (GTZSR) framework for solving ZS-SBIR tasks which\nuses a novel graph transformer to preserve the topology of the classes in the\nsemantic space and propagates the context-graph of the classes within the\nembedding features of the visual space. To bridge the domain gap between the\nvisual features, we propose minimizing the Wasserstein distance between images\nand sketches in a learned domain-shared space. We also propose a novel\ncompatibility loss that further aligns the two visual domains by bridging the\ndomain gap of one class with respect to the domain gap of all other classes in\nthe training set. Experimental results obtained on the extended Sketchy,\nTU-Berlin, and QuickDraw datasets exhibit sharp improvements over the existing\nstate-of-the-art methods in both ZS-SBIR and generalized ZS-SBIR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1\">Sumrit Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhuri_U/0/1/0/all/0/1\">Ushasi Chaudhuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_B/0/1/0/all/0/1\">Biplab Banerjee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Universal Generative Modeling for Calibration-free Parallel Mr Imaging. (arXiv:2201.10210v1 [cs.CV])","link":"http://arxiv.org/abs/2201.10210","description":"<p>The integration of compressed sensing and parallel imaging (CS-PI) provides a\nrobust mechanism for accelerating MRI acquisitions. However, most such\nstrategies require the explicit formation of either coil sensitivity profiles\nor a cross-coil correlation operator, and as a result reconstruction\ncorresponds to solving a challenging bilinear optimization problem. In this\nwork, we present an unsupervised deep learning framework for calibration-free\nparallel MRI, coined universal generative modeling for parallel imaging\n(UGM-PI). More precisely, we make use of the merits of both wavelet transform\nand the adaptive iteration strategy in a unified framework. We train a powerful\nnoise conditional score network by forming wavelet tensor as the network input\nat the training phase. Experimental results on both physical phantom and in\nvivo datasets implied that the proposed method is comparable and even superior\nto state-of-the-art CS-PI reconstruction approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wanqing Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_B/0/1/0/all/0/1\">Bing Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shanshan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Minghui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qiegen Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Feature Diversity Learning with Sample Dropout for Unsupervised Domain Adaptive Person Re-identification. (arXiv:2201.10212v1 [cs.CV])","link":"http://arxiv.org/abs/2201.10212","description":"<p>Clustering-based approach has proved effective in dealing with unsupervised\ndomain adaptive person re-identification (ReID) tasks. However, existing works\nalong this approach still suffer from noisy pseudo labels and the unreliable\ngeneralization ability during the whole training process. To solve these\nproblems, this paper proposes a new approach to learn the feature\nrepresentation with better generalization ability through limiting noisy pseudo\nlabels. At first, we propose a Sample Dropout (SD) method to prevent the\ntraining of the model from falling into the vicious circle caused by samples\nthat are frequently assigned with noisy pseudo labels. In addition, we put\nforward a brand-new method referred as to Feature Diversity Learning (FDL)\nunder the classic mutual-teaching architecture, which can significantly improve\nthe generalization ability of the feature representation on the target domain.\nExperimental results show that our proposed FDL-SD achieves the\nstate-of-the-art performance on multiple benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1\">Chunren Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_D/0/1/0/all/0/1\">Dingyu Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dongyue Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BERTHA: Video Captioning Evaluation Via Transfer-Learned Human Assessment. (arXiv:2201.10243v1 [cs.CV])","link":"http://arxiv.org/abs/2201.10243","description":"<p>Evaluating video captioning systems is a challenging task as there are\nmultiple factors to consider; for instance: the fluency of the caption,\nmultiple actions happening in a single scene, and the human bias of what is\nconsidered important. Most metrics try to measure how similar the system\ngenerated captions are to a single or a set of human-annotated captions. This\npaper presents a new method based on a deep learning model to evaluate these\nsystems. The model is based on BERT, which is a language model that has been\nshown to work well in multiple NLP tasks. The aim is for the model to learn to\nperform an evaluation similar to that of a human. To do so, we use a dataset\nthat contains human evaluations of system generated captions. The dataset\nconsists of the human judgments of the captions produce by the system\nparticipating in various years of the TRECVid video to text task. These\nannotations will be made publicly available. BERTHA obtain favourable results,\noutperforming the commonly used metrics in some setups.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lebron_L/0/1/0/all/0/1\">Luis Lebron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Graham_Y/0/1/0/all/0/1\">Yvette Graham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McGuinness_K/0/1/0/all/0/1\">Kevin McGuinness</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kouramas_K/0/1/0/all/0/1\">Konstantinos Kouramas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OConnor_N/0/1/0/all/0/1\">Noel E. O&#x27;Connor</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DocEnTr: An End-to-End Document Image Enhancement Transformer. (arXiv:2201.10252v1 [cs.CV])","link":"http://arxiv.org/abs/2201.10252","description":"<p>Document images can be affected by many degradation scenarios, which cause\nrecognition and processing difficulties. In this age of digitization, it is\nimportant to denoise them for proper usage. To address this challenge, we\npresent a new encoder-decoder architecture based on vision transformers to\nenhance both machine-printed and handwritten document images, in an end-to-end\nfashion. The encoder operates directly on the pixel patches with their\npositional information without the use of any convolutional layers, while the\ndecoder reconstructs a clean image from the encoded patches. Conducted\nexperiments show a superiority of the proposed model compared to the state-of\nthe-art methods on several DIBCO benchmarks. Code and models will be publicly\navailable at: \\url{https://github.com/dali92002/DocEnTR}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Souibgui_M/0/1/0/all/0/1\">Mohamed Ali Souibgui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biswas_S/0/1/0/all/0/1\">Sanket Biswas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jemni_S/0/1/0/all/0/1\">Sana Khamekhem Jemni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kessentini_Y/0/1/0/all/0/1\">Yousri Kessentini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fornes_A/0/1/0/all/0/1\">Alicia Forn&#xe9;s</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Llados_J/0/1/0/all/0/1\">Josep Llad&#xf3;s</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pal_U/0/1/0/all/0/1\">Umapada Pal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Combining Commonsense Reasoning and Knowledge Acquisition to Guide Deep Learning in Robotics. (arXiv:2201.10266v1 [cs.AI])","link":"http://arxiv.org/abs/2201.10266","description":"<p>Algorithms based on deep network models are being used for many pattern\nrecognition and decision-making tasks in robotics and AI. Training these models\nrequires a large labeled dataset and considerable computational resources,\nwhich are not readily available in many domains. Also, it is difficult to\nexplore the internal representations and reasoning mechanisms of these models.\nAs a step towards addressing the underlying knowledge representation,\nreasoning, and learning challenges, the architecture described in this paper\ndraws inspiration from research in cognitive systems. As a motivating example,\nwe consider an assistive robot trying to reduce clutter in any given scene by\nreasoning about the occlusion of objects and stability of object configurations\nin an image of the scene. In this context, our architecture incrementally\nlearns and revises a grounding of the spatial relations between objects and\nuses this grounding to extract spatial information from input images.\nNon-monotonic logical reasoning with this information and incomplete\ncommonsense domain knowledge is used to make decisions about stability and\nocclusion. For images that cannot be processed by such reasoning, regions\nrelevant to the tasks at hand are automatically identified and used to train\ndeep network models to make the desired decisions. Image regions used to train\nthe deep networks are also used to incrementally acquire previously unknown\nstate constraints that are merged with the existing knowledge for subsequent\nreasoning. Experimental evaluation performed using simulated and real-world\nimages indicates that in comparison with baselines based just on deep networks,\nour architecture improves reliability of decision making and reduces the effort\ninvolved in training data-driven deep network models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sridharan_M/0/1/0/all/0/1\">Mohan Sridharan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mota_T/0/1/0/all/0/1\">Tiago Mota</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Convolutional Xformers for Vision. (arXiv:2201.10271v1 [cs.CV])","link":"http://arxiv.org/abs/2201.10271","description":"<p>Vision transformers (ViTs) have found only limited practical use in\nprocessing images, in spite of their state-of-the-art accuracy on certain\nbenchmarks. The reason for their limited use include their need for larger\ntraining datasets and more computational resources compared to convolutional\nneural networks (CNNs), owing to the quadratic complexity of their\nself-attention mechanism. We propose a linear attention-convolution hybrid\narchitecture -- Convolutional X-formers for Vision (CXV) -- to overcome these\nlimitations. We replace the quadratic attention with linear attention\nmechanisms, such as Performer, Nystr\\\"omformer, and Linear Transformer, to\nreduce its GPU usage. Inductive prior for image data is provided by\nconvolutional sub-layers, thereby eliminating the need for class token and\npositional embeddings used by the ViTs. We also propose a new training method\nwhere we use two different optimizers during different phases of training and\nshow that it improves the top-1 image classification accuracy across different\narchitectures. CXV outperforms other architectures, token mixers (e.g.\nConvMixer, FNet and MLP Mixer), transformer models (e.g. ViT, CCT, CvT and\nhybrid Xformers), and ResNets for image classification in scenarios with\nlimited data and GPU resources (cores, RAM, power).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jeevan_P/0/1/0/all/0/1\">Pranav Jeevan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+sethi_A/0/1/0/all/0/1\">Amit sethi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"City3D: Large-scale Urban Reconstruction from Airborne Point Clouds. (arXiv:2201.10276v1 [cs.CV])","link":"http://arxiv.org/abs/2201.10276","description":"<p>We present a fully automatic approach for reconstructing compact 3D building\nmodels from large-scale airborne point clouds. A major challenge of urban\nreconstruction from airborne point clouds lies in that the vertical walls are\ntypically missing. Based on the observation that urban buildings typically\nconsist of planar roofs connected with vertical walls to the ground, we propose\nan approach to infer the vertical walls directly from the data. With the planar\nsegments of both roofs and walls, we hypothesize the faces of the building\nsurface, and the final model is obtained by using an extended\nhypothesis-and-selection-based polygonal surface reconstruction framework.\nSpecifically, we introduce a new energy term to encourage roof preferences and\ntwo additional hard constraints into the optimization step to ensure correct\ntopology and enhance detail recovery. Experiments on various large-scale\nairborne point clouds have demonstrated that the method is superior to the\nstate-of-the-art methods in terms of reconstruction accuracy and robustness. In\naddition, we have generated a new dataset with our method consisting of the\npoint clouds and 3D models of 20k real-world buildings. We believe this dataset\ncan stimulate research in urban reconstruction from airborne point clouds and\nthe use of 3D city models in urban applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoter_J/0/1/0/all/0/1\">Jantien Stoter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peters_R/0/1/0/all/0/1\">Ravi Peters</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nan_L/0/1/0/all/0/1\">Liangliang Nan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"S2MS: Self-Supervised Learning Driven Multi-Spectral CT Image Enhancement. (arXiv:2201.10294v1 [eess.IV])","link":"http://arxiv.org/abs/2201.10294","description":"<p>Photon counting spectral CT (PCCT) can produce reconstructed attenuation maps\nin different energy channels, reflecting energy properties of the scanned\nobject. Due to the limited photon numbers and the non-ideal detector response\nof each energy channel, the reconstructed images usually contain much noise.\nWith the development of Deep Learning (DL) technique, different kinds of\nDL-based models have been proposed for noise reduction. However, most of the\nmodels require clean data set as the training labels, which are not always\navailable in medical imaging field. Inspiring by the similarities of each\nchannel's reconstructed image, we proposed a self-supervised learning based\nPCCT image enhancement framework via multi-spectral channels (S2MS). In S2MS\nframework, both the input and output labels are noisy images. Specifically, one\nsingle channel image was used as output while images of other single channels\nand channel-sum image were used as input to train the network, which can fully\nuse the spectral data information without extra cost. The simulation results\nbased on the AAPM Low-dose CT Challenge database showed that the proposed S2MS\nmodel can suppress the noise and preserve details more effectively in\ncomparison with the traditional DL models, which has potential to improve the\nimage quality of PCCT in clinical applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_C/0/1/0/all/0/1\">Chaoyang Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chang_S/0/1/0/all/0/1\">Shaojie Chang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bai_T/0/1/0/all/0/1\">Ti Bai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1\">Xi Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mutual information neural estimation for unsupervised multi-modal registration of brain images. (arXiv:2201.10305v1 [eess.IV])","link":"http://arxiv.org/abs/2201.10305","description":"<p>Many applications in image-guided surgery and therapy require fast and\nreliable non-linear, multi-modal image registration. Recently proposed\nunsupervised deep learning-based registration methods have demonstrated\nsuperior performance compared to iterative methods in just a fraction of the\ntime. Most of the learning-based methods have focused on mono-modal image\nregistration. The extension to multi-modal registration depends on the use of\nan appropriate similarity function, such as the mutual information (MI). We\npropose guiding the training of a deep learning-based registration method with\nMI estimation between an image-pair in an end-to-end trainable network. Our\nresults show that a small, 2-layer network produces competitive results in both\nmono- and multimodal registration, with sub-second run-times. Comparisons to\nboth iterative and deep learning-based methods show that our MI-based method\nproduces topologically and qualitatively superior results with an extremely low\nrate of non-diffeomorphic transformations. Real-time clinical application will\nbenefit from a better visual matching of anatomical structures and less\nregistration failures/outliers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Snaauw_G/0/1/0/all/0/1\">Gerard Snaauw</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Sasdelli_M/0/1/0/all/0/1\">Michele Sasdelli</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Maicas_G/0/1/0/all/0/1\">Gabriel Maicas</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Lau_S/0/1/0/all/0/1\">Stephan Lau</a> (1 and 2), <a href=\"http://arxiv.org/find/eess/1/au:+Verjans_J/0/1/0/all/0/1\">Johan Verjans</a> (1 and 2), <a href=\"http://arxiv.org/find/eess/1/au:+Jenkinson_M/0/1/0/all/0/1\">Mark Jenkinson</a> (1 and 2), <a href=\"http://arxiv.org/find/eess/1/au:+Carneiro_G/0/1/0/all/0/1\">Gustavo Carneiro</a> (1) ((1) Australian Institute for Machine Learning (AIML), University of Adelaide, Adelaide, Australia, (2) South Australian Health and Medical Research Institute (SAHMRI), Adelaide, Australia)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Addressing the Intra-class Mode Collapse Problem using Adaptive Input Image Normalization in GAN-based X-ray Images. (arXiv:2201.10324v1 [eess.IV])","link":"http://arxiv.org/abs/2201.10324","description":"<p>Biomedical image datasets can be imbalanced due to the rarity of targeted\ndiseases. Generative Adversarial Networks play a key role in addressing this\nimbalance by enabling the generation of synthetic images to augment and balance\ndatasets. It is important to generate synthetic images that incorporate a\ndiverse range of features such that they accurately represent the distribution\nof features present in the training imagery. Furthermore, the absence of\ndiverse features in synthetic images can degrade the performance of machine\nlearning classifiers. The mode collapse problem can impact a Generative\nAdversarial Network's capacity to generate diversified images. The mode\ncollapse comes in two varieties; intra-class and inter-class. In this paper,\nthe intra-class mode collapse problem is investigated, and its subsequent\nimpact on the diversity of synthetic X-ray images is evaluated. This work\ncontributes an empirical demonstration of the benefits of integrating the\nadaptive input-image normalization for the Deep Convolutional GAN to alleviate\nthe intra-class mode collapse problem. Results demonstrate that the DCGAN with\nadaptive input-image normalization outperforms DCGAN with un-normalized X-ray\nimages as evident by the superior diversity scores.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Saad_M/0/1/0/all/0/1\">Muhammad Muneeb Saad</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rehmani_M/0/1/0/all/0/1\">Mubashir Husain Rehmani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+OReilly_R/0/1/0/all/0/1\">Ruairi O&#x27;Reilly</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ShapeFormer: Transformer-based Shape Completion via Sparse Representation. (arXiv:2201.10326v1 [cs.CV])","link":"http://arxiv.org/abs/2201.10326","description":"<p>We present ShapeFormer, a transformer-based network that produces a\ndistribution of object completions, conditioned on incomplete, and possibly\nnoisy, point clouds. The resultant distribution can then be sampled to generate\nlikely completions, each exhibiting plausible shape details while being\nfaithful to the input. To facilitate the use of transformers for 3D, we\nintroduce a compact 3D representation, vector quantized deep implicit function,\nthat utilizes spatial sparsity to represent a close approximation of a 3D shape\nby a short sequence of discrete variables. Experiments demonstrate that\nShapeFormer outperforms prior art for shape completion from ambiguous partial\ninputs in terms of both completion quality and diversity. We also show that our\napproach effectively handles a variety of shape types, incomplete patterns, and\nreal-world scans.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1\">Xingguang Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Liqiang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitra_N/0/1/0/all/0/1\">Niloy J. Mitra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lischinski_D/0/1/0/all/0/1\">Dani Lischinski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_Or_D/0/1/0/all/0/1\">Danny Cohen-Or</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hui Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ultra Low-Parameter Denoising: Trainable Bilateral Filter Layers in Computed Tomography. (arXiv:2201.10345v1 [eess.IV])","link":"http://arxiv.org/abs/2201.10345","description":"<p>Computed tomography is widely used as an imaging tool to visualize\nthree-dimensional structures with expressive bone-soft tissue contrast.\nHowever, CT resolution and radiation dose are tightly entangled, highlighting\nthe importance of low-dose CT combined with sophisticated denoising algorithms.\nMost data-driven denoising techniques are based on deep neural networks and,\ntherefore, contain hundreds of thousands of trainable parameters, making them\nincomprehensible and prone to prediction failures. Developing understandable\nand robust denoising algorithms achieving state-of-the-art performance helps to\nminimize radiation dose while maintaining data integrity. This work presents an\nopen-source CT denoising framework based on the idea of bilateral filtering. We\npropose a bilateral filter that can be incorporated into a deep learning\npipeline and optimized in a purely data-driven way by calculating the gradient\nflow toward its hyperparameters and its input. Denoising in pure image-to-image\npipelines and across different domains such as raw detector data and\nreconstructed volume, using a differentiable backprojection layer, is\ndemonstrated. Although only using three spatial parameters and one range\nparameter per filter layer, the proposed denoising pipelines can compete with\ndeep state-of-the-art denoising architectures with several hundred thousand\nparameters. Competitive denoising performance is achieved on x-ray microscope\nbone data (0.7053 and 33.10) and the 2016 Low Dose CT Grand Challenge dataset\n(0.9674 and 43.07) in terms of SSIM and PSNR. Due to the extremely low number\nof trainable parameters with well-defined effect, prediction reliance and data\nintegrity is guaranteed at any time in the proposed pipelines, in contrast to\nmost other deep learning-based denoising architectures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wagner_F/0/1/0/all/0/1\">Fabian Wagner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Thies_M/0/1/0/all/0/1\">Mareike Thies</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gu_M/0/1/0/all/0/1\">Mingxuan Gu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1\">Yixing Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pechmann_S/0/1/0/all/0/1\">Sabrina Pechmann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Patwari_M/0/1/0/all/0/1\">Mayank Patwari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ploner_S/0/1/0/all/0/1\">Stefan Ploner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Aust_O/0/1/0/all/0/1\">Oliver Aust</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Uderhardt_S/0/1/0/all/0/1\">Stefan Uderhardt</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schett_G/0/1/0/all/0/1\">Georg Schett</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Christiansen_S/0/1/0/all/0/1\">Silke Christiansen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maier_A/0/1/0/all/0/1\">Andreas Maier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Multi-modal Fusion Framework Based on Multi-task Correlation Learning for Cancer Prognosis Prediction. (arXiv:2201.10353v1 [cs.LG])","link":"http://arxiv.org/abs/2201.10353","description":"<p>Morphological attributes from histopathological images and molecular profiles\nfrom genomic data are important information to drive diagnosis, prognosis, and\ntherapy of cancers. By integrating these heterogeneous but complementary data,\nmany multi-modal methods are proposed to study the complex mechanisms of\ncancers, and most of them achieve comparable or better results from previous\nsingle-modal methods. However, these multi-modal methods are restricted to a\nsingle task (e.g., survival analysis or grade classification), and thus neglect\nthe correlation between different tasks. In this study, we present a\nmulti-modal fusion framework based on multi-task correlation learning\n(MultiCoFusion) for survival analysis and cancer grade classification, which\ncombines the power of multiple modalities and multiple tasks. Specifically, a\npre-trained ResNet-152 and a sparse graph convolutional network (SGCN) are used\nto learn the representations of histopathological images and mRNA expression\ndata respectively. Then these representations are fused by a fully connected\nneural network (FCNN), which is also a multi-task shared network. Finally, the\nresults of survival analysis and cancer grade classification output\nsimultaneously. The framework is trained by an alternate scheme. We\nsystematically evaluate our framework using glioma datasets from The Cancer\nGenome Atlas (TCGA). Results demonstrate that MultiCoFusion learns better\nrepresentations than traditional feature extraction methods. With the help of\nmulti-task alternating learning, even simple multi-modal concatenation can\nachieve better performance than other deep learning and traditional methods.\nMulti-task learning can improve the performance of multiple tasks not just one\nof them, and it is effective in both single-modal and multi-modal data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_K/0/1/0/all/0/1\">Kaiwen Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Weixian Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaofeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jinlong Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_S/0/1/0/all/0/1\">Shoubin Dong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Resource-efficient Deep Neural Networks for Automotive Radar Interference Mitigation. (arXiv:2201.10360v1 [eess.SP])","link":"http://arxiv.org/abs/2201.10360","description":"<p>Radar sensors are crucial for environment perception of driver assistance\nsystems as well as autonomous vehicles. With a rising number of radar sensors\nand the so far unregulated automotive radar frequency band, mutual interference\nis inevitable and must be dealt with. Algorithms and models operating on radar\ndata are required to run the early processing steps on specialized radar sensor\nhardware. This specialized hardware typically has strict resource-constraints,\ni.e. a low memory capacity and low computational power. Convolutional Neural\nNetwork (CNN)-based approaches for denoising and interference mitigation yield\npromising results for radar processing in terms of performance. Regarding\nresource-constraints, however, CNNs typically exceed the hardware's capacities\nby far.\n</p>\n<p>In this paper we investigate quantization techniques for CNN-based denoising\nand interference mitigation of radar signals. We analyze the quantization of\n(i) weights and (ii) activations of different CNN-based model architectures.\nThis quantization results in reduced memory requirements for model storage and\nduring inference. We compare models with fixed and learned bit-widths and\ncontrast two different methodologies for training quantized CNNs, i.e. the\nstraight-through gradient estimator and training distributions over discrete\nweights. We illustrate the importance of structurally small real-valued base\nmodels for quantization and show that learned bit-widths yield the smallest\nmodels. We achieve a memory reduction of around 80\\% compared to the\nreal-valued baseline. Due to practical reasons, however, we recommend the use\nof 8 bits for weights and activations, which results in models that require\nonly 0.2 megabytes of memory.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Rock_J/0/1/0/all/0/1\">Johanna Rock</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Roth_W/0/1/0/all/0/1\">Wolfgang Roth</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Toth_M/0/1/0/all/0/1\">Mate Toth</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meissner_P/0/1/0/all/0/1\">Paul Meissner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pernkopf_F/0/1/0/all/0/1\">Franz Pernkopf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ADAPT: An Open-Source sUAS Payload for Real-Time Disaster Prediction and Response with AI. (arXiv:2201.10366v1 [cs.CV])","link":"http://arxiv.org/abs/2201.10366","description":"<p>Small unmanned aircraft systems (sUAS) are becoming prominent components of\nmany humanitarian assistance and disaster response (HADR) operations. Pairing\nsUAS with onboard artificial intelligence (AI) substantially extends their\nutility in covering larger areas with fewer support personnel. A variety of\nmissions, such as search and rescue, assessing structural damage, and\nmonitoring forest fires, floods, and chemical spills, can be supported simply\nby deploying the appropriate AI models. However, adoption by\nresource-constrained groups, such as local municipalities, regulatory agencies,\nand researchers, has been hampered by the lack of a cost-effective,\nreadily-accessible baseline platform that can be adapted to their unique\nmissions. To fill this gap, we have developed the free and open-source ADAPT\nmulti-mission payload for deploying real-time AI and computer vision onboard a\nsUAS during local and beyond-line-of-site missions. We have emphasized a\nmodular design with low-cost, readily-available components, open-source\nsoftware, and thorough documentation (https://kitware.github.io/adapt/). The\nsystem integrates an inertial navigation system, high-resolution color camera,\ncomputer, and wireless downlink to process imagery and broadcast georegistered\nanalytics back to a ground station. Our goal is to make it easy for the HADR\ncommunity to build their own copies of the ADAPT payload and leverage the\nthousands of hours of engineering we have devoted to developing and testing. In\nthis paper, we detail the development and testing of the ADAPT payload. We\ndemonstrate the example mission of real-time, in-flight ice segmentation to\nmonitor river ice state and provide timely predictions of catastrophic flooding\nevents. We deploy a novel active learning workflow to annotate river ice\nimagery, train a real-time deep neural network for ice segmentation, and\ndemonstrate operation in the field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Davila_D/0/1/0/all/0/1\">Daniel Davila</a>, <a href=\"http://arxiv.org/find/cs/1/au:+VanPelt_J/0/1/0/all/0/1\">Joseph VanPelt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lynch_A/0/1/0/all/0/1\">Alexander Lynch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romlein_A/0/1/0/all/0/1\">Adam Romlein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Webley_P/0/1/0/all/0/1\">Peter Webley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_M/0/1/0/all/0/1\">Matthew S. Brown</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Winograd Convolution for Deep Neural Networks: Efficient Point Selection. (arXiv:2201.10369v1 [cs.CV])","link":"http://arxiv.org/abs/2201.10369","description":"<p>Convolutional neural networks (CNNs) have dramatically improved the accuracy\nof tasks such as object recognition, image segmentation and interactive speech\nsystems. CNNs require large amounts of computing resources because\nofcomputationally intensive convolution layers. Fast convolution algorithms\nsuch as Winograd convolution can greatly reduce the computational cost of these\nlayers at a cost of poor numeric properties, such that greater savings in\ncomputation exponentially increase floating point errors.\n</p>\n<p>A defining feature of each Winograd convolution algorithm is a set of\nreal-value points where polynomials are sampled. The choice of points impacts\nthe numeric accuracy of the algorithm, but the optimal set of points for small\nconvolutions remains unknown. Existing work considers only small integers and\nsimple fractions as candidate points. In this work, we propose a novel approach\nto point selection using points of the form {-1/c , -c, c, 1/c } using the full\nrange of real-valued numbers for c. We show that groups of this form cause\ncancellations in the Winograd transform matrices that reduce numeric error. We\nfind empirically that the error for different values of c forms a rough curve\nacross the range of real-value numbers helping to localize the values of c that\nreduce error and that lower errors can be achieved with non-obvious real-valued\nevaluation points instead of integers or simple fractions. We study a range of\nsizes for small convolutions and achieve reduction in error ranging from 2% to\naround 59% for both 1D and 2D convolution. Furthermore, we identify patterns in\ncases when we select a subset of our proposed points which will always lead to\na lower error. Finally we implement a complete Winograd convolution layer and\nuse it to run deep convolution neural networks on real datasets and show that\nour proposed points reduce error, ranging from 22% to 63%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alam_S/0/1/0/all/0/1\">Syed Asad Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anderson_A/0/1/0/all/0/1\">Andrew Anderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barabasz_B/0/1/0/all/0/1\">Barbara Barabasz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gregg_D/0/1/0/all/0/1\">David Gregg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BLDNet: A Semi-supervised Change Detection Building Damage Framework using Graph Convolutional Networks and Urban Domain Knowledge. (arXiv:2201.10389v1 [cs.CV])","link":"http://arxiv.org/abs/2201.10389","description":"<p>Change detection is instrumental to localize damage and understand\ndestruction in disaster informatics. While convolutional neural networks are at\nthe core of recent change detection solutions, we present in this work, BLDNet,\na novel graph formulation for building damage change detection and enable\nlearning relationships and representations from both local patterns and\nnon-stationary neighborhoods. More specifically, we use graph convolutional\nnetworks to efficiently learn these features in a semi-supervised framework\nwith few annotated data. Additionally, BLDNet formulation allows for the\ninjection of additional contextual building meta-features. We train and\nbenchmark on the xBD dataset to validate the effectiveness of our approach. We\nalso demonstrate on urban data from the 2020 Beirut Port Explosion that\nperformance is improved by incorporating domain knowledge building\nmeta-features.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ismail_A/0/1/0/all/0/1\">Ali Ismail</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awad_M/0/1/0/all/0/1\">Mariette Awad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Capturing Temporal Information in a Single Frame: Channel Sampling Strategies for Action Recognition. (arXiv:2201.10394v1 [cs.CV])","link":"http://arxiv.org/abs/2201.10394","description":"<p>We address the problem of capturing temporal information for video\nclassification in 2D networks, without increasing computational cost. Existing\napproaches focus on modifying the architecture of 2D networks (e.g. by\nincluding filters in the temporal dimension to turn them into 3D networks, or\nusing optical flow, etc.), which increases computation cost. Instead, we\npropose a novel sampling strategy, where we re-order the channels of the input\nvideo, to capture short-term frame-to-frame changes. We observe that without\nbells and whistles, the proposed sampling strategy improves performance on\nmultiple architectures (e.g. TSN, TRN, and TSM) and datasets (CATER,\nSomething-Something-V1 and V2), up to 24% over the baseline of using the\nstandard video input. In addition, our sampling strategies do not require\ntraining from scratch and do not increase the computational cost of training\nand testing. Given the generality of the results and the flexibility of the\napproach, we hope this can be widely useful to the video understanding\ncommunity. Code is available at https://github.com/kiyoon/PyVideoAI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kiyoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gowda_S/0/1/0/all/0/1\">Shreyank N Gowda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aodha_O/0/1/0/all/0/1\">Oisin Mac Aodha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sevilla_Lara_L/0/1/0/all/0/1\">Laura Sevilla-Lara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Cross-Disaster Building Damage Assessment with Graph Convolutional Networks. (arXiv:2201.10395v1 [cs.CV])","link":"http://arxiv.org/abs/2201.10395","description":"<p>In the aftermath of disasters, building damage maps are obtained using change\ndetection to plan rescue operations. Current convolutional neural network\napproaches do not consider the similarities between neighboring buildings for\npredicting the damage. We present a novel graph-based building damage detection\nsolution to capture these relationships. Our proposed model architecture learns\nfrom both local and neighborhood features to predict building damage.\nSpecifically, we adopt the sample and aggregate graph convolution strategy to\nlearn aggregation functions that generalize to unseen graphs which is essential\nfor alleviating the time needed to obtain predictions for new disasters. Our\nexperiments on the xBD dataset and comparisons with a classical convolutional\nneural network reveal that while our approach is handicapped by class\nimbalance, it presents a promising and distinct advantage when it comes to\ncross-disaster generalization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ismail_A/0/1/0/all/0/1\">Ali Ismail</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awad_M/0/1/0/all/0/1\">Mariette Awad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comparison of Evaluation Metrics for Landmark Detection in CMR Images. (arXiv:2201.10410v1 [cs.CV])","link":"http://arxiv.org/abs/2201.10410","description":"<p>Cardiac Magnetic Resonance (CMR) images are widely used for cardiac diagnosis\nand ventricular assessment. Extracting specific landmarks like the right\nventricular insertion points is of importance for spatial alignment and 3D\nmodeling. The automatic detection of such landmarks has been tackled by\nmultiple groups using Deep Learning, but relatively little attention has been\npaid to the failure cases of evaluation metrics in this field. In this work, we\nextended the public ACDC dataset with additional labels of the right\nventricular insertion points and compare different variants of a heatmap-based\nlandmark detection pipeline. In this comparison, we demonstrate very likely\npitfalls of apparently simple detection and localisation metrics which\nhighlights the importance of a clear detection strategy and the definition of\nan upper limit for localisation-based metrics. Our preliminary results indicate\nthat a combination of different metrics is necessary, as they yield different\nwinners for method comparison. Additionally, they highlight the need of a\ncomprehensive metric description and evaluation standardisation, especially for\nthe error cases where no metrics could be computed or where no lower/upper\nboundary of a metric exists. Code and labels:\nhttps://github.com/Cardio-AI/rvip_landmark_detection\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Koehler_S/0/1/0/all/0/1\">Sven Koehler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharan_L/0/1/0/all/0/1\">Lalith Sharan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuhm_J/0/1/0/all/0/1\">Julian Kuhm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanaat_A/0/1/0/all/0/1\">Arman Ghanaat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gordejeva_J/0/1/0/all/0/1\">Jelizaveta Gordejeva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simon_N/0/1/0/all/0/1\">Nike K. Simon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grell_N/0/1/0/all/0/1\">Niko M. Grell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andre_F/0/1/0/all/0/1\">Florian Andr&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Engelhardt_S/0/1/0/all/0/1\">Sandy Engelhardt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rayleigh EigenDirections (REDs): GAN latent space traversals for multidimensional features. (arXiv:2201.10423v1 [cs.CV])","link":"http://arxiv.org/abs/2201.10423","description":"<p>We present a method for finding paths in a deep generative model's latent\nspace that can maximally vary one set of image features while holding others\nconstant. Crucially, unlike past traversal approaches, ours can manipulate\nmultidimensional features of an image such as facial identity and pixels within\na specified region. Our method is principled and conceptually simple: optimal\ntraversal directions are chosen by maximizing differential changes to one\nfeature set such that changes to another set are negligible. We show that this\nproblem is nearly equivalent to one of Rayleigh quotient maximization, and\nprovide a closed-form solution to it based on solving a generalized eigenvalue\nequation. We use repeated computations of the corresponding optimal directions,\nwhich we call Rayleigh EigenDirections (REDs), to generate appropriately curved\npaths in latent space. We empirically evaluate our method using StyleGAN2 on\ntwo image domains: faces and living rooms. We show that our method is capable\nof controlling various multidimensional features out of the scope of previous\nlatent space traversal methods: face identity, spatial frequency bands, pixels\nwithin a region, and the appearance and position of an object. Our work\nsuggests that a wealth of opportunities lies in the local analysis of the\ngeometry and semantics of latent spaces.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Balakrishnan_G/0/1/0/all/0/1\">Guha Balakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gadde_R/0/1/0/all/0/1\">Raghudeep Gadde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martinez_A/0/1/0/all/0/1\">Aleix Martinez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perona_P/0/1/0/all/0/1\">Pietro Perona</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Plaque segmentation via masking of the artery wall. (arXiv:2201.10424v1 [eess.IV])","link":"http://arxiv.org/abs/2201.10424","description":"<p>The presence of plaques in the coronary arteries are a major risk to the\npatients' life. In particular, non-calcified plaques pose a great challenge, as\nthey are harder to detect and more likely to rupture than calcified plaques.\nWhile current deep learning techniques allow precise segmentation of regular\nimages, the performance in medical images is still low, caused mostly by\nblurriness and ambiguous voxel intensities of unrelated parts that fall on the\nsame range. In this paper, we propose a novel methodology for segmenting\ncalcified and non-calcified plaques in CCTA-CPR scans of coronary arteries. The\ninput slices are masked so only the voxels within the wall vessel are\nconsidered for segmentation. We also provide an exhaustive evaluation by\napplying different types of masks, in order to validate the potential of vessel\nmasking for plaque segmentation. Our methodology results in a prominent boost\nin segmentation performance, in both quantitative and qualitative evaluation,\nachieving accurate plaque shapes even for the challenging non-calcified\nplaques. We believe our findings can lead the future research for\nhigh-performance plaque segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Tejero_de_Pablos_A/0/1/0/all/0/1\">Antonio Tejero-de-Pablos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yamane_H/0/1/0/all/0/1\">Hiroaki Yamane</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kurose_Y/0/1/0/all/0/1\">Yusuke Kurose</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Iho_J/0/1/0/all/0/1\">Junichi Iho</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tokunaga_Y/0/1/0/all/0/1\">Youji Tokunaga</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Horie_M/0/1/0/all/0/1\">Makoto Horie</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nishizawa_K/0/1/0/all/0/1\">Keisuke Nishizawa</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hayashi_Y/0/1/0/all/0/1\">Yusaku Hayashi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Koyama_Y/0/1/0/all/0/1\">Yasushi Koyama</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Harada_T/0/1/0/all/0/1\">Tatsuya Harada</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Main Product Detection with Graph Networks for Fashion. (arXiv:2201.10431v1 [cs.CV])","link":"http://arxiv.org/abs/2201.10431","description":"<p>Computer vision has established a foothold in the online fashion retail\nindustry. Main product detection is a crucial step of vision-based fashion\nproduct feed parsing pipelines, focused in identifying the bounding boxes that\ncontain the product being sold in the gallery of images of the product page.\nThe current state-of-the-art approach does not leverage the relations between\nregions in the image, and treats images of the same product independently,\ntherefore not fully exploiting visual and product contextual information. In\nthis paper we propose a model that incorporates Graph Convolutional Networks\n(GCN) that jointly represent all detected bounding boxes in the gallery as\nnodes. We show that the proposed method is better than the state-of-the-art,\nespecially, when we consider the scenario where title-input is missing at\ninference time and for cross-dataset evaluation, our method outperforms\nprevious approaches by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yazici_V/0/1/0/all/0/1\">Vacit Oguz Yazici</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Longlong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramisa_A/0/1/0/all/0/1\">Arnau Ramisa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herranz_L/0/1/0/all/0/1\">Luis Herranz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weijer_J/0/1/0/all/0/1\">Joost van de Weijer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformer-Based Video Front-Ends for Audio-Visual Speech Recognition. (arXiv:2201.10439v1 [cs.CV])","link":"http://arxiv.org/abs/2201.10439","description":"<p>Audio-visual automatic speech recognition (AV-ASR) extends the speech\nrecognition by introducing the video modality. In particular, the information\ncontained in the motion of the speaker's mouth is used to augment the audio\nfeatures. The video modality is traditionally processed with a 3D convolutional\nneural network (e.g. 3D version of VGG). Recently, image transformer networks\n<a href=\"/abs/2010.11929\">arXiv:2010.11929</a> demonstrated the ability to extract rich visual features for\nthe image classification task. In this work, we propose to replace the 3D\nconvolution with a video transformer video feature extractor. We train our\nbaselines and the proposed model on a large scale corpus of the YouTube videos.\nThen we evaluate the performance on a labeled subset of YouTube as well as on\nthe public corpus LRS3-TED. Our best model video-only model achieves the\nperformance of 34.9% WER on YTDEV18 and 19.3% on LRS3-TED which is a 10% and 9%\nrelative improvements over the convolutional baseline. We achieve the state of\nthe art performance of the audio-visual recognition on the LRS3-TED after\nfine-tuning our model (1.6% WER).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Serdyuk_D/0/1/0/all/0/1\">Dmitriy Serdyuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Braga_O/0/1/0/all/0/1\">Otavio Braga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siohan_O/0/1/0/all/0/1\">Olivier Siohan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AggMatch: Aggregating Pseudo Labels for Semi-Supervised Learning. (arXiv:2201.10444v1 [cs.LG])","link":"http://arxiv.org/abs/2201.10444","description":"<p>Semi-supervised learning (SSL) has recently proven to be an effective\nparadigm for leveraging a huge amount of unlabeled data while mitigating the\nreliance on large labeled data. Conventional methods focused on extracting a\npseudo label from individual unlabeled data sample and thus they mostly\nstruggled to handle inaccurate or noisy pseudo labels, which degenerate\nperformance.\n</p>\n<p>In this paper, we address this limitation with a novel SSL framework for\naggregating pseudo labels, called AggMatch, which refines initial pseudo labels\nby using different confident instances. Specifically, we introduce an\naggregation module for consistency regularization framework that aggregates the\ninitial pseudo labels based on the similarity between the instances. To enlarge\nthe aggregation candidates beyond the mini-batch, we present a class-balanced\nconfidence-aware queue built with the momentum model, encouraging to provide\nmore stable and consistent aggregation. We also propose a novel\nuncertainty-based confidence measure for the pseudo label by considering the\nconsensus among multiple hypotheses with different subsets of the queue. We\nconduct experiments to demonstrate the effectiveness of AggMatch over the\nlatest methods on standard benchmarks and provide extensive analyses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jiwon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryoo_K/0/1/0/all/0/1\">Kwangrok Ryoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Gyuseong Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1\">Seokju Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_J/0/1/0/all/0/1\">Junyoung Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Daehwan Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_H/0/1/0/all/0/1\">Hansang Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seungryong Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Low Can We Go? Pixel Annotation for Semantic Segmentation. (arXiv:2201.10448v1 [cs.CV])","link":"http://arxiv.org/abs/2201.10448","description":"<p>How many labeled pixels are needed to segment an image, without any prior\nknowledge? We conduct an experiment to answer this question.\n</p>\n<p>In our experiment, an Oracle is using Active Learning to train a network from\nscratch. The Oracle has access to the entire label map of the image, but the\ngoal is to reveal as little pixel labels to the network as possible. We find\nthat, on average, the Oracle needs to reveal (i.e., annotate) less than $0.1\\%$\nof the pixels in order to train a network. The network can then label all\npixels in the image at an accuracy of more than $98\\%$.\n</p>\n<p>Based on this single-image-annotation experiment, we design an experiment to\nquickly annotate an entire data set. In the data set level experiment the\nOracle trains a new network for each image from scratch. The network can then\nbe used to create pseudo-labels, which are the network predicted labels of the\nunlabeled pixels, for the entire image. Only then, a data set level network is\ntrained from scratch on all the pseudo-labeled images at once.\n</p>\n<p>We repeat both image level and data set level experiments on two, very\ndifferent, real-world data sets, and find that it is possible to reach the\nperformance of a fully annotated data set using a fraction of the annotation\ncost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kigli_D/0/1/0/all/0/1\">Daniel Kigli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shamir_A/0/1/0/all/0/1\">Ariel Shamir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avidan_S/0/1/0/all/0/1\">Shai Avidan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sphere2Vec: Multi-Scale Representation Learning over a Spherical Surface for Geospatial Predictions. (arXiv:2201.10489v1 [cs.CV])","link":"http://arxiv.org/abs/2201.10489","description":"<p>Generating learning-friendly representations for points in a 2D space is a\nfundamental and long-standing problem in machine learning. Recently,\nmulti-scale encoding schemes (such as Space2Vec) were proposed to directly\nencode any point in 2D space as a high-dimensional vector, and has been\nsuccessfully applied to various (geo)spatial prediction tasks. However, a map\nprojection distortion problem rises when applying location encoding models to\nlarge-scale real-world GPS coordinate datasets (e.g., species images taken all\nover the world) - all current location encoding models are designed for\nencoding points in a 2D (Euclidean) space but not on a spherical surface, e.g.,\nearth surface. To solve this problem, we propose a multi-scale location\nencoding model called Sphere2V ec which directly encodes point coordinates on a\nspherical surface while avoiding the mapprojection distortion problem. We\nprovide theoretical proof that the Sphere2Vec encoding preserves the spherical\nsurface distance between any two points. We also developed a unified view of\ndistance-reserving encoding on spheres based on the Double Fourier Sphere\n(DFS). We apply Sphere2V ec to the geo-aware image classification task. Our\nanalysis shows that Sphere2V ec outperforms other 2D space location encoder\nmodels especially on the polar regions and data-sparse areas for image\nclassification tasks because of its nature for spherical surface distance\npreservation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mai_G/0/1/0/all/0/1\">Gengchen Mai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xuan_Y/0/1/0/all/0/1\">Yao Xuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_W/0/1/0/all/0/1\">Wenyun Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Janowicz_K/0/1/0/all/0/1\">Krzysztof Janowicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lao_N/0/1/0/all/0/1\">Ni Lao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Initial Investigations Towards Non-invasive Monitoring of Chronic Wound Healing Using Deep Learning and Ultrasound Imaging. (arXiv:2201.10511v1 [eess.IV])","link":"http://arxiv.org/abs/2201.10511","description":"<p>Chronic wounds including diabetic and arterial/venous insufficiency injuries\nhave become a major burden for healthcare systems worldwide. Demographic\nchanges suggest that wound care will play an even bigger role in the coming\ndecades. Predicting and monitoring response to therapy in wound care is\ncurrently largely based on visual inspection with little information on the\nunderlying tissue. Thus, there is an urgent unmet need for innovative\napproaches that facilitate personalized diagnostics and treatments at the\npoint-of-care. It has been recently shown that ultrasound imaging can monitor\nresponse to therapy in wound care, but this work required onerous manual image\nannotations. In this study, we present initial results of a deep learning-based\nautomatic segmentation of cross-sectional wound size in ultrasound images and\nidentify requirements and challenges for future research on this application.\nEvaluation of the segmentation results underscores the potential of the\nproposed deep learning approach to complement non-invasive imaging with Dice\nscores of 0.34 (U-Net, FCN) and 0.27 (ResNet-U-Net) but also highlights the\nneed for improving robustness further. We conclude that deep learning-supported\nanalysis of non-invasive ultrasound images is a promising area of research to\nautomatically extract cross-sectional wound size and depth information with\npotential value in monitoring response to therapy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Schlereth_M/0/1/0/all/0/1\">Maja Schlereth</a> (1,2), <a href=\"http://arxiv.org/find/eess/1/au:+Stromer_D/0/1/0/all/0/1\">Daniel Stromer</a> (2), <a href=\"http://arxiv.org/find/eess/1/au:+Mantri_Y/0/1/0/all/0/1\">Yash Mantri</a> (3), <a href=\"http://arxiv.org/find/eess/1/au:+Tsujimoto_J/0/1/0/all/0/1\">Jason Tsujimoto</a> (3), <a href=\"http://arxiv.org/find/eess/1/au:+Breininger_K/0/1/0/all/0/1\">Katharina Breininger</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Maier_A/0/1/0/all/0/1\">Andreas Maier</a> (2), <a href=\"http://arxiv.org/find/eess/1/au:+Anderson_C/0/1/0/all/0/1\">Caesar Anderson</a> (4), <a href=\"http://arxiv.org/find/eess/1/au:+Garimella_P/0/1/0/all/0/1\">Pranav S. Garimella</a> (5), <a href=\"http://arxiv.org/find/eess/1/au:+Jokerst_J/0/1/0/all/0/1\">Jesse V. Jokerst</a> (6) ((1) Department Artificial Intelligence in Biomedical Engineering, FAU Erlangen-N&#xfc;rnberg, Erlangen, (2) Pattern Recognition Lab, FAU Erlangen-N&#xfc;rnberg, Erlangen, (3) Department of Bioengineering, University of California, San Diego, (4) Department of Emergency Medicine, San Diego, (5) Division of Nephrology and Hypertension, Department of Medicine, San Diego, (6) Department of Nanoengineering, University of California, San Diego)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Activation-based Structured Pruning. (arXiv:2201.10520v1 [cs.CV])","link":"http://arxiv.org/abs/2201.10520","description":"<p>Pruning is a promising approach to compress complex deep learning models in\norder to deploy them on resource-constrained edge devices. However, many\nexisting pruning solutions are based on unstructured pruning, which yield\nmodels that cannot efficiently run on commodity hardware, and require users to\nmanually explore and tune the pruning process, which is time consuming and\noften leads to sub-optimal results. To address these limitations, this paper\npresents an adaptive, activation-based, structured pruning approach to\nautomatically and efficiently generate small, accurate, and hardware-efficient\nmodels that meet user requirements. First, it proposes iterative structured\npruning using activation-based attention feature maps to effectively identify\nand prune unimportant filters. Then, it proposes adaptive pruning policies for\nautomatically meeting the pruning objectives of accuracy-critical,\nmemory-constrained, and latency-sensitive tasks. A comprehensive evaluation\nshows that the proposed method can substantially outperform the\nstate-of-the-art structured pruning works on CIFAR-10 and ImageNet datasets.\nFor example, on ResNet-56 with CIFAR-10, without any accuracy drop, our method\nachieves the largest parameter reduction (79.11%), outperforming the related\nworks by 22.81% to 66.07%, and the largest FLOPs reduction (70.13%),\noutperforming the related works by 14.13% to 26.53%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_K/0/1/0/all/0/1\">Kaiqi Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Animesh Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1\">Ming Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Review of Deep Learning Based Image Super-resolution Techniques. (arXiv:2201.10521v1 [cs.CV])","link":"http://arxiv.org/abs/2201.10521","description":"<p>Image super-resolution technology is the process of obtaining high-resolution\nimages from one or more low-resolution images. With the development of deep\nlearning, image super-resolution technology based on deep learning method is\nemerging. This paper reviews the research progress of the application of depth\nlearning method in the field of image super-resolution, introduces this kind of\nsuper-resolution work from several aspects, and looks forward to the further\napplication of depth learning method in the field of image super-resolution. By\ncollecting and counting the relevant literature on the application of depth\nlearning in the field of image super-resolution, we preliminarily summarizes\nthe application results of depth learning method in the field of image\nsuper-resolution, and reports the latest progress of image super-resolution\ntechnology based on depth learning method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Fangyuan Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Blind Image Deblurring: a Review. (arXiv:2201.10522v1 [cs.CV])","link":"http://arxiv.org/abs/2201.10522","description":"<p>This is a review on blind image deblurring. First, we formulate the blind\nimage deblurring problem and explain why it is challenging. Next, we bring some\npsychological and cognitive studies on the way our human vision system deblurs.\nThen, relying on several previous reviews, we discuss the topic of metrics and\ndatasets, which is non-trivial to blind deblurring. Finally, we introduce some\ntypical optimization-based methods and learning-based methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xue_Z/0/1/0/all/0/1\">Zhengrong Xue</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interpretability in Convolutional Neural Networks for Building Damage Classification in Satellite Imagery. (arXiv:2201.10523v1 [cs.CV])","link":"http://arxiv.org/abs/2201.10523","description":"<p>Natural disasters ravage the world's cities, valleys, and shores on a regular\nbasis. Deploying precise and efficient computational mechanisms for assessing\ninfrastructure damage is essential to channel resources and minimize the loss\nof life. Using a dataset that includes labeled pre- and post- disaster\nsatellite imagery, we take a machine learning-based remote sensing approach and\ntrain multiple convolutional neural networks (CNNs) to assess building damage\non a per-building basis. We present a novel methodology of interpretable deep\nlearning that seeks to explicitly investigate the most useful modalities of\ninformation in the training data to create an accurate classification model. We\nalso investigate which loss functions best optimize these models. Our findings\ninclude that ordinal-cross entropy loss is the most optimal criterion for\noptimization to use and that including the type of disaster that caused the\ndamage in combination with pre- and post-disaster training data most accurately\npredicts the level of damage caused. Further, we make progress in the\nqualitative representation of which parts of the images that the model is using\nto predict damage levels, through gradient-weighted class activation mapping\n(Grad-CAM). Our research seeks to computationally contribute to aiding in this\nongoing and growing humanitarian crisis, heightened by anthropogenic climate\nchange.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Thomas Y. Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MonarchNet: Differentiating Monarch Butterflies from Butterflies Species with Similar Phenotypes. (arXiv:2201.10526v1 [cs.CV])","link":"http://arxiv.org/abs/2201.10526","description":"<p>In recent years, the monarch butterfly's iconic migration patterns have come\nunder threat from a number of factors, from climate change to pesticide use. To\ntrack trends in their populations, scientists as well as citizen scientists\nmust identify individuals accurately. This is uniquely key for the study of\nmonarch butterflies because there exist other species of butterfly, such as\nviceroy butterflies, that are \"look-alikes\" (coined by the Convention on\nInternational Trade in Endangered Species of Wild Fauna and Flora), having\nsimilar phenotypes. To tackle this problem and to aid in more efficient\nidentification, we present MonarchNet, the first comprehensive dataset\nconsisting of butterfly imagery for monarchs and five look-alike species. We\ntrain a baseline deep-learning classification model to serve as a tool for\ndifferentiating monarch butterflies and its various look-alikes. We seek to\ncontribute to the study of biodiversity and butterfly ecology by providing a\nnovel method for computational classification of these particular butterfly\nspecies. The ultimate aim is to help scientists track monarch butterfly\npopulation and migration trends in the most precise and efficient manner\npossible.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Thomas Y. Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Representation Distillation. (arXiv:1910.10699v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/1910.10699","description":"<p>Often we wish to transfer representational knowledge from one neural network\nto another. Examples include distilling a large network into a smaller one,\ntransferring knowledge from one sensory modality to a second, or ensembling a\ncollection of models into a single estimator. Knowledge distillation, the\nstandard approach to these problems, minimizes the KL divergence between the\nprobabilistic outputs of a teacher and student network. We demonstrate that\nthis objective ignores important structural knowledge of the teacher network.\nThis motivates an alternative objective by which we train a student to capture\nsignificantly more information in the teacher's representation of the data. We\nformulate this objective as contrastive learning. Experiments demonstrate that\nour resulting new objective outperforms knowledge distillation and other\ncutting-edge distillers on a variety of knowledge transfer tasks, including\nsingle model compression, ensemble distillation, and cross-modal transfer. Our\nmethod sets a new state-of-the-art in many transfer tasks, and sometimes even\noutperforms the teacher network when combined with knowledge distillation.\nCode: <a href=\"http://github.com/HobbitLong/RepDistiller.\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yonglong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnan_D/0/1/0/all/0/1\">Dilip Krishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Isola_P/0/1/0/all/0/1\">Phillip Isola</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Posterior Adaptation With New Priors. (arXiv:2007.01386v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2007.01386","description":"<p>Classification approaches based on the direct estimation and analysis of\nposterior probabilities will degrade if the original class priors begin to\nchange. We prove that a unique (up to scale) solution is possible to recover\nthe data likelihoods for a test example from its original class posteriors and\ndataset priors. Given the recovered likelihoods and a set of new priors, the\nposteriors can be re-computed using Bayes' Rule to reflect the influence of the\nnew priors. The method is simple to compute and allows a dynamic update of the\noriginal posteriors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Davis_J/0/1/0/all/0/1\">Jim Davis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vid2CAD: CAD Model Alignment using Multi-View Constraints from Videos. (arXiv:2012.04641v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.04641","description":"<p>We address the task of aligning CAD models to a video sequence of a complex\nscene containing multiple objects. Our method can process arbitrary videos and\nfully automatically recover the 9 DoF pose for each object appearing in it,\nthus aligning them in a common 3D coordinate frame. The core idea of our method\nis to integrate neural network predictions from individual frames with a\ntemporally global, multi-view constraint optimization formulation. This\nintegration process resolves the scale and depth ambiguities in the per-frame\npredictions, and generally improves the estimate of all pose parameters. By\nleveraging multi-view constraints, our method also resolves occlusions and\nhandles objects that are out of view in individual frames, thus reconstructing\nall objects into a single globally consistent CAD representation of the scene.\nIn comparison to the state-of-the-art single-frame method Mask2CAD that we\nbuild on, we achieve substantial improvements on the Scan2CAD dataset (from\n11.6% to 30.7% class average accuracy).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maninis_K/0/1/0/all/0/1\">Kevis-Kokitsi Maninis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Popov_S/0/1/0/all/0/1\">Stefan Popov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niessner_M/0/1/0/all/0/1\">Matthias Nie&#xdf;ner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrari_V/0/1/0/all/0/1\">Vittorio Ferrari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Polarization Guided Specular Reflection Separation. (arXiv:2103.11652v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.11652","description":"<p>Since specular reflection often exists in the real captured images and causes\ndeviation between the recorded color and intrinsic color, specular reflection\nseparation can bring advantages to multiple applications that require\nconsistent object surface appearance. However, due to the color of an object is\nsignificantly influenced by the color of the illumination, the existing\nresearches still suffer from the near-duplicate challenge, that is, the\nseparation becomes unstable when the illumination color is close to the surface\ncolor. In this paper, we derive a polarization guided model to incorporate the\npolarization information into a designed iteration optimization separation\nstrategy to separate the specular reflection. Based on the analysis of\npolarization, we propose a polarization guided model to generate a polarization\nchromaticity image, which is able to reveal the geometrical profile of the\ninput image in complex scenarios, such as diversity of illumination. The\npolarization chromaticity image can accurately cluster the pixels with similar\ndiffuse color. We further use the specular separation of all these clusters as\nan implicit prior to ensure that the diffuse components will not be mistakenly\nseparated as the specular components. With the polarization guided model, we\nreformulate the specular reflection separation into a unified optimization\nfunction which can be solved by the ADMM strategy. The specular reflection will\nbe detected and separated jointly by RGB and polarimetric information. Both\nqualitative and quantitative experimental results have shown that our method\ncan faithfully separate the specular reflection, especially in some challenging\nscenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wen_S/0/1/0/all/0/1\">Sijia Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yingqiang Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_F/0/1/0/all/0/1\">Feng Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Looking Beyond Two Frames: End-to-End Multi-Object Tracking Using Spatial and Temporal Transformers. (arXiv:2103.14829v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.14829","description":"<p>Tracking a time-varying indefinite number of objects in a video sequence over\ntime remains a challenge despite recent advances in the field. Ignoring\nlong-term temporal information, most existing approaches are not able to\nproperly handle multi-object tracking challenges such as occlusion. To address\nthese shortcomings, we present MO3TR: a truly end-to-end Transformer-based\nonline multi-object tracking (MOT) framework that learns to handle occlusions,\ntrack initiation and termination without the need for an explicit data\nassociation module or any heuristics/post-processing. MO3TR encodes object\ninteractions into long-term temporal embeddings using a combination of spatial\nand temporal Transformers, and recursively uses the information jointly with\nthe input data to estimate the states of all tracked objects over time. The\nspatial attention mechanism enables our framework to learn implicit\nrepresentations between all the objects and the objects to the measurements,\nwhile the temporal attention mechanism focuses on specific parts of past\ninformation, allowing our approach to resolve occlusions over multiple frames.\nOur experiments demonstrate the potential of this new approach, reaching new\nstate-of-the-art results on multiple MOT metrics for two popular multi-object\ntracking benchmarks. Our code will be made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_T/0/1/0/all/0/1\">Tianyu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hiller_M/0/1/0/all/0/1\">Markus Hiller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ehsanpour_M/0/1/0/all/0/1\">Mahsa Ehsanpour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_R/0/1/0/all/0/1\">Rongkai Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drummond_T/0/1/0/all/0/1\">Tom Drummond</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rezatofighi_H/0/1/0/all/0/1\">Hamid Rezatofighi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Hierarchical Approach to Remote Sensing Scene Classification. (arXiv:2103.15463v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.15463","description":"<p>Remote sensing scene classification deals with the problem of classifying\nland use/cover of a region from images. To predict the development and\nsocioeconomic structures of cities, the status of land use in regions is\ntracked by the national mapping agencies of countries. Many of these agencies\nuse land-use types that are arranged in multiple levels. In this paper, we\nexamined the efficiency of a hierarchically designed Convolutional Neural\nNetwork (CNN) based framework that is suitable for such arrangements. We use\nthe NWPU-RESISC45 dataset for our experiments and arranged this data set in a\ntwo-level nested hierarchy. Each node in the designed hierarchy is trained\nusing DenseNet-121 architectures. We provide detailed empirical analysis to\ncompare the performances of this hierarchical scheme and its non-hierarchical\ncounterpart, together with the individual model performances. We also evaluated\nthe performance of the hierarchical structure statistically to validate the\npresented empirical results. The results of our experiments show that although\nindividual classifiers for different sub-categories in the hierarchical scheme\nperform considerably well, the accumulation of the classification errors in the\ncascaded structure prevents its classification performance from exceeding that\nof the non-hierarchical deep model\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sen_O/0/1/0/all/0/1\">Ozlem Sen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keles_H/0/1/0/all/0/1\">Hacer Yalim Keles</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Category-Adaptive Domain Adaptation for Semantic Segmentation. (arXiv:2103.15467v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.15467","description":"<p>Unsupervised domain adaptation (UDA) becomes more and more popular in\ntackling real-world problems without ground truth of the target domain. Though\ntedious annotation work is not required, UDA unavoidably faces two problems: 1)\nhow to narrow the domain discrepancy to boost the transferring performance; 2)\nhow to improve pseudo annotation producing mechanism for self-supervised\nlearning (SSL). In this paper, we focus on UDA for semantic segmentation task.\nFirstly, we introduce adversarial learning into style gap bridging mechanism to\nkeep the style information from two domains in the similar space. Secondly, to\nkeep the balance of pseudo labels on each category, we propose a\ncategory-adaptive threshold mechanism to choose category-wise pseudo labels for\nSSL. The experiments are conducted using GTA5 as the source domain, Cityscapes\nas the target domain. The results show that our model outperforms the\nstate-of-the-arts with a noticeable gain on cross-domain adaptation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhiming Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yantian Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">Danlan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_N/0/1/0/all/0/1\">Ning Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jianhua Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GENESIS-V2: Inferring Unordered Object Representations without Iterative Refinement. (arXiv:2104.09958v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.09958","description":"<p>Advances in unsupervised learning of object-representations have culminated\nin the development of a broad range of methods for unsupervised object\nsegmentation and interpretable object-centric scene generation. These methods,\nhowever, are limited to simulated and real-world datasets with limited visual\ncomplexity. Moreover, object representations are often inferred using RNNs\nwhich do not scale well to large images or iterative refinement which avoids\nimposing an unnatural ordering on objects in an image but requires the a priori\ninitialisation of a fixed number of object representations. In contrast to\nestablished paradigms, this work proposes an embedding-based approach in which\nembeddings of pixels are clustered in a differentiable fashion using a\nstochastic stick-breaking process. Similar to iterative refinement, this\nclustering procedure also leads to randomly ordered object representations, but\nwithout the need of initialising a fixed number of clusters a priori. This is\nused to develop a new model, GENESIS-v2, which can infer a variable number of\nobject representations without using RNNs or iterative refinement. We show that\nGENESIS-v2 performs strongly in comparison to recent baselines in terms of\nunsupervised image segmentation and object-centric scene generation on\nestablished synthetic datasets as well as more complex real-world datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Engelcke_M/0/1/0/all/0/1\">Martin Engelcke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jones_O/0/1/0/all/0/1\">Oiwi Parker Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Posner_I/0/1/0/all/0/1\">Ingmar Posner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Heuristic Weakly Supervised 3D Human Pose Estimation in Novel Contexts without Any 3D Pose Ground Truth. (arXiv:2105.10996v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.10996","description":"<p>Monocular 3D human pose estimation from a single RGB image has received a lot\nattentions in the past few year. Pose inference models with competitive\nperformance however require supervision with 3D pose ground truth data or at\nleast known pose priors in their target domain. Yet, these data requirements in\nmany real-world applications with data collection constraints may not be\nattainable. In this paper, we present a heuristic weakly supervised human pose\n(HW-HuP) solution to estimate 3D human pose in contexts that no ground truth 3D\npose data is accessible, even for fine-tuning. HW-HuP learns partial pose\npriors from public 3D human pose datasets and uses easy-to-access observations\nfrom the target domain to iteratively estimate 3D human pose and shape in an\noptimization and regression hybrid cycle. In our design, depth data as an\nauxiliary information is employed as weak supervision during training, yet it\nis not needed for the inference. HW-HuP shows comparable performance on public\nbenchmarks to the state-of-the-art approaches which benefit from full 3D pose\nsupervision. In this paper, we focus on two practical applications of 3D pose\nestimation for individuals while in bed as well as infants, where no reliable\n3D pose data exists.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shuangjun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaofei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_N/0/1/0/all/0/1\">Nihang Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ostadabbas_S/0/1/0/all/0/1\">Sarah Ostadabbas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ZeroWaste Dataset: Towards Deformable Object Segmentation in Cluttered Scenes. (arXiv:2106.02740v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.02740","description":"<p>Less than 35% of recyclable waste is being actually recycled in the US, which\nleads to increased soil and sea pollution and is one of the major concerns of\nenvironmental researchers as well as the common public. At the heart of the\nproblem are the inefficiencies of the waste sorting process (separating paper,\nplastic, metal, glass, etc.) due to the extremely complex and cluttered nature\nof the waste stream. Recyclable waste detection poses a unique computer vision\nchallenge as it requires detection of highly deformable and often translucent\nobjects in cluttered scenes without the kind of context information usually\npresent in human-centric datasets. This challenging computer vision task\ncurrently lacks suitable datasets or methods in the available literature. In\nthis paper, we take a step towards computer-aided waste detection and present\nthe first in-the-wild industrial-grade waste detection and segmentation\ndataset, ZeroWaste. We believe that ZeroWaste will catalyze research in object\ndetection and semantic segmentation in extreme clutter as well as applications\nin the recycling domain. Our project page can be found at\n<a href=\"http://ai.bu.edu/zerowaste/.\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bashkirova_D/0/1/0/all/0/1\">Dina Bashkirova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdelfattah_M/0/1/0/all/0/1\">Mohamed Abdelfattah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Ziliang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akl_J/0/1/0/all/0/1\">James Akl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alladkani_F/0/1/0/all/0/1\">Fadi Alladkani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_P/0/1/0/all/0/1\">Ping Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ablavsky_V/0/1/0/all/0/1\">Vitaly Ablavsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calli_B/0/1/0/all/0/1\">Berk Calli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bargal_S/0/1/0/all/0/1\">Sarah Adel Bargal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1\">Kate Saenko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Stable Classifiers by Transferring Unstable Features. (arXiv:2106.07847v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.07847","description":"<p>While unbiased machine learning models are essential for many applications,\nbias is a human-defined concept that can vary across tasks. Given only\ninput-label pairs, algorithms may lack sufficient information to distinguish\nstable (causal) features from unstable (spurious) features. However, related\ntasks often share similar biases -- an observation we may leverage to develop\nstable classifiers in the transfer setting. In this work, we explicitly inform\nthe target classifier about unstable features in the source tasks.\nSpecifically, we derive a representation that encodes the unstable features by\ncontrasting different data environments in the source task. We achieve\nrobustness by clustering data of the target task according to this\nrepresentation and minimizing the worst-case risk across these clusters. We\nevaluate our method on both text and image classifications. Empirical results\ndemonstrate that our algorithm is able to maintain robustness on the target\ntask for both synthetically generated environments and real-world environments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bao_Y/0/1/0/all/0/1\">Yujia Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shiyu Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barzilay_R/0/1/0/all/0/1\">Regina Barzilay</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Synthesis in Style: Semantic Segmentation of Historical Documents using Synthetic Data. (arXiv:2107.06777v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.06777","description":"<p>One of the most pressing problems in the automated analysis of historical\ndocuments is the availability of annotated training data. The problem is that\nlabeling samples is a time-consuming task because it requires human expertise\nand thus, cannot be automated well. In this work, we propose a novel method to\nconstruct synthetic labeled datasets for historical documents where no\nannotations are available. We train a StyleGAN model to synthesize document\nimages that capture the core features of the original documents. While\noriginally, the StyleGAN architecture was not intended to produce labels, it\nindirectly learns the underlying semantics to generate realistic images. Using\nour approach, we can extract the semantic information from the intermediate\nfeature maps and use it to generate ground truth labels. To investigate if our\nsynthetic dataset can be used to segment the text in historical documents, we\nuse it to train multiple supervised segmentation models and evaluate their\nperformance. We also train these models on another dataset created by a\nstate-of-the-art synthesis approach to show that the models trained on our\ndataset achieve better results while requiring even less human annotation\neffort.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bartz_C/0/1/0/all/0/1\">Christian Bartz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ratz_H/0/1/0/all/0/1\">Hendrik R&#xe4;tz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Otholt_J/0/1/0/all/0/1\">Jona Otholt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meinel_C/0/1/0/all/0/1\">Christoph Meinel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haojin Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Applications of Artificial Neural Networks in Microorganism Image Analysis: A Comprehensive Review from Conventional Multilayer Perceptron to Popular Convolutional Neural Network and Potential Visual Transformer. (arXiv:2108.00358v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.00358","description":"<p>Microorganisms are widely distributed in the human daily living environment.\nThey play an essential role in environmental pollution control, disease\nprevention and treatment, and food and drug production. The analysis of\nmicroorganisms is the basic step for make full use of different microorganisms.\nThe conventional analysis methods are laborious and time-consuming. Therefore,\nthe automatic image analysis based on artificial neural networks is introduced\nto optimize it. However, the automatic microorganism image analysis faces many\nchallenges, such as the requirement of robust algorithm caused by various\napplication occasions, insignificant features and easy undersegmentation caused\nby the image characteristic, and various analysis tasks. Therefore, we conduct\nthis review to comprehensively discuss the characteristics of microorganism\nimage analysis based on artificial neural networks. In this review, the\nbackground and motivation are introduced first. Then, the development of\nartificial neural networks and representative networks are presented. After\nthat, the papers related to microorganism image analysis based on classical and\ndeep neural networks are reviewed from the perspectives of different tasks. In\nthe end, the methodology analysis and potential direction are discussed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jinghua Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1\">Yimin Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiawei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grzegorzek_M/0/1/0/all/0/1\">Marcin Grzegorzek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Active Observer Visual Problem-Solving Methods are Dynamically Hypothesized, Deployed and Tested. (arXiv:2108.08145v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.08145","description":"<p>The STAR architecture was designed to test the value of the full Selective\nTuning model of visual attention for complex real-world visuospatial tasks and\nbehaviors. However, knowledge of how humans solve such tasks in 3D as active\nobservers is lean. We thus devised a novel experimental setup and examined such\nbehavior. We discovered that humans exhibit a variety of problem-solving\nstrategies whose breadth and complexity are surprising and not easily handled\nby current methodologies. It is apparent that solution methods are dynamically\ncomposed by hypothesizing sequences of actions, testing them, and if they fail,\ntrying different ones. The importance of active observation is striking as is\nthe lack of any learning effect. These results inform our Cognitive Program\nrepresentation of STAR extending its relevance to real-world tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Solbach_M/0/1/0/all/0/1\">Markus D. Solbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsotsos_J/0/1/0/all/0/1\">John K. Tsotsos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UCTransNet: Rethinking the Skip Connections in U-Net from a Channel-wise Perspective with Transformer. (arXiv:2109.04335v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.04335","description":"<p>Most recent semantic segmentation methods adopt a U-Net framework with an\nencoder-decoder architecture. It is still challenging for U-Net with a simple\nskip connection scheme to model the global multi-scale context: 1) Not each\nskip connection setting is effective due to the issue of incompatible feature\nsets of encoder and decoder stage, even some skip connection negatively\ninfluence the segmentation performance; 2) The original U-Net is worse than the\none without any skip connection on some datasets. Based on our findings, we\npropose a new segmentation framework, named UCTransNet (with a proposed CTrans\nmodule in U-Net), from the channel perspective with attention mechanism.\nSpecifically, the CTrans module is an alternate of the U-Net skip connections,\nwhich consists of a sub-module to conduct the multi-scale Channel Cross fusion\nwith Transformer (named CCT) and a sub-module Channel-wise Cross-Attention\n(named CCA) to guide the fused multi-scale channel-wise information to\neffectively connect to the decoder features for eliminating the ambiguity.\nHence, the proposed connection consisting of the CCT and CCA is able to replace\nthe original skip connection to solve the semantic gaps for an accurate\nautomatic medical image segmentation. The experimental results suggest that our\nUCTransNet produces more precise segmentation performance and achieves\nconsistent improvements over the state-of-the-art for semantic segmentation\nacross different datasets and conventional architectures involving transformer\nor U-shaped framework. Code: https://github.com/McGregorWwww/UCTransNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haonan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_P/0/1/0/all/0/1\">Peng Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiaqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaiane_O/0/1/0/all/0/1\">Osmar R. Zaiane</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CAE-Transformer: Transformer-based Model to Predict Invasiveness of Lung Adenocarcinoma Subsolid Nodules from Non-thin Section 3D CT Scans. (arXiv:2110.08721v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2110.08721","description":"<p>Lung cancer is the leading cause of mortality from cancer worldwide and has\nvarious histologic types, among which Lung Adenocarcinoma (LUAC) has recently\nbeen the most prevalent one. The current approach to determine the invasiveness\nof LUACs is surgical resection, which is not a viable solution to fight lung\ncancer in a timely fashion. An alternative approach is to analyze chest\nComputed Tomography (CT) scans. The radiologists' analysis based on CT images,\nhowever, is subjective and might result in a low accuracy. In this paper, a\ntransformer-based framework, referred to as the \"CAE-Transformer\", is developed\nto efficiently classify LUACs using whole CT images instead of finely annotated\nnodules. The proposed CAE-Transformer can achieve high accuracy over a small\ndataset and requires minor supervision from radiologists. The CAE Transformer\nutilizes an encoder to automatically extract informative features from CT\nslices, which are then fed to a modified transformer to capture global\ninter-slice relations and provide classification labels. Experimental results\non our in-house dataset of 114 pathologically proven Sub-Solid Nodules (SSNs)\ndemonstrate the superiority of the CAE-Transformer over its counterparts,\nachieving an accuracy of 87.73%, sensitivity of 88.67%, specificity of 86.33%,\nand AUC of 0.913, using a 10-fold cross-validation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Heidarian_S/0/1/0/all/0/1\">Shahin Heidarian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Afshar_P/0/1/0/all/0/1\">Parnian Afshar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Oikonomou_A/0/1/0/all/0/1\">Anastasia Oikonomou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Plataniotis_K/0/1/0/all/0/1\">Konstantinos N. Plataniotis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mohammadi_A/0/1/0/all/0/1\">Arash Mohammadi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stochastic Primal-Dual Deep Unrolling. (arXiv:2110.10093v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2110.10093","description":"<p>We propose a new type of efficient deep-unrolling networks for solving\nimaging inverse problems. Conventional deep-unrolling methods require full\nforward operator and its adjoint across each layer, and hence can be\nsignificantly more expensive computationally as compared with other end-to-end\nmethods that are based on post-processing of model-based reconstructions,\nespecially for 3D image reconstruction tasks. We develop a stochastic\n(ordered-subsets) variant of the classical learned primal-dual (LPD), which is\na state-of-the-art unrolling network for tomographic image reconstruction. The\nproposed learned stochastic primal-dual (LSPD) network only uses subsets of the\nforward and adjoint operators and offers considerable computational efficiency.\nWe provide theoretical analysis of a special case of our LSPD framework,\nsuggesting that it has the potential to achieve image reconstruction quality\ncompetitive with the full-batch LPD while requiring only a fraction of the\ncomputation. The numerical results for two different X-ray computed tomography\n(CT) imaging tasks (namely, low-dose and sparse-view CT) corroborate this\ntheoretical finding, demonstrating the promise of LSPD networks for large-scale\nimaging problems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Tang_J/0/1/0/all/0/1\">Junqi Tang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mukherjee_S/0/1/0/all/0/1\">Subhadip Mukherjee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schonlieb_C/0/1/0/all/0/1\">Carola-Bibiane Sch&#xf6;nlieb</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Event-based Spatio-Temporal Feature Descriptors via Local Synaptic Plasticity: A Biologically-Plausible Perspective of Computer Vision. (arXiv:2111.00791v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.00791","description":"<p>We present an optimization-based theory describing spiking cortical ensembles\nequipped with Spike-Timing-Dependent Plasticity (STDP) learning, as empirically\nobserved in the visual cortex. Using our methods, we build a class of\nfully-connected, convolutional and action-based feature descriptors for\nevent-based camera that we respectively assess on N-MNIST, challenging\nCIFAR10-DVS and on the IBM DVS128 gesture dataset. We report significant\naccuracy improvements compared to conventional state-of-the-art event-based\nfeature descriptors (+8% on CIFAR10-DVS). We report large improvements in\naccuracy compared to state-of-the-art STDP-based systems (+10% on N-MNIST,\n+7.74% on IBM DVS128 Gesture). In addition to ultra-low-power learning in\nneuromorphic edge devices, our work helps paving the way towards a\nbiologically-realistic, optimization-based theory of cortical vision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Safa_A/0/1/0/all/0/1\">Ali Safa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahli_H/0/1/0/all/0/1\">Hichem Sahli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bourdoux_A/0/1/0/all/0/1\">Andr&#xe9; Bourdoux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ocket_I/0/1/0/all/0/1\">Ilja Ocket</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Catthoor_F/0/1/0/all/0/1\">Francky Catthoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gielen_G/0/1/0/all/0/1\">Georges Gielen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MQBench: Towards Reproducible and Deployable Model Quantization Benchmark. (arXiv:2111.03759v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2111.03759","description":"<p>Model quantization has emerged as an indispensable technique to accelerate\ndeep learning inference. While researchers continue to push the frontier of\nquantization algorithms, existing quantization work is often unreproducible and\nundeployable. This is because researchers do not choose consistent training\npipelines and ignore the requirements for hardware deployments. In this work,\nwe propose Model Quantization Benchmark (MQBench), a first attempt to evaluate,\nanalyze, and benchmark the reproducibility and deployability for model\nquantization algorithms. We choose multiple different platforms for real-world\ndeployments, including CPU, GPU, ASIC, DSP, and evaluate extensive\nstate-of-the-art quantization algorithms under a unified training pipeline.\nMQBench acts like a bridge to connect the algorithm and the hardware. We\nconduct a comprehensive analysis and find considerable intuitive or\ncounter-intuitive insights. By aligning the training settings, we find existing\nalgorithms have about the same performance on the conventional academic track.\nWhile for the hardware-deployable quantization, there is a huge accuracy gap\nwhich remains unsettled. Surprisingly, no existing algorithm wins every\nchallenge in MQBench, and we hope this work could inspire future research\ndirections.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuhang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_M/0/1/0/all/0/1\">Mingzhu Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jian Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Yan Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1\">Mingxin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_R/0/1/0/all/0/1\">Ruihao Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fengwei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Junjie Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic tumour segmentation in H&E-stained whole-slide images of the pancreas. (arXiv:2112.01533v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2112.01533","description":"<p>Pancreatic cancer will soon be the second leading cause of cancer-related\ndeath in Western society. Imaging techniques such as CT, MRI and ultrasound\ntypically help providing the initial diagnosis, but histopathological\nassessment is still the gold standard for final confirmation of disease\npresence and prognosis. In recent years machine learning approaches and\npathomics pipelines have shown potential in improving diagnostics and\nprognostics in other cancerous entities, such as breast and prostate cancer. A\ncrucial first step in these pipelines is typically identification and\nsegmentation of the tumour area. Ideally this step is done automatically to\nprevent time consuming manual annotation. We propose a multi-task convolutional\nneural network to balance disease detection and segmentation accuracy. We\nvalidated our approach on a dataset of 29 patients (for a total of 58 slides)\nat different resolutions. The best single task segmentation network achieved a\nmedian Dice of 0.885 (0.122) IQR at a resolution of 15.56 $\\mu$m. Our\nmulti-task network improved on that with a median Dice score of 0.934 (0.077)\nIQR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Vendittelli_P/0/1/0/all/0/1\">Pierpaolo Vendittelli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Smeets_E/0/1/0/all/0/1\">Esther M.M. Smeets</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Litjens_G/0/1/0/all/0/1\">Geert Litjens</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COROLLA: An Efficient Multi-Modality Fusion Framework with Supervised Contrastive Learning for Glaucoma Grading. (arXiv:2201.03795v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2201.03795","description":"<p>Glaucoma is one of the ophthalmic diseases that may cause blindness, for\nwhich early detection and treatment are very important. Fundus images and\noptical coherence tomography (OCT) images are both widely-used modalities in\ndiagnosing glaucoma. However, existing glaucoma grading approaches mainly\nutilize a single modality, ignoring the complementary information between\nfundus and OCT. In this paper, we propose an efficient multi-modality\nsupervised contrastive learning framework, named COROLLA, for glaucoma grading.\nThrough layer segmentation as well as thickness calculation and projection,\nretinal thickness maps are extracted from the original OCT volumes and used as\na replacing modality, resulting in more efficient calculations with less memory\nusage. Given the high structure and distribution similarities across medical\nimage samples, we employ supervised contrastive learning to increase our\nmodels' discriminative power with better convergence. Moreover, feature-level\nfusion of paired fundus image and thickness map is conducted for enhanced\ndiagnosis accuracy. On the GAMMA dataset, our COROLLA framework achieves\noverwhelming glaucoma grading performance compared to state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Cai_Z/0/1/0/all/0/1\">Zhiyuan Cai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_L/0/1/0/all/0/1\">Li Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+He_H/0/1/0/all/0/1\">Huaqing He</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tang_X/0/1/0/all/0/1\">Xiaoying Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Lightweight Neural Animation : Exploration of Neural Network Pruning in Mixture of Experts-based Animation Models. (arXiv:2201.04042v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.04042","description":"<p>In the past few years, neural character animation has emerged and offered an\nautomatic method for animating virtual characters. Their motion is synthesized\nby a neural network. Controlling this movement in real time with a user-defined\ncontrol signal is also an important task in video games for example. Solutions\nbased on fully-connected layers (MLPs) and Mixture-of-Experts (MoE) have given\nimpressive results in generating and controlling various movements with\nclose-range interactions between the environment and the virtual character.\nHowever, a major shortcoming of fully-connected layers is their computational\nand memory cost which may lead to sub-optimized solution. In this work, we\napply pruning algorithms to compress an MLP- MoE neural network in the context\nof interactive character animation, which reduces its number of parameters and\naccelerates its computation time with a trade-off between this acceleration and\nthe synthesized motion quality. This work demonstrates that, with the same\nnumber of experts and parameters, the pruned model produces less motion\nartifacts than the dense model and the learned high-level motion features are\nsimilar for both\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maiorca_A/0/1/0/all/0/1\">Antoine Maiorca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hubens_N/0/1/0/all/0/1\">Nathan Hubens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laraba_S/0/1/0/all/0/1\">Sohaib Laraba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dutoit_T/0/1/0/all/0/1\">Thierry Dutoit</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth. (arXiv:2201.07436v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.07436","description":"<p>Depth estimation from a single image is an important task that can be applied\nto various fields in computer vision, and has grown rapidly with the\ndevelopment of convolutional neural networks. In this paper, we propose a novel\nstructure and training strategy for monocular depth estimation to further\nimprove the prediction accuracy of the network. We deploy a hierarchical\ntransformer encoder to capture and convey the global context, and design a\nlightweight yet powerful decoder to generate an estimated depth map while\nconsidering local connectivity. By constructing connected paths between\nmulti-scale local features and the global decoding stream with our proposed\nselective feature fusion module, the network can integrate both representations\nand recover fine details. In addition, the proposed decoder shows better\nperformance than the previously proposed decoders, with considerably less\ncomputational complexity. Furthermore, we improve the depth-specific\naugmentation method by utilizing an important observation in depth estimation\nto enhance the model. Our network achieves state-of-the-art performance over\nthe challenging depth dataset NYU Depth V2. Extensive experiments have been\nconducted to validate and show the effectiveness of the proposed approach.\nFinally, our model shows better generalisation ability and robustness than\nother comparative models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Doyeon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ga_W/0/1/0/all/0/1\">Woonghyun Ga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahn_P/0/1/0/all/0/1\">Pyungwhan Ahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joo_D/0/1/0/all/0/1\">Donggyu Joo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chun_S/0/1/0/all/0/1\">Sehwan Chun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Junmo Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DCNGAN: A Deformable Convolutional-Based GAN with QP Adaptation for Perceptual Quality Enhancement of Compressed Video. (arXiv:2201.08944v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2201.08944","description":"<p>In this paper, we propose a deformable convolution-based generative\nadversarial network (DCNGAN) for perceptual quality enhancement of compressed\nvideos. DCNGAN is also adaptive to the quantization parameters (QPs). Compared\nwith optical flows, deformable convolutions are more effective and efficient to\nalign frames. Deformable convolutions can operate on multiple frames, thus\nleveraging more temporal information, which is beneficial for enhancing the\nperceptual quality of compressed videos. Instead of aligning frames in a\npairwise manner, the deformable convolution can process multiple frames\nsimultaneously, which leads to lower computational complexity. Experimental\nresults demonstrate that the proposed DCNGAN outperforms other state-of-the-art\ncompressed video quality enhancement algorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_S/0/1/0/all/0/1\">Saiping Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Herranz_L/0/1/0/all/0/1\">Luis Herranz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mrak_M/0/1/0/all/0/1\">Marta Mrak</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Blanch_M/0/1/0/all/0/1\">Marc Gorriz Blanch</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wan_S/0/1/0/all/0/1\">Shuai Wan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_F/0/1/0/all/0/1\">Fuzheng Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Semantics for Visual Place Recognition through Multi-Scale Attention. (arXiv:2201.09701v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.09701","description":"<p>In this paper we address the task of visual place recognition (VPR), where\nthe goal is to retrieve the correct GPS coordinates of a given query image\nagainst a huge geotagged gallery. While recent works have shown that building\ndescriptors incorporating semantic and appearance information is beneficial,\ncurrent state-of-the-art methods opt for a top down definition of the\nsignificant semantic content. Here we present the first VPR algorithm that\nlearns robust global embeddings from both visual appearance and semantic\ncontent of the data, with the segmentation process being dynamically guided by\nthe recognition of places through a multi-scale attention module. Experiments\non various scenarios validate this new approach and demonstrate its performance\nagainst state-of-the-art methods. Finally, we propose the first synthetic-world\ndataset suited for both place recognition and segmentation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Paolicelli_V/0/1/0/all/0/1\">Valerio Paolicelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tavera_A/0/1/0/all/0/1\">Antonio Tavera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masone_C/0/1/0/all/0/1\">Carlo Masone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berton_G/0/1/0/all/0/1\">Gabriele Berton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caputo_B/0/1/0/all/0/1\">Barbara Caputo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Symmetry Perception by Deep Networks: Inadequacy of Feed-Forward Architectures and Improvements with Recurrent Connections. (arXiv:2112.04162v2 [cs.CV] CROSS LISTED)","link":"http://arxiv.org/abs/2112.04162","description":"<p>Symmetry is omnipresent in nature and perceived by the visual system of many\nspecies, as it facilitates detecting ecologically important classes of objects\nin our environment. Symmetry perception requires abstraction of long-range\nspatial dependencies between image regions, and its underlying neural\nmechanisms remain elusive. In this paper, we evaluate Deep Neural Network (DNN)\narchitectures on the task of learning symmetry perception from examples. We\ndemonstrate that feed-forward DNNs that excel at modelling human performance on\nobject recognition tasks, are unable to acquire a general notion of symmetry.\nThis is the case even when the DNNs are architected to capture long-range\nspatial dependencies, such as through `dilated' convolutions and the recently\nintroduced `transformers' design. By contrast, we find that recurrent\narchitectures are capable of learning to perceive symmetry by decomposing the\nlong-range spatial dependencies into a sequence of local operations, that are\nreusable for novel images. These results suggest that recurrent connections\nlikely play an important role in symmetry perception in artificial systems, and\npossibly, biological ones too.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sundaram_S/0/1/0/all/0/1\">Shobhita Sundaram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sinha_D/0/1/0/all/0/1\">Darius Sinha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Groth_M/0/1/0/all/0/1\">Matthew Groth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sasaki_T/0/1/0/all/0/1\">Tomotake Sasaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boix_X/0/1/0/all/0/1\">Xavier Boix</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-01-25T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/"}}]}]}