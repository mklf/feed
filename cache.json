{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-07-06T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"A Cascade Model for Argument Mining in Japanese Political Discussions: the QA Lab-PoliInfo-3 Case Study. (arXiv:2207.01672v1 [cs.CL])","link":"http://arxiv.org/abs/2207.01672","description":"<p>The rVRAIN team tackled the Budget Argument Mining (BAM) task, consisting of\na combination of classification and information retrieval sub-tasks. For the\nargument classification (AC), the team achieved its best performing results\nwith a five-class BERT-based cascade model complemented with some handcrafted\nrules. The rules were used to determine if the expression was monetary or not.\nThen, each monetary expression was classified as a premise or as a conclusion\nin the first level of the cascade model. Finally, each premise was classified\ninto the three premise classes, and each conclusion into the two conclusion\nclasses. For the information retrieval (i.e., relation ID detection or RID),\nour best results were achieved by a combination of a BERT-based binary\nclassifier, and the cosine similarity of pairs consisting of the monetary\nexpression and budget dense embeddings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ruiz_Dolz_R/0/1/0/all/0/1\">Ramon Ruiz-Dolz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Location reference recognition from texts: A survey and comparison. (arXiv:2207.01683v1 [cs.CL])","link":"http://arxiv.org/abs/2207.01683","description":"<p>A vast amount of location information exists in unstructured texts, such as\nsocial media posts, news stories, scientific articles, web pages, travel blogs,\nand historical archives. Geoparsing refers to the process of recognizing\nlocation references from texts and identifying their geospatial\nrepresentations. While geoparsing can benefit many domains, a summary of the\nspecific applications is still missing. Further, there lacks a comprehensive\nreview and comparison of existing approaches for location reference\nrecognition, which is the first and a core step of geoparsing. To fill these\nresearch gaps, this review first summarizes seven typical application domains\nof geoparsing: geographic information retrieval, disaster management, disease\nsurveillance, traffic management, spatial humanities, tourism management, and\ncrime management. We then review existing approaches for location reference\nrecognition by categorizing these approaches into four groups based on their\nunderlying functional principle: rule-based, gazetteer matching-based,\nstatistical learning-based, and hybrid approaches. Next, we thoroughly evaluate\nthe correctness and computational efficiency of the 27 most widely used\napproaches for location reference recognition based on 26 public datasets with\ndifferent types of texts (e.g., social media posts and news stories) containing\n39,736 location references across the world. Results from this thorough\nevaluation can help inform future methodological developments for location\nreference recognition, and can help guide the selection of proper approaches\nbased on application needs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xuke Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhiyong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yingjie Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_F/0/1/0/all/0/1\">Fuqiang Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kersten_J/0/1/0/all/0/1\">Jens Kersten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1\">Hongchao Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klan_F/0/1/0/all/0/1\">Friederike Klan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Disentangled Action Recognition with Knowledge Bases. (arXiv:2207.01708v1 [cs.CV])","link":"http://arxiv.org/abs/2207.01708","description":"<p>Action in video usually involves the interaction of human with objects.\nAction labels are typically composed of various combinations of verbs and\nnouns, but we may not have training data for all possible combinations. In this\npaper, we aim to improve the generalization ability of the compositional action\nrecognition model to novel verbs or novel nouns that are unseen during training\ntime, by leveraging the power of knowledge graphs. Previous work utilizes\nverb-noun compositional action nodes in the knowledge graph, making it\ninefficient to scale since the number of compositional action nodes grows\nquadratically with respect to the number of verbs and nouns. To address this\nissue, we propose our approach: Disentangled Action Recognition with\nKnowledge-bases (DARK), which leverages the inherent compositionality of\nactions. DARK trains a factorized model by first extracting disentangled\nfeature representations for verbs and nouns, and then predicting classification\nweights using relations in external knowledge graphs. The type constraint\nbetween verb and noun is extracted from external knowledge bases and finally\napplied when composing actions. DARK has better scalability in the number of\nobjects and verbs, and achieves state-of-the-art performance on the Charades\ndataset. We further propose a new benchmark split based on the Epic-kitchen\ndataset which is an order of magnitude bigger in the numbers of classes and\nsamples, and benchmark various models on this benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Zhekun Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Shalini Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guillory_D/0/1/0/all/0/1\">Devin Guillory</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kato_K/0/1/0/all/0/1\">Keizo Kato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1\">Trevor Darrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Huijuan Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BERT, can HE predict contrastive focus? Predicting and controlling prominence in neural TTS using a language model. (arXiv:2207.01718v1 [cs.CL])","link":"http://arxiv.org/abs/2207.01718","description":"<p>Several recent studies have tested the use of transformer language model\nrepresentations to infer prosodic features for text-to-speech synthesis (TTS).\nWhile these studies have explored prosody in general, in this work, we look\nspecifically at the prediction of contrastive focus on personal pronouns. This\nis a particularly challenging task as it often requires semantic, discursive\nand/or pragmatic knowledge to predict correctly. We collect a corpus of\nutterances containing contrastive focus and we evaluate the accuracy of a BERT\nmodel, finetuned to predict quantized acoustic prominence features, on these\nsamples. We also investigate how past utterances can provide relevant\ninformation for this prediction. Furthermore, we evaluate the controllability\nof pronoun prominence in a TTS model conditioned on acoustic prominence\nfeatures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stephenson_B/0/1/0/all/0/1\">Brooke Stephenson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Besacier_L/0/1/0/all/0/1\">Laurent Besacier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Girin_L/0/1/0/all/0/1\">Laurent Girin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hueber_T/0/1/0/all/0/1\">Thomas Hueber</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Probing via Prompting. (arXiv:2207.01736v1 [cs.CL])","link":"http://arxiv.org/abs/2207.01736","description":"<p>Probing is a popular method to discern what linguistic information is\ncontained in the representations of pre-trained language models. However, the\nmechanism of selecting the probe model has recently been subject to intense\ndebate, as it is not clear if the probes are merely extracting information or\nmodeling the linguistic property themselves. To address this challenge, this\npaper introduces a novel model-free approach to probing, by formulating probing\nas a prompting task. We conduct experiments on five probing tasks and show that\nour approach is comparable or better at extracting information than diagnostic\nprobes while learning much less on its own. We further combine the probing via\nprompting approach with attention head pruning to analyze where the model\nstores the linguistic information in its architecture. We then examine the\nusefulness of a specific linguistic property for pre-training by removing the\nheads that are essential to that property and evaluating the resulting model's\nperformance on language modeling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiaoda Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PReGAN: Answer Oriented Passage Ranking with Weakly Supervised GAN. (arXiv:2207.01762v1 [cs.CL])","link":"http://arxiv.org/abs/2207.01762","description":"<p>Beyond topical relevance, passage ranking for open-domain factoid question\nanswering also requires a passage to contain an answer (answerability). While a\nfew recent studies have incorporated some reading capability into a ranker to\naccount for answerability, the ranker is still hindered by the noisy nature of\nthe training data typically available in this area, which considers any passage\ncontaining an answer entity as a positive sample. However, the answer entity in\na passage is not necessarily mentioned in relation with the given question. To\naddress the problem, we propose an approach called \\ttt{PReGAN} for Passage\nReranking based on Generative Adversarial Neural networks, which incorporates a\ndiscriminator on answerability, in addition to a discriminator on topical\nrelevance. The goal is to force the generator to rank higher a passage that is\ntopically relevant and contains an answer. Experiments on five public datasets\nshow that \\ttt{PReGAN} can better rank appropriate passages, which in turn,\nboosts the effectiveness of QA systems, and outperforms the existing approaches\nwithout using external data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_P/0/1/0/all/0/1\">Pan Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_J/0/1/0/all/0/1\">Jian-Yun Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yutao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Hao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_L/0/1/0/all/0/1\">Lixin Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1\">Xiaohui Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision-and-Language Pretraining. (arXiv:2207.01772v1 [cs.CL])","link":"http://arxiv.org/abs/2207.01772","description":"<p>With the burgeoning amount of data of image-text pairs and diversity of\nVision-and-Language (V&amp;L) tasks, scholars have introduced an abundance of deep\nlearning models in this research domain. Furthermore, in recent years, transfer\nlearning has also shown tremendous success in Computer Vision for tasks such as\nImage Classification, Object Detection, etc., and in Natural Language\nProcessing for Question Answering, Machine Translation, etc. Inheriting the\nspirit of Transfer Learning, research works in V&amp;L have devised multiple\npretraining techniques on large-scale datasets in order to enhance the\nperformance of downstream tasks. The aim of this article is to provide a\ncomprehensive revision of contemporary V&amp;L pretraining models. In particular,\nwe categorize and delineate pretraining approaches, along with the summary of\nstate-of-the-art vision-and-language pre-trained models. Moreover, a list of\ntraining datasets and downstream tasks is supplied to further polish the\nperspective on V&amp;L pretraining. Lastly, we decided to take a further step to\ndiscuss numerous directions for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Thong Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1\">Cong-Duy Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiaobao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luu_A/0/1/0/all/0/1\">Anh Tuan Luu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning. (arXiv:2207.01780v1 [cs.LG])","link":"http://arxiv.org/abs/2207.01780","description":"<p>Program synthesis or code generation aims to generate a program that\nsatisfies a problem specification. Recent approaches using large-scale\npretrained language models (LMs) have shown promising results, yet they have\nsome critical limitations. In particular, they often follow a standard\nsupervised fine-tuning procedure to train a code generation model only from the\npairs of natural-language problem descriptions and ground-truth programs. Such\nparadigm largely ignores some important but potentially useful signals in the\nproblem specification such as unit tests, which thus often results in poor\nperformance when solving complex unseen coding tasks. To address the\nlimitations, we propose \"CodeRL\", a new framework for program synthesis tasks\nthrough pretrained LMs and deep reinforcement learning (RL). Specifically,\nduring training, we treat the code-generating LM as an actor network, and\nintroduce a critic network that is trained to predict the functional\ncorrectness of generated programs and provide dense feedback signals to the\nactor. During inference, we introduce a new generation procedure with a\ncritical sampling strategy that allows a model to automatically regenerate\nprograms based on feedback from example unit tests and critic scores. For the\nmodel backbones, we extended the encoder-decoder architecture of CodeT5 with\nenhanced learning objectives, larger model sizes, and better pretraining data.\nOur method not only achieves new SOTA results on the challenging APPS\nbenchmark, but also shows strong zero-shot transfer capability with new SOTA\nresults on the simpler MBPP benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1\">Hung Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gotmare_A/0/1/0/all/0/1\">Akhilesh Deepak Gotmare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1\">Silvio Savarese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoi_S/0/1/0/all/0/1\">Steven C.H. Hoi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scene-Aware Prompt for Multi-modal Dialogue Understanding and Generation. (arXiv:2207.01823v1 [cs.CL])","link":"http://arxiv.org/abs/2207.01823","description":"<p>This paper introduces the schemes of Team LingJing's experiments in\nNLPCC-2022-Shared-Task-4 Multi-modal Dialogue Understanding and Generation\n(MDUG). The MDUG task can be divided into two phases: multi-modal context\nunderstanding and response generation. To fully leverage the visual information\nfor both scene understanding and dialogue generation, we propose the\nscene-aware prompt for the MDUG task. Specifically, we utilize the\nmulti-tasking strategy for jointly modelling the scene- and session-\nmulti-modal understanding. The visual captions are adopted to aware the scene\ninformation, while the fixed-type templated prompt based on the scene- and\nsession-aware labels are used to further improve the dialogue generation\nperformance. Extensive experimental results show that the proposed method has\nachieved state-of-the-art (SOTA) performance compared with other competitive\nmethods, where we rank the 1-st in all three subtasks in this MDUG competition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weng_Y/0/1/0/all/0/1\">Yixuan Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Ziyu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1\">Bin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shutao Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Keyword Extraction in Scientific Documents. (arXiv:2207.01888v1 [cs.CL])","link":"http://arxiv.org/abs/2207.01888","description":"<p>The scientific publication output grows exponentially. Therefore, it is\nincreasingly challenging to keep track of trends and changes. Understanding\nscientific documents is an important step in downstream tasks such as knowledge\ngraph building, text mining, and discipline classification. In this workshop,\nwe provide a better understanding of keyword and keyphrase extraction from the\nabstract of scientific publications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rao_S/0/1/0/all/0/1\">Susie Xi Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piriyatamwong_P/0/1/0/all/0/1\">Piriyakorn Piriyatamwong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghoshal_P/0/1/0/all/0/1\">Parijat Ghoshal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasirian_S/0/1/0/all/0/1\">Sara Nasirian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salis_E/0/1/0/all/0/1\">Emmanuel de Salis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitrovic_S/0/1/0/all/0/1\">Sandra Mitrovi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wechner_M/0/1/0/all/0/1\">Michael Wechner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brucker_V/0/1/0/all/0/1\">Vanya Brucker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Egger_P/0/1/0/all/0/1\">Peter Egger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Ce Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ASR-Generated Text for Language Model Pre-training Applied to Speech Tasks. (arXiv:2207.01893v1 [cs.CL])","link":"http://arxiv.org/abs/2207.01893","description":"<p>We aim at improving spoken language modeling (LM) using very large amount of\nautomatically transcribed speech. We leverage the INA (French National\nAudiovisual Institute) collection and obtain 19GB of text after applying ASR on\n350,000 hours of diverse TV shows. From this, spoken language models are\ntrained either by fine-tuning an existing LM (FlauBERT) or through training a\nLM from scratch. New models (FlauBERT-Oral) are shared with the community and\nevaluated for 3 downstream tasks: spoken language understanding, classification\nof TV shows and speech syntactic parsing. Results show that FlauBERT-Oral can\nbe beneficial compared to its initial FlauBERT version demonstrating that,\ndespite its inherent noisy nature, ASR-generated text can be used to build\nspoken language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pelloin_V/0/1/0/all/0/1\">Valentin Pelloin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dary_F/0/1/0/all/0/1\">Franck Dary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herve_N/0/1/0/all/0/1\">Nicolas Herve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Favre_B/0/1/0/all/0/1\">Benoit Favre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camelin_N/0/1/0/all/0/1\">Nathalie Camelin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laurent_A/0/1/0/all/0/1\">Antoine Laurent</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Besacier_L/0/1/0/all/0/1\">Laurent Besacier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Betti numbers of attention graphs is all you really need. (arXiv:2207.01903v1 [cs.CL])","link":"http://arxiv.org/abs/2207.01903","description":"<p>We apply methods of topological analysis to the attention graphs, calculated\non the attention heads of the BERT model ( <a href=\"/abs/1810.04805\">arXiv:1810.04805v2</a> ). Our research\nshows that the classifier built upon basic persistent topological features\n(namely, Betti numbers) of the trained neural network can achieve\nclassification results on par with the conventional classification method. We\nshow the relevance of such topological text representation on three text\nclassification benchmarks. For the best of our knowledge, it is the first\nattempt to analyze the topology of an attention-based neural network, widely\nused for Natural Language Processing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kushnareva_L/0/1/0/all/0/1\">Laida Kushnareva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piontkovski_D/0/1/0/all/0/1\">Dmitri Piontkovski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piontkovskaya_I/0/1/0/all/0/1\">Irina Piontkovskaya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Lingual QA as a Stepping Stone for Monolingual Open QA in Icelandic. (arXiv:2207.01918v1 [cs.CL])","link":"http://arxiv.org/abs/2207.01918","description":"<p>It can be challenging to build effective open question answering (open QA)\nsystems for languages other than English, mainly due to a lack of labeled data\nfor training. We present a data efficient method to bootstrap such a system for\nlanguages other than English. Our approach requires only limited QA resources\nin the given language, along with machine-translated data, and at least a\nbilingual language model. To evaluate our approach, we build such a system for\nthe Icelandic language and evaluate performance over trivia style datasets. The\ncorpora used for training are English in origin but machine translated into\nIcelandic. We train a bilingual Icelandic/English language model to embed\nEnglish context and Icelandic questions following methodology introduced with\nDensePhrases (Lee et al., 2021). The resulting system is an open domain\ncross-lingual QA system between Icelandic and English. Finally, the system is\nadapted for Icelandic only open QA, demonstrating how it is possible to\nefficiently create an open QA system with limited access to curated datasets in\nthe language of interest.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Snaebjarnarson_V/0/1/0/all/0/1\">V&#xe9;steinn Sn&#xe6;bjarnarson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Einarsson_H/0/1/0/all/0/1\">Hafsteinn Einarsson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Entity Linking in Tabular Data Needs the Right Attention. (arXiv:2207.01937v1 [cs.CL])","link":"http://arxiv.org/abs/2207.01937","description":"<p>Understanding the semantic meaning of tabular data requires Entity Linking\n(EL), in order to associate each cell value to a real-world entity in a\nKnowledge Base (KB). In this work, we focus on end-to-end solutions for EL on\ntabular data that do not rely on fact lookup in the target KB. Tabular data\ncontains heterogeneous and sparse context, including column headers, cell\nvalues and table captions. We experiment with various models to generate a\nvector representation for each cell value to be linked. Our results show that\nit is critical to apply an attention mechanism as well as an attention mask, so\nthat the model can only attend to the most relevant context and avoid\ninformation dilution. The most relevant context includes: same-row cells,\nsame-column cells, headers and caption. Computational complexity, however,\ngrows quadratically with the size of tabular data for such a complex model. We\nachieve constant memory usage by introducing a Tabular Entity Linking Lite\nmodel (TELL ) that generates vector representation for a cell based only on its\nvalue, the table headers and the table caption. TELL achieves 80.8% accuracy on\nWikipedia tables, which is only 0.1% lower than the state-of-the-art model with\nquadratic memory usage.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Katsakioris_M/0/1/0/all/0/1\">Miltiadis Marios Katsakioris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yiwei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masato_D/0/1/0/all/0/1\">Daniele Masato</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MIA 2022 Shared Task Submission: Leveraging Entity Representations, Dense-Sparse Hybrids, and Fusion-in-Decoder for Cross-Lingual Question Answering. (arXiv:2207.01940v1 [cs.CL])","link":"http://arxiv.org/abs/2207.01940","description":"<p>We describe our two-stage system for the Multi-lingual Information Access\n(MIA) 2022 Shared Task on Cross-Lingual Open-Retrieval Question Answering. The\nfirst stage consists of multilingual passage retrieval with a hybrid dense and\nsparse retrieval strategy. The second stage consists of a reader which outputs\nthe answer from the top passages returned by the first stage. We show the\nefficacy of using entity representations, sparse retrieval signals to help\ndense retrieval, and Fusion-in-Decoder. On the development set, we obtain 43.46\nF1 on XOR-TyDi QA and 21.99 F1 on MKQA, for an average F1 score of 32.73. On\nthe test set, we obtain 40.93 F1 on XOR-TyDi QA and 22.29 F1 on MKQA, for an\naverage F1 score of 31.61. We improve over the official baseline by over 4 F1\npoints on both the development and test sets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhucheng Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padmanabhan_S/0/1/0/all/0/1\">Sarguna Janani Padmanabhan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Making sense of spoken plurals. (arXiv:2207.01947v1 [cs.CL])","link":"http://arxiv.org/abs/2207.01947","description":"<p>Distributional semantics offers new ways to study the semantics of\nmorphology. This study focuses on the semantics of noun singulars and their\nplural inflectional variants in English. Our goal is to compare two models for\nthe conceptualization of plurality. One model (FRACSS) proposes that all\nsingular-plural pairs should be taken into account when predicting plural\nsemantics from singular semantics. The other model (CCA) argues that\nconceptualization for plurality depends primarily on the semantic class of the\nbase word. We compare the two models on the basis of how well the speech signal\nof plural tokens in a large corpus of spoken American English aligns with the\nsemantic vectors predicted by the two models. Two measures are employed: the\nperformance of a form-to-meaning mapping and the correlations between form\ndistances and meaning distances. Results converge on a superior alignment for\nCCA. Our results suggest that usage-based approaches to pluralization in which\na given word's own semantic neighborhood is given priority outperform theories\naccording to which pluralization is conceptualized as a process building on\nhigh-level abstraction. We see that what has often been conceived of as a\nhighly abstract concept, [+plural], is better captured via a family of\nmid-level partial generalizations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shafaei_Bajestan_E/0/1/0/all/0/1\">Elnaz Shafaei-Bajestan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uhrig_P/0/1/0/all/0/1\">Peter Uhrig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baayen_R/0/1/0/all/0/1\">R. Harald Baayen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quantum Circuit Compiler for a Shuttling-Based Trapped-Ion Quantum Computer. (arXiv:2207.01964v1 [quant-ph])","link":"http://arxiv.org/abs/2207.01964","description":"<p>Increasing capabilities of quantum computing hardware and the challenge to\nrealize deep quantum circuits call for fully automated and efficient tools to\ncompile quantum circuits. To express arbitrary circuits in a sequence of native\ngates pertaining to the specific quantum computer architecture is necessary to\nmake algorithms portable across the landscape of quantum hardware providers. In\nthis work, we present a compiler capable of transforming and optimizing a\nquantum circuit, targeting a shuttling-based trapped-ion quantum processor. It\nconsists of custom algorithms set on top of the Cambridge Quantum Computer's\nquantum circuit framework Pytket. The performance is evaluated for a wide range\nof quantum circuits, showing that the gate counts can be reduced by a factor of\nup to 3.6 compared to standard Pytket and up to 2.2 compared to standard Qiskit\ncompilation, while we achieve similar gate counts as compared to a Pytket\nextension targeting the AQT linear-static trapped ion addressing-based\narchitecture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/quant-ph/1/au:+Kreppel_F/0/1/0/all/0/1\">Fabian Kreppel</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Melzer_C/0/1/0/all/0/1\">Christian Melzer</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Wagner_J/0/1/0/all/0/1\">Janis Wagner</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Hilder_J/0/1/0/all/0/1\">Janine Hilder</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Poschinger_U/0/1/0/all/0/1\">Ulrich Poschinger</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Schmidt_Kaler_F/0/1/0/all/0/1\">Ferdinand Schmidt-Kaler</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Brinkmann_A/0/1/0/all/0/1\">Andr&#xe9; Brinkmann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Block-SCL: Blocking Matters for Supervised Contrastive Learning in Product Matching. (arXiv:2207.02008v1 [cs.CL])","link":"http://arxiv.org/abs/2207.02008","description":"<p>Product matching is a fundamental step for the global understanding of\nconsumer behavior in e-commerce. In practice, product matching refers to the\ntask of deciding if two product offers from different data sources (e.g.\nretailers) represent the same product. Standard pipelines use a previous stage\ncalled blocking, where for a given product offer a set of potential matching\ncandidates are retrieved based on similar characteristics (e.g. same brand,\ncategory, flavor, etc.). From these similar product candidates, those that are\nnot a match can be considered hard negatives. We present Block-SCL, a strategy\nthat uses the blocking output to make the most of Supervised Contrastive\nLearning (SCL). Concretely, Block-SCL builds enriched batches using the\nhard-negatives samples obtained in the blocking stage. These batches provide a\nstrong training signal leading the model to learn more meaningful sentence\nembeddings for product matching. Experimental results in several public\ndatasets demonstrate that Block-SCL achieves state-of-the-art results despite\nonly using short product titles as input, no data augmentation, and a lighter\ntransformer backbone than competing methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Almagro_M/0/1/0/all/0/1\">Mario Almagro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jimenez_D/0/1/0/all/0/1\">David Jim&#xe9;nez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ortego_D/0/1/0/all/0/1\">Diego Ortego</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Almazan_E/0/1/0/all/0/1\">Emilio Almaz&#xe1;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martinez_E/0/1/0/all/0/1\">Eva Mart&#xed;nez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Networks and the Chomsky Hierarchy. (arXiv:2207.02098v1 [cs.LG])","link":"http://arxiv.org/abs/2207.02098","description":"<p>Reliable generalization lies at the heart of safe ML and AI. However,\nunderstanding when and how neural networks generalize remains one of the most\nimportant unsolved problems in the field. In this work, we conduct an extensive\nempirical study (2200 models, 16 tasks) to investigate whether insights from\nthe theory of computation can predict the limits of neural network\ngeneralization in practice. We demonstrate that grouping tasks according to the\nChomsky hierarchy allows us to forecast whether certain architectures will be\nable to generalize to out-of-distribution inputs. This includes negative\nresults where even extensive amounts of data and training time never led to any\nnon-trivial generalization, despite models having sufficient capacity to\nperfectly fit the training data. Our results show that, for our subset of\ntasks, RNNs and Transformers fail to generalize on non-regular tasks, LSTMs can\nsolve regular and counter-language tasks, and only networks augmented with\nstructured memory (such as a stack or memory tape) can successfully generalize\non context-free and context-sensitive tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deletang_G/0/1/0/all/0/1\">Gr&#xe9;goire Del&#xe9;tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruoss_A/0/1/0/all/0/1\">Anian Ruoss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grau_Moya_J/0/1/0/all/0/1\">Jordi Grau-Moya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Genewein_T/0/1/0/all/0/1\">Tim Genewein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wenliang_L/0/1/0/all/0/1\">Li Kevin Wenliang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Catt_E/0/1/0/all/0/1\">Elliot Catt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hutter_M/0/1/0/all/0/1\">Marcus Hutter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Legg_S/0/1/0/all/0/1\">Shane Legg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ortega_P/0/1/0/all/0/1\">Pedro A. Ortega</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A cross-corpus study on speech emotion recognition. (arXiv:2207.02104v1 [cs.CL])","link":"http://arxiv.org/abs/2207.02104","description":"<p>For speech emotion datasets, it has been difficult to acquire large\nquantities of reliable data and acted emotions may be over the top compared to\nless expressive emotions displayed in everyday life. Lately, larger datasets\nwith natural emotions have been created. Instead of ignoring smaller, acted\ndatasets, this study investigates whether information learnt from acted\nemotions is useful for detecting natural emotions. Cross-corpus research has\nmostly considered cross-lingual and even cross-age datasets, and difficulties\narise from different methods of annotating emotions causing a drop in\nperformance. To be consistent, four adult English datasets covering acted,\nelicited and natural emotions are considered. A state-of-the-art model is\nproposed to accurately investigate the degradation of performance. The system\ninvolves a bi-directional LSTM with an attention mechanism to classify emotions\nacross datasets. Experiments study the effects of training models in a\ncross-corpus and multi-domain fashion and results show the transfer of\ninformation is not successful. Out-of-domain models, followed by adapting to\nthe missing dataset, and domain adversarial training (DAT) are shown to be more\nsuitable to generalising to emotions across datasets. This shows positive\ninformation transfer from acted datasets to those with more natural emotions\nand the benefits from training on different corpora.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Milner_R/0/1/0/all/0/1\">Rosanna Milner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jalal_M/0/1/0/all/0/1\">Md Asif Jalal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_R/0/1/0/all/0/1\">Raymond W. M. Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hain_T/0/1/0/all/0/1\">Thomas Hain</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comprehensive Review of Visual-Textual Sentiment Analysis from Social Media Networks. (arXiv:2207.02160v1 [cs.CL])","link":"http://arxiv.org/abs/2207.02160","description":"<p>Social media networks have become a significant aspect of people's lives,\nserving as a platform for their ideas, opinions and emotions. Consequently,\nautomated sentiment analysis (SA) is critical for recognising people's feelings\nin ways that other information sources cannot. The analysis of these feelings\nrevealed various applications, including brand evaluations, YouTube film\nreviews and healthcare applications. As social media continues to develop,\npeople post a massive amount of information in different forms, including text,\nphotos, audio and video. Thus, traditional SA algorithms have become limited,\nas they do not consider the expressiveness of other modalities. By including\nsuch characteristics from various material sources, these multimodal data\nstreams provide new opportunities for optimising the expected results beyond\ntext-based SA. Our study focuses on the forefront field of multimodal SA, which\nexamines visual and textual data posted on social media networks. Many people\nare more likely to utilise this information to express themselves on these\nplatforms. To serve as a resource for academics in this rapidly growing field,\nwe introduce a comprehensive overview of textual and visual SA, including data\npre-processing, feature extraction techniques, sentiment benchmark datasets,\nand the efficacy of multiple classification methodologies suited to each field.\nWe also provide a brief introduction of the most frequently utilised data\nfusion strategies and a summary of existing research on visual-textual SA.\nFinally, we highlight the most significant challenges and investigate several\nimportant sentiment applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Al_Tameemi_I/0/1/0/all/0/1\">Israa Khalaf Salman Al-Tameemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feizi_Derakhshi_M/0/1/0/all/0/1\">Mohammad-Reza Feizi-Derakhshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pashazadeh_S/0/1/0/all/0/1\">Saeed Pashazadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asadpour_M/0/1/0/all/0/1\">Mohammad Asadpour</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLEAR: Improving Vision-Language Navigation with Cross-Lingual, Environment-Agnostic Representations. (arXiv:2207.02185v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02185","description":"<p>Vision-and-Language Navigation (VLN) tasks require an agent to navigate\nthrough the environment based on language instructions. In this paper, we aim\nto solve two key challenges in this task: utilizing multilingual instructions\nfor improved instruction-path grounding and navigating through new environments\nthat are unseen during training. To address these challenges, we propose CLEAR:\nCross-Lingual and Environment-Agnostic Representations. First, our agent learns\na shared and visually-aligned cross-lingual language representation for the\nthree languages (English, Hindi and Telugu) in the Room-Across-Room dataset.\nOur language representation learning is guided by text pairs that are aligned\nby visual information. Second, our agent learns an environment-agnostic visual\nrepresentation by maximizing the similarity between semantically-aligned image\npairs (with constraints on object-matching) from different environments. Our\nenvironment agnostic visual representation can mitigate the environment bias\ninduced by low-level visual information. Empirically, on the Room-Across-Room\ndataset, we show that our multilingual agent gets large improvements in all\nmetrics over the strong baseline model when generalizing to unseen environments\nwith the cross-lingual language representation and the environment-agnostic\nvisual representation. Furthermore, we show that our learned language and\nvisual representations can be successfully transferred to the Room-to-Room and\nCooperative Vision-and-Dialogue Navigation task, and present detailed\nqualitative and quantitative generalization and grounding analysis. Our code is\navailable at https://github.com/jialuli-luka/CLEAR\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jialu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1\">Hao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BlonDe: An Automatic Evaluation Metric for Document-level Machine Translation. (arXiv:2103.11878v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.11878","description":"<p>Standard automatic metrics, e.g. BLEU, are not reliable for document-level MT\nevaluation. They can neither distinguish document-level improvements in\ntranslation quality from sentence-level ones, nor identify the discourse\nphenomena that cause context-agnostic translations. This paper introduces a\nnovel automatic metric BlonDe to widen the scope of automatic MT evaluation\nfrom sentence to document level. BlonDe takes discourse coherence into\nconsideration by categorizing discourse-related spans and calculating the\nsimilarity-based F1 measure of categorized spans. We conduct extensive\ncomparisons on a newly constructed dataset BWB. The experimental results show\nthat BlonDe possesses better selectivity and interpretability at the\ndocument-level, and is more sensitive to document-level nuances. In a\nlarge-scale human study, BlonDe also achieves significantly higher Pearson's r\ncorrelation with human judgments compared to previous metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yuchen Eleanor Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shuming Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongdong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Haoyang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sennrich_R/0/1/0/all/0/1\">Rico Sennrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Ming Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MReD: A Meta-Review Dataset for Structure-Controllable Text Generation. (arXiv:2110.07474v6 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.07474","description":"<p>When directly using existing text generation datasets for controllable\ngeneration, we are facing the problem of not having the domain knowledge and\nthus the aspects that could be controlled are limited. A typical example is\nwhen using CNN/Daily Mail dataset for controllable text summarization, there is\nno guided information on the emphasis of summary sentences. A more useful text\ngenerator should leverage both the input text and the control signal to guide\nthe generation, which can only be built with a deep understanding of the domain\nknowledge. Motivated by this vision, our paper introduces a new text generation\ndataset, named MReD. Our new dataset consists of 7,089 meta-reviews and all its\n45k meta-review sentences are manually annotated with one of the 9 carefully\ndefined categories, including abstract, strength, decision, etc. We present\nexperimental results on start-of-the-art summarization models, and propose\nmethods for structure-controlled generation with both extractive and\nabstractive models using our annotated data. By exploring various settings and\nanalyzing the model behavior with respect to the control signal, we demonstrate\nthe challenges of our proposed task and the values of our dataset MReD.\nMeanwhile, MReD also allows us to have a better understanding of the\nmeta-review domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chenhui Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Liying Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_R/0/1/0/all/0/1\">Ran Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bing_L/0/1/0/all/0/1\">Lidong Bing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1\">Yang You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Attention-Model Explainability through Faithfulness Violation Test. (arXiv:2201.12114v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2201.12114","description":"<p>Attention mechanisms are dominating the explainability of deep models. They\nproduce probability distributions over the input, which are widely deemed as\nfeature-importance indicators. However, in this paper, we find one critical\nlimitation in attention explanations: weakness in identifying the polarity of\nfeature impact. This would be somehow misleading -- features with higher\nattention weights may not faithfully contribute to model predictions; instead,\nthey can impose suppression effects. With this finding, we reflect on the\nexplainability of current attention-based techniques, such as\nAttentio$\\odot$Gradient and LRP-based attention explanations. We first propose\nan actionable diagnostic methodology (henceforth faithfulness violation test)\nto measure the consistency between explanation weights and the impact polarity.\nThrough the extensive experiments, we then show that most tested explanation\nmethods are unexpectedly hindered by the faithfulness violation issue,\nespecially the raw attention. Empirical analyses on the factors affecting\nviolation issues further provide useful observations for adopting explanation\nmethods in attention models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yibing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haoliang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yangyang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_C/0/1/0/all/0/1\">Chenqi Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shiqi Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"g2pW: A Conditional Weighted Softmax BERT for Polyphone Disambiguation in Mandarin. (arXiv:2203.10430v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.10430","description":"<p>Polyphone disambiguation is the most crucial task in Mandarin\ngrapheme-to-phoneme (g2p) conversion. Previous studies have approached this\nproblem using pre-trained language models, restricted output, and extra\ninformation from Part-Of-Speech (POS) tagging. Inspired by these strategies, we\npropose a novel approach, called g2pW, which adapts learnable softmax-weights\nto condition the outputs of BERT with the polyphonic character of interest and\nits POS tagging. Rather than using the hard mask as in previous works, our\nexperiments show that learning a soft-weighting function for the candidate\nphonemes benefits performance. In addition, our proposed g2pW does not require\nextra pre-trained POS tagging models while using POS tags as auxiliary features\nsince we train the POS tagging model simultaneously with the unified encoder.\nExperimental results show that our g2pW outperforms existing methods on the\npublic CPP dataset. All codes, model weights, and a user-friendly package are\npublicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yi-Chang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yu-Chuan Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yen-Cheng Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeh_Y/0/1/0/all/0/1\">Yi-Ren Yeh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WeNet 2.0: More Productive End-to-End Speech Recognition Toolkit. (arXiv:2203.15455v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2203.15455","description":"<p>Recently, we made available WeNet, a production-oriented end-to-end speech\nrecognition toolkit, which introduces a unified two-pass (U2) framework and a\nbuilt-in runtime to address the streaming and non-streaming decoding modes in a\nsingle model. To further improve ASR performance and facilitate various\nproduction requirements, in this paper, we present WeNet 2.0 with four\nimportant updates. (1) We propose U2++, a unified two-pass framework with\nbidirectional attention decoders, which includes the future contextual\ninformation by a right-to-left attention decoder to improve the representative\nability of the shared encoder and the performance during the rescoring stage.\n(2) We introduce an n-gram based language model and a WFST-based decoder into\nWeNet 2.0, promoting the use of rich text data in production scenarios. (3) We\ndesign a unified contextual biasing framework, which leverages user-specific\ncontext (e.g., contact lists) to provide rapid adaptation ability for\nproduction and improves ASR accuracy in both with-LM and without-LM scenarios.\n(4) We design a unified IO to support large-scale data for effective model\ntraining. In summary, the brand-new WeNet 2.0 achieves up to 10\\% relative\nrecognition performance improvement over the original WeNet on various corpora\nand makes available several important production-oriented features.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Binbin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Di Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1\">Zhendong Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xingchen Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1\">Zhuoyuan Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_H/0/1/0/all/0/1\">Hang Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1\">Lei Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_F/0/1/0/all/0/1\">Fuping Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_J/0/1/0/all/0/1\">Jianwei Niu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Empirical Study of Language Model Integration for Transducer based Speech Recognition. (arXiv:2203.16776v3 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2203.16776","description":"<p>Utilizing text-only data with an external language model (ELM) in end-to-end\nRNN-Transducer (RNN-T) for speech recognition is challenging. Recently, a class\nof methods such as density ratio (DR) and internal language model estimation\n(ILME) have been developed, outperforming the classic shallow fusion (SF)\nmethod. The basic idea behind these methods is that RNN-T posterior should\nfirst subtract the implicitly learned internal language model (ILM) prior, in\norder to integrate the ELM. While recent studies suggest that RNN-T only learns\nsome low-order language model information, the DR method uses a well-trained\nneural language model with full context, which may be inappropriate for the\nestimation of ILM and deteriorate the integration performance. Based on the DR\nmethod, we propose a low-order density ratio method (LODR) by replacing the\nestimation with a low-order weak language model. Extensive empirical\nexperiments are conducted on both in-domain and cross-domain scenarios on\nEnglish LibriSpeech &amp; Tedlium-2 and Chinese WenetSpeech &amp; AISHELL-1 datasets.\nIt is shown that LODR consistently outperforms SF in all tasks, while\nperforming generally close to ILME and better than DR in most tests.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zheng_H/0/1/0/all/0/1\">Huahuan Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+An_K/0/1/0/all/0/1\">Keyu An</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ou_Z/0/1/0/all/0/1\">Zhijian Ou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_C/0/1/0/all/0/1\">Chen Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ding_K/0/1/0/all/0/1\">Ke Ding</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wan_G/0/1/0/all/0/1\">Guanglu Wan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PanGu-Bot: Efficient Generative Dialogue Pre-training from Pre-trained Language Model. (arXiv:2203.17090v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.17090","description":"<p>In this paper, we introduce PanGu-Bot, a Chinese pre-trained open-domain\ndialogue generation model based on a large pre-trained language model (PLM)\nPANGU-alpha (Zeng et al.,2021). Different from other pre-trained dialogue\nmodels trained over a massive amount of dialogue data from scratch, we aim to\nbuild a powerful dialogue model with relatively fewer data and computation\ncosts by inheriting valuable language capabilities and knowledge from PLMs. To\nthis end, we train PanGu-Bot from the large PLM PANGU-alpha, which has been\nproven well-performed on a variety of Chinese natural language tasks. We\ninvestigate different aspects of responses generated by PanGu-Bot, including\nresponse quality, knowledge, and safety. We show that PanGu-Bot outperforms\nstate-of-the-art Chinese dialogue systems (CDIALGPT (Wang et al., 2020), EVA\n(Zhou et al., 2021), EVA2.0 (Gu et al., 2022)) w.r.t. the above three aspects.\nWe also demonstrate that PanGu-Bot can be easily deployed to generate emotional\nresponses without further training. Throughout our empirical analysis, we also\npoint out that the PanGu-Bot response quality, knowledge correctness, and\nsafety are still far from perfect, and further explorations are indispensable\nto building reliable and smart dialogue systems. Our model and code will be\navailable at\nhttps://github.com/huawei-noah/Pretrained-Language-Model/tree/master/PanGu-Bot\nsoon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mi_F/0/1/0/all/0/1\">Fei Mi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yitong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1\">Yulong Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jingyan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yasheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chuanfei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_L/0/1/0/all/0/1\">Lifeng Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shiqi Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Study of Gender Impact in Self-supervised Models for Speech-to-Text Systems. (arXiv:2204.01397v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.01397","description":"<p>Self-supervised models for speech processing emerged recently as popular\nfoundation blocks in speech processing pipelines. These models are pre-trained\non unlabeled audio data and then used in speech processing downstream tasks\nsuch as automatic speech recognition (ASR) or speech translation (ST). Since\nthese models are now used in research and industrial systems alike, it becomes\nnecessary to understand the impact caused by some features such as gender\ndistribution within pre-training data. Using French as our investigation\nlanguage, we train and compare gender-specific wav2vec 2.0 models against\nmodels containing different degrees of gender balance in their pre-training\ndata. The comparison is performed by applying these models to two\nspeech-to-text downstream tasks: ASR and ST. Results show the type of\ndownstream integration matters. We observe lower overall performance using\ngender-specific pre-training before fine-tuning an end-to-end ASR system.\nHowever, when self-supervised models are used as feature extractors, the\noverall ASR and ST results follow more complex patterns in which the balanced\npre-trained model does not necessarily lead to the best results. Lastly, our\ncrude 'fairness' metric, the relative performance difference measured between\nfemale and male test sets, does not display a strong variation from balanced to\ngender-specific pre-trained wav2vec 2.0 models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Boito_M/0/1/0/all/0/1\">Marcely Zanon Boito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Besacier_L/0/1/0/all/0/1\">Laurent Besacier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tomashenko_N/0/1/0/all/0/1\">Natalia Tomashenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Esteve_Y/0/1/0/all/0/1\">Yannick Est&#xe8;ve</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluation of Semantic Answer Similarity Metrics. (arXiv:2206.12664v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.12664","description":"<p>There are several issues with the existing general machine translation or\nnatural language generation evaluation metrics, and question-answering (QA)\nsystems are indifferent in that context. To build robust QA systems, we need\nthe ability to have equivalently robust evaluation systems to verify whether\nmodel predictions to questions are similar to ground-truth annotations. The\nability to compare similarity based on semantics as opposed to pure string\noverlap is important to compare models fairly and to indicate more realistic\nacceptance criteria in real-life applications. We build upon the first to our\nknowledge paper that uses transformer-based model metrics to assess semantic\nanswer similarity and achieve higher correlations to human judgement in the\ncase of no lexical overlap. We propose cross-encoder augmented bi-encoder and\nBERTScore models for semantic answer similarity, trained on a new dataset\nconsisting of name pairs of US-American public figures. As far as we are\nconcerned, we provide the first dataset of co-referent name string pairs along\nwith their similarities, which can be used for training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mustafazade_F/0/1/0/all/0/1\">Farida Mustafazade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ebbinghaus_P/0/1/0/all/0/1\">Peter F. Ebbinghaus</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language-specific Characteristic Assistance for Code-switching Speech Recognition. (arXiv:2206.14580v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.14580","description":"<p>Dual-encoder structure successfully utilizes two language-specific encoders\n(LSEs) for code-switching speech recognition. Because LSEs are initialized by\ntwo pre-trained language-specific models (LSMs), the dual-encoder structure can\nexploit sufficient monolingual data and capture the individual language\nattributes. However, existing methods have no language constraints on LSEs and\nunderutilize language-specific knowledge of LSMs. In this paper, we propose a\nlanguage-specific characteristic assistance (LSCA) method to mitigate the above\nproblems. Specifically, during training, we introduce two language-specific\nlosses as language constraints and generate corresponding language-specific\ntargets for them. During decoding, we take the decoding abilities of LSMs into\naccount by combining the output probabilities of two LSMs and the mixture model\nto obtain the final predictions. Experiments show that either the training or\ndecoding method of LSCA can improve the model's performance. Furthermore, the\nbest result can obtain up to 15.4% relative error reduction on the\ncode-switching test set by combining the training and decoding methods of LSCA.\nMoreover, the system can process code-switching speech recognition tasks well\nwithout extra shared parameters or even retraining based on two pre-trained\nLSMs by using our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_T/0/1/0/all/0/1\">Tongtong Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qiang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_M/0/1/0/all/0/1\">Meng Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Longbiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Hao Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_Y/0/1/0/all/0/1\">Yongjie Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yuqin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dang_J/0/1/0/all/0/1\">Jianwu Dang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Visual Grounding by Encouraging Consistent Gradient-based Explanations. (arXiv:2206.15462v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.15462","description":"<p>We propose a margin-based loss for vision-language model pretraining that\nencourages gradient-based explanations that are consistent with region-level\nannotations. We refer to this objective as Attention Mask Consistency (AMC) and\ndemonstrate that it produces superior visual grounding performance compared to\nmodels that rely instead on region-level annotations for explicitly training an\nobject detector such as Faster R-CNN. AMC works by encouraging gradient-based\nexplanation masks that focus their attention scores mostly within annotated\nregions of interest for images that contain such annotations. Particularly, a\nmodel trained with AMC on top of standard vision-language modeling objectives\nobtains a state-of-the-art accuracy of 86.59% in the Flickr30k visual grounding\nbenchmark, an absolute improvement of 5.48% when compared to the best previous\nmodel. Our approach also performs exceedingly well on established benchmarks\nfor referring expression comprehension and offers the added benefit by design\nof gradient-based explanations that better align with human annotations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Ziyan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kafle_K/0/1/0/all/0/1\">Kushal Kafle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dernoncourt_F/0/1/0/all/0/1\">Franck Dernoncourt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ordonez_V/0/1/0/all/0/1\">Vicente Ordonez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Assessing the Effects of Hyperparameters on Knowledge Graph Embedding Quality. (arXiv:2207.00473v3 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2207.00473","description":"<p>Embedding knowledge graphs into low-dimensional spaces is a popular method\nfor applying approaches, such as link prediction or node classification, to\nthese databases. This embedding process is very costly in terms of both\ncomputational time and space. Part of the reason for this is the optimisation\nof hyperparameters, which involves repeatedly sampling, by random, guided, or\nbrute-force selection, from a large hyperparameter space and testing the\nresulting embeddings for their quality. However, not all hyperparameters in\nthis search space will be equally important. In fact, with prior knowledge of\nthe relative importance of the hyperparameters, some could be eliminated from\nthe search altogether without significantly impacting the overall quality of\nthe outputted embeddings. To this end, we ran a Sobol sensitivity analysis to\nevaluate the effects of tuning different hyperparameters on the variance of\nembedding quality. This was achieved by performing thousands of embedding\ntrials, each time measuring the quality of embeddings produced by different\nhyperparameter configurations. We regressed the embedding quality on those\nhyperparameter configurations, using this model to generate Sobol sensitivity\nindices for each of the hyperparameters. By evaluating the correlation between\nSobol indices, we find substantial variability in the hyperparameter\nsensitivities between knowledge graphs, with differing dataset characteristics\nbeing the probable cause of these inconsistencies. As an additional\ncontribution of this work we identify several relations in the UMLS knowledge\ngraph that may cause data leakage via inverse relations, and derive and present\nUMLS-43, a leakage-robust variant of that graph.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lloyd_O/0/1/0/all/0/1\">Oliver Lloyd</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaunt_T/0/1/0/all/0/1\">Tom Gaunt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attributed Abnormality Graph Embedding for Clinically Accurate X-Ray Report Generation. (arXiv:2207.01208v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.01208","description":"<p>Automatic generation of medical reports from X-ray images can assist\nradiologists to perform the time-consuming and yet important reporting task.\nYet, achieving clinically accurate generated reports remains challenging.\nModeling the underlying abnormalities using the knowledge graph approach has\nbeen found promising in enhancing the clinical accuracy. In this paper, we\nintroduce a novel fined-grained knowledge graph structure called an attributed\nabnormality graph (ATAG). The ATAG consists of interconnected abnormality nodes\nand attribute nodes, allowing it to better capture the abnormality details. In\ncontrast to the existing methods where the abnormality graph was constructed\nmanually, we propose a methodology to automatically construct the fine-grained\ngraph structure based on annotations, medical reports in X-ray datasets, and\nthe RadLex radiology lexicon. We then learn the ATAG embedding using a deep\nmodel with an encoder-decoder architecture for the report generation. In\nparticular, graph attention networks are explored to encode the relationships\namong the abnormalities and their attributes. A gating mechanism is adopted and\nintegrated with various decoders for the generation. We carry out extensive\nexperiments based on the benchmark datasets, and show that the proposed\nATAG-based deep model outperforms the SOTA methods by a large margin and can\nimprove the clinical accuracy of the generated reports.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Sixing Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheung_W/0/1/0/all/0/1\">William K. Cheung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiu_K/0/1/0/all/0/1\">Keith Chiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_T/0/1/0/all/0/1\">Terence M. Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheung_C/0/1/0/all/0/1\">Charles K. Cheung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+See_S/0/1/0/all/0/1\">Simon See</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Student-AI Creative Writing: Pedagogical Strategies for Applying Natural Language Generation in Schools. (arXiv:2207.01484v2 [cs.CY] UPDATED)","link":"http://arxiv.org/abs/2207.01484","description":"<p>AI natural language generation (NLG) is a process where computer systems\ngenerate human-comprehensible language texts from information. It can become an\nintegral part of a human's creative writing process. Importantly, youths can\nlearn to apply NLG in mainstream education and become better prepared for\nAI-enhanced writing jobs and other writing endeavors. To explore how students\napply NLG to creative writing, we designed and implemented the 1st Human-AI\nCreative Writing Contest in a Hong Kong secondary school. In this contest, each\nstudent participant wrote a short story of up to 500-words using the student's\nown words and words generated by a computer and built on open-source language\nmodels. We designed four text generators for the contest as the computer's text\nentry. Additionally, using design-based research, we developed seven workshops\nwhere students learned to write with the four text generators and answered\nreflection questions. In analyzing four students' short stories and\nadjudicators' scores for the stories, we found different strategies in terms of\nthe number and the type of text generator words that students used. Some\nstrategies appeared more sophisticated than others. In analyzing students'\nreflections, we found students could describe text generator input and output\nas units of thought. Besides, students showed preferences for text generators;\nand they expressed a range of feelings when writing with text generators. The\nfindings provide design implications not only for NLG applications in formal\nschooling but also suggest pedagogical strategies for AI curriculum.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Woo_D/0/1/0/all/0/1\">David James Woo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Susanto_H/0/1/0/all/0/1\">Hengky Susanto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-07-05T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Egocentric Video-Language Pretraining @ Ego4D Challenge 2022. (arXiv:2207.01622v1 [cs.CV])","link":"http://arxiv.org/abs/2207.01622","description":"<p>In this report, we propose a video-language pretraining (VLP) based solution\n\\cite{kevin2022egovlp} for four Ego4D challenge tasks, including Natural\nLanguage Query (NLQ), Moment Query (MQ), Object State Change Classification\n(OSCC), and PNR Localization (PNR). Especially, we exploit the recently\nreleased Ego4D dataset \\cite{grauman2021ego4d} to pioneer Egocentric VLP from\npretraining dataset, pretraining objective, and development set. Based on the\nabove three designs, we develop a pretrained video-language model that is able\nto transfer its egocentric video-text representation or video-only\nrepresentation to several video downstream tasks. Our Egocentric VLP achieves\n10.46R@1&amp;IoU @0.3 on NLQ, 10.33 mAP on MQ, 74% Acc on OSCC, 0.67 sec error on\nPNR. The code is available at https://github.com/showlab/EgoVLP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_K/0/1/0/all/0/1\">Kevin Qinghong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1\">Alex Jinpeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soldan_M/0/1/0/all/0/1\">Mattia Soldan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wray_M/0/1/0/all/0/1\">Michael Wray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1\">Rui Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_E/0/1/0/all/0/1\">Eric Zhongcong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_D/0/1/0/all/0/1\">Difei Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_R/0/1/0/all/0/1\">Rongcheng Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wenzhe Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_W/0/1/0/all/0/1\">Weijie Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_C/0/1/0/all/0/1\">Chengfei Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongfa Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Damen_D/0/1/0/all/0/1\">Dima Damen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bernard Ghanem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shou_M/0/1/0/all/0/1\">Mike Zheng Shou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Slice-by-slice deep learning aided oropharyngeal cancer segmentation with adaptive thresholding for spatial uncertainty on FDG PET and CT images. (arXiv:2207.01623v1 [eess.IV])","link":"http://arxiv.org/abs/2207.01623","description":"<p>Tumor segmentation is a fundamental step for radiotherapy treatment planning.\nTo define an accurate segmentation of the primary tumor (GTVp) of oropharyngeal\ncancer patients (OPC), simultaneous assessment of different image modalities is\nneeded, and each image volume is explored slice-by-slice from different\norientations. Moreover, the manual fixed boundary of segmentation neglects the\nspatial uncertainty known to occur in tumor delineation. This study proposes a\nnovel automatic deep learning (DL) model to assist radiation oncologists in a\nslice-by-slice adaptive GTVp segmentation on registered FDG PET/CT images. We\nincluded 138 OPC patients treated with (chemo)radiation in our institute. Our\nDL framework exploits both inter and intra-slice context. Sequences of 3\nconsecutive 2D slices of concatenated FDG PET/CT images and GTVp contours were\nused as input. A 3-fold cross validation was performed three times, training on\nsequences extracted from the Axial (A), Sagittal (S), and Coronal (C) plane of\n113 patients. Since consecutive sequences in a volume contain overlapping\nslices, each slice resulted in three outcome predictions that were averaged. In\nthe A, S, and C planes, the output shows areas with different probabilities of\npredicting the tumor. The performance of the models was assessed on 25 patients\nat different probability thresholds using the mean Dice Score Coefficient\n(DSC). Predictions were the closest to the ground truth at a probability\nthreshold of 0.9 (DSC of 0.70 in the A, 0.77 in the S, and 0.80 in the C\nplane). The promising results of the proposed DL model show that the\nprobability maps on registered FDG PET/CT images could guide radiation\noncologists in a slice-by-slice adaptive GTVp segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Biase_A/0/1/0/all/0/1\">Alessia De Biase</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sijtsema_N/0/1/0/all/0/1\">Nanna Maria Sijtsema</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dijk_L/0/1/0/all/0/1\">Lisanne van Dijk</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Langendijk_J/0/1/0/all/0/1\">Johannes A. Langendijk</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ooijen_P/0/1/0/all/0/1\">Peter van Ooijen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interaction Transformer for Human Reaction Generation. (arXiv:2207.01685v1 [cs.CV])","link":"http://arxiv.org/abs/2207.01685","description":"<p>We address the challenging task of human reaction generation which aims to\ngenerate a corresponding reaction based on an input action. Most of the\nexisting works do not focus on generating and predicting the reaction and\ncannot generate the motion when only the action is given as input. To address\nthis limitation, we propose a novel interaction Transformer (InterFormer)\nconsisting of a Transformer network with both temporal and spatial attentions.\nSpecifically, the temporal attention captures the temporal dependencies of the\nmotion of both characters and of their interaction, while the spatial attention\nlearns the dependencies between the different body parts of each character and\nthose which are part of the interaction. Moreover, we propose using graphs to\nincrease the performance of the spatial attention via an interaction distance\nmodule that helps focus on nearby joints from both characters. Extensive\nexperiments on the SBU interaction, K3HI, and DuetDance datasets demonstrate\nthe effectiveness of InterFormer. Our method is general and can be used to\ngenerate more complex and long-term interactions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chopin_B/0/1/0/all/0/1\">Baptiste Chopin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Otberdout_N/0/1/0/all/0/1\">Naima Otberdout</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daoudi_M/0/1/0/all/0/1\">Mohamed Daoudi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1\">Nicu Sebe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Crime scene classification from skeletal trajectory analysis in surveillance settings. (arXiv:2207.01687v1 [cs.CV])","link":"http://arxiv.org/abs/2207.01687","description":"<p>Video anomaly analysis is a core task actively pursued in the field of\ncomputer vision, with applications extending to real-world crime detection in\nsurveillance footage. In this work, we address the task of human-related crime\nclassification. In our proposed approach, the human body in video frames,\nrepresented as skeletal joints trajectories, is used as the main source of\nexploration. First, we introduce the significance of extending the ground truth\nlabels for HR-Crime dataset and hence, propose a supervised and unsupervised\nmethodology to generate trajectory-level ground truth labels. Next, given the\navailability of the trajectory-level ground truth, we introduce a\ntrajectory-based crime classification framework. Ablation studies are conducted\nwith various architectures and feature fusion strategies for the representation\nof the human trajectories. The conducted experiments demonstrate the\nfeasibility of the task and pave the path for further research in the field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Matei_A/0/1/0/all/0/1\">Alina-Daniela Matei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talavera_E/0/1/0/all/0/1\">Estefania Talavera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aghaei_M/0/1/0/all/0/1\">Maya Aghaei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TM2T: Stochastic and Tokenized Modeling for the Reciprocal Generation of 3D Human Motions and Texts. (arXiv:2207.01696v1 [cs.CV])","link":"http://arxiv.org/abs/2207.01696","description":"<p>Inspired by the strong ties between vision and language, the two intimate\nhuman sensing and communication modalities, our paper aims to explore the\ngeneration of 3D human full-body motions from texts, as well as its reciprocal\ntask, shorthanded for text2motion and motion2text, respectively. To tackle the\nexisting challenges, especially to enable the generation of multiple distinct\nmotions from the same text, and to avoid the undesirable production of trivial\nmotionless pose sequences, we propose the use of motion token, a discrete and\ncompact motion representation. This provides one level playing ground when\nconsidering both motions and text signals, as the motion and text tokens,\nrespectively. Moreover, our motion2text module is integrated into the inverse\nalignment process of our text2motion training pipeline, where a significant\ndeviation of synthesized text from the input text would be penalized by a large\ntraining loss; empirically this is shown to effectively improve performance.\nFinally, the mappings in-between the two modalities of motions and texts are\nfacilitated by adapting the neural model for machine translation (NMT) to our\ncontext. This autoregressive modeling of the distribution over discrete motion\ntokens further enables non-deterministic production of pose sequences, of\nvariable lengths, from an input text. Our approach is flexible, could be used\nfor both text2motion and motion2text tasks. Empirical evaluations on two\nbenchmark datasets demonstrate the superior performance of our approach on both\ntasks over a variety of state-of-the-art methods. Project page:\nhttps://ericguo5513.github.io/TM2T/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1\">Chuan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xuo_X/0/1/0/all/0/1\">Xinxin Xuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Li Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WPPG Net: A Non-contact Video Based Heart Rate Extraction Network Framework with Compatible Training Capability. (arXiv:2207.01697v1 [cs.CV])","link":"http://arxiv.org/abs/2207.01697","description":"<p>Our facial skin presents subtle color change known as remote\nPhotoplethysmography (rPPG) signal, from which we could extract the heart rate\nof the subject. Recently many deep learning methods and related datasets on\nrPPG signal extraction are proposed. However, because of the time consumption\nblood flowing through our body and other factors, label waves such as BVP\nsignals have uncertain delays with real rPPG signals in some datasets, which\nresults in the difficulty on training of networks which output predicted rPPG\nwaves directly. In this paper, by analyzing the common characteristics on\nrhythm and periodicity of rPPG signals and label waves, we propose a whole set\nof training methodology which wraps these networks so that they could remain\nefficient when be trained at the presence of frequent uncertain delay in\ndatasets and gain more precise and robust heart rate prediction results than\nother delay-free rPPG extraction methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Weiyu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Ying Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yun Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_C/0/1/0/all/0/1\">Chunyu Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaolin Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Disentangled Action Recognition with Knowledge Bases. (arXiv:2207.01708v1 [cs.CV])","link":"http://arxiv.org/abs/2207.01708","description":"<p>Action in video usually involves the interaction of human with objects.\nAction labels are typically composed of various combinations of verbs and\nnouns, but we may not have training data for all possible combinations. In this\npaper, we aim to improve the generalization ability of the compositional action\nrecognition model to novel verbs or novel nouns that are unseen during training\ntime, by leveraging the power of knowledge graphs. Previous work utilizes\nverb-noun compositional action nodes in the knowledge graph, making it\ninefficient to scale since the number of compositional action nodes grows\nquadratically with respect to the number of verbs and nouns. To address this\nissue, we propose our approach: Disentangled Action Recognition with\nKnowledge-bases (DARK), which leverages the inherent compositionality of\nactions. DARK trains a factorized model by first extracting disentangled\nfeature representations for verbs and nouns, and then predicting classification\nweights using relations in external knowledge graphs. The type constraint\nbetween verb and noun is extracted from external knowledge bases and finally\napplied when composing actions. DARK has better scalability in the number of\nobjects and verbs, and achieves state-of-the-art performance on the Charades\ndataset. We further propose a new benchmark split based on the Epic-kitchen\ndataset which is an order of magnitude bigger in the numbers of classes and\nsamples, and benchmark various models on this benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Zhekun Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Shalini Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guillory_D/0/1/0/all/0/1\">Devin Guillory</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kato_K/0/1/0/all/0/1\">Keizo Kato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1\">Trevor Darrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Huijuan Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Fine-Grained Sketch-Based Image Retrieval. (arXiv:2207.01723v1 [cs.CV])","link":"http://arxiv.org/abs/2207.01723","description":"<p>The recent focus on Fine-Grained Sketch-Based Image Retrieval (FG-SBIR) has\nshifted towards generalising a model to new categories without any training\ndata from them. In real-world applications, however, a trained FG-SBIR model is\noften applied to both new categories and different human sketchers, i.e.,\ndifferent drawing styles. Although this complicates the generalisation problem,\nfortunately, a handful of examples are typically available, enabling the model\nto adapt to the new category/style. In this paper, we offer a novel perspective\n-- instead of asking for a model that generalises, we advocate for one that\nquickly adapts, with just very few samples during testing (in a few-shot\nmanner). To solve this new problem, we introduce a novel model-agnostic\nmeta-learning (MAML) based framework with several key modifications: (1) As a\nretrieval task with a margin-based contrastive loss, we simplify the MAML\ntraining in the inner loop to make it more stable and tractable. (2) The margin\nin our contrastive loss is also meta-learned with the rest of the model. (3)\nThree additional regularisation losses are introduced in the outer loop, to\nmake the meta-learned FG-SBIR model more effective for category/style\nadaptation. Extensive experiments on public datasets suggest a large gain over\ngeneralisation and zero-shot based approaches, and a few strong few-shot\nbaselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhunia_A/0/1/0/all/0/1\">Ayan Kumar Bhunia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sain_A/0/1/0/all/0/1\">Aneeshan Sain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_P/0/1/0/all/0/1\">Parth Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Animesh Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_P/0/1/0/all/0/1\">Pinaki Nath Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1\">Tao Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yi-Zhe Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Much More Data Do I Need? Estimating Requirements for Downstream Tasks. (arXiv:2207.01725v1 [cs.CV])","link":"http://arxiv.org/abs/2207.01725","description":"<p>Given a small training data set and a learning algorithm, how much more data\nis necessary to reach a target validation or test performance? This question is\nof critical importance in applications such as autonomous driving or medical\nimaging where collecting data is expensive and time-consuming. Overestimating\nor underestimating data requirements incurs substantial costs that could be\navoided with an adequate budget. Prior work on neural scaling laws suggest that\nthe power-law function can fit the validation performance curve and extrapolate\nit to larger data set sizes. We find that this does not immediately translate\nto the more difficult downstream task of estimating the required data set size\nto meet a target performance. In this work, we consider a broad class of\ncomputer vision tasks and systematically investigate a family of functions that\ngeneralize the power-law function to allow for better estimation of data\nrequirements. Finally, we show that incorporating a tuned correction factor and\ncollecting over multiple rounds significantly improves the performance of the\ndata estimators. Using our guidelines, practitioners can accurately estimate\ndata requirements of machine learning systems to gain savings in both\ndevelopment time and data acquisition costs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mahmood_R/0/1/0/all/0/1\">Rafid Mahmood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucas_J/0/1/0/all/0/1\">James Lucas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Acuna_D/0/1/0/all/0/1\">David Acuna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Daiqing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Philion_J/0/1/0/all/0/1\">Jonah Philion</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alvarez_J/0/1/0/all/0/1\">Jose M. Alvarez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhiding Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fidler_S/0/1/0/all/0/1\">Sanja Fidler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Law_M/0/1/0/all/0/1\">Marc T. Law</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are metrics measuring what they should? An evaluation of image captioning task metrics. (arXiv:2207.01733v1 [cs.CV])","link":"http://arxiv.org/abs/2207.01733","description":"<p>Image Captioning is a current research task to describe the image content\nusing the objects and their relationships in the scene. To tackle this task,\ntwo important research areas are used, artificial vision, and natural language\nprocessing. In Image Captioning, as in any computational intelligence task, the\nperformance metrics are crucial for knowing how well (or bad) a method\nperforms. In recent years, it has been observed that classical metrics based on\nn-grams are insufficient to capture the semantics and the critical meaning to\ndescribe the content in an image. Looking to measure how well or not the set of\ncurrent and more recent metrics are doing, in this manuscript, we present an\nevaluation of several kinds of Image Captioning metrics and a comparison\nbetween them using the well-known MS COCO dataset. For this, we designed two\nscenarios; 1) a set of artificially build captions with several quality, and 2)\na comparison of some state-of-the-art Image Captioning methods. We tried to\nanswer the questions: Are the current metrics helping to produce high quality\ncaptions? How do actual metrics compare to each other? What are the metrics\nreally measuring?\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Chavez_O/0/1/0/all/0/1\">Oth&#xf3;n Gonz&#xe1;lez-Ch&#xe1;vez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruiz_G/0/1/0/all/0/1\">Guillermo Ruiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moctezuma_D/0/1/0/all/0/1\">Daniela Moctezuma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramirez_delReal_T/0/1/0/all/0/1\">Tania A. Ramirez-delReal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Anomaly-aware multiple instance learning for rare anemia disorder classification. (arXiv:2207.01742v1 [cs.LG])","link":"http://arxiv.org/abs/2207.01742","description":"<p>Deep learning-based classification of rare anemia disorders is challenged by\nthe lack of training data and instance-level annotations. Multiple Instance\nLearning (MIL) has shown to be an effective solution, yet it suffers from low\naccuracy and limited explainability. Although the inclusion of attention\nmechanisms has addressed these issues, their effectiveness highly depends on\nthe amount and diversity of cells in the training samples. Consequently, the\npoor machine learning performance on rare anemia disorder classification from\nblood samples remains unresolved. In this paper, we propose an interpretable\npooling method for MIL to address these limitations. By benefiting from\ninstance-level information of negative bags (i.e., homogeneous benign cells\nfrom healthy individuals), our approach increases the contribution of anomalous\ninstances. We show that our strategy outperforms standard MIL classification\nalgorithms and provides a meaningful explanation behind its decisions.\nMoreover, it can denote anomalous instances of rare blood diseases that are not\nseen during the training phase.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kazeminia_S/0/1/0/all/0/1\">Salome Kazeminia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadafi_A/0/1/0/all/0/1\">Ario Sadafi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makhro_A/0/1/0/all/0/1\">Asya Makhro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bogdanova_A/0/1/0/all/0/1\">Anna Bogdanova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Albarqouni_S/0/1/0/all/0/1\">Shadi Albarqouni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marr_C/0/1/0/all/0/1\">Carsten Marr</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attention Guided Network for Salient Object Detection in Optical Remote Sensing Images. (arXiv:2207.01755v1 [cs.CV])","link":"http://arxiv.org/abs/2207.01755","description":"<p>Due to the extreme complexity of scale and shape as well as the uncertainty\nof the predicted location, salient object detection in optical remote sensing\nimages (RSI-SOD) is a very difficult task. The existing SOD methods can satisfy\nthe detection performance for natural scene images, but they are not well\nadapted to RSI-SOD due to the above-mentioned image characteristics in remote\nsensing images. In this paper, we propose a novel Attention Guided Network\n(AGNet) for SOD in optical RSIs, including position enhancement stage and\ndetail refinement stage. Specifically, the position enhancement stage consists\nof a semantic attention module and a contextual attention module to accurately\ndescribe the approximate location of salient objects. The detail refinement\nstage uses the proposed self-refinement module to progressively refine the\npredicted results under the guidance of attention and reverse attention. In\naddition, the hybrid loss is applied to supervise the training of the network,\nwhich can improve the performance of the model from three perspectives of\npixel, region and statistics. Extensive experiments on two popular benchmarks\ndemonstrate that AGNet achieves competitive performance compared to other\nstate-of-the-art methods. The code will be available at\nhttps://github.com/NuaaYH/AGNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yuhan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Han Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1\">Ningzhong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_Y/0/1/0/all/0/1\">Yetong Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cen_J/0/1/0/all/0/1\">Jun Cen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Huiyu Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Universal Domain Adaptive Object Detector. (arXiv:2207.01756v1 [cs.CV])","link":"http://arxiv.org/abs/2207.01756","description":"<p>Universal domain adaptive object detection (UniDAOD)is more challenging than\ndomain adaptive object detection (DAOD) since the label space of the source\ndomain may not be the same as that of the target and the scale of objects in\nthe universal scenarios can vary dramatically (i.e, category shift and scale\nshift). To this end, we propose US-DAF, namely Universal Scale-Aware Domain\nAdaptive Faster RCNN with Multi-Label Learning, to reduce the negative transfer\neffect during training while maximizing transferability as well as\ndiscriminability in both domains under a variety of scales. Specifically, our\nmethod is implemented by two modules: 1) We facilitate the feature alignment of\ncommon classes and suppress the interference of private classes by designing a\nFilter Mechanism module to overcome the negative transfer caused by category\nshift. 2) We fill the blank of scale-aware adaptation in object detection by\nintroducing a new Multi-Label Scale-Aware Adapter to perform individual\nalignment between the corresponding scale for two domains. Experiments show\nthat US-DAF achieves state-of-the-art results on three scenarios (i.e,\nOpen-Set, Partial-Set, and Closed-Set) and yields 7.1% and 5.9% relative\nimprovement on benchmark datasets Clipart1k and Watercolor in particular.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1\">Wenxu Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weijie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_S/0/1/0/all/0/1\">Shiliang Pu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FDVTS's Solution for 2nd COV19D Competition on COVID-19 Detection and Severity Analysis. (arXiv:2207.01758v1 [eess.IV])","link":"http://arxiv.org/abs/2207.01758","description":"<p>This paper presents our solution for the 2nd COVID-19 Competition, occurring\nin the framework of the AIMIA Workshop in the European Conference on Computer\nVision (ECCV 2022). In our approach, we employ an effective 3D Contrastive\nMixup Classification network for COVID-19 diagnosis on chest CT images, which\nis composed of contrastive representation learning and mixup classification.\nFor the COVID-19 detection challenge, our approach reaches 0.9245 macro F1\nscore on 484 validation CT scans, which significantly outperforms the baseline\nmethod by 16.5%. In the COVID-19 severity detection challenge, our approach\nachieves 0.7186 macro F1 score on 61 validation samples, which also surpasses\nthe baseline by 8.86%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hou_J/0/1/0/all/0/1\">Junlin Hou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_J/0/1/0/all/0/1\">Jilan Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Feng_R/0/1/0/all/0/1\">Rui Feng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuejie Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GP22: A Car Styling Dataset for Automotive Designers. (arXiv:2207.01760v1 [cs.CV])","link":"http://arxiv.org/abs/2207.01760","description":"<p>An automated design data archiving could reduce the time wasted by designers\nfrom working creatively and effectively. Though many datasets on classifying,\ndetecting, and instance segmenting on car exterior exist, these large datasets\nare not relevant for design practices as the primary purpose lies in autonomous\ndriving or vehicle verification. Therefore, we release GP22, composed of car\nstyling features defined by automotive designers. The dataset contains 1480 car\nside profile images from 37 brands and ten car segments. It also contains\nannotations of design features that follow the taxonomy of the car exterior\ndesign features defined in the eye of the automotive designer. We trained the\nbaseline model using YOLO v5 as the design feature detection model with the\ndataset. The presented model resulted in an mAP score of 0.995 and a recall of\n0.984. Furthermore, exploration of the model performance on sketches and\nrendering images of the car side profile implies the scalability of the dataset\nfor design purposes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Gyunpyo Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Taesu Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suk_H/0/1/0/all/0/1\">Hyeon-Jeong Suk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rank-Based Filter Pruning for Real-Time UAV Tracking. (arXiv:2207.01768v1 [cs.CV])","link":"http://arxiv.org/abs/2207.01768","description":"<p>Unmanned aerial vehicle (UAV) tracking has wide potential applications in\nsuch as agriculture, navigation, and public security. However, the limitations\nof computing resources, battery capacity, and maximum load of UAV hinder the\ndeployment of deep learning-based tracking algorithms on UAV. Consequently,\ndiscriminative correlation filters (DCF) trackers stand out in the UAV tracking\ncommunity because of their high efficiency. However, their precision is usually\nmuch lower than trackers based on deep learning. Model compression is a\npromising way to narrow the gap (i.e., effciency, precision) between DCF- and\ndeep learning- based trackers, which has not caught much attention in UAV\ntracking. In this paper, we propose the P-SiamFC++ tracker, which is the first\nto use rank-based filter pruning to compress the SiamFC++ model, achieving a\nremarkable balance between efficiency and precision. Our method is general and\nmay encourage further studies on UAV tracking with model compression. Extensive\nexperiments on four UAV benchmarks, including UAV123@10fps, DTB70, UAVDT and\nVistrone2018, show that P-SiamFC++ tracker significantly outperforms\nstate-of-the-art UAV tracking methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xucheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_D/0/1/0/all/0/1\">Dan Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1\">Qijun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuiwang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SESS: Saliency Enhancing with Scaling and Sliding. (arXiv:2207.01769v1 [cs.CV])","link":"http://arxiv.org/abs/2207.01769","description":"<p>High-quality saliency maps are essential in several machine learning\napplication areas including explainable AI and weakly supervised object\ndetection and segmentation. Many techniques have been developed to generate\nbetter saliency using neural networks. However, they are often limited to\nspecific saliency visualisation methods or saliency issues. We propose a novel\nsaliency enhancing approach called SESS (Saliency Enhancing with Scaling and\nSliding). It is a method and model agnostic extension to existing saliency map\ngeneration methods. With SESS, existing saliency approaches become robust to\nscale variance, multiple occurrences of target objects, presence of distractors\nand generate less noisy and more discriminative saliency maps. SESS improves\nsaliency by fusing saliency maps extracted from multiple patches at different\nscales from different areas, and combines these individual maps using a novel\nfusion scheme that incorporates channel-wise weights and spatial weighted\naverage. To improve efficiency, we introduce a pre-filtering step that can\nexclude uninformative saliency maps to improve efficiency while still enhancing\noverall results. We evaluate SESS on object recognition and detection\nbenchmarks where it achieves significant improvement. The code is released\npublicly to enable researchers to verify performance and further development.\nCode is available at: https://github.com/neouyghur/SESS\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tursun_O/0/1/0/all/0/1\">Osman Tursun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denman_S/0/1/0/all/0/1\">Simon Denman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sridharan_S/0/1/0/all/0/1\">Sridha Sridharan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fookes_C/0/1/0/all/0/1\">Clinton Fookes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Object-Level Targeted Selection via Deep Template Matching. (arXiv:2207.01778v1 [cs.CV])","link":"http://arxiv.org/abs/2207.01778","description":"<p>Retrieving images with objects that are semantically similar to objects of\ninterest (OOI) in a query image has many practical use cases. A few examples\ninclude fixing failures like false negatives/positives of a learned model or\nmitigating class imbalance in a dataset. The targeted selection task requires\nfinding the relevant data from a large-scale pool of unlabeled data. Manual\nmining at this scale is infeasible. Further, the OOI are often small and occupy\nless than 1% of image area, are occluded, and co-exist with many semantically\ndifferent objects in cluttered scenes. Existing semantic image retrieval\nmethods often focus on mining for larger sized geographical landmarks, and/or\nrequire extra labeled data, such as images/image-pairs with similar objects,\nfor mining images with generic objects. We propose a fast and robust template\nmatching algorithm in the DNN feature space, that retrieves semantically\nsimilar images at the object-level from a large unlabeled pool of data. We\nproject the region(s) around the OOI in the query image to the DNN feature\nspace for use as the template. This enables our method to focus on the\nsemantics of the OOI without requiring extra labeled data. In the context of\nautonomous driving, we evaluate our system for targeted selection by using\nfailure cases of object detectors as OOI. We demonstrate its efficacy on a\nlarge unlabeled dataset with 2.2M images and show high recall in mining for\nimages with small-sized OOI. We compare our method against a well-known\nsemantic image retrieval method, which also does not require extra labeled\ndata. Lastly, we show that our method is flexible and retrieves images with one\nor more semantically different co-occurring OOI seamlessly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kothawade_S/0/1/0/all/0/1\">Suraj Kothawade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_D/0/1/0/all/0/1\">Donna Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fenzi_M/0/1/0/all/0/1\">Michele Fenzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haussmann_E/0/1/0/all/0/1\">Elmar Haussmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alvarez_J/0/1/0/all/0/1\">Jose M. Alvarez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angerer_C/0/1/0/all/0/1\">Christoph Angerer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D Part Assembly Generation with Instance Encoded Transformer. (arXiv:2207.01779v1 [cs.RO])","link":"http://arxiv.org/abs/2207.01779","description":"<p>It is desirable to enable robots capable of automatic assembly. Structural\nunderstanding of object parts plays a crucial role in this task yet remains\nrelatively unexplored. In this paper, we focus on the setting of furniture\nassembly from a complete set of part geometries, which is essentially a 6-DoF\npart pose estimation problem. We propose a multi-layer transformer-based\nframework that involves geometric and relational reasoning between parts to\nupdate the part poses iteratively. We carefully design a unique instance\nencoding to solve the ambiguity between geometrically-similar parts so that all\nparts can be distinguished. In addition to assembling from scratch, we extend\nour framework to a new task called in-process part assembly. Analogous to\nfurniture maintenance, it requires robots to continue with unfinished products\nand assemble the remaining parts into appropriate positions. Our method\nachieves far more than 10% improvements over the current state-of-the-art in\nmultiple metrics on the public PartNet dataset. Extensive experiments and\nquantitative comparisons demonstrate the effectiveness of the proposed\nframework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rufeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_T/0/1/0/all/0/1\">Tao Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weihao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xuan Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_M/0/1/0/all/0/1\">Mingyu You</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A deep cascade of ensemble of dual domain networks with gradient-based T1 assistance and perceptual refinement for fast MRI reconstruction. (arXiv:2207.01791v1 [eess.IV])","link":"http://arxiv.org/abs/2207.01791","description":"<p>Deep learning networks have shown promising results in fast magnetic\nresonance imaging (MRI) reconstruction. In our work, we develop deep networks\nto further improve the quantitative and the perceptual quality of\nreconstruction. To begin with, we propose reconsynergynet (RSN), a network that\ncombines the complementary benefits of independently operating on both the\nimage and the Fourier domain. For a single-coil acquisition, we introduce deep\ncascade RSN (DC-RSN), a cascade of RSN blocks interleaved with data fidelity\n(DF) units. Secondly, we improve the structure recovery of DC-RSN for T2\nweighted Imaging (T2WI) through assistance of T1 weighted imaging (T1WI), a\nsequence with short acquisition time. T1 assistance is provided to DC-RSN\nthrough a gradient of log feature (GOLF) fusion. Furthermore, we propose\nperceptual refinement network (PRN) to refine the reconstructions for better\nvisual information fidelity (VIF), a metric highly correlated to radiologists\nopinion on the image quality. Lastly, for multi-coil acquisition, we propose\nvariable splitting RSN (VS-RSN), a deep cascade of blocks, each block\ncontaining RSN, multi-coil DF unit, and a weighted average module. We\nextensively validate our models DC-RSN and VS-RSN for single-coil and\nmulti-coil acquisitions and report the state-of-the-art performance. We obtain\na SSIM of 0.768, 0.923, 0.878 for knee single-coil-4x, multi-coil-4x, and\nmulti-coil-8x in fastMRI. We also conduct experiments to demonstrate the\nefficacy of GOLF based T1 assistance and PRN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Murugesan_B/0/1/0/all/0/1\">Balamurali Murugesan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ramanarayanan_S/0/1/0/all/0/1\">Sriprabha Ramanarayanan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vijayarangan_S/0/1/0/all/0/1\">Sricharan Vijayarangan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ram_K/0/1/0/all/0/1\">Keerthi Ram</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jagannathan_N/0/1/0/all/0/1\">Naranamangalam R Jagannathan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sivaprakasam_M/0/1/0/all/0/1\">Mohanasankar Sivaprakasam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Task-agnostic Defense against Adversarial Patch Attacks. (arXiv:2207.01795v1 [cs.CV])","link":"http://arxiv.org/abs/2207.01795","description":"<p>Adversarial patch attacks mislead neural networks by injecting adversarial\npixels within a designated local region. Patch attacks can be highly effective\nin a variety of tasks and physically realizable via attachment (e.g. a sticker)\nto the real-world objects. Despite the diversity in attack patterns,\nadversarial patches tend to be highly textured and different in appearance from\nnatural images. We exploit this property and present PatchZero, a task-agnostic\ndefense against white-box adversarial patches. Specifically, our defense\ndetects the adversarial pixels and \"zeros out\" the patch region by repainting\nwith mean pixel values. We formulate the patch detection problem as a semantic\nsegmentation task such that our model can generalize to patches of any size and\nshape. We further design a two-stage adversarial training scheme to defend\nagainst the stronger adaptive attacks. We thoroughly evaluate PatchZero on the\nimage classification (ImageNet, RESISC45), object detection (PASCAL VOC), and\nvideo classification (UCF101) datasets. Our method achieves SOTA robust\naccuracy without any degradation in the benign performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Ke Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yao Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zhaoheng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_K/0/1/0/all/0/1\">Kaijie Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nevatia_R/0/1/0/all/0/1\">Ram Nevatia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Parametric 3D Filters for Joint Video Denoising and Illumination Enhancement in Video Super Resolution. (arXiv:2207.01797v1 [cs.CV])","link":"http://arxiv.org/abs/2207.01797","description":"<p>Despite the quality improvement brought by the recent methods, video\nsuper-resolution (SR) is still very challenging, especially for videos that are\nlow-light and noisy. The current best solution is to subsequently employ best\nmodels of video SR, denoising, and illumination enhancement, but doing so often\nlowers the image quality, due to the inconsistency between the models. This\npaper presents a new parametric representation called the Deep Parametric 3D\nFilters (DP3DF), which incorporates local spatiotemporal information to enable\nsimultaneous denoising, illumination enhancement, and SR efficiently in a\nsingle encoder-and-decoder network. Also, a dynamic residual frame is jointly\nlearned with the DP3DF via a shared backbone to further boost the SR quality.\nWe performed extensive experiments, including a large-scale user study, to show\nour method's effectiveness. Our method consistently surpasses the best\nstate-of-the-art methods on all the challenging real datasets with top PSNR and\nuser ratings, yet having a very fast run time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiaogang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ruixing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1\">Chi-Wing Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\">Jiaya Jia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GSMFlow: Generation Shifts Mitigating Flow for Generalized Zero-Shot Learning. (arXiv:2207.01798v1 [cs.CV])","link":"http://arxiv.org/abs/2207.01798","description":"<p>Generalized Zero-Shot Learning (GZSL) aims to recognize images from both the\nseen and unseen classes by transferring semantic knowledge from seen to unseen\nclasses. It is a promising solution to take the advantage of generative models\nto hallucinate realistic unseen samples based on the knowledge learned from the\nseen classes. However, due to the generation shifts, the synthesized samples by\nmost existing methods may drift from the real distribution of the unseen data.\nTo address this issue, we propose a novel flow-based generative framework that\nconsists of multiple conditional affine coupling layers for learning unseen\ndata generation. Specifically, we discover and address three potential problems\nthat trigger the generation shifts, i.e., semantic inconsistency, variance\ncollapse, and structure disorder. First, to enhance the reflection of the\nsemantic information in the generated samples, we explicitly embed the semantic\ninformation into the transformation in each conditional affine coupling layer.\nSecond, to recover the intrinsic variance of the real unseen features, we\nintroduce a boundary sample mining strategy with entropy maximization to\ndiscover more difficult visual variants of semantic prototypes and hereby\nadjust the decision boundary of the classifiers. Third, a relative positioning\nstrategy is proposed to revise the attribute embeddings, guiding them to fully\npreserve the inter-class geometric structure and further avoid structure\ndisorder in the semantic space. Extensive experimental results on four GZSL\nbenchmark datasets demonstrate that GSMFlow achieves the state-of-the-art\nperformance on GZSL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yadan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_R/0/1/0/all/0/1\">Ruihong Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jingjing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ReMix: A General and Efficient Framework for Multiple Instance Learning based Whole Slide Image Classification. (arXiv:2207.01805v1 [cs.CV])","link":"http://arxiv.org/abs/2207.01805","description":"<p>Whole slide image (WSI) classification often relies on deep weakly supervised\nmultiple instance learning (MIL) methods to handle gigapixel resolution images\nand slide-level labels. Yet the decent performance of deep learning comes from\nharnessing massive datasets and diverse samples, urging the need for efficient\ntraining pipelines for scaling to large datasets and data augmentation\ntechniques for diversifying samples. However, current MIL-based WSI\nclassification pipelines are memory-expensive and computation-inefficient since\nthey usually assemble tens of thousands of patches as bags for computation. On\nthe other hand, despite their popularity in other tasks, data augmentations are\nunexplored for WSI MIL frameworks. To address them, we propose ReMix, a general\nand efficient framework for MIL based WSI classification. It comprises two\nsteps: reduce and mix. First, it reduces the number of instances in WSI bags by\nsubstituting instances with instance prototypes, i.e., patch cluster centroids.\nThen, we propose a ``Mix-the-bag'' augmentation that contains four online,\nstochastic and flexible latent space augmentations. It brings diverse and\nreliable class-identity-preserving semantic changes in the latent space while\nenforcing semantic-perturbation invariance. We evaluate ReMix on two public\ndatasets with two state-of-the-art MIL methods. In our experiments, consistent\nimprovements in precision, accuracy, and recall have been achieved but with\norders of magnitude reduced training time and memory consumption, demonstrating\nReMix's effectiveness and efficiency. Code is available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jiawei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hanbo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Lei He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1\">Jianhua Yao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Aesthetic Attribute Assessment of Images Numerically on Mixed Multi-attribute Datasets. (arXiv:2207.01806v1 [cs.CV])","link":"http://arxiv.org/abs/2207.01806","description":"<p>With the continuous development of social software and multimedia technology,\nimages have become a kind of important carrier for spreading information and\nsocializing. How to evaluate an image comprehensively has become the focus of\nrecent researches. The traditional image aesthetic assessment methods often\nadopt single numerical overall assessment scores, which has certain\nsubjectivity and can no longer meet the higher aesthetic requirements. In this\npaper, we construct an new image attribute dataset called aesthetic mixed\ndataset with attributes(AMD-A) and design external attribute features for\nfusion. Besides, we propose a efficient method for image aesthetic attribute\nassessment on mixed multi-attribute dataset and construct a multitasking\nnetwork architecture by using the EfficientNet-B0 as the backbone network. Our\nmodel can achieve aesthetic classification, overall scoring and attribute\nscoring. In each sub-network, we improve the feature extraction through ECA\nchannel attention module. As for the final overall scoring, we adopt the idea\nof the teacher-student network and use the classification sub-network to guide\nthe aesthetic overall fine-grain regression. Experimental results, using the\nMindSpore, show that our proposed method can effectively improve the\nperformance of the aesthetic overall and attribute assessment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1\">Xin Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xinning Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_H/0/1/0/all/0/1\">Hao Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1\">Chenyu Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Q/0/1/0/all/0/1\">Qiang Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chaoen Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1\">Shuai Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Amit Kumar Singh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deriving Surface Resistivity from Polarimetric SAR Data Using Dual-Input UNet. (arXiv:2207.01811v1 [physics.geo-ph])","link":"http://arxiv.org/abs/2207.01811","description":"<p>Traditional survey methods for finding surface resistivity are time-consuming\nand labor intensive. Very few studies have focused on finding the\nresistivity/conductivity using remote sensing data and deep learning\ntechniques. In this line of work, we assessed the correlation between surface\nresistivity and Synthetic Aperture Radar (SAR) by applying various deep\nlearning methods and tested our hypothesis in the Coso Geothermal Area, USA.\nFor detecting the resistivity, L-band full polarimetric SAR data acquired by\nUAVSAR were used, and MT (Magnetotellurics) inverted resistivity data of the\narea were used as the ground truth. We conducted experiments to compare various\ndeep learning architectures and suggest the use of Dual Input UNet (DI-UNet)\narchitecture. DI-UNet uses a deep learning architecture to predict the\nresistivity using full polarimetric SAR data by promising a quick survey\naddition to the traditional method. Our proposed approach accomplished improved\noutcomes for the mapping of MT resistivity from SAR data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Wilson_B/0/1/0/all/0/1\">Bibin Wilson</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Kumar_R/0/1/0/all/0/1\">Rajiv Kumar</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Bhogapurapu_N/0/1/0/all/0/1\">Narayanarao Bhogapurapu</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Singh_A/0/1/0/all/0/1\">Anand Singh</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Sethi_A/0/1/0/all/0/1\">Amit Sethi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Toward Explainable and Fine-Grained 3D Grounding through Referring Textual Phrases. (arXiv:2207.01821v1 [cs.CV])","link":"http://arxiv.org/abs/2207.01821","description":"<p>Recent progress on 3D scene understanding has explored visual grounding\n(3DVG) to localize a target object through a language description. However,\nexisting methods only consider the dependency between the entire sentence and\nthe target object, thus ignoring fine-grained relationships between contexts\nand non-target ones. In this paper, we extend 3DVG to a more reliable and\nexplainable task, called 3D Phrase Aware Grounding (3DPAG). The 3DPAG task aims\nto localize the target object in the 3D scenes by explicitly identifying all\nphrase-related objects and then conducting reasoning according to contextual\nphrases. To tackle this problem, we label about 400K phrase-level annotations\nfrom 170K sentences in available 3DVG datasets, i.e., Nr3D, Sr3D and ScanRefer.\nBy tapping on these developed datasets, we propose a novel framework, i.e.,\nPhraseRefer, which conducts phrase-aware and object-level representation\nlearning through phrase-object alignment optimization as well as\nphrase-specific pre-training. In our setting, we extend previous 3DVG methods\nto the phrase-aware scenario and provide metrics to measure the explainability\nof the 3DPAG task. Extensive results confirm that 3DPAG effectively boosts the\n3DVG, and PhraseRefer achieves state-of-the-arts across three datasets, i.e.,\n63.0%, 54.4% and 55.5% overall accuracy on Sr3D, Nr3D and ScanRefer,\nrespectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zhihao Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1\">Xu Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhuo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuhao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yao Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1\">Shuguang Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhen Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scene-Aware Prompt for Multi-modal Dialogue Understanding and Generation. (arXiv:2207.01823v1 [cs.CL])","link":"http://arxiv.org/abs/2207.01823","description":"<p>This paper introduces the schemes of Team LingJing's experiments in\nNLPCC-2022-Shared-Task-4 Multi-modal Dialogue Understanding and Generation\n(MDUG). The MDUG task can be divided into two phases: multi-modal context\nunderstanding and response generation. To fully leverage the visual information\nfor both scene understanding and dialogue generation, we propose the\nscene-aware prompt for the MDUG task. Specifically, we utilize the\nmulti-tasking strategy for jointly modelling the scene- and session-\nmulti-modal understanding. The visual captions are adopted to aware the scene\ninformation, while the fixed-type templated prompt based on the scene- and\nsession-aware labels are used to further improve the dialogue generation\nperformance. Extensive experimental results show that the proposed method has\nachieved state-of-the-art (SOTA) performance compared with other competitive\nmethods, where we rank the 1-st in all three subtasks in this MDUG competition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weng_Y/0/1/0/all/0/1\">Yixuan Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Ziyu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1\">Bin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shutao Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Local Implicit Fourier Representation for Image Warping. (arXiv:2207.01831v1 [cs.CV])","link":"http://arxiv.org/abs/2207.01831","description":"<p>Image warping aims to reshape images defined on rectangular grids into\narbitrary shapes. Recently, implicit neural functions have shown remarkable\nperformances in representing images in a continuous manner. However, a\nstandalone multi-layer perceptron suffers from learning high-frequency Fourier\ncoefficients. In this paper, we propose a local texture estimator for image\nwarping (LTEW) followed by an implicit neural representation to deform images\ninto continuous shapes. Local textures estimated from a deep super-resolution\n(SR) backbone are multiplied by locally-varying Jacobian matrices of a\ncoordinate transformation to predict Fourier responses of a warped image. Our\nLTEW-based neural function outperforms existing warping methods for\nasymmetric-scale SR and homography transform. Furthermore, our algorithm well\ngeneralizes arbitrary coordinate transformations, such as homography transform\nwith a large magnification factor and equirectangular projection (ERP)\nperspective transform, which are not provided in training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jaewon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_K/0/1/0/all/0/1\">Kwang Pyo Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_K/0/1/0/all/0/1\">Kyong Hwan Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ORF-Net: Deep Omni-supervised Rib Fracture Detection from Chest CT Scans. (arXiv:2207.01842v1 [cs.CV])","link":"http://arxiv.org/abs/2207.01842","description":"<p>Most of the existing object detection works are based on the bounding box\nannotation: each object has a precise annotated box. However, for rib\nfractures, the bounding box annotation is very labor-intensive and\ntime-consuming because radiologists need to investigate and annotate the rib\nfractures on a slice-by-slice basis. Although a few studies have proposed\nweakly-supervised methods or semi-supervised methods, they could not handle\ndifferent forms of supervision simultaneously. In this paper, we proposed a\nnovel omni-supervised object detection network, which can exploit multiple\ndifferent forms of annotated data to further improve the detection performance.\nSpecifically, the proposed network contains an omni-supervised detection head,\nin which each form of annotation data corresponds to a unique classification\nbranch. Furthermore, we proposed a dynamic label assignment strategy for\ndifferent annotated forms of data to facilitate better learning for each\nbranch. Moreover, we also design a confidence-aware classification loss to\nemphasize the samples with high confidence and further improve the model's\nperformance. Extensive experiments conducted on the testing dataset show our\nproposed method outperforms other state-of-the-art approaches consistently,\ndemonstrating the efficacy of deep omni-supervised learning on improving rib\nfracture detection performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chai_Z/0/1/0/all/0/1\">Zhizhong Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Huangjing Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_L/0/1/0/all/0/1\">Luyang Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heng_P/0/1/0/all/0/1\">Pheng-Ann Heng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Representation Learning via Adaptive Context Pooling. (arXiv:2207.01844v1 [cs.LG])","link":"http://arxiv.org/abs/2207.01844","description":"<p>Self-attention mechanisms model long-range context by using pairwise\nattention between all input tokens. In doing so, they assume a fixed attention\ngranularity defined by the individual tokens (e.g., text characters or image\npixels), which may not be optimal for modeling complex dependencies at higher\nlevels. In this paper, we propose ContextPool to address this problem by\nadapting the attention granularity for each token. Inspired by the success of\nConvNets that are combined with pooling to capture long-range dependencies, we\nlearn to pool neighboring features for each token before computing attention in\na given attention layer. The pooling weights and support size are adaptively\ndetermined, allowing the pooled features to encode meaningful context with\nvarying scale. We show that ContextPool makes attention models more expressive,\nachieving strong performance often with fewer layers and thus significantly\nreduced cost. Experiments validate that our ContextPool module, when plugged\ninto transformer models, matches or surpasses state-of-the-art performance\nusing less compute on several language and image benchmarks, outperforms recent\nworks with learned context sizes or sparse attention patterns, and is also\napplicable to ConvNets for efficient feature learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talbott_W/0/1/0/all/0/1\">Walter Talbott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaitly_N/0/1/0/all/0/1\">Navdeep Jaitly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Susskind_J/0/1/0/all/0/1\">Josh Susskind</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bayesian approaches for Quantifying Clinicians' Variability in Medical Image Quantification. (arXiv:2207.01868v1 [eess.IV])","link":"http://arxiv.org/abs/2207.01868","description":"<p>Medical imaging, including MRI, CT, and Ultrasound, plays a vital role in\nclinical decisions. Accurate segmentation is essential to measure the structure\nof interest from the image. However, manual segmentation is highly\noperator-dependent, which leads to high inter and intra-variability of\nquantitative measurements. In this paper, we explore the feasibility that\nBayesian predictive distribution parameterized by deep neural networks can\ncapture the clinicians' inter-intra variability. By exploring and analyzing\nrecently emerged approximate inference schemes, we evaluate whether approximate\nBayesian deep learning with the posterior over segmentations can learn\ninter-intra rater variability both in segmentation and clinical measurements.\nThe experiments are performed with two different imaging modalities: MRI and\nultrasound. We empirically demonstrated that Bayesian predictive distribution\nparameterized by deep neural networks could approximate the clinicians'\ninter-intra variability. We show a new perspective in analyzing medical images\nquantitatively by providing clinical measurement uncertainty.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Jeon_J/0/1/0/all/0/1\">Jaeik Jeon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jang_Y/0/1/0/all/0/1\">Yeonggul Jang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hong_Y/0/1/0/all/0/1\">Youngtaek Hong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shim_H/0/1/0/all/0/1\">Hackjoon Shim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_S/0/1/0/all/0/1\">Sekeun Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distance Matters in Human-Object Interaction Detection. (arXiv:2207.01869v1 [cs.CV])","link":"http://arxiv.org/abs/2207.01869","description":"<p>Human-Object Interaction (HOI) detection has received considerable attention\nin the context of scene understanding. Despite the growing progress on\nbenchmarks, we realize that existing methods often perform unsatisfactorily on\ndistant interactions, where the leading causes are two-fold: 1) Distant\ninteractions are by nature more difficult to recognize than close ones. A\nnatural scene often involves multiple humans and objects with intricate spatial\nrelations, making the interaction recognition for distant human-object largely\naffected by complex visual context. 2) Insufficient number of distant\ninteractions in benchmark datasets results in under-fitting on these instances.\nTo address these problems, in this paper, we propose a novel two-stage method\nfor better handling distant interactions in HOI detection. One essential\ncomponent in our method is a novel Far Near Distance Attention module. It\nenables information propagation between humans and objects, whereby the spatial\ndistance is skillfully taken into consideration. Besides, we devise a novel\nDistance-Aware loss function which leads the model to focus more on distant yet\nrare interactions. We conduct extensive experiments on two challenging datasets\n- HICO-DET and V-COCO. The results demonstrate that the proposed method can\nsurpass existing approaches by a large margin, resulting in new\nstate-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guangzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yangyang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_Y/0/1/0/all/0/1\">Yongkang Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kankanhalli_M/0/1/0/all/0/1\">Mohan Kankanhalli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Latents2Segments: Disentangling the Latent Space of Generative Models for Semantic Segmentation of Face Images. (arXiv:2207.01871v1 [cs.CV])","link":"http://arxiv.org/abs/2207.01871","description":"<p>With the advent of an increasing number of Augmented and Virtual Reality\napplications that aim to perform meaningful and controlled style edits on\nimages of human faces, the impetus for the task of parsing face images to\nproduce accurate and fine-grained semantic segmentation maps is more than ever\nbefore. Few State of the Art (SOTA) methods which solve this problem, do so by\nincorporating priors with respect to facial structure or other face attributes\nsuch as expression and pose in their deep classifier architecture. Our\nendeavour in this work is to do away with the priors and complex pre-processing\noperations required by SOTA multi-class face segmentation models by reframing\nthis operation as a downstream task post infusion of disentanglement with\nrespect to facial semantic regions of interest (ROIs) in the latent space of a\nGenerative Autoencoder model. We present results for our model's performance on\nthe CelebAMask-HQ and HELEN datasets. The encoded latent space of our model\nachieves significantly higher disentanglement with respect to semantic ROIs\nthan that of other SOTA works. Moreover, it achieves a 13\\% faster inference\nrate and comparable accuracy with respect to the publicly available SOTA for\nthe downstream task of semantic segmentation of face images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tomar_S/0/1/0/all/0/1\">Snehal Singh Tomar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajagopalan_A/0/1/0/all/0/1\">A.N. Rajagopalan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision-based Uneven BEV Representation Learning with Polar Rasterization and Surface Estimation. (arXiv:2207.01878v1 [cs.CV])","link":"http://arxiv.org/abs/2207.01878","description":"<p>In this work, we propose PolarBEV for vision-based uneven BEV representation\nlearning. To adapt to the foreshortening effect of camera imaging, we rasterize\nthe BEV space both angularly and radially, and introduce polar embedding\ndecomposition to model the associations among polar grids. Polar grids are\nrearranged to an array-like regular representation for efficient processing.\nBesides, to determine the 2D-to-3D correspondence, we iteratively update the\nBEV surface based on a hypothetical plane, and adopt height-based feature\ntransformation. PolarBEV keeps real-time inference speed on a single 2080Ti\nGPU, and outperforms other methods for both BEV semantic segmentation and BEV\ninstance segmentation. Thorough ablations are presented to validate the design.\nThe code will be released at \\url{https://github.com/SuperZ-Liu/PolarBEV}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shaoyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xiaojie Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinggang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_T/0/1/0/all/0/1\">Tianheng Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hongmei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MMGL: Multi-Scale Multi-View Global-Local Contrastive learning for Semi-supervised Cardiac Image Segmentation. (arXiv:2207.01883v1 [eess.IV])","link":"http://arxiv.org/abs/2207.01883","description":"<p>With large-scale well-labeled datasets, deep learning has shown significant\nsuccess in medical image segmentation. However, it is challenging to acquire\nabundant annotations in clinical practice due to extensive expertise\nrequirements and costly labeling efforts. Recently, contrastive learning has\nshown a strong capacity for visual representation learning on unlabeled data,\nachieving impressive performance rivaling supervised learning in many domains.\nIn this work, we propose a novel multi-scale multi-view global-local\ncontrastive learning (MMGL) framework to thoroughly explore global and local\nfeatures from different scales and views for robust contrastive learning\nperformance, thereby improving segmentation performance with limited\nannotations. Extensive experiments on the MM-WHS dataset demonstrate the\neffectiveness of MMGL framework on semi-supervised cardiac image segmentation,\noutperforming the state-of-the-art contrastive learning methods by a large\nmargin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhao_Z/0/1/0/all/0/1\">Ziyuan Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_J/0/1/0/all/0/1\">Jinxuan Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zeng_Z/0/1/0/all/0/1\">Zeng Zeng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_X/0/1/0/all/0/1\">Xulei Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qian_P/0/1/0/all/0/1\">Peisheng Qian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Veeravalli_B/0/1/0/all/0/1\">Bharadwaj Veeravalli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guan_C/0/1/0/all/0/1\">Cuntai Guan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Open-Vocabulary Multi-Label Classification via Multi-modal Knowledge Transfer. (arXiv:2207.01887v1 [cs.CV])","link":"http://arxiv.org/abs/2207.01887","description":"<p>Real-world recognition system often encounters a plenty of unseen labels in\npractice. To identify such unseen labels, multi-label zero-shot learning\n(ML-ZSL) focuses on transferring knowledge by a pre-trained textual label\nembedding (e.g., GloVe). However, such methods only exploit singlemodal\nknowledge from a language model, while ignoring the rich semantic information\ninherent in image-text pairs. Instead, recently developed open-vocabulary (OV)\nbased methods succeed in exploiting such information of image-text pairs in\nobject detection, and achieve impressive performance. Inspired by the success\nof OV-based methods, we propose a novel open-vocabulary framework, named\nmultimodal knowledge transfer (MKT), for multi-label classification.\nSpecifically, our method exploits multi-modal knowledge of image-text pairs\nbased on a vision and language pretraining (VLP) model. To facilitate\ntransferring the imagetext matching ability of VLP model, knowledge\ndistillation is used to guarantee the consistency of image and label\nembeddings, along with prompt tuning to further update the label embeddings. To\nfurther recognize multiple objects, a simple but effective two-stream module is\ndeveloped to capture both local and global features. Extensive experimental\nresults show that our method significantly outperforms state-of-theart methods\non public benchmark datasets. Code will be available at\nhttps://github.com/seanhe97/MKT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Sunan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1\">Taian Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_T/0/1/0/all/0/1\">Tao Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_R/0/1/0/all/0/1\">Ruizhi Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_B/0/1/0/all/0/1\">Bo Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1\">Shu-Tao Xia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ACT-Net: Asymmetric Co-Teacher Network for Semi-supervised Memory-efficient Medical Image Segmentation. (arXiv:2207.01900v1 [eess.IV])","link":"http://arxiv.org/abs/2207.01900","description":"<p>While deep models have shown promising performance in medical image\nsegmentation, they heavily rely on a large amount of well-annotated data, which\nis difficult to access, especially in clinical practice. On the other hand,\nhigh-accuracy deep models usually come in large model sizes, limiting their\nemployment in real scenarios. In this work, we propose a novel asymmetric\nco-teacher framework, ACT-Net, to alleviate the burden on both expensive\nannotations and computational costs for semi-supervised knowledge distillation.\nWe advance teacher-student learning with a co-teacher network to facilitate\nasymmetric knowledge distillation from large models to small ones by\nalternating student and teacher roles, obtaining tiny but accurate models for\nclinical employment. To verify the effectiveness of our ACT-Net, we employ the\nACDC dataset for cardiac substructure segmentation in our experiments.\nExtensive experimental results demonstrate that ACT-Net outperforms other\nknowledge distillation methods and achieves lossless segmentation performance\nwith 250x fewer parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhao_Z/0/1/0/all/0/1\">Ziyuan Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_A/0/1/0/all/0/1\">Andong Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zeng_Z/0/1/0/all/0/1\">Zeng Zeng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Veeravalli_B/0/1/0/all/0/1\">Bharadwaj Veeravalli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guan_C/0/1/0/all/0/1\">Cuntai Guan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spatial-Temporal Frequency Forgery Clue for Video Forgery Detection in VIS and NIR Scenario. (arXiv:2207.01906v1 [cs.CV])","link":"http://arxiv.org/abs/2207.01906","description":"<p>In recent years, with the rapid development of face editing and generation,\nmore and more fake videos are circulating on social media, which has caused\nextreme public concerns. Existing face forgery detection methods based on\nfrequency domain find that the GAN forged images have obvious grid-like visual\nartifacts in the frequency spectrum compared to the real images. But for\nsynthesized videos, these methods only confine to single frame and pay little\nattention to the most discriminative part and temporal frequency clue among\ndifferent frames. To take full advantage of the rich information in video\nsequences, this paper performs video forgery detection on both spatial and\ntemporal frequency domains and proposes a Discrete Cosine Transform-based\nForgery Clue Augmentation Network (FCAN-DCT) to achieve a more comprehensive\nspatial-temporal feature representation. FCAN-DCT consists of a backbone\nnetwork and two branches: Compact Feature Extraction (CFE) module and Frequency\nTemporal Attention (FTA) module. We conduct thorough experimental assessments\non two visible light (VIS) based datasets WildDeepfake and Celeb-DF (v2), and\nour self-built video forgery dataset DeepfakeNIR, which is the first video\nforgery dataset on near-infrared modality. The experimental results demonstrate\nthe effectiveness of our method on detecting forgery videos in both VIS and NIR\nscenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yukai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_C/0/1/0/all/0/1\">Chunlei Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Decheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1\">Nannan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xinbo Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StyleFlow For Content-Fixed Image to Image Translation. (arXiv:2207.01909v1 [cs.CV])","link":"http://arxiv.org/abs/2207.01909","description":"<p>Image-to-image (I2I) translation is a challenging topic in computer vision.\nWe divide this problem into three tasks: strongly constrained translation,\nnormally constrained translation, and weakly constrained translation. The\nconstraint here indicates the extent to which the content or semantic\ninformation in the original image is preserved. Although previous approaches\nhave achieved good performance in weakly constrained tasks, they failed to\nfully preserve the content in both strongly and normally constrained tasks,\nincluding photo-realism synthesis, style transfer, and colorization, etc. To\nachieve content-preserving transfer in strongly constrained and normally\nconstrained tasks, we propose StyleFlow, a new I2I translation model that\nconsists of normalizing flows and a novel Style-Aware Normalization (SAN)\nmodule. With the invertible network structure, StyleFlow first projects input\nimages into deep feature space in the forward pass, while the backward pass\nutilizes the SAN module to perform content-fixed feature transformation and\nthen projects back to image space. Our model supports both image-guided\ntranslation and multi-modal synthesis. We evaluate our model in several I2I\ntranslation benchmarks, and the results show that the proposed model has\nadvantages over previous methods in both strongly constrained and normally\nconstrained tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_W/0/1/0/all/0/1\">Weichen Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jinghuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jiabin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1\">Jun Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_S/0/1/0/all/0/1\">Shuai Yi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical Symbolic Reasoning in Hyperbolic Space for Deep Discriminative Models. (arXiv:2207.01916v1 [cs.CV])","link":"http://arxiv.org/abs/2207.01916","description":"<p>Explanations for \\emph{black-box} models help us understand model decisions\nas well as provide information on model biases and inconsistencies. Most of the\ncurrent explainability techniques provide a single level of explanation, often\nin terms of feature importance scores or feature attention maps in input space.\nOur focus is on explaining deep discriminative models at \\emph{multiple levels\nof abstraction}, from fine-grained to fully abstract explanations. We achieve\nthis by using the natural properties of \\emph{hyperbolic geometry} to more\nefficiently model a hierarchy of symbolic features and generate\n\\emph{hierarchical symbolic rules} as part of our explanations. Specifically,\nfor any given deep discriminative model, we distill the underpinning knowledge\nby discretisation of the continuous latent space using vector quantisation to\nform symbols, followed by a \\emph{hyperbolic reasoning block} to induce an\n\\emph{abstraction tree}. We traverse the tree to extract explanations in terms\nof symbolic rules and its corresponding visual semantics. We demonstrate the\neffectiveness of our method on the MNIST and AFHQ high-resolution animal faces\ndataset. Our framework is available at\n\\url{https://github.com/koriavinash1/SymbolicInterpretability}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Santhirasekaram_A/0/1/0/all/0/1\">Ainkaran Santhirasekaram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kori_A/0/1/0/all/0/1\">Avinash Kori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rockall_A/0/1/0/all/0/1\">Andrea Rockall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winkler_M/0/1/0/all/0/1\">Mathias Winkler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toni_F/0/1/0/all/0/1\">Francesca Toni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glocker_B/0/1/0/all/0/1\">Ben Glocker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GLANCE: Global to Local Architecture-Neutral Concept-based Explanations. (arXiv:2207.01917v1 [cs.CV])","link":"http://arxiv.org/abs/2207.01917","description":"<p>Most of the current explainability techniques focus on capturing the\nimportance of features in input space. However, given the complexity of models\nand data-generating processes, the resulting explanations are far from being\n`complete', in that they lack an indication of feature interactions and\nvisualization of their `effect'. In this work, we propose a novel\ntwin-surrogate explainability framework to explain the decisions made by any\nCNN-based image classifier (irrespective of the architecture). For this, we\nfirst disentangle latent features from the classifier, followed by aligning\nthese features to observed/human-defined `context' features. These aligned\nfeatures form semantically meaningful concepts that are used for extracting a\ncausal graph depicting the `perceived' data-generating process, describing the\ninter- and intra-feature interactions between unobserved latent features and\nobserved `context' features. This causal graph serves as a global model from\nwhich local explanations of different forms can be extracted. Specifically, we\nprovide a generator to visualize the `effect' of interactions among features in\nlatent space and draw feature importance therefrom as local explanations. Our\nframework utilizes adversarial knowledge distillation to faithfully learn a\nrepresentation from the classifiers' latent space and use it for extracting\nvisual explanations. We use the styleGAN-v2 architecture with an additional\nregularization term to enforce disentanglement and alignment. We demonstrate\nand evaluate explanations obtained with our framework on Morpho-MNIST and on\nthe FFHQ human faces dataset. Our framework is available at\n\\url{https://github.com/koriavinash1/GLANCE-Explanations}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kori_A/0/1/0/all/0/1\">Avinash Kori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glocker_B/0/1/0/all/0/1\">Ben Glocker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toni_F/0/1/0/all/0/1\">Francesca Toni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vector Quantisation for Robust Segmentation. (arXiv:2207.01919v1 [eess.IV])","link":"http://arxiv.org/abs/2207.01919","description":"<p>The reliability of segmentation models in the medical domain depends on the\nmodel's robustness to perturbations in the input space. Robustness is a\nparticular challenge in medical imaging exhibiting various sources of image\nnoise, corruptions, and domain shifts. Obtaining robustness is often attempted\nvia simulating heterogeneous environments, either heuristically in the form of\ndata augmentation or by learning to generate specific perturbations in an\nadversarial manner. We propose and justify that learning a discrete\nrepresentation in a low dimensional embedding space improves robustness of a\nsegmentation model. This is achieved with a dictionary learning method called\nvector quantisation. We use a set of experiments designed to analyse robustness\nin both the latent and output space under domain shift and noise perturbations\nin the input space. We adapt the popular UNet architecture, inserting a\nquantisation block in the bottleneck. We demonstrate improved segmentation\naccuracy and better robustness on three segmentation tasks. Code is available\nat\n\\url{https://github.com/AinkaranSanthi/Vector-Quantisation-for-Robust-Segmentation}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Santhirasekaram_A/0/1/0/all/0/1\">Ainkaran Santhirasekaram</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kori_A/0/1/0/all/0/1\">Avinash Kori</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Winkler_M/0/1/0/all/0/1\">Mathias Winkler</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rockall_A/0/1/0/all/0/1\">Andrea Rockall</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Glocker_B/0/1/0/all/0/1\">Ben Glocker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FishFormer: Annulus Slicing-based Transformer for Fisheye Rectification with Efficacy Domain Exploration. (arXiv:2207.01925v1 [cs.CV])","link":"http://arxiv.org/abs/2207.01925","description":"<p>Numerous significant progress on fisheye image rectification has been\nachieved through CNN. Nevertheless, constrained by a fixed receptive field, the\nglobal distribution and the local symmetry of the distortion have not been\nfully exploited. To leverage these two characteristics, we introduced\nFishformer that processes the fisheye image as a sequence to enhance global and\nlocal perception. We tuned the Transformer according to the structural\nproperties of fisheye images. First, the uneven distortion distribution in\npatches generated by the existing square slicing method confuses the network,\nresulting in difficult training. Therefore, we propose an annulus slicing\nmethod to maintain the consistency of the distortion in each patch, thus\nperceiving the distortion distribution well. Second, we analyze that different\ndistortion parameters have their own efficacy domains. Hence, the perception of\nthe local area is as important as the global, but Transformer has a weakness\nfor local texture perception. Therefore, we propose a novel layer attention\nmechanism to enhance the local perception and texture transfer. Our network\nsimultaneously implements global perception and focused local perception\ndecided by the different parameters. Extensive experiments demonstrate that our\nmethod provides superior performance compared with state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shangrong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chunyu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_K/0/1/0/all/0/1\">Kang Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yao Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Drone Detection and Tracking in Real-Time by Fusion of Different Sensing Modalities. (arXiv:2207.01927v1 [cs.CV])","link":"http://arxiv.org/abs/2207.01927","description":"<p>Automatic detection of flying drones is a key issue where its presence,\nspecially if unauthorized, can create risky situations or compromise security.\nHere, we design and evaluate a multi-sensor drone detection system. In\nconjunction with common video cameras and microphone sensors, we explore the\nuse of thermal infrared cameras, pointed out as a feasible and promising\nsolution that is scarcely addressed in the related literature. Our solution\nintegrates a fish-eye camera as well to monitor a wider part of the sky and\nsteer the other cameras towards objects of interest. The sensing solutions are\ncomplemented with an ADS-B receiver, a GPS receiver, and a radar module,\nalthough the latter has been not included in our final deployment due to its\nlimited detection range. The thermal camera is shown to be a feasible solution\nas good as the video camera, even if the camera employed here has a lower\nresolution. Two other novelties of our work are the creation of a new public\ndataset of multi-sensor annotated data that expand the number of classes in\ncomparison to existing ones, as well as the study of the detector performance\nas a function of the sensor-to-target distance. Sensor fusion is also explored,\nshowing that the system can be made more robust in this way, mitigating false\ndetections of the individual sensors\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Svanstrom_F/0/1/0/all/0/1\">Fredrik Svanstr&#xf6;m</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alonso_Fernandez_F/0/1/0/all/0/1\">Fernando Alonso-Fernandez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Englund_C/0/1/0/all/0/1\">Cristofer Englund</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image Coding for Machines with Omnipotent Feature Learning. (arXiv:2207.01932v1 [cs.CV])","link":"http://arxiv.org/abs/2207.01932","description":"<p>Image Coding for Machines (ICM) aims to compress images for AI tasks analysis\nrather than meeting human perception. Learning a kind of feature that is both\ngeneral (for AI tasks) and compact (for compression) is pivotal for its\nsuccess. In this paper, we attempt to develop an ICM framework by learning\nuniversal features while also considering compression. We name such features as\nomnipotent features and the corresponding framework as Omni-ICM. Considering\nself-supervised learning (SSL) improves feature generalization, we integrate it\nwith the compression task into the Omni-ICM framework to learn omnipotent\nfeatures. However, it is non-trivial to coordinate semantics modeling in SSL\nand redundancy removing in compression, so we design a novel information\nfiltering (IF) module between them by co-optimization of instance\ndistinguishment and entropy minimization to adaptively drop information that is\nweakly related to AI tasks (e.g., some texture redundancy). Different from\nprevious task-specific solutions, Omni-ICM could directly support AI tasks\nanalysis based on the learned omnipotent features without joint training or\nextra transformation. Albeit simple and intuitive, Omni-ICM significantly\noutperforms existing traditional and learning-based codecs on multiple\nfundamental vision tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_R/0/1/0/all/0/1\">Ruoyu Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1\">Xin Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zongyu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_R/0/1/0/all/0/1\">Runsen Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yixin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1\">Tianyu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhizheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1\">Simeng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhibo Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Safe Semi-supervised Graph Convolution Network. (arXiv:2207.01960v1 [cs.LG])","link":"http://arxiv.org/abs/2207.01960","description":"<p>In the semi-supervised learning field, Graph Convolution Network (GCN), as a\nvariant model of GNN, has achieved promising results for non-Euclidean data by\nintroducing convolution into GNN. However, GCN and its variant models fail to\nsafely use the information of risk unlabeled data, which will degrade the\nperformance of semi-supervised learning. Therefore, we propose a Safe GCN\nframework (Safe-GCN) to improve the learning performance. In the Safe-GCN, we\ndesign an iterative process to label the unlabeled data. In each iteration, a\nGCN and its supervised version(S-GCN) are learned to find the unlabeled data\nwith high confidence. The high-confidence unlabeled data and their pseudo\nlabels are then added to the label set. Finally, both added unlabeled data and\nlabeled ones are used to train a S-GCN which can achieve the safe exploration\nof the risk unlabeled data and enable safe use of large numbers of unlabeled\ndata. The performance of Safe-GCN is evaluated on three well-known citation\nnetwork datasets and the obtained results demonstrate the effectiveness of the\nproposed framework over several graph-based semi-supervised learning methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yadong Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_H/0/1/0/all/0/1\">Haitao Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jing Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Z/0/1/0/all/0/1\">Zhiwei Ye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DualAfford: Learning Collaborative Visual Affordance for Dual-gripper Object Manipulation. (arXiv:2207.01971v1 [cs.CV])","link":"http://arxiv.org/abs/2207.01971","description":"<p>It is essential yet challenging for future home-assistant robots to\nunderstand and manipulate diverse 3D objects in daily human environments.\nTowards building scalable systems that can perform diverse manipulation tasks\nover various 3D shapes, recent works have advocated and demonstrated promising\nresults learning visual actionable affordance, which labels every point over\nthe input 3D geometry with an action likelihood of accomplishing the downstream\ntask (e.g., pushing or picking-up). However, these works only studied\nsingle-gripper manipulation tasks, yet many real-world tasks require two hands\nto achieve collaboratively. In this work, we propose a novel learning\nframework, DualAfford, to learn collaborative affordance for dual-gripper\nmanipulation tasks. The core design of the approach is to reduce the quadratic\nproblem for two grippers into two disentangled yet interconnected subtasks for\nefficient learning. Using the large-scale PartNet-Mobility and ShapeNet\ndatasets, we set up four benchmark tasks for dual-gripper manipulation.\nExperiments prove the effectiveness and superiority of our method over three\nbaselines. Additional results and videos can be found at\nhttps://hyperplane-lab.github.io/DualAfford .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1\">Ruihai Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhehuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yourong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Q/0/1/0/all/0/1\">Qingnan Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mo_K/0/1/0/all/0/1\">Kaichun Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Hao Dong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding and Improving Group Normalization. (arXiv:2207.01972v1 [cs.CV])","link":"http://arxiv.org/abs/2207.01972","description":"<p>Various normalization layers have been proposed to help the training of\nneural networks. Group Normalization (GN) is one of the effective and\nattractive studies that achieved significant performances in the visual\nrecognition task. Despite the great success achieved, GN still has several\nissues that may negatively impact neural network training. In this paper, we\nintroduce an analysis framework and discuss the working principles of GN in\naffecting the training process of the neural network. From experimental\nresults, we conclude the real cause of GN's inferior performance against Batch\nnormalization (BN): 1) \\textbf{unstable training performance}, 2) \\textbf{more\nsensitive} to distortion, whether it comes from external noise or perturbations\nintroduced by the regularization. In addition, we found that GN can only help\nthe neural network training in some specific period, unlike BN, which helps the\nnetwork throughout the training. To solve these issues, we propose a new\nnormalization layer built on top of GN, by incorporating the advantages of BN.\nExperimental results on the image classification task demonstrated that the\nproposed normalization layer outperforms the official GN to improve recognition\naccuracy regardless of the batch sizes and stabilize the network training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gunawan_A/0/1/0/all/0/1\">Agus Gunawan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1\">Xu Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Federated Self-supervised Learning for Video Understanding. (arXiv:2207.01975v1 [cs.CV])","link":"http://arxiv.org/abs/2207.01975","description":"<p>The ubiquity of camera-enabled mobile devices has lead to large amounts of\nunlabelled video data being produced at the edge. Although various\nself-supervised learning (SSL) methods have been proposed to harvest their\nlatent spatio-temporal representations for task-specific training, practical\nchallenges including privacy concerns and communication costs prevent SSL from\nbeing deployed at large scales. To mitigate these issues, we propose the use of\nFederated Learning (FL) to the task of video SSL. In this work, we evaluate the\nperformance of current state-of-the-art (SOTA) video-SSL techniques and\nidentify their shortcomings when integrated into the large-scale FL setting\nsimulated with kinetics-400 dataset. We follow by proposing a novel federated\nSSL framework for video, dubbed FedVSSL, that integrates different aggregation\nstrategies and partial weight updating. Extensive experiments demonstrate the\neffectiveness and significance of FedVSSL as it outperforms the centralized\nSOTA for the downstream retrieval task by 6.66% on UCF-101 and 5.13% on\nHMDB-51.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rehman_Y/0/1/0/all/0/1\">Yasar Abbas Ur Rehman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jiajun Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gusmao_P/0/1/0/all/0/1\">Pedro Porto Buarque de Gusmao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lane_N/0/1/0/all/0/1\">Nicholas Lane</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Open-Vocabulary 3D Detection via Image-level Class and Debiased Cross-modal Contrastive Learning. (arXiv:2207.01987v1 [cs.CV])","link":"http://arxiv.org/abs/2207.01987","description":"<p>Current point-cloud detection methods have difficulty detecting the\nopen-vocabulary objects in the real world, due to their limited generalization\ncapability. Moreover, it is extremely laborious and expensive to collect and\nfully annotate a point-cloud detection dataset with numerous classes of\nobjects, leading to the limited classes of existing point-cloud datasets and\nhindering the model to learn general representations to achieve open-vocabulary\npoint-cloud detection. As far as we know, we are the first to study the problem\nof open-vocabulary 3D point-cloud detection. Instead of seeking a point-cloud\ndataset with full labels, we resort to ImageNet1K to broaden the vocabulary of\nthe point-cloud detector. We propose OV-3DETIC, an Open-Vocabulary 3D DETector\nusing Image-level Class supervision. Specifically, we take advantage of two\nmodalities, the image modality for recognition and the point-cloud modality for\nlocalization, to generate pseudo labels for unseen classes. Then we propose a\nnovel debiased cross-modal contrastive learning method to transfer the\nknowledge from image modality to point-cloud modality during training. Without\nhurting the latency during inference, OV-3DETIC makes the point-cloud detector\ncapable of achieving open-vocabulary detection. Extensive experiments\ndemonstrate that the proposed OV-3DETIC achieves at least 10.77 % mAP\nimprovement (absolute value) and 9.56 % mAP improvement (absolute value) by a\nwide range of baselines on the SUN-RGBD dataset and ScanNet dataset,\nrespectively. Besides, we conduct sufficient experiments to shed light on why\nthe proposed OV-3DETIC works.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yuheng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chenfeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xiaobao Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xiaodong Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tomizuka_M/0/1/0/all/0/1\">Masayoshi Tomizuka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1\">Kurt Keutzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shanghang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multiview Detection with Cardboard Human Modeling. (arXiv:2207.02013v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02013","description":"<p>Multiview detection uses multiple calibrated cameras with overlapping fields\nof views to locate occluded pedestrians. In this field, existing methods\ntypically adopt a ``human modeling - aggregation'' strategy. To find robust\npedestrian representations, some intuitively use locations of detected 2D\nbounding boxes, while others use entire frame features projected to the ground\nplane. However, the former does not consider human appearance and leads to many\nambiguities, and the latter suffers from projection errors due to the lack of\naccurate height of the human torso and head. In this paper, we propose a new\npedestrian representation scheme based on human point clouds modeling.\nSpecifically, using ray tracing for holistic human depth estimation, we model\npedestrians as upright, thin cardboard point clouds on the ground. Then, we\naggregate the point clouds of the pedestrian cardboard across multiple views\nfor a final decision. Compared with existing representations, the proposed\nmethod explicitly leverages human appearance and reduces projection errors\nsignificantly by relatively accurate height estimation. On two standard\nevaluation benchmarks, the proposed method achieves very competitive results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jiahao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_Z/0/1/0/all/0/1\">Zicheng Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yunzhong Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Liang Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1\">Chuong Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepPS2: Revisiting Photometric Stereo Using Two Differently Illuminated Images. (arXiv:2207.02025v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02025","description":"<p>Photometric stereo, a problem of recovering 3D surface normals using images\nof an object captured under different lightings, has been of great interest and\nimportance in computer vision research. Despite the success of existing\ntraditional and deep learning-based methods, it is still challenging due to:\n(i) the requirement of three or more differently illuminated images, (ii) the\ninability to model unknown general reflectance, and (iii) the requirement of\naccurate 3D ground truth surface normals and known lighting information for\ntraining. In this work, we attempt to address an under-explored problem of\nphotometric stereo using just two differently illuminated images, referred to\nas the PS2 problem. It is an intermediate case between a single image-based\nreconstruction method like Shape from Shading (SfS) and the traditional\nPhotometric Stereo (PS), which requires three or more images. We propose an\ninverse rendering-based deep learning framework, called DeepPS2, that jointly\nperforms surface normal, albedo, lighting estimation, and image relighting in a\ncompletely self-supervised manner with no requirement of ground truth data. We\ndemonstrate how image relighting in conjunction with image reconstruction\nenhances the lighting estimation in a self-supervised setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tiwari_A/0/1/0/all/0/1\">Ashish Tiwari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raman_S/0/1/0/all/0/1\">Shanmuganathan Raman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CNN-based Local Vision Transformer for COVID-19 Diagnosis. (arXiv:2207.02027v1 [eess.IV])","link":"http://arxiv.org/abs/2207.02027","description":"<p>Deep learning technology can be used as an assistive technology to help\ndoctors quickly and accurately identify COVID-19 infections. Recently, Vision\nTransformer (ViT) has shown great potential towards image classification due to\nits global receptive field. However, due to the lack of inductive biases\ninherent to CNNs, the ViT-based structure leads to limited feature richness and\ndifficulty in model training. In this paper, we propose a new structure called\nTransformer for COVID-19 (COVT) to improve the performance of ViT-based\narchitectures on small COVID-19 datasets. It uses CNN as a feature extractor to\neffectively extract local structural information, and introduces average\npooling to ViT's Multilayer Perception(MLP) module for global information.\nExperiments show the effectiveness of our method on the two COVID-19 datasets\nand the ImageNet dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Xu_H/0/1/0/all/0/1\">Hongyan Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Su_X/0/1/0/all/0/1\">Xiu Su</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_D/0/1/0/all/0/1\">Dadong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AvatarCap: Animatable Avatar Conditioned Monocular Human Volumetric Capture. (arXiv:2207.02031v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02031","description":"<p>To address the ill-posed problem caused by partial observations in monocular\nhuman volumetric capture, we present AvatarCap, a novel framework that\nintroduces animatable avatars into the capture pipeline for high-fidelity\nreconstruction in both visible and invisible regions. Our method firstly\ncreates an animatable avatar for the subject from a small number (~20) of 3D\nscans as a prior. Then given a monocular RGB video of this subject, our method\nintegrates information from both the image observation and the avatar prior,\nand accordingly recon-structs high-fidelity 3D textured models with dynamic\ndetails regardless of the visibility. To learn an effective avatar for\nvolumetric capture from only few samples, we propose GeoTexAvatar, which\nleverages both geometry and texture supervisions to constrain the\npose-dependent dynamics in a decomposed implicit manner. An avatar-conditioned\nvolumetric capture method that involves a canonical normal fusion and a\nreconstruction network is further proposed to integrate both image observations\nand avatar dynamics for high-fidelity reconstruction in both observed and\ninvisible regions. Overall, our method enables monocular human volumetric\ncapture with detailed and pose-dependent dynamics, and the experiments show\nthat our method outperforms state of the art. Code is available at\nhttps://github.com/lizhe00/AvatarCap.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhe Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zerong Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongwen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_C/0/1/0/all/0/1\">Chaonan Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yebin Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PRoA: A Probabilistic Robustness Assessment against Functional Perturbations. (arXiv:2207.02036v1 [cs.LG])","link":"http://arxiv.org/abs/2207.02036","description":"<p>In safety-critical deep learning applications robustness measurement is a\nvital pre-deployment phase. However, existing robustness verification methods\nare not sufficiently practical for deploying machine learning systems in the\nreal world. On the one hand, these methods attempt to claim that no\nperturbations can ``fool'' deep neural networks (DNNs), which may be too\nstringent in practice. On the other hand, existing works rigorously consider\n$L_p$ bounded additive perturbations on the pixel space, although\nperturbations, such as colour shifting and geometric transformations, are more\npractically and frequently occurring in the real world. Thus, from the\npractical standpoint, we present a novel and general {\\it probabilistic\nrobustness assessment method} (PRoA) based on the adaptive concentration, and\nit can measure the robustness of deep learning models against functional\nperturbations. PRoA can provide statistical guarantees on the probabilistic\nrobustness of a model, \\textit{i.e.}, the probability of failure encountered by\nthe trained model after deployment. Our experiments demonstrate the\neffectiveness and flexibility of PRoA in terms of evaluating the probabilistic\nrobustness against a broad range of functional perturbations, and PRoA can\nscale well to various large-scale deep neural networks compared to existing\nstate-of-the-art baselines. For the purpose of reproducibility, we release our\ntool on GitHub: \\url{ https://github.com/TrustAI/PRoA}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianle Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruan_W/0/1/0/all/0/1\">Wenjie Ruan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fieldsend_J/0/1/0/all/0/1\">Jonathan E. Fieldsend</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PKD: General Distillation Framework for Object Detectors via Pearson Correlation Coefficient. (arXiv:2207.02039v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02039","description":"<p>Knowledge distillation(KD) is a widely-used technique to train compact models\nin object detection. However, there is still a lack of study on how to distill\nbetween heterogeneous detectors. In this paper, we empirically find that better\nFPN features from a heterogeneous teacher detector can help the student\nalthough their detection heads and label assignments are different. However,\ndirectly aligning the feature maps to distill detectors suffers from two\nproblems. First, the difference in feature magnitude between the teacher and\nthe student could enforce overly strict constraints on the student. Second, the\nFPN stages and channels with large feature magnitude from the teacher model\ncould dominate the gradient of distillation loss, which will overwhelm the\neffects of other features in KD and introduce much noise. To address the above\nissues, we propose to imitate features with Pearson Correlation Coefficient to\nfocus on the relational information from the teacher and relax constraints on\nthe magnitude of the features. Our method consistently outperforms the existing\ndetection KD methods and works for both homogeneous and heterogeneous\nstudent-teacher pairs. Furthermore, it converges faster. With a powerful\nMaskRCNN-Swin detector as the teacher, ResNet-50 based RetinaNet and FCOS\nachieve 41.5% and 43.9% mAP on COCO2017, which are 4.1\\% and 4.8\\% higher than\nthe baseline, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_W/0/1/0/all/0/1\">Weihan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yifan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfei Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_A/0/1/0/all/0/1\">Anda Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_K/0/1/0/all/0/1\">Ke Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1\">Jian Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MVP: Robust Multi-View Practice for Driving Action Localization. (arXiv:2207.02042v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02042","description":"<p>Distracted driving causes thousands of deaths per year, and how to apply\ndeep-learning methods to prevent these tragedies has become a crucial problem.\nIn Track3 of the 6th AI City Challenge, researchers provide a high-quality\nvideo dataset with densely action annotations. Due to the small data scale and\nunclear action boundary, the dataset presents a unique challenge to precisely\nlocalize all the different actions and classify their categories. In this\npaper, we make good use of the multi-view synchronization among videos, and\nconduct robust Multi-View Practice (MVP) for driving action localization. To\navoid overfitting, we fine-tune SlowFast with Kinetics-700 pre-training as the\nfeature extractor. Then the features of different views are passed to\nActionFormer to generate candidate action proposals. For precisely localizing\nall the actions, we design elaborate post-processing, including model voting,\nthreshold filtering and duplication removal. The results show that our MVP is\nrobust for driving action localization, which achieves 28.49% F1-score in the\nTrack3 test set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shang_J/0/1/0/all/0/1\">Jingjie Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kunchang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_K/0/1/0/all/0/1\">Kaibin Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Haisheng Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yangguang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformer based Models for Unsupervised Anomaly Segmentation in Brain MR Images. (arXiv:2207.02059v1 [eess.IV])","link":"http://arxiv.org/abs/2207.02059","description":"<p>The quality of patient care associated with diagnostic radiology is\nproportionate to a physician workload. Segmentation is a fundamental limiting\nprecursor to diagnostic and therapeutic procedures. Advances in Machine\nLearning (ML) aim to increase diagnostic efficiency to replace single\napplication with generalized algorithms. In Unsupervised Anomaly Detection\n(UAD), Convolutional Neural Network (CNN) based Autoencoders (AEs) and\nVariational Autoencoders (VAEs) are considered as a de facto approach for\nreconstruction based anomaly segmentation. Looking for anomalous regions in\nmedical images is one of the main applications that use anomaly segmentation.\nThe restricted receptive field in CNNs limit the CNN to model the global\ncontext and hence if the anomalous regions cover parts of the image, the\nCNN-based AEs are not capable to bring semantic understanding of the image. On\nthe other hand, Vision Transformers (ViTs) have emerged as a competitive\nalternative to CNNs. It relies on the self-attention mechanism that is capable\nto relate image patches to each other. To reconstruct a coherent and more\nrealistic image, in this work, we investigate Transformer capabilities in\nbuilding AEs for reconstruction based UAD task. We focus on anomaly\nsegmentation for Brain Magnetic Resonance Imaging (MRI) and present five\nTransformer-based models while enabling segmentation performance comparable or\nsuperior to State-of-The-Art (SOTA) models. The source code is available on\nGithub\nhttps://github.com/ahmedgh970/Transformers_Unsupervised_Anomaly_Segmentation.git\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ghorbel_A/0/1/0/all/0/1\">Ahmed Ghorbel</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Aldahdooh_A/0/1/0/all/0/1\">Ahmed Aldahdooh</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Albarqouni_S/0/1/0/all/0/1\">Shadi Albarqouni</a> (2), <a href=\"http://arxiv.org/find/eess/1/au:+Hamidouche_W/0/1/0/all/0/1\">Wassim Hamidouche</a> (1) ((1) Univ. Rennes, INSA Rennes, CNRS, IETR - UMR 6164, Rennes, France (2) University Hospital Bonn, Venusberg-Campus 1, D-53127, Bonn, Germany, Helmholtz Munich, Ingolst&#xe4;dter Landstra&#xdf;e 1, D-85764, Neuherberg, Germany, Technical University of Munich, Boltzmannstr. 3, D-85748 Garching, Germany)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image Amodal Completion: A Survey. (arXiv:2207.02062v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02062","description":"<p>Existing computer vision systems can compete with humans in understanding the\nvisible parts of objects, but still fall far short of humans when it comes to\ndepicting the invisible parts of partially occluded objects. Image amodal\ncompletion aims to equip computers with human-like amodal completion functions\nto understand an intact object despite it being partially occluded. The main\npurpose of this survey is to provide an intuitive understanding of the research\nhotspots, key technologies and future trends in the field of image amodal\ncompletion. Firstly, we present a comprehensive review of the latest literature\nin this emerging field, exploring three key tasks in image amodal completion,\nincluding amodal shape completion, amodal appearance completion, and order\nperception. Then we examine popular datasets related to image amodal completion\nalong with their common data collection methods and evaluation metrics.\nFinally, we discuss real-world applications and future research directions for\nimage amodal completion, facilitating the reader's understanding of the\nchallenges of existing technologies and upcoming research trends.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ao_J/0/1/0/all/0/1\">Jiayang Ao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ehinger_K/0/1/0/all/0/1\">Krista A. Ehinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ke_Q/0/1/0/all/0/1\">Qiuhong Ke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RepMix: Representation Mixing for Robust Attribution of Synthesized Images. (arXiv:2207.02063v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02063","description":"<p>Rapid advances in Generative Adversarial Networks (GANs) raise new challenges\nfor image attribution; detecting whether an image is synthetic and, if so,\ndetermining which GAN architecture created it. Uniquely, we present a solution\nto this task capable of 1) matching images invariant to their semantic content;\n2) robust to benign transformations (changes in quality, resolution, shape,\netc.) commonly encountered as images are re-shared online. In order to\nformalize our research, a challenging benchmark, Attribution88, is collected\nfor robust and practical image attribution. We then propose RepMix, our GAN\nfingerprinting technique based on representation mixing and a novel loss. We\nvalidate its capability of tracing the provenance of GAN-generated images\ninvariant to the semantic content of the image and also robust to\nperturbations. We show our approach improves significantly from existing GAN\nfingerprinting works on both semantic generalization and robustness. Data and\ncode are available at https://github.com/TuBui/image_attribution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1\">Tu Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_N/0/1/0/all/0/1\">Ning Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collomosse_J/0/1/0/all/0/1\">John Collomosse</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Test-time Adaptation for Real Image Denoising via Meta-transfer Learning. (arXiv:2207.02066v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02066","description":"<p>In recent years, a ton of research has been conducted on real image denoising\ntasks. However, the efforts are more focused on improving real image denoising\nthrough creating a better network architecture. We explore a different\ndirection where we propose to improve real image denoising performance through\na better learning strategy that can enable test-time adaptation on the\nmulti-task network. The learning strategy is two stages where the first stage\npre-train the network using meta-auxiliary learning to get better\nmeta-initialization. Meanwhile, we use meta-learning for fine-tuning\n(meta-transfer learning) the network as the second stage of our training to\nenable test-time adaptation on real noisy images. To exploit a better learning\nstrategy, we also propose a network architecture with self-supervised masked\nreconstruction loss. Experiments on a real noisy dataset show the contribution\nof the proposed method and show that the proposed method can outperform other\nSOTA methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gunawan_A/0/1/0/all/0/1\">Agus Gunawan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nugroho_M/0/1/0/all/0/1\">Muhammad Adi Nugroho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Se Jin Park</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Densely Interconnected Network for Deep Learning Accelerated MRI. (arXiv:2207.02073v1 [eess.IV])","link":"http://arxiv.org/abs/2207.02073","description":"<p>Objective: To improve accelerated MRI reconstruction through a densely\nconnected cascading deep learning reconstruction framework.\n</p>\n<p>Materials and Methods: A cascading deep learning reconstruction framework\n(baseline model) was modified by applying three architectural modifications:\nInput-level dense connections between cascade inputs and outputs, an improved\ndeep learning sub-network, and long-range skip-connections between subsequent\ndeep learning networks. An ablation study was performed, where five model\nconfigurations were trained on the NYU fastMRI neuro dataset with an end-to-end\nscheme conjunct on four- and eight-fold acceleration. The trained models were\nevaluated by comparing their respective structural similarity index measure\n(SSIM), normalized mean square error (NMSE) and peak signal to noise ratio\n(PSNR).\n</p>\n<p>Results: The proposed densely interconnected residual cascading network\n(DIRCN), utilizing all three suggested modifications, achieved a SSIM\nimprovement of 8% and 11% for four- and eight-fold acceleration, respectively.\nFor eight-fold acceleration, the model achieved a 23% decrease in the NMSE when\ncompared to the baseline model. In an ablation study, the individual\narchitectural modifications all contributed to this improvement, by reducing\nthe SSIM and NMSE with approximately 3% and 5% for four-fold acceleration,\nrespectively.\n</p>\n<p>Conclusion: The proposed architectural modifications allow for simple\nadjustments on an already existing cascading framework to further improve the\nresulting reconstructions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ottesen_J/0/1/0/all/0/1\">Jon Andre Ottesen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Caan_M/0/1/0/all/0/1\">Matthan W.A. Caan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Groote_I/0/1/0/all/0/1\">Inge Rasmus Groote</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bjornerud_A/0/1/0/all/0/1\">Atle Bj&#xf8;rnerud</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SiamMask: A Framework for Fast Online Object Tracking and Segmentation. (arXiv:2207.02088v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02088","description":"<p>In this paper we introduce SiamMask, a framework to perform both visual\nobject tracking and video object segmentation, in real-time, with the same\nsimple method. We improve the offline training procedure of popular\nfully-convolutional Siamese approaches by augmenting their losses with a binary\nsegmentation task. Once the offline training is completed, SiamMask only\nrequires a single bounding box for initialization and can simultaneously carry\nout visual object tracking and segmentation at high frame-rates. Moreover, we\nshow that it is possible to extend the framework to handle multiple object\ntracking and segmentation by simply re-using the multi-task model in a cascaded\nfashion. Experimental results show that our approach has high processing\nefficiency, at around 55 frames per second. It yields real-time\nstate-of-the-art results on visual-object tracking benchmarks, while at the\nsame time demonstrating competitive performance at a high speed for video\nobject segmentation benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Weiming Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bertinetto_L/0/1/0/all/0/1\">Luca Bertinetto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip H.S. Torr</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CASHformer: Cognition Aware SHape Transformer for Longitudinal Analysis. (arXiv:2207.02091v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02091","description":"<p>Modeling temporal changes in subcortical structures is crucial for a better\nunderstanding of the progression of Alzheimer's disease (AD). Given their\nflexibility to adapt to heterogeneous sequence lengths, mesh-based transformer\narchitectures have been proposed in the past for predicting hippocampus\ndeformations across time. However, one of the main limitations of transformers\nis the large amount of trainable parameters, which makes the application on\nsmall datasets very challenging. In addition, current methods do not include\nrelevant non-image information that can help to identify AD-related patterns in\nthe progression. To this end, we introduce CASHformer, a transformer-based\nframework to model longitudinal shape trajectories in AD. CASHformer\nincorporates the idea of pre-trained transformers as universal compute engines\nthat generalize across a wide range of tasks by freezing most layers during\nfine-tuning. This reduces the number of parameters by over 90% with respect to\nthe original model and therefore enables the application of large models on\nsmall datasets without overfitting. In addition, CASHformer models cognitive\ndecline to reveal AD atrophy patterns in the temporal sequence. Our results\nshow that CASHformer reduces the reconstruction error by 73% compared to\npreviously proposed methods. Moreover, the accuracy of detecting patients\nprogressing to AD increases by 3% with imputing missing longitudinal shape\ndata.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sarasua_I/0/1/0/all/0/1\">Ignacio Sarasua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polsterl_S/0/1/0/all/0/1\">Sebastian P&#xf6;lsterl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wachinger_C/0/1/0/all/0/1\">Christian Wachinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is a PET all you need? A multi-modal study for Alzheimer's disease using 3D CNNs. (arXiv:2207.02094v1 [eess.IV])","link":"http://arxiv.org/abs/2207.02094","description":"<p>Alzheimer's Disease (AD) is the most common form of dementia and often\ndifficult to diagnose due to the multifactorial etiology of dementia. Recent\nworks on neuroimaging-based computer-aided diagnosis with deep neural networks\n(DNNs) showed that fusing structural magnetic resonance images (sMRI) and\nfluorodeoxyglucose positron emission tomography (FDG-PET) leads to improved\naccuracy in a study population of healthy controls and subjects with AD.\nHowever, this result conflicts with the established clinical knowledge that\nFDG-PET better captures AD-specific pathologies than sMRI. Therefore, we\npropose a framework for the systematic evaluation of multi-modal DNNs and\ncritically re-evaluate single- and multi-modal DNNs based on FDG-PET and sMRI\nfor binary healthy vs. AD, and three-way healthy/mild cognitive impairment/AD\nclassification. Our experiments demonstrate that a single-modality network\nusing FDG-PET performs better than MRI (accuracy 0.91 vs 0.87) and does not\nshow improvement when combined. This conforms with the established clinical\nknowledge on AD biomarkers, but raises questions about the true benefit of\nmulti-modal DNNs. We argue that future work on multi-modal fusion should\nsystematically assess the contribution of individual modalities following our\nproposed evaluation framework. Finally, we encourage the community to go beyond\nhealthy vs. AD classification and focus on differential diagnosis of dementia,\nwhere fusing multi-modal image information conforms with a clinical need.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Narazani_M/0/1/0/all/0/1\">Marla Narazani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sarasua_I/0/1/0/all/0/1\">Ignacio Sarasua</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Polsterl_S/0/1/0/all/0/1\">Sebastian P&#xf6;lsterl</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lizarraga_A/0/1/0/all/0/1\">Aldana Lizarraga</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yakushev_I/0/1/0/all/0/1\">Igor Yakushev</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wachinger_C/0/1/0/all/0/1\">Christian Wachinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Covariance Conditioning of the SVD Meta-layer by Orthogonality. (arXiv:2207.02119v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02119","description":"<p>Inserting an SVD meta-layer into neural networks is prone to make the\ncovariance ill-conditioned, which could harm the model in the training\nstability and generalization abilities. In this paper, we systematically study\nhow to improve the covariance conditioning by enforcing orthogonality to the\nPre-SVD layer. Existing orthogonal treatments on the weights are first\ninvestigated. However, these techniques can improve the conditioning but would\nhurt the performance. To avoid such a side effect, we propose the Nearest\nOrthogonal Gradient (NOG) and Optimal Learning Rate (OLR). The effectiveness of\nour methods is validated in two applications: decorrelated Batch Normalization\n(BN) and Global Covariance Pooling (GCP). Extensive experiments on visual\nrecognition demonstrate that our methods can simultaneously improve the\ncovariance conditioning and generalization. Moreover, the combinations with\northogonal weight can further boost the performances.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yue Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1\">Nicu Sebe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Semantic Segmentation in Transformers using Hierarchical Inter-Level Attention. (arXiv:2207.02126v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02126","description":"<p>Existing transformer-based image backbones typically propagate feature\ninformation in one direction from lower to higher-levels. This may not be ideal\nsince the localization ability to delineate accurate object boundaries, is most\nprominent in the lower, high-resolution feature maps, while the semantics that\ncan disambiguate image signals belonging to one object vs. another, typically\nemerges in a higher level of processing. We present Hierarchical Inter-Level\nAttention (HILA), an attention-based method that captures Bottom-Up and\nTop-Down Updates between features of different levels. HILA extends\nhierarchical vision transformer architectures by adding local connections\nbetween features of higher and lower levels to the backbone encoder. In each\niteration, we construct a hierarchy by having higher-level features compete for\nassignments to update lower-level features belonging to them, iteratively\nresolving object-part relationships. These improved lower-level features are\nthen used to re-update the higher-level features. HILA can be integrated into\nthe majority of hierarchical architectures without requiring any changes to the\nbase model. We add HILA into SegFormer and the Swin Transformer and show\nnotable improvements in accuracy in semantic segmentation with fewer parameters\nand FLOPS. Project website and code:\nhttps://www.cs.toronto.edu/~garyleung/hila/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Leung_G/0/1/0/all/0/1\">Gary Leung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jun Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1\">Xiaohui Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fidler_S/0/1/0/all/0/1\">Sanja Fidler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning for Finger Vein Recognition: A Brief Survey of Recent Trend. (arXiv:2207.02148v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02148","description":"<p>Finger vein image recognition technology plays an important role in biometric\nrecognition and has been successfully applied in many fields. Because veins are\nburied beneath the skin tissue, finger vein image recognition has an\nunparalleled advantage, which is not easily disturbed by external factors. This\nreview summarizes 46 papers about deep learning for finger vein image\nrecognition from 2017 to 2021. These papers are summarized according to the\ntasks of deep neural networks. Besides, we present the challenges and potential\ndevelopment directions of finger vein image recognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Renye Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1\">Yimin Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_W/0/1/0/all/0/1\">Wanxia Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jinghua Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Class-Specific Semantic Reconstruction for Open Set Recognition. (arXiv:2207.02158v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02158","description":"<p>Open set recognition enables deep neural networks (DNNs) to identify samples\nof unknown classes, while maintaining high classification accuracy on samples\nof known classes. Existing methods basing on auto-encoder (AE) and prototype\nlearning show great potential in handling this challenging task. In this study,\nwe propose a novel method, called Class-Specific Semantic Reconstruction\n(CSSR), that integrates the power of AE and prototype learning. Specifically,\nCSSR replaces prototype points with manifolds represented by class-specific\nAEs. Unlike conventional prototype-based methods, CSSR models each known class\non an individual AE manifold, and measures class belongingness through AE's\nreconstruction error. Class-specific AEs are plugged into the top of the DNN\nbackbone and reconstruct the semantic representations learned by the DNN\ninstead of the raw image. Through end-to-end learning, the DNN and the AEs\nboost each other to learn both discriminative and representative information.\nThe results of experiments conducted on multiple datasets show that the\nproposed method achieves outstanding performance in both close and open set\nrecognition and is sufficiently simple and flexible to incorporate into\nexisting frameworks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hongzhi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1\">Qinghua Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1\">Ming-Ming Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-modal Robustness Analysis Against Language and Visual Perturbations. (arXiv:2207.02159v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02159","description":"<p>Joint visual and language modeling on large-scale datasets has recently shown\na good progress in multi-modal tasks when compared to single modal learning.\nHowever, robustness of these approaches against real-world perturbations has\nnot been studied. In this work, we perform the first extensive robustness study\nof such models against various real-world perturbations focusing on video and\nlanguage. We focus on text-to-video retrieval and propose two large-scale\nbenchmark datasets, MSRVTT-P and YouCook2-P, which utilize 90 different visual\nand 35 different textual perturbations. The study reveals some interesting\nfindings: 1) The studied models are more robust when text is perturbed versus\nwhen video is perturbed 2) The transformer text encoder is more robust on\nnon-semantic changing text perturbations and visual perturbations compared to\nword embedding approaches. 3) Using two-branch encoders in isolation is\ntypically more robust than when architectures use cross-attention. We hope this\nstudy will serve as a benchmark and guide future research in robust multimodal\nlearning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schiappa_M/0/1/0/all/0/1\">Madeline C. Schiappa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rawat_Y/0/1/0/all/0/1\">Yogesh S. Rawat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vyas_S/0/1/0/all/0/1\">Shruti Vyas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vineet_V/0/1/0/all/0/1\">Vibhav Vineet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palangi_H/0/1/0/all/0/1\">Hamid Palangi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic inspection of cultural monuments using deep and tensor-based learning on hyperspectral imagery. (arXiv:2207.02163v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02163","description":"<p>In Cultural Heritage, hyperspectral images are commonly used since they\nprovide extended information regarding the optical properties of materials.\nThus, the processing of such high-dimensional data becomes challenging from the\nperspective of machine learning techniques to be applied. In this paper, we\npropose a Rank-$R$ tensor-based learning model to identify and classify\nmaterial defects on Cultural Heritage monuments. In contrast to conventional\ndeep learning approaches, the proposed high order tensor-based learning\ndemonstrates greater accuracy and robustness against overfitting. Experimental\nresults on real-world data from UNESCO protected areas indicate the superiority\nof the proposed scheme compared to conventional deep learning models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tzortzis_I/0/1/0/all/0/1\">Ioannis N. Tzortzis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rallis_I/0/1/0/all/0/1\">Ioannis Rallis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makantasis_K/0/1/0/all/0/1\">Konstantinos Makantasis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doulamis_A/0/1/0/all/0/1\">Anastasios Doulamis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doulamis_N/0/1/0/all/0/1\">Nikolaos Doulamis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Voulodimos_A/0/1/0/all/0/1\">Athanasios Voulodimos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DBN-Mix: Training Dual Branch Network Using Bilateral Mixup Augmentation for Long-Tailed Visual Recognition. (arXiv:2207.02173v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02173","description":"<p>There is a growing interest in the challenging visual perception task of\nlearning from long-tailed class distributions. The extreme class imbalance in\nthe training dataset biases the model to prefer to recognize majority-class\ndata over minority-class data. Recently, the dual branch network (DBN)\nframework has been proposed, where two branch networks; the conventional branch\nand the re-balancing branch were employed to improve the accuracy of\nlong-tailed visual recognition. The re-balancing branch uses a reverse sampler\nto generate class-balanced training samples to mitigate bias due to class\nimbalance. Although this strategy has been quite successful in handling bias,\nusing a reversed sampler for training can degrade the representation learning\nperformance. To alleviate this issue, the conventional method used a carefully\ndesigned cumulative learning strategy, in which the influence of the\nre-balancing branch gradually increases throughout the entire training phase.\nIn this study, we aim to develop a simple yet effective method to improve the\nperformance of DBN without cumulative learning that is difficult to optimize.\nWe devise a simple data augmentation method termed bilateral mixup\naugmentation, which combines one sample from the uniform sampler with another\nsample from the reversed sampler to produce a training sample. Furthermore, we\npresent class-conditional temperature scaling that mitigates bias toward the\nmajority class for the proposed DBN architecture. Our experiments performed on\nwidely used long-tailed visual recognition datasets show that bilateral mixup\naugmentation is quite effective in improving the representation learning\nperformance of DBNs, and that the proposed method achieves state-of-the-art\nperformance for some categories.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baik_J/0/1/0/all/0/1\">Jae Soon Baik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_I/0/1/0/all/0/1\">In Young Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jun Won Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Activation Template Matching Loss for Explainable Face Recognition. (arXiv:2207.02179v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02179","description":"<p>Can we construct an explainable face recognition network able to learn a\nfacial part-based feature like eyes, nose, mouth and so forth, without any\nmanual annotation or additionalsion datasets? In this paper, we propose a\ngeneric Explainable Channel Loss (ECLoss) to construct an explainable face\nrecognition network. The explainable network trained with ECLoss can easily\nlearn the facial part-based representation on the target convolutional layer,\nwhere an individual channel can detect a certain face part. Our experiments on\ndozens of datasets show that ECLoss achieves superior explainability metrics,\nand at the same time improves the performance of face verification without face\nalignment. In addition, our visualization results also illustrate the\neffectiveness of the proposed ECLoss.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Huawei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Haozhe Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qiufu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Linlin Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ST-CoNAL: Consistency-Based Acquisition Criterion Using Temporal Self-Ensemble for Active Learning. (arXiv:2207.02182v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02182","description":"<p>Modern deep learning has achieved great success in various fields. However,\nit requires the labeling of huge amounts of data, which is expensive and\nlabor-intensive. Active learning (AL), which identifies the most informative\nsamples to be labeled, is becoming increasingly important to maximize the\nefficiency of the training process. The existing AL methods mostly use only a\nsingle final fixed model for acquiring the samples to be labeled. This strategy\nmay not be good enough in that the structural uncertainty of a model for given\ntraining data is not considered to acquire the samples. In this study, we\npropose a novel acquisition criterion based on temporal self-ensemble generated\nby conventional stochastic gradient descent (SGD) optimization. These\nself-ensemble models are obtained by capturing the intermediate network weights\nobtained through SGD iterations. Our acquisition function relies on a\nconsistency measure between the student and teacher models. The student models\nare given a fixed number of temporal self-ensemble models, and the teacher\nmodel is constructed by averaging the weights of the student models. Using the\nproposed acquisition criterion, we present an AL algorithm, namely\nstudent-teacher consistency-based AL (ST-CoNAL). Experiments conducted for\nimage classification tasks on CIFAR-10, CIFAR-100, Caltech-256, and Tiny\nImageNet datasets demonstrate that the proposed ST-CoNAL achieves significantly\nbetter performance than the existing acquisition methods. Furthermore,\nextensive experiments show the robustness and effectiveness of our methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baik_J/0/1/0/all/0/1\">Jae Soon Baik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_I/0/1/0/all/0/1\">In Young Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jun Won Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLEAR: Improving Vision-Language Navigation with Cross-Lingual, Environment-Agnostic Representations. (arXiv:2207.02185v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02185","description":"<p>Vision-and-Language Navigation (VLN) tasks require an agent to navigate\nthrough the environment based on language instructions. In this paper, we aim\nto solve two key challenges in this task: utilizing multilingual instructions\nfor improved instruction-path grounding and navigating through new environments\nthat are unseen during training. To address these challenges, we propose CLEAR:\nCross-Lingual and Environment-Agnostic Representations. First, our agent learns\na shared and visually-aligned cross-lingual language representation for the\nthree languages (English, Hindi and Telugu) in the Room-Across-Room dataset.\nOur language representation learning is guided by text pairs that are aligned\nby visual information. Second, our agent learns an environment-agnostic visual\nrepresentation by maximizing the similarity between semantically-aligned image\npairs (with constraints on object-matching) from different environments. Our\nenvironment agnostic visual representation can mitigate the environment bias\ninduced by low-level visual information. Empirically, on the Room-Across-Room\ndataset, we show that our multilingual agent gets large improvements in all\nmetrics over the strong baseline model when generalizing to unseen environments\nwith the cross-lingual language representation and the environment-agnostic\nvisual representation. Furthermore, we show that our learned language and\nvisual representations can be successfully transferred to the Room-to-Room and\nCooperative Vision-and-Dialogue Navigation task, and present detailed\nqualitative and quantitative generalization and grounding analysis. Our code is\navailable at https://github.com/jialuli-luka/CLEAR\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jialu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1\">Hao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NeuralPassthrough: Learned Real-Time View Synthesis for VR. (arXiv:2207.02186v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02186","description":"<p>Virtual reality (VR) headsets provide an immersive, stereoscopic visual\nexperience, but at the cost of blocking users from directly observing their\nphysical environment. Passthrough techniques are intended to address this\nlimitation by leveraging outward-facing cameras to reconstruct the images that\nwould otherwise be seen by the user without the headset. This is inherently a\nreal-time view synthesis challenge, since passthrough cameras cannot be\nphysically co-located with the eyes. Existing passthrough techniques suffer\nfrom distracting reconstruction artifacts, largely due to the lack of accurate\ndepth information (especially for near-field and disoccluded objects), and also\nexhibit limited image quality (e.g., being low resolution and monochromatic).\nIn this paper, we propose the first learned passthrough method and assess its\nperformance using a custom VR headset that contains a stereo pair of RGB\ncameras. Through both simulations and experiments, we demonstrate that our\nlearned passthrough method delivers superior image quality compared to\nstate-of-the-art methods, while meeting strict VR requirements for real-time,\nperspective-correct stereoscopic view synthesis over a wide field of view for\ndesktop-connected headsets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_L/0/1/0/all/0/1\">Lei Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nouri_S/0/1/0/all/0/1\">Salah Nouri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hegland_J/0/1/0/all/0/1\">Joel Hegland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_A/0/1/0/all/0/1\">Alberto Garcia Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lanman_D/0/1/0/all/0/1\">Douglas Lanman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Accelerating Score-based Generative Models with Preconditioned Diffusion Sampling. (arXiv:2207.02196v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02196","description":"<p>Score-based generative models (SGMs) have recently emerged as a promising\nclass of generative models. However, a fundamental limitation is that their\ninference is very slow due to a need for many (e.g., 2000) iterations of\nsequential computations. An intuitive acceleration method is to reduce the\nsampling iterations which however causes severe performance degradation. We\ninvestigate this problem by viewing the diffusion sampling process as a\nMetropolis adjusted Langevin algorithm, which helps reveal the underlying cause\nto be ill-conditioned curvature. Under this insight, we propose a\nmodel-agnostic preconditioned diffusion sampling (PDS) method that leverages\nmatrix preconditioning to alleviate the aforementioned problem. Crucially, PDS\nis proven theoretically to converge to the original target distribution of a\nSGM, no need for retraining. Extensive experiments on three image datasets with\na variety of resolutions and diversity validate that PDS consistently\naccelerates off-the-shelf SGMs whilst maintaining the synthesis quality. In\nparticular, PDS can accelerate by up to 29x on more challenging high resolution\n(1024x1024) image generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1\">Hengyuan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiatian Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jianfeng Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Spatial-Temporal Information Fusion for LiDAR-Based 3D Moving Object Segmentation. (arXiv:2207.02201v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02201","description":"<p>Accurate moving object segmentation is an essential task for autonomous\ndriving. It can provide effective information for many downstream tasks, such\nas collision avoidance, path planning, and static map construction. How to\neffectively exploit the spatial-temporal information is a critical question for\n3D LiDAR moving object segmentation (LiDAR-MOS). In this work, we propose a\nnovel deep neural network exploiting both spatial-temporal information and\ndifferent representation modalities of LiDAR scans to improve LiDAR-MOS\nperformance. Specifically, we first use a range image-based dual-branch\nstructure to separately deal with spatial and temporal information that can be\nobtained from sequential LiDAR scans, and later combine them using\nmotion-guided attention modules. We also use a point refinement module via 3D\nsparse convolution to fuse the information from both LiDAR range image and\npoint cloud representations and reduce the artifacts on the borders of the\nobjects. We verify the effectiveness of our proposed approach on the LiDAR-MOS\nbenchmark of SemanticKITTI. Our method outperforms the state-of-the-art methods\nsignificantly in terms of LiDAR-MOS IoU. Benefiting from the devised\ncoarse-to-fine architecture, our method operates online at sensor frame rate.\nThe implementation of our method is available as open source at:\nhttps://github.com/haomo-ai/MotionSeg3D.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jiadai Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1\">Yuchao Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xianjing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jintao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ai_R/0/1/0/all/0/1\">Rui Ai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_W/0/1/0/all/0/1\">Weihao Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xieyuanli Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CoBEVT: Cooperative Bird's Eye View Semantic Segmentation with Sparse Transformers. (arXiv:2207.02202v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02202","description":"<p>Bird's eye view (BEV) semantic segmentation plays a crucial role in spatial\nsensing for autonomous driving. Although recent literature has made significant\nprogress on BEV map understanding, they are all based on single-agent\ncamera-based systems which are difficult to handle occlusions and detect\ndistant objects in complex traffic scenes. Vehicle-to-Vehicle (V2V)\ncommunication technologies have enabled autonomous vehicles to share sensing\ninformation, which can dramatically improve the perception performance and\nrange as compared to single-agent systems. In this paper, we propose CoBEVT,\nthe first generic multi-agent multi-camera perception framework that can\ncooperatively generate BEV map predictions. To efficiently fuse camera features\nfrom multi-view and multi-agent data in an underlying Transformer architecture,\nwe design a fused axial attention or FAX module, which can capture sparsely\nlocal and global spatial interactions across views and agents. The extensive\nexperiments on the V2V perception dataset, OPV2V, demonstrate that CoBEVT\nachieves state-of-the-art performance for cooperative BEV semantic\nsegmentation. Moreover, CoBEVT is shown to be generalizable to other tasks,\nincluding 1) BEV segmentation with single-agent multi-camera and 2) 3D object\ndetection with multi-agent LiDAR systems, and achieves state-of-the-art\nperformance with real-time inference speed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Runsheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhengzhong Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_H/0/1/0/all/0/1\">Hao Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_W/0/1/0/all/0/1\">Wei Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Bolei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jiaqi Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting and Recovering Sequential DeepFake Manipulation. (arXiv:2207.02204v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02204","description":"<p>Since photorealistic faces can be readily generated by facial manipulation\ntechnologies nowadays, potential malicious abuse of these technologies has\ndrawn great concerns. Numerous deepfake detection methods are thus proposed.\nHowever, existing methods only focus on detecting one-step facial manipulation.\nAs the emergence of easy-accessible facial editing applications, people can\neasily manipulate facial components using multi-step operations in a sequential\nmanner. This new threat requires us to detect a sequence of facial\nmanipulations, which is vital for both detecting deepfake media and recovering\noriginal faces afterwards. Motivated by this observation, we emphasize the need\nand propose a novel research problem called Detecting Sequential DeepFake\nManipulation (Seq-DeepFake). Unlike the existing deepfake detection task only\ndemanding a binary label prediction, detecting Seq-DeepFake manipulation\nrequires correctly predicting a sequential vector of facial manipulation\noperations. To support a large-scale investigation, we construct the first\nSeq-DeepFake dataset, where face images are manipulated sequentially with\ncorresponding annotations of sequential facial manipulation vectors. Based on\nthis new dataset, we cast detecting Seq-DeepFake manipulation as a specific\nimage-to-sequence (e.g. image captioning) task and propose a concise yet\neffective Seq-DeepFake Transformer (SeqFakeFormer). Moreover, we build a\ncomprehensive benchmark and set up rigorous evaluation protocols and metrics\nfor this new research problem. Extensive experiments demonstrate the\neffectiveness of SeqFakeFormer. Several valuable observations are also revealed\nto facilitate future research in broader deepfake detection problems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shao_R/0/1/0/all/0/1\">Rui Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tianxing Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Clustered Saliency Prediction. (arXiv:2207.02205v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02205","description":"<p>We present a new method for image salience prediction, Clustered Saliency\nPrediction. This method divides individuals into clusters based on their\npersonal features and their known saliency maps, and generates a separate image\nsalience model for each cluster. We test our approach on a public dataset of\npersonalized saliency maps, with varying importance weights for personal\nfeature factors and observe the effects on the clusters. For each cluster, we\nuse an image-to-image translation method, mainly Pix2Pix model, to convert\nuniversal saliency maps to saliency maps of that cluster. We try three\nstate-of-the-art universal saliency prediction methods, DeepGaze II, ML-Net and\nSalGAN, and see their impact on the results. We show that our Clustered\nSaliency Prediction technique outperforms the state-of-the-art universal\nsaliency prediction models. Also we demonstrate the effectiveness of our\nclustering method by comparing the results of Clustered Saliency Prediction\nusing clusters obtained by Subject Similarity Clustering algorithm with two\nbaseline methods. We propose an approach to assign new people to the most\nappropriate cluster, based on their personal features and any known saliency\nmaps. In our experiments we see that this method of assigning new people to a\ncluster on average chooses the cluster that gives higher saliency scores.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sherkati_R/0/1/0/all/0/1\">Rezvan Sherkati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_J/0/1/0/all/0/1\">James J. Clark</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Segmenting Moving Objects via an Object-Centric Layered Representation. (arXiv:2207.02206v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02206","description":"<p>The objective of this paper is a model that is able to discover, track and\nsegment multiple moving objects in a video. We make four contributions: First,\nwe introduce an object-centric segmentation model with a depth-ordered layer\nrepresentation. This is implemented using a variant of the transformer\narchitecture that ingests optical flow, where each query vector specifies an\nobject and its layer for the entire video. The model can effectively discover\nmultiple moving objects and handle mutual occlusions; Second, we introduce a\nscalable pipeline for generating synthetic training data with multiple objects,\nsignificantly reducing the requirements for labour-intensive annotations, and\nsupporting Sim2Real generalisation; Third, we show that the model is able to\nlearn object permanence and temporal shape consistency, and is able to predict\namodal segmentation masks; Fourth, we evaluate the model on standard video\nsegmentation benchmarks, DAVIS, MoCA, SegTrack, FBMS-59, and achieve\nstate-of-the-art unsupervised segmentation performance, even outperforming\nseveral supervised approaches. With test-time adaptation, we observe further\nperformance boosts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Junyu Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1\">Weidi Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1\">Andrew Zisserman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DPC-Net: Deep Pose Correction for Visual Localization. (arXiv:1709.03128v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1709.03128","description":"<p>We present a novel method to fuse the power of deep networks with the\ncomputational efficiency of geometric and probabilistic localization\nalgorithms. In contrast to other methods that completely replace a classical\nvisual estimator with a deep network, we propose an approach that uses a\nconvolutional neural network to learn difficult-to-model corrections to the\nestimator from ground-truth training data. To this end, we derive a novel loss\nfunction for learning SE(3) corrections based on a matrix Lie groups approach,\nwith a natural formulation for balancing translation and rotation errors. We\nuse this loss to train a Deep Pose Correction network (DPC-Net) that predicts\ncorrections for a particular estimator, sensor and environment. Using the KITTI\nodometry dataset, we demonstrate significant improvements to the accuracy of a\ncomputationally-efficient sparse stereo visual odometry pipeline, that render\nit as accurate as a modern computationally-intensive dense estimator. Further,\nwe show how DPC-Net can be used to mitigate the effect of poorly calibrated\nlens distortion parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peretroukhin_V/0/1/0/all/0/1\">Valentin Peretroukhin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kelly_J/0/1/0/all/0/1\">Jonathan Kelly</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Matchable Image Transformations for Long-term Metric Visual Localization. (arXiv:1904.01080v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1904.01080","description":"<p>Long-term metric self-localization is an essential capability of autonomous\nmobile robots, but remains challenging for vision-based systems due to\nappearance changes caused by lighting, weather, or seasonal variations. While\nexperience-based mapping has proven to be an effective technique for bridging\nthe `appearance gap,' the number of experiences required for reliable metric\nlocalization over days or months can be very large, and methods for reducing\nthe necessary number of experiences are needed for this approach to scale.\nTaking inspiration from color constancy theory, we learn a nonlinear\nRGB-to-grayscale mapping that explicitly maximizes the number of inlier feature\nmatches for images captured under different lighting and weather conditions,\nand use it as a pre-processing step in a conventional single-experience\nlocalization pipeline to improve its robustness to appearance change. We train\nthis mapping by approximating the target non-differentiable localization\npipeline with a deep neural network, and find that incorporating a learned\nlow-dimensional context feature can further improve cross-appearance feature\nmatching. Using synthetic and real-world datasets, we demonstrate substantial\nimprovements in localization performance across day-night cycles, enabling\ncontinuous metric localization over a 30-hour period using a single mapping\nexperience, and allowing experience-based localization to scale to long\ndeployments with dramatically reduced data requirements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Clement_L/0/1/0/all/0/1\">Lee Clement</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gridseth_M/0/1/0/all/0/1\">Mona Gridseth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tomasi_J/0/1/0/all/0/1\">Justin Tomasi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kelly_J/0/1/0/all/0/1\">Jonathan Kelly</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting mechanical loosening of total hip replacement implant from plain radiograph using deep convolutional neural network. (arXiv:1912.00943v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/1912.00943","description":"<p>Plain radiography is widely used to detect mechanical loosening of total hip\nreplacement (THR) implants. Currently, radiographs are assessed manually by\nmedical professionals, which may be prone to poor inter and intra observer\nreliability and low accuracy. Furthermore, manual detection of mechanical\nloosening of THR implants requires experienced clinicians who might not always\nbe readily available, potentially resulting in delayed diagnosis. In this\nstudy, we present a novel, fully automatic and interpretable approach to detect\nmechanical loosening of THR implants from plain radiographs using deep\nconvolutional neural network (CNN). We trained a CNN on 40 patients\nanteroposterior hip x rays using five fold cross validation and compared its\nperformance with a high volume board certified orthopaedic surgeon (AFC). To\nincrease the confidence in the machine outcome, we also implemented saliency\nmaps to visualize where the CNN looked at to make a diagnosis. CNN outperformed\nthe orthopaedic surgeon in diagnosing mechanical loosening of THR implants\nachieving significantly higher sensitively (0.94) than the orthopaedic surgeon\n(0.53) with the same specificity (0.96). The saliency maps showed that the CNN\nlooked at clinically relevant features to make a diagnosis. Such CNNs can be\nused for automatic radiologic assessment of mechanical loosening of THR\nimplants to supplement the practitioners decision making process, increasing\ntheir diagnostic accuracy, and freeing them to engage in more patient centric\ncare.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Borjali_A/0/1/0/all/0/1\">Alireza Borjali</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_A/0/1/0/all/0/1\">Antonia F. Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Muratoglu_O/0/1/0/all/0/1\">Orhun K. Muratoglu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Morid_M/0/1/0/all/0/1\">Mohammad A. Morid</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Varadarajan_K/0/1/0/all/0/1\">Kartik M. Varadarajan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UltraSR: Spatial Encoding is a Missing Key for Implicit Image Function-based Arbitrary-Scale Super-Resolution. (arXiv:2103.12716v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.12716","description":"<p>The recent success of NeRF and other related implicit neural representation\nmethods has opened a new path for continuous image representation, where pixel\nvalues no longer need to be looked up from stored discrete 2D arrays but can be\ninferred from neural network models on a continuous spatial domain. Although\nthe recent work LIIF has demonstrated that such novel approaches can achieve\ngood performance on the arbitrary-scale super-resolution task, their upscaled\nimages frequently show structural distortion due to the inaccurate prediction\nof high-frequency textures. In this work, we propose UltraSR, a simple yet\neffective new network design based on implicit image functions in which we\ndeeply integrated spatial coordinates and periodic encoding with the implicit\nneural representation. Through extensive experiments and ablation studies, we\nshow that spatial encoding is a missing key toward the next-stage\nhigh-performing implicit image function. Our UltraSR sets new state-of-the-art\nperformance on the DIV2K benchmark under all super-resolution scales compared\nto previous state-of-the-art methods. UltraSR also achieves superior\nperformance on other standard benchmark datasets in which it outperforms prior\nworks in almost all experiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xingqian Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Humphrey Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continual 3D Convolutional Neural Networks for Real-time Processing of Videos. (arXiv:2106.00050v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.00050","description":"<p>We introduce Continual 3D Convolutional Neural Networks (Co3D CNNs), a new\ncomputational formulation of spatio-temporal 3D CNNs, in which videos are\nprocessed frame-by-frame rather than by clip. In online tasks demanding\nframe-wise predictions, Co3D CNNs dispense with the computational redundancies\nof regular 3D CNNs, namely the repeated convolutions over frames, which appear\nin overlapping clips. We show that Continual 3D CNNs can reuse preexisting\n3D-CNN weights to reduce the per-prediction floating point operations (FLOPs)\nin proportion to the temporal receptive field while retaining similar memory\nrequirements and accuracy. This is validated with multiple models on\nKinetics-400 and Charades with remarkable results: CoX3D models attain\nstate-of-the-art complexity/accuracy trade-offs on Kinetics-400 with 12.1-15.3x\nreductions of FLOPs and 2.3-3.8% improvements in accuracy compared to regular\nX3D models while reducing peak memory consumption by up to 48%. Moreover, we\ninvestigate the transient response of Co3D CNNs at start-up and perform\nextensive benchmarks of on-hardware processing characteristics for publicly\navailable 3D CNNs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hedegaard_L/0/1/0/all/0/1\">Lukas Hedegaard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iosifidis_A/0/1/0/all/0/1\">Alexandros Iosifidis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recent Deep Semi-supervised Learning Approaches and Related Works. (arXiv:2106.11528v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.11528","description":"<p>The author of this work proposes an overview of the recent semi-supervised\nlearning approaches and related works. Despite the remarkable success of neural\nnetworks in various applications, there exist few formidable constraints\nincluding the need for a large amount of labeled data. Therefore,\nsemi-supervised learning, which is a learning scheme in which the scarce labels\nand a larger amount of unlabeled data are utilized to train models (e.g., deep\nneural networks) is getting more important. Based on the key assumptions of\nsemi-supervised learning, which are the manifold assumption, cluster\nassumption, and continuity assumption, the work reviews the recent\nsemi-supervised learning approaches. In particular, the methods in regard to\nusing deep neural networks in a semi-supervised learning setting are primarily\ndiscussed. In addition, the existing works are first classified based on the\nunderlying idea and explained, and then the holistic approaches that unify the\naforementioned ideas are detailed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1\">Gyeongho Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Practicality of Deterministic Epistemic Uncertainty. (arXiv:2107.00649v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.00649","description":"<p>A set of novel approaches for estimating epistemic uncertainty in deep neural\nnetworks with a single forward pass has recently emerged as a valid alternative\nto Bayesian Neural Networks. On the premise of informative representations,\nthese deterministic uncertainty methods (DUMs) achieve strong performance on\ndetecting out-of-distribution (OOD) data while adding negligible computational\ncosts at inference time. However, it remains unclear whether DUMs are well\ncalibrated and can seamlessly scale to real-world applications - both\nprerequisites for their practical deployment. To this end, we first provide a\ntaxonomy of DUMs, and evaluate their calibration under continuous\ndistributional shifts. Then, we extend them to semantic segmentation. We find\nthat, while DUMs scale to realistic vision tasks and perform well on OOD\ndetection, the practicality of current methods is undermined by poor\ncalibration under distributional shifts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Postels_J/0/1/0/all/0/1\">Janis Postels</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Segu_M/0/1/0/all/0/1\">Mattia Segu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1\">Tao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sieber_L/0/1/0/all/0/1\">Luca Sieber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fisher Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tombari_F/0/1/0/all/0/1\">Federico Tombari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SimCC: a Simple Coordinate Classification Perspective for Human Pose Estimation. (arXiv:2107.03332v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.03332","description":"<p>The 2D heatmap-based approaches have dominated Human Pose Estimation (HPE)\nfor years due to high performance. However, the long-standing quantization\nerror problem in the 2D heatmap-based methods leads to several well-known\ndrawbacks: 1) The performance for the low-resolution inputs is limited; 2) To\nimprove the feature map resolution for higher localization precision, multiple\ncostly upsampling layers are required; 3) Extra post-processing is adopted to\nreduce the quantization error. To address these issues, we aim to explore a\nbrand new scheme, called \\textit{SimCC}, which reformulates HPE as two\nclassification tasks for horizontal and vertical coordinates. The proposed\nSimCC uniformly divides each pixel into several bins, thus achieving\n\\emph{sub-pixel} localization precision and low quantization error. Benefiting\nfrom that, SimCC can omit additional refinement post-processing and exclude\nupsampling layers under certain settings, resulting in a more simple and\neffective pipeline for HPE. Extensive experiments conducted over COCO,\nCrowdPose, and MPII datasets show that SimCC outperforms heatmap-based\ncounterparts, especially in low-resolution settings by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanjie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Peidong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shoukui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunxiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhicheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wankou Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1\">Shu-Tao Xia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Benign Adversarial Attack: Tricking Models for Goodness. (arXiv:2107.11986v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2107.11986","description":"<p>In spite of the successful application in many fields, machine learning\nmodels today suffer from notorious problems like vulnerability to adversarial\nexamples. Beyond falling into the cat-and-mouse game between adversarial attack\nand defense, this paper provides alternative perspective to consider\nadversarial example and explore whether we can exploit it in benign\napplications. We first attribute adversarial example to the human-model\ndisparity on employing non-semantic features. While largely ignored in\nclassical machine learning mechanisms, non-semantic feature enjoys three\ninteresting characteristics as (1) exclusive to model, (2) critical to affect\ninference, and (3) utilizable as features. Inspired by this, we present brave\nnew idea of benign adversarial attack to exploit adversarial examples for\ngoodness in three directions: (1) adversarial Turing test, (2) rejecting\nmalicious model application, and (3) adversarial data augmentation. Each\ndirection is positioned with motivation elaboration, justification analysis and\nprototype applications to showcase its potential.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sang_J/0/1/0/all/0/1\">Jitao Sang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xian Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiaming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhiyu Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-grained Data Distribution Alignment for Post-Training Quantization. (arXiv:2109.04186v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.04186","description":"<p>While post-training quantization receives popularity mostly due to its\nevasion in accessing the original complete training dataset, its poor\nperformance also stems from scarce images. To alleviate this limitation, in\nthis paper, we leverage the synthetic data introduced by zero-shot quantization\nwith calibration dataset and propose a fine-grained data distribution alignment\n(FDDA) method to boost the performance of post-training quantization. The\nmethod is based on two important properties of batch normalization statistics\n(BNS) we observed in deep layers of the trained network, (i.e.), inter-class\nseparation and intra-class incohesion. To preserve this fine-grained\ndistribution information: 1) We calculate the per-class BNS of the calibration\ndataset as the BNS centers of each class and propose a BNS-centralized loss to\nforce the synthetic data distributions of different classes to be close to\ntheir own centers. 2) We add Gaussian noise into the centers to imitate the\nincohesion and propose a BNS-distorted loss to force the synthetic data\ndistribution of the same class to be close to the distorted centers. By\nutilizing these two fine-grained losses, our method manifests the\nstate-of-the-art performance on ImageNet, especially when both the first and\nlast layers are quantized to the low-bit. Code is at\n\\url{https://github.com/zysxmu/FDDA}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1\">Yunshan Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Mingbao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mengzhao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Ke Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yunhang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_F/0/1/0/all/0/1\">Fei Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yongjian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pixel-wise Energy-biased Abstention Learning for Anomaly Segmentation on Complex Urban Driving Scenes. (arXiv:2111.12264v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.12264","description":"<p>State-of-the-art (SOTA) anomaly segmentation approaches on complex urban\ndriving scenes explore pixel-wise classification uncertainty learned from\noutlier exposure, or external reconstruction models. However, previous\nuncertainty approaches that directly associate high uncertainty to anomaly may\nsometimes lead to incorrect anomaly predictions, and external reconstruction\nmodels tend to be too inefficient for real-time self-driving embedded systems.\nIn this paper, we propose a new anomaly segmentation method, named pixel-wise\nenergy-biased abstention learning (PEBAL), that explores pixel-wise abstention\nlearning (AL) with a model that learns an adaptive pixel-level anomaly class,\nand an energy-based model (EBM) that learns inlier pixel distribution. More\nspecifically, PEBAL is based on a non-trivial joint training of EBM and AL,\nwhere EBM is trained to output high-energy for anomaly pixels (from outlier\nexposure) and AL is trained such that these high-energy pixels receive adaptive\nlow penalty for being included to the anomaly class. We extensively evaluate\nPEBAL against the SOTA and show that it achieves the best performance across\nfour benchmarks. Code is available at https://github.com/tianyu0207/PEBAL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yu Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_G/0/1/0/all/0/1\">Guansong Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fengbei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuanhong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carneiro_G/0/1/0/all/0/1\">Gustavo Carneiro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Online Adaptation for Implicit Object Tracking and Shape Reconstruction in the Wild. (arXiv:2111.12728v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.12728","description":"<p>Tracking and reconstructing 3D objects from cluttered scenes are the key\ncomponents for computer vision, robotics and autonomous driving systems. While\nrecent progress in implicit function has shown encouraging results on\nhigh-quality 3D shape reconstruction, it is still very challenging to\ngeneralize to cluttered and partially observable LiDAR data. In this paper, we\npropose to leverage the continuity in video data. We introduce a novel and\nunified framework which utilizes a neural implicit function to simultaneously\ntrack and reconstruct 3D objects in the wild. Our approach adapts the DeepSDF\nmodel (i.e., an instantiation of the implicit function) in the video online,\niteratively improving the shape reconstruction while in return improving the\ntracking, and vice versa. We experiment with both Waymo and KITTI datasets and\nshow significant improvements over state-of-the-art methods for both tracking\nand shape reconstruction tasks. Our project page is at\nhttps://jianglongye.com/implicit-tracking .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jianglong Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuntao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1\">Naiyan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaolong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Seeking Salient Facial Regions for Cross-Database Micro-Expression Recognition. (arXiv:2111.15361v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.15361","description":"<p>Cross-Database Micro-Expression Recognition (CDMER) aims to develop the\nMicro-Expression Recognition (MER) methods with strong domain adaptability,\ni.e., the ability to recognize the Micro-Expressions (MEs) of different\nsubjects captured by different imaging devices in different scenes. The\ndevelopment of CDMER is faced with two key problems: 1) the severe feature\ndistribution gap between the source and target databases; 2) the feature\nrepresentation bottleneck of ME such local and subtle facial expressions. To\nsolve these problems, this paper proposes a novel Transfer Group Sparse\nRegression method, namely TGSR, which aims to 1) optimize the measurement and\nbetter alleviate the difference between the source and target databases, and 2)\nhighlight the valid facial regions to enhance extracted features, by the\noperation of selecting the group features from the raw face feature, where each\nregion is associated with a group of raw face feature, i.e., the salient facial\nregion selection. Compared with previous transfer group sparse methods, our\nproposed TGSR has the ability to select the salient facial regions, which is\neffective in alleviating the aforementioned problems for better performance and\nreducing the computational cost at the same time. We use two public ME\ndatabases, i.e., CASME II and SMIC, to evaluate our proposed TGSR method.\nExperimental results show that our proposed TGSR learns the discriminative and\nexplicable regions, and outperforms most state-of-the-art\nsubspace-learning-based domain-adaptive methods for CDMER.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xingxun Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zong_Y/0/1/0/all/0/1\">Yuan Zong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1\">Wenming Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiateng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_M/0/1/0/all/0/1\">Mengting Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ScaleNet: A Shallow Architecture for Scale Estimation. (arXiv:2112.04846v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.04846","description":"<p>In this paper, we address the problem of estimating scale factors between\nimages. We formulate the scale estimation problem as a prediction of a\nprobability distribution over scale factors. We design a new architecture,\nScaleNet, that exploits dilated convolutions as well as self and\ncross-correlation layers to predict the scale between images. We demonstrate\nthat rectifying images with estimated scales leads to significant performance\nimprovements for various tasks and methods. Specifically, we show how ScaleNet\ncan be combined with sparse local features and dense correspondence networks to\nimprove camera pose estimation, 3D reconstruction, or dense geometric matching\nin different benchmarks and datasets. We provide an extensive evaluation on\nseveral tasks and analyze the computational overhead of ScaleNet. The code,\nevaluation protocols, and trained models are publicly available at\nhttps://github.com/axelBarroso/ScaleNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Barroso_Laguna_A/0/1/0/all/0/1\">Axel Barroso-Laguna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yurun Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mikolajczyk_K/0/1/0/all/0/1\">Krystian Mikolajczyk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring the Equivalence of Siamese Self-Supervised Learning via A Unified Gradient Framework. (arXiv:2112.05141v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.05141","description":"<p>Self-supervised learning has shown its great potential to extract powerful\nvisual representations without human annotations. Various works are proposed to\ndeal with self-supervised learning from different perspectives: (1) contrastive\nlearning methods (e.g., MoCo, SimCLR) utilize both positive and negative\nsamples to guide the training direction; (2) asymmetric network methods (e.g.,\nBYOL, SimSiam) get rid of negative samples via the introduction of a predictor\nnetwork and the stop-gradient operation; (3) feature decorrelation methods\n(e.g., Barlow Twins, VICReg) instead aim to reduce the redundancy between\nfeature dimensions. These methods appear to be quite different in the designed\nloss functions from various motivations. The final accuracy numbers also vary,\nwhere different networks and tricks are utilized in different works. In this\nwork, we demonstrate that these methods can be unified into the same form.\nInstead of comparing their loss functions, we derive a unified formula through\ngradient analysis. Furthermore, we conduct fair and detailed experiments to\ncompare their performances. It turns out that there is little gap between these\nmethods, and the use of momentum encoder is the key factor to boost\nperformance. From this unified framework, we propose UniGrad, a simple but\neffective gradient form for self-supervised learning. It does not require a\nmemory bank or a predictor network, but can still achieve state-of-the-art\nperformance and easily adopt other training strategies. Extensive experiments\non linear evaluation and many downstream tasks also show its effectiveness.\nCode is released at https://github.com/fundamentalvision/UniGrad.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1\">Chenxin Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Honghui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xizhou Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Jiahua Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Shiji Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Gao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1\">Jifeng Dai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SAC-GAN: Structure-Aware Image Composition. (arXiv:2112.06596v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.06596","description":"<p>We introduce an end-to-end learning framework for image-to-image composition,\naiming to seamlessly compose an object represented as a cropped patch from an\nobject image into a background scene image. As our approach emphasizes more on\nsemantic and structural coherence of the composed images, rather than their\npixel-level RGB accuracies, we tailor the input and output of our network with\nstructure-aware features and design our network losses accordingly, with ground\ntruth established in a self-supervised setting through the object cropping.\nSpecifically, our network takes the semantic layout features from the input\nscene image, features encoded from the edges and silhouette in the input object\npatch, as well as a latent code as inputs, and generates a 2D spatial affine\ntransform defining the translation and scaling of the object patch. The learned\nparameters are further fed into a differentiable spatial transformer network to\ntransform the object patch into the target image, where our model is trained\nadversarially using an affine transform discriminator and a layout\ndiscriminator. We evaluate our network, coined SAC-GAN, for various image\ncomposition scenarios in terms of quality, composability, and generalizability\nof the composite images. Comparisons are made to state-of-the-art alternatives,\nincluding Instance Insertion, ST-GAN, CompGAN and PlaceNet, confirming\nsuperiority of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_R/0/1/0/all/0/1\">Rui Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lingxiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Lin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahdavi_Amiri_A/0/1/0/all/0/1\">Ali Mahdavi-Amiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neuromorphic Camera Denoising using Graph Neural Network-driven Transformers. (arXiv:2112.09685v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.09685","description":"<p>Neuromorphic vision is a bio-inspired technology that has triggered a\nparadigm shift in the computer-vision community and is serving as a key-enabler\nfor a multitude of applications. This technology has offered significant\nadvantages including reduced power consumption, reduced processing needs, and\ncommunication speed-ups. However, neuromorphic cameras suffer from significant\namounts of measurement noise. This noise deteriorates the performance of\nneuromorphic event-based perception and navigation algorithms. In this paper,\nwe propose a novel noise filtration algorithm to eliminate events which do not\nrepresent real log-intensity variations in the observed scene. We employ a\nGraph Neural Network (GNN)-driven transformer algorithm, called\nGNN-Transformer, to classify every active event pixel in the raw stream into\nreal-log intensity variation or noise. Within the GNN, a message-passing\nframework, called EventConv, is carried out to reflect the spatiotemporal\ncorrelation among the events, while preserving their asynchronous nature. We\nalso introduce the Known-object Ground-Truth Labeling (KoGTL) approach for\ngenerating approximate ground truth labels of event streams under various\nillumination conditions. KoGTL is used to generate labeled datasets, from\nexperiments recorded in chalenging lighting conditions. These datasets are used\nto train and extensively test our proposed algorithm. When tested on unseen\ndatasets, the proposed algorithm outperforms existing methods by 8.8% in terms\nof filtration accuracy. Additional tests are also conducted on publicly\navailable datasets to demonstrate the generalization capabilities of the\nproposed algorithm in the presence of illumination variations and different\nmotion dynamics. Compared to existing solutions, qualitative results verified\nthe superior capability of the proposed algorithm to eliminate noise while\npreserving meaningful scene events.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alkendi_Y/0/1/0/all/0/1\">Yusra Alkendi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azzam_R/0/1/0/all/0/1\">Rana Azzam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayyad_A/0/1/0/all/0/1\">Abdulla Ayyad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Javed_S/0/1/0/all/0/1\">Sajid Javed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seneviratne_L/0/1/0/all/0/1\">Lakmal Seneviratne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zweiri_Y/0/1/0/all/0/1\">Yahya Zweiri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Attention-Model Explainability through Faithfulness Violation Test. (arXiv:2201.12114v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2201.12114","description":"<p>Attention mechanisms are dominating the explainability of deep models. They\nproduce probability distributions over the input, which are widely deemed as\nfeature-importance indicators. However, in this paper, we find one critical\nlimitation in attention explanations: weakness in identifying the polarity of\nfeature impact. This would be somehow misleading -- features with higher\nattention weights may not faithfully contribute to model predictions; instead,\nthey can impose suppression effects. With this finding, we reflect on the\nexplainability of current attention-based techniques, such as\nAttentio$\\odot$Gradient and LRP-based attention explanations. We first propose\nan actionable diagnostic methodology (henceforth faithfulness violation test)\nto measure the consistency between explanation weights and the impact polarity.\nThrough the extensive experiments, we then show that most tested explanation\nmethods are unexpectedly hindered by the faithfulness violation issue,\nespecially the raw attention. Empirical analyses on the factors affecting\nviolation issues further provide useful observations for adopting explanation\nmethods in attention models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yibing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haoliang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yangyang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_C/0/1/0/all/0/1\">Chenqi Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shiqi Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Color Image Inpainting via Robust Pure Quaternion Matrix Completion: Error Bound and Weighted Loss. (arXiv:2202.02063v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.02063","description":"<p>In this paper, we study color image inpainting as a pure quaternion matrix\ncompletion problem. In the literature, the theoretical guarantee for quaternion\nmatrix completion is not well-established. Our main aim is to propose a new\nminimization problem with an objective combining nuclear norm and a quadratic\nloss weighted among three channels. To fill the theoretical vacancy, we obtain\nthe error bound in both clean and corrupted regimes, which relies on some new\nresults of quaternion matrices. A general Gaussian noise is considered in\nrobust completion where all observations are corrupted. Motivated by the error\nbound, we propose to handle unbalanced or correlated noise via a cross-channel\nweight in the quadratic loss, with the main purpose of rebalancing noise level,\nor removing noise correlation. Extensive experimental results on synthetic and\ncolor image data are presented to confirm and demonstrate our theoretical\nfindings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Junren Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_M/0/1/0/all/0/1\">Michael K. Ng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Offline Text-Independent Writer Identification based on word level data. (arXiv:2202.10207v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.10207","description":"<p>This paper proposes a novel scheme to identify the authorship of a document\nbased on handwritten input word images of an individual. Our approach is\ntext-independent and does not place any restrictions on the size of the input\nword images under consideration. To begin with, we employ the SIFT algorithm to\nextract multiple key points at various levels of abstraction (comprising\nallograph, character, or combination of characters). These key points are then\npassed through a trained CNN network to generate feature maps corresponding to\na convolution layer. However, owing to the scale corresponding to the SIFT key\npoints, the size of a generated feature map may differ. As an alleviation to\nthis issue, the histogram of gradients is applied on the feature map to produce\na fixed representation. Typically, in a CNN, the number of filters of each\nconvolution block increase depending on the depth of the network. Thus,\nextracting histogram features for each of the convolution feature map increase\nthe dimension as well as the computational load. To address this aspect, we use\nan entropy-based method to learn the weights of the feature maps of a\nparticular CNN layer during the training phase of our algorithm. The efficacy\nof our proposed system has been demonstrated on two publicly available\ndatabases namely CVL and IAM. We empirically show that the results obtained are\npromising when compared with previous works.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Vineet Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sundaram_S/0/1/0/all/0/1\">Suresh Sundaram</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep learning based domain adaptation for mitochondria segmentation on EM volumes. (arXiv:2202.10773v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.10773","description":"<p>Accurate segmentation of electron microscopy (EM) volumes of the brain is\nessential to characterize neuronal structures at a cell or organelle level.\nWhile supervised deep learning methods have led to major breakthroughs in that\ndirection during the past years, they usually require large amounts of\nannotated data to be trained, and perform poorly on other data acquired under\nsimilar experimental and imaging conditions. This is a problem known as domain\nadaptation, since models that learned from a sample distribution (or source\ndomain) struggle to maintain their performance on samples extracted from a\ndifferent distribution or target domain. In this work, we address the complex\ncase of deep learning based domain adaptation for mitochondria segmentation\nacross EM datasets from different tissues and species. We present three\nunsupervised domain adaptation strategies to improve mitochondria segmentation\nin the target domain based on (1) state-of-the-art style transfer between\nimages of both domains; (2) self-supervised learning to pre-train a model using\nunlabeled source and target images, and then fine-tune it only with the source\nlabels; and (3) multi-task neural network architectures trained end-to-end with\nboth labeled and unlabeled images. Additionally, we propose a new training\nstopping criterion based on morphological priors obtained exclusively in the\nsource domain. We carried out all possible cross-dataset experiments using\nthree publicly available EM datasets. We evaluated our proposed strategies on\nthe mitochondria semantic labels predicted on the target datasets. The methods\nintroduced here outperform the baseline methods and compare favorably to the\nstate of the art. In the absence of validation labels, monitoring our proposed\nmorphology-based metric is an intuitive and effective way to stop the training\nprocess and select in average optimal models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Franco_Barranco_D/0/1/0/all/0/1\">Daniel Franco-Barranco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pastor_Tronch_J/0/1/0/all/0/1\">Julio Pastor-Tronch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Marfil_A/0/1/0/all/0/1\">Aitor Gonzalez-Marfil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Munoz_Barrutia_A/0/1/0/all/0/1\">Arrate Mu&#xf1;oz-Barrutia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arganda_Carreras_I/0/1/0/all/0/1\">Ignacio Arganda-Carreras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Severity classification in cases of Collagen VI-related myopathy with Convolutional Neural Networks and handcrafted texture features. (arXiv:2202.13853v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2202.13853","description":"<p>Magnetic Resonance Imaging (MRI) is a non-invasive tool for the clinical\nassessment of low-prevalence neuromuscular disorders. Automated diagnosis\nmethods might reduce the need for biopsies and provide valuable information on\ndisease follow-up. In this paper, three methods are proposed to classify target\nmuscles in Collagen VI-related myopathy cases, based on their degree of\ninvolvement, notably a Convolutional Neural Network, a Fully Connected Network\nto classify texture features, and a hybrid method combining the two feature\nsets. The proposed methods were evaluated on axial T1-weighted Turbo Spin-Echo\nMRI from 26 subjects, including Ullrich Congenital Muscular Dystrophy and\nBethlem Myopathy patients at different evolution stages. The hybrid model\nachieved the best cross-validation results, with a global accuracy of 93.8%,\nand F-scores of 0.99, 0.82, and 0.95, for healthy, mild and moderate/severe\ncases, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Rodrigues_R/0/1/0/all/0/1\">Rafael Rodrigues</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Quijano_Roy_S/0/1/0/all/0/1\">Susana Quijano-Roy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Carlier_R/0/1/0/all/0/1\">Robert-Yves Carlier</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pinheiro_A/0/1/0/all/0/1\">Antonio M. G. Pinheiro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Progressive Subsampling for Oversampled Data -- Application to Quantitative MRI. (arXiv:2203.09268v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2203.09268","description":"<p>We present PROSUB: PROgressive SUBsampling, a deep learning based, automated\nmethodology that subsamples an oversampled data set (e.g. multi-channeled 3D\nimages) with minimal loss of information. We build upon a recent dual-network\napproach that won the MICCAI MUlti-DIffusion (MUDI) quantitative MRI\nmeasurement sampling-reconstruction challenge, but suffers from deep learning\ntraining instability, by subsampling with a hard decision boundary. PROSUB uses\nthe paradigm of recursive feature elimination (RFE) and progressively\nsubsamples measurements during deep learning training, improving optimization\nstability. PROSUB also integrates a neural architecture search (NAS) paradigm,\nallowing the network architecture hyperparameters to respond to the subsampling\nprocess. We show PROSUB outperforms the winner of the MUDI MICCAI challenge,\nproducing large improvements &gt;18% MSE on the MUDI challenge sub-tasks and\nqualitative improvements on downstream processes useful for clinical\napplications. We also show the benefits of incorporating NAS and analyze the\neffect of PROSUB's components. As our method generalizes to other problems\nbeyond MRI measurement selection-reconstruction, our code is\nhttps://github.com/sbb-gh/PROSUB\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Blumberg_S/0/1/0/all/0/1\">Stefano B. Blumberg</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_H/0/1/0/all/0/1\">Hongxiang Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Grussu_F/0/1/0/all/0/1\">Francesco Grussu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_Y/0/1/0/all/0/1\">Yukun Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Figini_M/0/1/0/all/0/1\">Matteo Figini</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Alexander_D/0/1/0/all/0/1\">Daniel C. Alexander</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End Audio Strikes Back: Boosting Augmentations Towards An Efficient Audio Classification Network. (arXiv:2204.11479v5 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2204.11479","description":"<p>While efficient architectures and a plethora of augmentations for end-to-end\nimage classification tasks have been suggested and heavily investigated,\nstate-of-the-art techniques for audio classifications still rely on numerous\nrepresentations of the audio signal together with large architectures,\nfine-tuned from large datasets. By utilizing the inherited lightweight nature\nof audio and novel audio augmentations, we were able to present an efficient\nend-to-end network with strong generalization ability. Experiments on a variety\nof sound classification sets demonstrate the effectiveness and robustness of\nour approach, by achieving state-of-the-art results in various settings. Public\ncode is available at:\n\\href{https://github.com/Alibaba-MIIL/AudioClassfication}{this http url}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gazneli_A/0/1/0/all/0/1\">Avi Gazneli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zimerman_G/0/1/0/all/0/1\">Gadi Zimerman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ridnik_T/0/1/0/all/0/1\">Tal Ridnik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharir_G/0/1/0/all/0/1\">Gilad Sharir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noy_A/0/1/0/all/0/1\">Asaf Noy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Masked Generative Distillation. (arXiv:2205.01529v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.01529","description":"<p>Knowledge distillation has been applied to various tasks successfully. The\ncurrent distillation algorithm usually improves students' performance by\nimitating the output of the teacher. This paper shows that teachers can also\nimprove students' representation power by guiding students' feature recovery.\nFrom this point of view, we propose Masked Generative Distillation (MGD), which\nis simple: we mask random pixels of the student's feature and force it to\ngenerate the teacher's full feature through a simple block. MGD is a truly\ngeneral feature-based distillation method, which can be utilized on various\ntasks, including image classification, object detection, semantic segmentation\nand instance segmentation. We experiment on different models with extensive\ndatasets and the results show that all the students achieve excellent\nimprovements. Notably, we boost ResNet-18 from 69.90% to 71.69% ImageNet top-1\naccuracy, RetinaNet with ResNet-50 backbone from 37.4 to 41.0 Boundingbox mAP,\nSOLO based on ResNet-50 from 33.1 to 36.2 Mask mAP and DeepLabV3 based on\nResNet-18 from 73.20 to 76.02 mIoU. Our codes are available at\nhttps://github.com/yzd-v/MGD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhendong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhe Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_M/0/1/0/all/0/1\">Mingqi Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_D/0/1/0/all/0/1\">Dachuan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zehuan Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1\">Chun Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Individual Topology Structure of Eye Movement Trajectories. (arXiv:2205.10667v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.10667","description":"<p>Traditionally, extracting patterns from eye movement data relies on\nstatistics of different macro-events such as fixations and saccades. This\nrequires an additional preprocessing step to separate the eye movement\nsubtypes, often with a number of parameters on which the classification results\ndepend. Besides that, definitions of such macro events are formulated in\ndifferent ways by different researchers.\n</p>\n<p>We propose an application of a new class of features to the quantitative\nanalysis of personal eye movement trajectories structure. This new class of\nfeatures based on algebraic topology allows extracting patterns from different\nmodalities of gaze such as time series of coordinates and amplitudes, heatmaps,\nand point clouds in a unified way at all scales from micro to macro. We\nexperimentally demonstrate the competitiveness of the new class of features\nwith the traditional ones and their significant synergy while being used\ntogether for the person authentication task on the recently published eye\nmovement trajectories dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Onuchin_A/0/1/0/all/0/1\">Arsenii Onuchin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kachan_O/0/1/0/all/0/1\">Oleg Kachan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Muti-expert Distribution Calibration for Long-tailed Video Classification. (arXiv:2205.10788v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.10788","description":"<p>Most existing state-of-the-art video classification methods assume that the\ntraining data obey a uniform distribution. However, video data in the real\nworld typically exhibit an imbalanced long-tailed class distribution, resulting\nin a model bias towards head class and relatively low performance on tail\nclass. While the current long-tailed classification methods usually focus on\nimage classification, adapting it to video data is not a trivial extension. We\npropose an end-to-end multi-expert distribution calibration method to address\nthese challenges based on two-level distribution information. The method\njointly considers the distribution of samples in each class (intra-class\ndistribution) and the overall distribution of diverse data (inter-class\ndistribution) to solve the issue of imbalanced data under long-tailed\ndistribution. By modeling the two-level distribution information, the model can\njointly consider the head classes and the tail classes and significantly\ntransfer the knowledge from the head classes to improve the performance of the\ntail classes. Extensive experiments verify that our method achieves\nstate-of-the-art performance on the long-tailed video classification task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yufan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Junyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Changsheng Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EfficientFormer: Vision Transformers at MobileNet Speed. (arXiv:2206.01191v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.01191","description":"<p>Vision Transformers (ViT) have shown rapid progress in computer vision tasks,\nachieving promising results on various benchmarks. However, due to the massive\nnumber of parameters and model design, e.g., attention mechanism, ViT-based\nmodels are generally times slower than lightweight convolutional networks.\nTherefore, the deployment of ViT for real-time applications is particularly\nchallenging, especially on resource-constrained hardware such as mobile\ndevices. Recent efforts try to reduce the computation complexity of ViT through\nnetwork architecture search or hybrid design with MobileNet block, yet the\ninference speed is still unsatisfactory. This leads to an important question:\ncan transformers run as fast as MobileNet while obtaining high performance? To\nanswer this, we first revisit the network architecture and operators used in\nViT-based models and identify inefficient designs. Then we introduce a\ndimension-consistent pure transformer (without MobileNet blocks) as a design\nparadigm. Finally, we perform latency-driven slimming to get a series of final\nmodels dubbed EfficientFormer. Extensive experiments show the superiority of\nEfficientFormer in performance and speed on mobile devices. Our fastest model,\nEfficientFormer-L1, achieves $79.2\\%$ top-1 accuracy on ImageNet-1K with only\n$1.6$ ms inference latency on iPhone 12 (compiled with CoreML), which { runs as\nfast as MobileNetV2$\\times 1.4$ ($1.6$ ms, $74.7\\%$ top-1),} and our largest\nmodel, EfficientFormer-L7, obtains $83.3\\%$ accuracy with only $7.0$ ms\nlatency. Our work proves that properly designed transformers can reach\nextremely low latency on mobile devices while maintaining high performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_G/0/1/0/all/0/1\">Geng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yang Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_E/0/1/0/all/0/1\">Eric Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Evangelidis_G/0/1/0/all/0/1\">Georgios Evangelidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tulyakov_S/0/1/0/all/0/1\">Sergey Tulyakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Jian Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Siamese Image Modeling for Self-Supervised Vision Representation Learning. (arXiv:2206.01204v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.01204","description":"<p>Self-supervised learning (SSL) has delivered superior performance on a\nvariety of downstream vision tasks. Two main-stream SSL frameworks have been\nproposed, i.e., Instance Discrimination (ID) and Masked Image Modeling (MIM).\nID pulls together the representations of different views from the same image,\nwhile avoiding feature collapse. It does well on linear probing but is inferior\nin detection performance. On the other hand, MIM reconstructs the original\ncontent given a masked image. It excels at dense prediction but fails to\nperform well on linear probing. Their distinctions are caused by neglecting the\nrepresentation requirements of either semantic alignment or spatial\nsensitivity. Specifically, we observe that (1) semantic alignment demands\nsemantically similar views to be projected into nearby representation, which\ncan be achieved by contrasting different views with strong augmentations; (2)\nspatial sensitivity requires to model the local structure within an image.\nPredicting dense representations with masked image is therefore beneficial\nbecause it models the conditional distribution of image content. Driven by\nthese analysis, we propose Siamese Image Modeling (SIM), which predicts the\ndense representations of an augmented view, based on another masked view from\nthe same image but with different augmentations. Our method uses a Siamese\nnetwork with two branches. The online branch encodes the first view, and\npredicts the second view's representation according to the relative positions\nbetween these two views. The target branch produces the target by encoding the\nsecond view. In this way, we are able to achieve comparable linear probing and\ndense prediction performances with ID and MIM, respectively. We also\ndemonstrate that decent linear probing result can be obtained without a global\nloss. Code shall be released at\nhttps://github.com/fundamentalvision/Siamese-Image-Modeling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1\">Chenxin Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xizhou Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Gao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaogang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1\">Jifeng Dai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards a General Purpose CNN for Long Range Dependencies in $N$D. (arXiv:2206.03398v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2206.03398","description":"<p>The use of Convolutional Neural Networks (CNNs) is widespread in Deep\nLearning due to a range of desirable model properties which result in an\nefficient and effective machine learning framework. However, performant CNN\narchitectures must be tailored to specific tasks in order to incorporate\nconsiderations such as the input length, resolution, and dimentionality. In\nthis work, we overcome the need for problem-specific CNN architectures with our\nContinuous Convolutional Neural Network (CCNN): a single CNN architecture\nequipped with continuous convolutional kernels that can be used for tasks on\ndata of arbitrary resolution, dimensionality and length without structural\nchanges. Continuous convolutional kernels model long range dependencies at\nevery layer, and remove the need for downsampling layers and task-dependent\ndepths needed in current CNN architectures. We show the generality of our\napproach by applying the same CCNN to a wide set of tasks on sequential\n(1$\\mathrm{D}$) and visual data (2$\\mathrm{D}$). Our CCNN performs\ncompetitively and often outperforms the current state-of-the-art across all\ntasks considered.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Romero_D/0/1/0/all/0/1\">David W. Romero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knigge_D/0/1/0/all/0/1\">David M. Knigge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_A/0/1/0/all/0/1\">Albert Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bekkers_E/0/1/0/all/0/1\">Erik J. Bekkers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gavves_E/0/1/0/all/0/1\">Efstratios Gavves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tomczak_J/0/1/0/all/0/1\">Jakub M. Tomczak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoogendoorn_M/0/1/0/all/0/1\">Mark Hoogendoorn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uni-Perceiver-MoE: Learning Sparse Generalist Models with Conditional MoEs. (arXiv:2206.04674v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.04674","description":"<p>To build an artificial neural network like the biological intelligence\nsystem, recent works have unified numerous tasks into a generalist model, which\ncan process various tasks with shared parameters and do not have any\ntask-specific modules. While generalist models achieve promising results on\nvarious benchmarks, they have performance degradation on some tasks compared\nwith task-specialized models. In this work, we find that interference among\ndifferent tasks and modalities is the main factor to this phenomenon. To\nmitigate such interference, we introduce the Conditional Mixture-of-Experts\n(Conditional MoEs) to generalist models. Routing strategies under different\nlevels of conditions are proposed to take both the training/inference cost and\ngeneralization ability into account. By incorporating the proposed Conditional\nMoEs, the recently proposed generalist model Uni-Perceiver can effectively\nmitigate the interference across tasks and modalities, and achieves\nstate-of-the-art results on a series of downstream tasks via prompt tuning on\n1% of downstream data. Moreover, the introduction of Conditional MoEs still\nholds the generalization ability of generalist models to conduct zero-shot\ninference on new tasks, e.g., video-text retrieval and video caption. Code and\npre-trained generalist models shall be released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jinguo Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xizhou Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenhai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaohua Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaogang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1\">Jifeng Dai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Making Sense of Dependence: Efficient Black-box Explanations Using Dependence Measure. (arXiv:2206.06219v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.06219","description":"<p>This paper presents a new efficient black-box attribution method based on\nHilbert-Schmidt Independence Criterion (HSIC), a dependence measure based on\nReproducing Kernel Hilbert Spaces (RKHS). HSIC measures the dependence between\nregions of an input image and the output of a model based on kernel embeddings\nof distributions. It thus provides explanations enriched by RKHS representation\ncapabilities. HSIC can be estimated very efficiently, significantly reducing\nthe computational cost compared to other black-box attribution methods. Our\nexperiments show that HSIC is up to 8 times faster than the previous best\nblack-box attribution methods while being as faithful. Indeed, we improve or\nmatch the state-of-the-art of both black-box and white-box attribution methods\nfor several fidelity metrics on Imagenet with various recent model\narchitectures. Importantly, we show that these advances can be transposed to\nefficiently and faithfully explain object detection models such as YOLOv4.\nFinally, we extend the traditional attribution methods by proposing a new\nkernel enabling an orthogonal decomposition of importance scores based on HSIC,\nallowing us to evaluate not only the importance of each image patch but also\nthe importance of their pairwise interactions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Novello_P/0/1/0/all/0/1\">Paul Novello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fel_T/0/1/0/all/0/1\">Thomas Fel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vigouroux_D/0/1/0/all/0/1\">David Vigouroux</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Open-Sampling: Exploring Out-of-Distribution data for Re-balancing Long-tailed datasets. (arXiv:2206.08802v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2206.08802","description":"<p>Deep neural networks usually perform poorly when the training dataset suffers\nfrom extreme class imbalance. Recent studies found that directly training with\nout-of-distribution data (i.e., open-set samples) in a semi-supervised manner\nwould harm the generalization performance. In this work, we theoretically show\nthat out-of-distribution data can still be leveraged to augment the minority\nclasses from a Bayesian perspective. Based on this motivation, we propose a\nnovel method called Open-sampling, which utilizes open-set noisy labels to\nre-balance the class priors of the training dataset. For each open-set\ninstance, the label is sampled from our pre-defined distribution that is\ncomplementary to the distribution of original class priors. We empirically show\nthat Open-sampling not only re-balances the class priors but also encourages\nthe neural network to learn separable representations. Extensive experiments\ndemonstrate that our proposed method significantly outperforms existing data\nre-balancing methods and can boost the performance of existing state-of-the-art\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_H/0/1/0/all/0/1\">Hongxin Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_L/0/1/0/all/0/1\">Lue Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_R/0/1/0/all/0/1\">Renchunzi Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_L/0/1/0/all/0/1\">Lei Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_B/0/1/0/all/0/1\">Bo An</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Visual Grounding by Encouraging Consistent Gradient-based Explanations. (arXiv:2206.15462v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.15462","description":"<p>We propose a margin-based loss for vision-language model pretraining that\nencourages gradient-based explanations that are consistent with region-level\nannotations. We refer to this objective as Attention Mask Consistency (AMC) and\ndemonstrate that it produces superior visual grounding performance compared to\nmodels that rely instead on region-level annotations for explicitly training an\nobject detector such as Faster R-CNN. AMC works by encouraging gradient-based\nexplanation masks that focus their attention scores mostly within annotated\nregions of interest for images that contain such annotations. Particularly, a\nmodel trained with AMC on top of standard vision-language modeling objectives\nobtains a state-of-the-art accuracy of 86.59% in the Flickr30k visual grounding\nbenchmark, an absolute improvement of 5.48% when compared to the best previous\nmodel. Our approach also performs exceedingly well on established benchmarks\nfor referring expression comprehension and offers the added benefit by design\nof gradient-based explanations that better align with human annotations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Ziyan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kafle_K/0/1/0/all/0/1\">Kushal Kafle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dernoncourt_F/0/1/0/all/0/1\">Franck Dernoncourt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ordonez_V/0/1/0/all/0/1\">Vicente Ordonez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Feature-selected Graph Spatial Attention Network for Addictive Brain-Networks Identification. (arXiv:2207.00583v2 [q-bio.NC] UPDATED)","link":"http://arxiv.org/abs/2207.00583","description":"<p>Functional alterations in the relevant neural circuits occur from drug\naddiction over a certain period. And these significant alterations are also\nrevealed by analyzing fMRI. However, because of fMRI's high dimensionality and\npoor signal-to-noise ratio, it is challenging to encode efficient and robust\nbrain regional embeddings for both graph-level identification and region-level\nbiomarkers detection tasks between nicotine addiction (NA) and healthy control\n(HC) groups. In this work, we represent the fMRI of the rat brain as a graph\nwith biological attributes and propose a novel feature-selected graph spatial\nattention network(FGSAN) to extract the biomarkers of addiction and identify\nfrom these brain networks. Specially, a graph spatial attention encoder is\nemployed to capture the features of spatiotemporal brain networks with spatial\ninformation. The method simultaneously adopts a Bayesian feature selection\nstrategy to optimize the model and improve classification task by constraining\nfeatures. Experiments on an addiction-related neural imaging dataset show that\nthe proposed model can obtain superior performance and detect interpretable\nbiomarkers associated with addiction-relevant neural circuits.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Gong_C/0/1/0/all/0/1\">Changwei Gong</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Jing_C/0/1/0/all/0/1\">Changhong Jing</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Pan_J/0/1/0/all/0/1\">Junren Pan</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Wang_S/0/1/0/all/0/1\">Shuqiang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ABAW: Learning from Synthetic Data & Multi-Task Learning Challenges. (arXiv:2207.01138v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.01138","description":"<p>This paper describes the fourth Affective Behavior Analysis in-the-wild\n(ABAW) Competition, held in conjunction with European Conference on Computer\nVision (ECCV), 2022. The 4th ABAW Competition is a continuation of the\nCompetitions held at IEEE CVPR 2022, ICCV 2021, IEEE FG 2020 and IEEE CVPR 2017\nConferences, and aims at automatically analyzing affect. In the previous runs\nof this Competition, the Challenges targeted Valence-Arousal Estimation,\nExpression Classification and Action Unit Detection. This year the Competition\nencompasses two different Challenges: i) a Multi-Task-Learning one in which the\ngoal is to learn at the same time (i.e., in a multi-task learning setting) all\nthe three above mentioned tasks; and ii) a Learning from Synthetic Data one in\nwhich the goal is to learn to recognise the basic expressions from artificially\ngenerated data and generalise to real data. The Aff-Wild2 database is a large\nscale in-the-wild database and the first one that contains annotations for\nvalence and arousal, expressions and action units. This database is the basis\nfor the above Challenges. In more detail: i) s-Aff-Wild2 -- a static version of\nAff-Wild2 database -- has been constructed and utilized for the purposes of the\nMulti-Task-Learning Challenge; and ii) some specific frames-images from the\nAff-Wild2 database have been used in an expression manipulation manner for\ncreating the synthetic dataset, which is the basis for the Learning from\nSynthetic Data Challenge. In this paper, at first we present the two\nChallenges, along with the utilized corpora, then we outline the evaluation\nmetrics and finally present the baseline systems per Challenge, as well as\ntheir derived results. More information regarding the Competition can be found\nin the competition's website:\nhttps://ibug.doc.ic.ac.uk/resources/eccv-2023-4th-abaw/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kollias_D/0/1/0/all/0/1\">Dimitrios Kollias</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attributed Abnormality Graph Embedding for Clinically Accurate X-Ray Report Generation. (arXiv:2207.01208v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.01208","description":"<p>Automatic generation of medical reports from X-ray images can assist\nradiologists to perform the time-consuming and yet important reporting task.\nYet, achieving clinically accurate generated reports remains challenging.\nModeling the underlying abnormalities using the knowledge graph approach has\nbeen found promising in enhancing the clinical accuracy. In this paper, we\nintroduce a novel fined-grained knowledge graph structure called an attributed\nabnormality graph (ATAG). The ATAG consists of interconnected abnormality nodes\nand attribute nodes, allowing it to better capture the abnormality details. In\ncontrast to the existing methods where the abnormality graph was constructed\nmanually, we propose a methodology to automatically construct the fine-grained\ngraph structure based on annotations, medical reports in X-ray datasets, and\nthe RadLex radiology lexicon. We then learn the ATAG embedding using a deep\nmodel with an encoder-decoder architecture for the report generation. In\nparticular, graph attention networks are explored to encode the relationships\namong the abnormalities and their attributes. A gating mechanism is adopted and\nintegrated with various decoders for the generation. We carry out extensive\nexperiments based on the benchmark datasets, and show that the proposed\nATAG-based deep model outperforms the SOTA methods by a large margin and can\nimprove the clinical accuracy of the generated reports.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Sixing Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheung_W/0/1/0/all/0/1\">William K. Cheung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiu_K/0/1/0/all/0/1\">Keith Chiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_T/0/1/0/all/0/1\">Terence M. Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheung_C/0/1/0/all/0/1\">Charles K. Cheung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+See_S/0/1/0/all/0/1\">Simon See</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detection of ADHD based on Eye Movements during Natural Viewing. (arXiv:2207.01377v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.01377","description":"<p>Attention-deficit/hyperactivity disorder (ADHD) is a neurodevelopmental\ndisorder that is highly prevalent and requires clinical specialists to\ndiagnose. It is known that an individual's viewing behavior, reflected in their\neye movements, is directly related to attentional mechanisms and higher-order\ncognitive processes. We therefore explore whether ADHD can be detected based on\nrecorded eye movements together with information about the video stimulus in a\nfree-viewing task. To this end, we develop an end-to-end deep learning-based\nsequence model which we pre-train on a related task for which more data are\navailable. We find that the method is in fact able to detect ADHD and\noutperforms relevant baselines. We investigate the relevance of the input\nfeatures in an ablation study. Interestingly, we find that the model's\nperformance is closely related to the content of the video, which provides\ninsights for future experimental designs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shuwen Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prasse_P/0/1/0/all/0/1\">Paul Prasse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reich_D/0/1/0/all/0/1\">David R. Reich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dziemian_S/0/1/0/all/0/1\">Sabine Dziemian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stegenwallner_Schutz_M/0/1/0/all/0/1\">Maja Stegenwallner-Sch&#xfc;tz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krakowczyk_D/0/1/0/all/0/1\">Daniel Krakowczyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makowski_S/0/1/0/all/0/1\">Silvia Makowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Langer_N/0/1/0/all/0/1\">Nicolas Langer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scheffer_T/0/1/0/all/0/1\">Tobias Scheffer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jager_L/0/1/0/all/0/1\">Lena A. J&#xe4;ger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Positive-Negative Equal Contrastive Loss for Semantic Segmentation. (arXiv:2207.01417v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.01417","description":"<p>The contextual information is critical for various computer vision tasks,\nprevious works commonly design plug-and-play modules and structural losses to\neffectively extract and aggregate the global context. These methods utilize\nfine-label to optimize the model but ignore that fine-trained features are also\nprecious training resources, which can introduce preferable distribution to\nhard pixels (i.e., misclassified pixels). Inspired by contrastive learning in\nunsupervised paradigm, we apply the contrastive loss in a supervised manner and\nre-design the loss function to cast off the stereotype of unsupervised learning\n(e.g., imbalance of positives and negatives, confusion of anchors computing).\nTo this end, we propose Positive-Negative Equal contrastive loss (PNE loss),\nwhich increases the latent impact of positive embedding on the anchor and\ntreats the positive as well as negative sample pairs equally. The PNE loss can\nbe directly plugged right into existing semantic segmentation frameworks and\nleads to excellent performance with neglectable extra computational costs. We\nutilize a number of classic segmentation methods (e.g., DeepLabV3, OCRNet,\nUperNet) and backbone (e.g., ResNet, HRNet, Swin Transformer) to conduct\ncomprehensive experiments and achieve state-of-the-art performance on two\nbenchmark datasets (e.g., Cityscapes and COCO-Stuff). Our code will be publicly\navailable soon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xuan_L/0/1/0/all/0/1\">Lingfei Xuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianxiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiangyun Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fidelity of Ensemble Aggregation for Saliency Map Explanations using Bayesian Optimization Techniques. (arXiv:2207.01565v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.01565","description":"<p>In recent years, an abundance of feature attribution methods for explaining\nneural networks have been developed. Especially in the field of computer\nvision, many methods for generating saliency maps providing pixel attributions\nexist. However, their explanations often contradict each other and it is not\nclear which explanation to trust. A natural solution to this problem is the\naggregation of multiple explanations. We present and compare different\npixel-based aggregation schemes with the goal of generating a new explanation,\nwhose fidelity to the model's decision is higher than each individual\nexplanation. Using methods from the field of Bayesian Optimization, we\nincorporate the variance between the individual explanations into the\naggregation process. Additionally, we analyze the effect of multiple\nnormalization techniques on ensemble aggregation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mahlau_Y/0/1/0/all/0/1\">Yannik Mahlau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nolde_C/0/1/0/all/0/1\">Christian Nolde</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LaTeRF: Label and Text Driven Object Radiance Fields. (arXiv:2207.01583v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.01583","description":"<p>Obtaining 3D object representations is important for creating photo-realistic\nsimulators and collecting assets for AR/VR applications. Neural fields have\nshown their effectiveness in learning a continuous volumetric representation of\na scene from 2D images, but acquiring object representations from these models\nwith weak supervision remains an open challenge. In this paper we introduce\nLaTeRF, a method for extracting an object of interest from a scene given 2D\nimages of the entire scene and known camera poses, a natural language\ndescription of the object, and a small number of point-labels of object and\nnon-object points in the input images. To faithfully extract the object from\nthe scene, LaTeRF extends the NeRF formulation with an additional `objectness'\nprobability at each 3D point. Additionally, we leverage the rich latent space\nof a pre-trained CLIP model combined with our differentiable object renderer,\nto inpaint the occluded parts of the object. We demonstrate high-fidelity\nobject extraction on both synthetic and real datasets and justify our design\nchoices through an extensive ablation study.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mirzaei_A/0/1/0/all/0/1\">Ashkan Mirzaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kant_Y/0/1/0/all/0/1\">Yash Kant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kelly_J/0/1/0/all/0/1\">Jonathan Kelly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gilitschenski_I/0/1/0/all/0/1\">Igor Gilitschenski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-07-05T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}}]}]}