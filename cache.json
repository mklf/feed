{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-02-14T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Describing image focused in cognitive and visual details for visually impaired people: An approach to generating inclusive paragraphs. (arXiv:2202.05331v1 [cs.CV])","link":"http://arxiv.org/abs/2202.05331","description":"<p>Several services for people with visual disabilities have emerged recently\ndue to achievements in Assistive Technologies and Artificial Intelligence\nareas. Despite the growth in assistive systems availability, there is a lack of\nservices that support specific tasks, such as understanding the image context\npresented in online content, e.g., webinars. Image captioning techniques and\ntheir variants are limited as Assistive Technologies as they do not match the\nneeds of visually impaired people when generating specific descriptions. We\npropose an approach for generating context of webinar images combining a dense\ncaptioning technique with a set of filters, to fit the captions in our domain,\nand a language model for the abstractive summary task. The results demonstrated\nthat we can produce descriptions with higher interpretability and focused on\nthe relevant information for that group of people by combining image analysis\nmethods and neural language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fernandes_D/0/1/0/all/0/1\">Daniel Louzada Fernandes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ribeiro_M/0/1/0/all/0/1\">Marcos Henrique Fonseca Ribeiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cerqueira_F/0/1/0/all/0/1\">Fabio Ribeiro Cerqueira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_M/0/1/0/all/0/1\">Michel Melo Silva</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Including Facial Expressions in Contextual Embeddings for Sign Language Generation. (arXiv:2202.05383v1 [cs.CL])","link":"http://arxiv.org/abs/2202.05383","description":"<p>State-of-the-art sign language generation frameworks lack expressivity and\nnaturalness which is the result of only focusing manual signs, neglecting the\naffective, grammatical and semantic functions of facial expressions. The\npurpose of this work is to augment semantic representation of sign language\nthrough grounding facial expressions. We study the effect of modeling the\nrelationship between text, gloss, and facial expressions on the performance of\nthe sign generation systems. In particular, we propose a Dual Encoder\nTransformer able to generate manual signs as well as facial expressions by\ncapturing the similarities and differences found in text and sign gloss\nannotation. We take into consideration the role of facial muscle activity to\nexpress intensities of manual signs by being the first to employ facial action\nunits in sign language generation. We perform a series of experiments showing\nthat our proposed model improves the quality of automatically generated sign\nlanguage.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Viegas_C/0/1/0/all/0/1\">Carla Viegas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inan_M/0/1/0/all/0/1\">Mert &#x130;nan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quandt_L/0/1/0/all/0/1\">Lorna Quandt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alikhani_M/0/1/0/all/0/1\">Malihe Alikhani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing ASR for Stuttered Speech with Limited Data Using Detect and Pass. (arXiv:2202.05396v1 [eess.AS])","link":"http://arxiv.org/abs/2202.05396","description":"<p>It is estimated that around 70 million people worldwide are affected by a\nspeech disorder called stuttering. With recent advances in Automatic Speech\nRecognition (ASR), voice assistants are increasingly useful in our everyday\nlives. Many technologies in education, retail, telecommunication and healthcare\ncan now be operated through voice. Unfortunately, these benefits are not\naccessible for People Who Stutter (PWS). We propose a simple but effective\nmethod called 'Detect and Pass' to make modern ASR systems accessible for\nPeople Who Stutter in a limited data setting. The algorithm uses a context\naware classifier trained on a limited amount of data, to detect acoustic frames\nthat contain stutter. To improve robustness on stuttered speech, this extra\ninformation is passed on to the ASR model to be utilized during inference. Our\nexperiments show a reduction of 12.18% to 71.24% in Word Error Rate (WER)\nacross various state of the art ASR systems. Upon varying the threshold of the\nassociated posterior probability of stutter for each stacked frame used in\ndetermining low frame rate (LFR) acoustic features, we were able to determine\nan optimal setting that reduced the WER by 23.93% to 71.67% across different\nASR systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Shonibare_O/0/1/0/all/0/1\">Olabanji Shonibare</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tong_X/0/1/0/all/0/1\">Xiaosu Tong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ravichandran_V/0/1/0/all/0/1\">Venkatesh Ravichandran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dual Task Framework for Debiasing Persona-grounded Dialogue Dataset. (arXiv:2202.05435v1 [cs.CL])","link":"http://arxiv.org/abs/2202.05435","description":"<p>This paper introduces a simple yet effective data-centric approach for the\ntask of improving persona-conditioned dialogue agents. Prior model-centric\napproaches unquestioningly depend on the raw crowdsourced benchmark datasets\nsuch as Persona-Chat. In contrast, we aim to fix annotation artifacts in\nbenchmarking, which is orthogonally applicable to any dialogue model.\nSpecifically, we augment relevant personas to improve dialogue dataset/agent,\nby leveraging the primal-dual structure of the two tasks, predicting dialogue\nresponses and personas based on each other. Experiments on Persona-Chat show\nthat our approach outperforms pre-trained LMs by an 11.7 point gain in terms of\naccuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Minju Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_B/0/1/0/all/0/1\">Beong-woo Kwak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Youngwook Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hong-in Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Seung-won Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeo_J/0/1/0/all/0/1\">Jinyoung Yeo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ACORT: A Compact Object Relation Transformer for Parameter Efficient Image Captioning. (arXiv:2202.05451v1 [cs.CV])","link":"http://arxiv.org/abs/2202.05451","description":"<p>Recent research that applies Transformer-based architectures to image\ncaptioning has resulted in state-of-the-art image captioning performance,\ncapitalising on the success of Transformers on natural language tasks.\nUnfortunately, though these models work well, one major flaw is their large\nmodel sizes. To this end, we present three parameter reduction methods for\nimage captioning Transformers: Radix Encoding, cross-layer parameter sharing,\nand attention parameter sharing. By combining these methods, our proposed ACORT\nmodels have 3.7x to 21.6x fewer parameters than the baseline model without\ncompromising test performance. Results on the MS-COCO dataset demonstrate that\nour ACORT models are competitive against baselines and SOTA approaches, with\nCIDEr score &gt;=126. Finally, we present qualitative results and ablation studies\nto demonstrate the efficacy of the proposed changes further. Code and\npre-trained models are publicly available at\nhttps://github.com/jiahuei/sparse-image-captioning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1\">Jia Huei Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1\">Ying Hua Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_C/0/1/0/all/0/1\">Chee Seng Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chuah_J/0/1/0/all/0/1\">Joon Huang Chuah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hindi/Bengali Sentiment Analysis Using Transfer Learning and Joint Dual Input Learning with Self Attention. (arXiv:2202.05457v1 [cs.CL])","link":"http://arxiv.org/abs/2202.05457","description":"<p>Sentiment Analysis typically refers to using natural language processing,\ntext analysis and computational linguistics to extract affect and emotion based\ninformation from text data. Our work explores how we can effectively use deep\nneural networks in transfer learning and joint dual input learning settings to\neffectively classify sentiments and detect hate speech in Hindi and Bengali\ndata. We start by training Word2Vec word embeddings for Hindi \\textbf{HASOC\ndataset} and Bengali hate speech and then train LSTM and subsequently, employ\nparameter sharing based transfer learning to Bengali sentiment classifiers by\nreusing and fine-tuning the trained weights of Hindi classifiers with both\nclassifier being used as baseline in our study. Finally, we use BiLSTM with\nself attention in joint dual input learning setting where we train a single\nneural network on Hindi and Bengali dataset simultaneously using their\nrespective embeddings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1\">Shahrukh Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahid_M/0/1/0/all/0/1\">Mahnoor Shahid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Weakly-Supervised Text Spotting using a Multi-Task Transformer. (arXiv:2202.05508v1 [cs.CV])","link":"http://arxiv.org/abs/2202.05508","description":"<p>Text spotting end-to-end methods have recently gained attention in the\nliterature due to the benefits of jointly optimizing the text detection and\nrecognition components. Existing methods usually have a distinct separation\nbetween the detection and recognition branches, requiring exact annotations for\nthe two tasks. We introduce TextTranSpotter (TTS), a transformer-based approach\nfor text spotting and the first text spotting framework which may be trained\nwith both fully- and weakly-supervised settings. By learning a single latent\nrepresentation per word detection, and using a novel loss function based on the\nHungarian loss, our method alleviates the need for expensive localization\nannotations. Trained with only text transcription annotations on real data, our\nweakly-supervised method achieves competitive performance with previous\nstate-of-the-art fully-supervised methods. When trained in a fully-supervised\nmanner, TextTranSpotter shows state-of-the-art results on multiple benchmarks\n\\footnote {Our code will be publicly available upon publication.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kittenplon_Y/0/1/0/all/0/1\">Yair Kittenplon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lavi_I/0/1/0/all/0/1\">Inbal Lavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fogel_S/0/1/0/all/0/1\">Sharon Fogel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bar_Y/0/1/0/all/0/1\">Yarin Bar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manmatha_R/0/1/0/all/0/1\">R. Manmatha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perona_P/0/1/0/all/0/1\">Pietro Perona</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What Does it Mean for a Language Model to Preserve Privacy?. (arXiv:2202.05520v1 [stat.ML])","link":"http://arxiv.org/abs/2202.05520","description":"<p>Natural language reflects our private lives and identities, making its\nprivacy concerns as broad as those of real life. Language models lack the\nability to understand the context and sensitivity of text, and tend to memorize\nphrases present in their training sets. An adversary can exploit this tendency\nto extract training data. Depending on the nature of the content and the\ncontext in which this data was collected, this could violate expectations of\nprivacy. Thus there is a growing interest in techniques for training language\nmodels that preserve privacy. In this paper, we discuss the mismatch between\nthe narrow assumptions made by popular data protection techniques (data\nsanitization and differential privacy), and the broadness of natural language\nand of privacy as a social norm. We argue that existing protection methods\ncannot guarantee a generic and meaningful notion of privacy for language\nmodels. We conclude that language models should be trained on text data which\nwas explicitly produced for public use.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Brown_H/0/1/0/all/0/1\">Hannah Brown</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lee_K/0/1/0/all/0/1\">Katherine Lee</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Mireshghalla_F/0/1/0/all/0/1\">Fatemehsadat Mireshghalla</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Shokri_R/0/1/0/all/0/1\">Reza Shokri</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Tramer_F/0/1/0/all/0/1\">Florian Tram&#xe8;r</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ClidSum: A Benchmark Dataset for Cross-Lingual Dialogue Summarization. (arXiv:2202.05599v1 [cs.CL])","link":"http://arxiv.org/abs/2202.05599","description":"<p>We present ClidSum, a benchmark dataset for building cross-lingual\nsummarization systems on dialogue documents. It consists of 67k+ dialogue\ndocuments from two subsets (i.e., SAMSum and MediaSum) and 112k+ annotated\nsummaries in different target languages. Based on the proposed ClidSum, we\nintroduce two benchmark settings for supervised and semi-supervised scenarios,\nrespectively. We then build various baseline systems in different paradigms\n(pipeline and end-to-end) and conduct extensive experiments on ClidSum to\nprovide deeper analyses. Furthermore, we propose mDialBART which extends\nmBART-50 (a multi-lingual BART) via further pre-training. The multiple\nobjectives used in the further pre-training stage help the pre-trained model\ncapture the structural characteristics as well as important content in\ndialogues and the transformation from source to the target language.\nExperimental results show the superiority of mDialBART, as an end-to-end model,\noutperforms strong pipeline models on ClidSum. Finally, we discuss specific\nchallenges that current approaches faced with this task and give multiple\npromising directions for future research. We have released the dataset and code\nat https://github.com/krystalan/ClidSum.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiaan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Ziyao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_D/0/1/0/all/0/1\">Duo Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhixu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_J/0/1/0/all/0/1\">Jianfeng Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GenderedNews: Une approche computationnelle des \\'ecarts de repr\\'esentation des genres dans la presse fran\\c{c}aise. (arXiv:2202.05682v1 [cs.CL])","link":"http://arxiv.org/abs/2202.05682","description":"<p>In this article, we present GenderedNews (https://gendered-news.imag.fr), an\nonline dashboard which gives weekly measures of gender imbalance in French\nonline press. We use Natural Language Processing (NLP) methods to quantify\ngender inequalities in the media, in the wake of global projects like the\nGlobal Media Monitoring Project. Such projects are instrumental in highlighting\ngender imbalance in the media and its very slow evolution. However, their\ngeneralisation is limited by their sampling and cost in terms of time, data and\nstaff. Automation allows us to offer complementary measures to quantify\ninequalities in gender representation. We understand representation as the\npresence and distribution of men and women mentioned and quoted in the news --\nas opposed to representation as stereotypification. In this paper, we first\nreview different means adopted by previous studies on gender inequality in the\nmedia : qualitative content analysis, quantitative content analysis and\ncomputational methods. We then detail the methods adopted by {\\it GenderedNews}\nand the two metrics implemented: the masculinity rate of mentions and the\nproportion of men quoted in online news. We describe the data collected daily\n(seven main titles of French online news media) and the methodology behind our\nmetrics, as well as a few visualisations. We finally propose to illustrate\npossible analysis of our data by conducting an in-depth observation of a sample\nof two months of our database.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Richard_A/0/1/0/all/0/1\">Ange Richard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bastin_G/0/1/0/all/0/1\">Gilles Bastin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Portet_F/0/1/0/all/0/1\">Fran&#xe7;ois Portet</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HaT5: Hate Language Identification using Text-to-Text Transfer Transformer. (arXiv:2202.05690v1 [cs.CL])","link":"http://arxiv.org/abs/2202.05690","description":"<p>We investigate the performance of a state-of-the art (SoTA) architecture T5\n(available on the SuperGLUE) and compare with it 3 other previous SoTA\narchitectures across 5 different tasks from 2 relatively diverse datasets. The\ndatasets are diverse in terms of the number and types of tasks they have. To\nimprove performance, we augment the training data by using an autoregressive\nmodel. We achieve near-SoTA results on a couple of the tasks - macro F1 scores\nof 81.66% for task A of the OLID 2019 dataset and 82.54% for task A of the hate\nspeech and offensive content (HASOC) 2021 dataset, where SoTA are 82.9% and\n83.05%, respectively. We perform error analysis and explain why one of the\nmodels (Bi-LSTM) makes the predictions it does by using a publicly available\nalgorithm: Integrated Gradient (IG). This is because explainable artificial\nintelligence (XAI) is essential for earning the trust of users. The main\ncontributions of this work are the implementation method of T5, which is\ndiscussed; the data augmentation using a new conversational AI model\ncheckpoint, which brought performance improvements; and the revelation on the\nshortcomings of HASOC 2021 dataset. It reveals the difficulties of poor data\nannotation by using a small set of examples where the T5 model made the correct\npredictions, even when the ground truth of the test set were incorrect (in our\nopinion). We also provide our model checkpoints on the HuggingFace hub1 to\nfoster transparency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sabry_S/0/1/0/all/0/1\">Sana Sabah Sabry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adewumi_T/0/1/0/all/0/1\">Tosin Adewumi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abid_N/0/1/0/all/0/1\">Nosheen Abid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kovacs_G/0/1/0/all/0/1\">Gy&#xf6;rgy Kovacs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liwicki_F/0/1/0/all/0/1\">Foteini Liwicki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liwicki_M/0/1/0/all/0/1\">Marcus Liwicki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Constrained Optimization with Dynamic Bound-scaling for Effective NLPBackdoor Defense. (arXiv:2202.05749v1 [cs.CL])","link":"http://arxiv.org/abs/2202.05749","description":"<p>We develop a novel optimization method for NLPbackdoor inversion. We leverage\na dynamically reducing temperature coefficient in the softmax function to\nprovide changing loss landscapes to the optimizer such that the process\ngradually focuses on the ground truth trigger, which is denoted as a one-hot\nvalue in a convex hull. Our method also features a temperature rollback\nmechanism to step away from local optimals, exploiting the observation that\nlocal optimals can be easily deter-mined in NLP trigger inversion (while not in\ngeneral optimization). We evaluate the technique on over 1600 models (with\nroughly half of them having injected backdoors) on 3 prevailing NLP tasks, with\n4 different backdoor attacks and 7 architectures. Our results show that the\ntechnique is able to effectively and efficiently detect and remove backdoors,\noutperforming 4 baseline methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_G/0/1/0/all/0/1\">Guangyu Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yingqi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_G/0/1/0/all/0/1\">Guanhong Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qiuling Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhuo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_S/0/1/0/all/0/1\">Shengwei An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shiqing Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangyu Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using Random Perturbations to Mitigate Adversarial Attacks on Sentiment Analysis Models. (arXiv:2202.05758v1 [cs.CL])","link":"http://arxiv.org/abs/2202.05758","description":"<p>Attacks on deep learning models are often difficult to identify and therefore\nare difficult to protect against. This problem is exacerbated by the use of\npublic datasets that typically are not manually inspected before use. In this\npaper, we offer a solution to this vulnerability by using, during testing,\nrandom perturbations such as spelling correction if necessary, substitution by\nrandom synonym, or simply dropping the word. These perturbations are applied to\nrandom words in random sentences to defend NLP models against adversarial\nattacks. Our Random Perturbations Defense and Increased Randomness Defense\nmethods are successful in returning attacked models to similar accuracy of\nmodels before attacks. The original accuracy of the model used in this work is\n80% for sentiment classification. After undergoing attacks, the accuracy drops\nto accuracy between 0% and 44%. After applying our defense methods, the\naccuracy of the model is returned to the original accuracy within statistical\nsignificance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Swenor_A/0/1/0/all/0/1\">Abigail Swenor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalita_J/0/1/0/all/0/1\">Jugal Kalita</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"White-Box Attacks on Hate-speech BERT Classifiers in German with Explicit and Implicit Character Level Defense. (arXiv:2202.05778v1 [cs.CL])","link":"http://arxiv.org/abs/2202.05778","description":"<p>In this work, we evaluate the adversarial robustness of BERT models trained\non German Hate Speech datasets. We also complement our evaluation with two\nnovel white-box character and word level attacks thereby contributing to the\nrange of attacks available. Furthermore, we also perform a comparison of two\nnovel character-level defense strategies and evaluate their robustness with one\nanother.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1\">Shahrukh Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahid_M/0/1/0/all/0/1\">Mahnoor Shahid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_N/0/1/0/all/0/1\">Navdeeppal Singh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Modal Knowledge Graph Construction and Application: A Survey. (arXiv:2202.05786v1 [cs.AI])","link":"http://arxiv.org/abs/2202.05786","description":"<p>Recent years have witnessed the resurgence of knowledge engineering which is\nfeatured by the fast growth of knowledge graphs. However, most of existing\nknowledge graphs are represented with pure symbols, which hurts the machine's\ncapability to understand the real world. The multi-modalization of knowledge\ngraphs is an inevitable key step towards the realization of human-level machine\nintelligence. The results of this endeavor are Multi-modal Knowledge Graphs\n(MMKGs). In this survey on MMKGs constructed by texts and images, we first give\ndefinitions of MMKGs, followed with the preliminaries on multi-modal tasks and\ntechniques. We then systematically review the challenges, progresses and\nopportunities on the construction and application of MMKGs respectively, with\ndetailed analyses of the strength and weakness of different solutions. We\nfinalize this survey with open research problems relevant to MMKGs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiangru Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhixu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaodan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xueyao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_P/0/1/0/all/0/1\">Penglei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuwu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yanghua Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_N/0/1/0/all/0/1\">Nicholas Jing Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating MT Systems: A Theoretical Framework. (arXiv:2202.05806v1 [cs.CL])","link":"http://arxiv.org/abs/2202.05806","description":"<p>This paper outlines a theoretical framework using which different automatic\nmetrics can be designed for evaluation of Machine Translation systems. It\nintroduces the concept of {\\em cognitive ease} which depends on {\\em adequacy}\nand {\\em lack of fluency}. Thus, cognitive ease becomes the main parameter to\nbe measured rather than comprehensibility. The framework allows the components\nof cognitive ease to be broken up and computed based on different linguistic\nlevels etc. Independence of dimensions and linearly combining them provides for\na highly modular approach.\n</p>\n<p>The paper places the existing automatic methods in an overall framework, to\nunderstand them better and to improve upon them in future. It can also be used\nto evaluate the newer types of MT systems, such as speech to speech translation\nand discourse translation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sangal_R/0/1/0/all/0/1\">Rajeev Sangal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Post-hoc Interpretability for Neural NLP: A Survey. (arXiv:2108.04840v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.04840","description":"<p>Neural networks for NLP are becoming increasingly complex and widespread, and\nthere is a growing concern if these models are responsible to use. Explaining\nmodels helps to address the safety and ethical concerns and is essential for\naccountability. Interpretability serves to provide these explanations in terms\nthat are understandable to humans. Additionally, post-hoc methods provide\nexplanations after a model is learned and are generally model-agnostic. This\nsurvey provides a categorization of how recent post-hoc interpretability\nmethods communicate explanations to humans, it discusses each method in-depth,\nand how they are validated, as the latter is often a common concern.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Madsen_A/0/1/0/all/0/1\">Andreas Madsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1\">Siva Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandar_S/0/1/0/all/0/1\">Sarath Chandar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AdapterHub Playground: Simple and Flexible Few-Shot Learning with Adapters. (arXiv:2108.08103v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.08103","description":"<p>The open-access dissemination of pretrained language models through online\nrepositories has led to a democratization of state-of-the-art natural language\nprocessing (NLP) research. This also allows people outside of NLP to use such\nmodels and adapt them to specific use-cases. However, a certain amount of\ntechnical proficiency is still required which is an entry barrier for users who\nwant to apply these models to a certain task but lack the necessary knowledge\nor resources. In this work, we aim to overcome this gap by providing a tool\nwhich allows researchers to leverage pretrained models without writing a single\nline of code. Built upon the parameter-efficient adapter modules for transfer\nlearning, our AdapterHub Playground provides an intuitive interface, allowing\nthe usage of adapters for prediction, training and analysis of textual data for\na variety of NLP tasks. We present the tool's architecture and demonstrate its\nadvantages with prototypical use-cases, where we show that predictive\nperformance can easily be increased in a few-shot learning scenario. Finally,\nwe evaluate its usability in a user study. We provide the code and a live\ninterface at https://adapter-hub.github.io/playground.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Beck_T/0/1/0/all/0/1\">Tilman Beck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bohlender_B/0/1/0/all/0/1\">Bela Bohlender</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Viehmann_C/0/1/0/all/0/1\">Christina Viehmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hane_V/0/1/0/all/0/1\">Vincent Hane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adamson_Y/0/1/0/all/0/1\">Yanik Adamson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khuri_J/0/1/0/all/0/1\">Jaber Khuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brossmann_J/0/1/0/all/0/1\">Jonas Brossmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfeiffer_J/0/1/0/all/0/1\">Jonas Pfeiffer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Differentiable Prompt Makes Pre-trained Language Models Better Few-shot Learners. (arXiv:2108.13161v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.13161","description":"<p>Large-scale pre-trained language models have contributed significantly to\nnatural language processing by demonstrating remarkable abilities as few-shot\nlearners. However, their effectiveness depends mainly on scaling the model\nparameters and prompt design, hindering their implementation in most real-world\napplications. This study proposes a novel pluggable, extensible, and efficient\napproach named DifferentiAble pRompT (DART), which can convert small language\nmodels into better few-shot learners without any prompt engineering. The main\nprinciple behind this approach involves reformulating potential natural\nlanguage processing tasks into the task of a pre-trained language model and\ndifferentially optimizing the prompt template as well as the target label with\nbackpropagation. Furthermore, the proposed approach can be: (i) Plugged to any\npre-trained language models; (ii) Extended to widespread classification tasks.\nA comprehensive evaluation of standard NLP tasks demonstrates that the proposed\napproach achieves a better few-shot performance. Code is available in\nhttps://github.com/zjunlp/DART.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Luoqiu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_Z/0/1/0/all/0/1\">Zhen Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Combining Unsupervised and Text Augmented Semi-Supervised Learning for Low Resourced Autoregressive Speech Recognition. (arXiv:2110.15836v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.15836","description":"<p>Recent advances in unsupervised representation learning have demonstrated the\nimpact of pretraining on large amounts of read speech. We adapt these\ntechniques for domain adaptation in low-resource -- both in terms of data and\ncompute -- conversational and broadcast domains. Moving beyond CTC, we pretrain\nstate-of-the-art Conformer models in an unsupervised manner. While the\nunsupervised approach outperforms traditional semi-supervised training, the\ntechniques are complementary. Combining the techniques is a 5% absolute\nimprovement in WER, averaged over all conditions, compared to semi-supervised\ntraining alone. Additional text data is incorporated through external language\nmodels. By using CTC-based decoding, we are better able to take advantage of\nthe additional text data. When used as a transcription model, it allows the\nConformer model to better incorporate the knowledge from the language model\nthrough semi-supervised training than shallow fusion. Final performance is an\nadditional 2% better absolute when using CTC-based decoding for semi-supervised\ntraining compared to shallow fusion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chak-Fai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keith_F/0/1/0/all/0/1\">Francis Keith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hartmann_W/0/1/0/all/0/1\">William Hartmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snover_M/0/1/0/all/0/1\">Matthew Snover</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TiltedBERT: Resource Adjustable Version of BERT. (arXiv:2201.03327v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.03327","description":"<p>In this paper, we proposed a novel adjustable finetuning method that improves\nthe training and inference time of the BERT model on downstream tasks. In the\nproposed method, we first detect more important word vectors in each layer by\nour proposed redundancy metric and then eliminate the less important word\nvectors with our proposed strategy. In our method, the word vector elimination\nrate in each layer is controlled by the Tilt-Rate hyper-parameter, and the\nmodel learns to work with a considerably lower number of Floating Point\nOperations (FLOPs) than the original BERTbase model. Our proposed method does\nnot need any extra training steps, and also it can be generalized to other\ntransformer-based models. We perform extensive experiments that show the word\nvectors in higher layers have an impressive amount of redundancy that can be\neliminated and decrease the training and inference time. Experimental results\non extensive sentiment analysis, classification and regression datasets, and\nbenchmarks like IMDB and GLUE showed that our proposed method is effective in\nvarious datasets. By applying our method on the BERTbase model, we decrease the\ninference time up to 5.3 times with less than 0.85% accuracy degradation on\naverage. After the fine-tuning stage, the inference time of our model can be\nadjusted with our method offline-tuning property for a wide range of the\nTilt-Rate value selections. Also, we propose a mathematical speedup analysis\nthat can estimate the speedup of our method accurately. With the help of this\nanalysis, the proper Tilt-Rate value can be selected before fine-tuning or\nwhile offline-tuning stages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kachuee_S/0/1/0/all/0/1\">Sajjad Kachuee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharifkhani_M/0/1/0/all/0/1\">Mohammad Sharifkhani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Knowledge Integration in Language Models with Graph Convolutions. (arXiv:2202.00964v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.00964","description":"<p>Pretrained language models (LMs) do not capture factual knowledge very well.\nThis has led to the development of a number of knowledge integration (KI)\nmethods which aim to incorporate external knowledge into pretrained LMs. Even\nthough KI methods show some performance gains over vanilla LMs, the\ninner-workings of these methods are not well-understood. For instance, it is\nunclear how and what kind of knowledge is effectively integrated into these\nmodels and if such integration may lead to catastrophic forgetting of already\nlearned knowledge. This paper revisits the KI process in these models with an\ninformation-theoretic view and shows that KI can be interpreted using a graph\nconvolution operation. We propose a probe model called \\textit{Graph\nConvolution Simulator} (GCS) for interpreting knowledge-enhanced LMs and\nexposing what kind of knowledge is integrated into these models. We conduct\nexperiments to verify that our GCS can indeed be used to correctly interpret\nthe KI process, and we use it to analyze two well-known knowledge-enhanced LMs:\nERNIE and K-Adapter, and find that only a small amount of factual knowledge is\nintegrated in them. We stratify knowledge in terms of various relation types\nand find that ERNIE and K-Adapter integrate different kinds of knowledge to\ndifferent extent. Our analysis also shows that simply increasing the size of\nthe KI corpus may not lead to better KI; fundamental advances may be needed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yifan Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_G/0/1/0/all/0/1\">Guoji Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-02-13T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"On Real-time Image Reconstruction with Neural Networks for MRI-guided Radiotherapy. (arXiv:2202.05267v1 [physics.med-ph])","link":"http://arxiv.org/abs/2202.05267","description":"<p>MRI-guidance techniques that dynamically adapt radiation beams to follow\ntumor motion in real-time will lead to more accurate cancer treatments and\nreduced collateral healthy tissue damage. The gold-standard for reconstruction\nof undersampled MR data is compressed sensing (CS) which is computationally\nslow and limits the rate that images can be available for real-time adaptation.\nHere, we demonstrate the use of automated transform by manifold approximation\n(AUTOMAP), a generalized framework that maps raw MR signal to the target image\ndomain, to rapidly reconstruct images from undersampled radial k-space data.\nThe AUTOMAP neural network was trained to reconstruct images from a\ngolden-angle radial acquisition, a benchmark for motion-sensitive imaging, on\nlung cancer patient data and generic images from ImageNet. Model training was\nsubsequently augmented with motion-encoded k-space data derived from videos in\nthe YouTube-8M dataset to encourage motion robust reconstruction. We find that\nAUTOMAP-reconstructed radial k-space has equivalent accuracy to CS but with\nmuch shorter processing times after initial fine-tuning on retrospectively\nacquired lung cancer patient data. Validation of motion-trained models with a\nvirtual dynamic lung tumor phantom showed that the generalized motion\nproperties learned from YouTube lead to improved target tracking accuracy. Our\nwork shows that AUTOMAP can achieve real-time, accurate reconstruction of\nradial data. These findings imply that neural-network-based reconstruction is\npotentially superior to existing approaches for real-time image guidance\napplications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Waddington_D/0/1/0/all/0/1\">David E. J. Waddington</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Hindley_N/0/1/0/all/0/1\">Nicholas Hindley</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Koonjoo_N/0/1/0/all/0/1\">Neha Koonjoo</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Chiu_C/0/1/0/all/0/1\">Christopher Chiu</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Reynolds_T/0/1/0/all/0/1\">Tess Reynolds</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Liu_P/0/1/0/all/0/1\">Paul Z. Y. Liu</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Zhu_B/0/1/0/all/0/1\">Bo Zhu</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Bhutto_D/0/1/0/all/0/1\">Danyal Bhutto</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Paganelli_C/0/1/0/all/0/1\">Chiara Paganelli</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Keall_P/0/1/0/all/0/1\">Paul J. Keall</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Rosen_M/0/1/0/all/0/1\">Matthew S. Rosen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HNF-Netv2 for Brain Tumor Segmentation using multi-modal MR Imaging. (arXiv:2202.05268v1 [eess.IV])","link":"http://arxiv.org/abs/2202.05268","description":"<p>In our previous work, $i.e.$, HNF-Net, high-resolution feature representation\nand light-weight non-local self-attention mechanism are exploited for brain\ntumor segmentation using multi-modal MR imaging. In this paper, we extend our\nHNF-Net to HNF-Netv2 by adding inter-scale and intra-scale semantic\ndiscrimination enhancing blocks to further exploit global semantic\ndiscrimination for the obtained high-resolution features. We trained and\nevaluated our HNF-Netv2 on the multi-modal Brain Tumor Segmentation Challenge\n(BraTS) 2021 dataset. The result on the test set shows that our HNF-Netv2\nachieved the average Dice scores of 0.878514, 0.872985, and 0.924919, as well\nas the Hausdorff distances ($95\\%$) of 8.9184, 16.2530, and 4.4895 for the\nenhancing tumor, tumor core, and whole tumor, respectively. Our method won the\nRSNA 2021 Brain Tumor AI Challenge Prize (Segmentation Task), which ranks 8th\nout of all 1250 submitted results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Jia_H/0/1/0/all/0/1\">Haozhe Jia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bai_C/0/1/0/all/0/1\">Chao Bai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cai_W/0/1/0/all/0/1\">Weidong Cai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_H/0/1/0/all/0/1\">Heng Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xia_Y/0/1/0/all/0/1\">Yong Xia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Plug-and-Play Approach to Multiparametric Quantitative MRI: Image Reconstruction using Pre-Trained Deep Denoisers. (arXiv:2202.05269v1 [eess.IV])","link":"http://arxiv.org/abs/2202.05269","description":"<p>Current spatiotemporal deep learning approaches to Magnetic Resonance\nFingerprinting (MRF) build artefact-removal models customised to a particular\nk-space subsampling pattern which is used for fast (compressed) acquisition.\nThis may not be useful when the acquisition process is unknown during training\nof the deep learning model and/or changes during testing time. This paper\nproposes an iterative deep learning plug-and-play reconstruction approach to\nMRF which is adaptive to the forward acquisition process. Spatiotemporal image\npriors are learned by an image denoiser i.e. a Convolutional Neural Network\n(CNN), trained to remove generic white gaussian noise (not a particular\nsubsampling artefact) from data. This CNN denoiser is then used as a\ndata-driven shrinkage operator within the iterative reconstruction algorithm.\nThis algorithm with the same denoiser model is then tested on two simulated\nacquisition processes with distinct subsampling patterns. The results show\nconsistent de-aliasing performance against both acquisition schemes and\naccurate mapping of tissues' quantitative bio-properties. Software available:\nhttps://github.com/ketanfatania/QMRI-PnP-Recon-POC\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Fatania_K/0/1/0/all/0/1\">Ketan Fatania</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pirkl_C/0/1/0/all/0/1\">Carolin M. Pirkl</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Menzel_M/0/1/0/all/0/1\">Marion I. Menzel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hall_P/0/1/0/all/0/1\">Peter Hall</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Golbabaee_M/0/1/0/all/0/1\">Mohammad Golbabaee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Deep Learning Approach for Digital ColorReconstruction of Lenticular Films. (arXiv:2202.05270v1 [eess.IV])","link":"http://arxiv.org/abs/2202.05270","description":"<p>We propose the first accurate digitization and color reconstruction process\nfor historical lenticular film that is robust to artifacts. Lenticular films\nemerged in the 1920s and were one of the first technologies that permitted to\ncapture full color information in motion. The technology leverages an RGB\nfilter and cylindrical lenticules embossed on the film surface to encode the\ncolor in the horizontal spatial dimension of the image. To project the pictures\nthe encoding process was reversed using an appropriate analog device. In this\nwork, we introduce an automated, fully digital pipeline to process the scan of\nlenticular films and colorize the image. Our method merges deep learning with a\nmodel-based approach in order to maximize the performance while making sure\nthat the reconstructed colored images truthfully match the encoded color\ninformation. Our model employs different strategies to achieve an effective\ncolor reconstruction, in particular (i) we use data augmentation to create a\nrobust lenticule segmentation network, (ii) we fit the lenticules raster\nprediction to obtain a precise vectorial lenticule localization, and (iii) we\ntrain a colorization network that predicts interpolation coefficients in order\nto obtain a truthful colorization. We validate the proposed method on a\nlenticular film dataset and compare it to other approaches. Since no colored\ngroundtruth is available as reference, we conduct a user study to validate our\nmethod in a subjective manner. The results of the study show that the proposed\nmethod is largely preferred with respect to other existing and baseline\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+DAronco_S/0/1/0/all/0/1\">Stefano D&#x27;Aronco</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Trumpy_G/0/1/0/all/0/1\">Giorgio Trumpy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pfluger_D/0/1/0/all/0/1\">David Pfluger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wegner_J/0/1/0/all/0/1\">Jan Dirk Wegner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Field of Experts Prior for Adapting Neural Networks at Test Time. (arXiv:2202.05271v1 [cs.CV])","link":"http://arxiv.org/abs/2202.05271","description":"<p>Performance of convolutional neural networks (CNNs) in image analysis tasks\nis often marred in the presence of acquisition-related distribution shifts\nbetween training and test images. Recently, it has been proposed to tackle this\nproblem by fine-tuning trained CNNs for each test image. Such\ntest-time-adaptation (TTA) is a promising and practical strategy for improving\nrobustness to distribution shifts as it requires neither data sharing between\ninstitutions nor annotating additional data. Previous TTA methods use a helper\nmodel to increase similarity between outputs and/or features extracted from a\ntest image with those of the training images. Such helpers, which are typically\nmodeled using CNNs, can be task-specific and themselves vulnerable to\ndistribution shifts in their inputs. To overcome these problems, we propose to\ncarry out TTA by matching the feature distributions of test and training\nimages, as modelled by a field-of-experts (FoE) prior. FoEs model complicated\nprobability distributions as products of many simpler expert distributions. We\nuse 1D marginal distributions of a trained task CNN's features as experts in\nthe FoE model. Further, we compute principal components of patches of the task\nCNN's features, and consider the distributions of PCA loadings as additional\nexperts. We validate the method on 5 MRI segmentation tasks (healthy tissues in\n4 anatomical regions and lesions in 1 one anatomy), using data from 17 clinics,\nand on a MRI registration task, using data from 3 clinics. We find that the\nproposed FoE-based TTA is generically applicable in multiple tasks, and\noutperforms all previous TTA methods for lesion segmentation. For healthy\ntissue segmentation, the proposed method outperforms other task-agnostic\nmethods, but a previous TTA method which is specifically designed for\nsegmentation performs the best for most of the tested datasets. Our code is\npublicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karani_N/0/1/0/all/0/1\">Neerav Karani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brunner_G/0/1/0/all/0/1\">Georg Brunner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erdil_E/0/1/0/all/0/1\">Ertunc Erdil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_S/0/1/0/all/0/1\">Simin Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tezcan_K/0/1/0/all/0/1\">Kerem Tezcan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaitanya_K/0/1/0/all/0/1\">Krishna Chaitanya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konukoglu_E/0/1/0/all/0/1\">Ender Konukoglu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards a Guideline for Evaluation Metrics in Medical Image Segmentation. (arXiv:2202.05273v1 [eess.IV])","link":"http://arxiv.org/abs/2202.05273","description":"<p>In the last decade, research on artificial intelligence has seen rapid growth\nwith deep learning models, especially in the field of medical image\nsegmentation. Various studies demonstrated that these models have powerful\nprediction capabilities and achieved similar results as clinicians. However,\nrecent studies revealed that the evaluation in image segmentation studies lacks\nreliable model performance assessment and showed statistical bias by incorrect\nmetric implementation or usage. Thus, this work provides an overview and\ninterpretation guide on the following metrics for medical image segmentation\nevaluation in binary as well as multi-class problems: Dice similarity\ncoefficient, Jaccard, Sensitivity, Specificity, Rand index, ROC curves, Cohen's\nKappa, and Hausdorff distance. As a summary, we propose a guideline for\nstandardized medical image segmentation evaluation to improve evaluation\nquality, reproducibility, and comparability in the research field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Muller_D/0/1/0/all/0/1\">Dominik M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Soto_Rey_I/0/1/0/all/0/1\">I&#xf1;aki Soto-Rey</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kramer_F/0/1/0/all/0/1\">Frank Kramer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Motion Puzzle: Arbitrary Motion Style Transfer by Body Part. (arXiv:2202.05274v1 [cs.GR])","link":"http://arxiv.org/abs/2202.05274","description":"<p>This paper presents Motion Puzzle, a novel motion style transfer network that\nadvances the state-of-the-art in several important respects. The Motion Puzzle\nis the first that can control the motion style of individual body parts,\nallowing for local style editing and significantly increasing the range of\nstylized motions. Designed to keep the human's kinematic structure, our\nframework extracts style features from multiple style motions for different\nbody parts and transfers them locally to the target body parts. Another major\nadvantage is that it can transfer both global and local traits of motion style\nby integrating the adaptive instance normalization and attention modules while\nkeeping the skeleton topology. Thus, it can capture styles exhibited by dynamic\nmovements, such as flapping and staggering, significantly better than previous\nwork. In addition, our framework allows for arbitrary motion style transfer\nwithout datasets with style labeling or motion pairing, making many publicly\navailable motion datasets available for training. Our framework can be easily\nintegrated with motion generation frameworks to create many applications, such\nas real-time motion transfer. We demonstrate the advantages of our framework\nwith a number of examples and comparisons with previous work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jang_D/0/1/0/all/0/1\">Deok-Kyeong Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Soomin Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sung-Hee Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Face Beneath the Ink: Synthetic Data and Tattoo Removal with Application to Face Recognition. (arXiv:2202.05297v1 [cs.CV])","link":"http://arxiv.org/abs/2202.05297","description":"<p>Systems that analyse faces have seen significant improvements in recent years\nand are today used in numerous application scenarios. However, these systems\nhave been found to be negatively affected by facial alterations such as\ntattoos. To better understand and mitigate the effect of facial tattoos in\nfacial analysis systems, large datasets of images of individuals with and\nwithout tattoos are needed. To this end, we propose a generator for\nautomatically adding realistic tattoos to facial images. Moreover, we\ndemonstrate the feasibility of the generation by training a deep learning-based\nmodel for removing tattoos from face images. The experimental results show that\nit is possible to remove facial tattoos from real images without degrading the\nquality of the image. Additionally, we show that it is possible to improve face\nrecognition accuracy by using the proposed deep learning-based tattoo removal\nbefore extracting and comparing facial features.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ibsen_M/0/1/0/all/0/1\">Mathias Ibsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rathgeb_C/0/1/0/all/0/1\">Christian Rathgeb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drozdowski_P/0/1/0/all/0/1\">Pawel Drozdowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Busch_C/0/1/0/all/0/1\">Christoph Busch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Characterizing and overcoming the greedy nature of learning in multi-modal deep neural networks. (arXiv:2202.05306v1 [cs.LG])","link":"http://arxiv.org/abs/2202.05306","description":"<p>We hypothesize that due to the greedy nature of learning in multi-modal deep\nneural networks, these models tend to rely on just one modality while\nunder-fitting the other modalities. Such behavior is counter-intuitive and\nhurts the models' generalization, as we observe empirically. To estimate the\nmodel's dependence on each modality, we compute the gain on the accuracy when\nthe model has access to it in addition to another modality. We refer to this\ngain as the conditional utilization rate. In the experiments, we consistently\nobserve an imbalance in conditional utilization rates between modalities,\nacross multiple tasks and architectures. Since conditional utilization rate\ncannot be computed efficiently during training, we introduce a proxy for it\nbased on the pace at which the model learns from each modality, which we refer\nto as the conditional learning speed. We propose an algorithm to balance the\nconditional learning speeds between modalities during training and demonstrate\nthat it indeed addresses the issue of greedy learning. The proposed algorithm\nimproves the model's generalization on three datasets: Colored MNIST, Princeton\nModelNet40, and NVIDIA Dynamic Hand Gesture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_N/0/1/0/all/0/1\">Nan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jastrzebski_S/0/1/0/all/0/1\">Stanis&#x142;aw Jastrz&#x119;bski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1\">Kyunghyun Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geras_K/0/1/0/all/0/1\">Krzysztof J. Geras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mining the manifolds of deep generative models for multiple data-consistent solutions of ill-posed tomographic imaging problems. (arXiv:2202.05311v1 [eess.IV])","link":"http://arxiv.org/abs/2202.05311","description":"<p>Tomographic imaging is in general an ill-posed inverse problem. Typically, a\nsingle regularized image estimate of the sought-after object is obtained from\ntomographic measurements. However, there may be multiple objects that are all\nconsistent with the same measurement data. The ability to generate such\nalternate solutions is important because it may enable new assessments of\nimaging systems. In principle, this can be achieved by means of posterior\nsampling methods. In recent years, deep neural networks have been employed for\nposterior sampling with promising results. However, such methods are not yet\nfor use with large-scale tomographic imaging applications. On the other hand,\nempirical sampling methods may be computationally feasible for large-scale\nimaging systems and enable uncertainty quantification for practical\napplications. Empirical sampling involves solving a regularized inverse problem\nwithin a stochastic optimization framework in order to obtain alternate\ndata-consistent solutions. In this work, we propose a new empirical sampling\nmethod that computes multiple solutions of a tomographic inverse problem that\nare consistent with the same acquired measurement data. The method operates by\nrepeatedly solving an optimization problem in the latent space of a style-based\ngenerative adversarial network (StyleGAN), and was inspired by the Photo\nUpsampling via Latent Space Exploration (PULSE) method that was developed for\nsuper-resolution tasks. The proposed method is demonstrated and analyzed via\nnumerical studies that involve two stylized tomographic imaging modalities.\nThese studies establish the ability of the method to perform efficient\nempirical sampling and uncertainty quantification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bhadra_S/0/1/0/all/0/1\">Sayantan Bhadra</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Villa_U/0/1/0/all/0/1\">Umberto Villa</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Anastasio_M/0/1/0/all/0/1\">Mark A. Anastasio</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Describing image focused in cognitive and visual details for visually impaired people: An approach to generating inclusive paragraphs. (arXiv:2202.05331v1 [cs.CV])","link":"http://arxiv.org/abs/2202.05331","description":"<p>Several services for people with visual disabilities have emerged recently\ndue to achievements in Assistive Technologies and Artificial Intelligence\nareas. Despite the growth in assistive systems availability, there is a lack of\nservices that support specific tasks, such as understanding the image context\npresented in online content, e.g., webinars. Image captioning techniques and\ntheir variants are limited as Assistive Technologies as they do not match the\nneeds of visually impaired people when generating specific descriptions. We\npropose an approach for generating context of webinar images combining a dense\ncaptioning technique with a set of filters, to fit the captions in our domain,\nand a language model for the abstractive summary task. The results demonstrated\nthat we can produce descriptions with higher interpretability and focused on\nthe relevant information for that group of people by combining image analysis\nmethods and neural language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fernandes_D/0/1/0/all/0/1\">Daniel Louzada Fernandes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ribeiro_M/0/1/0/all/0/1\">Marcos Henrique Fonseca Ribeiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cerqueira_F/0/1/0/all/0/1\">Fabio Ribeiro Cerqueira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_M/0/1/0/all/0/1\">Michel Melo Silva</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning the Pedestrian-Vehicle Interaction for Pedestrian Trajectory Prediction. (arXiv:2202.05334v1 [cs.CV])","link":"http://arxiv.org/abs/2202.05334","description":"<p>In this paper, we study the interaction between pedestrians and vehicles and\npropose a novel neural network structure called the Pedestrian-Vehicle\nInteraction (PVI) extractor for learning the pedestrian-vehicle interaction. We\nimplement the proposed PVI extractor on both sequential approaches (long\nshort-term memory (LSTM) models) and non-sequential approaches (convolutional\nmodels). We use the Waymo Open Dataset that contains real-world urban traffic\nscenes with both pedestrian and vehicle annotations. For the LSTM-based models,\nour proposed model is compared with Social-LSTM and Social-GAN, and using our\nproposed PVI extractor reduces the average displacement error (ADE) and the\nfinal displacement error (FDE) by 7.46% and 5.24%, respectively. For the\nconvolutional-based models, our proposed model is compared with Social-STGCNN\nand Social-IWSTCNN, and using our proposed PVI extractor reduces the ADE and\nFDE by 2.10% and 1.27%, respectively. The results show that the\npedestrian-vehicle interaction influences pedestrian behavior, and the models\nusing the proposed PVI extractor can capture the interaction between\npedestrians and vehicles, and thereby outperform the compared methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chi Zhang</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Berger_C/0/1/0/all/0/1\">Christian Berger</a> (1) ((1) Department of Computer Science and Engineering, University of Gothenburg, Gothenburg, Sweden)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Background Subtraction by Generative Neural Networks. (arXiv:2202.05336v1 [eess.IV])","link":"http://arxiv.org/abs/2202.05336","description":"<p>Background subtraction is a significant task in computer vision and an\nessential step for many real world applications. One of the challenges for\nbackground subtraction methods is dynamic background, which constitute\nstochastic movements in some parts of the background. In this paper, we have\nproposed a new background subtraction method, called DBSGen, which uses two\ngenerative neural networks, one for dynamic motion removal and another for\nbackground generation. At the end, the foreground moving objects are obtained\nby a pixel-wise distance threshold based on a dynamic entropy map. The proposed\nmethod has a unified framework that can be optimized in an end-to-end and\nunsupervised fashion. The performance of the method is evaluated over dynamic\nbackground sequences and it outperforms most of state-of-the-art methods. Our\ncode is publicly available at https://github.com/FatemeBahri/DBSGen.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bahri_F/0/1/0/all/0/1\">Fateme Bahri</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ray_N/0/1/0/all/0/1\">Nilanjan Ray</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Coded ResNeXt: a network for designing disentangled information paths. (arXiv:2202.05343v1 [cs.CV])","link":"http://arxiv.org/abs/2202.05343","description":"<p>To avoid treating neural networks as highly complex black boxes, the deep\nlearning research community has tried to build interpretable models allowing\nhumans to understand the decisions taken by the model. Unfortunately, the focus\nis mostly on manipulating only the very high-level features associated with the\nlast layers. In this work, we look at neural network architectures for\nclassification in a more general way and introduce an algorithm which defines\nbefore the training the paths of the network through which the per-class\ninformation flows. We show that using our algorithm we can extract a lighter\nsingle-purpose binary classifier for a particular class by removing the\nparameters that do not participate in the predefined information path of that\nclass, which is approximately 60% of the total parameters. Notably, leveraging\ncoding theory to design the information paths enables us to use intermediate\nnetwork layers for making early predictions without having to evaluate the full\nnetwork. We demonstrate that a slightly modified ResNeXt model, trained with\nour algorithm, can achieve higher classification accuracy on CIFAR-10/100 and\nImageNet than the original ResNeXt, while having all the aforementioned\nproperties.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Avranas_A/0/1/0/all/0/1\">Apostolos Avranas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kountouris_M/0/1/0/all/0/1\">Marios Kountouris</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain Adversarial Training: A Game Perspective. (arXiv:2202.05352v1 [cs.LG])","link":"http://arxiv.org/abs/2202.05352","description":"<p>The dominant line of work in domain adaptation has focused on learning\ninvariant representations using domain-adversarial training. In this paper, we\ninterpret this approach from a game theoretical perspective. Defining optimal\nsolutions in domain-adversarial training as a local Nash equilibrium, we show\nthat gradient descent in domain-adversarial training can violate the asymptotic\nconvergence guarantees of the optimizer, oftentimes hindering the transfer\nperformance. Our analysis leads us to replace gradient descent with high-order\nODE solvers (i.e., Runge-Kutta), for which we derive asymptotic convergence\nguarantees. This family of optimizers is significantly more stable and allows\nmore aggressive learning rates, leading to high performance gains when used as\na drop-in replacement over standard optimizers. Our experiments show that in\nconjunction with state-of-the-art domain-adversarial methods, we achieve up to\n3.5% improvement with less than of half training iterations. Our optimizers are\neasy to implement, free of additional parameters, and can be plugged into any\ndomain-adversarial framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Acuna_D/0/1/0/all/0/1\">David Acuna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Law_M/0/1/0/all/0/1\">Marc T Law</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guojun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fidler_S/0/1/0/all/0/1\">Sanja Fidler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Optimal Transport for Super Resolution Applied to Astronomy Imaging. (arXiv:2202.05354v1 [eess.IV])","link":"http://arxiv.org/abs/2202.05354","description":"<p>Super resolution is an essential tool in optics, especially on interstellar\nscales, due to physical laws restricting possible imaging resolution. We\npropose using optimal transport and entropy for super resolution applications.\nWe prove that the reconstruction is accurate when sparsity is known and noise\nor distortion is small enough. We prove that the optimizer is stable and robust\nto noise and perturbations. We compare this method to a state of the art\nconvolutional neural network and get similar results for much less\ncomputational cost and greater methodological flexibility.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Rawson_M/0/1/0/all/0/1\">Michael Rawson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hultgren_J/0/1/0/all/0/1\">Jakob Hultgren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The MeLa BitChute Dataset. (arXiv:2202.05364v1 [cs.SI])","link":"http://arxiv.org/abs/2202.05364","description":"<p>In this paper we present a near-complete dataset of over 3M videos from 61K\nchannels over 2.5 years (June 2019 to December 2021) from the social video\nhosting platform BitChute, a commonly used alternative to YouTube.\nAdditionally, we include a variety of video-level metadata, including comments,\nchannel descriptions, and views for each video. The MeLa-BitChute dataset can\nbe found at:\nhttps://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/KRD1VS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Trujillo_M/0/1/0/all/0/1\">Milo Trujillo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gruppi_M/0/1/0/all/0/1\">Maur&#xed;cio Gruppi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buntain_C/0/1/0/all/0/1\">Cody Buntain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horne_B/0/1/0/all/0/1\">Benjamin D. Horne</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Give me a knee radiograph, I will tell you where the knee joint area is: a deep convolutional neural network adventure. (arXiv:2202.05382v1 [eess.IV])","link":"http://arxiv.org/abs/2202.05382","description":"<p>Knee pain is undoubtedly the most common musculoskeletal symptom that impairs\nquality of life, confines mobility and functionality across all ages. Knee pain\nis clinically evaluated by routine radiographs, where the widespread adoption\nof radiographic images and their availability at low cost, make them the\nprinciple component in the assessment of knee pain and knee pathologies, such\nas arthritis, trauma, and sport injuries. However, interpretation of the knee\nradiographs is still highly subjective, and overlapping structures within the\nradiographs and the large volume of images needing to be analyzed on a daily\nbasis, make interpretation challenging for both naive and experienced\npractitioners. There is thus a need to implement an artificial intelligence\nstrategy to objectively and automatically interpret knee radiographs,\nfacilitating triage of abnormal radiographs in a timely fashion. The current\nwork proposes an accurate and effective pipeline for autonomous detection,\nlocalization, and classification of knee joint area in plain radiographs\ncombining the You Only Look Once (YOLO v3) deep convolutional neural network\nwith a large and fully-annotated knee radiographs dataset. The present work is\nexpected to stimulate more interest from the deep learning computer vision\ncommunity to this pragmatic and clinical application.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yan_S/0/1/0/all/0/1\">Shi Yan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ramazanian_T/0/1/0/all/0/1\">Taghi Ramazanian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sagheb_E/0/1/0/all/0/1\">Elham Sagheb</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kremers_W/0/1/0/all/0/1\">Walter K. Kremers</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chaudhary_V/0/1/0/all/0/1\">Vipin Chaudhary</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Taunton_M/0/1/0/all/0/1\">Michael Taunton</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kremers_H/0/1/0/all/0/1\">Hilal Maradit Kremers</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tafti_A/0/1/0/all/0/1\">Ahmad P. Tafti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Including Facial Expressions in Contextual Embeddings for Sign Language Generation. (arXiv:2202.05383v1 [cs.CL])","link":"http://arxiv.org/abs/2202.05383","description":"<p>State-of-the-art sign language generation frameworks lack expressivity and\nnaturalness which is the result of only focusing manual signs, neglecting the\naffective, grammatical and semantic functions of facial expressions. The\npurpose of this work is to augment semantic representation of sign language\nthrough grounding facial expressions. We study the effect of modeling the\nrelationship between text, gloss, and facial expressions on the performance of\nthe sign generation systems. In particular, we propose a Dual Encoder\nTransformer able to generate manual signs as well as facial expressions by\ncapturing the similarities and differences found in text and sign gloss\nannotation. We take into consideration the role of facial muscle activity to\nexpress intensities of manual signs by being the first to employ facial action\nunits in sign language generation. We perform a series of experiments showing\nthat our proposed model improves the quality of automatically generated sign\nlanguage.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Viegas_C/0/1/0/all/0/1\">Carla Viegas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inan_M/0/1/0/all/0/1\">Mert &#x130;nan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quandt_L/0/1/0/all/0/1\">Lorna Quandt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alikhani_M/0/1/0/all/0/1\">Malihe Alikhani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Incremental Learning of Structured Memory via Closed-Loop Transcription. (arXiv:2202.05411v1 [cs.CV])","link":"http://arxiv.org/abs/2202.05411","description":"<p>This work proposes a minimal computational model for learning a structured\nmemory of multiple object classes in an incremental setting. Our approach is\nbased on establishing a closed-loop transcription between multiple classes and\ntheir corresponding subspaces, known as a linear discriminative representation,\nin a low-dimensional feature space. Our method is both simpler and more\nefficient than existing approaches to incremental learning, in terms of model\nsize, storage, and computation: it requires only a single, fixed-capacity\nautoencoding network with a feature space that is used for both discriminative\nand generative purposes. All network parameters are optimized simultaneously\nwithout architectural manipulations, by solving a constrained minimax game\nbetween the encoding and decoding maps over a single rate reduction-based\nobjective. Experimental results show that our method can effectively alleviate\ncatastrophic forgetting, achieving significantly better performance than prior\nwork for both generative and discriminative purposes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tong_S/0/1/0/all/0/1\">Shengbang Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1\">Xili Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Ziyang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mingyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_B/0/1/0/all/0/1\">Brent Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yi Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ACORT: A Compact Object Relation Transformer for Parameter Efficient Image Captioning. (arXiv:2202.05451v1 [cs.CV])","link":"http://arxiv.org/abs/2202.05451","description":"<p>Recent research that applies Transformer-based architectures to image\ncaptioning has resulted in state-of-the-art image captioning performance,\ncapitalising on the success of Transformers on natural language tasks.\nUnfortunately, though these models work well, one major flaw is their large\nmodel sizes. To this end, we present three parameter reduction methods for\nimage captioning Transformers: Radix Encoding, cross-layer parameter sharing,\nand attention parameter sharing. By combining these methods, our proposed ACORT\nmodels have 3.7x to 21.6x fewer parameters than the baseline model without\ncompromising test performance. Results on the MS-COCO dataset demonstrate that\nour ACORT models are competitive against baselines and SOTA approaches, with\nCIDEr score &gt;=126. Finally, we present qualitative results and ablation studies\nto demonstrate the efficacy of the proposed changes further. Code and\npre-trained models are publicly available at\nhttps://github.com/jiahuei/sparse-image-captioning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1\">Jia Huei Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1\">Ying Hua Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_C/0/1/0/all/0/1\">Chee Seng Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chuah_J/0/1/0/all/0/1\">Joon Huang Chuah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WAD-CMSN: Wasserstein Distance based Cross-Modal Semantic Network for Zero-Shot Sketch-Based Image Retrieval. (arXiv:2202.05465v1 [cs.CV])","link":"http://arxiv.org/abs/2202.05465","description":"<p>Zero-shot sketch-based image retrieval (ZSSBIR), as a popular studied branch\nof computer vision, attracts wide attention recently. Unlike sketch-based image\nretrieval (SBIR), the main aim of ZSSBIR is to retrieve natural images given\nfree hand-drawn sketches that may not appear during training. Previous\napproaches used semantic aligned sketch-image pairs or utilized memory\nexpensive fusion layer for projecting the visual information to a low\ndimensional subspace, which ignores the significant heterogeneous cross-domain\ndiscrepancy between highly abstract sketch and relevant image. This may yield\npoor performance in the training phase. To tackle this issue and overcome this\ndrawback, we propose a Wasserstein distance based cross-modal semantic network\n(WAD-CMSN) for ZSSBIR. Specifically, it first projects the visual information\nof each branch (sketch, image) to a common low dimensional semantic subspace\nvia Wasserstein distance in an adversarial training manner. Furthermore,\nidentity matching loss is employed to select useful features, which can not\nonly capture complete semantic knowledge, but also alleviate the over-fitting\nphenomenon caused by the WAD-CMSN model. Experimental results on the\nchallenging Sketchy (Extended) and TU-Berlin (Extended) datasets indicate the\neffectiveness of the proposed WAD-CMSN model over several competitors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guanglong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhensheng Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jia Cai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bench-Marking And Improving Arabic Automatic Image Captioning Through The Use Of Multi-Task Learning Paradigm. (arXiv:2202.05474v1 [cs.CV])","link":"http://arxiv.org/abs/2202.05474","description":"<p>The continuous increase in the use of social media and the visual content on\nthe internet have accelerated the research in computer vision field in general\nand the image captioning task in specific. The process of generating a caption\nthat best describes an image is a useful task for various applications such as\nit can be used in image indexing and as a hearing aid for the visually\nimpaired. In recent years, the image captioning task has witnessed remarkable\nadvances regarding both datasets and architectures, and as a result, the\ncaptioning quality has reached an astounding performance. However, the majority\nof these advances especially in datasets are targeted for English, which left\nother languages such as Arabic lagging behind. Although Arabic language, being\nspoken by more than 450 million people and being the most growing language on\nthe internet, lacks the fundamental pillars it needs to advance its image\ncaptioning research, such as benchmarks or unified datasets. This works is an\nattempt to expedite the synergy in this task by providing unified datasets and\nbenchmarks, while also exploring methods and techniques that could enhance the\nperformance of Arabic image captioning. The use of multi-task learning is\nexplored, alongside exploring various word representations and different\nfeatures. The results showed that the use of multi-task learning and\npre-trained word embeddings noticeably enhanced the quality of image\ncaptioning, however the presented results shows that Arabic captioning still\nlags behind when compared to the English language. The used dataset and code\nare available at this link.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zater_M/0/1/0/all/0/1\">Muhy Eddin Za&#x27;ter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talaftha_B/0/1/0/all/0/1\">Bashar Talaftha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exemplar-free Online Continual Learning. (arXiv:2202.05491v1 [cs.CV])","link":"http://arxiv.org/abs/2202.05491","description":"<p>Targeted for real world scenarios, online continual learning aims to learn\nnew tasks from sequentially available data under the condition that each data\nis observed only once by the learner. Though recent works have made remarkable\nachievements by storing part of learned task data as exemplars for knowledge\nreplay, the performance is greatly relied on the size of stored exemplars while\nthe storage consumption is a significant constraint in continual learning. In\naddition, storing exemplars may not always be feasible for certain applications\ndue to privacy concerns. In this work, we propose a novel exemplar-free method\nby leveraging nearest-class-mean (NCM) classifier where the class mean is\nestimated during training phase on all data seen so far through online mean\nupdate criteria. We focus on image classification task and conduct extensive\nexperiments on benchmark datasets including CIFAR-100 and Food-1k. The results\ndemonstrate that our method without using any exemplar outperforms\nstate-of-the-art exemplar-based approaches with large margins under standard\nprotocol (20 exemplars per class) and is able to achieve competitive\nperformance even with larger exemplar size (100 exemplars per class).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jiangpeng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Fengqing Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Entroformer: A Transformer-based Entropy Model for Learned Image Compression. (arXiv:2202.05492v1 [eess.IV])","link":"http://arxiv.org/abs/2202.05492","description":"<p>One critical component in lossy deep image compression is the entropy model,\nwhich predicts the probability distribution of the quantized latent\nrepresentation in the encoding and decoding modules. Previous works build\nentropy models upon convolutional neural networks which are inefficient in\ncapturing global dependencies. In this work, we propose a novel\ntransformer-based entropy model, termed Entroformer, to capture long-range\ndependencies in probability distribution estimation effectively and\nefficiently. Different from vision transformers in image classification, the\nEntroformer is highly optimized for image compression, including a top-k\nself-attention and a diamond relative position encoding. Meanwhile, we further\nexpand this architecture with a parallel bidirectional context model to speed\nup the decoding process. The experiments show that the Entroformer achieves\nstate-of-the-art performance on image compression while being time-efficient.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Qian_Y/0/1/0/all/0/1\">Yichen Qian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_M/0/1/0/all/0/1\">Ming Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_X/0/1/0/all/0/1\">Xiuyu Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tan_Z/0/1/0/all/0/1\">Zhiyu Tan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jin_R/0/1/0/all/0/1\">Rong Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Modal Fusion for Sensorimotor Coordination in Steering Angle Prediction. (arXiv:2202.05500v1 [cs.CV])","link":"http://arxiv.org/abs/2202.05500","description":"<p>Imitation learning is employed to learn sensorimotor coordination for\nsteering angle prediction in an end-to-end fashion requires expert\ndemonstrations. These expert demonstrations are paired with environmental\nperception and vehicle control data. The conventional frame-based RGB camera is\nthe most common exteroceptive sensor modality used to acquire the environmental\nperception data. The frame-based RGB camera has produced promising results when\nused as a single modality in learning end-to-end lateral control. However, the\nconventional frame-based RGB camera has limited operability in illumination\nvariation conditions and is affected by the motion blur. The event camera\nprovides complementary information to the frame-based RGB camera. This work\nexplores the fusion of frame-based RGB and event data for learning end-to-end\nlateral control by predicting steering angle. In addition, how the\nrepresentation from event data fuse with frame-based RGB data helps to predict\nthe lateral control robustly for the autonomous vehicle. To this end, we\npropose DRFuser, a novel convolutional encoder-decoder architecture for\nlearning end-to-end lateral control. The encoder module is branched between the\nframe-based RGB data and event data along with the self-attention layers.\nMoreover, this study has also contributed to our own collected dataset\ncomprised of event, frame-based RGB, and vehicle control data. The efficacy of\nthe proposed method is experimentally evaluated on our collected dataset, Davis\nDriving dataset (DDD), and Carla Eventscape dataset. The experimental results\nillustrate that the proposed method DRFuser outperforms the state-of-the-art in\nterms of root-mean-square error (RMSE) and mean absolute error (MAE) used as\nevaluation metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Munir_F/0/1/0/all/0/1\">Farzeen Munir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azam_S/0/1/0/all/0/1\">Shoaib Azam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1\">Byung-Geun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeon_M/0/1/0/all/0/1\">Moongu Jeon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Weakly-Supervised Text Spotting using a Multi-Task Transformer. (arXiv:2202.05508v1 [cs.CV])","link":"http://arxiv.org/abs/2202.05508","description":"<p>Text spotting end-to-end methods have recently gained attention in the\nliterature due to the benefits of jointly optimizing the text detection and\nrecognition components. Existing methods usually have a distinct separation\nbetween the detection and recognition branches, requiring exact annotations for\nthe two tasks. We introduce TextTranSpotter (TTS), a transformer-based approach\nfor text spotting and the first text spotting framework which may be trained\nwith both fully- and weakly-supervised settings. By learning a single latent\nrepresentation per word detection, and using a novel loss function based on the\nHungarian loss, our method alleviates the need for expensive localization\nannotations. Trained with only text transcription annotations on real data, our\nweakly-supervised method achieves competitive performance with previous\nstate-of-the-art fully-supervised methods. When trained in a fully-supervised\nmanner, TextTranSpotter shows state-of-the-art results on multiple benchmarks\n\\footnote {Our code will be publicly available upon publication.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kittenplon_Y/0/1/0/all/0/1\">Yair Kittenplon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lavi_I/0/1/0/all/0/1\">Inbal Lavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fogel_S/0/1/0/all/0/1\">Sharon Fogel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bar_Y/0/1/0/all/0/1\">Yarin Bar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manmatha_R/0/1/0/all/0/1\">R. Manmatha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perona_P/0/1/0/all/0/1\">Pietro Perona</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dilated convolutional neural network-based deep reference picture generation for video compression. (arXiv:2202.05514v1 [eess.IV])","link":"http://arxiv.org/abs/2202.05514","description":"<p>Motion estimation and motion compensation are indispensable parts of inter\nprediction in video coding. Since the motion vector of objects is mostly in\nfractional pixel units, original reference pictures may not accurately provide\na suitable reference for motion compensation. In this paper, we propose a deep\nreference picture generator which can create a picture that is more relevant to\nthe current encoding frame, thereby further reducing temporal redundancy and\nimproving video compression efficiency. Inspired by the recent progress of\nConvolutional Neural Network(CNN), this paper proposes to use a dilated CNN to\nbuild the generator. Moreover, we insert the generated deep picture into\nVersatile Video Coding(VVC) as a reference picture and perform a comprehensive\nset of experiments to evaluate the effectiveness of our network on the latest\nVVC Test Model VTM. The experimental results demonstrate that our proposed\nmethod achieves on average 9.7% bit saving compared with VVC under low-delay P\nconfiguration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Tian_H/0/1/0/all/0/1\">Haoyue Tian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gao_P/0/1/0/all/0/1\">Pan Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wei_R/0/1/0/all/0/1\">Ran Wei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Paul_M/0/1/0/all/0/1\">Manoranjan Paul</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised HDR Imaging: What Can Be Learned from a Single 8-bit Video?. (arXiv:2202.05522v1 [cs.GR])","link":"http://arxiv.org/abs/2202.05522","description":"<p>Recently, Deep Learning-based methods for inverse tone-mapping standard\ndynamic range (SDR) images to obtain high dynamic range (HDR) images have\nbecome very popular. These methods manage to fill over-exposed areas\nconvincingly both in terms of details and dynamic range. Typically, these\nmethods, to be effective, need to learn from large datasets and to transfer\nthis knowledge to the network weights. In this work, we tackle this problem\nfrom a completely different perspective. What can we learn from a single SDR\nvideo? With the presented zero-shot approach, we show that, in many cases, a\nsingle SDR video is sufficient to be able to generate an HDR video of the same\nquality or better than other state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Banterle_F/0/1/0/all/0/1\">Francesco Banterle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marnerides_D/0/1/0/all/0/1\">Demetris Marnerides</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Debattista_K/0/1/0/all/0/1\">Kurt Debattista</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bashford_Rogers_T/0/1/0/all/0/1\">Thomas Bashford-Rogers</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video-driven Neural Physically-based Facial Asset for Production. (arXiv:2202.05592v1 [cs.CV])","link":"http://arxiv.org/abs/2202.05592","description":"<p>Production-level workflows for producing convincing 3D dynamic human faces\nhave long relied on a disarray of labor-intensive tools for geometry and\ntexture generation, motion capture and rigging, and expression synthesis.\nRecent neural approaches automate individual components but the corresponding\nlatent representations cannot provide artists explicit controls as in\nconventional tools. In this paper, we present a new learning-based,\nvideo-driven approach for generating dynamic facial geometries with\nhigh-quality physically-based assets. Two key components are well-structured\nlatent spaces due to dense temporal samplings from videos and explicit facial\nexpression controls to regulate the latent spaces. For data collection, we\nconstruct a hybrid multiview-photometric capture stage, coupling with an\nultra-fast video camera to obtain raw 3D facial assets. We then model the\nfacial expression, geometry and physically-based textures using separate VAEs\nwith a global MLP-based expression mapping across the latent spaces, to\npreserve characteristics across respective attributes while maintaining\nexplicit controls over geometry and texture. We also introduce to model the\ndelta information as wrinkle maps for physically-base textures, achieving\nhigh-quality rendering of dynamic textures. We demonstrate our approach in\nhigh-fidelity performer-specific facial capture and cross-identity facial\nmotion retargeting. In addition, our neural asset along with fast adaptation\nschemes can also be deployed to handle in-the-wild videos. Besides, we motivate\nthe utility of our explicit facial disentangle strategy by providing promising\nphysically-based editing results like geometry and material editing or winkle\ntransfer with high realism. Comprehensive experiments show that our technique\nprovides higher accuracy and visual fidelity than previous video-driven facial\nreconstruction and animation methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Longwen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_C/0/1/0/all/0/1\">Chuxiao Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qixuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hongyang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_R/0/1/0/all/0/1\">Ruixiang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jingyi Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Wasserstein GAN for Joint Learning of Inpainting and its Spatial Optimisation. (arXiv:2202.05623v1 [eess.IV])","link":"http://arxiv.org/abs/2202.05623","description":"<p>Classic image inpainting is a restoration method that reconstructs missing\nimage parts. However, a carefully selected mask of known pixels that yield a\nhigh quality inpainting can also act as a sparse image representation. This\nchallenging spatial optimisation problem is essential for practical\napplications such as compression. So far, it has been almost exclusively\naddressed by model-based approaches. First attempts with neural networks seem\npromising, but are tailored towards specific inpainting operators or require\npostprocessing. To address this issue, we propose the first generative\nadversarial network for spatial inpainting data optimisation. In contrast to\nprevious approaches, it allows joint training of an inpainting generator and a\ncorresponding mask optimisation network. With a Wasserstein distance, we ensure\nthat our inpainting results accurately reflect the statistics of natural\nimages. This yields significant improvements in visual quality and speed over\nconventional stochastic models and also outperforms current spatial\noptimisation networks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Peter_P/0/1/0/all/0/1\">Pascal Peter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Artemis: Articulated Neural Pets with Appearance and Motion synthesis. (arXiv:2202.05628v1 [cs.GR])","link":"http://arxiv.org/abs/2202.05628","description":"<p>We human are entering into a virtual era, and surely want to bring animals to\nvirtual world as well for companion. Yet, computer-generated (CGI) furry\nanimals is limited by tedious off-line rendering, let alone interactive motion\ncontrol. In this paper, we present ARTEMIS, a novel neural modeling and\nrendering pipeline for generating ARTiculated neural pets with appEarance and\nMotion synthesIS. Our ARTEMIS enables interactive motion control, real-time\nanimation and photo-realistic rendering of furry animals. The core of ARTEMIS\nis a neural-generated (NGI) animal engine, which adopts an efficient octree\nbased representation for animal animation and fur rendering. The animation then\nbecomes equivalent to voxel level skeleton based deformation. We further use a\nfast octree indexing, an efficient volumetric rendering scheme to generate\nappearance and density features maps. Finally, we propose a novel shading\nnetwork to generate high-fidelity details of appearance and opacity under novel\nposes. For the motion control module in ARTEMIS, we combine state-of-the-art\nanimal motion capture approach with neural character control scheme. We\nintroduce an effective optimization scheme to reconstruct skeletal motion of\nreal animals captured by a multi-view RGB and Vicon camera array. We feed the\ncaptured motion into a neural character control scheme to generate abstract\ncontrol signals with motion styles. We further integrate ARTEMIS into existing\nengines that support VR headsets, providing an unprecedented immersive\nexperience where a user can intimately interact with a variety of virtual\nanimals with vivid movements and photo-realistic appearance. Extensive\nexperiments and showcases demonstrate the effectiveness of our ARTEMIS system\nto achieve highly realistic rendering of NGI animals in real-time, providing\ndaily immersive and interactive experience with digital animals unseen before.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1\">Haimin Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Teng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yuheng Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chenglin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Q/0/1/0/all/0/1\">QIwei Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yingliang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jingyi Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vehicle and License Plate Recognition with Novel Dataset for Toll Collection. (arXiv:2202.05631v1 [eess.IV])","link":"http://arxiv.org/abs/2202.05631","description":"<p>We propose an automatic framework for toll collection, consisting of three\nsteps: vehicle type recognition, license plate localization, and reading.\nHowever, each of the three steps becomes non-trivial due to image variations\ncaused by several factors. The traditional vehicle decorations on the front\ncause variations among vehicles of the same type. These decorations make\nlicense plate localization and recognition difficult due to severe background\nclutter and partial occlusions. Likewise, on most vehicles, specifically\ntrucks, the position of the license plate is not consistent. Lastly, for\nlicense plate reading, the variations are induced by non-uniform font styles,\nsizes, and partially occluded letters and numbers. Our proposed framework takes\nadvantage of both data availability and performance evaluation of the backbone\ndeep learning architectures. We gather a novel dataset, \\emph{Diverse Vehicle\nand License Plates Dataset (DVLPD)}, consisting of 10k images belonging to six\nvehicle types. Each image is then manually annotated for vehicle type, license\nplate, and its characters and digits. For each of the three tasks, we evaluate\nYou Only Look Once (YOLO)v2, YOLOv3, YOLOv4, and FasterRCNN. For real-time\nimplementation on a Raspberry Pi, we evaluate the lighter versions of YOLO\nnamed Tiny YOLOv3 and Tiny YOLOv4. The best Mean Average Precision (mAP@0.5) of\n98.8% for vehicle type recognition, 98.5% for license plate detection, and\n98.3% for license plate reading is achieved by YOLOv4, while its lighter\nversion, i.e., Tiny YOLOv4 obtained a mAP of 97.1%, 97.4%, and 93.7% on vehicle\ntype recognition, license plate detection, and license plate reading,\nrespectively. The dataset and the training codes are available at\nhttps://github.com/usama-x930/VT-LPR\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Usama_M/0/1/0/all/0/1\">Muhammad Usama</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Anwar_H/0/1/0/all/0/1\">Hafeez Anwar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shahid_M/0/1/0/all/0/1\">Muhammad Muaz Shahid</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Anwar_A/0/1/0/all/0/1\">Abbas Anwar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Anwar_S/0/1/0/all/0/1\">Saeed Anwar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hlavacs_H/0/1/0/all/0/1\">Helmuth Hlavacs</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tiny Object Tracking: A Large-scale Dataset and A Baseline. (arXiv:2202.05659v1 [cs.CV])","link":"http://arxiv.org/abs/2202.05659","description":"<p>Tiny objects, frequently appearing in practical applications, have weak\nappearance and features, and receive increasing interests in meany vision\ntasks, such as object detection and segmentation. To promote the research and\ndevelopment of tiny object tracking, we create a large-scale video dataset,\nwhich contains 434 sequences with a total of more than 217K frames. Each frame\nis carefully annotated with a high-quality bounding box. In data creation, we\ntake 12 challenge attributes into account to cover a broad range of viewpoints\nand scene complexities, and annotate these attributes for facilitating the\nattribute-based performance analysis. To provide a strong baseline in tiny\nobject tracking, we propose a novel Multilevel Knowledge Distillation Network\n(MKDNet), which pursues three-level knowledge distillations in a unified\nframework to effectively enhance the feature representation, discrimination and\nlocalization abilities in tracking tiny objects. Extensive experiments are\nperformed on the proposed dataset, and the results prove the superiority and\neffectiveness of MKDNet compared with state-of-the-art methods. The dataset,\nthe algorithm code, and the evaluation code are available at\nhttps://github.com/mmic-lcl/Datasets-and-benchmark-code.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yabin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chenglong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jin Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_B/0/1/0/all/0/1\">Bin Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhixiang Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SuperCon: Supervised Contrastive Learning for Imbalanced Skin Lesion Classification. (arXiv:2202.05685v1 [cs.CV])","link":"http://arxiv.org/abs/2202.05685","description":"<p>Convolutional neural networks (CNNs) have achieved great success in skin\nlesion classification. A balanced dataset is required to train a good model.\nHowever, due to the appearance of different skin lesions in practice, severe or\neven deadliest skin lesion types (e.g., melanoma) naturally have quite small\namount represented in a dataset. In that, classification performance\ndegradation occurs widely, it is significantly important to have CNNs that work\nwell on class imbalanced skin lesion image dataset. In this paper, we propose\nSuperCon, a two-stage training strategy to overcome the class imbalance problem\non skin lesion classification. It contains two stages: (i) representation\ntraining that tries to learn a feature representation that closely aligned\namong intra-classes and distantly apart from inter-classes, and (ii) classifier\nfine-tuning that aims to learn a classifier that correctly predict the label\nbased on the learnt representations. In the experimental evaluation, extensive\ncomparisons have been made among our approach and other existing approaches on\nskin lesion benchmark datasets. The results show that our two-stage training\nstrategy effectively addresses the class imbalance classification problem, and\nsignificantly improves existing works in terms of F1-score and AUC score,\nresulting in state-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Keyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_D/0/1/0/all/0/1\">Di Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1\">J. Morris Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Adversarially Robust Deepfake Detection: An Ensemble Approach. (arXiv:2202.05687v1 [cs.LG])","link":"http://arxiv.org/abs/2202.05687","description":"<p>Detecting deepfakes is an important problem, but recent work has shown that\nDNN-based deepfake detectors are brittle against adversarial deepfakes, in\nwhich an adversary adds imperceptible perturbations to a deepfake to evade\ndetection. In this work, we show that a modification to the detection strategy\nin which we replace a single classifier with a carefully chosen ensemble, in\nwhich input transformations for each model in the ensemble induces pairwise\northogonal gradients, can significantly improve robustness beyond the de facto\nsolution of adversarial training. We present theoretical results to show that\nsuch orthogonal gradients can help thwart a first-order adversary by reducing\nthe dimensionality of the input subspace in which adversarial deepfakes lie. We\nvalidate the results empirically by instantiating and evaluating a randomized\nversion of such \"orthogonal\" ensembles for adversarial deepfake detection and\nfind that these randomized ensembles exhibit significantly higher robustness as\ndeepfake detectors compared to state-of-the-art deepfake detectors against\nadversarial deepfakes, even those created using strong PGD-500 attacks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hooda_A/0/1/0/all/0/1\">Ashish Hooda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mangaokar_N/0/1/0/all/0/1\">Neal Mangaokar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_R/0/1/0/all/0/1\">Ryan Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fawaz_K/0/1/0/all/0/1\">Kassem Fawaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1\">Somesh Jha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prakash_A/0/1/0/all/0/1\">Atul Prakash</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep soccer captioning with transformer: dataset, semantics-related losses, and multi-level evaluation. (arXiv:2202.05728v1 [cs.CV])","link":"http://arxiv.org/abs/2202.05728","description":"<p>This work aims at generating captions for soccer videos using deep learning.\nIn this context, this paper introduces a dataset, model, and triple-level\nevaluation. The dataset consists of 22k caption-clip pairs and three visual\nfeatures (images, optical flow, inpainting) for ~500 hours of \\emph{SoccerNet}\nvideos. The model is divided into three parts: a transformer learns language,\nConvNets learn vision, and a fusion of linguistic and visual features generates\ncaptions. The paper suggests evaluating generated captions at three levels:\nsyntax (the commonly used evaluation metrics such as BLEU-score and CIDEr),\nmeaning (the quality of descriptions for a domain expert), and corpus (the\ndiversity of generated captions). The paper shows that the diversity of\ngenerated captions has improved (from 0.07 reaching 0.18) with\nsemantics-related losses that prioritize selected words. Semantics-related\nlosses and the utilization of more visual features (optical flow, inpainting)\nimproved the normalized captioning score by 28\\%. The web page of this work:\nhttps://sites.google.com/view/soccercaptioning}{https://sites.google.com/view/soccercaptioning\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hammoudeh_A/0/1/0/all/0/1\">Ahmad Hammoudeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vanderplaetse_B/0/1/0/all/0/1\">Bastein Vanderplaetse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dupont_S/0/1/0/all/0/1\">St&#xe9;phane Dupont</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Patch-NetVLAD+: Learned patch descriptor and weighted matching strategy for place recognition. (arXiv:2202.05738v1 [cs.CV])","link":"http://arxiv.org/abs/2202.05738","description":"<p>Visual Place Recognition (VPR) in areas with similar scenes such as urban or\nindoor scenarios is a major challenge. Existing VPR methods using global\ndescriptors have difficulty capturing local specific regions (LSR) in the scene\nand are therefore prone to localization confusion in such scenarios. As a\nresult, finding the LSR that are critical for location recognition becomes key.\nTo address this challenge, we introduced Patch-NetVLAD+, which was inspired by\npatch-based VPR researches. Our method proposed a fine-tuning strategy with\ntriplet loss to make NetVLAD suitable for extracting patch-level descriptors.\nMoreover, unlike existing methods that treat all patches in an image equally,\nour method extracts patches of LSR, which present less frequently throughout\nthe dataset, and makes them play an important role in VPR by assigning proper\nweights to them. Experiments on Pittsburgh30k and Tokyo247 datasets show that\nour approach achieved up to 6.35\\% performance improvement than existing\npatch-based methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1\">Yingfeng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Junqiao Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_J/0/1/0/all/0/1\">Jiafeng Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Fenglin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_C/0/1/0/all/0/1\">Chen Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_T/0/1/0/all/0/1\">Tiantian Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Borrowing from yourself: Faster future video segmentation with partial channel update. (arXiv:2202.05748v1 [cs.CV])","link":"http://arxiv.org/abs/2202.05748","description":"<p>Semantic segmentation is a well-addressed topic in the computer vision\nliterature, but the design of fast and accurate video processing networks\nremains challenging. In addition, to run on embedded hardware, computer vision\nmodels often have to make compromises on accuracy to run at the required speed,\nso that a latency/accuracy trade-off is usually at the heart of these real-time\nsystems' design. For the specific case of videos, models have the additional\npossibility to make use of computations made for previous frames to mitigate\nthe accuracy loss while being real-time.\n</p>\n<p>In this work, we propose to tackle the task of fast future video segmentation\nprediction through the use of convolutional layers with time-dependent channel\nmasking. This technique only updates a chosen subset of the feature maps at\neach time-step, bringing simultaneously less computation and latency, and\nallowing the network to leverage previously computed features. We apply this\ntechnique to several fast architectures and experimentally confirm its benefits\nfor the future prediction subtask.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Courdier_E/0/1/0/all/0/1\">Evann Courdier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fleuret_F/0/1/0/all/0/1\">Fran&#xe7;ois Fleuret</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Assessing Privacy Risks from Feature Vector Reconstruction Attacks. (arXiv:2202.05760v1 [cs.CR])","link":"http://arxiv.org/abs/2202.05760","description":"<p>In deep neural networks for facial recognition, feature vectors are numerical\nrepresentations that capture the unique features of a given face. While it is\nknown that a version of the original face can be recovered via \"feature\nreconstruction,\" we lack an understanding of the end-to-end privacy risks\nproduced by these attacks. In this work, we address this shortcoming by\ndeveloping metrics that meaningfully capture the threat of reconstructed face\nimages. Using end-to-end experiments and user studies, we show that\nreconstructed face images enable re-identification by both commercial facial\nrecognition systems and humans, at a rate that is at worst, a factor of four\ntimes higher than randomized baselines. Our results confirm that feature\nvectors should be recognized as Personal Identifiable Information (PII) in\norder to protect user privacy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wenger_E/0/1/0/all/0/1\">Emily Wenger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Falzon_F/0/1/0/all/0/1\">Francesca Falzon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Passananti_J/0/1/0/all/0/1\">Josephine Passananti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Haitao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1\">Ben Y. Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Modal Knowledge Graph Construction and Application: A Survey. (arXiv:2202.05786v1 [cs.AI])","link":"http://arxiv.org/abs/2202.05786","description":"<p>Recent years have witnessed the resurgence of knowledge engineering which is\nfeatured by the fast growth of knowledge graphs. However, most of existing\nknowledge graphs are represented with pure symbols, which hurts the machine's\ncapability to understand the real world. The multi-modalization of knowledge\ngraphs is an inevitable key step towards the realization of human-level machine\nintelligence. The results of this endeavor are Multi-modal Knowledge Graphs\n(MMKGs). In this survey on MMKGs constructed by texts and images, we first give\ndefinitions of MMKGs, followed with the preliminaries on multi-modal tasks and\ntechniques. We then systematically review the challenges, progresses and\nopportunities on the construction and application of MMKGs respectively, with\ndetailed analyses of the strength and weakness of different solutions. We\nfinalize this survey with open research problems relevant to MMKGs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiangru Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhixu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaodan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xueyao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_P/0/1/0/all/0/1\">Penglei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuwu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yanghua Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_N/0/1/0/all/0/1\">Nicholas Jing Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Meta-learning with GANs for anomaly detection, with deployment in high-speed rail inspection system. (arXiv:2202.05795v1 [cs.LG])","link":"http://arxiv.org/abs/2202.05795","description":"<p>Anomaly detection has been an active research area with a wide range of\npotential applications. Key challenges for anomaly detection in the AI era with\nbig data include lack of prior knowledge of potential anomaly types, highly\ncomplex and noisy background in input data, scarce abnormal samples, and\nimbalanced training dataset. In this work, we propose a meta-learning framework\nfor anomaly detection to deal with these issues. Within this framework, we\nincorporate the idea of generative adversarial networks (GANs) with appropriate\nchoices of loss functions including structural similarity index measure (SSIM).\nExperiments with limited labeled data for high-speed rail inspection\ndemonstrate that our meta-learning framework is sharp and robust in identifying\nanomalies. Our framework has been deployed in five high-speed railways of China\nsince 2021: it has reduced more than 99.7% workload and saved 96.7% inspection\ntime.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_H/0/1/0/all/0/1\">Haoyang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xin Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLIPasso: Semantically-Aware Object Sketching. (arXiv:2202.05822v1 [cs.GR])","link":"http://arxiv.org/abs/2202.05822","description":"<p>Abstraction is at the heart of sketching due to the simple and minimal nature\nof line drawings. Abstraction entails identifying the essential visual\nproperties of an object or scene, which requires semantic understanding and\nprior knowledge of high-level concepts. Abstract depictions are therefore\nchallenging for artists, and even more so for machines. We present an object\nsketching method that can achieve different levels of abstraction, guided by\ngeometric and semantic simplifications. While sketch generation methods often\nrely on explicit sketch datasets for training, we utilize the remarkable\nability of CLIP (Contrastive-Language-Image-Pretraining) to distill semantic\nconcepts from sketches and images alike. We define a sketch as a set of\nB\\'ezier curves and use a differentiable rasterizer to optimize the parameters\nof the curves directly with respect to a CLIP-based perceptual loss. The\nabstraction degree is controlled by varying the number of strokes. The\ngenerated sketches demonstrate multiple levels of abstraction while maintaining\nrecognizability, underlying structure, and essential visual components of the\nsubject drawn.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vinker_Y/0/1/0/all/0/1\">Yael Vinker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pajouheshgar_E/0/1/0/all/0/1\">Ehsan Pajouheshgar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bo_J/0/1/0/all/0/1\">Jessica Y. Bo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bachmann_R/0/1/0/all/0/1\">Roman Christian Bachmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bermano_A/0/1/0/all/0/1\">Amit Haim Bermano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_Or_D/0/1/0/all/0/1\">Daniel Cohen-Or</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zamir_A/0/1/0/all/0/1\">Amir Zamir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shamir_A/0/1/0/all/0/1\">Ariel Shamir</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SafePicking: Learning Safe Object Extraction via Object-Level Mapping. (arXiv:2202.05832v1 [cs.RO])","link":"http://arxiv.org/abs/2202.05832","description":"<p>Robots need object-level scene understanding to manipulate objects while\nreasoning about contact, support, and occlusion among objects. Given a pile of\nobjects, object recognition and reconstruction can identify the boundary of\nobject instances, giving important cues as to how the objects form and support\nthe pile. In this work, we present a system, SafePicking, that integrates\nobject-level mapping and learning-based motion planning to generate a motion\nthat safely extracts occluded target objects from a pile. Planning is done by\nlearning a deep Q-network that receives observations of predicted poses and a\ndepth-based heightmap to output a motion trajectory, trained to maximize a\nsafety metric reward. Our results show that the observation fusion of poses and\ndepth-sensing gives both better performance and robustness to the model. We\nevaluate our methods using the YCB objects in both simulation and the real\nworld, achieving safe object extraction from piles.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wada_K/0/1/0/all/0/1\">Kentaro Wada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+James_S/0/1/0/all/0/1\">Stephen James</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davison_A/0/1/0/all/0/1\">Andrew J. Davison</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multiple Object Tracking: A Literature Review. (arXiv:1409.7618v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1409.7618","description":"<p>Multiple Object Tracking (MOT) has gained increasing attention due to its\nacademic and commercial potential. Although different approaches have been\nproposed to tackle this problem, it still remains challenging due to factors\nlike abrupt appearance changes and severe object occlusions. In this work, we\ncontribute the first comprehensive and most recent review on this problem. We\ninspect the recent advances in various aspects and propose some interesting\ndirections for future research. To the best of our knowledge, there has not\nbeen any extensive review on this topic in the community. We endeavor to\nprovide a thorough review on the development of this problem in recent decades.\nThe main contributions of this review are fourfold: 1) Key aspects in an MOT\nsystem, including formulation, categorization, key principles, evaluation of\nMOT are discussed; 2) Instead of enumerating individual works, we discuss\nexisting approaches according to various aspects, in each of which methods are\ndivided into different groups and each group is discussed in detail for the\nprinciples, advances and drawbacks; 3) We examine experiments of existing\npublications and summarize results on popular datasets to provide quantitative\nand comprehensive comparisons. By analyzing the results from different\nperspectives, we have verified some basic agreements in the field; and 4) We\nprovide a discussion about issues of MOT research, as well as some interesting\ndirections which will become potential research effort in the future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1\">Wenhan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_J/0/1/0/all/0/1\">Junliang Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milan_A/0/1/0/all/0/1\">Anton Milan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaoqin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Tae-Kyun Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Filter Pruning by Switching to Neighboring CNNs with Good Attributes. (arXiv:1904.03961v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1904.03961","description":"<p>Filter pruning is effective to reduce the computational costs of neural\nnetworks. Existing methods show that updating the previous pruned filter would\nenable large model capacity and achieve better performance. However, during the\niterative pruning process, even if the network weights are updated to new\nvalues, the pruning criterion remains the same. In addition, when evaluating\nthe filter importance, only the magnitude information of the filters is\nconsidered. However, in neural networks, filters do not work individually, but\nthey would affect other filters. As a result, the magnitude information of each\nfilter, which merely reflects the information of an individual filter itself,\nis not enough to judge the filter importance. To solve the above problems, we\npropose Meta-attribute-based Filter Pruning (MFP). First, to expand the\nexisting magnitude information based pruning criteria, we introduce a new set\nof criteria to consider the geometric distance of filters. Additionally, to\nexplicitly assess the current state of the network, we adaptively select the\nmost suitable criteria for pruning via a meta-attribute, a property of the\nneural network at the current state. Experiments on two image classification\nbenchmarks validate our method. For ResNet-50 on ILSVRC-2012, we could reduce\nmore than 50% FLOPs with only 0.44% top-5 accuracy loss.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Ping Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Linchao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Puzzle-AE: Novelty Detection in Images through Solving Puzzles. (arXiv:2008.12959v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2008.12959","description":"<p>Autoencoder, as an essential part of many anomaly detection methods, is\nlacking flexibility on normal data in complex datasets. U-Net is proved to be\neffective for this purpose but overfits on the training data if trained by just\nusing reconstruction error similar to other AE-based frameworks.\nPuzzle-solving, as a pretext task of self-supervised learning (SSL) methods,\nhas earlier proved its ability in learning semantically meaningful features. We\nshow that training U-Nets based on this task is an effective remedy that\nprevents overfitting and facilitates learning beyond pixel-level features.\nShortcut solutions, however, are a big challenge in SSL tasks, including jigsaw\npuzzles. We propose adversarial robust training as an effective automatic\nshortcut removal. We achieve competitive or superior results compared to the\nState of the Art (SOTA) anomaly detection methods on various toy and real-world\ndatasets. Unlike many competitors, the proposed framework is stable, fast,\ndata-efficient, and does not require unprincipled early stopping.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Salehi_M/0/1/0/all/0/1\">Mohammadreza Salehi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eftekhar_A/0/1/0/all/0/1\">Ainaz Eftekhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadjadi_N/0/1/0/all/0/1\">Niousha Sadjadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohban_M/0/1/0/all/0/1\">Mohammad Hossein Rohban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rabiee_H/0/1/0/all/0/1\">Hamid R. Rabiee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NITI: Training Integer Neural Networks Using Integer-only Arithmetic. (arXiv:2009.13108v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2009.13108","description":"<p>While integer arithmetic has been widely adopted for improved performance in\ndeep quantized neural network inference, training remains a task primarily\nexecuted using floating point arithmetic. This is because both high dynamic\nrange and numerical accuracy are central to the success of most modern training\nalgorithms. However, due to its potential for computational, storage and energy\nadvantages in hardware accelerators, neural network training methods that can\nbe implemented with low precision integer-only arithmetic remains an active\nresearch challenge. In this paper, we present NITI, an efficient deep neural\nnetwork training framework that stores all parameters and intermediate values\nas integers, and computes exclusively with integer arithmetic. A pseudo\nstochastic rounding scheme that eliminates the need for external random number\ngeneration is proposed to facilitate conversion from wider intermediate results\nto low precision storage. Furthermore, a cross-entropy loss backpropagation\nscheme computed with integer-only arithmetic is proposed. A proof-of-concept\nopen-source software implementation of NITI that utilizes native 8-bit integer\noperations in modern GPUs to achieve end-to-end training is presented. When\ncompared with an equivalent training setup implemented with floating point\nstorage and arithmetic, NITI achieves negligible accuracy degradation on the\nMNIST and CIFAR10 datasets using 8-bit integer storage and computation. On\nImageNet, 16-bit integers are needed for weight accumulation with an 8-bit\ndatapath. This achieves training results comparable to all-floating-point\nimplementations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Maolin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rasoulinezhad_S/0/1/0/all/0/1\">Seyedramin Rasoulinezhad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leong_P/0/1/0/all/0/1\">Philip H.W. Leong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+So_H/0/1/0/all/0/1\">Hayden K.H. So</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Camera Convolutional Color Constancy. (arXiv:2011.11890v6 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.11890","description":"<p>We present \"Cross-Camera Convolutional Color Constancy\" (C5), a\nlearning-based method, trained on images from multiple cameras, that accurately\nestimates a scene's illuminant color from raw images captured by a new camera\npreviously unseen during training. C5 is a hypernetwork-like extension of the\nconvolutional color constancy (CCC) approach: C5 learns to generate the weights\nof a CCC model that is then evaluated on the input image, with the CCC weights\ndynamically adapted to different input content. Unlike prior cross-camera color\nconstancy models, which are usually designed to be agnostic to the spectral\nproperties of test-set images from unobserved cameras, C5 approaches this\nproblem through the lens of transductive inference: additional unlabeled images\nare provided as input to the model at test time, which allows the model to\ncalibrate itself to the spectral properties of the test-set camera during\ninference. C5 achieves state-of-the-art accuracy for cross-camera color\nconstancy on several datasets, is fast to evaluate (~7 and ~90 ms per image on\na GPU or CPU, respectively), and requires little memory (~2 MB), and thus is a\npractical solution to the problem of calibration-free automatic white balance\nfor mobile photography.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Afifi_M/0/1/0/all/0/1\">Mahmoud Afifi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barron_J/0/1/0/all/0/1\">Jonathan T. Barron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+LeGendre_C/0/1/0/all/0/1\">Chloe LeGendre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsai_Y/0/1/0/all/0/1\">Yun-Ta Tsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bleibel_F/0/1/0/all/0/1\">Francois Bleibel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Handling Noisy Labels via One-Step Abductive Multi-Target Learning: An Application to Helicobacter Pylori Segmentation. (arXiv:2011.14956v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2011.14956","description":"<p>Learning from noisy labels is an important concern because of the lack of\naccurate ground-truth labels in plenty of real-world scenarios. In practice,\nvarious approaches for this concern first make some corrections corresponding\nto potentially noisy-labeled instances, and then update predictive model with\ninformation of the made corrections. However, in specific areas, such as\nmedical histopathology whole slide image analysis (MHWSIA), it is often\ndifficult or even impossible for experts to manually achieve the noisy-free\nground-truth labels which leads to labels with complex noise. This situation\nraises two more difficult problems: 1) the methodology of approaches making\ncorrections corresponding to potentially noisy-labeled instances has\nlimitations due to the complex noise existing in labels; and 2) the appropriate\nevaluation strategy for validation/testing is unclear because of the great\ndifficulty in collecting the noisy-free ground-truth labels. In this paper, we\nfocus on alleviating these two problems. For the problem 1), we present\none-step abductive multi-target learning (OSAMTL) that imposes a one-step\nlogical reasoning upon machine learning via a multi-target learning procedure\nto constrain the predictions of the learning model to be subject to our prior\nknowledge about the true target. For the problem 2), we propose a logical\nassessment formula (LAF) that evaluates the logical rationality of the outputs\nof an approach by estimating the consistencies between the predictions of the\nlearning model and the logical facts narrated from the results of the one-step\nlogical reasoning of OSAMTL. Applying OSAMTL and LAF to the Helicobacter pylori\n(H. pylori) segmentation task in MHWSIA, we show that OSAMTL is able to enable\nthe machine learning model achieving logically more rational predictions, which\nis beyond various state-of-the-art approaches in handling complex noisy labels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yongquan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yiming Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Jiayi Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zhongxi Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mitigating Bias in Calibration Error Estimation. (arXiv:2012.08668v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2012.08668","description":"<p>For an AI system to be reliable, the confidence it expresses in its decisions\nmust match its accuracy. To assess the degree of match, examples are typically\nbinned by confidence and the per-bin mean confidence and accuracy are compared.\nMost research in calibration focuses on techniques to reduce this empirical\nmeasure of calibration error, ECE_bin. We instead focus on assessing\nstatistical bias in this empirical measure, and we identify better estimators.\nWe propose a framework through which we can compute the bias of a particular\nestimator for an evaluation data set of a given size. The framework involves\nsynthesizing model outputs that have the same statistics as common neural\narchitectures on popular data sets. We find that binning-based estimators with\nbins of equal mass (number of instances) have lower bias than estimators with\nbins of equal width. Our results indicate two reliable calibration-error\nestimators: the debiased estimator (Brocker, 2012; Ferro and Fricker, 2012) and\na method we propose, ECE_sweep, which uses equal-mass bins and chooses the\nnumber of bins to be as large as possible while preserving monotonicity in the\ncalibration function. With these estimators, we observe improvements in the\neffectiveness of recalibration methods and in the detection of model\nmiscalibration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Roelofs_R/0/1/0/all/0/1\">Rebecca Roelofs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cain_N/0/1/0/all/0/1\">Nicholas Cain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shlens_J/0/1/0/all/0/1\">Jonathon Shlens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mozer_M/0/1/0/all/0/1\">Michael C. Mozer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Single Model Deep Learning on Imbalanced Small Datasets for Skin Lesion Classification. (arXiv:2102.01284v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.01284","description":"<p>Deep convolutional neural network (DCNN) models have been widely explored for\nskin disease diagnosis and some of them have achieved the diagnostic outcomes\ncomparable or even superior to those of dermatologists. However, broad\nimplementation of DCNN in skin disease detection is hindered by small size and\ndata imbalance of the publically accessible skin lesion datasets. This paper\nproposes a novel single-model based strategy for classification of skin lesions\non small and imbalanced datasets. First, various DCNNs are trained on different\nsmall and imbalanced datasets to verify that the models with moderate\ncomplexity outperform the larger models. Second, regularization DropOut and\nDropBlock are added to reduce overfitting and a Modified RandAugment\naugmentation strategy is proposed to deal with the defects of sample\nunderrepresentation in the small dataset. Finally, a novel Multi-Weighted New\nLoss (MWNL) function and an end-to-end cumulative learning strategy (CLS) are\nintroduced to overcome the challenge of uneven sample size and classification\ndifficulty and to reduce the impact of abnormal samples on training. By\ncombining Modified RandAugment, MWNL and CLS, our single DCNN model method\nachieved the classification accuracy comparable or superior to those of\nmultiple ensembling models on different dermoscopic image datasets. Our study\nshows that this method is able to achieve a high classification performance at\na low cost of computational resources and inference time, potentially suitable\nto implement in mobile devices for automated screening of skin lesions and many\nother malignancies in low resource settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_P/0/1/0/all/0/1\">Peng Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1\">Shuwei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mengjuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Peng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Fan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_J/0/1/0/all/0/1\">Jinyu Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_P/0/1/0/all/0/1\">Pengfei Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaffenberger_B/0/1/0/all/0/1\">Benjamin Kaffenberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ronald X. Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Graph-based Facial Affect Analysis: A Review. (arXiv:2103.15599v6 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.15599","description":"<p>As one of the most important affective signals, facial affect analysis (FAA)\nis essential for developing human-computer interaction systems. Early methods\nfocus on extracting appearance and geometry features associated with human\naffects while ignoring the latent semantic information among individual facial\nchanges, leading to limited performance and generalization. Recent work\nattempts to establish a graph-based representation to model these semantic\nrelationships and develop frameworks to leverage them for various FAA tasks.\nThis paper provides a comprehensive review of graph-based FAA, including the\nevolution of algorithms and their applications. First, the FAA background\nknowledge is introduced, especially on the role of the graph. We then discuss\napproaches widely used for graph-based affective representation in literature\nand show a trend towards graph construction. For the relational reasoning in\ngraph-based FAA, existing studies are categorized according to their non-deep\nor deep learning methods, emphasizing the latest graph neural networks.\nPerformance comparisons of the state-of-the-art graph-based FAA methods are\nalso summarized. Finally, we discuss the challenges and potential directions.\nAs far as we know, this is the first survey of graph-based FAA methods. Our\nfindings can serve as a reference for future research in this field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xingming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yante Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jinzhao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1\">Guoying Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Latent Variable Sequential Set Transformers For Joint Multi-Agent Motion Prediction. (arXiv:2104.00563v3 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2104.00563","description":"<p>Robust multi-agent trajectory prediction is essential for the safe control of\nrobotic systems. A major challenge is to efficiently learn a representation\nthat approximates the true joint distribution of contextual, social, and\ntemporal information to enable planning. We propose Latent Variable Sequential\nSet Transformers which are encoder-decoder architectures that generate\nscene-consistent multi-agent trajectories. We refer to these architectures as\n\"AutoBots\". The encoder is a stack of interleaved temporal and social\nmulti-head self-attention (MHSA) modules which alternately perform equivariant\nprocessing across the temporal and social dimensions. The decoder employs\nlearnable seed parameters in combination with temporal and social MHSA modules\nallowing it to perform inference over the entire future scene in a single\nforward pass efficiently. AutoBots can produce either the trajectory of one\nego-agent or a distribution over the future trajectories for all agents in the\nscene. For the single-agent prediction case, our model achieves top results on\nthe global nuScenes vehicle motion prediction leaderboard, and produces strong\nresults on the Argoverse vehicle prediction challenge. In the multi-agent\nsetting, we evaluate on the synthetic partition of TrajNet++ dataset to\nshowcase the model's socially-consistent predictions. We also demonstrate our\nmodel on general sequences of sets and provide illustrative experiments\nmodelling the sequential structure of the multiple strokes that make up symbols\nin the Omniglot data. A distinguishing feature of AutoBots is that all models\nare trainable on a single desktop GPU (1080 Ti) in under 48h.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Girgis_R/0/1/0/all/0/1\">Roger Girgis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golemo_F/0/1/0/all/0/1\">Florian Golemo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Codevilla_F/0/1/0/all/0/1\">Felipe Codevilla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weiss_M/0/1/0/all/0/1\">Martin Weiss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DSouza_J/0/1/0/all/0/1\">Jim Aldon D&#x27;Souza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kahou_S/0/1/0/all/0/1\">Samira Ebrahimi Kahou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heide_F/0/1/0/all/0/1\">Felix Heide</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pal_C/0/1/0/all/0/1\">Christopher Pal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multiscale Domain Adaptive YOLO for Cross-Domain Object Detection. (arXiv:2106.01483v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.01483","description":"<p>The area of domain adaptation has been instrumental in addressing the domain\nshift problem encountered by many applications. This problem arises due to the\ndifference between the distributions of source data used for training in\ncomparison with target data used during realistic testing scenarios. In this\npaper, we introduce a novel MultiScale Domain Adaptive YOLO (MS-DAYOLO)\nframework that employs multiple domain adaptation paths and corresponding\ndomain classifiers at different scales of the recently introduced YOLOv4 object\ndetector to generate domain-invariant features. We train and test our proposed\nmethod using popular datasets. Our experiments show significant improvements in\nobject detection performance when training YOLOv4 using the proposed MS-DAYOLO\nand when tested on target data representing challenging weather conditions for\nautonomous driving applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hnewa_M/0/1/0/all/0/1\">Mazin Hnewa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radha_H/0/1/0/all/0/1\">Hayder Radha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-end Trainable Deep Neural Network for Robotic Grasp Detection and Semantic Segmentation from RGB. (arXiv:2107.05287v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.05287","description":"<p>In this work, we introduce a novel, end-to-end trainable CNN-based\narchitecture to deliver high quality results for grasp detection suitable for a\nparallel-plate gripper, and semantic segmentation. Utilizing this, we propose a\nnovel refinement module that takes advantage of previously calculated grasp\ndetection and semantic segmentation and further increases grasp detection\naccuracy. Our proposed network delivers state-of-the-art accuracy on two\npopular grasp dataset, namely Cornell and Jacquard. As additional contribution,\nwe provide a novel dataset extension for the OCID dataset, making it possible\nto evaluate grasp detection in highly challenging scenes. Using this dataset,\nwe show that semantic segmentation can additionally be used to assign grasp\ncandidates to object classes, which can be used to pick specific objects in the\nscene.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ainetter_S/0/1/0/all/0/1\">Stefan Ainetter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fraundorfer_F/0/1/0/all/0/1\">Friedrich Fraundorfer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PU-Flow: a Point Cloud Upsampling Network with Normalizing Flows. (arXiv:2107.05893v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.05893","description":"<p>Point cloud upsampling aims to generate dense point clouds from given sparse\nones, which is a challenging task due to the irregular and unordered nature of\npoint sets. To address this issue, we present a novel deep learning-based\nmodel, called PU-Flow, which incorporates normalizing flows and weight\nprediction techniques to produce dense points uniformly distributed on the\nunderlying surface. Specifically, we exploit the invertible characteristics of\nnormalizing flows to transform points between Euclidean and latent spaces and\nformulate the upsampling process as ensemble of neighbouring points in a latent\nspace, where the ensemble weights are adaptively learned from local geometric\ncontext. Extensive experiments show that our method is competitive and, in most\ntest cases, it outperforms state-of-the-art methods in terms of reconstruction\nquality, proximity-to-surface accuracy, and computation efficiency. The source\ncode will be publicly available at https://github.com/unknownue/pu-flow.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mao_A/0/1/0/all/0/1\">Aihua Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Z/0/1/0/all/0/1\">Zihui Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1\">Junhui Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_Y/0/1/0/all/0/1\">Yaqi Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong-jin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Ying He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Perception-and-Regulation Network for Salient Object Detection. (arXiv:2107.12560v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.12560","description":"<p>Effective fusion of different types of features is the key to salient object\ndetection. The majority of existing network structure design is based on the\nsubjective experience of scholars and the process of feature fusion does not\nconsider the relationship between the fused features and highest-level\nfeatures. In this paper, we focus on the feature relationship and propose a\nnovel global attention unit, which we term the \"perception- and-regulation\"\n(PR) block, that adaptively regulates the feature fusion process by explicitly\nmodeling interdependencies between features. The perception part uses the\nstructure of fully-connected layers in classification networks to learn the\nsize and shape of objects. The regulation part selectively strengthens and\nweakens the features to be fused. An imitating eye observation module (IEO) is\nfurther employed for improving the global perception ability of the network.\nThe imitation of foveal vision and peripheral vision enables IEO to scrutinize\nhighly detailed objects and to organize the broad spatial scene to better\nsegment objects. Sufficient experiments conducted on SOD datasets demonstrate\nthat the proposed method performs favorably against 22 state-of-the-art\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jinchao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaoyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_X/0/1/0/all/0/1\">Xian Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_F/0/1/0/all/0/1\">Feng Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuehua_L/0/1/0/all/0/1\">Li Yuehua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Junnan Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are socially-aware trajectory prediction models really socially-aware?. (arXiv:2108.10879v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.10879","description":"<p>Our field has recently witnessed an arms race of neural network-based\ntrajectory predictors. While these predictors are at the core of many\napplications such as autonomous navigation or pedestrian flow simulations,\ntheir adversarial robustness has not been carefully studied. In this paper, we\nintroduce a socially-attended attack to assess the social understanding of\nprediction models in terms of collision avoidance. An attack is a small yet\ncarefully-crafted perturbations to fail predictors. Technically, we define\ncollision as a failure mode of the output, and propose hard- and soft-attention\nmechanisms to guide our attack. Thanks to our attack, we shed light on the\nlimitations of the current models in terms of their social understanding. We\ndemonstrate the strengths of our method on the recent trajectory prediction\nmodels. Finally, we show that our attack can be employed to increase the social\nunderstanding of state-of-the-art models. The code is available online:\nhttps://s-attack.github.io/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saadatnejad_S/0/1/0/all/0/1\">Saeed Saadatnejad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahari_M/0/1/0/all/0/1\">Mohammadhossein Bahari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khorsandi_P/0/1/0/all/0/1\">Pedram Khorsandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saneian_M/0/1/0/all/0/1\">Mohammad Saneian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moosavi_Dezfooli_S/0/1/0/all/0/1\">Seyed-Mohsen Moosavi-Dezfooli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alahi_A/0/1/0/all/0/1\">Alexandre Alahi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"YOLOP: You Only Look Once for Panoptic Driving Perception. (arXiv:2108.11250v6 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.11250","description":"<p>A panoptic driving perception system is an essential part of autonomous\ndriving. A high-precision and real-time perception system can assist the\nvehicle in making the reasonable decision while driving. We present a panoptic\ndriving perception network (YOLOP) to perform traffic object detection,\ndrivable area segmentation and lane detection simultaneously. It is composed of\none encoder for feature extraction and three decoders to handle the specific\ntasks. Our model performs extremely well on the challenging BDD100K dataset,\nachieving state-of-the-art on all three tasks in terms of accuracy and speed.\nBesides, we verify the effectiveness of our multi-task learning model for joint\ntraining via ablative studies. To our best knowledge, this is the first work\nthat can process these three visual perception tasks simultaneously in\nreal-time on an embedded device Jetson TX2(23 FPS) and maintain excellent\naccuracy. To facilitate further research, the source codes and pre-trained\nmodels will be released at https://github.com/hustvl/YOLOP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Dong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_M/0/1/0/all/0/1\">Manwen Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weitian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinggang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Decidability-Based Loss Function. (arXiv:2109.05524v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.05524","description":"<p>Nowadays, deep learning is the standard approach for a wide range of\nproblems, including biometrics, such as face recognition and speech\nrecognition, etc. Biometric problems often use deep learning models to extract\nfeatures from images, also known as embeddings. Moreover, the loss function\nused during training strongly influences the quality of the generated\nembeddings. In this work, a loss function based on the decidability index is\nproposed to improve the quality of embeddings for the verification routine. Our\nproposal, the D-loss, avoids some Triplet-based loss disadvantages such as the\nuse of hard samples and tricky parameter tuning, which can lead to slow\nconvergence. The proposed approach is compared against the Softmax\n(cross-entropy), Triplets Soft-Hard, and the Multi Similarity losses in four\ndifferent benchmarks: MNIST, Fashion-MNIST, CIFAR10 and CASIA-IrisV4. The\nachieved results show the efficacy of the proposal when compared to other\npopular metrics in the literature. The D-loss computation, besides being\nsimple, non-parametric and easy to implement, favors both the inter-class and\nintra-class scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Silva_P/0/1/0/all/0/1\">Pedro Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreira_G/0/1/0/all/0/1\">Gladston Moreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freitas_V/0/1/0/all/0/1\">Vander Freitas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_R/0/1/0/all/0/1\">Rodrigo Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menotti_D/0/1/0/all/0/1\">David Menotti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luz_E/0/1/0/all/0/1\">Eduardo Luz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UMPNet: Universal Manipulation Policy Network for Articulated Objects. (arXiv:2109.05668v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.05668","description":"<p>We introduce the Universal Manipulation Policy Network (UMPNet) -- a single\nimage-based policy network that infers closed-loop action sequences for\nmanipulating arbitrary articulated objects. To infer a wide range of action\ntrajectories, the policy supports 6DoF action representation and varying\ntrajectory length. To handle a diverse set of objects, the policy learns from\nobjects with different articulation structures and generalizes to unseen\nobjects or categories. The policy is trained with self-guided exploration\nwithout any human demonstrations, scripted policy, or pre-defined goal\nconditions. To support effective multi-step interaction, we introduce a novel\nArrow-of-Time action attribute that indicates whether an action will change the\nobject state back to the past or forward into the future. With the\nArrow-of-Time inference at each interaction step, the learned policy is able to\nselect actions that consistently lead towards or away from a given state,\nthereby, enabling both effective state exploration and goal-conditioned\nmanipulation. Video is available at https://youtu.be/KqlvcL9RqKM\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhenjia Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhanpeng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Shuran Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Holistic Semi-Supervised Approaches for EEG Representation Learning. (arXiv:2109.11732v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2109.11732","description":"<p>Recently, supervised methods, which often require substantial amounts of\nclass labels, have achieved promising results for EEG representation learning.\nHowever, labeling EEG data is a challenging task. More recently, holistic\nsemi-supervised learning approaches, which only require few output labels, have\nshown promising results in the field of computer vision. These methods,\nhowever, have not yet been adapted for EEG learning. In this paper, we adapt\nthree state-of-the-art holistic semi-supervised approaches, namely MixMatch,\nFixMatch, and AdaMatch, as well as five classical semi-supervised methods for\nEEG learning. We perform rigorous experiments with all 8 methods on two public\nEEG-based emotion recognition datasets, namely SEED and SEED-IV. The\nexperiments with different amounts of limited labeled samples show that the\nholistic approaches achieve strong results even when only 1 labeled sample is\nused per class. Further experiments show that in most cases, AdaMatch is the\nmost effective method, followed by MixMatch and FixMatch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guangyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Etemad_A/0/1/0/all/0/1\">Ali Etemad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Camera Calibration through Camera Projection Loss. (arXiv:2110.03479v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.03479","description":"<p>Camera calibration is a necessity in various tasks including 3D\nreconstruction, hand-eye coordination for a robotic interaction, autonomous\ndriving, etc. In this work we propose a novel method to predict extrinsic\n(baseline, pitch, and translation), intrinsic (focal length and principal point\noffset) parameters using an image pair. Unlike existing methods, instead of\ndesigning an end-to-end solution, we proposed a new representation that\nincorporates camera model equations as a neural network in multi-task learning\nframework. We estimate the desired parameters via novel camera projection loss\n(CPL) that uses the camera model neural network to reconstruct the 3D points\nand uses the reconstruction loss to estimate the camera parameters. To the best\nof our knowledge, ours is the first method to jointly estimate both the\nintrinsic and extrinsic parameters via a multi-task learning methodology that\ncombines analytical equations in learning framework for the estimation of\ncamera parameters. We also proposed a novel dataset using CARLA Simulator.\nEmpirically, we demonstrate that our proposed approach achieves better\nperformance with respect to both deep learning-based and traditional methods on\n8 out of 10 parameters evaluated using both synthetic and real data. Our code\nand generated dataset are available at\nhttps://github.com/thanif/Camera-Calibration-through-Camera-Projection-Loss.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Butt_T/0/1/0/all/0/1\">Talha Hanif Butt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taj_M/0/1/0/all/0/1\">Murtaza Taj</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EchoVPR: Echo State Networks for Visual Place Recognition. (arXiv:2110.05572v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.05572","description":"<p>Recognising previously visited locations is an important, but unsolved, task\nin autonomous navigation. Current visual place recognition (VPR) benchmarks\ntypically challenge models to recover the position of a query image (or images)\nfrom sequential datasets that include both spatial and temporal components.\nRecently, Echo State Network (ESN) varieties have proven particularly powerful\nat solving machine learning tasks that require spatio-temporal modelling. These\nnetworks are simple, yet powerful neural architectures that--exhibiting memory\nover multiple time-scales and non-linear high-dimensional representations--can\ndiscover temporal relations in the data while still maintaining linearity in\nthe learning time. In this paper, we present a series of ESNs and analyse their\napplicability to the VPR problem. We report that the addition of ESNs to\npre-processed convolutional neural networks led to a dramatic boost in\nperformance in comparison to non-recurrent networks in five out of six standard\nbenchmarks (GardensPoint, SPEDTest, ESSEX3IN1, Oxford RobotCar, and Nordland),\ndemonstrating that ESNs are able to capture the temporal structure inherent in\nVPR problems. Moreover, we show that models that include ESNs can outperform\nclass-leading VPR models which also exploit the sequential dynamics of the\ndata. Finally, our results demonstrate that ESNs improve generalisation\nabilities, robustness, and accuracy further supporting their suitability to VPR\napplications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ozdemir_A/0/1/0/all/0/1\">Anil Ozdemir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scerri_M/0/1/0/all/0/1\">Mark Scerri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barron_A/0/1/0/all/0/1\">Andrew B. Barron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Philippides_A/0/1/0/all/0/1\">Andrew Philippides</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mangan_M/0/1/0/all/0/1\">Michael Mangan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasilaki_E/0/1/0/all/0/1\">Eleni Vasilaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manneschi_L/0/1/0/all/0/1\">Luca Manneschi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLOOB: Modern Hopfield Networks with InfoLOOB Outperform CLIP. (arXiv:2110.11316v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.11316","description":"<p>CLIP yielded impressive results on zero-shot transfer learning tasks and is\nconsidered as a foundation model like BERT or GPT3. CLIP vision models that\nhave a rich representation are pre-trained using the InfoNCE objective and\nnatural language supervision before they are fine-tuned on the particular\ntasks. Though CLIP excels at zero-shot transfer learning, it suffers from\nexplaining away, that is, it focuses too much on few specific features and/or\ninsufficiently extracts the covariance structure in the data. The former\nproblem of focusing on few features only is caused by a saturation of the\nInfoNCE objective, which is severe for high mutual information. The latter\nproblem of insufficiently exploiting the covariance structure is caused by a\ndeficiency in extracting feature associations and co-occurrences. We introduce\n\"Contrastive Leave One Out Boost\" (CLOOB), which uses the InfoLOOB objective\nand modern Hopfield networks. In contrast to InfoNCE, the InfoLOOB objective\n(leave one out bound) does not saturate and works well for high mutual\ninformation. Modern Hopfield networks, on the other hand, allow to use\nretrieved embeddings, which have an enriched covariance structure via\nco-occurrences of stored features. We compare CLOOB to CLIP after pre-training\non the Conceptual Captions and the YFCC dataset with respect to their zero-shot\ntransfer learning performance on other datasets. CLOOB consistently outperforms\nCLIP at zero-shot transfer learning across all considered architectures and\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Furst_A/0/1/0/all/0/1\">Andreas F&#xfc;rst</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rumetshofer_E/0/1/0/all/0/1\">Elisabeth Rumetshofer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lehner_J/0/1/0/all/0/1\">Johannes Lehner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_V/0/1/0/all/0/1\">Viet Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_F/0/1/0/all/0/1\">Fei Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramsauer_H/0/1/0/all/0/1\">Hubert Ramsauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kreil_D/0/1/0/all/0/1\">David Kreil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kopp_M/0/1/0/all/0/1\">Michael Kopp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klambauer_G/0/1/0/all/0/1\">G&#xfc;nter Klambauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bitto_Nemling_A/0/1/0/all/0/1\">Angela Bitto-Nemling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hochreiter_S/0/1/0/all/0/1\">Sepp Hochreiter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Longitudinal Analysis of Mask and No-Mask on Child Face Recognition. (arXiv:2111.00121v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.00121","description":"<p>Face is one of the most widely employed traits for person recognition, even\nin many large-scale applications. Despite technological advancements in face\nrecognition systems, they still face obstacles caused by pose, expression,\nocclusion, and aging variations. Owing to the COVID-19 pandemic, contactless\nidentity verification has become exceedingly vital. Recently, few studies have\nbeen conducted on the effect of face mask on adult face recognition systems\n(FRS). However, the impact of aging with face mask on child subject recognition\nhas not been adequately explored. Thus, the main objective of this study is\nanalyzing the child longitudinal impact together with face mask and other\ncovariates on FRS. Specifically, we performed a comparative investigation of\nthree top performing publicly available face matchers and a post-COVID-19\ncommercial-off-the-shelf (COTS) system under child cross-age verification and\nidentification settings using our generated synthetic mask and no-mask samples.\nFurthermore, we investigated the longitudinal consequence of eyeglasses with\nmask and no-mask. The study exploited no-mask longitudinal child face dataset\n(i.e., extended Indian Child Longitudinal Face Dataset) that contains 26,258\nface images of 7,473 subjects in the age group of [2, 18] over an average time\nspan of 3.35 years. Due to the combined effects of face mask and face aging,\nthe FaceNet, PFE, ArcFace, and COTS face verification system accuracies\ndecrease approximately 25%, 22%, 18%, 12%, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chandaliya_P/0/1/0/all/0/1\">Praveen Kumar Chandaliya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akhtar_Z/0/1/0/all/0/1\">Zahid Akhtar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nain_N/0/1/0/all/0/1\">Neeta Nain</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Frequency-Aware Physics-Inspired Degradation Model for Real-World Image Super-Resolution. (arXiv:2111.03301v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2111.03301","description":"<p>Current learning-based single image super-resolution (SISR) algorithms\nunderperform on real data due to the deviation in the assumed degrada-tion\nprocess from that in the real-world scenario. Conventional degradation\nprocesses consider applying blur, noise, and downsampling (typicallybicubic\ndownsampling) on high-resolution (HR) images to synthesize low-resolution (LR)\ncounterparts. However, few works on degradation modelling have taken the\nphysical aspects of the optical imaging system intoconsideration. In this\npaper, we analyze the imaging system optically andexploit the characteristics\nof the real-world LR-HR pairs in the spatial frequency domain. We formulate a\nreal-world physics-inspired degradationmodel by considering\nbothopticsandsensordegradation; The physical degradation of an imaging system\nis modelled as a low-pass filter, whose cut-off frequency is dictated by the\nobject distance, the focal length of thelens, and the pixel size of the image\nsensor. In particular, we propose to use a convolutional neural network (CNN)\nto learn the cutoff frequency of real-world degradation process. The learned\nnetwork is then applied to synthesize LR images from unpaired HR images. The\nsynthetic HR-LR image pairs are later used to train an SISR network. We\nevaluatethe effectiveness and generalization capability of the proposed\ndegradation model on real-world images captured by different imaging systems.\nExperimental results showcase that the SISR network trained by using our\nsynthetic data performs favorably against the network using the traditional\ndegradation model. Moreover, our results are comparable to that obtained by the\nsame network trained by using real-world LR-HR pairs, which are challenging to\nobtain in real scenes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Dong_Z/0/1/0/all/0/1\">Zhenxing Dong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cao_H/0/1/0/all/0/1\">Hong Cao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shen_W/0/1/0/all/0/1\">Wang Shen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gan_Y/0/1/0/all/0/1\">Yu Gan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ling_Y/0/1/0/all/0/1\">Yuye Ling</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhai_G/0/1/0/all/0/1\">Guangtao Zhai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Su_Y/0/1/0/all/0/1\">Yikai Su</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Imagine by Reasoning: A Reasoning-Based Implicit Semantic Data Augmentation for Long-Tailed Classification. (arXiv:2112.07928v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.07928","description":"<p>Real-world data often follows a long-tailed distribution, which makes the\nperformance of existing classification algorithms degrade heavily. A key issue\nis that samples in tail categories fail to depict their intra-class diversity.\nHumans can imagine a sample in new poses, scenes, and view angles with their\nprior knowledge even if it is the first time to see this category. Inspired by\nthis, we propose a novel reasoning-based implicit semantic data augmentation\nmethod to borrow transformation directions from other classes. Since the\ncovariance matrix of each category represents the feature transformation\ndirections, we can sample new directions from similar categories to generate\ndefinitely different instances. Specifically, the long-tailed distributed data\nis first adopted to train a backbone and a classifier. Then, a covariance\nmatrix for each category is estimated, and a knowledge graph is constructed to\nstore the relations of any two categories. Finally, tail samples are adaptively\nenhanced via propagating information from all the similar categories in the\nknowledge graph. Experimental results on CIFAR-100-LT, ImageNet-LT, and\niNaturalist 2018 have demonstrated the effectiveness of our proposed method\ncompared with the state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaohua Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yucan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Dayan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wanqian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiping Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A New Deep Hybrid Boosted and Ensemble Learning-based Brain Tumor Analysis using MRI. (arXiv:2201.05373v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2201.05373","description":"<p>Brain tumors analysis is important in timely diagnosis and effective\ntreatment to cure patients. Tumor analysis is challenging because of tumor\nmorphology like size, location, texture, and heteromorphic appearance in the\nmedical images. In this regard, a novel two-phase deep learning-based framework\nis proposed to detect and categorize brain tumors in magnetic resonance images\n(MRIs). In the first phase, a novel deep boosted features and ensemble\nclassifiers (DBF-EC) scheme is proposed to detect tumor MRI images from healthy\nindividuals effectively. The deep boosted feature space is achieved through the\ncustomized and well-performing deep convolutional neural networks (CNNs), and\nconsequently, fed into the ensemble of machine learning (ML) classifiers. While\nin the second phase, a new hybrid features fusion-based brain tumor\nclassification approach is proposed, comprised of dynamic-static feature and ML\nclassifier to categorize different tumor types. The dynamic features are\nextracted from the proposed BRAIN-RENet CNN, which carefully learns\nheteromorphic and inconsistent behavior of various tumors, while the static\nfeatures are extracted using HOG. The effectiveness of the proposed two-phase\nbrain tumor analysis framework is validated on two standard benchmark datasets;\ncollected from Kaggle and Figshare containing different types of tumor,\nincluding glioma, meningioma, pituitary, and normal images. Experimental\nresults proved that the proposed DBF-EC detection scheme outperforms and\nachieved accuracy (99.56%), precision (0.9991), recall (0.9899), F1-Score\n(0.9945), MCC (0.9892), and AUC-PR (0.9990). While the classification scheme,\nthe joint employment of the deep features fusion of proposed BRAIN-RENet and\nHOG features improves performance significantly in terms of recall (0.9913),\nprecision (0.9906), F1-Score (0.9909), and accuracy (99.20%) on diverse\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zahoor_M/0/1/0/all/0/1\">Mirza Mumtaz Zahoor</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qureshi_S/0/1/0/all/0/1\">Shahzad Ahmad Qureshi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khan_S/0/1/0/all/0/1\">Saddam Hussain Khan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khan_A/0/1/0/all/0/1\">Asifullah Khan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Eikonal Fields for Refractive Novel-View Synthesis. (arXiv:2202.00948v2 [cs.GR] UPDATED)","link":"http://arxiv.org/abs/2202.00948","description":"<p>We tackle the problem of generating novel-view images from collections of 2D\nimages showing refractive and reflective objects. Current solutions assume\nopaque or transparent light transport along straight paths following the\nemission-absorption model. Instead, we optimize for a field of 3D-varying Index\nof Refraction (IoR) and trace light through it that bends toward the spatial\ngradients of said IoR according to the laws of eikonal light transport.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bemana_M/0/1/0/all/0/1\">Mojtaba Bemana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Myszkowski_K/0/1/0/all/0/1\">Karol Myszkowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frisvad_J/0/1/0/all/0/1\">Jeppe Revall Frisvad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seidel_H/0/1/0/all/0/1\">Hans-Peter Seidel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ritschel_T/0/1/0/all/0/1\">Tobias Ritschel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly Supervised Nuclei Segmentation via Instance Learning. (arXiv:2202.01564v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2202.01564","description":"<p>Weakly supervised nuclei segmentation is a critical problem for pathological\nimage analysis and greatly benefits the community due to the significant\nreduction of labeling cost. Adopting point annotations, previous methods mostly\nrely on less expressive representations for nuclei instances and thus have\ndifficulty in handling crowded nuclei. In this paper, we propose to decouple\nweakly supervised semantic and instance segmentation in order to enable more\neffective subtask learning and to promote instance-aware representation\nlearning. To achieve this, we design a modular deep network with two branches:\na semantic proposal network and an instance encoding network, which are trained\nin a two-stage manner with an instance-sensitive loss. Empirical results show\nthat our approach achieves the state-of-the-art performance on two public\nbenchmarks of pathological images from different types of organs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Liu_W/0/1/0/all/0/1\">Weizhen Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+He_Q/0/1/0/all/0/1\">Qian He</a>, <a href=\"http://arxiv.org/find/eess/1/au:+He_X/0/1/0/all/0/1\">Xuming He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comparative study of 3D object detection frameworks based on LiDAR data and sensor fusion techniques. (arXiv:2202.02521v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.02521","description":"<p>Estimating and understanding the surroundings of the vehicle precisely forms\nthe basic and crucial step for the autonomous vehicle. The perception system\nplays a significant role in providing an accurate interpretation of a vehicle's\nenvironment in real-time. Generally, the perception system involves various\nsubsystems such as localization, obstacle (static and dynamic) detection, and\navoidance, mapping systems, and others. For perceiving the environment, these\nvehicles will be equipped with various exteroceptive (both passive and active)\nsensors in particular cameras, Radars, LiDARs, and others. These systems are\nequipped with deep learning techniques that transform the huge amount of data\nfrom the sensors into semantic information on which the object detection and\nlocalization tasks are performed. For numerous driving tasks, to provide\naccurate results, the location and depth information of a particular object is\nnecessary. 3D object detection methods, by utilizing the additional pose data\nfrom the sensors such as LiDARs, stereo cameras, provides information on the\nsize and location of the object. Based on recent research, 3D object detection\nframeworks performing object detection and localization on LiDAR data and\nsensor fusion techniques show significant improvement in their performance. In\nthis work, a comparative study of the effect of using LiDAR data for object\ndetection frameworks and the performance improvement seen by using sensor\nfusion techniques are performed. Along with discussing various state-of-the-art\nmethods in both the cases, performing experimental analysis, and providing\nfuture research directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Venugopala_S/0/1/0/all/0/1\">Sreenivasa Hikkal Venugopala</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Integrated Multiscale Domain Adaptive YOLO. (arXiv:2202.03527v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.03527","description":"<p>The area of domain adaptation has been instrumental in addressing the domain\nshift problem encountered by many applications. This problem arises due to the\ndifference between the distributions of source data used for training in\ncomparison with target data used during realistic testing scenarios. In this\npaper, we introduce a novel MultiScale Domain Adaptive YOLO (MS-DAYOLO)\nframework that employs multiple domain adaptation paths and corresponding\ndomain classifiers at different scales of the recently introduced YOLOv4 object\ndetector. Building on our baseline multiscale DAYOLO framework, we introduce\nthree novel deep learning architectures for a Domain Adaptation Network (DAN)\nthat generates domain-invariant features. In particular, we propose a\nProgressive Feature Reduction (PFR), a Unified Classifier (UC), and an\nIntegrated architecture. We train and test our proposed DAN architectures in\nconjunction with YOLOv4 using popular datasets. Our experiments show\nsignificant improvements in object detection performance when training YOLOv4\nusing the proposed MS-DAYOLO architectures and when tested on target data for\nautonomous driving applications. Moreover, MS-DAYOLO framework achieves an\norder of magnitude real-time speed improvement relative to Faster R-CNN\nsolutions while providing comparable object detection performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hnewa_M/0/1/0/all/0/1\">Mazin Hnewa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radha_H/0/1/0/all/0/1\">Hayder Radha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards the automated large-scale reconstruction of past road networks from historical maps. (arXiv:2202.04883v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.04883","description":"<p>Transportation infrastructure, such as road or railroad networks, represent a\nfundamental component of our civilization. For sustainable planning and\ninformed decision making, a thorough understanding of the long-term evolution\nof transportation infrastructure such as road networks is crucial. However,\nspatially explicit, multi-temporal road network data covering large spatial\nextents are scarce and rarely available prior to the 2000s. Herein, we propose\na framework that employs increasingly available scanned and georeferenced\nhistorical map series to reconstruct past road networks, by integrating\nabundant, contemporary road network data and color information extracted from\nhistorical maps. Specifically, our method uses contemporary road segments as\nanalytical units and extracts historical roads by inferring their existence in\nhistorical map series based on image processing and clustering techniques. We\ntested our method on over 300,000 road segments representing more than 50,000\nkm of the road network in the United States, extending across three study areas\nthat cover 53 historical topographic map sheets dated between 1890 and 1950. We\nevaluated our approach by comparison to other historical datasets and against\nmanually created reference data, achieving F-1 scores of up to 0.95, and showed\nthat the extracted road network statistics are highly plausible over time,\ni.e., following general growth patterns. We demonstrated that contemporary\ngeospatial data integrated with information extracted from historical map\nseries open up new avenues for the quantitative analysis of long-term\nurbanization processes and landscape changes far beyond the era of operational\nremote sensing and digital cartography.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Uhl_J/0/1/0/all/0/1\">Johannes H. Uhl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leyk_S/0/1/0/all/0/1\">Stefan Leyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiang_Y/0/1/0/all/0/1\">Yao-Yi Chiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knoblock_C/0/1/0/all/0/1\">Craig A. Knoblock</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spherical Transformer. (arXiv:2202.04942v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.04942","description":"<p>Using convolutional neural networks for 360images can induce sub-optimal\nperformance due to distortions entailed by a planar projection. The distortion\ngets deteriorated when a rotation is applied to the 360image. Thus, many\nresearches based on convolutions attempt to reduce the distortions to learn\naccurate representation. In contrast, we leverage the transformer architecture\nto solve image classification problems for 360images. Using the proposed\ntransformer for 360images has two advantages. First, our method does not\nrequire the erroneous planar projection process by sampling pixels from the\nsphere surface. Second, our sampling method based on regular polyhedrons makes\nlow rotation equivariance errors, because specific rotations can be reduced to\npermutations of faces. In experiments, we validate our network on two aspects,\nas follows. First, we show that using a transformer with highly uniform\nsampling methods can help reduce the distortion. Second, we demonstrate that\nthe transformer architecture can achieve rotation equivariance on specific\nrotations. We compare our method to other state-of-the-art algorithms using the\nSPH-MNIST, SPH-CIFAR, and SUN360 datasets and show that our method is\ncompetitive with other methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1\">Sungmin Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_R/0/1/0/all/0/1\">Raehyuk Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwon_J/0/1/0/all/0/1\">Junseok Kwon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Feature-level augmentation to improve robustness of deep neural networks to affine transformations. (arXiv:2202.05152v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.05152","description":"<p>Recent studies revealed that convolutional neural networks do not generalize\nwell to small image transformations, e.g. rotations by a few degrees or\ntranslations of a few pixels. To improve the robustness to such\ntransformations, we propose to introduce data augmentation at intermediate\nlayers of the neural architecture, in addition to the common data augmentation\napplied on the input images. By introducing small perturbations to activation\nmaps (features) at various levels, we develop the capacity of the neural\nnetwork to cope with such transformations. We conduct experiments on three\nimage classification benchmarks (Tiny ImageNet, Caltech-256 and Food-101),\nconsidering two different convolutional architectures (ResNet-18 and\nDenseNet-121). When compared with two state-of-the-art stabilization methods,\nthe empirical results show that our approach consistently attains the best\ntrade-off between accuracy and mean flip rate.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sandru_A/0/1/0/all/0/1\">Adrian Sandru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Georgescu_M/0/1/0/all/0/1\">Mariana-Iuliana Georgescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ionescu_R/0/1/0/all/0/1\">Radu Tudor Ionescu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Servoing for Pose Control of Soft Continuum Arm in a Structured Environment. (arXiv:2202.05200v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2202.05200","description":"<p>For soft continuum arms, visual servoing is a popular control strategy that\nrelies on visual feedback to close the control loop. However, robust visual\nservoing is challenging as it requires reliable feature extraction from the\nimage, accurate control models and sensors to perceive the shape of the arm,\nboth of which can be hard to implement in a soft robot. This letter circumvents\nthese challenges by presenting a deep neural network-based method to perform\nsmooth and robust 3D positioning tasks on a soft arm by visual servoing using a\ncamera mounted at the distal end of the arm. A convolutional neural network is\ntrained to predict the actuations required to achieve the desired pose in a\nstructured environment. Integrated and modular approaches for estimating the\nactuations from the image are proposed and are experimentally compared. A\nproportional control law is implemented to reduce the error between the desired\nand current image as seen by the camera. The model together with the\nproportional feedback control makes the described approach robust to several\nvariations such as new targets, lighting, loads, and diminution of the soft\narm. Furthermore, the model lends itself to be transferred to a new environment\nwith minimal effort.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kamtikar_S/0/1/0/all/0/1\">Shivani Kamtikar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marri_S/0/1/0/all/0/1\">Samhita Marri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walt_B/0/1/0/all/0/1\">Benjamin Walt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uppalapati_N/0/1/0/all/0/1\">Naveen Kumar Uppalapati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnan_G/0/1/0/all/0/1\">Girish Krishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhary_G/0/1/0/all/0/1\">Girish Chowdhary</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-02-13T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/"}}]}]}