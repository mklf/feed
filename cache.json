{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.3","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-09-08T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"SS-BERT: Mitigating Identity Terms Bias in Toxic Comment Classification by Utilising the Notion of \"Subjectivity\" and \"Identity Terms\". (arXiv:2109.02691v1 [cs.CL])","link":"http://arxiv.org/abs/2109.02691","description":"<p>Toxic comment classification models are often found biased toward identity\nterms which are terms characterizing a specific group of people such as\n\"Muslim\" and \"black\". Such bias is commonly reflected in false-positive\npredictions, i.e. non-toxic comments with identity terms. In this work, we\npropose a novel approach to tackle such bias in toxic comment classification,\nleveraging the notion of subjectivity level of a comment and the presence of\nidentity terms. We hypothesize that when a comment is made about a group of\npeople that is characterized by an identity term, the likelihood of that\ncomment being toxic is associated with the subjectivity level of the comment,\ni.e. the extent to which the comment conveys personal feelings and opinions.\nBuilding upon the BERT model, we propose a new structure that is able to\nleverage these features, and thoroughly evaluate our model on 4 datasets of\nvarying sizes and representing different social media platforms. The results\nshow that our model can consistently outperform BERT and a SOTA model devised\nto address identity term bias in a different way, with a maximum improvement in\nF1 of 2.43% and 1.91% respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhixue Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Ziqi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hopfgartner_F/0/1/0/all/0/1\">Frank Hopfgartner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text-to-Table: A New Way of Information Extraction. (arXiv:2109.02707v1 [cs.CL])","link":"http://arxiv.org/abs/2109.02707","description":"<p>We study a new problem setting of information extraction (IE), referred to as\ntext-to-table, which can be viewed as an inverse problem of the well-studied\ntable-to-text. In text-to-table, given a text, one creates a table or several\ntables expressing the main content of the text, while the model is learned from\ntext-table pair data. The problem setting differs from those of the existing\nmethods for IE. First, the extraction can be carried out from long texts to\nlarge tables with complex structures. Second, the extraction is entirely\ndata-driven, and there is no need to explicitly define the schemas. As far as\nwe know, there has been no previous work that studies the problem. In this\nwork, we formalize text-to-table as a sequence-to-sequence (seq2seq) problem.\nWe first employ a seq2seq model fine-tuned from a pre-trained language model to\nperform the task. We also develop a new method within the seq2seq approach,\nexploiting two additional techniques in table generation: table constraint and\ntable relation embeddings. We make use of four existing table-to-text datasets\nin our experiments on text-to-table. Experimental results show that the vanilla\nseq2seq model can outperform the baseline methods of using relation extraction\nand named entity extraction. The results also show that our method can further\nboost the performances of the vanilla seq2seq model. We further discuss the\nmain challenges of the proposed task. The code and data will be made publicly\navailable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xueqing Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiacheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Inspiring Content on Social Media. (arXiv:2109.02734v1 [cs.CL])","link":"http://arxiv.org/abs/2109.02734","description":"<p>Inspiration moves a person to see new possibilities and transforms the way\nthey perceive their own potential. Inspiration has received little attention in\npsychology, and has not been researched before in the NLP community. To the\nbest of our knowledge, this work is the first to study inspiration through\nmachine learning methods. We aim to automatically detect inspiring content from\nsocial media data. To this end, we analyze social media posts to tease out what\nmakes a post inspiring and what topics are inspiring. We release a dataset of\n5,800 inspiring and 5,800 non-inspiring English-language public post unique ids\ncollected from a dump of Reddit public posts made available by a third party\nand use linguistic heuristics to automatically detect which social media\nEnglish-language posts are inspiring.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ignat_O/0/1/0/all/0/1\">Oana Ignat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boureau_Y/0/1/0/all/0/1\">Y-Lan Boureau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jane A. Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Halevy_A/0/1/0/all/0/1\">Alon Halevy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Does BERT Learn as Humans Perceive? Understanding Linguistic Styles through Lexica. (arXiv:2109.02738v1 [cs.CL])","link":"http://arxiv.org/abs/2109.02738","description":"<p>People convey their intention and attitude through linguistic styles of the\ntext that they write. In this study, we investigate lexicon usages across\nstyles throughout two lenses: human perception and machine word importance,\nsince words differ in the strength of the stylistic cues that they provide. To\ncollect labels of human perception, we curate a new dataset, Hummingbird, on\ntop of benchmarking style datasets. We have crowd workers highlight the\nrepresentative words in the text that makes them think the text has the\nfollowing styles: politeness, sentiment, offensiveness, and five emotion types.\nWe then compare these human word labels with word importance derived from a\npopular fine-tuned style classifier like BERT. Our results show that the BERT\noften finds content words not relevant to the target style as important words\nused in style prediction, but humans do not perceive the same way even though\nfor some styles (e.g., positive sentiment and joy) human- and\nmachine-identified words share significant overlap for some styles.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hayati_S/0/1/0/all/0/1\">Shirley Anugrah Hayati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_D/0/1/0/all/0/1\">Dongyeop Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ungar_L/0/1/0/all/0/1\">Lyle Ungar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WhyAct: Identifying Action Reasons in Lifestyle Vlogs. (arXiv:2109.02747v1 [cs.CV])","link":"http://arxiv.org/abs/2109.02747","description":"<p>We aim to automatically identify human action reasons in online videos. We\nfocus on the widespread genre of lifestyle vlogs, in which people perform\nactions while verbally describing them. We introduce and make publicly\navailable the {\\sc WhyAct} dataset, consisting of 1,077 visual actions manually\nannotated with their reasons. We describe a multimodal model that leverages\nvisual and textual information to automatically infer the reasons corresponding\nto an action presented in the video.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ignat_O/0/1/0/all/0/1\">Oana Ignat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castro_S/0/1/0/all/0/1\">Santiago Castro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_H/0/1/0/all/0/1\">Hanwen Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Weiji Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1\">Rada Mihalcea</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-end Neural Information Status Classification. (arXiv:2109.02753v1 [cs.CL])","link":"http://arxiv.org/abs/2109.02753","description":"<p>Most previous studies on information status (IS) classification and bridging\nanaphora recognition assume that the gold mention or syntactic tree information\nis given (Hou et al., 2013; Roesiger et al., 2018; Hou, 2020; Yu and Poesio,\n2020). In this paper, we propose an end-to-end neural approach for information\nstatus classification. Our approach consists of a mention extraction component\nand an information status assignment component. During the inference time, our\nsystem takes a raw text as the input and generates mentions together with their\ninformation status. On the ISNotes corpus (Markert et al., 2012), we show that\nour information status assignment component achieves new state-of-the-art\nresults on fine-grained IS classification based on gold mentions. Furthermore,\nour system performs significantly better than other baselines for both mention\nextraction and fine-grained IS classification in the end-to-end setting.\nFinally, we apply our system on BASHI (Roesiger, 2018) and SciCorp (Roesiger,\n2016) to recognize referential bridging anaphora. We find that our end-to-end\nsystem trained on ISNotes achieves competitive results on bridging anaphora\nrecognition compared to the previous state-of-the-art system that relies on\nsyntactic information and is trained on the in-domain datasets (Yu and Poesio,\n2020).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yufang Hou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mixed Attention Transformer for LeveragingWord-Level Knowledge to Neural Cross-Lingual Information Retrieval. (arXiv:2109.02789v1 [cs.IR])","link":"http://arxiv.org/abs/2109.02789","description":"<p>Pretrained contextualized representations offer great success for many\ndownstream tasks, including document ranking. The multilingual versions of such\npretrained representations provide a possibility of jointly learning many\nlanguages with the same model. Although it is expected to gain big with such\njoint training, in the case of cross lingual information retrieval (CLIR), the\nmodels under a multilingual setting are not achieving the same level of\nperformance as those under a monolingual setting. We hypothesize that the\nperformance drop is due to the translation gap between query and documents. In\nthe monolingual retrieval task, because of the same lexical inputs, it is\neasier for model to identify the query terms that occurred in documents.\nHowever, in the multilingual pretrained models that the words in different\nlanguages are projected into the same hyperspace, the model tends to translate\nquery terms into related terms, i.e., terms that appear in a similar context,\nin addition to or sometimes rather than synonyms in the target language. This\nproperty is creating difficulties for the model to connect terms that cooccur\nin both query and document. To address this issue, we propose a novel Mixed\nAttention Transformer (MAT) that incorporates external word level knowledge,\nsuch as a dictionary or translation table. We design a sandwich like\narchitecture to embed MAT into the recent transformer based deep neural models.\nBy encoding the translation knowledge into an attention matrix, the model with\nMAT is able to focus on the mutually translated words in the input sequence.\nExperimental results demonstrate the effectiveness of the external knowledge\nand the significant improvement of MAT embedded neural reranking model on CLIR\ntask.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhiqi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bonab_H/0/1/0/all/0/1\">Hamed Bonab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarwar_S/0/1/0/all/0/1\">Sheikh Muhammad Sarwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahimi_R/0/1/0/all/0/1\">Razieh Rahimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allan_J/0/1/0/all/0/1\">James Allan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Puzzle Solving without Search or Human Knowledge: An Unnatural Language Approach. (arXiv:2109.02797v1 [cs.LG])","link":"http://arxiv.org/abs/2109.02797","description":"<p>The application of Generative Pre-trained Transformer (GPT-2) to learn\ntext-archived game notation provides a model environment for exploring sparse\nreward gameplay. The transformer architecture proves amenable to training on\nsolved text archives describing mazes, Rubik's Cube, and Sudoku solvers. The\nmethod benefits from fine-tuning the transformer architecture to visualize\nplausible strategies derived outside any guidance from human heuristics or\ndomain expertise. The large search space ($&gt;10^{19}$) for the games provides a\npuzzle environment in which the solution has few intermediate rewards and a\nfinal move that solves the challenge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Noever_D/0/1/0/all/0/1\">David Noever</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burdick_R/0/1/0/all/0/1\">Ryerson Burdick</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Scalable AI Approach for Clinical Trial Cohort Optimization. (arXiv:2109.02808v1 [cs.CL])","link":"http://arxiv.org/abs/2109.02808","description":"<p>FDA has been promoting enrollment practices that could enhance the diversity\nof clinical trial populations, through broadening eligibility criteria.\nHowever, how to broaden eligibility remains a significant challenge. We propose\nan AI approach to Cohort Optimization (AICO) through transformer-based natural\nlanguage processing of the eligibility criteria and evaluation of the criteria\nusing real-world data. The method can extract common eligibility criteria\nvariables from a large set of relevant trials and measure the generalizability\nof trial designs to real-world patients. It overcomes the scalability limits of\nexisting manual methods and enables rapid simulation of eligibility criteria\ndesign for a disease of interest. A case study on breast cancer trial design\ndemonstrates the utility of the method in improving trial generalizability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1\">Cheng Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deore_U/0/1/0/all/0/1\">Uday Deore</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yingbo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_M/0/1/0/all/0/1\">Myah Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khalil_I/0/1/0/all/0/1\">Iya Khalil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Devarakonda_M/0/1/0/all/0/1\">Murthy Devarakonda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Strategies for Generalizable Commonsense Reasoning with Pre-trained Models. (arXiv:2109.02837v1 [cs.CL])","link":"http://arxiv.org/abs/2109.02837","description":"<p>Commonsense reasoning benchmarks have been largely solved by fine-tuning\nlanguage models. The downside is that fine-tuning may cause models to overfit\nto task-specific data and thereby forget their knowledge gained during\npre-training. Recent works only propose lightweight model updates as models may\nalready possess useful knowledge from past experience, but a challenge remains\nin understanding what parts and to what extent models should be refined for a\ngiven task. In this paper, we investigate what models learn from commonsense\nreasoning datasets. We measure the impact of three different adaptation methods\non the generalization and accuracy of models. Our experiments with two models\nshow that fine-tuning performs best, by learning both the content and the\nstructure of the task, but suffers from overfitting and limited generalization\nto novel answers. We observe that alternative adaptation methods like\nprefix-tuning have comparable accuracy, but generalize better to unseen answers\nand are more robust to adversarial splits.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Kaixin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilievski_F/0/1/0/all/0/1\">Filip Ilievski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Francis_J/0/1/0/all/0/1\">Jonathan Francis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozaki_S/0/1/0/all/0/1\">Satoru Ozaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nyberg_E/0/1/0/all/0/1\">Eric Nyberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oltramari_A/0/1/0/all/0/1\">Alessandro Oltramari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Datasets: A Community Library for Natural Language Processing. (arXiv:2109.02846v1 [cs.CL])","link":"http://arxiv.org/abs/2109.02846","description":"<p>The scale, variety, and quantity of publicly-available NLP datasets has grown\nrapidly as researchers propose new tasks, larger models, and novel benchmarks.\nDatasets is a community library for contemporary NLP designed to support this\necosystem. Datasets aims to standardize end-user interfaces, versioning, and\ndocumentation, while providing a lightweight front-end that behaves similarly\nfor small datasets as for internet-scale corpora. The design of the library\nincorporates a distributed, community-driven approach to adding datasets and\ndocumenting usage. After a year of development, the library now includes more\nthan 650 unique datasets, has more than 250 contributors, and has helped\nsupport a variety of novel cross-dataset research projects and shared tasks.\nThe library is available at https://github.com/huggingface/datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lhoest_Q/0/1/0/all/0/1\">Quentin Lhoest</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moral_A/0/1/0/all/0/1\">Albert Villanova del Moral</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jernite_Y/0/1/0/all/0/1\">Yacine Jernite</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thakur_A/0/1/0/all/0/1\">Abhishek Thakur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Platen_P/0/1/0/all/0/1\">Patrick von Platen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patil_S/0/1/0/all/0/1\">Suraj Patil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaumond_J/0/1/0/all/0/1\">Julien Chaumond</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drame_M/0/1/0/all/0/1\">Mariama Drame</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plu_J/0/1/0/all/0/1\">Julien Plu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tunstall_L/0/1/0/all/0/1\">Lewis Tunstall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davison_J/0/1/0/all/0/1\">Joe Davison</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sasko_M/0/1/0/all/0/1\">Mario &#x160;a&#x161;ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chhablani_G/0/1/0/all/0/1\">Gunjan Chhablani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malik_B/0/1/0/all/0/1\">Bhavitvya Malik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brandeis_S/0/1/0/all/0/1\">Simon Brandeis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scao_T/0/1/0/all/0/1\">Teven Le Scao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanh_V/0/1/0/all/0/1\">Victor Sanh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Canwen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patry_N/0/1/0/all/0/1\">Nicolas Patry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McMillan_Major_A/0/1/0/all/0/1\">Angelina McMillan-Major</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_P/0/1/0/all/0/1\">Philipp Schmid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gugger_S/0/1/0/all/0/1\">Sylvain Gugger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Delangue_C/0/1/0/all/0/1\">Cl&#xe9;ment Delangue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matussiere_T/0/1/0/all/0/1\">Th&#xe9;o Matussi&#xe8;re</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Debut_L/0/1/0/all/0/1\">Lysandre Debut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bekman_S/0/1/0/all/0/1\">Stas Bekman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cistac_P/0/1/0/all/0/1\">Pierric Cistac</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goehringer_T/0/1/0/all/0/1\">Thibault Goehringer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mustar_V/0/1/0/all/0/1\">Victor Mustar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lagunas_F/0/1/0/all/0/1\">Fran&#xe7;ois Lagunas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rush_A/0/1/0/all/0/1\">Alexander M. Rush</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_T/0/1/0/all/0/1\">Thomas Wolf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Integrating Regular Expressions with Neural Networks via DFA. (arXiv:2109.02882v1 [cs.CL])","link":"http://arxiv.org/abs/2109.02882","description":"<p>Human-designed rules are widely used to build industry applications. However,\nit is infeasible to maintain thousands of such hand-crafted rules. So it is\nvery important to integrate the rule knowledge into neural networks to build a\nhybrid model that achieves better performance. Specifically, the human-designed\nrules are formulated as Regular Expressions (REs), from which the equivalent\nMinimal Deterministic Finite Automatons (MDFAs) are constructed. We propose to\nuse the MDFA as an intermediate model to capture the matched RE patterns as\nrule-based features for each input sentence and introduce these additional\nfeatures into neural networks. We evaluate the proposed method on the ATIS\nintent classification task. The experiment results show that the proposed\nmethod achieves the best performance compared to neural networks and four other\nmethods that combine REs and neural networks when the training dataset is\nrelatively small.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shaobo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1\">Yichun Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Chengjie Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bingquan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Z/0/1/0/all/0/1\">Zhenzhou Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_L/0/1/0/all/0/1\">Lifeng Shang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IndicBART: A Pre-trained Model for Natural Language Generation of Indic Languages. (arXiv:2109.02903v1 [cs.CL])","link":"http://arxiv.org/abs/2109.02903","description":"<p>In this paper we present IndicBART, a multilingual, sequence-to-sequence\npre-trained model focusing on 11 Indic languages and English. Different from\nexisting pre-trained models, IndicBART utilizes the orthographic similarity\nbetween Indic scripts to improve transfer learning between similar Indic\nlanguages. We evaluate IndicBART on two NLG tasks: Neural Machine Translation\n(NMT) and extreme summarization. Our experiments on NMT for 12 language pairs\nand extreme summarization for 7 languages using multilingual fine-tuning show\nthat IndicBART is competitive with or better than mBART50 despite containing\nsignificantly fewer parameters. Our analyses focus on identifying the impact of\nscript unification (to Devanagari), corpora size as well as multilingualism on\nthe final performance. The IndicBART model is available under the MIT license\nat https://indicnlp.ai4bharat.org/indic-bart .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dabre_R/0/1/0/all/0/1\">Raj Dabre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrotriya_H/0/1/0/all/0/1\">Himani Shrotriya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kunchukuttan_A/0/1/0/all/0/1\">Anoop Kunchukuttan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Puduppully_R/0/1/0/all/0/1\">Ratish Puduppully</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khapra_M/0/1/0/all/0/1\">Mitesh M. Khapra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_P/0/1/0/all/0/1\">Pratyush Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploiting Reasoning Chains for Multi-hop Science Question Answering. (arXiv:2109.02905v1 [cs.CL])","link":"http://arxiv.org/abs/2109.02905","description":"<p>We propose a novel Chain Guided Retriever-reader ({\\tt CGR}) framework to\nmodel the reasoning chain for multi-hop Science Question Answering. Our\nframework is capable of performing explainable reasoning without the need of\nany corpus-specific annotations, such as the ground-truth reasoning chain, or\nhuman-annotated entity mentions. Specifically, we first generate reasoning\nchains from a semantic graph constructed by Abstract Meaning Representation of\nretrieved evidence facts. A \\textit{Chain-aware loss}, concerning both local\nand global chain information, is also designed to enable the generated chains\nto serve as distant supervision signals for training the retriever, where\nreinforcement learning is also adopted to maximize the utility of the reasoning\nchains. Our framework allows the retriever to capture step-by-step clues of the\nentire reasoning process, which is not only shown to be effective on two\nchallenging multi-hop Science QA tasks, namely OpenBookQA and ARC-Challenge,\nbut also favors explainability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Weiwen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yang Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Huihui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1\">Deng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_W/0/1/0/all/0/1\">Wai Lam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data Driven Content Creation using Statistical and Natural Language Processing Techniques for Financial Domain. (arXiv:2109.02935v1 [cs.CL])","link":"http://arxiv.org/abs/2109.02935","description":"<p>Over the years customers' expectation of getting information instantaneously\nhas given rise to the increased usage of channels like virtual assistants.\nTypically, customers try to get their questions answered by low-touch channels\nlike search and virtual assistant first, before getting in touch with a live\nchat agent or the phone representative. Higher usage of these low-touch systems\nis a win-win for both customers and the organization since it enables\norganizations to attain a low cost of service while customers get served\nwithout delay. In this paper, we propose a two-part framework where the first\npart describes methods to combine the information from different interaction\nchannels like call, search, and chat. We do this by summarizing (using a\nstacked Bi-LSTM network) the high-touch interaction channel data such as call\nand chat into short searchquery like customer intents and then creating an\norganically grown intent taxonomy from interaction data (using Hierarchical\nAgglomerative Clustering). The second part of the framework focuses on\nextracting customer questions by analyzing interaction data sources. It\ncalculates similarity scores using TF-IDF and BERT(Devlin et al., 2019). It\nalso maps these identified questions to the output of the first part of the\nframework using syntactic and semantic similarity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chopra_A/0/1/0/all/0/1\">Ankush Chopra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagwanshi_P/0/1/0/all/0/1\">Prateek Nagwanshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Sohom Ghosh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Naturalness Evaluation of Natural Language Generation in Task-oriented Dialogues using BERT. (arXiv:2109.02938v1 [cs.CL])","link":"http://arxiv.org/abs/2109.02938","description":"<p>This paper presents an automatic method to evaluate the naturalness of\nnatural language generation in dialogue systems. While this task was previously\nrendered through expensive and time-consuming human labor, we present this\nnovel task of automatic naturalness evaluation of generated language. By\nfine-tuning the BERT model, our proposed naturalness evaluation method shows\nrobust results and outperforms the baselines: support vector machines,\nbi-directional LSTMs, and BLEURT. In addition, the training speed and\nevaluation performance of naturalness model are improved by transfer learning\nfrom quality and informativeness linguistic knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Ye Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maier_W/0/1/0/all/0/1\">Wolfgang Maier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minker_W/0/1/0/all/0/1\">Wolfgang Minker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ultes_S/0/1/0/all/0/1\">Stefan Ultes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Countering Online Hate Speech: An NLP Perspective. (arXiv:2109.02941v1 [cs.CL])","link":"http://arxiv.org/abs/2109.02941","description":"<p>Online hate speech has caught everyone's attention from the news related to\nthe COVID-19 pandemic, US elections, and worldwide protests. Online toxicity -\nan umbrella term for online hateful behavior, manifests itself in forms such as\nonline hate speech. Hate speech is a deliberate attack directed towards an\nindividual or a group motivated by the targeted entity's identity or opinions.\nThe rising mass communication through social media further exacerbates the\nharmful consequences of online hate speech. While there has been significant\nresearch on hate-speech identification using Natural Language Processing (NLP),\nthe work on utilizing NLP for prevention and intervention of online hate speech\nlacks relatively. This paper presents a holistic conceptual framework on\nhate-speech NLP countering methods along with a thorough survey on the current\nprogress of NLP for countering online hate speech. It classifies the countering\ntechniques based on their time of action, and identifies potential future\nresearch areas on this topic.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chaudhary_M/0/1/0/all/0/1\">Mudit Chaudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saxena_C/0/1/0/all/0/1\">Chandni Saxena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_H/0/1/0/all/0/1\">Helen Meng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Paraphrase Generation as Unsupervised Machine Translation. (arXiv:2109.02950v1 [cs.CL])","link":"http://arxiv.org/abs/2109.02950","description":"<p>In this paper, we propose a new paradigm for paraphrase generation by\ntreating the task as unsupervised machine translation (UMT) based on the\nassumption that there must be pairs of sentences expressing the same meaning in\na large-scale unlabeled monolingual corpus. The proposed paradigm first splits\na large unlabeled corpus into multiple clusters, and trains multiple UMT models\nusing pairs of these clusters. Then based on the paraphrase pairs produced by\nthese UMT models, a unified surrogate model can be trained to serve as the\nfinal Seq2Seq model to generate paraphrases, which can be directly used for\ntest in the unsupervised setup, or be finetuned on labeled datasets in the\nsupervised setup. The proposed method offers merits over\nmachine-translation-based paraphrase generation methods, as it avoids reliance\non bilingual sentence pairs. It also allows human intervene with the model so\nthat more diverse paraphrases can be generated using different filtering\ncriteria. Extensive experiments on existing paraphrase dataset for both the\nsupervised and unsupervised setups demonstrate the effectiveness the proposed\nparadigm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1\">Chun Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yufei Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1\">Yuxian Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiaofei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiwei Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FH-SWF SG at GermEval 2021: Using Transformer-Based Language Models to Identify Toxic, Engaging, & Fact-Claiming Comments. (arXiv:2109.02966v1 [cs.CL])","link":"http://arxiv.org/abs/2109.02966","description":"<p>In this paper we describe the methods we used for our submissions to the\nGermEval 2021 shared task on the identification of toxic, engaging, and\nfact-claiming comments. For all three subtasks we fine-tuned freely available\ntransformer-based models from the Huggingface model hub. We evaluated the\nperformance of various pre-trained models after fine-tuning on 80% of the\ntraining data with different hyperparameters and submitted predictions of the\ntwo best performing resulting models. We found that this approach worked best\nfor subtask 3, for which we achieved an F1-score of 0.736.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gawron_C/0/1/0/all/0/1\">Christian Gawron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_S/0/1/0/all/0/1\">Sebastian Schmidt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Don't Go Far Off: An Empirical Study on Neural Poetry Translation. (arXiv:2109.02972v1 [cs.CL])","link":"http://arxiv.org/abs/2109.02972","description":"<p>Despite constant improvements in machine translation quality, automatic\npoetry translation remains a challenging problem due to the lack of\nopen-sourced parallel poetic corpora, and to the intrinsic complexities\ninvolved in preserving the semantics, style, and figurative nature of poetry.\nWe present an empirical investigation for poetry translation along several\ndimensions: 1) size and style of training data (poetic vs. non-poetic),\nincluding a zero-shot setup; 2) bilingual vs. multilingual learning; and 3)\nlanguage-family-specific models vs. mixed-multilingual models. To accomplish\nthis, we contribute a parallel dataset of poetry translations for several\nlanguage pairs. Our results show that multilingual fine-tuning on poetic text\nsignificantly outperforms multilingual fine-tuning on non-poetic text that is\n35X larger in size, both in terms of automatic metrics (BLEU, BERTScore) and\nhuman evaluation metrics such as faithfulness (meaning and poetic style).\nMoreover, multilingual fine-tuning on poetic data outperforms \\emph{bilingual}\nfine-tuning on poetic data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chakrabarty_T/0/1/0/all/0/1\">Tuhin Chakrabarty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saakyan_A/0/1/0/all/0/1\">Arkadiy Saakyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muresan_S/0/1/0/all/0/1\">Smaranda Muresan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting Context Choices for Context-aware Machine Translation. (arXiv:2109.02995v1 [cs.CL])","link":"http://arxiv.org/abs/2109.02995","description":"<p>One of the most popular methods for context-aware machine translation (MT) is\nto use separate encoders for the source sentence and context as multiple\nsources for one target sentence. Recent work has cast doubt on whether these\nmodels actually learn useful signals from the context or are improvements in\nautomatic evaluation metrics just a side-effect. We show that multi-source\ntransformer models improve MT over standard transformer-base models even with\nempty lines provided as context, but the translation quality improves\nsignificantly (1.51 - 2.65 BLEU) when a sufficient amount of correct context is\nprovided. We also show that even though randomly shuffling in-domain context\ncan also improve over baselines, the correct context further improves\ntranslation quality and random out-of-domain context further degrades it.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rikters_M/0/1/0/all/0/1\">Mat&#x12b;ss Rikters</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakazawa_T/0/1/0/all/0/1\">Toshiaki Nakazawa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Empathetic Dialogue Generation with Pre-trained RoBERTa-GPT2 and External Knowledge. (arXiv:2109.03004v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03004","description":"<p>One challenge for dialogue agents is to recognize feelings of the\nconversation partner and respond accordingly. In this work, RoBERTa-GPT2 is\nproposed for empathetic dialogue generation, where the pre-trained\nauto-encoding RoBERTa is utilised as encoder and the pre-trained\nauto-regressive GPT-2 as decoder. With the combination of the pre-trained\nRoBERTa and GPT-2, our model realizes a new state-of-the-art emotion accuracy.\nTo enable the empathetic ability of RoBERTa-GPT2 model, we propose a\ncommonsense knowledge and emotional concepts extractor, in which the\ncommonsensible and emotional concepts of dialogue context are extracted for the\nGPT-2 decoder. The experiment results demonstrate that the empathetic dialogue\ngeneration benefits from both pre-trained encoder-decoder architecture and\nexternal knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Ye Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maier_W/0/1/0/all/0/1\">Wolfgang Maier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minker_W/0/1/0/all/0/1\">Wolfgang Minker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ultes_S/0/1/0/all/0/1\">Stefan Ultes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sequential Attention Module for Natural Language Processing. (arXiv:2109.03009v1 [cs.AI])","link":"http://arxiv.org/abs/2109.03009","description":"<p>Recently, large pre-trained neural language models have attained remarkable\nperformance on many downstream natural language processing (NLP) applications\nvia fine-tuning. In this paper, we target at how to further improve the token\nrepresentations on the language models. We, therefore, propose a simple yet\neffective plug-and-play module, Sequential Attention Module (SAM), on the token\nembeddings learned from a pre-trained language model. Our proposed SAM consists\nof two main attention modules deployed sequentially: Feature-wise Attention\nModule (FAM) and Token-wise Attention Module (TAM). More specifically, FAM can\neffectively identify the importance of features at each dimension and promote\nthe effect via dot-product on the original token embeddings for downstream NLP\napplications. Meanwhile, TAM can further re-weight the features at the\ntoken-wise level. Moreover, we propose an adaptive filter on FAM to prevent\nnoise impact and increase information absorption. Finally, we conduct extensive\nexperiments to demonstrate the advantages and properties of our proposed SAM.\nWe first show how SAM plays a primary role in the champion solution of two\nsubtasks of SemEval'21 Task 7. After that, we apply SAM on sentiment analysis\nand three popular NLP tasks and demonstrate that SAM consistently outperforms\nthe state-of-the-art baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Mengyuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jian Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haiqin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Lianxin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mo_Y/0/1/0/all/0/1\">Yang Mo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generate & Rank: A Multi-task Framework for Math Word Problems. (arXiv:2109.03034v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03034","description":"<p>Math word problem (MWP) is a challenging and critical task in natural\nlanguage processing. Many recent studies formalize MWP as a generation task and\nhave adopted sequence-to-sequence models to transform problem descriptions to\nmathematical expressions. However, mathematical expressions are prone to minor\nmistakes while the generation objective does not explicitly handle such\nmistakes. To address this limitation, we devise a new ranking task for MWP and\npropose Generate &amp; Rank, a multi-task framework based on a generative\npre-trained language model. By joint training with generation and ranking, the\nmodel learns from its own mistakes and is able to distinguish between correct\nand incorrect expressions. Meanwhile, we perform tree-based disturbance\nspecially designed for MWP and an online update to boost the ranker. We\ndemonstrate the effectiveness of our proposed method on the benchmark and the\nresults show that our method consistently outperforms baselines in all\ndatasets. Particularly, in the classical Math23k, our method is 7% (78.4%\n$\\rightarrow$ 85.4%) higher than the state-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jianhao Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1\">Yichun Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_L/0/1/0/all/0/1\">Lifeng Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Ming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"POSSCORE: A Simple Yet Effective Evaluation of Conversational Search with Part of Speech Labelling. (arXiv:2109.03039v1 [cs.IR])","link":"http://arxiv.org/abs/2109.03039","description":"<p>Conversational search systems, such as Google Assistant and Microsoft\nCortana, provide a new search paradigm where users are allowed, via natural\nlanguage dialogues, to communicate with search systems. Evaluating such systems\nis very challenging since search results are presented in the format of natural\nlanguage sentences. Given the unlimited number of possible responses,\ncollecting relevance assessments for all the possible responses is infeasible.\nIn this paper, we propose POSSCORE, a simple yet effective automatic evaluation\nmethod for conversational search. The proposed embedding-based metric takes the\ninfluence of part of speech (POS) of the terms in the response into account. To\nthe best knowledge, our work is the first to systematically demonstrate the\nimportance of incorporating syntactic information, such as POS labels, for\nconversational search evaluation. Experimental results demonstrate that our\nmetrics can correlate with human preference, achieving significant improvements\nover state-of-the-art baseline metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zeyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Ke Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_J/0/1/0/all/0/1\">Jiaxin Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilson_M/0/1/0/all/0/1\">Max L. Wilson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Patient Outcome and Zero-shot Diagnosis Prediction with Hypernetwork-guided Multitask Learning. (arXiv:2109.03062v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03062","description":"<p>Multitask deep learning has been applied to patient outcome prediction from\ntext, taking clinical notes as input and training deep neural networks with a\njoint loss function of multiple tasks. However, the joint training scheme of\nmultitask learning suffers from inter-task interference, and diagnosis\nprediction among the multiple tasks has the generalizability issue due to rare\ndiseases or unseen diagnoses. To solve these challenges, we propose a\nhypernetwork-based approach that generates task-conditioned parameters and\ncoefficients of multitask prediction heads to learn task-specific prediction\nand balance the multitask learning. We also incorporate semantic task\ninformation to improves the generalizability of our task-conditioned multitask\nmodel. Experiments on early and discharge notes extracted from the real-world\nMIMIC database show our method can achieve better performance on multitask\npatient outcome prediction than strong baselines in most cases. Besides, our\nmethod can effectively handle the scenario with limited information and improve\nzero-shot prediction on unseen diagnosis categories.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shaoxiong Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marttinen_P/0/1/0/all/0/1\">Pekka Marttinen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GOLD: Improving Out-of-Scope Detection in Dialogues using Data Augmentation. (arXiv:2109.03079v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03079","description":"<p>Practical dialogue systems require robust methods of detecting out-of-scope\n(OOS) utterances to avoid conversational breakdowns and related failure modes.\nDirectly training a model with labeled OOS examples yields reasonable\nperformance, but obtaining such data is a resource-intensive process. To tackle\nthis limited-data problem, previous methods focus on better modeling the\ndistribution of in-scope (INS) examples. We introduce GOLD as an orthogonal\ntechnique that augments existing data to train better OOS detectors operating\nin low-data regimes. GOLD generates pseudo-labeled candidates using samples\nfrom an auxiliary dataset and keeps only the most beneficial candidates for\ntraining through a novel filtering mechanism. In experiments across three\ntarget benchmarks, the top GOLD model outperforms all existing methods on all\nkey metrics, achieving relative gains of 52.4%, 48.9% and 50.3% against median\nbaseline performance. We also analyze the unique properties of OOS data to\nidentify key factors for optimally applying our proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Derek Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning grounded word meaning representations on similarity graphs. (arXiv:2109.03084v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03084","description":"<p>This paper introduces a novel approach to learn visually grounded meaning\nrepresentations of words as low-dimensional node embeddings on an underlying\ngraph hierarchy. The lower level of the hierarchy models modality-specific word\nrepresentations through dedicated but communicating graphs, while the higher\nlevel puts these representations together on a single graph to learn a\nrepresentation jointly from both modalities. The topology of each graph models\nsimilarity relations among words, and is estimated jointly with the graph\nembedding. The assumption underlying this model is that words sharing similar\nmeaning correspond to communities in an underlying similarity graph in a\nlow-dimensional space. We named this model Hierarchical Multi-Modal Similarity\nGraph Embedding (HM-SGE). Experimental results validate the ability of HM-SGE\nto simulate human similarity judgements and concept categorization,\noutperforming the state of the art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dimiccoli_M/0/1/0/all/0/1\">Mariella Dimiccoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wendt_H/0/1/0/all/0/1\">Herwig Wendt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batlle_P/0/1/0/all/0/1\">Pau Batlle</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FHAC at GermEval 2021: Identifying German toxic, engaging, and fact-claiming comments with ensemble learning. (arXiv:2109.03094v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03094","description":"<p>The availability of language representations learned by large pretrained\nneural network models (such as BERT and ELECTRA) has led to improvements in\nmany downstream Natural Language Processing tasks in recent years. Pretrained\nmodels usually differ in pretraining objectives, architectures, and datasets\nthey are trained on which can affect downstream performance. In this\ncontribution, we fine-tuned German BERT and German ELECTRA models to identify\ntoxic (subtask 1), engaging (subtask 2), and fact-claiming comments (subtask 3)\nin Facebook data provided by the GermEval 2021 competition. We created\nensembles of these models and investigated whether and how classification\nperformance depends on the number of ensemble members and their composition. On\nout-of-sample data, our best ensemble achieved a macro-F1 score of 0.73 (for\nall subtasks), and F1 scores of 0.72, 0.70, and 0.76 for subtasks 1, 2, and 3,\nrespectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bornheim_T/0/1/0/all/0/1\">Tobias Bornheim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grieger_N/0/1/0/all/0/1\">Niklas Grieger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bialonski_S/0/1/0/all/0/1\">Stephan Bialonski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Infusing Future Information into Monotonic Attention Through Language Models. (arXiv:2109.03121v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03121","description":"<p>Simultaneous neural machine translation(SNMT) models start emitting the\ntarget sequence before they have processed the source sequence. The recent\nadaptive policies for SNMT use monotonic attention to perform read/write\ndecisions based on the partial source and target sequences. The lack of\nsufficient information might cause the monotonic attention to take poor\nread/write decisions, which in turn negatively affects the performance of the\nSNMT model. On the other hand, human translators make better read/write\ndecisions since they can anticipate the immediate future words using linguistic\ninformation and domain knowledge.Motivated by human translators, in this work,\nwe propose a framework to aid monotonic attention with an external language\nmodel to improve its decisions.We conduct experiments on the MuST-C\nEnglish-German and English-French speech-to-text translation tasks to show the\neffectiveness of the proposed framework.The proposed SNMT method improves the\nquality-latency trade-off over the state-of-the-art monotonic multihead\nattention.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zaidi_M/0/1/0/all/0/1\">Mohd Abbas Zaidi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Indurthi_S/0/1/0/all/0/1\">Sathish Indurthi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1\">Beomseok Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakumarapu_N/0/1/0/all/0/1\">Nikhil Kumar Lakumarapu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sangha Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rare Words Degenerate All Words. (arXiv:2109.03127v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03127","description":"<p>Despite advances in neural network language model, the representation\ndegeneration problem of embeddings is still challenging. Recent studies have\nfound that the learned output embeddings are degenerated into a narrow-cone\ndistribution which makes the similarity between each embeddings positive. They\nanalyzed the cause of the degeneration problem has been demonstrated as common\nto most embeddings. However, we found that the degeneration problem is\nespecially originated from the training of embeddings of rare words. In this\nstudy, we analyze the intrinsic mechanism of the degeneration of rare word\nembeddings with respect of their gradient about the negative log-likelihood\nloss function. Furthermore, we theoretically and empirically demonstrate that\nthe degeneration of rare word embeddings causes the degeneration of non-rare\nword embeddings, and that the overall degeneration problem can be alleviated by\npreventing the degeneration of rare word embeddings. Based on our analyses, we\npropose a novel method, Adaptive Gradient Partial Scaling(AGPS), to address the\ndegeneration problem. Experimental results demonstrate the effectiveness of the\nproposed method qualitatively and quantitatively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Sangwon Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jongyoon Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Heeseung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seong-min Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryu_W/0/1/0/all/0/1\">Woo-Jong Ryu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Sungroh Yoon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NumGPT: Improving Numeracy Ability of Generative Pre-trained Models. (arXiv:2109.03137v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03137","description":"<p>Existing generative pre-trained language models (e.g., GPT) focus on modeling\nthe language structure and semantics of general texts. However, those models do\nnot consider the numerical properties of numbers and cannot perform robustly on\nnumerical reasoning tasks (e.g., math word problems and measurement\nestimation). In this paper, we propose NumGPT, a generative pre-trained model\nthat explicitly models the numerical properties of numbers in texts.\nSpecifically, it leverages a prototype-based numeral embedding to encode the\nmantissa of the number and an individual embedding to encode the exponent of\nthe number. A numeral-aware loss function is designed to integrate numerals\ninto the pre-training objective of NumGPT. We conduct extensive experiments on\nfour different datasets to evaluate the numeracy ability of NumGPT. The\nexperiment results show that NumGPT outperforms baseline models (e.g., GPT and\nGPT with DICE) on a range of numerical reasoning tasks such as measurement\nestimation, number comparison, math word problems, and magnitude\nclassification. Ablation studies are also conducted to evaluate the impact of\npre-training and model hyperparameters on the performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhihua Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xingbo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiaozhe Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_H/0/1/0/all/0/1\">Huamin Qu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PAUSE: Positive and Annealed Unlabeled Sentence Embedding. (arXiv:2109.03155v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03155","description":"<p>Sentence embedding refers to a set of effective and versatile techniques for\nconverting raw text into numerical vector representations that can be used in a\nwide range of natural language processing (NLP) applications. The majority of\nthese techniques are either supervised or unsupervised. Compared to the\nunsupervised methods, the supervised ones make less assumptions about\noptimization objectives and usually achieve better results. However, the\ntraining requires a large amount of labeled sentence pairs, which is not\navailable in many industrial scenarios. To that end, we propose a generic and\nend-to-end approach -- PAUSE (Positive and Annealed Unlabeled Sentence\nEmbedding), capable of learning high-quality sentence embeddings from a\npartially labeled dataset. We experimentally show that PAUSE achieves, and\nsometimes surpasses, state-of-the-art results using only a small fraction of\nlabeled sentence pairs on various benchmark tasks. When applied to a real\nindustrial use case where labeled samples are scarce, PAUSE encourages us to\nextend our dataset without the liability of extensive manual annotation work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1\">Lele Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Larsson_E/0/1/0/all/0/1\">Emil Larsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ehrenheim_V/0/1/0/all/0/1\">Vilhelm von Ehrenheim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rocha_D/0/1/0/all/0/1\">Dhiana Deva Cavalcanti Rocha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_A/0/1/0/all/0/1\">Anna Martin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horn_S/0/1/0/all/0/1\">Sonja Horn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Idiosyncratic but not Arbitrary: Learning Idiolects in Online Registers Reveals Distinctive yet Consistent Individual Styles. (arXiv:2109.03158v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03158","description":"<p>An individual's variation in writing style is often a function of both social\nand personal attributes. While structured social variation has been extensively\nstudied, e.g., gender based variation, far less is known about how to\ncharacterize individual styles due to their idiosyncratic nature. We introduce\na new approach to studying idiolects through a massive cross-author comparison\nto identify and encode stylistic features. The neural model achieves strong\nperformance at authorship identification on short texts and through an\nanalogy-based probing task, showing that the learned representations exhibit\nsurprising regularities that encode qualitative and quantitative shifts of\nidiolectal styles. Through text perturbation, we quantify the relative\ncontributions of different linguistic elements to idiolectal variation.\nFurthermore, we provide a description of idiolects through measuring inter- and\nintra-author variation, showing that variation in idiolects is often\ndistinctive yet consistent.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jian Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurgens_D/0/1/0/all/0/1\">David Jurgens</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How much pretraining data do language models need to learn syntax?. (arXiv:2109.03160v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03160","description":"<p>Transformers-based pretrained language models achieve outstanding results in\nmany well-known NLU benchmarks. However, while pretraining methods are very\nconvenient, they are expensive in terms of time and resources. This calls for a\nstudy of the impact of pretraining data size on the knowledge of the models. We\nexplore this impact on the syntactic capabilities of RoBERTa, using models\ntrained on incremental sizes of raw text data. First, we use syntactic\nstructural probes to determine whether models pretrained on more data encode a\nhigher amount of syntactic information. Second, we perform a targeted syntactic\nevaluation to analyze the impact of pretraining data size on the syntactic\ngeneralization performance of the models. Third, we compare the performance of\nthe different models on three downstream applications: part-of-speech tagging,\ndependency parsing and paraphrase identification. We complement our study with\nan analysis of the cost-benefit trade-off of training such models. Our\nexperiments show that while models pretrained on more data encode more\nsyntactic knowledge and perform better on downstream applications, they do not\nalways offer a better performance across the different syntactic phenomena and\ncome at a higher financial and environmental cost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Perez_Mayos_L/0/1/0/all/0/1\">Laura P&#xe9;rez-Mayos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ballesteros_M/0/1/0/all/0/1\">Miguel Ballesteros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wanner_L/0/1/0/all/0/1\">Leo Wanner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Aspect-Controllable Opinion Summarization. (arXiv:2109.03171v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03171","description":"<p>Recent work on opinion summarization produces general summaries based on a\nset of input reviews and the popularity of opinions expressed in them. In this\npaper, we propose an approach that allows the generation of customized\nsummaries based on aspect queries (e.g., describing the location and room of a\nhotel). Using a review corpus, we create a synthetic training dataset of\n(review, summary) pairs enriched with aspect controllers which are induced by a\nmulti-instance learning model that predicts the aspects of a document at\ndifferent levels of granularity. We fine-tune a pretrained model using our\nsynthetic dataset and generate aspect-specific summaries by modifying the\naspect controllers. Experiments on two benchmarks show that our model\noutperforms the previous state of the art and generates personalized summaries\nby controlling the number of aspects discussed in them.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Amplayo_R/0/1/0/all/0/1\">Reinald Kim Amplayo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angelidis_S/0/1/0/all/0/1\">Stefanos Angelidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lapata_M/0/1/0/all/0/1\">Mirella Lapata</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"When differential privacy meets NLP: The devil is in the detail. (arXiv:2109.03175v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03175","description":"<p>Differential privacy provides a formal approach to privacy of individuals.\nApplications of differential privacy in various scenarios, such as protecting\nusers' original utterances, must satisfy certain mathematical properties. Our\ncontribution is a formal analysis of ADePT, a differentially private\nauto-encoder for text rewriting (Krishna et al, 2021). ADePT achieves promising\nresults on downstream tasks while providing tight privacy guarantees. Our proof\nreveals that ADePT is not differentially private, thus rendering the\nexperimental results unsubstantiated. We also quantify the impact of the error\nin its private mechanism, showing that the true sensitivity is higher by at\nleast factor 6 in an optimistic case of a very small encoder's dimension and\nthat the amount of utterances that are not privatized could easily reach 100%\nof the entire dataset. Our intention is neither to criticize the authors, nor\nthe peer-reviewing process, but rather point out that if differential privacy\napplications in NLP rely on formal guarantees, these should be outlined in full\nand put under detailed scrutiny.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Habernal_I/0/1/0/all/0/1\">Ivan Habernal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Conversation Disentanglement through Co-Training. (arXiv:2109.03199v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03199","description":"<p>Conversation disentanglement aims to separate intermingled messages into\ndetached sessions, which is a fundamental task in understanding multi-party\nconversations. Existing work on conversation disentanglement relies heavily\nupon human-annotated datasets, which are expensive to obtain in practice. In\nthis work, we explore to train a conversation disentanglement model without\nreferencing any human annotations. Our method is built upon a deep co-training\nalgorithm, which consists of two neural networks: a message-pair classifier and\na session classifier. The former is responsible for retrieving local relations\nbetween two messages while the latter categorizes a message to a session by\ncapturing context-aware information. Both networks are initialized respectively\nwith pseudo data built from an unannotated corpus. During the deep co-training\nprocess, we use the session classifier as a reinforcement learning component to\nlearn a session assigning policy by maximizing the local rewards given by the\nmessage-pair classifier. For the message-pair classifier, we enrich its\ntraining data by retrieving message pairs with high confidence from the\ndisentangled sessions predicted by the session classifier. Experimental results\non the large Movie Dialogue Dataset demonstrate that our proposed approach\nachieves competitive performance compared to the previous supervised methods.\nFurther experiments show that the predicted disentangled conversations can\npromote the performance on the downstream task of multi-party response\nselection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Zhan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaodan Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ExCode-Mixed: Explainable Approaches towards Sentiment Analysis on Code-Mixed Data using BERT models. (arXiv:2109.03200v1 [cs.AI])","link":"http://arxiv.org/abs/2109.03200","description":"<p>The increasing use of social media sites in countries like India has given\nrise to large volumes of code-mixed data. Sentiment analysis of this data can\nprovide integral insights into people's perspectives and opinions. Developing\nrobust explainability techniques which explain why models make their\npredictions becomes essential. In this paper, we propose an adequate\nmethodology to integrate explainable approaches into code-mixed sentiment\nanalysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Priyanshu_A/0/1/0/all/0/1\">Aman Priyanshu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vardhan_A/0/1/0/all/0/1\">Aleti Vardhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sivakumar_S/0/1/0/all/0/1\">Sudarshan Sivakumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vijay_S/0/1/0/all/0/1\">Supriti Vijay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chhabra_N/0/1/0/all/0/1\">Nipuna Chhabra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Joint model for intent and entity recognition. (arXiv:2109.03221v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03221","description":"<p>The semantic understanding of natural dialogues composes of several parts.\nSome of them, like intent classification and entity detection, have a crucial\nrole in deciding the next steps in handling user input. Handling each task as\nan individual problem can be wasting of training resources, and also each\nproblem can benefit from each other. This paper tackles these problems as one.\nOur new model, which combine intent and entity recognition into one system, is\nachieving better metrics in both tasks with lower training requirements than\nsolving each task separately. We also optimize the model based on the inputs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lorenc_P/0/1/0/all/0/1\">Petr Lorenc</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond Preserved Accuracy: Evaluating Loyalty and Robustness of BERT Compression. (arXiv:2109.03228v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03228","description":"<p>Recent studies on compression of pretrained language models (e.g., BERT)\nusually use preserved accuracy as the metric for evaluation. In this paper, we\npropose two new metrics, label loyalty and probability loyalty that measure how\nclosely a compressed model (i.e., student) mimics the original model (i.e.,\nteacher). We also explore the effect of compression with regard to robustness\nunder adversarial attacks. We benchmark quantization, pruning, knowledge\ndistillation and progressive module replacing with loyalty and robustness. By\ncombining multiple compression techniques, we provide a practical strategy to\nachieve better accuracy, loyalty and robustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Canwen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wangchunshu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_T/0/1/0/all/0/1\">Tao Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Ke Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1\">Julian McAuley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Query-Variant Advertisement Text Generation with Association Knowledge. (arXiv:2004.06438v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2004.06438","description":"<p>Advertising is an important revenue source for many companies. However, it is\nexpensive to manually create advertisements that meet the needs of various\nqueries for massive items. In this paper, we propose the query-variant\nadvertisement text generation task that aims to generate candidate\nadvertisements for different queries with various needs given the item\nkeywords. In this task, for many different queries there is only one general\npurposed advertisement with no predefined query-advertisement pair, which would\ndiscourage traditional End-to-End models from generating query-variant\nadvertisements for different queries with different needs. To deal with the\nproblem, we propose a query-variant advertisement text generation model that\ntakes keywords and associated external knowledge as input during training and\nadds different queries during inference. Adding external knowledge helps the\nmodel adapted to the information besides the item keywords during training,\nwhich makes the transition between training and inference more smoothing when\nthe query is added during inference. Both automatic and human evaluation show\nthat our model can generate more attractive and query-focused advertisements\nthan the strong baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Duan_S/0/1/0/all/0/1\">Siyu Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jing_C/0/1/0/all/0/1\">Cai Jing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yancheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yunfang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xu Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Intuitive Contrasting Map for Antonym Embeddings. (arXiv:2004.12835v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2004.12835","description":"<p>This paper shows that, modern word embeddings contain information that\ndistinguishes synonyms and antonyms despite small cosine similarities between\ncorresponding vectors. This information is encoded in the geometry of the\nembeddings and could be extracted with a straight-forward and intuitive\nmanifold learning procedure or a contrasting map. Such a map is trained on a\nsmall labeled subset of the data and can produce new embeddings that explicitly\nhighlight specific semantic attributes of the word. The new embeddings produced\nby the map are shown to improve the performance on downstream tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Samenko_I/0/1/0/all/0/1\">Igor Samenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tikhonov_A/0/1/0/all/0/1\">Alexey Tikhonov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamshchikov_I/0/1/0/all/0/1\">Ivan P. Yamshchikov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Injecting Entity Types into Entity-Guided Text Generation. (arXiv:2009.13401v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2009.13401","description":"<p>Recent successes in deep generative modeling have led to significant advances\nin natural language generation (NLG). Incorporating entities into neural\ngeneration models has demonstrated great improvements by assisting to infer the\nsummary topic and to generate coherent content. To enhance the role of entity\nin NLG, in this paper, we aim to model the entity type in the decoding phase to\ngenerate contextual words accurately. We develop a novel NLG model to produce a\ntarget sequence based on a given list of entities. Our model has a multi-step\ndecoder that injects the entity types into the process of entity mention\ngeneration. Experiments on two public news datasets demonstrate type injection\nperforms better than existing type embedding concatenation baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xiangyu Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Wenhao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1\">Meng Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fully Synthetic Data Improves Neural Machine Translation withKnowledge Distillation. (arXiv:2012.15455v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2012.15455","description":"<p>This paper explores augmenting monolingual data for knowledge distillation in\nneural machine translation. Source language monolingual text can be\nincorporated as a forward translation. Interestingly, we find the best way to\nincorporate target language monolingual text is to translate it to the source\nlanguage and round-trip translate it back to the target language, resulting in\na fully synthetic corpus. We find that combining monolingual data from both\nsource and target languages yields better performance than a corpus twice as\nlarge only in one language. Moreover, experiments reveal that the improvement\ndepends upon the provenance of the test set. If the test set was originally in\nthe source language (with the target side written by translators), then forward\ntranslating source monolingual data matters. If the test set was originally in\nthe target language (with the source written by translators), then\nincorporating target monolingual data matters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aji_A/0/1/0/all/0/1\">Alham Fikri Aji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heafield_K/0/1/0/all/0/1\">Kenneth Heafield</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Subformer: Exploring Weight Sharing for Parameter Efficiency in Generative Transformers. (arXiv:2101.00234v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2101.00234","description":"<p>Transformers have shown improved performance when compared to previous\narchitectures for sequence processing such as RNNs. Despite their sizeable\nperformance gains, as recently suggested, the model is computationally\nexpensive to train and with a high parameter budget. In light of this, we\nexplore parameter-sharing methods in Transformers with a specific focus on\ngenerative models. We perform an analysis of different parameter\nsharing/reduction methods and develop the Subformer. Our model combines\nsandwich-style parameter sharing, which overcomes naive cross-layer parameter\nsharing in generative models, and self-attentive embedding factorization\n(SAFE). Experiments on machine translation, abstractive summarization and\nlanguage modeling show that the Subformer can outperform the Transformer even\nwhen using significantly fewer parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Reid_M/0/1/0/all/0/1\">Machel Reid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marrese_Taylor_E/0/1/0/all/0/1\">Edison Marrese-Taylor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsuo_Y/0/1/0/all/0/1\">Yutaka Matsuo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Toward Improving Coherence and Diversity of Slogan Generation. (arXiv:2102.05924v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2102.05924","description":"<p>Previous work in slogan generation focused on utilising slogan skeletons\nmined from existing slogans. While some generated slogans can be catchy, they\nare often not coherent with the company's focus or style across their marketing\ncommunications because the skeletons are mined from other companies' slogans.\nWe propose a sequence-to-sequence (seq2seq) transformer model to generate\nslogans from a brief company description. A naive seq2seq model fine-tuned for\nslogan generation is prone to introducing false information. We use company\nname delexicalisation and entity masking to alleviate this problem and improve\nthe generated slogans' quality and truthfulness. Furthermore, we apply\nconditional training based on the first words' POS tag to generate\nsyntactically diverse slogans. Our best model achieved a ROUGE-1/-2/-L F1 score\nof 35.58/18.47/33.32. Besides, automatic and human evaluations indicate that\nour method generates significantly more factual, diverse and catchy slogans\nthan strong LSTM and transformer seq2seq baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yiping Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatia_A/0/1/0/all/0/1\">Akshay Bhatia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wanvarie_D/0/1/0/all/0/1\">Dittaya Wanvarie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_P/0/1/0/all/0/1\">Phu T. V. Le</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WordBias: An Interactive Visual Tool for Discovering Intersectional Biases Encoded in Word Embeddings. (arXiv:2103.03598v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.03598","description":"<p>Intersectional bias is a bias caused by an overlap of multiple social factors\nlike gender, sexuality, race, disability, religion, etc. A recent study has\nshown that word embedding models can be laden with biases against\nintersectional groups like African American females, etc. The first step\ntowards tackling such intersectional biases is to identify them. However,\ndiscovering biases against different intersectional groups remains a\nchallenging task. In this work, we present WordBias, an interactive visual tool\ndesigned to explore biases against intersectional groups encoded in static word\nembeddings. Given a pretrained static word embedding, WordBias computes the\nassociation of each word along different groups based on race, age, etc. and\nthen visualizes them using a novel interactive interface. Using a case study,\nwe demonstrate how WordBias can help uncover biases against intersectional\ngroups like Black Muslim Males, Poor Females, etc. encoded in word embedding.\nIn addition, we also evaluate our tool using qualitative feedback from expert\ninterviews. The source code for this tool can be publicly accessed for\nreproducibility at github.com/bhavyaghai/WordBias.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghai_B/0/1/0/all/0/1\">Bhavya Ghai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoque_M/0/1/0/all/0/1\">Md Naimul Hoque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mueller_K/0/1/0/all/0/1\">Klaus Mueller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sentence-Permuted Paragraph Generation. (arXiv:2104.07228v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07228","description":"<p>Generating paragraphs of diverse contents is important in many applications.\nExisting generation models produce similar contents from homogenized contexts\ndue to the fixed left-to-right sentence order. Our idea is permuting the\nsentence orders to improve the content diversity of multi-sentence paragraph.\nWe propose a novel framework PermGen whose objective is to maximize the\nexpected log-likelihood of output paragraph distributions with respect to all\npossible sentence orders. PermGen uses hierarchical positional embedding and\ndesigns new procedures for training, decoding, and candidate ranking in the\nsentence-permuted generation. Experiments on three paragraph generation\nbenchmarks demonstrate PermGen generates more diverse outputs with a higher\nquality than existing models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Wenhao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zhichun Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1\">Meng Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data-QuestEval: A Referenceless Metric for Data-to-Text Semantic Evaluation. (arXiv:2104.07555v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07555","description":"<p>QuestEval is a reference-less metric used in text-to-text tasks, that\ncompares the generated summaries directly to the source text, by automatically\nasking and answering questions. Its adaptation to Data-to-Text tasks is not\nstraightforward, as it requires multimodal Question Generation and Answering\nsystems on the considered tasks, which are seldom available. To this purpose,\nwe propose a method to build synthetic multimodal corpora enabling to train\nmultimodal components for a data-QuestEval metric. The resulting metric is\nreference-less and multimodal; it obtains state-of-the-art correlations with\nhuman judgment on the WebNLG and WikiBio benchmarks. We make data-QuestEval's\ncode and models available for reproducibility purpose, as part of the QuestEval\nproject.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rebuffel_C/0/1/0/all/0/1\">Cl&#xe9;ment Rebuffel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scialom_T/0/1/0/all/0/1\">Thomas Scialom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soulier_L/0/1/0/all/0/1\">Laure Soulier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piwowarski_B/0/1/0/all/0/1\">Benjamin Piwowarski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lamprier_S/0/1/0/all/0/1\">Sylvain Lamprier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Staiano_J/0/1/0/all/0/1\">Jacopo Staiano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scoutheeten_G/0/1/0/all/0/1\">Geoffrey Scoutheeten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gallinari_P/0/1/0/all/0/1\">Patrick Gallinari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Graph Augmented Political Perspective Detection in News Media. (arXiv:2108.03861v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.03861","description":"<p>Identifying political perspective in news media has become an important task\ndue to the rapid growth of political commentary and the increasingly polarized\nideologies. Previous approaches only focus on leveraging the semantic\ninformation and leaves out the rich social and political context that helps\nindividuals understand political stances. In this paper, we propose a\nperspective detection method that incorporates external knowledge of real-world\npolitics. Specifically, we construct a contemporary political knowledge graph\nwith 1,071 entities and 10,703 triples. We then build a heterogeneous\ninformation network for each news document that jointly models article\nsemantics and external knowledge in knowledge graphs. Finally, we apply gated\nrelational graph convolutional networks and conduct political perspective\ndetection as graph-level classification. Extensive experiments show that our\nmethod achieves the best performance and outperforms state-of-the-art methods\nby 5.49%. Numerous ablation studies further bear out the necessity of external\nknowledge and the effectiveness of our graph-based approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Shangbin Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_M/0/1/0/all/0/1\">Minnan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zilong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qingyao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xiaojun Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Q/0/1/0/all/0/1\">Qinghua Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Encoding Heterogeneous Social and Political Context for Entity Stance Prediction. (arXiv:2108.03881v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.03881","description":"<p>Political stance detection has become an important task due to the\nincreasingly polarized political ideologies. Most existing works focus on\nidentifying perspectives in news articles or social media posts, while social\nentities, such as individuals and organizations, produce these texts and\nactually take stances. In this paper, we propose the novel task of entity\nstance prediction, which aims to predict entities' stances given their social\nand political context. Specifically, we retrieve facts from Wikipedia about\nsocial entities regarding contemporary U.S. politics. We then annotate social\nentities' stances towards political ideologies with the help of domain experts.\nAfter defining the task of entity stance prediction, we propose a graph-based\nsolution, which constructs a heterogeneous information network from collected\nfacts and adopts gated relational graph convolutional networks for\nrepresentation learning. Our model is then trained with a combination of\nsupervised, self-supervised and unsupervised loss functions, which are\nmotivated by multiple social and political phenomenons. We conduct extensive\nexperiments to compare our method with existing text and graph analysis\nbaselines. Our model achieves highest stance detection accuracy and yields\ninspiring insights regarding social entity stances. We further conduct ablation\nstudy and parameter analysis to study the mechanism and effectiveness of our\nproposed approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Shangbin Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_M/0/1/0/all/0/1\">Minnan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zilong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Peisheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xiaojun Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Q/0/1/0/all/0/1\">Qinghua Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DuTrust: A Sentiment Analysis Dataset for Trustworthiness Evaluation. (arXiv:2108.13140v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.13140","description":"<p>While deep learning models have greatly improved the performance of most\nartificial intelligence tasks, they are often criticized to be untrustworthy\ndue to the black-box problem. Consequently, many works have been proposed to\nstudy the trustworthiness of deep learning. However, as most open datasets are\ndesigned for evaluating the accuracy of model outputs, there is still a lack of\nappropriate datasets for evaluating the inner workings of neural networks. The\nlack of datasets obviously hinders the development of trustworthiness research.\nTherefore, in order to systematically evaluate the factors for building\ntrustworthy systems, we propose a novel and well-annotated sentiment analysis\ndataset to evaluate robustness and interpretability. To evaluate these factors,\nour dataset contains diverse annotations about the challenging distribution of\ninstances, manual adversarial instances and sentiment explanations. Several\nevaluation metrics are further proposed for interpretability and robustness.\nBased on the dataset and metrics, we conduct comprehensive comparisons for the\ntrustworthiness of three typical models, and also study the relations between\naccuracy, robustness and interpretability. We release this trustworthiness\nevaluation dataset at \\url{https://github/xyz} and hope our work can facilitate\nthe progress on building more trustworthy systems for real-world applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lijie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1\">Shuyuan Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hongxuan Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1\">Xinyan Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Ying Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haifeng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FinQA: A Dataset of Numerical Reasoning over Financial Data. (arXiv:2109.00122v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.00122","description":"<p>The sheer volume of financial statements makes it difficult for humans to\naccess and analyze a business's financials. Robust numerical reasoning likewise\nfaces unique challenges in this domain. In this work, we focus on answering\ndeep questions over financial data, aiming to automate the analysis of a large\ncorpus of financial documents. In contrast to existing tasks on general domain,\nthe finance domain includes complex numerical reasoning and understanding of\nheterogeneous representations. To facilitate analytical progress, we propose a\nnew large-scale dataset, FinQA, with Question-Answering pairs over Financial\nreports, written by financial experts. We also annotate the gold reasoning\nprograms to ensure full explainability. We further introduce baselines and\nconduct comprehensive experiments in our dataset. The results demonstrate that\npopular, large, pre-trained models fall far short of expert humans in acquiring\nfinance knowledge and in complex multi-step numerical reasoning on that\nknowledge. Our dataset -- the first of its kind -- should therefore enable\nsignificant, new community research into complex application domains. The\ndataset and code are publicly available\\url{https://github.com/czyssrs/FinQA}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhiyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wenhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smiley_C/0/1/0/all/0/1\">Charese Smiley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_S/0/1/0/all/0/1\">Sameena Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borova_I/0/1/0/all/0/1\">Iana Borova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Langdon_D/0/1/0/all/0/1\">Dylan Langdon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moussa_R/0/1/0/all/0/1\">Reema Moussa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beane_M/0/1/0/all/0/1\">Matt Beane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Ting-Hao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Routledge_B/0/1/0/all/0/1\">Bryan Routledge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MultiEURLEX -- A multi-lingual and multi-label legal document classification dataset for zero-shot cross-lingual transfer. (arXiv:2109.00904v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.00904","description":"<p>We introduce MULTI-EURLEX, a new multilingual dataset for topic\nclassification of legal documents. The dataset comprises 65k European Union\n(EU) laws, officially translated in 23 languages, annotated with multiple\nlabels from the EUROVOC taxonomy. We highlight the effect of temporal concept\ndrift and the importance of chronological, instead of random splits. We use the\ndataset as a testbed for zero-shot cross-lingual transfer, where we exploit\nannotated training documents in one language (source) to classify documents in\nanother language (target). We find that fine-tuning a multilingually pretrained\nmodel (XLM-ROBERTA, MT5) in a single source language leads to catastrophic\nforgetting of multilingual knowledge and, consequently, poor zero-shot transfer\nto other languages. Adaptation strategies, namely partial fine-tuning,\nadapters, BITFIT, LNFIT, originally proposed to accelerate fine-tuning for new\nend-tasks, help retain multilingual knowledge from pretraining, substantially\nimproving zero-shot cross-lingual transfer, but their impact also depends on\nthe pretrained model used and the size of the label set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chalkidis_I/0/1/0/all/0/1\">Ilias Chalkidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fergadiotis_M/0/1/0/all/0/1\">Manos Fergadiotis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Androutsopoulos_I/0/1/0/all/0/1\">Ion Androutsopoulos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Eliminating Sentiment Bias for Aspect-Level Sentiment Classification with Unsupervised Opinion Extraction. (arXiv:2109.02403v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.02403","description":"<p>Aspect-level sentiment classification (ALSC) aims at identifying the\nsentiment polarity of a specified aspect in a sentence. ALSC is a practical\nsetting in aspect-based sentiment analysis due to no opinion term labeling\nneeded, but it fails to interpret why a sentiment polarity is derived for the\naspect. To address this problem, recent works fine-tune pre-trained Transformer\nencoders for ALSC to extract an aspect-centric dependency tree that can locate\nthe opinion words. However, the induced opinion words only provide an intuitive\ncue far below human-level interpretability. Besides, the pre-trained encoder\ntends to internalize an aspect's intrinsic sentiment, causing sentiment bias\nand thus affecting model performance. In this paper, we propose a span-based\nanti-bias aspect representation learning framework. It first eliminates the\nsentiment bias in the aspect embedding by adversarial learning against aspects'\nprior sentiment. Then, it aligns the distilled opinion candidates with the\naspect by span-based dependency modeling to highlight the interpretable opinion\nterms. Our method achieves new state-of-the-art performance on five benchmarks,\nwith the capability of unsupervised opinion extraction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_T/0/1/0/all/0/1\">Tao Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_G/0/1/0/all/0/1\">Guodong Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1\">Tianyi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yi Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-09-07T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"On the Out-of-distribution Generalization of Probabilistic Image Modelling. (arXiv:2109.02639v1 [cs.CV])","link":"http://arxiv.org/abs/2109.02639","description":"<p>Out-of-distribution (OOD) detection and lossless compression constitute two\nproblems that can be solved by the training of probabilistic models on a first\ndataset with subsequent likelihood evaluation on a second dataset, where data\ndistributions differ. By defining the generalization of probabilistic models in\nterms of likelihood we show that, in the case of image models, the OOD\ngeneralization ability is dominated by local features. This motivates our\nproposal of a Local Autoregressive model that exclusively models local image\nfeatures towards improving OOD performance. We apply the proposed model to OOD\ndetection tasks and achieve state-of-the-art unsupervised OOD detection\nperformance without the introduction of additional data. Additionally, we\nemploy our model to build a new lossless image compressor: NeLLoC (Neural Local\nLossless Compressor) and report state-of-the-art compression rates and model\nsize.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mingtian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Andi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McDonagh_S/0/1/0/all/0/1\">Steven McDonagh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End to end hyperspectral imaging system with coded compression imaging process. (arXiv:2109.02643v1 [eess.IV])","link":"http://arxiv.org/abs/2109.02643","description":"<p>Hyperspectral images (HSIs) can provide rich spatial and spectral information\nwith extensive application prospects. Recently, several methods using\nconvolutional neural networks (CNNs) to reconstruct HSIs have been developed.\nHowever, most deep learning methods fit a brute-force mapping relationship\nbetween the compressive and standard HSIs. Thus, the learned mapping would be\ninvalid when the observation data deviate from the training data. To recover\nthe three-dimensional HSIs from two-dimensional compressive images, we present\ndual-camera equipment with a physics-informed self-supervising CNN method based\non a coded aperture snapshot spectral imaging system. Our method effectively\nexploits the spatial-spectral relativization from the coded spectral\ninformation and forms a self-supervising system based on the camera quantum\neffect model. The experimental results show that our method can be adapted to a\nwide imaging environment with good performance. In addition, compared with most\nof the network-based methods, our system does not require a dedicated dataset\nfor pre-training. Therefore, it has greater scenario adaptability and better\ngeneralization ability. Meanwhile, our system can be constantly fine-tuned and\nself-improved in real-life scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Xie_H/0/1/0/all/0/1\">Hui Xie</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhuang Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Han_J/0/1/0/all/0/1\">Jing Han</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bai_L/0/1/0/all/0/1\">Lianfa Bai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_J/0/1/0/all/0/1\">Jun Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Crowdsourcing Annotation: Partial Annotation with Salient Labels for Multi-Label Image Classification. (arXiv:2109.02688v1 [cs.CV])","link":"http://arxiv.org/abs/2109.02688","description":"<p>Annotated images are required for both supervised model training and\nevaluation in image classification. Manually annotating images is arduous and\nexpensive, especially for multi-labeled images. A recent trend for conducting\nsuch laboursome annotation tasks is through crowdsourcing, where images are\nannotated by volunteers or paid workers online (e.g., workers of Amazon\nMechanical Turk) from scratch. However, the quality of crowdsourcing image\nannotations cannot be guaranteed, and incompleteness and incorrectness are two\nmajor concerns for crowdsourcing annotations. To address such concerns, we have\na rethinking of crowdsourcing annotations: Our simple hypothesis is that if the\nannotators only partially annotate multi-label images with salient labels they\nare confident in, there will be fewer annotation errors and annotators will\nspend less time on uncertain labels. As a pleasant surprise, with the same\nannotation budget, we show a multi-label image classifier supervised by images\nwith salient annotations can outperform models supervised by fully annotated\nimages. Our method contributions are 2-fold: An active learning way is proposed\nto acquire salient labels for multi-label images; and a novel Adaptive\nTemperature Associated Model (ATAM) specifically using partial annotations is\nproposed for multi-label image classification. We conduct experiments on\npractical crowdsourcing data, the Open Street Map (OSM) dataset and benchmark\ndataset COCO 2014. When compared with state-of-the-art classification methods\ntrained on fully annotated images, the proposed ATAM can achieve higher\naccuracy. The proposed idea is promising for crowdsourcing data annotation. Our\ncode will be publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jianzhe Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tianze Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Z. Jane Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Transferability of Domain Adaptation Networks Through Domain Alignment Layers. (arXiv:2109.02693v1 [cs.CV])","link":"http://arxiv.org/abs/2109.02693","description":"<p>Deep learning (DL) has been the primary approach used in various computer\nvision tasks due to its relevant results achieved on many tasks. However, on\nreal-world scenarios with partially or no labeled data, DL methods are also\nprone to the well-known domain shift problem. Multi-source unsupervised domain\nadaptation (MSDA) aims at learning a predictor for an unlabeled domain by\nassigning weak knowledge from a bag of source models. However, most works\nconduct domain adaptation leveraging only the extracted features and reducing\ntheir domain shift from the perspective of loss function designs. In this\npaper, we argue that it is not sufficient to handle domain shift only based on\ndomain-level features, but it is also essential to align such information on\nthe feature space. Unlike previous works, we focus on the network design and\npropose to embed Multi-Source version of DomaIn Alignment Layers (MS-DIAL) at\ndifferent levels of the predictor. These layers are designed to match the\nfeature distributions between different domains and can be easily applied to\nvarious MSDA methods. To show the robustness of our approach, we conducted an\nextensive experimental evaluation considering two challenging scenarios: digit\nrecognition and object classification. The experimental results indicated that\nour approach can improve state-of-the-art MSDA methods, yielding relative gains\nof up to +30.64% on their classification accuracies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Silva_L/0/1/0/all/0/1\">Lucas Fernando Alvarenga e Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pedronette_D/0/1/0/all/0/1\">Daniel Carlos Guimar&#xe3;es Pedronette</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faria_F/0/1/0/all/0/1\">F&#xe1;bio Augusto Faria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papa_J/0/1/0/all/0/1\">Jo&#xe3;o Paulo Papa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Almeida_J/0/1/0/all/0/1\">Jurandy Almeida</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Intelligent Motion Planning for a Cost-effective Object Follower Mobile Robotic System with Obstacle Avoidance. (arXiv:2109.02700v1 [cs.RO])","link":"http://arxiv.org/abs/2109.02700","description":"<p>There are few industries which use manually controlled robots for carrying\nmaterial and this cannot be used all the time in all the places. So, it is very\ntranquil to have robots which can follow a specific human by following the\nunique coloured object held by that person. So, we propose a robotic system\nwhich uses robot vision and deep learning to get the required linear and\nangular velocities which are {\\nu} and {\\omega}, respectively. Which in turn\nmakes the robot to avoid obstacles when following the unique coloured object\nheld by the human. The novel methodology that we are proposing is accurate in\ndetecting the position of the unique coloured object in any kind of lighting\nand tells us the horizontal pixel value where the robot is present and also\ntells if the object is close to or far from the robot. Moreover, the artificial\nneural networks that we have used in this problem gave us a meagre error in\nlinear and angular velocity prediction and the PI controller which was used to\ncontrol the linear and angular velocities, which in turn controls the position\nof the robot gave us impressive results and this methodology outperforms all\nother methodologies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gona_S/0/1/0/all/0/1\">Sai Nikhil Gona</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bandhakavi_P/0/1/0/all/0/1\">Prithvi Raj Bandhakavi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Crash Report Data Analysis for Creating Scenario-Wise, Spatio-Temporal Attention Guidance to Support Computer Vision-based Perception of Fatal Crash Risks. (arXiv:2109.02710v1 [stat.AP])","link":"http://arxiv.org/abs/2109.02710","description":"<p>Reducing traffic fatalities and serious injuries is a top priority of the US\nDepartment of Transportation. The computer vision (CV)-based crash anticipation\nin the near-crash phase is receiving growing attention. The ability to perceive\nfatal crash risks earlier is also critical because it will improve the\nreliability of crash anticipation. Yet, annotated image data for training a\nreliable AI model for the early visual perception of crash risks are not\nabundant. The Fatality Analysis Reporting System contains big data of fatal\ncrashes. It is a reliable data source for learning the relationship between\ndriving scene characteristics and fatal crashes to compensate for the\nlimitation of CV. Therefore, this paper develops a data analytics model, named\nscenario-wise, Spatio-temporal attention guidance, from fatal crash report\ndata, which can estimate the relevance of detected objects to fatal crashes\nfrom their environment and context information. First, the paper identifies\nfive sparse variables that allow for decomposing the 5-year fatal crash dataset\nto develop scenario-wise attention guidance. Then, exploratory analysis of\nlocation- and time-related variables of the crash report data suggests reducing\nfatal crashes to spatially defined groups. The group's temporal pattern is an\nindicator of the similarity of fatal crashes in the group. Hierarchical\nclustering and K-means clustering merge the spatially defined groups into six\nclusters according to the similarity of their temporal patterns. After that,\nassociation rule mining discovers the statistical relationship between the\ntemporal information of driving scenes with crash features, for each cluster.\nThe paper shows how the developed attention guidance supports the design and\nimplementation of a preliminary CV model that can identify objects of a\npossibility to involve in fatal crashes from their environment and context\ninformation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Li_Y/0/1/0/all/0/1\">Yu Li</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Karim_M/0/1/0/all/0/1\">Muhammad Monjurul Karim</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Qin_R/0/1/0/all/0/1\">Ruwen Qin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Graph Attention Layer Evolves Semantic Segmentation for Road Pothole Detection: A Benchmark and Algorithms. (arXiv:2109.02711v1 [cs.CV])","link":"http://arxiv.org/abs/2109.02711","description":"<p>Existing road pothole detection approaches can be classified as computer\nvision-based or machine learning-based. The former approaches typically employ\n2-D image analysis/understanding or 3-D point cloud modeling and segmentation\nalgorithms to detect road potholes from vision sensor data. The latter\napproaches generally address road pothole detection using convolutional neural\nnetworks (CNNs) in an end-to-end manner. However, road potholes are not\nnecessarily ubiquitous and it is challenging to prepare a large well-annotated\ndataset for CNN training. In this regard, while computer vision-based methods\nwere the mainstream research trend in the past decade, machine learning-based\nmethods were merely discussed. Recently, we published the first stereo\nvision-based road pothole detection dataset and a novel disparity\ntransformation algorithm, whereby the damaged and undamaged road areas can be\nhighly distinguished. However, there are no benchmarks currently available for\nstate-of-the-art (SoTA) CNNs trained using either disparity images or\ntransformed disparity images. Therefore, in this paper, we first discuss the\nSoTA CNNs designed for semantic segmentation and evaluate their performance for\nroad pothole detection with extensive experiments. Additionally, inspired by\ngraph neural network (GNN), we propose a novel CNN layer, referred to as graph\nattention layer (GAL), which can be easily deployed in any existing CNN to\noptimize image feature representations for semantic segmentation. Our\nexperiments compare GAL-DeepLabv3+, our best-performing implementation, with\nnine SoTA CNNs on three modalities of training data: RGB images, disparity\nimages, and transformed disparity images. The experimental results suggest that\nour proposed GAL-DeepLabv3+ achieves the best overall pothole detection\naccuracy on all training data modalities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_R/0/1/0/all/0/1\">Rui Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hengli Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Ming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pitas_I/0/1/0/all/0/1\">Ioannis Pitas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision Transformers For Weeds and Crops Classification Of High Resolution UAV Images. (arXiv:2109.02716v1 [cs.CV])","link":"http://arxiv.org/abs/2109.02716","description":"<p>Crop and weed monitoring is an important challenge for agriculture and food\nproduction nowadays. Thanks to recent advances in data acquisition and\ncomputation technologies, agriculture is evolving to a more smart and precision\nfarming to meet with the high yield and high quality crop production.\nClassification and recognition in Unmanned Aerial Vehicles (UAV) images are\nimportant phases for crop monitoring. Advances in deep learning models relying\non Convolutional Neural Network (CNN) have achieved high performances in image\nclassification in the agricultural domain. Despite the success of this\narchitecture, CNN still faces many challenges such as high computation cost,\nthe need of large labelled datasets, ... Natural language processing's\ntransformer architecture can be an alternative approach to deal with CNN's\nlimitations. Making use of the self-attention paradigm, Vision Transformer\n(ViT) models can achieve competitive or better results without applying any\nconvolution operations. In this paper, we adopt the self-attention mechanism\nvia the ViT models for plant classification of weeds and crops: red beet,\noff-type beet (green leaves), parsley and spinach. Our experiments show that\nwith small set of labelled training data, ViT models perform better compared to\nstate-of-the-art CNN-based models EfficientNet and ResNet, with a top accuracy\nof 99.8\\% achieved by the ViT model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Reedha_R/0/1/0/all/0/1\">Reenul Reedha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dericquebourg_E/0/1/0/all/0/1\">Eric Dericquebourg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Canals_R/0/1/0/all/0/1\">Raphael Canals</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hafiane_A/0/1/0/all/0/1\">Adel Hafiane</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Landmarks Correspondence Detection in Medical Images with an Application to Deformable Image Registration. (arXiv:2109.02722v1 [cs.CV])","link":"http://arxiv.org/abs/2109.02722","description":"<p>Deformable Image Registration (DIR) can benefit from additional guidance\nusing corresponding landmarks in the images. However, the benefits thereof are\nlargely understudied, especially due to the lack of automatic detection methods\nfor corresponding landmarks in three-dimensional (3D) medical images. In this\nwork, we present a Deep Convolutional Neural Network (DCNN), called DCNN-Match,\nthat learns to predict landmark correspondences in 3D images in a\nself-supervised manner. We explored five variants of DCNN-Match that use\ndifferent loss functions and tested DCNN-Match separately as well as in\ncombination with the open-source registration software Elastix to assess its\nimpact on a common DIR approach. We employed lower-abdominal Computed\nTomography (CT) scans from cervical cancer patients: 121 pelvic CT scan pairs\ncontaining simulated elastic transformations and 11 pairs demonstrating\nclinical deformations. Our results show significant improvement in DIR\nperformance when landmark correspondences predicted by DCNN-Match were used in\ncase of simulated as well as clinical deformations. We also observed that the\nspatial distribution of the automatically identified landmarks and the\nassociated matching errors affect the extent of improvement in DIR. Finally,\nDCNN-Match was found to generalize well to Magnetic Resonance Imaging (MRI)\nscans without requiring retraining, indicating easy applicability to other\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Grewal_M/0/1/0/all/0/1\">Monika Grewal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiersma_J/0/1/0/all/0/1\">Jan Wiersma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Westerveld_H/0/1/0/all/0/1\">Henrike Westerveld</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bosman_P/0/1/0/all/0/1\">Peter A. N. Bosman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alderliesten_T/0/1/0/all/0/1\">Tanja Alderliesten</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Dietary Assessment Via Integrated Hierarchy Food Classification. (arXiv:2109.02736v1 [cs.CV])","link":"http://arxiv.org/abs/2109.02736","description":"<p>Image-based dietary assessment refers to the process of determining what\nsomeone eats and how much energy and nutrients are consumed from visual data.\nFood classification is the first and most crucial step. Existing methods focus\non improving accuracy measured by the rate of correct classification based on\nvisual information alone, which is very challenging due to the high complexity\nand inter-class similarity of foods. Further, accuracy in food classification\nis conceptual as description of a food can always be improved. In this work, we\nintroduce a new food classification framework to improve the quality of\npredictions by integrating the information from multiple domains while\nmaintaining the classification accuracy. We apply a multi-task network based on\na hierarchical structure that uses both visual and nutrition domain specific\ninformation to cluster similar foods. Our method is validated on the modified\nVIPER-FoodNet (VFN) food image dataset by including associated energy and\nnutrient information. We achieve comparable classification accuracy with\nexisting methods that use visual information only, but with less error in terms\nof energy and nutrient values for the wrong predictions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mao_R/0/1/0/all/0/1\">Runyu Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jiangpeng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Luotao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Z/0/1/0/all/0/1\">Zeman Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eicher_Miller_H/0/1/0/all/0/1\">Heather A. Eicher-Miller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Fengqing Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Single-Camera 3D Head Fitting for Mixed Reality Clinical Applications. (arXiv:2109.02740v1 [cs.CV])","link":"http://arxiv.org/abs/2109.02740","description":"<p>We address the problem of estimating the shape of a person's head, defined as\nthe geometry of the complete head surface, from a video taken with a single\nmoving camera, and determining the alignment of the fitted 3D head for all\nvideo frames, irrespective of the person's pose. 3D head reconstructions\ncommonly tend to focus on perfecting the face reconstruction, leaving the scalp\nto a statistical approximation. Our goal is to reconstruct the head model of\neach person to enable future mixed reality applications. To do this, we recover\na dense 3D reconstruction and camera information via structure-from-motion and\nmulti-view stereo. These are then used in a new two-stage fitting process to\nrecover the 3D head shape by iteratively fitting a 3D morphable model of the\nhead with the dense reconstruction in canonical space and fitting it to each\nperson's head, using both traditional facial landmarks and scalp features\nextracted from the head's segmentation mask. Our approach recovers consistent\ngeometry for varying head shapes, from videos taken by different people, with\ndifferent smartphones, and in a variety of environments from living rooms to\noutdoor spaces.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mane_T/0/1/0/all/0/1\">Tejas Mane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bayramova_A/0/1/0/all/0/1\">Aylar Bayramova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daniilidis_K/0/1/0/all/0/1\">Kostas Daniilidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mordohai_P/0/1/0/all/0/1\">Philippos Mordohai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bernardis_E/0/1/0/all/0/1\">Elena Bernardis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WhyAct: Identifying Action Reasons in Lifestyle Vlogs. (arXiv:2109.02747v1 [cs.CV])","link":"http://arxiv.org/abs/2109.02747","description":"<p>We aim to automatically identify human action reasons in online videos. We\nfocus on the widespread genre of lifestyle vlogs, in which people perform\nactions while verbally describing them. We introduce and make publicly\navailable the {\\sc WhyAct} dataset, consisting of 1,077 visual actions manually\nannotated with their reasons. We describe a multimodal model that leverages\nvisual and textual information to automatically infer the reasons corresponding\nto an action presented in the video.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ignat_O/0/1/0/all/0/1\">Oana Ignat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castro_S/0/1/0/all/0/1\">Santiago Castro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_H/0/1/0/all/0/1\">Hanwen Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Weiji Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1\">Rada Mihalcea</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-Shot Open Set Detection by Extending CLIP. (arXiv:2109.02748v1 [cs.CV])","link":"http://arxiv.org/abs/2109.02748","description":"<p>In a regular open set detection problem, samples of known classes (also\ncalled closed set classes) are used to train a special classifier. In testing,\nthe classifier can (1) classify the test samples of known classes to their\nrespective classes and (2) also detect samples that do not belong to any of the\nknown classes (we say they belong to some unknown or open set classes). This\npaper studies the problem of zero-shot open-set detection, which still performs\nthe same two tasks in testing but has no training except using the given known\nclass names. This paper proposes a novel and yet simple method (called ZO-CLIP)\nto solve the problem. ZO-CLIP builds on top of the recent advances in zero-shot\nclassification through multi-modal representation learning. It first extends\nthe pre-trained multi-modal model CLIP by training a text-based image\ndescription generator on top of CLIP. In testing, it uses the extended model to\ngenerate some candidate unknown class names for each test sample and computes a\nconfidence score based on both the known class names and candidate unknown\nclass names for zero-shot open set detection. Experimental results on 5\nbenchmark datasets for open set detection confirm that ZO-CLIP outperforms the\nbaselines by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Esmaeilpour_S/0/1/0/all/0/1\">Sepideh Esmaeilpour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robertson_E/0/1/0/all/0/1\">Eric Robertson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_L/0/1/0/all/0/1\">Lei Shu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pano3D: A Holistic Benchmark and a Solid Baseline for $360^o$ Depth Estimation. (arXiv:2109.02749v1 [cs.CV])","link":"http://arxiv.org/abs/2109.02749","description":"<p>Pano3D is a new benchmark for depth estimation from spherical panoramas. It\naims to assess performance across all depth estimation traits, the primary\ndirect depth estimation performance targeting precision and accuracy, and also\nthe secondary traits, boundary preservation, and smoothness. Moreover, Pano3D\nmoves beyond typical intra-dataset evaluation to inter-dataset performance\nassessment. By disentangling the capacity to generalize to unseen data into\ndifferent test splits, Pano3D represents a holistic benchmark for $360^o$ depth\nestimation. We use it as a basis for an extended analysis seeking to offer\ninsights into classical choices for depth estimation. This results in a solid\nbaseline for panoramic depth that follow-up works can build upon to steer\nfuture progress.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Albanis_G/0/1/0/all/0/1\">Georgios Albanis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zioulis_N/0/1/0/all/0/1\">Nikolaos Zioulis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drakoulis_P/0/1/0/all/0/1\">Petros Drakoulis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gkitsas_V/0/1/0/all/0/1\">Vasileios Gkitsas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sterzentsenko_V/0/1/0/all/0/1\">Vladimiros Sterzentsenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alvarez_F/0/1/0/all/0/1\">Federico Alvarez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zarpalas_D/0/1/0/all/0/1\">Dimitrios Zarpalas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daras_P/0/1/0/all/0/1\">Petros Daras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Training Deep Networks from Zero to Hero: avoiding pitfalls and going beyond. (arXiv:2109.02752v1 [cs.LG])","link":"http://arxiv.org/abs/2109.02752","description":"<p>Training deep neural networks may be challenging in real world data. Using\nmodels as black-boxes, even with transfer learning, can result in poor\ngeneralization or inconclusive results when it comes to small datasets or\nspecific applications. This tutorial covers the basic steps as well as more\nrecent options to improve models, in particular, but not restricted to,\nsupervised learning. It can be particularly useful in datasets that are not as\nwell-prepared as those in challenges, and also under scarce annotation and/or\nsmall data. We describe basic procedures: as data preparation, optimization and\ntransfer learning, but also recent architectural choices such as use of\ntransformer modules, alternative convolutional layers, activation functions,\nwide and deep networks, as well as training procedures including as curriculum,\ncontrastive and self-supervised learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ponti_M/0/1/0/all/0/1\">Moacir Antonelli Ponti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santos_F/0/1/0/all/0/1\">Fernando Pereira dos Santos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ribeiro_L/0/1/0/all/0/1\">Leo Sampaio Ferraz Ribeiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cavallari_G/0/1/0/all/0/1\">Gabriel Biscaro Cavallari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"STRIVE: Scene Text Replacement In Videos. (arXiv:2109.02762v1 [cs.CV])","link":"http://arxiv.org/abs/2109.02762","description":"<p>We propose replacing scene text in videos using deep style transfer and\nlearned photometric transformations.Building on recent progress on still image\ntext replacement,we present extensions that alter text while preserving the\nappearance and motion characteristics of the original video.Compared to the\nproblem of still image text replacement,our method addresses additional\nchallenges introduced by video, namely effects induced by changing lighting,\nmotion blur, diverse variations in camera-object pose over time,and\npreservation of temporal consistency. We parse the problem into three steps.\nFirst, the text in all frames is normalized to a frontal pose using a\nspatio-temporal trans-former network. Second, the text is replaced in a single\nreference frame using a state-of-art still-image text replacement method.\nFinally, the new text is transferred from the reference to remaining frames\nusing a novel learned image transformation network that captures lighting and\nblur effects in a temporally consistent manner. Results on synthetic and\nchallenging real videos show realistic text trans-fer, competitive quantitative\nand qualitative performance,and superior inference speed relative to\nalternatives. We introduce new synthetic and real-world datasets with paired\ntext objects. To the best of our knowledge this is the first attempt at deep\nvideo text replacement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+G_V/0/1/0/all/0/1\">Vijay Kumar B G</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subramanian_J/0/1/0/all/0/1\">Jeyasri Subramanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chordia_V/0/1/0/all/0/1\">Varnith Chordia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bart_E/0/1/0/all/0/1\">Eugene Bart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_S/0/1/0/all/0/1\">Shaobo Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_K/0/1/0/all/0/1\">Kelly Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bala_R/0/1/0/all/0/1\">Raja Bala</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Binaural SoundNet: Predicting Semantics, Depth and Motion with Binaural Sounds. (arXiv:2109.02763v1 [cs.SD])","link":"http://arxiv.org/abs/2109.02763","description":"<p>Humans can robustly recognize and localize objects by using visual and/or\nauditory cues. While machines are able to do the same with visual data already,\nless work has been done with sounds. This work develops an approach for scene\nunderstanding purely based on binaural sounds. The considered tasks include\npredicting the semantic masks of sound-making objects, the motion of\nsound-making objects, and the depth map of the scene. To this aim, we propose a\nnovel sensor setup and record a new audio-visual dataset of street scenes with\neight professional binaural microphones and a 360-degree camera. The\nco-existence of visual and audio cues is leveraged for supervision transfer. In\nparticular, we employ a cross-modal distillation framework that consists of\nmultiple vision teacher methods and a sound student method -- the student\nmethod is trained to generate the same results as the teacher methods do. This\nway, the auditory system can be trained without using human annotations. To\nfurther boost the performance, we propose another novel auxiliary task, coined\nSpatial Sound Super-Resolution, to increase the directional resolution of\nsounds. We then formulate the four tasks into one end-to-end trainable\nmulti-tasking network aiming to boost the overall performance. Experimental\nresults show that 1) our method achieves good results for all four tasks, 2)\nthe four tasks are mutually beneficial -- training them together achieves the\nbest performance, 3) the number and orientation of microphones are both\nimportant, and 4) features learned from the standard spectrogram and features\nobtained by the classic signal processing pipeline are complementary for\nauditory perception tasks. The data and code are released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1\">Dengxin Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasudevan_A/0/1/0/all/0/1\">Arun Balajee Vasudevan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matas_J/0/1/0/all/0/1\">Jiri Matas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robustness and Generalization via Generative Adversarial Training. (arXiv:2109.02765v1 [cs.CV])","link":"http://arxiv.org/abs/2109.02765","description":"<p>While deep neural networks have achieved remarkable success in various\ncomputer vision tasks, they often fail to generalize to new domains and subtle\nvariations of input images. Several defenses have been proposed to improve the\nrobustness against these variations. However, current defenses can only\nwithstand the specific attack used in training, and the models often remain\nvulnerable to other input variations. Moreover, these methods often degrade\nperformance of the model on clean images and do not generalize to out-of-domain\nsamples. In this paper we present Generative Adversarial Training, an approach\nto simultaneously improve the model's generalization to the test set and\nout-of-domain samples as well as its robustness to unseen adversarial attacks.\nInstead of altering a low-level pre-defined aspect of images, we generate a\nspectrum of low-level, mid-level and high-level changes using generative models\nwith a disentangled latent space. Adversarial training with these examples\nenable the model to withstand a wide range of attacks by observing a variety of\ninput alterations during training. We show that our approach not only improves\nperformance of the model on clean images and out-of-domain samples but also\nmakes it robust against unforeseen attacks and outperforms prior work. We\nvalidate effectiveness of our method by demonstrating results on various tasks\nsuch as classification, segmentation and object detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Poursaeed_O/0/1/0/all/0/1\">Omid Poursaeed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1\">Tianxing Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Harry Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belongie_S/0/1/0/all/0/1\">Serge Belongie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1\">SerNam Lim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analysis of MRI Biomarkers for Brain Cancer Survival Prediction. (arXiv:2109.02785v1 [q-bio.QM])","link":"http://arxiv.org/abs/2109.02785","description":"<p>Prediction of Overall Survival (OS) of brain cancer patients from multi-modal\nMRI is a challenging field of research. Most of the existing literature on\nsurvival prediction is based on Radiomic features, which does not consider\neither non-biological factors or the functional neurological status of the\npatient(s). Besides, the selection of an appropriate cut-off for survival and\nthe presence of censored data create further problems. Application of deep\nlearning models for OS prediction is also limited due to the lack of large\nannotated publicly available datasets. In this scenario we analyse the\npotential of two novel neuroimaging feature families, extracted from brain\nparcellation atlases and spatial habitats, along with classical radiomic and\ngeometric features; to study their combined predictive power for analysing\noverall survival. A cross validation strategy with grid search is proposed to\nsimultaneously select and evaluate the most predictive feature subset based on\nits predictive power. A Cox Proportional Hazard (CoxPH) model is employed for\nunivariate feature selection, followed by the prediction of patient-specific\nsurvival functions by three multivariate parsimonious models viz. Coxnet,\nRandom survival forests (RSF) and Survival SVM (SSVM). The brain cancer MRI\ndata used for this research was taken from two open-access collections TCGA-GBM\nand TCGA-LGG available from The Cancer Imaging Archive (TCIA). Corresponding\nsurvival data for each patient was downloaded from The Cancer Genome Atlas\n(TCGA). A high cross validation $C-index$ score of $0.82\\pm.10$ was achieved\nusing RSF with the best $24$ selected features. Age was found to be the most\nimportant biological predictor. There were $9$, $6$, $6$ and $2$ features\nselected from the parcellation, habitat, radiomic and region-based feature\ngroups respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Banerjee_S/0/1/0/all/0/1\">Subhashis Banerjee</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Mitra_S/0/1/0/all/0/1\">Sushmita Mitra</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Hall_L/0/1/0/all/0/1\">Lawrence O. Hall</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep SIMBAD: Active Landmark-based Self-localization Using Ranking -based Scene Descriptor. (arXiv:2109.02786v1 [cs.RO])","link":"http://arxiv.org/abs/2109.02786","description":"<p>Landmark-based robot self-localization has recently garnered interest as a\nhighly-compressive domain-invariant approach for performing visual place\nrecognition (VPR) across domains (e.g., time of day, weather, and season).\nHowever, landmark-based self-localization can be an ill-posed problem for a\npassive observer (e.g., manual robot control), as many viewpoints may not\nprovide an effective landmark view. In this study, we consider an active\nself-localization task by an active observer and present a novel reinforcement\nlearning (RL)-based next-best-view (NBV) planner. Our contributions are as\nfollows. (1) SIMBAD-based VPR: We formulate the problem of landmark-based\ncompact scene description as SIMBAD (similarity-based pattern recognition) and\nfurther present its deep learning extension. (2) VPR-to-NBV knowledge transfer:\nWe address the challenge of RL under uncertainty (i.e., active\nself-localization) by transferring the state recognition ability of VPR to the\nNBV. (3) NNQL-based NBV: We regard the available VPR as the experience database\nby adapting nearest-neighbor approximation of Q-learning (NNQL). The result\nshows an extremely compact data structure that compresses both the VPR and NBV\ninto a single incremental inverted index. Experiments using the public NCLT\ndataset validated the effectiveness of the proposed approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kanji_T/0/1/0/all/0/1\">Tanaka Kanji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Collaborative Multi-Modal Learning for Unsupervised Kinship Estimation. (arXiv:2109.02804v1 [cs.CV])","link":"http://arxiv.org/abs/2109.02804","description":"<p>Kinship verification is a long-standing research challenge in computer\nvision. The visual differences presented to the face have a significant effect\non the recognition capabilities of the kinship systems. We argue that\naggregating multiple visual knowledge can better describe the characteristics\nof the subject for precise kinship identification. Typically, the age-invariant\nfeatures can represent more natural facial details. Such age-related\ntransformations are essential for face recognition due to the biological\neffects of aging. However, the existing methods mainly focus on employing the\nsingle-view image features for kinship identification, while more meaningful\nvisual properties such as race and age are directly ignored in the feature\nlearning step. To this end, we propose a novel deep collaborative multi-modal\nlearning (DCML) to integrate the underlying information presented in facial\nproperties in an adaptive manner to strengthen the facial details for effective\nunsupervised kinship verification. Specifically, we construct a well-designed\nadaptive feature fusion mechanism, which can jointly leverage the complementary\nproperties from different visual perspectives to produce composite features and\ndraw greater attention to the most informative components of spatial feature\nmaps. Particularly, an adaptive weighting strategy is developed based on a\nnovel attention mechanism, which can enhance the dependencies between different\nproperties by decreasing the information redundancy in channels in a\nself-adaptive manner. To validate the effectiveness of the proposed method,\nextensive experimental evaluations conducted on four widely-used datasets show\nthat our DCML method is always superior to some state-of-the-art kinship\nverification methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_G/0/1/0/all/0/1\">Guan-Nan Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pun_C/0/1/0/all/0/1\">Chi-Man Pun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Kinship Verification Based on Cross-Generation Feature Interaction Learning. (arXiv:2109.02809v1 [cs.CV])","link":"http://arxiv.org/abs/2109.02809","description":"<p>Kinship verification from facial images has been recognized as an emerging\nyet challenging technique in many potential computer vision applications. In\nthis paper, we propose a novel cross-generation feature interaction learning\n(CFIL) framework for robust kinship verification. Particularly, an effective\ncollaborative weighting strategy is constructed to explore the characteristics\nof cross-generation relations by corporately extracting features of both\nparents and children image pairs. Specifically, we take parents and children as\na whole to extract the expressive local and non-local features. Different from\nthe traditional works measuring similarity by distance, we interpolate the\nsimilarity calculations as the interior auxiliary weights into the deep CNN\narchitecture to learn the whole and natural features. These similarity weights\nnot only involve corresponding single points but also excavate the multiple\nrelationships cross points, where local and non-local features are calculated\nby using these two kinds of distance measurements. Importantly, instead of\nseparately conducting similarity computation and feature extraction, we\nintegrate similarity learning and feature extraction into one unified learning\nprocess. The integrated representations deduced from local and non-local\nfeatures can comprehensively express the informative semantics embedded in\nimages and preserve abundant correlation knowledge from image pairs. Extensive\nexperiments demonstrate the efficiency and superiority of the proposed model\ncompared to some state-of-the-art kinship verification methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_G/0/1/0/all/0/1\">Guan-Nan Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pun_C/0/1/0/all/0/1\">Chi-Man Pun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-shot Learning via Dependency Maximization and Instance Discriminant Analysis. (arXiv:2109.02820v1 [cs.CV])","link":"http://arxiv.org/abs/2109.02820","description":"<p>We study the few-shot learning (FSL) problem, where a model learns to\nrecognize new objects with extremely few labeled training data per category.\nMost of previous FSL approaches resort to the meta-learning paradigm, where the\nmodel accumulates inductive bias through learning many training tasks so as to\nsolve a new unseen few-shot task. In contrast, we propose a simple approach to\nexploit unlabeled data accompanying the few-shot task for improving few-shot\nperformance. Firstly, we propose a Dependency Maximization method based on the\nHilbert-Schmidt norm of the cross-covariance operator, which maximizes the\nstatistical dependency between the embedded feature of those unlabeled data and\ntheir label predictions, together with the supervised loss over the support\nset. We then use the obtained model to infer the pseudo-labels for those\nunlabeled data. Furthermore, we propose anInstance Discriminant Analysis to\nevaluate the credibility of each pseudo-labeled example and select the most\nfaithful ones into an augmented support set to retrain the model as in the\nfirst step. We iterate the above process until the pseudo-labels for the\nunlabeled data becomes stable. Following the standard transductive and\nsemi-supervised FSL setting, our experiments show that the proposed method\nout-performs previous state-of-the-art methods on four widely used benchmarks,\nincluding mini-ImageNet, tiered-ImageNet, CUB, and CIFARFS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hou_Z/0/1/0/all/0/1\">Zejiang Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kung_S/0/1/0/all/0/1\">Sun-Yuan Kung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CIM: Class-Irrelevant Mapping for Few-Shot Classification. (arXiv:2109.02840v1 [cs.CV])","link":"http://arxiv.org/abs/2109.02840","description":"<p>Few-shot classification (FSC) is one of the most concerned hot issues in\nrecent years. The general setting consists of two phases: (1) Pre-train a\nfeature extraction model (FEM) with base data (has large amounts of labeled\nsamples). (2) Use the FEM to extract the features of novel data (with few\nlabeled samples and totally different categories from base data), then classify\nthem with the to-be-designed classifier. The adaptability of pre-trained FEM to\nnovel data determines the accuracy of novel features, thereby affecting the\nfinal classification performances. To this end, how to appraise the pre-trained\nFEM is the most crucial focus in the FSC community. It sounds like traditional\nClass Activate Mapping (CAM) based methods can achieve this by overlaying\nweighted feature maps. However, due to the particularity of FSC (e.g., there is\nno backpropagation when using the pre-trained FEM to extract novel features),\nwe cannot activate the feature map with the novel classes. To address this\nchallenge, we propose a simple, flexible method, dubbed as Class-Irrelevant\nMapping (CIM). Specifically, first, we introduce dictionary learning theory and\nview the channels of the feature map as the bases in a dictionary. Then we\nutilize the feature map to fit the feature vector of an image to achieve the\ncorresponding channel weights. Finally, we overlap the weighted feature map for\nvisualization to appraise the ability of pre-trained FEM on novel data. For\nfair use of CIM in evaluating different models, we propose a new measurement\nindex, called Feature Localization Accuracy (FLA). In experiments, we first\ncompare our CIM with CAM in regular tasks and achieve outstanding performances.\nNext, we use our CIM to appraise several classical FSC frameworks without\nconsidering the classification results and discuss them.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shao_S/0/1/0/all/0/1\">Shuai Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_L/0/1/0/all/0/1\">Lei Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yixin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan-Jiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bao-Di Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yicong Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GCsT: Graph Convolutional Skeleton Transformer for Action Recognition. (arXiv:2109.02860v1 [cs.CV])","link":"http://arxiv.org/abs/2109.02860","description":"<p>Graph convolutional networks (GCNs) achieve promising performance for\nskeleton-based action recognition. However, in most GCN-based methods, the\nspatial-temporal graph convolution is strictly restricted by the graph topology\nwhile only captures the short-term temporal context, thus lacking the\nflexibility of feature extraction. In this work, we present a novel\narchitecture, named Graph Convolutional skeleton Transformer (GCsT), which\naddresses limitations in GCNs by introducing Transformer. Our GCsT employs all\nthe benefits of Transformer (i.e. dynamical attention and global context) while\nkeeps the advantages of GCNs (i.e. hierarchy and local topology structure). In\nGCsT, the spatial-temporal GCN forces the capture of local dependencies while\nTransformer dynamically extracts global spatial-temporal relationships.\nFurthermore, the proposed GCsT shows stronger expressive capability by adding\nadditional information present in skeleton sequences. Incorporating the\nTransformer allows that information to be introduced into the model almost\neffortlessly. We validate the proposed GCsT by conducting extensive\nexperiments, which achieves the state-of-the-art performance on NTU RGB+D, NTU\nRGB+D 120 and Northwestern-UCLA datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_R/0/1/0/all/0/1\">Ruwen Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Min Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_B/0/1/0/all/0/1\">Bo Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fengfa Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Junxing Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1\">Miao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_D/0/1/0/all/0/1\">Degang Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ICCAD Special Session Paper: Quantum-Classical Hybrid Machine Learning for Image Classification. (arXiv:2109.02862v1 [cs.CV])","link":"http://arxiv.org/abs/2109.02862","description":"<p>Image classification is a major application domain for conventional deep\nlearning (DL). Quantum machine learning (QML) has the potential to\nrevolutionize image classification. In any typical DL-based image\nclassification, we use convolutional neural network (CNN) to extract features\nfrom the image and multi-layer perceptron network (MLP) to create the actual\ndecision boundaries. On one hand, QML models can be useful in both of these\ntasks. Convolution with parameterized quantum circuits (Quanvolution) can\nextract rich features from the images. On the other hand, quantum neural\nnetwork (QNN) models can create complex decision boundaries. Therefore,\nQuanvolution and QNN can be used to create an end-to-end QML model for image\nclassification. Alternatively, we can extract image features separately using\nclassical dimension reduction techniques such as, Principal Components Analysis\n(PCA) or Convolutional Autoencoder (CAE) and use the extracted features to\ntrain a QNN. We review two proposals on quantum-classical hybrid ML models for\nimage classification namely, Quanvolutional Neural Network and dimension\nreduction using a classical algorithm followed by QNN. Particularly, we make a\ncase for trainable filters in Quanvolution and CAE-based feature extraction for\nimage datasets (instead of dimension reduction using linear transformations\nsuch as, PCA). We discuss various design choices, potential opportunities, and\ndrawbacks of these models. We also release a Python-based framework to create\nand explore these hybrid models with a variety of design choices.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alam_M/0/1/0/all/0/1\">Mahabubul Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kundu_S/0/1/0/all/0/1\">Satwik Kundu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Topaloglu_R/0/1/0/all/0/1\">Rasit Onur Topaloglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Swaroop Ghosh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Journalistic Guidelines Aware News Image Captioning. (arXiv:2109.02865v1 [cs.CV])","link":"http://arxiv.org/abs/2109.02865","description":"<p>The task of news article image captioning aims to generate descriptive and\ninformative captions for news article images. Unlike conventional image\ncaptions that simply describe the content of the image in general terms, news\nimage captions follow journalistic guidelines and rely heavily on named\nentities to describe the image content, often drawing context from the whole\narticle they are associated with. In this work, we propose a new approach to\nthis task, motivated by caption guidelines that journalists follow. Our\napproach, Journalistic Guidelines Aware News Image Captioning (JoGANIC),\nleverages the structure of captions to improve the generation quality and guide\nour representation design. Experimental results, including detailed ablation\nstudies, on two large-scale publicly available datasets show that JoGANIC\nsubstantially outperforms state-of-the-art methods both on caption generation\nand named entity related metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xuewen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karaman_S/0/1/0/all/0/1\">Svebor Karaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tetreault_J/0/1/0/all/0/1\">Joel Tetreault</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaimes_A/0/1/0/all/0/1\">Alex Jaimes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepFakes: Detecting Forged and Synthetic Media Content Using Machine Learning. (arXiv:2109.02874v1 [cs.CV])","link":"http://arxiv.org/abs/2109.02874","description":"<p>The rapid advancement in deep learning makes the differentiation of authentic\nand manipulated facial images and video clips unprecedentedly harder. The\nunderlying technology of manipulating facial appearances through deep\ngenerative approaches, enunciated as DeepFake that have emerged recently by\npromoting a vast number of malicious face manipulation applications.\nSubsequently, the need of other sort of techniques that can assess the\nintegrity of digital visual content is indisputable to reduce the impact of the\ncreations of DeepFake. A large body of research that are performed on DeepFake\ncreation and detection create a scope of pushing each other beyond the current\nstatus. This study presents challenges, research trends, and directions related\nto DeepFake creation and detection techniques by reviewing the notable research\nin the DeepFake domain to facilitate the development of more robust approaches\nthat could deal with the more advance DeepFake in the future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zobaed_S/0/1/0/all/0/1\">Sm Zobaed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rabby_M/0/1/0/all/0/1\">Md Fazle Rabby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hossain_M/0/1/0/all/0/1\">Md Istiaq Hossain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hossain_E/0/1/0/all/0/1\">Ekram Hossain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_S/0/1/0/all/0/1\">Sazib Hasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karim_A/0/1/0/all/0/1\">Asif Karim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasib_K/0/1/0/all/0/1\">Khan Md. Hasib</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-grained Hand Gesture Recognition in Multi-viewpoint Hand Hygiene. (arXiv:2109.02917v1 [cs.CV])","link":"http://arxiv.org/abs/2109.02917","description":"<p>This paper contributes a new high-quality dataset for hand gesture\nrecognition in hand hygiene systems, named \"MFH\". Generally, current datasets\nare not focused on: (i) fine-grained actions; and (ii) data mismatch between\ndifferent viewpoints, which are available under realistic settings. To address\nthe aforementioned issues, the MFH dataset is proposed to contain a total of\n731147 samples obtained by different camera views in 6 non-overlapping\nlocations. Additionally, each sample belongs to one of seven steps introduced\nby the World Health Organization (WHO). As a minor contribution, inspired by\nadvances in fine-grained image recognition and distribution adaptation, this\npaper recommends using the self-supervised learning method to handle these\npreceding problems. The extensive experiments on the benchmarking MFH dataset\nshow that the introduced method yields competitive performance in both the\nAccuracy and the Macro F1-score. The code and the MFH dataset are available at\nhttps://github.com/willogy-team/hand-gesture-recognition-smc2021.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vo_H/0/1/0/all/0/1\">Huy Q.Vo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Do_T/0/1/0/all/0/1\">Tuong Do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pham_V/0/1/0/all/0/1\">Vi C.Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Duy Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duong_A/0/1/0/all/0/1\">An T.Duong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_Q/0/1/0/all/0/1\">Quang D.Tran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FDA: Feature Decomposition and Aggregation for Robust Airway Segmentation. (arXiv:2109.02920v1 [eess.IV])","link":"http://arxiv.org/abs/2109.02920","description":"<p>3D Convolutional Neural Networks (CNNs) have been widely adopted for airway\nsegmentation. The performance of 3D CNNs is greatly influenced by the dataset\nwhile the public airway datasets are mainly clean CT scans with coarse\nannotation, thus difficult to be generalized to noisy CT scans (e.g. COVID-19\nCT scans). In this work, we proposed a new dual-stream network to address the\nvariability between the clean domain and noisy domain, which utilizes the clean\nCT scans and a small amount of labeled noisy CT scans for airway segmentation.\nWe designed two different encoders to extract the transferable clean features\nand the unique noisy features separately, followed by two independent decoders.\nFurther on, the transferable features are refined by the channel-wise feature\nrecalibration and Signed Distance Map (SDM) regression. The feature\nrecalibration module emphasizes critical features and the SDM pays more\nattention to the bronchi, which is beneficial to extracting the transferable\ntopological features robust to the coarse labels. Extensive experimental\nresults demonstrated the obvious improvement brought by our proposed method.\nCompared to other state-of-the-art transfer learning methods, our method\naccurately segmented more bronchi in the noisy CT scans.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_M/0/1/0/all/0/1\">Minghui Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_X/0/1/0/all/0/1\">Xin Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1\">Hanxiao Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_H/0/1/0/all/0/1\">Hao Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_W/0/1/0/all/0/1\">Weihao Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pan_H/0/1/0/all/0/1\">Hong Pan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cai_X/0/1/0/all/0/1\">Xiangran Cai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gu_Y/0/1/0/all/0/1\">Yun Gu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Combine the Modalities of Language and Video for Temporal Moment Localization. (arXiv:2109.02925v1 [cs.CV])","link":"http://arxiv.org/abs/2109.02925","description":"<p>Temporal moment localization aims to retrieve the best video segment matching\na moment specified by a query. The existing methods generate the visual and\nsemantic embeddings independently and fuse them without full consideration of\nthe long-term temporal relationship between them. To address these\nshortcomings, we introduce a novel recurrent unit, cross-modal long short-term\nmemory (CM-LSTM), by mimicking the human cognitive process of localizing\ntemporal moments that focuses on the part of a video segment related to the\npart of a query, and accumulates the contextual information across the entire\nvideo recurrently. In addition, we devise a two-stream attention mechanism for\nboth attended and unattended video features by the input query to prevent\nnecessary visual information from being neglected. To obtain more precise\nboundaries, we propose a two-stream attentive cross-modal interaction network\n(TACI) that generates two 2D proposal maps obtained globally from the\nintegrated contextual features, which are generated by using CM-LSTM, and\nlocally from boundary score sequences and then combines them into a final 2D\nmap in an end-to-end manner. On the TML benchmark dataset,\nActivityNet-Captions, the TACI outperform state-of-the-art TML methods with R@1\nof 45.50% and 27.23% for IoU@0.5 and IoU@0.7, respectively. In addition, we\nshow that the revised state-of-the-arts methods by replacing the original LSTM\nwith our CM-LSTM achieve performance gains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1\">Jungkyoo Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moon_J/0/1/0/all/0/1\">Jinyoung Moon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Brand Label Albedo Extraction of eCommerce Products using Generative Adversarial Network. (arXiv:2109.02929v1 [cs.CV])","link":"http://arxiv.org/abs/2109.02929","description":"<p>In this paper we present our solution to extract albedo of branded labels for\ne-commerce products. To this end, we generate a large-scale photo-realistic\nsynthetic data set for albedo extraction followed by training a generative\nmodel to translate images with diverse lighting conditions to albedo. We\nperformed an extensive evaluation to test the generalisation of our method to\nin-the-wild images. From the experimental results, we observe that our solution\ngeneralises well compared to the existing method both in the unseen rendered\nimages as well as in the wild image.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sapkota_S/0/1/0/all/0/1\">Suman Sapkota</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Juneja_M/0/1/0/all/0/1\">Manish Juneja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keleras_L/0/1/0/all/0/1\">Laurynas Keleras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kotwal_P/0/1/0/all/0/1\">Pranav Kotwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattarai_B/0/1/0/all/0/1\">Binod Bhattarai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fishr: Invariant Gradient Variances for Out-of-distribution Generalization. (arXiv:2109.02934v1 [cs.LG])","link":"http://arxiv.org/abs/2109.02934","description":"<p>Learning robust models that generalize well under changes in the data\ndistribution is critical for real-world applications. To this end, there has\nbeen a growing surge of interest to learn simultaneously from multiple training\ndomains - while enforcing different types of invariance across those domains.\nYet, all existing approaches fail to show systematic benefits under fair\nevaluation protocols. In this paper, we propose a new learning scheme to\nenforce domain invariance in the space of the gradients of the loss function:\nspecifically, we introduce a regularization term that matches the domain-level\nvariances of gradients across training domains. Critically, our strategy, named\nFishr, exhibits close relations with the Fisher Information and the Hessian of\nthe loss. We show that forcing domain-level gradient covariances to be similar\nduring the learning procedure eventually aligns the domain-level loss\nlandscapes locally around the final weights. Extensive experiments demonstrate\nthe effectiveness of Fishr for out-of-distribution generalization. In\nparticular, Fishr improves the state of the art on the DomainBed benchmark and\nperforms significantly better than Empirical Risk Minimization. The code is\nreleased at https://github.com/alexrame/fishr.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rame_A/0/1/0/all/0/1\">Alexandre Rame</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dancette_C/0/1/0/all/0/1\">Corentin Dancette</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cord_M/0/1/0/all/0/1\">Matthieu Cord</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sensor-Augmented Egocentric-Video Captioning with Dynamic Modal Attention. (arXiv:2109.02955v1 [cs.CV])","link":"http://arxiv.org/abs/2109.02955","description":"<p>Automatically describing video, or video captioning, has been widely studied\nin the multimedia field. This paper proposes a new task of sensor-augmented\negocentric-video captioning, a newly constructed dataset for it called MMAC\nCaptions, and a method for the newly proposed task that effectively utilizes\nmulti-modal data of video and motion sensors, or inertial measurement units\n(IMUs). While conventional video captioning tasks have difficulty in dealing\nwith detailed descriptions of human activities due to the limited view of a\nfixed camera, egocentric vision has greater potential to be used for generating\nthe finer-grained descriptions of human activities on the basis of a much\ncloser view. In addition, we utilize wearable-sensor data as auxiliary\ninformation to mitigate the inherent problems in egocentric vision: motion\nblur, self-occlusion, and out-of-camera-range activities. We propose a method\nfor effectively utilizing the sensor data in combination with the video data on\nthe basis of an attention mechanism that dynamically determines the modality\nthat requires more attention, taking the contextual information into account.\nWe compared the proposed sensor-fusion method with strong baselines on the MMAC\nCaptions dataset and found that using sensor data as supplementary information\nto the egocentric-video data was beneficial, and that our proposed method\noutperformed the strong baselines, demonstrating the effectiveness of the\nproposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nakamura_K/0/1/0/all/0/1\">Katsuyuki Nakamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ohashi_H/0/1/0/all/0/1\">Hiroki Ohashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okada_M/0/1/0/all/0/1\">Mitsuhiro Okada</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CovarianceNet: Conditional Generative Model for Correct Covariance Prediction in Human Motion Prediction. (arXiv:2109.02965v1 [cs.CV])","link":"http://arxiv.org/abs/2109.02965","description":"<p>The correct characterization of uncertainty when predicting human motion is\nequally important as the accuracy of this prediction. We present a new method\nto correctly predict the uncertainty associated with the predicted distribution\nof future trajectories. Our approach, CovariaceNet, is based on a Conditional\nGenerative Model with Gaussian latent variables in order to predict the\nparameters of a bi-variate Gaussian distribution. The combination of\nCovarianceNet with a motion prediction model results in a hybrid approach that\noutputs a uni-modal distribution. We will show how some state of the art\nmethods in motion prediction become overconfident when predicting uncertainty,\naccording to our proposed metric and validated in the ETH data-set\n\\cite{pellegrini2009you}. CovarianceNet correctly predicts uncertainty, which\nmakes our method suitable for applications that use predicted distributions,\ne.g., planning or decision making.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Postnikov_A/0/1/0/all/0/1\">Aleksey Postnikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gamayunov_A/0/1/0/all/0/1\">Aleksander Gamayunov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrer_G/0/1/0/all/0/1\">Gonzalo Ferrer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient ADMM-based Algorithms for Convolutional Sparse Coding. (arXiv:2109.02969v1 [cs.LG])","link":"http://arxiv.org/abs/2109.02969","description":"<p>Convolutional sparse coding improves on the standard sparse approximation by\nincorporating a global shift-invariant model. The most efficient convolutional\nsparse coding methods are based on the alternating direction method of\nmultipliers and the convolution theorem. The only major difference between\nthese methods is how they approach a convolutional least-squares fitting\nsubproblem. This letter presents a solution to this subproblem, which improves\nthe efficiency of the state-of-the-art algorithms. We also use the same\napproach for developing an efficient convolutional dictionary learning method.\nFurthermore, we propose a novel algorithm for convolutional sparse coding with\na constraint on the approximation error.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Veshki_F/0/1/0/all/0/1\">Farshad G. Veshki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vorobyov_S/0/1/0/all/0/1\">Sergiy A. Vorobyov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unpaired Adversarial Learning for Single Image Deraining with Rain-Space Contrastive Constraints. (arXiv:2109.02973v1 [cs.CV])","link":"http://arxiv.org/abs/2109.02973","description":"<p>Deep learning-based single image deraining (SID) with unpaired information is\nof immense importance, as relying on paired synthetic data often limits their\ngenerality and scalability in real-world applications. However, we noticed that\ndirect employ of unpaired adversarial learning and cycle-consistency\nconstraints in the SID task is insufficient to learn the underlying\nrelationship from rainy input to clean outputs, since the domain knowledge\nbetween rainy and rain-free images is asymmetrical. To address such limitation,\nwe develop an effective unpaired SID method which explores mutual properties of\nthe unpaired exemplars by a contrastive learning manner in a GAN framework,\nnamed as CDR-GAN. The proposed method mainly consists of two cooperative\nbranches: Bidirectional Translation Branch (BTB) and Contrastive Guidance\nBranch (CGB). Specifically, BTB takes full advantage of the circulatory\narchitecture of adversarial consistency to exploit latent feature distributions\nand guide transfer ability between two domains by equipping it with\nbidirectional mapping. Simultaneously, CGB implicitly constrains the embeddings\nof different exemplars in rain space by encouraging the similar feature\ndistributions closer while pushing the dissimilar further away, in order to\nbetter help rain removal and image restoration. During training, we explore\nseveral loss functions to further constrain the proposed CDR-GAN. Extensive\nexperiments show that our method performs favorably against existing unpaired\nderaining approaches on both synthetic and real-world datasets, even\noutperforms several fully-supervised or semi-supervised models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Jinshan Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_K/0/1/0/all/0/1\">Kui Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yufeng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_C/0/1/0/all/0/1\">Caihua Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_L/0/1/0/all/0/1\">Longgang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yufeng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FuseFormer: Fusing Fine-Grained Information in Transformers for Video Inpainting. (arXiv:2109.02974v1 [cs.CV])","link":"http://arxiv.org/abs/2109.02974","description":"<p>Transformer, as a strong and flexible architecture for modelling long-range\nrelations, has been widely explored in vision tasks. However, when used in\nvideo inpainting that requires fine-grained representation, existed method\nstill suffers from yielding blurry edges in detail due to the hard patch\nsplitting. Here we aim to tackle this problem by proposing FuseFormer, a\nTransformer model designed for video inpainting via fine-grained feature fusion\nbased on novel Soft Split and Soft Composition operations. The soft split\ndivides feature map into many patches with given overlapping interval. On the\ncontrary, the soft composition operates by stitching different patches into a\nwhole feature map where pixels in overlapping regions are summed up. These two\nmodules are first used in tokenization before Transformer layers and\nde-tokenization after Transformer layers, for effective mapping between tokens\nand features. Therefore, sub-patch level information interaction is enabled for\nmore effective feature propagation between neighboring patches, resulting in\nsynthesizing vivid content for hole regions in videos. Moreover, in FuseFormer,\nwe elaborately insert the soft composition and soft split into the feed-forward\nnetwork, enabling the 1D linear layers to have the capability of modelling 2D\nstructure. And, the sub-patch level feature fusion ability is further enhanced.\nIn both quantitative and qualitative evaluations, our proposed FuseFormer\nsurpasses state-of-the-art methods. We also conduct detailed analysis to\nexamine its superiority.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Rui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_H/0/1/0/all/0/1\">Hanming Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yangyi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xiaoyu Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1\">Lewei Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Wenxiu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaogang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1\">Jifeng Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Grassmannian Graph-attentional Landmark Selection for Domain Adaptation. (arXiv:2109.02990v1 [cs.CV])","link":"http://arxiv.org/abs/2109.02990","description":"<p>Domain adaptation aims to leverage information from the source domain to\nimprove the classification performance in the target domain. It mainly utilizes\ntwo schemes: sample reweighting and feature matching. While the first scheme\nallocates different weights to individual samples, the second scheme matches\nthe feature of two domains using global structural statistics. The two schemes\nare complementary with each other, which are expected to jointly work for\nrobust domain adaptation. Several methods combine the two schemes, but the\nunderlying relationship of samples is insufficiently analyzed due to the\nneglect of the hierarchy of samples and the geometric properties between\nsamples. To better combine the advantages of the two schemes, we propose a\nGrassmannian graph-attentional landmark selection (GGLS) framework for domain\nadaptation. GGLS presents a landmark selection scheme using attention-induced\nneighbors of the graphical structure of samples and performs distribution\nadaptation and knowledge adaptation over Grassmann manifold. the former treats\nthe landmarks of each sample differently, and the latter avoids feature\ndistortion and achieves better geometric properties. Experimental results on\ndifferent real-world cross-domain visual recognition tasks demonstrate that\nGGLS provides better classification accuracies compared with state-of-the-art\ndomain adaptation methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1\">Bin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shaofan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_D/0/1/0/all/0/1\">Dehui Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinghua Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1\">Baocai Yin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluation of an Audio-Video Multimodal Deepfake Dataset using Unimodal and Multimodal Detectors. (arXiv:2109.02993v1 [cs.CV])","link":"http://arxiv.org/abs/2109.02993","description":"<p>Significant advancements made in the generation of deepfakes have caused\nsecurity and privacy issues. Attackers can easily impersonate a person's\nidentity in an image by replacing his face with the target person's face.\nMoreover, a new domain of cloning human voices using deep-learning technologies\nis also emerging. Now, an attacker can generate realistic cloned voices of\nhumans using only a few seconds of audio of the target person. With the\nemerging threat of potential harm deepfakes can cause, researchers have\nproposed deepfake detection methods. However, they only focus on detecting a\nsingle modality, i.e., either video or audio. On the other hand, to develop a\ngood deepfake detector that can cope with the recent advancements in deepfake\ngeneration, we need to have a detector that can detect deepfakes of multiple\nmodalities, i.e., videos and audios. To build such a detector, we need a\ndataset that contains video and respective audio deepfakes. We were able to\nfind a most recent deepfake dataset, Audio-Video Multimodal Deepfake Detection\nDataset (FakeAVCeleb), that contains not only deepfake videos but synthesized\nfake audios as well. We used this multimodal deepfake dataset and performed\ndetailed baseline experiments using state-of-the-art unimodal, ensemble-based,\nand multimodal detection methods to evaluate it. We conclude through detailed\nexperimentation that unimodals, addressing only a single modality, video or\naudio, do not perform well compared to ensemble-based methods. Whereas purely\nmultimodal-based baselines provide the worst performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khalid_H/0/1/0/all/0/1\">Hasam Khalid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Minha Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tariq_S/0/1/0/all/0/1\">Shahroz Tariq</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woo_S/0/1/0/all/0/1\">Simon S. Woo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Statistical analysis of locally parameterized shapes. (arXiv:2109.03027v1 [stat.ME])","link":"http://arxiv.org/abs/2109.03027","description":"<p>The alignment of shapes has been a crucial step in statistical shape\nanalysis, for example, in calculating mean shape, detecting locational\ndifferences between two shape populations, and classification. Procrustes\nalignment is the most commonly used method and state of the art. In this work,\nwe uncover that alignment might seriously affect the statistical analysis. For\nexample, alignment can induce false shape differences and lead to misleading\nresults and interpretations. We propose a novel hierarchical shape\nparameterization based on local coordinate systems. The local parameterized\nshapes are translation and rotation invariant. Thus, the inherent alignment\nproblems from the commonly used global coordinate system for shape\nrepresentation can be avoided using this parameterization. The new\nparameterization is also superior for shape deformation and simulation. The\nmethod's power is demonstrated on the hypothesis testing of simulated data as\nwell as the left hippocampi of patients with Parkinson's disease and controls.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Taheri_M/0/1/0/all/0/1\">Mohsen Taheri</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Schulz_J/0/1/0/all/0/1\">J&#xf6;rn Schulz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Distillation Using Hierarchical Self-Supervision Augmented Distribution. (arXiv:2109.03075v1 [cs.CV])","link":"http://arxiv.org/abs/2109.03075","description":"<p>Knowledge distillation (KD) is an effective framework that aims to transfer\nmeaningful information from a large teacher to a smaller student. Generally, KD\noften involves how to define and transfer knowledge. Previous KD methods often\nfocus on mining various forms of knowledge, for example, feature maps and\nrefined information. However, the knowledge is derived from the primary\nsupervised task and thus is highly task-specific. Motivated by the recent\nsuccess of self-supervised representation learning, we propose an auxiliary\nself-supervision augmented task to guide networks to learn more meaningful\nfeatures. Therefore, we can derive soft self-supervision augmented\ndistributions as richer dark knowledge from this task for KD. Unlike previous\nknowledge, this distribution encodes joint knowledge from supervised and\nself-supervised feature learning. Beyond knowledge exploration, another crucial\naspect is how to learn and distill our proposed knowledge effectively. To fully\ntake advantage of hierarchical feature maps, we propose to append several\nauxiliary branches at various hidden layers. Each auxiliary branch is guided to\nlearn self-supervision augmented task and distill this distribution from\nteacher to student. Thus we call our KD method as Hierarchical Self-Supervision\nAugmented Knowledge Distillation (HSSAKD). Experiments on standard image\nclassification show that both offline and online HSSAKD achieves\nstate-of-the-art performance in the field of KD. Further transfer experiments\non object detection further verify that HSSAKD can guide the network to learn\nbetter features, which can be attributed to learn and distill an auxiliary\nself-supervision augmented task effectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chuanguang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_Z/0/1/0/all/0/1\">Zhulin An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_L/0/1/0/all/0/1\">Linhang Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yongjun Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Support Vector Machine for Handwritten Character Recognition. (arXiv:2109.03081v1 [cs.CV])","link":"http://arxiv.org/abs/2109.03081","description":"<p>Handwriting recognition has been one of the most fascinating and challenging\nresearch areas in field of image processing and pattern recognition. It\ncontributes enormously to the improvement of automation process. In this paper,\na system for recognition of unconstrained handwritten Malayalam characters is\nproposed. A database of 10,000 character samples of 44 basic Malayalam\ncharacters is used in this work. A discriminate feature set of 64 local and 4\nglobal features are used to train and test SVM classifier and achieved 92.24%\naccuracy\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+John_J/0/1/0/all/0/1\">Jomy John</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Perceptual Video Compression with Recurrent Conditional GAN. (arXiv:2109.03082v1 [eess.IV])","link":"http://arxiv.org/abs/2109.03082","description":"<p>This paper proposes a Perceptual Learned Video Compression (PLVC) approach\nwith recurrent conditional generative adversarial network. In our approach, the\nrecurrent auto-encoder-based generator learns to fully explore the temporal\ncorrelation for compressing video. More importantly, we propose a recurrent\nconditional discriminator, which judges raw and compressed video conditioned on\nboth spatial and temporal information, including the latent representation,\ntemporal motion and hidden states in recurrent cells. This way, in the\nadversarial training, it pushes the generated video to be not only spatially\nphoto-realistic but also temporally consistent with groundtruth and coherent\namong video frames. Therefore, the proposed PLVC model learns to compress video\ntowards good perceptual quality at low bit-rate. The experimental results show\nthat our PLVC approach outperforms the previous traditional and learned\napproaches on several perceptual quality metrics. The user study further\nvalidates the outstanding perceptual performance of PLVC in comparison with the\nlatest learned video compression approaches and the official HEVC test model\n(HM 16.20). The codes will be released at https://github.com/RenYang-home/PLVC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yang_R/0/1/0/all/0/1\">Ren Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Timofte_R/0/1/0/all/0/1\">Radu Timofte</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Phenotype Prediction using Long-Range Spatio-Temporal Dynamics of Functional Connectivity. (arXiv:2109.03115v1 [q-bio.NC])","link":"http://arxiv.org/abs/2109.03115","description":"<p>The study of functional brain connectivity (FC) is important for\nunderstanding the underlying mechanisms of many psychiatric disorders. Many\nrecent analyses adopt graph convolutional networks, to study non-linear\ninteractions between functionally-correlated states. However, although patterns\nof brain activation are known to be hierarchically organised in both space and\ntime, many methods have failed to extract powerful spatio-temporal features. To\novercome those challenges, and improve understanding of long-range functional\ndynamics, we translate an approach, from the domain of skeleton-based action\nrecognition, designed to model interactions across space and time. We evaluate\nthis approach using the Human Connectome Project (HCP) dataset on sex\nclassification and fluid intelligence prediction. To account for subject\ntopographic variability of functional organisation, we modelled functional\nconnectomes using multi-resolution dual-regressed (subject-specific) ICA nodes.\nResults show a prediction accuracy of 94.4% for sex classification (an increase\nof 6.2% compared to other methods), and an improvement of correlation with\nfluid intelligence of 0.325 vs 0.144, relative to a baseline model that encodes\nspace and time separately. Results suggest that explicit encoding of\nspatio-temporal dynamics of brain functional activity may improve the precision\nwith which behavioural and cognitive phenotypes may be predicted in the future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Dahan_S/0/1/0/all/0/1\">Simon Dahan</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Williams_L/0/1/0/all/0/1\">Logan Z. J. Williams</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Rueckert_D/0/1/0/all/0/1\">Daniel Rueckert</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Robinson_E/0/1/0/all/0/1\">Emma C. Robinson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Smart Traffic Monitoring System using Computer Vision and Edge Computing. (arXiv:2109.03141v1 [cs.CV])","link":"http://arxiv.org/abs/2109.03141","description":"<p>Traffic management systems capture tremendous video data and leverage\nadvances in video processing to detect and monitor traffic incidents. The\ncollected data are traditionally forwarded to the traffic management center\n(TMC) for in-depth analysis and may thus exacerbate the network paths to the\nTMC. To alleviate such bottlenecks, we propose to utilize edge computing by\nequipping edge nodes that are close to cameras with computing resources (e.g.\ncloudlets). A cloudlet, with limited computing resources as compared to TMC,\nprovides limited video processing capabilities. In this paper, we focus on two\ncommon traffic monitoring tasks, congestion detection, and speed detection, and\npropose a two-tier edge computing based model that takes into account of both\nthe limited computing capability in cloudlets and the unstable network\ncondition to the TMC. Our solution utilizes two algorithms for each task, one\nimplemented at the edge and the other one at the TMC, which are designed with\nthe consideration of different computing resources. While the TMC provides\nstrong computation power, the video quality it receives depends on the\nunderlying network conditions. On the other hand, the edge processes very\nhigh-quality video but with limited computing resources. Our model captures\nthis trade-off. We evaluate the performance of the proposed two-tier model as\nwell as the traffic monitoring algorithms via test-bed experiments under\ndifferent weather as well as network conditions and show that our proposed\nhybrid edge-cloud solution outperforms both the cloud-only and edge-only\nsolutions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Guanxiong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Hang Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiani_A/0/1/0/all/0/1\">Abbas Kiani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khreishah_A/0/1/0/all/0/1\">Abdallah Khreishah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jo Young Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ansari_N/0/1/0/all/0/1\">Nirwan Ansari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chengjun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yousef_M/0/1/0/all/0/1\">Mustafa Yousef</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PP-OCRv2: Bag of Tricks for Ultra Lightweight OCR System. (arXiv:2109.03144v1 [cs.CV])","link":"http://arxiv.org/abs/2109.03144","description":"<p>Optical Character Recognition (OCR) systems have been widely used in various\nof application scenarios. Designing an OCR system is still a challenging task.\nIn previous work, we proposed a practical ultra lightweight OCR system (PP-OCR)\nto balance the accuracy against the efficiency. In order to improve the\naccuracy of PP-OCR and keep high efficiency, in this paper, we propose a more\nrobust OCR system, i.e. PP-OCRv2. We introduce bag of tricks to train a better\ntext detector and a better text recognizer, which include Collaborative Mutual\nLearning (CML), CopyPaste, Lightweight CPUNetwork (LCNet), Unified-Deep Mutual\nLearning (U-DML) and Enhanced CTCLoss. Experiments on real data show that the\nprecision of PP-OCRv2 is 7% higher than PP-OCR under the same inference cost.\nIt is also comparable to the server models of the PP-OCR which uses ResNet\nseries as backbones. All of the above mentioned models are open-sourced and the\ncode is available in the GitHub repository PaddleOCR which is powered by\nPaddlePaddle.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yuning Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chenxia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_R/0/1/0/all/0/1\">Ruoyu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_C/0/1/0/all/0/1\">Cheng Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weiwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_B/0/1/0/all/0/1\">Bin Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yehua Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qiwen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaoguang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dianhai Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yanjun Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fair Comparison: Quantifying Variance in Resultsfor Fine-grained Visual Categorization. (arXiv:2109.03156v1 [cs.CV])","link":"http://arxiv.org/abs/2109.03156","description":"<p>For the task of image classification, researchers work arduously to develop\nthe next state-of-the-art (SOTA) model, each bench-marking their own\nperformance against that of their predecessors and of their peers.\nUnfortunately, the metric used most frequently to describe a model's\nperformance, average categorization accuracy, is often used in isolation. As\nthe number of classes increases, such as in fine-grained visual categorization\n(FGVC), the amount of information conveyed by average accuracy alone dwindles.\nWhile its most glaring weakness is its failure to describe the model's\nperformance on a class-by-class basis, average accuracy also fails to describe\nhow performance may vary from one trained model of the same architecture, on\nthe same dataset, to another (both averaged across all categories and at the\nper-class level). We first demonstrate the magnitude of these variations across\nmodels and across class distributions based on attributes of the data,\ncomparing results on different visual domains and different per-class image\ndistributions, including long-tailed distributions and few-shot subsets. We\nthen analyze the impact various FGVC methods have on overall and per-class\nvariance. From this analysis, we both highlight the importance of reporting and\ncomparing methods based on information beyond overall accuracy, as well as\npoint out techniques that mitigate variance in FGVC results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gwilliam_M/0/1/0/all/0/1\">Matthew Gwilliam</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Teuscher_A/0/1/0/all/0/1\">Adam Teuscher</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Anderson_C/0/1/0/all/0/1\">Connor Anderson</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Farrell_R/0/1/0/all/0/1\">Ryan Farrell</a> (1) ((1) Brigham Young University, (2) University of Maryland)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"nnFormer: Interleaved Transformer for Volumetric Segmentation. (arXiv:2109.03201v1 [cs.CV])","link":"http://arxiv.org/abs/2109.03201","description":"<p>Transformers, the default model of choices in natural language processing,\nhave drawn scant attention from the medical imaging community. Given the\nability to exploit long-term dependencies, transformers are promising to help\natypical convolutional neural networks (convnets) to overcome its inherent\nshortcomings of spatial inductive bias. However, most of recently proposed\ntransformer-based segmentation approaches simply treated transformers as\nassisted modules to help encode global context into convolutional\nrepresentations without investigating how to optimally combine self-attention\n(i.e., the core of transformers) with convolution. To address this issue, in\nthis paper, we introduce nnFormer (i.e., Not-aNother transFormer), a powerful\nsegmentation model with an interleaved architecture based on empirical\ncombination of self-attention and convolution. In practice, nnFormer learns\nvolumetric representations from 3D local volumes. Compared to the naive\nvoxel-level self-attention implementation, such volume-based operations help to\nreduce the computational complexity by approximate 98% and 99.5% on Synapse and\nACDC datasets, respectively. In comparison to prior-art network configurations,\nnnFormer achieves tremendous improvements over previous transformer-based\nmethods on two commonly used datasets Synapse and ACDC. For instance, nnFormer\noutperforms Swin-UNet by over 7 percents on Synapse. Even when compared to\nnnUNet, currently the best performing fully-convolutional medical segmentation\nnetwork, nnFormer still provides slightly better performance on Synapse and\nACDC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hong-Yu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jiansen Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yinghao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Lequan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liansheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yizhou Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Fast Sample Re-weighting Without Reward Data. (arXiv:2109.03216v1 [cs.LG])","link":"http://arxiv.org/abs/2109.03216","description":"<p>Training sample re-weighting is an effective approach for tackling data\nbiases such as imbalanced and corrupted labels. Recent methods develop\nlearning-based algorithms to learn sample re-weighting strategies jointly with\nmodel training based on the frameworks of reinforcement learning and meta\nlearning. However, depending on additional unbiased reward data is limiting\ntheir general applicability. Furthermore, existing learning-based sample\nre-weighting methods require nested optimizations of models and weighting\nparameters, which requires expensive second-order computation. This paper\naddresses these two problems and presents a novel learning-based fast sample\nre-weighting (FSR) method that does not require additional reward data. The\nmethod is based on two key ideas: learning from history to build proxy reward\ndata and feature sharing to reduce the optimization cost. Our experiments show\nthe proposed method achieves competitive results compared to state of the arts\non label noise robustness and long-tailed recognition, and does so while\nachieving significantly improved training efficiency. The source code is\npublicly available at\nhttps://github.com/google-research/google-research/tree/master/ieg.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zizhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfister_T/0/1/0/all/0/1\">Tomas Pfister</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rendezvous: Attention Mechanisms for the Recognition of Surgical Action Triplets in Endoscopic Videos. (arXiv:2109.03223v1 [cs.CV])","link":"http://arxiv.org/abs/2109.03223","description":"<p>Out of all existing frameworks for surgical workflow analysis in endoscopic\nvideos, action triplet recognition stands out as the only one aiming to provide\ntruly fine-grained and comprehensive information on surgical activities. This\ninformation, presented as &lt;instrument, verb, target&gt; combinations, is highly\nchallenging to be accurately identified. Triplet components can be difficult to\nrecognize individually; in this task, it requires not only performing\nrecognition simultaneously for all three triplet components, but also correctly\nestablishing the data association between them. To achieve this task, we\nintroduce our new model, the Rendezvous (RDV), which recognizes triplets\ndirectly from surgical videos by leveraging attention at two different levels.\nWe first introduce a new form of spatial attention to capture individual action\ntriplet components in a scene; called the Class Activation Guided Attention\nMechanism (CAGAM). This technique focuses on the recognition of verbs and\ntargets using activations resulting from instruments. To solve the association\nproblem, our RDV model adds a new form of semantic attention inspired by\nTransformer networks. Using multiple heads of cross and self attentions, RDV is\nable to effectively capture relationships between instruments, verbs, and\ntargets. We also introduce CholecT50 - a dataset of 50 endoscopic videos in\nwhich every frame has been annotated with labels from 100 triplet classes. Our\nproposed RDV model significantly improves the triplet prediction mAP by over 9%\ncompared to the state-of-the-art methods on this dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nwoye_C/0/1/0/all/0/1\">Chinedu Innocent Nwoye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_C/0/1/0/all/0/1\">Cristians Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seeliger_B/0/1/0/all/0/1\">Barbara Seeliger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mascagni_P/0/1/0/all/0/1\">Pietro Mascagni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mutter_D/0/1/0/all/0/1\">Didier Mutter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marescaux_J/0/1/0/all/0/1\">Jacques Marescaux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padoy_N/0/1/0/all/0/1\">Nicolas Padoy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Common Assumptions to Mitigate Racial Bias in Face Recognition Datasets. (arXiv:2109.03229v1 [cs.CV])","link":"http://arxiv.org/abs/2109.03229","description":"<p>Many existing works have made great strides towards reducing racial bias in\nface recognition. However, most of these methods attempt to rectify bias that\nmanifests in models during training instead of directly addressing a major\nsource of the bias, the dataset itself. Exceptions to this are\nBUPT-Balancedface/RFW and Fairface, but these works assume that primarily\ntraining on a single race or not racially balancing the dataset are inherently\ndisadvantageous. We demonstrate that these assumptions are not necessarily\nvalid. In our experiments, training on only African faces induced less bias\nthan training on a balanced distribution of faces and distributions skewed to\ninclude more African faces produced more equitable models. We additionally\nnotice that adding more images of existing identities to a dataset in place of\nadding new identities can lead to accuracy boosts across racial categories. Our\ncode is available at\nhttps://github.com/j-alex-hanson/rethinking-race-face-datasets\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gwilliam_M/0/1/0/all/0/1\">Matthew Gwilliam</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Hegde_S/0/1/0/all/0/1\">Srinidhi Hegde</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Tinubu_L/0/1/0/all/0/1\">Lade Tinubu</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Hanson_A/0/1/0/all/0/1\">Alex Hanson</a> (1) ((1) University of Maryland, (2) University of Chicago)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised Tumor Segmentation through Layer Decomposition. (arXiv:2109.03230v1 [cs.CV])","link":"http://arxiv.org/abs/2109.03230","description":"<p>In this paper, we propose a self-supervised approach for tumor segmentation.\nSpecifically, we advocate a zero-shot setting, where models from\nself-supervised learning should be directly applicable for the downstream task,\nwithout using any manual annotations whatsoever. We make the following\ncontributions. First, with careful examination on existing self-supervised\nlearning approaches, we reveal the surprising result that, given suitable data\naugmentation, models trained from scratch in fact achieve comparable\nperformance to those pre-trained with self-supervised learning. Second,\ninspired by the fact that tumors tend to be characterized independently to the\ncontexts, we propose a scalable pipeline for generating synthetic tumor data,\nand train a self-supervised model that minimises the generalisation gap with\nthe downstream task. Third, we conduct extensive ablation studies on different\ndownstream datasets, BraTS2018 for brain tumor segmentation and LiTS2017 for\nliver tumor segmentation. While evaluating the model transferability for tumor\nsegmentation under a low-annotation regime, including an extreme case of\nzero-shot segmentation, the proposed approach demonstrates state-of-the-art\nperformance, substantially outperforming all existing self-supervised\napproaches, and opening up the usage of self-supervised learning in practical\nscenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaoman Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1\">Weidi Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chaoqin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ya Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanfeng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Generation of Dense Non-rigid Optical Flow. (arXiv:1812.01946v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1812.01946","description":"<p>There hardly exists any large-scale datasets with dense optical flow of\nnon-rigid motion from real-world imagery as of today. The reason lies mainly in\nthe required setup to derive ground truth optical flows: a series of images\nwith known camera poses along its trajectory, and an accurate 3D model from a\ntextured scene. Human annotation is not only too tedious for large databases,\nit can simply hardly contribute to accurate optical flow. To circumvent the\nneed for manual annotation, we propose a framework to automatically generate\noptical flow from real-world videos. The method extracts and matches objects\nfrom video frames to compute initial constraints, and applies a deformation\nover the objects of interest to obtain dense optical flow fields. We propose\nseveral ways to augment the optical flow variations. Extensive experimental\nresults show that training on our automatically generated optical flow\noutperforms methods that are trained on rigid synthetic data using FlowNet-S,\nLiteFlowNet, PWC-Net, and RAFT. Datasets and implementation of our optical flow\ngeneration framework are released at https://github.com/lhoangan/arap_flow\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1\">Ho&#xe0;ng-&#xc2;n L&#xea;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nimbhorkar_T/0/1/0/all/0/1\">Tushar Nimbhorkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mensink_T/0/1/0/all/0/1\">Thomas Mensink</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baslamisli_A/0/1/0/all/0/1\">Anil S. Baslamisli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karaoglu_S/0/1/0/all/0/1\">Sezer Karaoglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gevers_T/0/1/0/all/0/1\">Theo Gevers</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-Relaxed Quantization with DropBits: Training Low-Bit Neural Networks via Bit-wise Regularization. (arXiv:1911.12990v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1911.12990","description":"<p>Network quantization, which aims to reduce the bit-lengths of the network\nweights and activations, has emerged as one of the key ingredients to reduce\nthe size of neural networks for their deployments to resource-limited devices.\nIn order to overcome the nature of transforming continuous activations and\nweights to discrete ones, recent study called Relaxed Quantization (RQ)\n[Louizos et al. 2019] successfully employ the popular Gumbel-Softmax that\nallows this transformation with efficient gradient-based optimization. However,\nRQ with this Gumbel-Softmax relaxation still suffers from bias-variance\ntrade-off depending on the temperature parameter of Gumbel-Softmax. To resolve\nthe issue, we propose a novel method, Semi-Relaxed Quantization (SRQ) that uses\nmulti-class straight-through estimator to effectively reduce the bias and\nvariance, along with a new regularization technique, DropBits that replaces\ndropout regularization to randomly drop the bits instead of neurons to further\nreduce the bias of the multi-class straight-through estimator in SRQ. As a\nnatural extension of DropBits, we further introduce the way of learning\nheterogeneous quantization levels to find proper bit-length for each layer\nusing DropBits. We experimentally validate our method on various benchmark\ndatasets and network architectures, and also support the quantized lottery\nticket hypothesis: learning heterogeneous quantization levels outperforms the\ncase using the same but fixed quantization levels from scratch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jung Hyun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yun_J/0/1/0/all/0/1\">Jihun Yun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Sung Ju Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_E/0/1/0/all/0/1\">Eunho Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Backdoor Attacks Against Deep Learning Systems in the Physical World. (arXiv:2006.14580v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2006.14580","description":"<p>Backdoor attacks embed hidden malicious behaviors into deep learning models,\nwhich only activate and cause misclassifications on model inputs containing a\nspecific trigger. Existing works on backdoor attacks and defenses, however,\nmostly focus on digital attacks that use digitally generated patterns as\ntriggers. A critical question remains unanswered: can backdoor attacks succeed\nusing physical objects as triggers, thus making them a credible threat against\ndeep learning systems in the real world? We conduct a detailed empirical study\nto explore this question for facial recognition, a critical deep learning task.\nUsing seven physical objects as triggers, we collect a custom dataset of 3205\nimages of ten volunteers and use it to study the feasibility of physical\nbackdoor attacks under a variety of real-world conditions. Our study reveals\ntwo key findings. First, physical backdoor attacks can be highly successful if\nthey are carefully configured to overcome the constraints imposed by physical\nobjects. In particular, the placement of successful triggers is largely\nconstrained by the target model's dependence on key facial features. Second,\nfour of today's state-of-the-art defenses against (digital) backdoors are\nineffective against physical backdoors, because the use of physical objects\nbreaks core assumptions used to construct these defenses. Our study confirms\nthat (physical) backdoor attacks are not a hypothetical phenomenon but rather\npose a serious real-world threat to critical classification tasks. We need new\nand more robust defenses against backdoors in the physical world.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wenger_E/0/1/0/all/0/1\">Emily Wenger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Passananti_J/0/1/0/all/0/1\">Josephine Passananti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhagoji_A/0/1/0/all/0/1\">Arjun Bhagoji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yuanshun Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Haitao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1\">Ben Y. Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A New Basis for Sparse Principal Component Analysis. (arXiv:2007.00596v2 [stat.ML] UPDATED)","link":"http://arxiv.org/abs/2007.00596","description":"<p>Previous versions of sparse principal component analysis (PCA) have presumed\nthat the eigen-basis (a $p \\times k$ matrix) is approximately sparse. We\npropose a method that presumes the $p \\times k$ matrix becomes approximately\nsparse after a $k \\times k$ rotation. The simplest version of the algorithm\ninitializes with the leading $k$ principal components. Then, the principal\ncomponents are rotated with an $k \\times k$ orthogonal rotation to make them\napproximately sparse. Finally, soft-thresholding is applied to the rotated\nprincipal components. This approach differs from prior approaches because it\nuses an orthogonal rotation to approximate a sparse basis. One consequence is\nthat a sparse component need not to be a leading eigenvector, but rather a\nmixture of them. In this way, we propose a new (rotated) basis for sparse PCA.\nIn addition, our approach avoids \"deflation\" and multiple tuning parameters\nrequired for that. Our sparse PCA framework is versatile; for example, it\nextends naturally to a two-way analysis of a data matrix for simultaneous\ndimensionality reduction of rows and columns. We provide evidence showing that\nfor the same level of sparsity, the proposed sparse PCA method is more stable\nand can explain more variance compared to alternative methods. Through three\napplications -- sparse coding of images, analysis of transcriptome sequencing\ndata, and large-scale clustering of social networks, we demonstrate the modern\nusefulness of sparse PCA in exploring multivariate data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Chen_F/0/1/0/all/0/1\">Fan Chen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Rohe_K/0/1/0/all/0/1\">Karl Rohe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Meta-Semi: A Meta-learning Approach for Semi-supervised Learning. (arXiv:2007.02394v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2007.02394","description":"<p>Deep learning based semi-supervised learning (SSL) algorithms have led to\npromising results in recent years. However, they tend to introduce multiple\ntunable hyper-parameters, making them less practical in real SSL scenarios\nwhere the labeled data is scarce for extensive hyper-parameter search. In this\npaper, we propose a novel meta-learning based SSL algorithm (Meta-Semi) that\nrequires tuning only one additional hyper-parameter, compared with a standard\nsupervised deep learning algorithm, to achieve competitive performance under\nvarious conditions of SSL. We start by defining a meta optimization problem\nthat minimizes the loss on labeled data through dynamically reweighting the\nloss on unlabeled samples, which are associated with soft pseudo labels during\ntraining. As the meta problem is computationally intensive to solve directly,\nwe propose an efficient algorithm to dynamically obtain the approximate\nsolutions. We show theoretically that Meta-Semi converges to the stationary\npoint of the loss function on labeled data under mild conditions. Empirically,\nMeta-Semi outperforms state-of-the-art SSL algorithms significantly on the\nchallenging semi-supervised CIFAR-100 and STL-10 tasks, and achieves\ncompetitive performance on CIFAR-10 and SVHN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yulin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jiayi Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Shiji Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Gao Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Experimental Quantum Generative Adversarial Networks for Image Generation. (arXiv:2010.06201v3 [quant-ph] UPDATED)","link":"http://arxiv.org/abs/2010.06201","description":"<p>Quantum machine learning is expected to be one of the first practical\napplications of near-term quantum devices. Pioneer theoretical works suggest\nthat quantum generative adversarial networks (GANs) may exhibit a potential\nexponential advantage over classical GANs, thus attracting widespread\nattention. However, it remains elusive whether quantum GANs implemented on\nnear-term quantum devices can actually solve real-world learning tasks. Here,\nwe devise a flexible quantum GAN scheme to narrow this knowledge gap, which\ncould accomplish image generation with arbitrarily high-dimensional features,\nand could also take advantage of quantum superposition to train multiple\nexamples in parallel. For the first time, we experimentally achieve the\nlearning and generation of real-world hand-written digit images on a\nsuperconducting quantum processor. Moreover, we utilize a gray-scale bar\ndataset to exhibit the competitive performance between quantum GANs and the\nclassical GANs based on multilayer perceptron and convolutional neural network\narchitectures, respectively, benchmarked by the Fr\\'echet Distance score. Our\nwork provides guidance for developing advanced quantum generative models on\nnear-term quantum devices and opens up an avenue for exploring quantum\nadvantages in various GAN-related learning tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/quant-ph/1/au:+Huang_H/0/1/0/all/0/1\">He-Liang Huang</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Du_Y/0/1/0/all/0/1\">Yuxuan Du</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Gong_M/0/1/0/all/0/1\">Ming Gong</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Zhao_Y/0/1/0/all/0/1\">Youwei Zhao</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Wu_Y/0/1/0/all/0/1\">Yulin Wu</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Wang_C/0/1/0/all/0/1\">Chaoyue Wang</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Li_S/0/1/0/all/0/1\">Shaowei Li</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Liang_F/0/1/0/all/0/1\">Futian Liang</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Lin_J/0/1/0/all/0/1\">Jin Lin</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Xu_Y/0/1/0/all/0/1\">Yu Xu</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Yang_R/0/1/0/all/0/1\">Rui Yang</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Liu_T/0/1/0/all/0/1\">Tongliang Liu</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Hsieh_M/0/1/0/all/0/1\">Min-Hsiu Hsieh</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Deng_H/0/1/0/all/0/1\">Hui Deng</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Rong_H/0/1/0/all/0/1\">Hao Rong</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Peng_C/0/1/0/all/0/1\">Cheng-Zhi Peng</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Lu_C/0/1/0/all/0/1\">Chao-Yang Lu</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Chen_Y/0/1/0/all/0/1\">Yu-Ao Chen</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaobo Zhu</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Pan_J/0/1/0/all/0/1\">Jian-Wei Pan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Infer Shape Programs Using Self Training. (arXiv:2011.13045v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.13045","description":"<p>Inferring programs which generate 2D and 3D shapes is important for reverse\nengineering, editing, and more. Training such inference models is challenging\ndue to the lack of paired (shape, program) data in most domains. A popular\napproach is to pre-train a model on synthetic data and then fine-tune on real\nshapes using slow, unstable reinforcement learning. In this paper, we argue\nthat self-training is a viable alternative for fine-tuning such models.\nSelf-training is a semi-supervised learning paradigm where a model assigns\npseudo-labels to unlabeled data, and then retrains with (data, pseudo-label)\npairs as the new ground truth. We show that for constructive solid geometry and\nassembly-based modeling, self-training outperforms state-of-the-art\nreinforcement learning approaches. Additionally, shape program inference has a\nunique property that circumvents a potential downside of self-training\n(incorrect pseudo-label assignment): inferred programs are executable. For a\ngiven shape from our distribution of interest $\\mathbf{x}^*$ and its predicted\nprogram $\\mathbf{z}$, one can execute $\\mathbf{z}$ to obtain a shape\n$\\mathbf{x}$ and train on $(\\mathbf{z}, \\mathbf{x})$ pairs, rather than\n$(\\mathbf{z}, \\mathbf{x}^*)$ pairs. We term this procedure latent execution\nself training (LEST). We demonstrate that self training infers shape programs\nwith higher shape reconstruction accuracy and converges significantly faster\nthan reinforcement learning approaches, and in some domains, LEST can further\nimprove this performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Walke_H/0/1/0/all/0/1\">Homer Walke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jones_R/0/1/0/all/0/1\">R. Kenny Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ritchie_D/0/1/0/all/0/1\">Daniel Ritchie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Siamese Basis Function Networks for Data-efficient Defect Classification in Technical Domains. (arXiv:2012.01338v8 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.01338","description":"<p>Training deep learning models in technical domains is often accompanied by\nthe challenge that although the task is clear, insufficient data for training\nis available. In this work, we propose a novel approach based on the\ncombination of Siamese networks and radial basis function networks to perform\ndata-efficient classification without pretraining by measuring the distance\nbetween images in semantic space in a data-efficient manner. We develop the\nmodels using three technical datasets, the NEU dataset, the BSD dataset, and\nthe TEX dataset. In addition to the technical domain, we show the general\napplicability to classical datasets (cifar10 and MNIST) as well. The approach\nis tested against state-of-the-art models (Resnet50 and Resnet101) by stepwise\nreduction of the number of samples available for training. The authors show\nthat the proposed approach outperforms the state-of-the-art models in the low\ndata regime.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schlagenhauf_T/0/1/0/all/0/1\">Tobias Schlagenhauf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yildirim_F/0/1/0/all/0/1\">Faruk Yildirim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruckner_B/0/1/0/all/0/1\">Benedikt Br&#xfc;ckner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fleischer_J/0/1/0/all/0/1\">J&#xfc;rgen Fleischer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DS-Net: Dynamic Spatiotemporal Network for Video Salient Object Detection. (arXiv:2012.04886v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.04886","description":"<p>As moving objects always draw more attention of human eyes, the temporal\nmotive information is always exploited complementarily with spatial information\nto detect salient objects in videos. Although efficient tools such as optical\nflow have been proposed to extract temporal motive information, it often\nencounters difficulties when used for saliency detection due to the movement of\ncamera or the partial movement of salient objects. In this paper, we\ninvestigate the complimentary roles of spatial and temporal information and\npropose a novel dynamic spatiotemporal network (DS-Net) for more effective\nfusion of spatiotemporal information. We construct a symmetric two-bypass\nnetwork to explicitly extract spatial and temporal features. A dynamic weight\ngenerator (DWG) is designed to automatically learn the reliability of\ncorresponding saliency branch. And a top-down cross attentive aggregation (CAA)\nprocedure is designed so as to facilitate dynamic complementary aggregation of\nspatiotemporal features. Finally, the features are modified by spatial\nattention with the guidance of coarse saliency map and then go through decoder\npart for final saliency map. Experimental results on five benchmarks VOS,\nDAVIS, FBMS, SegTrack-v2, and ViSal demonstrate that the proposed method\nachieves superior performance than state-of-the-art algorithms. The source code\nis available at https://github.com/TJUMMG/DS-Net.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yuting Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weikang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jing_P/0/1/0/all/0/1\">Peiguang Jing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaokang Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Progressive Self-Guided Loss for Salient Object Detection. (arXiv:2101.02412v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.02412","description":"<p>We present a simple yet effective progressive self-guided loss function to\nfacilitate deep learning-based salient object detection (SOD) in images. The\nsaliency maps produced by the most relevant works still suffer from incomplete\npredictions due to the internal complexity of salient objects. Our proposed\nprogressive self-guided loss simulates a morphological closing operation on the\nmodel predictions for progressively creating auxiliary training supervisions to\nstep-wisely guide the training process. We demonstrate that this new loss\nfunction can guide the SOD model to highlight more complete salient objects\nstep-by-step and meanwhile help to uncover the spatial dependencies of the\nsalient object pixels in a region growing manner. Moreover, a new feature\naggregation module is proposed to capture multi-scale features and aggregate\nthem adaptively by a branch-wise attention mechanism. Benefiting from this\nmodule, our SOD framework takes advantage of adaptively aggregated multi-scale\nfeatures to locate and detect salient objects effectively. Experimental results\non several benchmark datasets show that our loss function not only advances the\nperformance of existing SOD models without architecture modification but also\nhelps our proposed framework to achieve state-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sheng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Weisi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1\">Guosheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Q/0/1/0/all/0/1\">Qiuping Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zichuan Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ConvNets for Counting: Object Detection of Transient Phenomena in Steelpan Drums. (arXiv:2102.00632v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.00632","description":"<p>We train an object detector built from convolutional neural networks to count\ninterference fringes in elliptical antinode regions in frames of high-speed\nvideo recordings of transient oscillations in Caribbean steelpan drums\nilluminated by electronic speckle pattern interferometry (ESPI). The\nannotations provided by our model aim to contribute to the understanding of\ntime-dependent behavior in such drums by tracking the development of\nsympathetic vibration modes. The system is trained on a dataset of crowdsourced\nhuman-annotated images obtained from the Zooniverse Steelpan Vibrations\nProject. Due to the small number of human-annotated images and the ambiguity of\nthe annotation task, we also evaluate the model on a large corpus of synthetic\nimages whose properties have been matched to the real images by style transfer\nusing a Generative Adversarial Network. Applying the model to thousands of\nunlabeled video frames, we measure oscillations consistent with audio\nrecordings of these drum strikes. One unanticipated result is that sympathetic\noscillations of higher-octave notes significantly precede the rise in sound\nintensity of the corresponding second harmonic tones; the mechanism responsible\nfor this remains unidentified. This paper primarily concerns the development of\nthe predictive model; further exploration of the steelpan images and deeper\nphysical insights await its further application.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hawley_S/0/1/0/all/0/1\">Scott H. Hawley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morrison_A/0/1/0/all/0/1\">Andrew C. Morrison</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PLAM: a Posit Logarithm-Approximate Multiplier. (arXiv:2102.09262v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2102.09262","description":"<p>The Posit Number System was introduced in 2017 as a replacement for\nfloating-point numbers. Since then, the community has explored its application\nin Neural Network related tasks and produced some unit designs which are still\nfar from being competitive with their floating-point counterparts. This paper\nproposes a Posit Logarithm-Approximate Multiplication (PLAM) scheme to\nsignificantly reduce the complexity of posit multipliers, the most power-hungry\nunits within Deep Neural Network architectures. When comparing with\nstate-of-the-art posit multipliers, experiments show that the proposed\ntechnique reduces the area, power, and delay of hardware multipliers up to\n72.86%, 81.79%, and 17.01%, respectively, without accuracy degradation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Murillo_R/0/1/0/all/0/1\">Raul Murillo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barrio_A/0/1/0/all/0/1\">Alberto A. Del Barrio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Botella_G/0/1/0/all/0/1\">Guillermo Botella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Min Soo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">HyunJin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bagherzadeh_N/0/1/0/all/0/1\">Nader Bagherzadeh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Material Measurement Units for a Circular Economy: Foundations through a Survey. (arXiv:2103.01997v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.01997","description":"<p>Long-term availability of minerals and industrial materials is a necessary\ncondition for sustainable development as they are the constituents of any\nmanufacturing product. To enhance the efficiency of material management, we\ndefine a computer-vision-enabled material measurement system and provide a\nsurvey of works relevant to its development with particular emphasis on the\nfoundations. A network of such systems for wide-area material stock monitoring\nis also covered. Finally, challenges and future research directions are\ndiscussed. As the first article bridging industrial ecology and advanced\ncomputer vision, this survey is intended to support both research communities\ntowards more sustainable manufacturing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zocco_F/0/1/0/all/0/1\">Federico Zocco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smyth_B/0/1/0/all/0/1\">Beatrice Smyth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McLoone_S/0/1/0/all/0/1\">Se&#xe1;n McLoone</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Benchmarking Robustness of Deep Learning Classifiers Using Two-Factor Perturbation. (arXiv:2103.03102v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2103.03102","description":"<p>The accuracy of DL classifiers is unstable in that it often changes\nsignificantly when retested on adversarial images, imperfect images, or\nperturbed images. This paper adds to the small but fundamental body of work on\nbenchmarking the robustness of DL classifiers on defective images. Unlike\nexisted single-factor digital perturbation work, we provide state-of-the-art\ntwo-factor perturbation that provides two natural perturbations on images\napplied in different sequences. The two-factor perturbation includes (1) two\ndigital perturbations (Salt &amp; pepper noise and Gaussian noise) applied in both\nsequences. (2) one digital perturbation (salt &amp; pepper noise) and a geometric\nperturbation (rotation) applied in different sequences. To measure robust DL\nclassifiers, previous scientists provided 15 types of single-factor corruption.\nWe created 69 benchmarking image sets, including a clean set, sets with single\nfactor perturbations, and sets with two-factor perturbation conditions. To be\nbest of our knowledge, this is the first report that two-factor perturbed\nimages improves both robustness and accuracy of DL classifiers. Previous\nresearch evaluating deep learning (DL) classifiers has often used top-1/top-5\naccuracy, so researchers have usually offered tables, line diagrams, and bar\ncharts to display accuracy of DL classifiers. But these existed approaches\ncannot quantitively evaluate robustness of DL classifiers. We innovate a new\ntwo-dimensional, statistical visualization tool, including mean accuracy and\ncoefficient of variation (CV), to benchmark the robustness of DL classifiers.\nAll source codes and related image sets are shared on websites\n(<a href=\"http://cslinux.semo.edu/david/data.html\">this http URL</a> or\nhttps://github.com/daiweiworking/RobustDeepLearningUsingPerturbations ) to\nsupport future academic research and industry projects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1\">Wei Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berleant_D/0/1/0/all/0/1\">Daniel Berleant</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative-Adversarial-Networks-based Ghost Recognition. (arXiv:2103.13858v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.13858","description":"<p>Nowadays, target recognition technique plays an important role in many\nfields. However, the current target image information based methods suffer from\nthe influence of image quality and the time cost of image reconstruction. In\nthis paper, we propose a novel imaging-free target recognition method combining\nghost imaging (GI) and generative adversarial networks (GAN). Based on the\nmechanism of GI, a set of random speckles sequence is employed to illuminate\ntarget, and a bucket detector without resolution is utilized to receive echo\nsignal. The bucket signal sequence formed after continuous detections is\nconstructed into a bucket signal array, which is regarded as the sample of GAN.\nThen, conditional GAN is used to map bucket signal array and target category.\nIn practical application, the speckles sequence in training step is employed to\nilluminate target, and the bucket signal array is input GAN for recognition.\nThe proposed method can improve the problems caused by conventional recognition\nmethods that based on target image information, and provide a certain\nturbulence-free ability. Extensive experiments show that the proposed method\nachieves promising performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yuchen He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yibing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1\">Sheng Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianxing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhuo Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Attentive 3D Human Pose and Shape Estimation from Videos. (arXiv:2103.14182v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.14182","description":"<p>We consider the task of estimating 3D human pose and shape from videos. While\nexisting frame-based approaches have made significant progress, these methods\nare independently applied to each image, thereby often leading to inconsistent\npredictions. In this work, we present a video-based learning algorithm for 3D\nhuman pose and shape estimation. The key insights of our method are two-fold.\nFirst, to address the inconsistent temporal prediction issue, we exploit\ntemporal information in videos and propose a self-attention module that jointly\nconsiders short-range and long-range dependencies across frames, resulting in\ntemporally coherent estimations. Second, we model human motion with a\nforecasting module that allows the transition between adjacent frames to be\nsmooth. We evaluate our method on the 3DPW, MPI-INF-3DHP, and Human3.6M\ndatasets. Extensive experimental results show that our algorithm performs\nfavorably against the state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yun-Chun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piccirilli_M/0/1/0/all/0/1\">Marco Piccirilli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piramuthu_R/0/1/0/all/0/1\">Robinson Piramuthu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Ming-Hsuan Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Modal RGB-D Scene Recognition Across Domains. (arXiv:2103.14672v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.14672","description":"<p>Scene recognition is one of the basic problems in computer vision research\nwith extensive applications in robotics. When available, depth images provide\nhelpful geometric cues that complement the RGB texture information and help to\nidentify discriminative scene image features. Depth sensing technology\ndeveloped fast in the last years and a great variety of 3D cameras have been\nintroduced, each with different acquisition properties. However, those\nproperties are often neglected when targeting big data collections, so\nmulti-modal images are gathered disregarding their original nature. In this\nwork, we put under the spotlight the existence of a possibly severe domain\nshift issue within multi-modality scene recognition datasets. As a consequence,\na scene classification model trained on one camera may not generalize on data\nfrom a different camera, only providing a low recognition performance. Starting\nfrom the well-known SUN RGB-D dataset, we designed an experimental testbed to\nstudy this problem and we use it to benchmark the performance of existing\nmethods. Finally, we introduce a novel adaptive scene recognition approach that\nleverages self-supervised translation between modalities. Indeed, learning to\ngo from RGB to depth and vice-versa is an unsupervised procedure that can be\ntrained jointly on data of multiple cameras and may help to bridge the gap\namong the extracted feature distributions. Our experimental results confirm the\neffectiveness of the proposed approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ferreri_A/0/1/0/all/0/1\">Andrea Ferreri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bucci_S/0/1/0/all/0/1\">Silvia Bucci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tommasi_T/0/1/0/all/0/1\">Tatiana Tommasi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning from Self-Discrepancy via Multiple Co-teaching for Cross-Domain Person Re-Identification. (arXiv:2104.02265v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.02265","description":"<p>Employing clustering strategy to assign unlabeled target images with pseudo\nlabels has become a trend for person re-identification (re-ID) algorithms in\ndomain adaptation. A potential limitation of these clustering-based methods is\nthat they always tend to introduce noisy labels, which will undoubtedly hamper\nthe performance of our re-ID system. To handle this limitation, an intuitive\nsolution is to utilize collaborative training to purify the pseudo label\nquality. However, there exists a challenge that the complementarity of two\nnetworks, which inevitably share a high similarity, becomes weakened gradually\nas training process goes on; worse still, these approaches typically ignore to\nconsider the self-discrepancy of intra-class relations. To address this issue,\nin this paper, we propose a multiple co-teaching framework for domain adaptive\nperson re-ID, opening up a promising direction about self-discrepancy problem\nunder unsupervised condition. On top of that, a mean-teaching mechanism is\nleveraged to enlarge the difference and discover more complementary features.\nComprehensive experiments conducted on several large-scale datasets show that\nour method achieves competitive performance compared with the\nstate-of-the-arts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiang_S/0/1/0/all/0/1\">Suncheng Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yuzhuo Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_M/0/1/0/all/0/1\">Mengyuan Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Ting Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MultiScene: A Large-scale Dataset and Benchmark for Multi-scene Recognition in Single Aerial Images. (arXiv:2104.02846v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.02846","description":"<p>Aerial scene recognition is a fundamental research problem in interpreting\nhigh-resolution aerial imagery. Over the past few years, most studies focus on\nclassifying an image into one scene category, while in real-world scenarios, it\nis more often that a single image contains multiple scenes. Therefore, in this\npaper, we investigate a more practical yet underexplored task -- multi-scene\nrecognition in single images. To this end, we create a large-scale dataset,\ncalled MultiScene, composed of 100,000 unconstrained high-resolution aerial\nimages. Considering that manually labeling such images is extremely arduous, we\nresort to low-cost annotations from crowdsourcing platforms, e.g.,\nOpenStreetMap (OSM). However, OSM data might suffer from incompleteness and\nincorrectness, which introduce noise into image labels. To address this issue,\nwe visually inspect 14,000 images and correct their scene labels, yielding a\nsubset of cleanly-annotated images, named MultiScene-Clean. With it, we can\ndevelop and evaluate deep networks for multi-scene recognition using clean\ndata. Moreover, we provide crowdsourced annotations of all images for the\npurpose of studying network learning with noisy labels. We conduct experiments\nwith extensive baseline models on both MultiScene-Clean and MultiScene to offer\nbenchmarks for multi-scene recognition in single images and learning from noisy\nlabels for this task, respectively. To facilitate progress, we make our dataset\nand trained models available on\nhttps://gitlab.lrz.de/ai4eo/reasoning/multiscene.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hua_Y/0/1/0/all/0/1\">Yuansheng Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mou_L/0/1/0/all/0/1\">Lichao Mou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_P/0/1/0/all/0/1\">Pu Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiao Xiang Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DPR-CAE: Capsule Autoencoder with Dynamic Part Representation for Image Parsing. (arXiv:2104.14735v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.14735","description":"<p>Parsing an image into a hierarchy of objects, parts, and relations is\nimportant and also challenging in many computer vision tasks. This paper\nproposes a simple and effective capsule autoencoder to address this issue,\ncalled DPR-CAE. In our approach, the encoder parses the input into a set of\npart capsules, including pose, intensity, and dynamic vector. The decoder\nintroduces a novel dynamic part representation (DPR) by combining the dynamic\nvector and a shared template bank. These part representations are then\nregulated by corresponding capsules to composite the final output in an\ninterpretable way. Besides, an extra translation-invariant module is proposed\nto avoid directly learning the uncertain scene-part relationship in our\nDPR-CAE, which makes the resulting method achieves a promising performance gain\non $rm$-MNIST and $rm$-Fashion-MNIST. % to model the scene-object relationship\nDPR-CAE can be easily combined with the existing stacked capsule autoencoder\nand experimental results show it significantly improves performance in terms of\nunsupervised object classification. Our code is available in the Appendix.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiang_C/0/1/0/all/0/1\">Canqun Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhennan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_W/0/1/0/all/0/1\">Wenbin Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chen Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Real-time Human Action Recognition Using Locally Aggregated Kinematic-Guided Skeletonlet and Supervised Hashing-by-Analysis Model. (arXiv:2105.11312v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.11312","description":"<p>3D action recognition is referred to as the classification of action\nsequences which consist of 3D skeleton joints. While many research work are\ndevoted to 3D action recognition, it mainly suffers from three problems: highly\ncomplicated articulation, a great amount of noise, and a low implementation\nefficiency. To tackle all these problems, we propose a real-time 3D action\nrecognition framework by integrating the locally aggregated kinematic-guided\nskeletonlet (LAKS) with a supervised hashing-by-analysis (SHA) model. We first\ndefine the skeletonlet as a few combinations of joint offsets grouped in terms\nof kinematic principle, and then represent an action sequence using LAKS, which\nconsists of a denoising phase and a locally aggregating phase. The denoising\nphase detects the noisy action data and adjust it by replacing all the features\nwithin it with the features of the corresponding previous frame, while the\nlocally aggregating phase sums the difference between an offset feature of the\nskeletonlet and its cluster center together over all the offset features of the\nsequence. Finally, the SHA model which combines sparse representation with a\nhashing model, aiming at promoting the recognition accuracy while maintaining a\nhigh efficiency. Experimental results on MSRAction3D, UTKinectAction3D and\nFlorence3DAction datasets demonstrate that the proposed method outperforms\nstate-of-the-art methods in both recognition accuracy and implementation\nefficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1\">Bin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shaofan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_D/0/1/0/all/0/1\">Dehui Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lichun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1\">Baocai Yin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How to Design a Three-Stage Architecture for Audio-Visual Active Speaker Detection in the Wild. (arXiv:2106.03932v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.03932","description":"<p>Successful active speaker detection requires a three-stage pipeline: (i)\naudio-visual encoding for all speakers in the clip, (ii) inter-speaker relation\nmodeling between a reference speaker and the background speakers within each\nframe, and (iii) temporal modeling for the reference speaker. Each stage of\nthis pipeline plays an important role for the final performance of the created\narchitecture. Based on a series of controlled experiments, this work presents\nseveral practical guidelines for audio-visual active speaker detection.\nCorrespondingly, we present a new architecture called ASDNet, which achieves a\nnew state-of-the-art on the AVA-ActiveSpeaker dataset with a mAP of 93.5%\noutperforming the second best with a large margin of 4.7%. Our code and\npretrained models are publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kopuklu_O/0/1/0/all/0/1\">Okan K&#xf6;p&#xfc;kl&#xfc;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taseska_M/0/1/0/all/0/1\">Maja Taseska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rigoll_G/0/1/0/all/0/1\">Gerhard Rigoll</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Animatable Neural Radiance Fields from Monocular RGB Videos. (arXiv:2106.13629v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.13629","description":"<p>We present animatable neural radiance fields (animatable NeRF) for detailed\nhuman avatar creation from monocular videos. Our approach extends neural\nradiance fields (NeRF) to the dynamic scenes with human movements via\nintroducing explicit pose-guided deformation while learning the scene\nrepresentation network. In particular, we estimate the human pose for each\nframe and learn a constant canonical space for the detailed human template,\nwhich enables natural shape deformation from the observation space to the\ncanonical space under the explicit control of the pose parameters. To\ncompensate for inaccurate pose estimation, we introduce the pose refinement\nstrategy that updates the initial pose during the learning process, which not\nonly helps to learn more accurate human reconstruction but also accelerates the\nconvergence. In experiments we show that the proposed approach achieves 1)\nimplicit human geometry and appearance reconstruction with high-quality\ndetails, 2) photo-realistic rendering of the human from novel views, and 3)\nanimation of the human with novel poses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jianchuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ying Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_D/0/1/0/all/0/1\">Di Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhe_X/0/1/0/all/0/1\">Xuefei Zhe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_L/0/1/0/all/0/1\">Linchao Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1\">Xu Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Huchuan Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RCNN-SliceNet: A Slice and Cluster Approach for Nuclei Centroid Detection in Three-Dimensional Fluorescence Microscopy Images. (arXiv:2106.15753v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2106.15753","description":"<p>Robust and accurate nuclei centroid detection is important for the\nunderstanding of biological structures in fluorescence microscopy images.\nExisting automated nuclei localization methods face three main challenges: (1)\nMost of object detection methods work only on 2D images and are difficult to\nextend to 3D volumes; (2) Segmentation-based models can be used on 3D volumes\nbut it is computational expensive for large microscopy volumes and they have\ndifficulty distinguishing different instances of objects; (3) Hand annotated\nground truth is limited for 3D microscopy volumes. To address these issues, we\npresent a scalable approach for nuclei centroid detection of 3D microscopy\nvolumes. We describe the RCNN-SliceNet to detect 2D nuclei centroids for each\nslice of the volume from different directions and 3D agglomerative hierarchical\nclustering (AHC) is used to estimate the 3D centroids of nuclei in a volume.\nThe model was trained with the synthetic microscopy data generated using\nSpatially Constrained Cycle-Consistent Adversarial Networks (SpCycleGAN) and\ntested on different types of real 3D microscopy data. Extensive experimental\nresults demonstrate that our proposed method can accurately count and detect\nthe nuclei centroids in a 3D microscopy volume.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wu_L/0/1/0/all/0/1\">Liming Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Han_S/0/1/0/all/0/1\">Shuo Han</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_A/0/1/0/all/0/1\">Alain Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Salama_P/0/1/0/all/0/1\">Paul Salama</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dunn_K/0/1/0/all/0/1\">Kenneth W. Dunn</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Delp_E/0/1/0/all/0/1\">Edward J. Delp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Vision Transformers via Fine-Grained Manifold Distillation. (arXiv:2107.01378v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.01378","description":"<p>This paper studies the model compression problem of vision transformers.\nBenefit from the self-attention module, transformer architectures have shown\nextraordinary performance on many computer vision tasks. Although the network\nperformance is boosted, transformers are often required more computational\nresources including memory usage and the inference complexity. Compared with\nthe existing knowledge distillation approaches, we propose to excavate useful\ninformation from the teacher transformer through the relationship between\nimages and the divided patches. We then explore an efficient fine-grained\nmanifold distillation approach that simultaneously calculates cross-images,\ncross-patch, and random-selected manifolds in teacher and student models.\nExperimental results conducted on several benchmarks demonstrate the\nsuperiority of the proposed algorithm for distilling portable transformer\nmodels with higher performance. For example, our approach achieves 75.06% Top-1\naccuracy on the ImageNet-1k dataset for training a DeiT-Tiny model, which\noutperforms other ViT distillation methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jia_D/0/1/0/all/0/1\">Ding Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1\">Kai Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunhe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yehui Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jianyuan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Graph Representation Learning for Road Type Classification. (arXiv:2107.07791v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2107.07791","description":"<p>We present a novel learning-based approach to graph representations of road\nnetworks employing state-of-the-art graph convolutional neural networks. Our\napproach is applied to realistic road networks of 17 cities from Open Street\nMap. While edge features are crucial to generate descriptive graph\nrepresentations of road networks, graph convolutional networks usually rely on\nnode features only. We show that the highly representative edge features can\nstill be integrated into such networks by applying a line graph transformation.\nWe also propose a method for neighborhood sampling based on a topological\nneighborhood composed of both local and global neighbors. We compare the\nperformance of learning representations using different types of neighborhood\naggregation functions in transductive and inductive tasks and in supervised and\nunsupervised learning. Furthermore, we propose a novel aggregation approach,\nGraph Attention Isomorphism Network, GAIN. Our results show that GAIN\noutperforms state-of-the-art methods on the road type classification problem.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gharaee_Z/0/1/0/all/0/1\">Zahra Gharaee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kowshik_S/0/1/0/all/0/1\">Shreyas Kowshik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stromann_O/0/1/0/all/0/1\">Oliver Stromann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Felsberg_M/0/1/0/all/0/1\">Michael Felsberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Agent-Environment Network for Temporal Action Proposal Generation. (arXiv:2107.08323v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.08323","description":"<p>Temporal action proposal generation is an essential and challenging task that\naims at localizing temporal intervals containing human actions in untrimmed\nvideos. Most of existing approaches are unable to follow the human cognitive\nprocess of understanding the video context due to lack of attention mechanism\nto express the concept of an action or an agent who performs the action or the\ninteraction between the agent and the environment. Based on the action\ndefinition that a human, known as an agent, interacts with the environment and\nperforms an action that affects the environment, we propose a contextual\nAgent-Environment Network. Our proposed contextual AEN involves (i) agent\npathway, operating at a local level to tell about which humans/agents are\nacting and (ii) environment pathway operating at a global level to tell about\nhow the agents interact with the environment. Comprehensive evaluations on\n20-action THUMOS-14 and 200-action ActivityNet-1.3 datasets with different\nbackbone networks, i.e C3D and SlowFast, show that our method robustly exhibits\noutperformance against state-of-the-art methods regardless of the employed\nbackbone network.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vo_Ho_V/0/1/0/all/0/1\">Viet-Khoa Vo-Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_N/0/1/0/all/0/1\">Ngan Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamazaki_K/0/1/0/all/0/1\">Kashu Yamazaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugimoto_A/0/1/0/all/0/1\">Akihiro Sugimoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_M/0/1/0/all/0/1\">Minh-Triet Tran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Go Wider Instead of Deeper. (arXiv:2107.11817v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2107.11817","description":"<p>More transformer blocks with residual connections have recently achieved\nimpressive results on various tasks. To achieve better performance with fewer\ntrainable parameters, recent methods are proposed to go shallower by parameter\nsharing or model compressing along with the depth. However, weak modeling\ncapacity limits their performance. Contrastively, going wider by inducing more\ntrainable matrixes and parameters would produce a huge model requiring advanced\nparallelism to train and inference.\n</p>\n<p>In this paper, we propose a parameter-efficient framework, going wider\ninstead of deeper. Specially, following existing works, we adapt parameter\nsharing to compress along depth. But, such deployment would limit the\nperformance. To maximize modeling capacity, we scale along model width by\nreplacing feed-forward network (FFN) with mixture-of-experts (MoE). Across\ntransformer blocks, instead of sharing normalization layers, we propose to use\nindividual layernorms to transform various semantic representations in a more\nparameter-efficient way. To evaluate our plug-and-run framework, we design\nWideNet and conduct comprehensive experiments on popular computer vision and\nnatural language processing benchmarks. On ImageNet-1K, our best model\noutperforms Vision Transformer (ViT) by $1.5\\%$ with $0.72 \\times$ trainable\nparameters. Using $0.46 \\times$ and $0.13 \\times$ parameters, our WideNet can\nstill surpass ViT and ViT-MoE by $0.8\\%$ and $2.1\\%$, respectively. On four\nnatural language processing datasets, WideNet outperforms ALBERT by $1.8\\%$ on\naverage and surpass BERT using factorized embedding parameterization by $0.8\\%$\nwith fewer parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xue_F/0/1/0/all/0/1\">Fuzhao Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Ziji Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Futao Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_Y/0/1/0/all/0/1\">Yuxuan Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1\">Yang You</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Robust Object Detection: Bayesian RetinaNet for Homoscedastic Aleatoric Uncertainty Modeling. (arXiv:2108.00784v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.00784","description":"<p>According to recent studies, commonly used computer vision datasets contain\nabout 4% of label errors. For example, the COCO dataset is known for its high\nlevel of noise in data labels, which limits its use for training robust neural\ndeep architectures in a real-world scenario. To model such a noise, in this\npaper we have proposed the homoscedastic aleatoric uncertainty estimation, and\npresent a series of novel loss functions to address the problem of image object\ndetection at scale. Specifically, the proposed functions are based on Bayesian\ninference and we have incorporated them into the common community-adopted\nobject detection deep learning architecture RetinaNet. We have also shown that\nmodeling of homoscedastic aleatoric uncertainty using our novel functions\nallows to increase the model interpretability and to improve the object\ndetection performance being evaluated on the COCO dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khanzhina_N/0/1/0/all/0/1\">Natalia Khanzhina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lapenok_A/0/1/0/all/0/1\">Alexey Lapenok</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Filchenkov_A/0/1/0/all/0/1\">Andrey Filchenkov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pseudo-mask Matters in Weakly-supervised Semantic Segmentation. (arXiv:2108.12995v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.12995","description":"<p>Most weakly supervised semantic segmentation (WSSS) methods follow the\npipeline that generates pseudo-masks initially and trains the segmentation\nmodel with the pseudo-masks in fully supervised manner after. However, we find\nsome matters related to the pseudo-masks, including high quality pseudo-masks\ngeneration from class activation maps (CAMs), and training with noisy\npseudo-mask supervision. For these matters, we propose the following designs to\npush the performance to new state-of-art: (i) Coefficient of Variation\nSmoothing to smooth the CAMs adaptively; (ii) Proportional Pseudo-mask\nGeneration to project the expanded CAMs to pseudo-mask based on a new metric\nindicating the importance of each class on each location, instead of the scores\ntrained from binary classifiers. (iii) Pretended Under-Fitting strategy to\nsuppress the influence of noise in pseudo-mask; (iv) Cyclic Pseudo-mask to\nboost the pseudo-masks during training of fully supervised semantic\nsegmentation (FSSS). Experiments based on our methods achieve new state-of-art\nresults on two changeling weakly supervised semantic segmentation datasets,\npushing the mIoU to 70.0% and 40.2% on PAS-CAL VOC 2012 and MS COCO 2014\nrespectively. Codes including segmentation framework are released at\nhttps://github.com/Eli-YiLi/PMM\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuang_Z/0/1/0/all/0/1\">Zhanghui Kuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Liyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yimin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wayne Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LSD-StructureNet: Modeling Levels of Structural Detail in 3D Part Hierarchies. (arXiv:2108.13459v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.13459","description":"<p>Generative models for 3D shapes represented by hierarchies of parts can\ngenerate realistic and diverse sets of outputs. However, existing models suffer\nfrom the key practical limitation of modelling shapes holistically and thus\ncannot perform conditional sampling, i.e. they are not able to generate\nvariants on individual parts of generated shapes without modifying the rest of\nthe shape. This is limiting for applications such as 3D CAD design that involve\nadjusting created shapes at multiple levels of detail. To address this, we\nintroduce LSD-StructureNet, an augmentation to the StructureNet architecture\nthat enables re-generation of parts situated at arbitrary positions in the\nhierarchies of its outputs. We achieve this by learning individual,\nprobabilistic conditional decoders for each hierarchy depth. We evaluate\nLSD-StructureNet on the PartNet dataset, the largest dataset of 3D shapes\nrepresented by hierarchies of parts. Our results show that contrarily to\nexisting methods, LSD-StructureNet can perform conditional sampling without\nimpacting inference speed or the realism and diversity of its outputs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Roberts_D/0/1/0/all/0/1\">Dominic Roberts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danielyan_A/0/1/0/all/0/1\">Ara Danielyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_H/0/1/0/all/0/1\">Hang Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golparvar_Fard_M/0/1/0/all/0/1\">Mani Golparvar-Fard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Forsyth_D/0/1/0/all/0/1\">David Forsyth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TrouSPI-Net: Spatio-temporal attention on parallel atrous convolutions and U-GRUs for skeletal pedestrian crossing prediction. (arXiv:2109.00953v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.00953","description":"<p>Understanding the behaviors and intentions of pedestrians is still one of the\nmain challenges for vehicle autonomy, as accurate predictions of their\nintentions can guarantee their safety and driving comfort of vehicles. In this\npaper, we address pedestrian crossing prediction in urban traffic environments\nby linking the dynamics of a pedestrian's skeleton to a binary crossing\nintention. We introduce TrouSPI-Net: a context-free, lightweight, multi-branch\npredictor. TrouSPI-Net extracts spatio-temporal features for different time\nresolutions by encoding pseudo-images sequences of skeletal joints' positions\nand processes them with parallel attention modules and atrous convolutions. The\nproposed approach is then enhanced by processing features such as relative\ndistances of skeletal joints, bounding box positions, or ego-vehicle speed with\nU-GRUs. Using the newly proposed evaluation procedures for two large public\nnaturalistic data sets for studying pedestrian behavior in traffic: JAAD and\nPIE, we evaluate TrouSPI-Net and analyze its performance. Experimental results\nshow that TrouSPI-Net achieved 0.76 F1 score on JAAD and 0.80 F1 score on PIE,\ntherefore outperforming current state-of-the-art while being lightweight and\ncontext-free.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gesnouin_J/0/1/0/all/0/1\">Joseph Gesnouin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pechberti_S/0/1/0/all/0/1\">Steve Pechberti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stanciulescu_B/0/1/0/all/0/1\">Bogdan Stanciulescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moutarde_F/0/1/0/all/0/1\">Fabien Moutarde</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spatiotemporal Inconsistency Learning for DeepFake Video Detection. (arXiv:2109.01860v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.01860","description":"<p>The rapid development of facial manipulation techniques has aroused public\nconcerns in recent years. Following the success of deep learning, existing\nmethods always formulate DeepFake video detection as a binary classification\nproblem and develop frame-based and video-based solutions. However, little\nattention has been paid to capturing the spatial-temporal inconsistency in\nforged videos. To address this issue, we term this task as a Spatial-Temporal\nInconsistency Learning (STIL) process and instantiate it into a novel STIL\nblock, which consists of a Spatial Inconsistency Module (SIM), a Temporal\nInconsistency Module (TIM), and an Information Supplement Module (ISM).\nSpecifically, we present a novel temporal modeling paradigm in TIM by\nexploiting the temporal difference over adjacent frames along with both\nhorizontal and vertical directions. And the ISM simultaneously utilizes the\nspatial information from SIM and temporal information from TIM to establish a\nmore comprehensive spatial-temporal representation. Moreover, our STIL block is\nflexible and could be plugged into existing 2D CNNs. Extensive experiments and\nvisualizations are presented to demonstrate the effectiveness of our method\nagainst the state-of-the-art competitors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_Z/0/1/0/all/0/1\">Zhihao Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_T/0/1/0/all/0/1\">Taiping Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1\">Shouhong Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jilin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Feiyue Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lizhuang Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Navigational Path-Planning For All-Terrain Autonomous Agricultural Robot. (arXiv:2109.02015v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2109.02015","description":"<p>The shortage of workforce and increasing cost of maintenance has forced many\nfarm industrialists to shift towards automated and mechanized approaches. The\nkey component for autonomous systems is the path planning techniques used.\nCoverage path planning (CPP) algorithm is used for navigating over farmlands to\nperform various agricultural operations such as seeding, ploughing, or spraying\npesticides and fertilizers. This report paper compares novel algorithms for\nautonomous navigation of farmlands. For reduction of navigational constraints,\na high-resolution grid map representation is taken into consideration specific\nto Indian environments. The free space is covered by distinguishing the grid\ncells as covered, unexplored, partially explored and presence of an obstacle.\nThe performance of the compared algorithms is evaluated with metrics such as\ntime efficiency, space efficiency, accuracy, and robustness to changes in the\nenvironment. Robotic Operating System (ROS), Dassault Systemes Experience\nPlatform (3DS Experience), MATLAB along Python were used for the simulation of\nthe compared algorithms. The results proved the applicability of the algorithms\nfor autonomous field navigation and feasibility with robotic path planning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghodke_V/0/1/0/all/0/1\">Vedant Ghodke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madake_J/0/1/0/all/0/1\">Jyoti Madake</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Identification of Driver Phone Usage Violations via State-of-the-Art Object Detection with Tracking. (arXiv:2109.02119v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.02119","description":"<p>The use of mobiles phones when driving have been a major factor when it comes\nto road traffic incidents and the process of capturing such violations can be a\nlaborious task. Advancements in both modern object detection frameworks and\nhigh-performance hardware has paved the way for a more automated approach when\nit comes to video surveillance. In this work, we propose a custom-trained\nstate-of-the-art object detector to work with roadside cameras to capture\ndriver phone usage without the need for human intervention. The proposed\napproach also addresses the issues caused by windscreen glare and introduces\nthe steps required to remedy this. Twelve pre-trained models are fine-tuned\nwith our custom dataset using four popular object detection methods: YOLO, SSD,\nFaster R-CNN, and CenterNet. Out of all the object detectors tested, the YOLO\nyields the highest accuracy levels of up to 96% (AP10) and frame rates of up to\n~30 FPS. DeepSort object tracking algorithm is also integrated into the\nbest-performing model to collect records of only the unique violations, and\nenable the proposed approach to count the number of vehicles. The proposed\nautomated system will collect the output images of the identified violations,\ntimestamps of each violation, and total vehicle count. Data can be accessed via\na purpose-built user interface.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Carrell_S/0/1/0/all/0/1\">Steven Carrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atapour_Abarghouei_A/0/1/0/all/0/1\">Amir Atapour-Abarghouei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Toward Realistic Single-View 3D Object Reconstruction with Unsupervised Learning from Multiple Images. (arXiv:2109.02288v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.02288","description":"<p>Recovering the 3D structure of an object from a single image is a challenging\ntask due to its ill-posed nature. One approach is to utilize the plentiful\nphotos of the same object category to learn a strong 3D shape prior for the\nobject. This approach has successfully been demonstrated by a recent work of Wu\net al. (2020), which obtained impressive 3D reconstruction networks with\nunsupervised learning. However, their algorithm is only applicable to symmetric\nobjects. In this paper, we eliminate the symmetry requirement with a novel\nunsupervised algorithm that can learn a 3D reconstruction network from a\nmulti-image dataset. Our algorithm is more general and covers the\nsymmetry-required scenario as a special case. Besides, we employ a novel albedo\nloss that improves the reconstructed details and realisticity. Our method\nsurpasses the previous work in both quality and robustness, as shown in\nexperiments on datasets of various structures, including single-view,\nmulti-view, image-collection, and video sets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ho_L/0/1/0/all/0/1\">Long-Nhat Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_A/0/1/0/all/0/1\">Anh Tuan Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phung_Q/0/1/0/all/0/1\">Quynh Phung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoai_M/0/1/0/all/0/1\">Minh Hoai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-09-07T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/"}}]}]}