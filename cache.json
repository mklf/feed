{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-01-10T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Large-scale protein-protein post-translational modification extraction with distant supervision and confidence calibrated BioBERT. (arXiv:2201.02229v1 [cs.LG])","link":"http://arxiv.org/abs/2201.02229","description":"<p>Protein-protein interactions (PPIs) are critical to normal cellular function\nand are related to many disease pathways. However, only 4% of PPIs are\nannotated with PTMs in biological knowledge databases such as IntAct, mainly\nperformed through manual curation, which is neither time nor cost-effective. We\nuse the IntAct PPI database to create a distant supervised dataset annotated\nwith interacting protein pairs, their corresponding PTM type, and associated\nabstracts from the PubMed database. We train an ensemble of BioBERT models -\ndubbed PPI-BioBERT-x10 to improve confidence calibration. We extend the use of\nensemble average confidence approach with confidence variation to counteract\nthe effects of class imbalance to extract high confidence predictions. The\nPPI-BioBERT-x10 model evaluated on the test set resulted in a modest F1-micro\n41.3 (P =5 8.1, R = 32.1). However, by combining high confidence and low\nvariation to identify high quality predictions, tuning the predictions for\nprecision, we retained 19% of the test predictions with 100% precision. We\nevaluated PPI-BioBERT-x10 on 18 million PubMed abstracts and extracted 1.6\nmillion (546507 unique PTM-PPI triplets) PTM-PPI predictions, and filter ~ 5700\n(4584 unique) high confidence predictions. Of the 5700, human evaluation on a\nsmall randomly sampled subset shows that the precision drops to 33.7% despite\nconfidence calibration and highlights the challenges of generalisability beyond\nthe test set even with confidence calibration. We circumvent the problem by\nonly including predictions associated with multiple papers, improving the\nprecision to 58.8%. In this work, we highlight the benefits and challenges of\ndeep learning-based text mining in practice, and the need for increased\nemphasis on confidence calibration to facilitate human curation efforts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Elangovan_A/0/1/0/all/0/1\">Aparna Elangovan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pires_D/0/1/0/all/0/1\">Douglas E. V. Pires</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davis_M/0/1/0/all/0/1\">Melissa J. Davis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verspoor_K/0/1/0/all/0/1\">Karin Verspoor</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Applying Word Embeddings to Measure Valence in Information Operations Targeting Journalists in Brazil. (arXiv:2201.02257v1 [cs.CL])","link":"http://arxiv.org/abs/2201.02257","description":"<p>Among the goals of information operations are to change the overall\ninformation environment vis-\\'a-vis specific actors. For example, \"trolling\ncampaigns\" seek to undermine the credibility of specific public figures,\nleading others to distrust them and intimidating these figures into silence. To\naccomplish these aims, information operations frequently make use of \"trolls\"\n-- malicious online actors who target verbal abuse at these figures. In Brazil,\nin particular, allies of Brazil's current president have been accused of\noperating a \"hate cabinet\" -- a trolling operation that targets journalists who\nhave alleged corruption by this politician and other members of his regime.\nLeading approaches to detecting harmful speech, such as Google's Perspective\nAPI, seek to identify specific messages with harmful content. While this\napproach is helpful in identifying content to downrank, flag, or remove, it is\nknown to be brittle, and may miss attempts to introduce more subtle biases into\nthe discourse. Here, we aim to develop a measure that might be used to assess\nhow targeted information operations seek to change the overall valence, or\nappraisal, of specific actors. Preliminary results suggest known campaigns\ntarget female journalists more so than male journalists, and that these\ncampaigns may leave detectable traces in overall Twitter discourse.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Broniatowski_D/0/1/0/all/0/1\">David A. Broniatowski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Repurposing Existing Deep Networks for Caption and Aesthetic-Guided Image Cropping. (arXiv:2201.02280v1 [cs.CV])","link":"http://arxiv.org/abs/2201.02280","description":"<p>We propose a novel optimization framework that crops a given image based on\nuser description and aesthetics. Unlike existing image cropping methods, where\none typically trains a deep network to regress to crop parameters or cropping\nactions, we propose to directly optimize for the cropping parameters by\nrepurposing pre-trained networks on image captioning and aesthetic tasks,\nwithout any fine-tuning, thereby avoiding training a separate network.\nSpecifically, we search for the best crop parameters that minimize a combined\nloss of the initial objectives of these networks. To make the optimization\ntable, we propose three strategies: (i) multi-scale bilinear sampling, (ii)\nannealing the scale of the crop region, therefore effectively reducing the\nparameter space, (iii) aggregation of multiple optimization results. Through\nvarious quantitative and qualitative evaluations, we show that our framework\ncan produce crops that are well-aligned to intended user descriptions and\naesthetically pleasing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Horanyi_N/0/1/0/all/0/1\">Nora Horanyi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_K/0/1/0/all/0/1\">Kedi Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_K/0/1/0/all/0/1\">Kwang Moo Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bojja_A/0/1/0/all/0/1\">Abhishake Kumar Bojja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leonardis_A/0/1/0/all/0/1\">Ales Leonardis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Hyung Jin Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Transfer Learning Pipeline for Educational Resource Discovery with Application in Leading Paragraph Generation. (arXiv:2201.02312v1 [cs.CL])","link":"http://arxiv.org/abs/2201.02312","description":"<p>Effective human learning depends on a wide selection of educational materials\nthat align with the learner's current understanding of the topic. While the\nInternet has revolutionized human learning or education, a substantial resource\naccessibility barrier still exists. Namely, the excess of online information\ncan make it challenging to navigate and discover high-quality learning\nmaterials. In this paper, we propose the educational resource discovery (ERD)\npipeline that automates web resource discovery for novel domains. The pipeline\nconsists of three main steps: data collection, feature extraction, and resource\nclassification. We start with a known source domain and conduct resource\ndiscovery on two unseen target domains via transfer learning. We first collect\nfrequent queries from a set of seed documents and search on the web to obtain\ncandidate resources, such as lecture slides and introductory blog posts. Then\nwe introduce a novel pretrained information retrieval deep neural network\nmodel, query-document masked language modeling (QD-MLM), to extract deep\nfeatures of these candidate resources. We apply a tree-based classifier to\ndecide whether the candidate is a positive learning resource. The pipeline\nachieves F1 scores of 0.94 and 0.82 when evaluated on two similar but novel\ntarget domains. Finally, we demonstrate how this pipeline can benefit an\napplication: leading paragraph generation for surveys. This is the first study\nthat considers various web resources for survey generation, to the best of our\nknowledge. We also release a corpus of 39,728 manually labeled web resources\nand 659 queries from NLP, Computer Vision (CV), and Statistics (STATS).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_I/0/1/0/all/0/1\">Irene Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+George_T/0/1/0/all/0/1\">Thomas George</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fabbri_A/0/1/0/all/0/1\">Alexander Fabbri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_T/0/1/0/all/0/1\">Tammy Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Benjamin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawamura_R/0/1/0/all/0/1\">Rina Kawamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_R/0/1/0/all/0/1\">Richard Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_V/0/1/0/all/0/1\">Vanessa Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hingmire_S/0/1/0/all/0/1\">Swapnil Hingmire</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1\">Dragomir Radev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Unsupervised Masking Objective for Abstractive Multi-Document News Summarization. (arXiv:2201.02321v1 [cs.CL])","link":"http://arxiv.org/abs/2201.02321","description":"<p>We show that a simple unsupervised masking objective can approach near\nsupervised performance on abstractive multi-document news summarization. Our\nmethod trains a state-of-the-art neural summarization model to predict the\nmasked out source document with highest lexical centrality relative to the\nmulti-document group. In experiments on the Multi-News dataset, our masked\ntraining objective yields a system that outperforms past unsupervised methods\nand, in human evaluation, surpasses the best supervised method without\nrequiring access to any ground-truth summaries. Further, we evaluate how\ndifferent measures of lexical centrality, inspired by past work on extractive\nsummarization, affect final performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vogler_N/0/1/0/all/0/1\">Nikolai Vogler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Songlin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yujie Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mi_Y/0/1/0/all/0/1\">Yujian Mi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berg_Kirkpatrick_T/0/1/0/all/0/1\">Taylor Berg-Kirkpatrick</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Defeat of the Winograd Schema Challenge. (arXiv:2201.02387v1 [cs.CL])","link":"http://arxiv.org/abs/2201.02387","description":"<p>The Winograd Schema Challenge -- a set of twin sentences involving pronoun\nreference disambiguation that seem to require the use of commonsense knowledge\n-- was proposed by Hector Levesque in 2011. By 2019, a number of AI systems,\nbased on large pre-trained transformer-based language models and fine-tuned on\nthese kinds of problems, achieved better than 90% accuracy. In this paper, we\nreview the history of the Winograd Schema Challenge and assess its\nsignificance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kocijan_V/0/1/0/all/0/1\">Vid Kocijan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davis_E/0/1/0/all/0/1\">Ernest Davis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lukasiewicz_T/0/1/0/all/0/1\">Thomas Lukasiewicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marcus_G/0/1/0/all/0/1\">Gary Marcus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morgenstern_L/0/1/0/all/0/1\">Leora Morgenstern</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Speech Recognition Datasets in Cantonese Language: A Survey and a New Dataset. (arXiv:2201.02419v1 [cs.CL])","link":"http://arxiv.org/abs/2201.02419","description":"<p>Automatic speech recognition (ASR) on low resource languages improves access\nof linguistic minorities to technological advantages provided by Artificial\nIntelligence (AI). In this paper, we address a problem of data scarcity of Hong\nKong Cantonese language by creating a new Cantonese dataset. Our dataset,\nMulti-Domain Cantonese Corpus (MDCC), consists of 73.6 hours of clean read\nspeech paired with transcripts, collected from Cantonese audiobooks from Hong\nKong. It combines philosophy, politics, education, culture, lifestyle and\nfamily domains, covering a wide range of topics. We also review all existing\nCantonese datasets and perform experiments on the two biggest datasets (MDCC\nand Common Voice zh-HK). We analyze the existing datasets according to their\nspeech type, data source, total size and availability. The results of\nexperiments conducted with Fairseq S2T Transformer, a state-of-the-art ASR\nmodel, show the effectiveness of our dataset. In addition, we create a powerful\nand robust Cantonese ASR model by applying multi-dataset learning on MDCC and\nCommon Voice zh-HK.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tiezheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frieske_R/0/1/0/all/0/1\">Rita Frieske</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1\">Peng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cahyawijaya_S/0/1/0/all/0/1\">Samuel Cahyawijaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yiu_C/0/1/0/all/0/1\">Cheuk Tung Shadow Yiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lovenia_H/0/1/0/all/0/1\">Holy Lovenia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1\">Wenliang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barezi_E/0/1/0/all/0/1\">Elham J. Barezi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qifeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaojuan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1\">Bertram E. Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1\">Pascale Fung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic-based Data Augmentation for Math Word Problems. (arXiv:2201.02489v1 [cs.CL])","link":"http://arxiv.org/abs/2201.02489","description":"<p>It's hard for neural MWP solvers to deal with tiny local variances. In MWP\ntask, some local changes conserve the original semantic while the others may\ntotally change the underlying logic. Currently, existing datasets for MWP task\ncontain limited samples which are key for neural models to learn to\ndisambiguate different kinds of local variances in questions and solve the\nquestions correctly. In this paper, we propose a set of novel data augmentation\napproaches to supplement existing datasets with such data that are augmented\nwith different kinds of local variances, and help to improve the generalization\nability of current neural models. New samples are generated by knowledge guided\nentity replacement, and logic guided problem reorganization. The augmentation\napproaches are ensured to keep the consistency between the new data and their\nlabels. Experimental results have shown the necessity and the effectiveness of\nour methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1\">Ailisi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jiaqing Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yanghua Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video Summarization Based on Video-text Representation. (arXiv:2201.02494v1 [cs.CV])","link":"http://arxiv.org/abs/2201.02494","description":"<p>Modern video summarization methods are based on deep neural networks which\nrequire a large amount of annotated data for training. However, existing\ndatasets for video summarization are small-scale, easily leading to\nover-fitting of the deep models. Considering that the annotation of large-scale\ndatasets is time-consuming, we propose a multimodal self-supervised learning\nframework to obtain semantic representations of videos, which benefits the\nvideo summarization task. Specifically, we explore the semantic consistency\nbetween the visual information and text information of videos, for the\nself-supervised pretraining of a multimodal encoder on a newly-collected\ndataset of video-text pairs. Additionally, we introduce a progressive video\nsummarization method, where the important content in a video is pinpointed\nprogressively to generate better summaries. Finally, an objective evaluation\nframework is proposed to measure the quality of video summaries based on video\nclassification. Extensive experiments have proved the effectiveness and\nsuperiority of our method in rank correlation coefficients, F-score, and the\nproposed objective evaluation compared to the state of the art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Haopeng_L/0/1/0/all/0/1\">Li Haopeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiuhong_K/0/1/0/all/0/1\">Ke Qiuhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mingming_G/0/1/0/all/0/1\">Gong Mingming</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rui_Z/0/1/0/all/0/1\">Zhang Rui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sign Language Video Retrieval with Free-Form Textual Queries. (arXiv:2201.02495v1 [cs.CV])","link":"http://arxiv.org/abs/2201.02495","description":"<p>Systems that can efficiently search collections of sign language videos have\nbeen highlighted as a useful application of sign language technology. However,\nthe problem of searching videos beyond individual keywords has received limited\nattention in the literature. To address this gap, in this work we introduce the\ntask of sign language retrieval with free-form textual queries: given a written\nquery (e.g., a sentence) and a large collection of sign language videos, the\nobjective is to find the signing video in the collection that best matches the\nwritten query. We propose to tackle this task by learning cross-modal\nembeddings on the recently introduced large-scale How2Sign dataset of American\nSign Language (ASL). We identify that a key bottleneck in the performance of\nthe system is the quality of the sign video embedding which suffers from a\nscarcity of labeled training data. We, therefore, propose SPOT-ALIGN, a\nframework for interleaving iterative rounds of sign spotting and feature\nalignment to expand the scope and scale of available training data. We validate\nthe effectiveness of SPOT-ALIGN for learning a robust sign video embedding\nthrough improvements in both sign recognition and the proposed video retrieval\ntask.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Duarte_A/0/1/0/all/0/1\">Amanda Duarte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Albanie_S/0/1/0/all/0/1\">Samuel Albanie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giro_i_Nieto_X/0/1/0/all/0/1\">Xavier Gir&#xf3;-i-Nieto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varol_G/0/1/0/all/0/1\">G&#xfc;l Varol</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Repairing Adversarial Texts through Perturbation. (arXiv:2201.02504v1 [cs.CL])","link":"http://arxiv.org/abs/2201.02504","description":"<p>It is known that neural networks are subject to attacks through adversarial\nperturbations, i.e., inputs which are maliciously crafted through perturbations\nto induce wrong predictions. Furthermore, such attacks are impossible to\neliminate, i.e., the adversarial perturbation is still possible after applying\nmitigation methods such as adversarial training. Multiple approaches have been\ndeveloped to detect and reject such adversarial inputs, mostly in the image\ndomain. Rejecting suspicious inputs however may not be always feasible or\nideal. First, normal inputs may be rejected due to false alarms generated by\nthe detection algorithm. Second, denial-of-service attacks may be conducted by\nfeeding such systems with adversarial inputs. To address the gap, in this work,\nwe propose an approach to automatically repair adversarial texts at runtime.\nGiven a text which is suspected to be adversarial, we novelly apply multiple\nadversarial perturbation methods in a positive way to identify a repair, i.e.,\na slightly mutated but semantically equivalent text that the neural network\ncorrectly classifies. Our approach has been experimented with multiple models\ntrained for natural language processing tasks and the results show that our\napproach is effective, i.e., it successfully repairs about 80\\% of the\nadversarial texts. Furthermore, depending on the applied perturbation method,\nan adversarial text could be repaired in as short as one second on average.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_G/0/1/0/all/0/1\">Guoliang Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jun Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chattopadhyay_S/0/1/0/all/0/1\">Sudipta Chattopadhyay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_T/0/1/0/all/0/1\">Ting Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jie Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Jin Song Dong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predicting Patient Readmission Risk from Medical Text via Knowledge Graph Enhanced Multiview Graph Convolution. (arXiv:2201.02510v1 [cs.CL])","link":"http://arxiv.org/abs/2201.02510","description":"<p>Unplanned intensive care unit (ICU) readmission rate is an important metric\nfor evaluating the quality of hospital care. Efficient and accurate prediction\nof ICU readmission risk can not only help prevent patients from inappropriate\ndischarge and potential dangers, but also reduce associated costs of\nhealthcare. In this paper, we propose a new method that uses medical text of\nElectronic Health Records (EHRs) for prediction, which provides an alternative\nperspective to previous studies that heavily depend on numerical and\ntime-series features of patients. More specifically, we extract discharge\nsummaries of patients from their EHRs, and represent them with multiview graphs\nenhanced by an external knowledge graph. Graph convolutional networks are then\nused for representation learning. Experimental results prove the effectiveness\nof our method, yielding state-of-the-art performance for this task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_Q/0/1/0/all/0/1\">Qiuhao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Thien Huu Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_D/0/1/0/all/0/1\">Dejing Dou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RxWhyQA: a clinical question-answering dataset with the challenge of multi-answer questions. (arXiv:2201.02517v1 [cs.CL])","link":"http://arxiv.org/abs/2201.02517","description":"<p>Objectives Create a dataset for the development and evaluation of clinical\nquestion-answering (QA) systems that can handle multi-answer questions.\nMaterials and Methods We leveraged the annotated relations from the 2018\nNational NLP Clinical Challenges (n2c2) corpus to generate a QA dataset. The\n1-to-0 and 1-to-N drug-reason relations formed the unanswerable and\nmulti-answer entries, which represent challenging scenarios lacking in the\nexisting clinical QA datasets. Results The result RxWhyQA dataset contains\n91,440 QA entries, of which half are unanswerable, and 21% (n=19,269) of the\nanswerable ones require multiple answers. The dataset conforms to the\ncommunity-vetted Stanford Question Answering Dataset (SQuAD) format. Discussion\nThe RxWhyQA is useful for comparing different systems that need to handle the\nzero- and multi-answer challenges, demanding dual mitigation of both false\npositive and false negative answers. Conclusion We created and shared a\nclinical QA dataset with a focus on multi-answer questions to represent\nreal-world scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moon_S/0/1/0/all/0/1\">Sungrim Moon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Huan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hongfang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1\">Jungwei W. Fan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Code-Switching Text Augmentation for Multilingual Speech Processing. (arXiv:2201.02550v1 [cs.CL])","link":"http://arxiv.org/abs/2201.02550","description":"<p>The pervasiveness of intra-utterance Code-switching (CS) in spoken content\nhas enforced ASR systems to handle mixed input. Yet, designing a CS-ASR has\nmany challenges, mainly due to the data scarcity, grammatical structure\ncomplexity, and mismatch along with unbalanced language usage distribution.\nRecent ASR studies showed the predominance of E2E-ASR using multilingual data\nto handle CS phenomena with little CS data. However, the dependency on the CS\ndata still remains. In this work, we propose a methodology to augment the\nmonolingual data for artificially generating spoken CS text to improve\ndifferent speech modules. We based our approach on Equivalence Constraint\ntheory while exploiting aligned translation pairs, to generate grammatically\nvalid CS content. Our empirical results show a relative gain of 29-34 % in\nperplexity and around 2% in WER for two ecological and noisy CS test sets.\nFinally, the human evaluation suggests that 83.8% of the generated data is\nacceptable to humans.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hussein_A/0/1/0/all/0/1\">Amir Hussein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1\">Shammur Absar Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdelali_A/0/1/0/all/0/1\">Ahmed Abdelali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dehak_N/0/1/0/all/0/1\">Najim Dehak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ali_A/0/1/0/all/0/1\">Ahmed Ali</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Assigning function to protein-protein interactions: a weakly supervised BioBERT based approach using PubMed abstracts. (arXiv:2008.08727v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2008.08727","description":"<p>Motivation: Protein-protein interactions (PPI) are critical to the function\nof proteins in both normal and diseased cells, and many critical protein\nfunctions are mediated by interactions.Knowledge of the nature of these\ninteractions is important for the construction of networks to analyse\nbiological data. However, only a small percentage of PPIs captured in protein\ninteraction databases have annotations of function available, e.g. only 4% of\nPPI are functionally annotated in the IntAct database. Here, we aim to label\nthe function type of PPIs by extracting relationships described in PubMed\nabstracts.\n</p>\n<p>Method: We create a weakly supervised dataset from the IntAct PPI database\ncontaining interacting protein pairs with annotated function and associated\nabstracts from the PubMed database. We apply a state-of-the-art deep learning\ntechnique for biomedical natural language processing tasks, BioBERT, to build a\nmodel - dubbed PPI-BioBERT - for identifying the function of PPIs. In order to\nextract high quality PPI functions at large scale, we use an ensemble of\nPPI-BioBERT models to improve uncertainty estimation and apply an interaction\ntype-specific threshold to counteract the effects of variations in the number\nof training samples per interaction type.\n</p>\n<p>Results: We scan 18 million PubMed abstracts to automatically identify 3253\nnew typed PPIs, including phosphorylation and acetylation interactions, with an\noverall precision of 46% (87% for acetylation) based on a human-reviewed\nsample. This work demonstrates that analysis of biomedical abstracts for PPI\nfunction extraction is a feasible approach to substantially increasing the\nnumber of interactions annotated with function captured in online databases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Elangovan_A/0/1/0/all/0/1\">Aparna Elangovan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davis_M/0/1/0/all/0/1\">Melissa Davis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verspoor_K/0/1/0/all/0/1\">Karin Verspoor</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Monitoring Covid-19 on social media using a novel triage and diagnosis approach. (arXiv:2103.11850v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.11850","description":"<p>Objective: This study aims to develop an end-to-end natural language\nprocessing pipeline for triage and diagnosis of COVID-19 from patient-authored\nsocial media posts, in order to provide researchers and public health\npractitioners with additional information on the symptoms, severity and\nprevalence of the disease rather than to provide an actionable decision at the\nindividual level. Materials and Methods: The text processing pipeline first\nextracts COVID-19 symptoms and related concepts such as severity, duration,\nnegations, and body parts from patients' posts using conditional random fields.\nAn unsupervised rule-based algorithm is then applied to establish relations\nbetween concepts in the next step of the pipeline. The extracted concepts and\nrelations are subsequently used to construct two different vector\nrepresentations of each post. These vectors are applied separately to build\nsupport vector machine learning models to triage patients into three categories\nand diagnose them for COVID-19. Results: We report that macro- and\nmicro-averaged F1 scores in the range of 71-96% and 61-87%, respectively, for\nthe triage and diagnosis of COVID-19, when the models are trained on human\nlabelled data. Our experimental results indicate that similar performance can\nbe achieved when the models are trained using predicted labels from concept\nextraction and rule-based classifiers, thus yielding end-to-end machine\nlearning. Also, we highlight important features uncovered by our diagnostic\nmachine learning models and compare them with the most frequent symptoms\nrevealed in another COVID-19 dataset. In particular, we found that the most\nimportant features are not always the most frequent ones.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hasan_A/0/1/0/all/0/1\">Abul Hasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levene_M/0/1/0/all/0/1\">Mark Levene</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weston_D/0/1/0/all/0/1\">David Weston</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fromson_R/0/1/0/all/0/1\">Renate Fromson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koslover_N/0/1/0/all/0/1\">Nicolas Koslover</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levene_T/0/1/0/all/0/1\">Tamara Levene</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Toolbox for Construction and Analysis of Speech Datasets. (arXiv:2104.04896v3 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2104.04896","description":"<p>Automatic Speech Recognition and Text-to-Speech systems are primarily trained\nin a supervised fashion and require high-quality, accurately labeled speech\ndatasets. In this work, we examine common problems with speech data and\nintroduce a toolbox for the construction and interactive error analysis of\nspeech datasets. The construction tool is based on K\\\"urzinger et al. work,\nand, to the best of our knowledge, the dataset exploration tool is the world's\nfirst open-source tool of this kind. We demonstrate how to apply these tools to\ncreate a Russian speech dataset and analyze existing speech datasets\n(Multilingual LibriSpeech, Mozilla Common Voice). The tools are open sourced as\na part of the NeMo framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bakhturina_E/0/1/0/all/0/1\">Evelina Bakhturina</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lavrukhin_V/0/1/0/all/0/1\">Vitaly Lavrukhin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ginsburg_B/0/1/0/all/0/1\">Boris Ginsburg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Audiomer: A Convolutional Transformer For Keyword Spotting. (arXiv:2109.10252v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2109.10252","description":"<p>Transformers have seen an unprecedented rise in Natural Language Processing\nand Computer Vision tasks. However, in audio tasks, they are either infeasible\nto train due to extremely large sequence length of audio waveforms or incur a\nperformance penalty when trained on Fourier-based features. In this work, we\nintroduce an architecture, Audiomer, where we combine 1D Residual Networks with\nPerformer Attention to achieve state-of-the-art performance in keyword spotting\nwith raw audio waveforms, outperforming all previous methods while being\ncomputationally cheaper and parameter-efficient. Additionally, our model has\npractical advantages for speech processing, such as inference on arbitrarily\nlong audio clips owing to the absence of positional encoding. The code is\navailable at https://github.com/The-Learning-Machines/Audiomer-PyTorch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sahu_S/0/1/0/all/0/1\">Surya Kant Sahu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitheran_S/0/1/0/all/0/1\">Sai Mitheran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamdar_J/0/1/0/all/0/1\">Juhi Kamdar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gandhi_M/0/1/0/all/0/1\">Meet Gandhi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Multimodal Language Representations using Convolutional Autoencoders. (arXiv:2110.03007v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.03007","description":"<p>Multimodal Language Analysis is a demanding area of research, since it is\nassociated with two requirements: combining different modalities and capturing\ntemporal information. During the last years, several works have been proposed\nin the area, mostly centered around supervised learning in downstream tasks. In\nthis paper we propose extracting unsupervised Multimodal Language\nrepresentations that are universal and can be applied to different tasks.\nTowards this end, we map the word-level aligned multimodal sequences to 2-D\nmatrices and then use Convolutional Autoencoders to learn embeddings by\ncombining multiple datasets. Extensive experimentation on Sentiment Analysis\n(MOSEI) and Emotion Recognition (IEMOCAP) indicate that the learned\nrepresentations can achieve near-state-of-the-art performance with just the use\nof a Logistic Regression algorithm for downstream classification. It is also\nshown that our method is extremely lightweight and can be easily generalized to\nother tasks and unseen data with small performance drop and almost the same\nnumber of parameters. The proposed multimodal representation models are\nopen-sourced and will help grow the applicability of Multimodal Language.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Koromilas_P/0/1/0/all/0/1\">Panagiotis Koromilas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giannakopoulos_T/0/1/0/all/0/1\">Theodoros Giannakopoulos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ASCEND: A Spontaneous Chinese-English Dataset for Code-switching in Multi-turn Conversation. (arXiv:2112.06223v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.06223","description":"<p>Code-switching is a speech phenomenon when a speaker switches language during\na conversation. Despite the spontaneous nature of code-switching in\nconversational spoken language, most existing works collect code-switching data\nthrough read speech instead of spontaneous speech. ASCEND (A Spontaneous\nChinese-English Dataset) introduces a high-quality resource of spontaneous\nmulti-turn conversational dialogue Chinese-English code-switching corpus\ncollected in Hong Kong. We report ASCEND's design and procedure of collecting\nthe speech data, including the annotations in this work. ASCEND includes 23\nbilinguals that are fluent in both Chinese and English and consists of 10.62\nhours clean speech corpus. We also conduct a baseline experiment using\npre-trained wav2vec 2.0 models, achieving the best performance of 22.69%\ncharacter error rate and 27.05% mixed error rate.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lovenia_H/0/1/0/all/0/1\">Holy Lovenia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cahyawijaya_S/0/1/0/all/0/1\">Samuel Cahyawijaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winata_G/0/1/0/all/0/1\">Genta Indra Winata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1\">Peng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1\">Xu Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zihan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frieske_R/0/1/0/all/0/1\">Rita Frieske</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tiezheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1\">Wenliang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barezi_E/0/1/0/all/0/1\">Elham J. Barezi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qifeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaojuan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1\">Bertram E. Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1\">Pascale Fung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Consistency and Coherence from Points of Contextual Similarity. (arXiv:2112.11638v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.11638","description":"<p>Factual consistency is one of important summary evaluation dimensions,\nespecially as summary generation becomes more fluent and coherent. The ESTIME\nmeasure, recently proposed specifically for factual consistency, achieves high\ncorrelations with human expert scores both for consistency and fluency, while\nin principle being restricted to evaluating such text-summary pairs that have\nhigh dictionary overlap. This is not a problem for current styles of\nsummarization, but it may become an obstacle for future summarization systems,\nor for evaluating arbitrary claims against the text. In this work we generalize\nthe method, and make a variant of the measure applicable to any text-summary\npairs. As ESTIME uses points of contextual similarity, it provides insights\ninto usefulness of information taken from different BERT layers. We observe\nthat useful information exists in almost all of the layers except the several\nlowest ones. For consistency and fluency - qualities focused on local text\ndetails - the most useful layers are close to the top (but not at the top); for\ncoherence and relevance we found a more complicated and interesting picture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vasilyev_O/0/1/0/all/0/1\">Oleg Vasilyev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bohannon_J/0/1/0/all/0/1\">John Bohannon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-01-09T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/","dc":"http://purl.org/dc/elements/1.1/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"3D Intracranial Aneurysm Classification and Segmentation via Unsupervised Dual-branch Learning. (arXiv:2201.02198v1 [eess.IV])","link":"http://arxiv.org/abs/2201.02198","description":"<p>Intracranial aneurysms are common nowadays and how to detect them\nintelligently is of great significance in digital health. While most existing\ndeep learning research focused on medical images in a supervised way, we\nintroduce an unsupervised method for the detection of intracranial aneurysms\nbased on 3D point cloud data. In particular, our method consists of two stages:\nunsupervised pre-training and downstream tasks. As for the former, the main\nidea is to pair each point cloud with its jittered counterpart and maximise\ntheir correspondence. Then we design a dual-branch contrastive network with an\nencoder for each branch and a subsequent common projection head. As for the\nlatter, we design simple networks for supervised classification and\nsegmentation training. Experiments on the public dataset (IntrA) show that our\nunsupervised method achieves comparable or even better performance than some\nstate-of-the-art supervised techniques, and it is most prominent in the\ndetection of aneurysmal vessels. Experiments on the ModelNet40 also show that\nour method achieves the accuracy of 90.79\\% which outperforms existing\nstate-of-the-art unsupervised models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Shao_D/0/1/0/all/0/1\">Di Shao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_X/0/1/0/all/0/1\">Xuequan Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Consistent Style Transfer. (arXiv:2201.02233v1 [cs.CV])","link":"http://arxiv.org/abs/2201.02233","description":"<p>Recently, attentional arbitrary style transfer methods have been proposed to\nachieve fine-grained results, which manipulates the point-wise similarity\nbetween content and style features for stylization. However, the attention\nmechanism based on feature points ignores the feature multi-manifold\ndistribution, where each feature manifold corresponds to a semantic region in\nthe image. Consequently, a uniform content semantic region is rendered by\nhighly different patterns from various style semantic regions, producing\ninconsistent stylization results with visual artifacts. We proposed the\nprogressive attentional manifold alignment (PAMA) to alleviate this problem,\nwhich repeatedly applies attention operations and space-aware interpolations.\nThe attention operation rearranges style features dynamically according to the\nspatial distribution of content features. This makes the content and style\nmanifolds correspond on the feature map. Then the space-aware interpolation\nadaptively interpolates between the corresponding content and style manifolds\nto increase their similarity. By gradually aligning the content manifolds to\nstyle manifolds, the proposed PAMA achieves state-of-the-art performance while\navoiding the inconsistency of semantic regions. Codes are available at\nhttps://github.com/computer-vision2022/PAMA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xuan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1\">Zhen Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lingkang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lingling Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Keypoint Detection and Description Network Based on the Vessel Structure for Multi-Modal Retinal Image Registration. (arXiv:2201.02242v1 [eess.IV])","link":"http://arxiv.org/abs/2201.02242","description":"<p>Ophthalmological imaging utilizes different imaging systems, such as color\nfundus, infrared, fluorescein angiography, optical coherence tomography (OCT)\nor OCT angiography. Multiple images with different modalities or acquisition\ntimes are often analyzed for the diagnosis of retinal diseases. Automatically\naligning the vessel structures in the images by means of multi-modal\nregistration can support the ophthalmologists in their work. Our method uses a\nconvolutional neural network to extract features of the vessel structure in\nmulti-modal retinal images. We jointly train a keypoint detection and\ndescription network on small patches using a classification and a cross-modal\ndescriptor loss function and apply the network to the full image size in the\ntest phase. Our method demonstrates the best registration performance on our\nand a public multi-modal dataset in comparison to competing methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Sindel_A/0/1/0/all/0/1\">Aline Sindel</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Hohberger_B/0/1/0/all/0/1\">Bettina Hohberger</a> (2), <a href=\"http://arxiv.org/find/eess/1/au:+Dehcordi_S/0/1/0/all/0/1\">Sebastian Fassihi Dehcordi</a> (2), <a href=\"http://arxiv.org/find/eess/1/au:+Mardin_C/0/1/0/all/0/1\">Christian Mardin</a> (2), <a href=\"http://arxiv.org/find/eess/1/au:+Lammer_R/0/1/0/all/0/1\">Robert L&#xe4;mmer</a> (2), <a href=\"http://arxiv.org/find/eess/1/au:+Maier_A/0/1/0/all/0/1\">Andreas Maier</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Christlein_V/0/1/0/all/0/1\">Vincent Christlein</a> (1) ((1) Pattern Recognition Lab, FAU Erlangen-N&#xfc;rnberg, (2) Department of Ophthalmology, Universit&#xe4;tsklinikum Erlangen)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CitySurfaces: City-Scale Semantic Segmentation of Sidewalk Materials. (arXiv:2201.02260v1 [cs.CV])","link":"http://arxiv.org/abs/2201.02260","description":"<p>While designing sustainable and resilient urban built environment is\nincreasingly promoted around the world, significant data gaps have made\nresearch on pressing sustainability issues challenging to carry out. Pavements\nare known to have strong economic and environmental impacts; however, most\ncities lack a spatial catalog of their surfaces due to the cost-prohibitive and\ntime-consuming nature of data collection. Recent advancements in computer\nvision, together with the availability of street-level images, provide new\nopportunities for cities to extract large-scale built environment data with\nlower implementation costs and higher accuracy. In this paper, we propose\nCitySurfaces, an active learning-based framework that leverages computer vision\ntechniques for classifying sidewalk materials using widely available\nstreet-level images. We trained the framework on images from New York City and\nBoston and the evaluation results show a 90.5% mIoU score. Furthermore, we\nevaluated the framework using images from six different cities, demonstrating\nthat it can be applied to regions with distinct urban fabrics, even outside the\ndomain of the training data. CitySurfaces can provide researchers and city\nagencies with a low-cost, accurate, and extensible method to collect sidewalk\nmaterial data which plays a critical role in addressing major sustainability\nissues, including climate change and surface water management.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hosseini_M/0/1/0/all/0/1\">Maryam Hosseini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miranda_F/0/1/0/all/0/1\">Fabio Miranda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jianzhe Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_C/0/1/0/all/0/1\">Claudio Silva</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ITSA: An Information-Theoretic Approach to Automatic Shortcut Avoidance and Domain Generalization in Stereo Matching Networks. (arXiv:2201.02263v1 [cs.CV])","link":"http://arxiv.org/abs/2201.02263","description":"<p>State-of-the-art stereo matching networks trained only on synthetic data\noften fail to generalize to more challenging real data domains. In this paper,\nwe attempt to unfold an important factor that hinders the networks from\ngeneralizing across domains: through the lens of shortcut learning. We\ndemonstrate that the learning of feature representations in stereo matching\nnetworks is heavily influenced by synthetic data artefacts (shortcut\nattributes). To mitigate this issue, we propose an Information-Theoretic\nShortcut Avoidance~(ITSA) approach to automatically restrict shortcut-related\ninformation from being encoded into the feature representations. As a result,\nour proposed method learns robust and shortcut-invariant features by minimizing\nthe sensitivity of latent features to input variations. To avoid the\nprohibitive computational cost of direct input sensitivity optimization, we\npropose an effective yet feasible algorithm to achieve robustness. We show that\nusing this method, state-of-the-art stereo matching networks that are trained\npurely on synthetic data can effectively generalize to challenging and\npreviously unseen real data scenarios. Importantly, the proposed method\nenhances the robustness of the synthetic trained networks to the point that\nthey outperform their fine-tuned counterparts (on real data) for challenging\nout-of-domain stereo datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chuah_W/0/1/0/all/0/1\">WeiQin Chuah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tennakoon_R/0/1/0/all/0/1\">Ruwan Tennakoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoseinnezhad_R/0/1/0/all/0/1\">Reza Hoseinnezhad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bab_Hadiashar_A/0/1/0/all/0/1\">Alireza Bab-Hadiashar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suter_D/0/1/0/all/0/1\">David Suter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"De-rendering 3D Objects in the Wild. (arXiv:2201.02279v1 [cs.CV])","link":"http://arxiv.org/abs/2201.02279","description":"<p>With increasing focus on augmented and virtual reality applications (XR)\ncomes the demand for algorithms that can lift objects from images and videos\ninto representations that are suitable for a wide variety of related 3D tasks.\nLarge-scale deployment of XR devices and applications means that we cannot\nsolely rely on supervised learning, as collecting and annotating data for the\nunlimited variety of objects in the real world is infeasible. We present a\nweakly supervised method that is able to decompose a single image of an object\ninto shape (depth and normals), material (albedo, reflectivity and shininess)\nand global lighting parameters. For training, the method only relies on a rough\ninitial shape estimate of the training objects to bootstrap the learning\nprocess. This shape supervision can come for example from a pretrained depth\nnetwork or - more generically - from a traditional structure-from-motion\npipeline. In our experiments, we show that the method can successfully\nde-render 2D images into a decomposed 3D representation and generalizes to\nunseen object categories. Since in-the-wild evaluation is difficult due to the\nlack of ground truth data, we also introduce a photo-realistic synthetic test\nset that allows for quantitative evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wimbauer_F/0/1/0/all/0/1\">Felix Wimbauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shangzhe Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rupprecht_C/0/1/0/all/0/1\">Christian Rupprecht</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Repurposing Existing Deep Networks for Caption and Aesthetic-Guided Image Cropping. (arXiv:2201.02280v1 [cs.CV])","link":"http://arxiv.org/abs/2201.02280","description":"<p>We propose a novel optimization framework that crops a given image based on\nuser description and aesthetics. Unlike existing image cropping methods, where\none typically trains a deep network to regress to crop parameters or cropping\nactions, we propose to directly optimize for the cropping parameters by\nrepurposing pre-trained networks on image captioning and aesthetic tasks,\nwithout any fine-tuning, thereby avoiding training a separate network.\nSpecifically, we search for the best crop parameters that minimize a combined\nloss of the initial objectives of these networks. To make the optimization\ntable, we propose three strategies: (i) multi-scale bilinear sampling, (ii)\nannealing the scale of the crop region, therefore effectively reducing the\nparameter space, (iii) aggregation of multiple optimization results. Through\nvarious quantitative and qualitative evaluations, we show that our framework\ncan produce crops that are well-aligned to intended user descriptions and\naesthetically pleasing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Horanyi_N/0/1/0/all/0/1\">Nora Horanyi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_K/0/1/0/all/0/1\">Kedi Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_K/0/1/0/all/0/1\">Kwang Moo Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bojja_A/0/1/0/all/0/1\">Abhishake Kumar Bojja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leonardis_A/0/1/0/all/0/1\">Ales Leonardis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Hyung Jin Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Persistent Homology for Breast Tumor Classification using Mammogram Scans. (arXiv:2201.02295v1 [eess.IV])","link":"http://arxiv.org/abs/2201.02295","description":"<p>An Important tool in the field topological data analysis is known as\npersistent Homology (PH) which is used to encode abstract representation of the\nhomology of data at different resolutions in the form of persistence diagram\n(PD). In this work we build more than one PD representation of a single image\nbased on a landmark selection method, known as local binary patterns, that\nencode different types of local textures from images. We employed different PD\nvectorizations using persistence landscapes, persistence images, persistence\nbinning (Betti Curve) and statistics. We tested the effectiveness of proposed\nlandmark based PH on two publicly available breast abnormality detection\ndatasets using mammogram scans. Sensitivity of landmark based PH obtained is\nover 90% in both datasets for the detection of abnormal breast scans. Finally,\nexperimental results give new insights on using different types of PD\nvectorizations which help in utilising PH in conjunction with machine learning\nclassifiers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Asaad_A/0/1/0/all/0/1\">Aras Asaad</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ali_D/0/1/0/all/0/1\">Dashti Ali</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Majeed_T/0/1/0/all/0/1\">Taban Majeed</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rashid_R/0/1/0/all/0/1\">Rasber Rashid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extending One-Stage Detection with Open-World Proposals. (arXiv:2201.02302v1 [cs.CV])","link":"http://arxiv.org/abs/2201.02302","description":"<p>In many applications, such as autonomous driving, hand manipulation, or robot\nnavigation, object detection methods must be able to detect objects unseen in\nthe training set. Open World Detection(OWD) seeks to tackle this problem by\ngeneralizing detection performance to seen and unseen class categories. Recent\nworks have seen success in the generation of class-agnostic proposals, which we\ncall Open-World Proposals(OWP), but this comes at the cost of a big drop on the\nclassification task when both tasks are considered in the detection model.\nThese works have investigated two-stage Region Proposal Networks (RPN) by\ntaking advantage of objectness scoring cues; however, for its simplicity,\nrun-time, and decoupling of localization and classification, we investigate OWP\nthrough the lens of fully convolutional one-stage detection network, such as\nFCOS. We show that our architectural and sampling optimizations on FCOS can\nincrease OWP performance by as much as 6% in recall on novel classes, marking\nthe first proposal-free one-stage detection network to achieve comparable\nperformance to RPN-based two-stage networks. Furthermore, we show that the\ninherent, decoupled architecture of FCOS has benefits to retaining\nclassification performance. While two-stage methods worsen by 6% in recall on\nnovel classes, we show that FCOS only drops 2% when jointly optimizing for OWP\nand classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Konan_S/0/1/0/all/0/1\">Sachin Konan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_K/0/1/0/all/0/1\">Kevin J Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_L/0/1/0/all/0/1\">Li Yin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Budget-aware Few-shot Learning via Graph Convolutional Network. (arXiv:2201.02304v1 [cs.CV])","link":"http://arxiv.org/abs/2201.02304","description":"<p>This paper tackles the problem of few-shot learning, which aims to learn new\nvisual concepts from a few examples. A common problem setting in few-shot\nclassification assumes random sampling strategy in acquiring data labels, which\nis inefficient in practical applications. In this work, we introduce a new\nbudget-aware few-shot learning problem that not only aims to learn novel object\ncategories, but also needs to select informative examples to annotate in order\nto achieve data efficiency.\n</p>\n<p>We develop a meta-learning strategy for our budget-aware few-shot learning\ntask, which jointly learns a novel data selection policy based on a Graph\nConvolutional Network (GCN) and an example-based few-shot classifier. Our\nselection policy computes a context-sensitive representation for each unlabeled\ndata by graph message passing, which is then used to predict an informativeness\nscore for sequential selection. We validate our method by extensive experiments\non the mini-ImageNet, tiered-ImageNet and Omniglot datasets. The results show\nour few-shot learning strategy outperforms baselines by a sizable margin, which\ndemonstrates the efficacy of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Shipeng Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Songyang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xuming He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A three-dimensional dual-domain deep network for high-pitch and sparse helical CT reconstruction. (arXiv:2201.02309v1 [eess.IV])","link":"http://arxiv.org/abs/2201.02309","description":"<p>In this paper, we propose a new GPU implementation of the Katsevich algorithm\nfor helical CT reconstruction. Our implementation divides the sinograms and\nreconstructs the CT images pitch by pitch. By utilizing the periodic properties\nof the parameters of the Katsevich algorithm, our method only needs to\ncalculate these parameters once for all the pitches and so has lower GPU-memory\nburdens and is very suitable for deep learning. By embedding our implementation\ninto the network, we propose an end-to-end deep network for the high pitch\nhelical CT reconstruction with sparse detectors. Since our network utilizes the\nfeatures extracted from both sinograms and CT images, it can simultaneously\nreduce the streak artifacts caused by the sparsity of sinograms and preserve\nfine details in the CT images. Experiments show that our network outperforms\nthe related methods both in subjective and objective evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xia_X/0/1/0/all/0/1\">Xiang-Gen Xia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+He_C/0/1/0/all/0/1\">Chuanjiang He</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ren_Z/0/1/0/all/0/1\">Zemin Ren</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_J/0/1/0/all/0/1\">Jian Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RestoreDet: Degradation Equivariant Representation for Object Detection in Low Resolution Images. (arXiv:2201.02314v1 [eess.IV])","link":"http://arxiv.org/abs/2201.02314","description":"<p>Image restoration algorithms such as super resolution (SR) are indispensable\npre-processing modules for object detection in degraded images. However, most\nof these algorithms assume the degradation is fixed and known a priori. When\nthe real degradation is unknown or differs from assumption, both the\npre-processing module and the consequent high-level task such as object\ndetection would fail. Here, we propose a novel framework, RestoreDet, to detect\nobjects in degraded low resolution images. RestoreDet utilizes the downsampling\ndegradation as a kind of transformation for self-supervised signals to explore\nthe equivariant representation against various resolutions and other\ndegradation conditions. Specifically, we learn this intrinsic visual structure\nby encoding and decoding the degradation transformation from a pair of original\nand randomly degraded images. The framework could further take the advantage of\nadvanced SR architectures with an arbitrary resolution restoring decoder to\nreconstruct the original correspondence from the degraded input image. Both the\nrepresentation learning and object detection are optimized jointly in an\nend-to-end training fashion. RestoreDet is a generic framework that could be\nimplemented on any mainstream object detection architectures. The extensive\nexperiment shows that our framework based on CenterNet has achieved superior\nperformance compared with existing methods when facing variant degradation\nsituations. Our code would be released soon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Cui_Z/0/1/0/all/0/1\">Ziteng Cui</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_Y/0/1/0/all/0/1\">Yingying Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gu_L/0/1/0/all/0/1\">Lin Gu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qi_G/0/1/0/all/0/1\">Guo-Jun Qi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1\">Xiaoxiao Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gao_P/0/1/0/all/0/1\">Peng Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Z/0/1/0/all/0/1\">Zenghui Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Harada_T/0/1/0/all/0/1\">Tatsuya Harada</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multiresolution Fully Convolutional Networks to detect Clouds and Snow through Optical Satellite Images. (arXiv:2201.02350v1 [eess.IV])","link":"http://arxiv.org/abs/2201.02350","description":"<p>Clouds and snow have similar spectral features in the visible and\nnear-infrared (VNIR) range and are thus difficult to distinguish from each\nother in high resolution VNIR images. We address this issue by introducing a\nshortwave-infrared (SWIR) band where clouds are highly reflective, and snow is\nabsorptive. As SWIR is typically of a lower resolution compared to VNIR, this\nstudy proposes a multiresolution fully convolutional neural network (FCN) that\ncan effectively detect clouds and snow in VNIR images. We fuse the\nmultiresolution bands within a deep FCN and perform semantic segmentation at\nthe higher, VNIR resolution. Such a fusion-based classifier, trained in an\nend-to-end manner, achieved 94.31% overall accuracy and an F1 score of 97.67%\nfor clouds on Resourcesat-2 data captured over the state of Uttarakhand, India.\nThese scores were found to be 30% higher than a Random Forest classifier, and\n10% higher than a standalone single-resolution FCN. Apart from being useful for\ncloud detection purposes, the study also highlights the potential of\nconvolutional neural networks for multi-sensor fusion problems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Varshney_D/0/1/0/all/0/1\">Debvrat Varshney</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Persello_C/0/1/0/all/0/1\">Claudio Persello</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gupta_P/0/1/0/all/0/1\">Prasun Kumar Gupta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nikam_B/0/1/0/all/0/1\">Bhaskar Ramachandra Nikam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Modality Deep Feature Learning for Brain Tumor Segmentation. (arXiv:2201.02356v1 [eess.IV])","link":"http://arxiv.org/abs/2201.02356","description":"<p>Recent advances in machine learning and prevalence of digital medical images\nhave opened up an opportunity to address the challenging brain tumor\nsegmentation (BTS) task by using deep convolutional neural networks. However,\ndifferent from the RGB image data that are very widespread, the medical image\ndata used in brain tumor segmentation are relatively scarce in terms of the\ndata scale but contain the richer information in terms of the modality\nproperty. To this end, this paper proposes a novel cross-modality deep feature\nlearning framework to segment brain tumors from the multi-modality MRI data.\nThe core idea is to mine rich patterns across the multi-modality data to make\nup for the insufficient data scale. The proposed cross-modality deep feature\nlearning framework consists of two learning processes: the cross-modality\nfeature transition (CMFT) process and the cross-modality feature fusion (CMFF)\nprocess, which aims at learning rich feature representations by transiting\nknowledge across different modality data and fusing knowledge from different\nmodality data, respectively. Comprehensive experiments are conducted on the\nBraTS benchmarks, which show that the proposed cross-modality deep feature\nlearning framework can effectively improve the brain tumor segmentation\nperformance when compared with the baseline methods and state-of-the-art\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_D/0/1/0/all/0/1\">Dingwen Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_G/0/1/0/all/0/1\">Guohai Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Q/0/1/0/all/0/1\">Qiang Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Han_J/0/1/0/all/0/1\">Jungong Han</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Han_J/0/1/0/all/0/1\">Junwei Han</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_Y/0/1/0/all/0/1\">Yizhou Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Motion Prediction via Joint Dependency Modeling in Phase Space. (arXiv:2201.02365v1 [cs.CV])","link":"http://arxiv.org/abs/2201.02365","description":"<p>Motion prediction is a classic problem in computer vision, which aims at\nforecasting future motion given the observed pose sequence. Various deep\nlearning models have been proposed, achieving state-of-the-art performance on\nmotion prediction. However, existing methods typically focus on modeling\ntemporal dynamics in the pose space. Unfortunately, the complicated and high\ndimensionality nature of human motion brings inherent challenges for dynamic\ncontext capturing. Therefore, we move away from the conventional pose based\nrepresentation and present a novel approach employing a phase space trajectory\nrepresentation of individual joints. Moreover, current methods tend to only\nconsider the dependencies between physically connected joints. In this paper,\nwe introduce a novel convolutional neural model to effectively leverage\nexplicit prior knowledge of motion anatomy, and simultaneously capture both\nspatial and temporal information of joint trajectory dynamics. We then propose\na global optimization module that learns the implicit relationships between\nindividual joint features.\n</p>\n<p>Empirically, our method is evaluated on large-scale 3D human motion benchmark\ndatasets (i.e., Human3.6M, CMU MoCap). These results demonstrate that our\nmethod sets the new state-of-the-art on the benchmark datasets. Our code will\nbe available at https://github.com/Pose-Group/TEID.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_P/0/1/0/all/0/1\">Pengxiang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhenguang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shuang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Lei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1\">Yifang Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xuanjing Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncertainty-Aware Cascaded Dilation Filtering for High-Efficiency Deraining. (arXiv:2201.02366v1 [cs.CV])","link":"http://arxiv.org/abs/2201.02366","description":"<p>Deraining is a significant and fundamental computer vision task, aiming to\nremove the rain streaks and accumulations in an image or video captured under a\nrainy day. Existing deraining methods usually make heuristic assumptions of the\nrain model, which compels them to employ complex optimization or iterative\nrefinement for high recovery quality. This, however, leads to time-consuming\nmethods and affects the effectiveness for addressing rain patterns deviated\nfrom from the assumptions. In this paper, we propose a simple yet efficient\nderaining method by formulating deraining as a predictive filtering problem\nwithout complex rain model assumptions. Specifically, we identify\nspatially-variant predictive filtering (SPFilt) that adaptively predicts proper\nkernels via a deep network to filter different individual pixels. Since the\nfiltering can be implemented via well-accelerated convolution, our method can\nbe significantly efficient. We further propose the EfDeRain+ that contains\nthree main contributions to address residual rain traces, multi-scale, and\ndiverse rain patterns without harming the efficiency. First, we propose the\nuncertainty-aware cascaded predictive filtering (UC-PFilt) that can identify\nthe difficulties of reconstructing clean pixels via predicted kernels and\nremove the residual rain traces effectively. Second, we design the\nweight-sharing multi-scale dilated filtering (WS-MS-DFilt) to handle\nmulti-scale rain streaks without harming the efficiency. Third, to eliminate\nthe gap across diverse rain patterns, we propose a novel data augmentation\nmethod (i.e., RainMix) to train our deep models. By combining all contributions\nwith sophisticated analysis on different variants, our final method outperforms\nbaseline methods on four single-image deraining datasets and one video\nderaining dataset in terms of both recovery quality and speed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1\">Qing Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jingyang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Juefei_Xu_F/0/1/0/all/0/1\">Felix Juefei-Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1\">Di Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1\">Wei Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Song Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Generative Framework for Interactive 3D Terrain Authoring and Manipulation. (arXiv:2201.02369v1 [cs.CV])","link":"http://arxiv.org/abs/2201.02369","description":"<p>Automated generation and (user) authoring of the realistic virtual terrain is\nmost sought for by the multimedia applications like VR models and gaming. The\nmost common representation adopted for terrain is Digital Elevation Model\n(DEM). Existing terrain authoring and modeling techniques have addressed some\nof these and can be broadly categorized as: procedural modeling, simulation\nmethod, and example-based methods. In this paper, we propose a novel realistic\nterrain authoring framework powered by a combination of VAE and generative\nconditional GAN model. Our framework is an example-based method that attempts\nto overcome the limitations of existing methods by learning a latent space from\na real-world terrain dataset. This latent space allows us to generate multiple\nvariants of terrain from a single input as well as interpolate between terrains\nwhile keeping the generated terrains close to real-world data distribution. We\nalso developed an interactive tool, that lets the user generate diverse\nterrains with minimalist inputs. We perform thorough qualitative and\nquantitative analysis and provide comparisons with other SOTA methods. We\nintend to release our code/tool to the academic community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Naik_S/0/1/0/all/0/1\">Shanthika Naik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Aryamaan Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Avinash Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajan_K/0/1/0/all/0/1\">KS Rajan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Human-to-Human-or-Object (H2O) Interactions with DIABOLO. (arXiv:2201.02396v1 [cs.CV])","link":"http://arxiv.org/abs/2201.02396","description":"<p>Detecting human interactions is crucial for human behavior analysis. Many\nmethods have been proposed to deal with Human-to-Object Interaction (HOI)\ndetection, i.e., detecting in an image which person and object interact\ntogether and classifying the type of interaction. However, Human-to-Human\nInteractions, such as social and violent interactions, are generally not\nconsidered in available HOI training datasets. As we think these types of\ninteractions cannot be ignored and decorrelated from HOI when analyzing human\nbehavior, we propose a new interaction dataset to deal with both types of human\ninteractions: Human-to-Human-or-Object (H2O). In addition, we introduce a novel\ntaxonomy of verbs, intended to be closer to a description of human body\nattitude in relation to the surrounding targets of interaction, and more\nindependent of the environment. Unlike some existing datasets, we strive to\navoid defining synonymous verbs when their use highly depends on the target\ntype or requires a high level of semantic interpretation. As H2O dataset\nincludes V-COCO images annotated with this new taxonomy, images obviously\ncontain more interactions. This can be an issue for HOI detection methods whose\ncomplexity depends on the number of people, targets or interactions. Thus, we\npropose DIABOLO (Detecting InterActions By Only Looking Once), an efficient\nsubject-centric single-shot method to detect all interactions in one forward\npass, with constant inference time independent of image content. In addition,\nthis multi-task network simultaneously detects all people and objects. We show\nhow sharing a network for these tasks does not only save computation resource\nbut also improves performance collaboratively. Finally, DIABOLO is a strong\nbaseline for the new proposed challenge of H2O Interaction detection, as it\noutperforms all state-of-the-art methods when trained and evaluated on HOI\ndataset V-COCO.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Orcesi_A/0/1/0/all/0/1\">Astrid Orcesi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Audigier_R/0/1/0/all/0/1\">Romaric Audigier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toukam_F/0/1/0/all/0/1\">Fritz Poka Toukam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luvison_B/0/1/0/all/0/1\">Bertrand Luvison</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Amplitude SAR Imagery Splicing Localization. (arXiv:2201.02409v1 [eess.IV])","link":"http://arxiv.org/abs/2201.02409","description":"<p>Synthetic Aperture Radar (SAR) images are a valuable asset for a wide variety\nof tasks. In the last few years, many websites have been offering them for free\nin the form of easy to manage products, favoring their widespread diffusion and\nresearch work in the SAR field. The drawback of these opportunities is that\nsuch images might be exposed to forgeries and manipulations by malicious users,\nraising new concerns about their integrity and trustworthiness. Up to now, the\nmultimedia forensics literature has proposed various techniques to localize\nmanipulations in natural photographs, but the integrity assessment of SAR\nimages was never investigated. This task poses new challenges, since SAR images\nare generated with a processing chain completely different from that of natural\nphotographs. This implies that many forensics methods developed for natural\nimages are not guaranteed to succeed. In this paper, we investigate the problem\nof amplitude SAR imagery splicing localization. Our goal is to localize regions\nof an amplitude SAR image that have been copied and pasted from another image,\npossibly undergoing some kind of editing in the process. To do so, we leverage\na Convolutional Neural Network (CNN) to extract a fingerprint highlighting\ninconsistencies in the processing traces of the analyzed input. Then, we\nexamine this fingerprint to produce a binary tampering mask indicating the\npixel region under splicing attack. Results show that our proposed method,\ntailored to the nature of SAR signals, provides better performances than\nstate-of-the-art forensic tools developed for natural images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Cannas_E/0/1/0/all/0/1\">Edoardo Daniele Cannas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bonettini_N/0/1/0/all/0/1\">Nicol&#xf2; Bonettini</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mandelli_S/0/1/0/all/0/1\">Sara Mandelli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bestagini_P/0/1/0/all/0/1\">Paolo Bestagini</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tubaro_S/0/1/0/all/0/1\">Stefano Tubaro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Auto-Weighted Layer Representation Based View Synthesis Distortion Estimation for 3-D Video Coding. (arXiv:2201.02420v1 [eess.IV])","link":"http://arxiv.org/abs/2201.02420","description":"<p>Recently, various view synthesis distortion estimation models have been\nstudied to better serve for 3-D video coding. However, they can hardly model\nthe relationship quantitatively among different levels of depth changes,\ntexture degeneration, and the view synthesis distortion (VSD), which is crucial\nfor rate-distortion optimization and rate allocation. In this paper, an\nauto-weighted layer representation based view synthesis distortion estimation\nmodel is developed. Firstly, the sub-VSD (S-VSD) is defined according to the\nlevel of depth changes and their associated texture degeneration. After that, a\nset of theoretical derivations demonstrate that the VSD can be approximately\ndecomposed into the S-VSDs multiplied by their associated weights. To obtain\nthe S-VSDs, a layer-based representation of S-VSD is developed, where all the\npixels with the same level of depth changes are represented with a layer to\nenable efficient S-VSD calculation at the layer level. Meanwhile, a nonlinear\nmapping function is learnt to accurately represent the relationship between the\nVSD and S-VSDs, automatically providing weights for S-VSDs during the VSD\nestimation. To learn such function, a dataset of VSD and its associated S-VSDs\nare built. Experimental results show that the VSD can be accurately estimated\nwith the weights learnt by the nonlinear mapping function once its associated\nS-VSDs are available. The proposed method outperforms the relevant\nstate-of-the-art methods in both accuracy and efficiency. The dataset and\nsource code of the proposed method will be available at\nhttps://github.com/jianjin008/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Jin_J/0/1/0/all/0/1\">Jian Jin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1\">Xingxing Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meng_L/0/1/0/all/0/1\">Lili Meng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_W/0/1/0/all/0/1\">Weisi Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liang_J/0/1/0/all/0/1\">Jie Liang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1\">Huaxiang Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_Y/0/1/0/all/0/1\">Yao Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Effect of Prior-based Losses on Segmentation Performance: A Benchmark. (arXiv:2201.02428v1 [eess.IV])","link":"http://arxiv.org/abs/2201.02428","description":"<p>Today, deep convolutional neural networks (CNNs) have demonstrated\nstate-of-the-art performance for medical image segmentation, on various imaging\nmodalities and tasks. Despite early success, segmentation networks may still\ngenerate anatomically aberrant segmentations, with holes or inaccuracies near\nthe object boundaries. To enforce anatomical plausibility, recent research\nstudies have focused on incorporating prior knowledge such as object shape or\nboundary, as constraints in the loss function. Prior integrated could be\nlow-level referring to reformulated representations extracted from the\nground-truth segmentations, or high-level representing external medical\ninformation such as the organ's shape or size. Over the past few years,\nprior-based losses exhibited a rising interest in the research field since they\nallow integration of expert knowledge while still being architecture-agnostic.\nHowever, given the diversity of prior-based losses on different medical imaging\nchallenges and tasks, it has become hard to identify what loss works best for\nwhich dataset. In this paper, we establish a benchmark of recent prior-based\nlosses for medical image segmentation. The main objective is to provide\nintuition onto which losses to choose given a particular task or dataset. To\nthis end, four low-level and high-level prior-based losses are selected. The\nconsidered losses are validated on 8 different datasets from a variety of\nmedical image segmentation challenges including the Decathlon, the ISLES and\nthe WMH challenge. Results show that whereas low-level prior-based losses can\nguarantee an increase in performance over the Dice loss baseline regardless of\nthe dataset characteristics, high-level prior-based losses can increase\nanatomical plausibility as per data characteristics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+JURDI%7D_R/0/1/0/all/0/1\">Rosana {EL JURDI}</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Petitjean_C/0/1/0/all/0/1\">Caroline Petitjean</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheplygina_V/0/1/0/all/0/1\">Veronika Cheplygina</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Honeine_P/0/1/0/all/0/1\">Paul Honeine</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Abdallah_F/0/1/0/all/0/1\">Fahed Abdallah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Negative Evidence Matters in Interpretable Histology Image Classification. (arXiv:2201.02445v1 [eess.IV])","link":"http://arxiv.org/abs/2201.02445","description":"<p>Using only global annotations such as the image class labels,\nweakly-supervised learning methods allow CNN classifiers to jointly classify an\nimage, and yield the regions of interest associated with the predicted class.\nHowever, without any guidance at the pixel level, such methods may yield\ninaccurate regions. This problem is known to be more challenging with histology\nimages than with natural ones, since objects are less salient, structures have\nmore variations, and foreground and background regions have stronger\nsimilarities. Therefore, methods in computer vision literature for visual\ninterpretation of CNNs may not directly apply. In this work, we propose a\nsimple yet efficient method based on a composite loss function that leverages\ninformation from the fully negative samples. Our new loss function contains two\ncomplementary terms: the first exploits positive evidence collected from the\nCNN classifier, while the second leverages the fully negative samples from the\ntraining dataset. In particular, we equip a pre-trained classifier with a\ndecoder that allows refining the regions of interest. The same classifier is\nexploited to collect both the positive and negative evidence at the pixel level\nto train the decoder. This enables to take advantages of the fully negative\nsamples that occurs naturally in the data, without any additional supervision\nsignals and using only the image class as supervision. Compared to several\nrecent related methods, over the public benchmark GlaS for colon cancer and a\nCamelyon16 patch-based benchmark for breast cancer using three different\nbackbones, we show the substantial improvements introduced by our method. Our\nresults shows the benefits of using both negative and positive evidence, ie,\nthe one obtained from a classifier and the one naturally available in datasets.\nWe provide an ablation study of both terms. Our code is publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Belharbi_S/0/1/0/all/0/1\">Soufiane Belharbi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pedersoli_M/0/1/0/all/0/1\">Marco Pedersoli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ayed_I/0/1/0/all/0/1\">Ismail Ben Ayed</a>, <a href=\"http://arxiv.org/find/eess/1/au:+McCaffrey_L/0/1/0/all/0/1\">Luke McCaffrey</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Granger_E/0/1/0/all/0/1\">Eric Granger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Domain Adversarial Adaptation for Photon-efficient Imaging Based on Spatiotemporal Inception Network. (arXiv:2201.02475v1 [eess.IV])","link":"http://arxiv.org/abs/2201.02475","description":"<p>In single-photon LiDAR, photon-efficient imaging captures the 3D structure of\na scene by only several detected signal photons per pixel. The existing deep\nlearning models for this task are trained on simulated datasets, which poses\nthe domain shift challenge when applied to realistic scenarios. In this paper,\nwe propose a spatiotemporal inception network (STIN) for photon-efficient\nimaging, which is able to precisely predict the depth from a sparse and\nhigh-noise photon counting histogram by fully exploiting spatial and temporal\ninformation. Then the domain adversarial adaptation frameworks, including\ndomain-adversarial neural network and adversarial discriminative domain\nadaptation, are effectively applied to STIN to alleviate the domain shift\nproblem for realistic applications. Comprehensive experiments on the simulated\ndata generated from the NYU~v2 and the Middlebury datasets demonstrate that\nSTIN outperforms the state-of-the-art models at low signal-to-background ratios\nfrom 2:10 to 2:100. Moreover, experimental results on the real-world dataset\ncaptured by the single-photon imaging prototype show that the STIN with domain\nadversarial training achieves better generalization performance compared with\nthe state-of-the-arts as well as the baseline STIN trained by simulated data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1\">Yiwei Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yao_G/0/1/0/all/0/1\">Gongxin Yao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yong Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pan_Y/0/1/0/all/0/1\">Yu Pan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bayesian Neural Networks for Reversible Steganography. (arXiv:2201.02478v1 [cs.LG])","link":"http://arxiv.org/abs/2201.02478","description":"<p>Recent advances in deep learning have led to a paradigm shift in reversible\nsteganography. A fundamental pillar of reversible steganography is predictive\nmodelling which can be realised via deep neural networks. However, non-trivial\nerrors exist in inferences about some out-of-distribution and noisy data. In\nview of this issue, we propose to consider uncertainty in predictive models\nbased upon a theoretical framework of Bayesian deep learning. Bayesian neural\nnetworks can be regarded as self-aware machinery; that is, a machine that knows\nits own limitations. To quantify uncertainty, we approximate the posterior\npredictive distribution through Monte Carlo sampling with stochastic forward\npasses. We further show that predictive uncertainty can be disentangled into\naleatoric and epistemic uncertainties and these quantities can be learnt in an\nunsupervised manner. Experimental results demonstrate an improvement delivered\nby Bayesian uncertainty analysis upon steganographic capacity-distortion\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1\">Ching-Chun Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video Summarization Based on Video-text Representation. (arXiv:2201.02494v1 [cs.CV])","link":"http://arxiv.org/abs/2201.02494","description":"<p>Modern video summarization methods are based on deep neural networks which\nrequire a large amount of annotated data for training. However, existing\ndatasets for video summarization are small-scale, easily leading to\nover-fitting of the deep models. Considering that the annotation of large-scale\ndatasets is time-consuming, we propose a multimodal self-supervised learning\nframework to obtain semantic representations of videos, which benefits the\nvideo summarization task. Specifically, we explore the semantic consistency\nbetween the visual information and text information of videos, for the\nself-supervised pretraining of a multimodal encoder on a newly-collected\ndataset of video-text pairs. Additionally, we introduce a progressive video\nsummarization method, where the important content in a video is pinpointed\nprogressively to generate better summaries. Finally, an objective evaluation\nframework is proposed to measure the quality of video summaries based on video\nclassification. Extensive experiments have proved the effectiveness and\nsuperiority of our method in rank correlation coefficients, F-score, and the\nproposed objective evaluation compared to the state of the art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Haopeng_L/0/1/0/all/0/1\">Li Haopeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiuhong_K/0/1/0/all/0/1\">Ke Qiuhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mingming_G/0/1/0/all/0/1\">Gong Mingming</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rui_Z/0/1/0/all/0/1\">Zhang Rui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sign Language Video Retrieval with Free-Form Textual Queries. (arXiv:2201.02495v1 [cs.CV])","link":"http://arxiv.org/abs/2201.02495","description":"<p>Systems that can efficiently search collections of sign language videos have\nbeen highlighted as a useful application of sign language technology. However,\nthe problem of searching videos beyond individual keywords has received limited\nattention in the literature. To address this gap, in this work we introduce the\ntask of sign language retrieval with free-form textual queries: given a written\nquery (e.g., a sentence) and a large collection of sign language videos, the\nobjective is to find the signing video in the collection that best matches the\nwritten query. We propose to tackle this task by learning cross-modal\nembeddings on the recently introduced large-scale How2Sign dataset of American\nSign Language (ASL). We identify that a key bottleneck in the performance of\nthe system is the quality of the sign video embedding which suffers from a\nscarcity of labeled training data. We, therefore, propose SPOT-ALIGN, a\nframework for interleaving iterative rounds of sign spotting and feature\nalignment to expand the scope and scale of available training data. We validate\nthe effectiveness of SPOT-ALIGN for learning a robust sign video embedding\nthrough improvements in both sign recognition and the proposed video retrieval\ntask.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Duarte_A/0/1/0/all/0/1\">Amanda Duarte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Albanie_S/0/1/0/all/0/1\">Samuel Albanie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giro_i_Nieto_X/0/1/0/all/0/1\">Xavier Gir&#xf3;-i-Nieto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varol_G/0/1/0/all/0/1\">G&#xfc;l Varol</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Review of Deep Learning Techniques for Markerless Human Motion on Synthetic Datasets. (arXiv:2201.02503v1 [cs.CV])","link":"http://arxiv.org/abs/2201.02503","description":"<p>Markerless motion capture has become an active field of research in computer\nvision in recent years. Its extensive applications are known in a great variety\nof fields, including computer animation, human motion analysis, biomedical\nresearch, virtual reality, and sports science. Estimating human posture has\nrecently gained increasing attention in the computer vision community, but due\nto the depth of uncertainty and the lack of the synthetic datasets, it is a\nchallenging task. Various approaches have recently been proposed to solve this\nproblem, many of which are based on deep learning. They are primarily focused\non improving the performance of existing benchmarks with significant advances,\nespecially 2D images. Based on powerful deep learning techniques and recently\ncollected real-world datasets, we explored a model that can predict the\nskeleton of an animation based solely on 2D images. Frames generated from\ndifferent real-world datasets with synthesized poses using different body\nshapes from simple to complex. The implementation process uses DeepLabCut on\nits own dataset to perform many necessary steps, then use the input frames to\ntrain the model. The output is an animated skeleton for human movement. The\ncomposite dataset and other results are the \"ground truth\" of the deep model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vo_D/0/1/0/all/0/1\">Doan Duy Vo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Butler_R/0/1/0/all/0/1\">Russell Butler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Target-aware Representation for Visual Tracking via Informative Interactions. (arXiv:2201.02526v1 [cs.CV])","link":"http://arxiv.org/abs/2201.02526","description":"<p>We introduce a novel backbone architecture to improve target-perception\nability of feature representation for tracking. Specifically, having observed\nthat de facto frameworks perform feature matching simply using the outputs from\nbackbone for target localization, there is no direct feedback from the matching\nmodule to the backbone network, especially the shallow layers. More concretely,\nonly the matching module can directly access the target information (in the\nreference frame), while the representation learning of candidate frame is blind\nto the reference target. As a consequence, the accumulation effect of\ntarget-irrelevant interference in the shallow stages may degrade the feature\nquality of deeper layers. In this paper, we approach the problem from a\ndifferent angle by conducting multiple branch-wise interactions inside the\nSiamese-like backbone networks (InBN). At the core of InBN is a general\ninteraction modeler (GIM) that injects the prior knowledge of reference image\nto different stages of the backbone network, leading to better\ntarget-perception and robust distractor-resistance of candidate feature\nrepresentation with negligible computation cost. The proposed GIM module and\nInBN mechanism are general and applicable to different backbone types including\nCNN and Transformer for improvements, as evidenced by our extensive experiments\non multiple benchmarks. In particular, the CNN version (based on SiamCAR)\nimproves the baseline with 3.2/6.9 absolute gains of SUC on LaSOT/TNL2K,\nrespectively. The Transformer version obtains SUC scores of 65.7/52.0 on\nLaSOT/TNL2K, which are on par with recent state of the arts. Code and models\nwill be released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1\">Mingzhe Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhipeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1\">Heng Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jing_L/0/1/0/all/0/1\">Liping Jing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Y/0/1/0/all/0/1\">Yilin Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Weiming Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NeROIC: Neural Rendering of Objects from Online Image Collections. (arXiv:2201.02533v1 [cs.CV])","link":"http://arxiv.org/abs/2201.02533","description":"<p>We present a novel method to acquire object representations from online image\ncollections, capturing high-quality geometry and material properties of\narbitrary objects from photographs with varying cameras, illumination, and\nbackgrounds. This enables various object-centric rendering applications such as\nnovel-view synthesis, relighting, and harmonized background composition from\nchallenging in-the-wild input. Using a multi-stage approach extending neural\nradiance fields, we first infer the surface geometry and refine the coarsely\nestimated initial camera parameters, while leveraging coarse foreground object\nmasks to improve the training efficiency and geometry quality. We also\nintroduce a robust normal estimation technique which eliminates the effect of\ngeometric noise while retaining crucial details. Lastly, we extract surface\nmaterial properties and ambient illumination, represented in spherical\nharmonics with extensions that handle transient elements, e.g. sharp shadows.\nThe union of these components results in a highly modular and efficient object\nacquisition framework. Extensive evaluations and comparisons demonstrate the\nadvantages of our approach in capturing high-quality geometry and appearance\nproperties useful for rendering applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kuang_Z/0/1/0/all/0/1\">Zhengfei Kuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olszewski_K/0/1/0/all/0/1\">Kyle Olszewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_M/0/1/0/all/0/1\">Menglei Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zeng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Achlioptas_P/0/1/0/all/0/1\">Panos Achlioptas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tulyakov_S/0/1/0/all/0/1\">Sergey Tulyakov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Novel Incremental Learning Driven Instance Segmentation Framework to Recognize Highly Cluttered Instances of the Contraband Items. (arXiv:2201.02560v1 [cs.CV])","link":"http://arxiv.org/abs/2201.02560","description":"<p>Screening cluttered and occluded contraband items from baggage X-ray scans is\na cumbersome task even for the expert security staff. This paper presents a\nnovel strategy that extends a conventional encoder-decoder architecture to\nperform instance-aware segmentation and extract merged instances of contraband\nitems without using any additional sub-network or an object detector. The\nencoder-decoder network first performs conventional semantic segmentation and\nretrieves cluttered baggage items. The model then incrementally evolves during\ntraining to recognize individual instances using significantly reduced training\nbatches. To avoid catastrophic forgetting, a novel objective function minimizes\nthe network loss in each iteration by retaining the previously acquired\nknowledge while learning new class representations and resolving their complex\nstructural inter-dependencies through Bayesian inference. A thorough evaluation\nof our framework on two publicly available X-ray datasets shows that it\noutperforms state-of-the-art methods, especially within the challenging\ncluttered scenarios, while achieving an optimal trade-off between detection\naccuracy and efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hassan_T/0/1/0/all/0/1\">Taimur Hassan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akcay_S/0/1/0/all/0/1\">Samet Akcay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bennamoun_M/0/1/0/all/0/1\">Mohammed Bennamoun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1\">Salman Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Werghi_N/0/1/0/all/0/1\">Naoufel Werghi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Incremental Learning Approach to Automatically Recognize Pulmonary Diseases from the Multi-vendor Chest Radiographs. (arXiv:2201.02574v1 [eess.IV])","link":"http://arxiv.org/abs/2201.02574","description":"<p>Pulmonary diseases can cause severe respiratory problems, leading to sudden\ndeath if not treated timely. Many researchers have utilized deep learning\nsystems to diagnose pulmonary disorders using chest X-rays (CXRs). However,\nsuch systems require exhaustive training efforts on large-scale data to\neffectively diagnose chest abnormalities. Furthermore, procuring such\nlarge-scale data is often infeasible and impractical, especially for rare\ndiseases. With the recent advances in incremental learning, researchers have\nperiodically tuned deep neural networks to learn different classification tasks\nwith few training examples. Although, such systems can resist catastrophic\nforgetting, they treat the knowledge representations independently of each\nother, and this limits their classification performance. Also, to the best of\nour knowledge, there is no incremental learning-driven image diagnostic\nframework that is specifically designed to screen pulmonary disorders from the\nCXRs. To address this, we present a novel framework that can learn to screen\ndifferent chest abnormalities incrementally. In addition to this, the proposed\nframework is penalized through an incremental learning loss function that\ninfers Bayesian theory to recognize structural and semantic inter-dependencies\nbetween incrementally learned knowledge representations to diagnose the\npulmonary diseases effectively, regardless of the scanner specifications. We\ntested the proposed framework on five public CXR datasets containing different\nchest abnormalities, where it outperformed various state-of-the-art system\nthrough various metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Sirshar_M/0/1/0/all/0/1\">Mehreen Sirshar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hassan_T/0/1/0/all/0/1\">Taimur Hassan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Akram_M/0/1/0/all/0/1\">Muhammad Usman Akram</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khan_S/0/1/0/all/0/1\">Shoab Ahmed Khan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Scale-Invariance and Uncertainity with Self-Supervised Domain Adaptation for Semantic Segmentation of Foggy Scenes. (arXiv:2201.02588v1 [cs.CV])","link":"http://arxiv.org/abs/2201.02588","description":"<p>This paper presents FogAdapt, a novel approach for domain adaptation of\nsemantic segmentation for dense foggy scenes. Although significant research has\nbeen directed to reduce the domain shift in semantic segmentation, adaptation\nto scenes with adverse weather conditions remains an open question. Large\nvariations in the visibility of the scene due to weather conditions, such as\nfog, smog, and haze, exacerbate the domain shift, thus making unsupervised\nadaptation in such scenarios challenging. We propose a self-entropy and\nmulti-scale information augmented self-supervised domain adaptation method\n(FogAdapt) to minimize the domain shift in foggy scenes segmentation. Supported\nby the empirical evidence that an increase in fog density results in high\nself-entropy for segmentation probabilities, we introduce a self-entropy based\nloss function to guide the adaptation method. Furthermore, inferences obtained\nat different image scales are combined and weighted by the uncertainty to\ngenerate scale-invariant pseudo-labels for the target domain. These\nscale-invariant pseudo-labels are robust to visibility and scale variations. We\nevaluate the proposed model on real clear-weather scenes to real foggy scenes\nadaptation and synthetic non-foggy images to real foggy scenes adaptation\nscenarios. Our experiments demonstrate that FogAdapt significantly outperforms\nthe current state-of-the-art in semantic segmentation of foggy images.\nSpecifically, by considering the standard settings compared to state-of-the-art\n(SOTA) methods, FogAdapt gains 3.8% on Foggy Zurich, 6.0% on Foggy\nDriving-dense, and 3.6% on Foggy Driving in mIoU when adapted from Cityscapes\nto Foggy Zurich.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Iqbal_J/0/1/0/all/0/1\">Javed Iqbal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hafiz_R/0/1/0/all/0/1\">Rehan Hafiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ali_M/0/1/0/all/0/1\">Mohsen Ali</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Equalized Focal Loss for Dense Long-Tailed Object Detection. (arXiv:2201.02593v1 [cs.CV])","link":"http://arxiv.org/abs/2201.02593","description":"<p>Despite the recent success of long-tailed object detection, almost all\nlong-tailed object detectors are developed based on the two-stage paradigm. In\npractice, one-stage detectors are more prevalent in the industry because they\nhave a simple and fast pipeline that is easy to deploy. However, in the\nlong-tailed scenario, this line of work has not been explored so far. In this\npaper, we investigate whether one-stage detectors can perform well in this\ncase. We discover the primary obstacle that prevents one-stage detectors from\nachieving excellent performance is: categories suffer from different degrees of\npositive-negative imbalance problems under the long-tailed data distribution.\nThe conventional focal loss balances the training process with the same\nmodulating factor for all categories, thus failing to handle the long-tailed\nproblem. To address this issue, we propose the Equalized Focal Loss (EFL) that\nrebalances the loss contribution of positive and negative samples of different\ncategories independently according to their imbalance degrees. Specifically,\nEFL adopts a category-relevant modulating factor which can be adjusted\ndynamically by the training status of different categories. Extensive\nexperiments conducted on the challenging LVIS v1 benchmark demonstrate the\neffectiveness of our proposed method. With an end-to-end training pipeline, EFL\nachieves 29.2% in terms of overall AP and obtains significant performance\nimprovements on rare categories, surpassing all existing state-of-the-art\nmethods. The code is available at https://github.com/ModelTC/EOD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yongqiang Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1\">Jingru Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Gang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fengwei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jianwei Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Ye Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Twenty-thousand Classes using Image-level Supervision. (arXiv:2201.02605v1 [cs.CV])","link":"http://arxiv.org/abs/2201.02605","description":"<p>Current object detectors are limited in vocabulary size due to the small\nscale of detection datasets. Image classifiers, on the other hand, reason about\nmuch larger vocabularies, as their datasets are larger and easier to collect.\nWe propose Detic, which simply trains the classifiers of a detector on image\nclassification data and thus expands the vocabulary of detectors to tens of\nthousands of concepts. Unlike prior work, Detic does not assign image labels to\nboxes based on model predictions, making it much easier to implement and\ncompatible with a range of detection architectures and backbones. Our results\nshow that Detic yields excellent detectors even for classes without box\nannotations. It outperforms prior work on both open-vocabulary and long-tail\ndetection benchmarks. Detic provides a gain of 2.4 mAP for all classes and 8.3\nmAP for novel classes on the open-vocabulary LVIS benchmark. On the standard\nLVIS benchmark, Detic reaches 41.7 mAP for all classes and 41.7 mAP for rare\nclasses. For the first time, we train a detector with all the\ntwenty-one-thousand classes of the ImageNet dataset and show that it\ngeneralizes to new datasets without fine-tuning. Code is available at\nhttps://github.com/facebookresearch/Detic.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xingyi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Girdha_R/0/1/0/all/0/1\">Rohit Girdha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joulin_A/0/1/0/all/0/1\">Armand Joulin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krahenbuhl_P/0/1/0/all/0/1\">Phillip Kr&#xe4;henb&#xfc;hl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Misra_I/0/1/0/all/0/1\">Ishan Misra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalized Category Discovery. (arXiv:2201.02609v1 [cs.CV])","link":"http://arxiv.org/abs/2201.02609","description":"<p>In this paper, we consider a highly general image recognition setting\nwherein, given a labelled and unlabelled set of images, the task is to\ncategorize all images in the unlabelled set. Here, the unlabelled images may\ncome from labelled classes or from novel ones. Existing recognition methods are\nnot able to deal with this setting, because they make several restrictive\nassumptions, such as the unlabelled instances only coming from known - or\nunknown - classes and the number of unknown classes being known a-priori. We\naddress the more unconstrained setting, naming it 'Generalized Category\nDiscovery', and challenge all these assumptions. We first establish strong\nbaselines by taking state-of-the-art algorithms from novel category discovery\nand adapting them for this task. Next, we propose the use of vision\ntransformers with contrastive representation learning for this open world\nsetting. We then introduce a simple yet effective semi-supervised $k$-means\nmethod to cluster the unlabelled data into seen and unseen classes\nautomatically, substantially outperforming the baselines. Finally, we also\npropose a new approach to estimate the number of classes in the unlabelled\ndata. We thoroughly evaluate our approach on public datasets for generic object\nclassification including CIFAR10, CIFAR100 and ImageNet-100, and for\nfine-grained visual recognition including CUB, Stanford Cars and Herbarium19,\nbenchmarking on this new setting to foster future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vaze_S/0/1/0/all/0/1\">Sagar Vaze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1\">Kai Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vedaldi_A/0/1/0/all/0/1\">Andrea Vedaldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1\">Andrew Zisserman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Embodied Hands: Modeling and Capturing Hands and Bodies Together. (arXiv:2201.02610v1 [cs.GR])","link":"http://arxiv.org/abs/2201.02610","description":"<p>Humans move their hands and bodies together to communicate and solve tasks.\nCapturing and replicating such coordinated activity is critical for virtual\ncharacters that behave realistically. Surprisingly, most methods treat the 3D\nmodeling and tracking of bodies and hands separately. Here we formulate a model\nof hands and bodies interacting together and fit it to full-body 4D sequences.\nWhen scanning or capturing the full body in 3D, hands are small and often\npartially occluded, making their shape and pose hard to recover. To cope with\nlow-resolution, occlusion, and noise, we develop a new model called MANO (hand\nModel with Articulated and Non-rigid defOrmations). MANO is learned from around\n1000 high-resolution 3D scans of hands of 31 subjects in a wide variety of hand\nposes. The model is realistic, low-dimensional, captures non-rigid shape\nchanges with pose, is compatible with standard graphics packages, and can fit\nany human hand. MANO provides a compact mapping from hand poses to pose blend\nshape corrections and a linear manifold of pose synergies. We attach MANO to a\nstandard parameterized 3D body shape model (SMPL), resulting in a fully\narticulated body and hand model (SMPL+H). We illustrate SMPL+H by fitting\ncomplex, natural, activities of subjects captured with a 4D scanner. The\nfitting is fully automatic and results in full body models that move naturally\nwith detailed hand motions and a realism not seen before in full body\nperformance capture. The models and data are freely available for research\npurposes in our website (<a href=\"http://mano.is.tue.mpg.de\">this http URL</a>).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Romero_J/0/1/0/all/0/1\">Javier Romero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tzionas_D/0/1/0/all/0/1\">Dimitrios Tzionas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Black_M/0/1/0/all/0/1\">Michael J. Black</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Compare Relation: Semantic Alignment for Few-Shot Learning. (arXiv:2003.00210v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2003.00210","description":"<p>Few-shot learning is a fundamental and challenging problem since it requires\nrecognizing novel categories from only a few examples. The objects for\nrecognition have multiple variants and can locate anywhere in images. Directly\ncomparing query images with example images can not handle content misalignment.\nThe representation and metric for comparison are critical but challenging to\nlearn due to the scarcity and wide variation of the samples in few-shot\nlearning. In this paper, we present a novel semantic alignment model to compare\nrelations, which is robust to content misalignment. We propose to add two key\ningredients to existing few-shot learning frameworks for better feature and\nmetric learning ability. First, we introduce a semantic alignment loss to align\nthe relation statistics of the features from samples that belong to the same\ncategory. And second, local and global mutual information maximization is\nintroduced, allowing for representations that contain locally-consistent and\nintra-class shared information across structural locations in an image.\nThirdly, we introduce a principled approach to weigh multiple loss functions by\nconsidering the homoscedastic uncertainty of each stream. We conduct extensive\nexperiments on several few-shot learning datasets. Experimental results show\nthat the proposed method is capable of comparing relations with semantic\nalignment strategies, and achieves state-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_C/0/1/0/all/0/1\">Congqi Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yanning Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SpinalNet: Deep Neural Network with Gradual Input. (arXiv:2007.03347v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2007.03347","description":"<p>Deep neural networks (DNNs) have achieved the state of the art performance in\nnumerous fields. However, DNNs need high computation times, and people always\nexpect better performance in a lower computation. Therefore, we study the human\nsomatosensory system and design a neural network (SpinalNet) to achieve higher\naccuracy with fewer computations. Hidden layers in traditional NNs receive\ninputs in the previous layer, apply activation function, and then transfer the\noutcomes to the next layer. In the proposed SpinalNet, each layer is split into\nthree splits: 1) input split, 2) intermediate split, and 3) output split. Input\nsplit of each layer receives a part of the inputs. The intermediate split of\neach layer receives outputs of the intermediate split of the previous layer and\noutputs of the input split of the current layer. The number of incoming weights\nbecomes significantly lower than traditional DNNs. The SpinalNet can also be\nused as the fully connected or classification layer of DNN and supports both\ntraditional learning and transfer learning. We observe significant error\nreductions with lower computational costs in most of the DNNs. Traditional\nlearning on the VGG-5 network with SpinalNet classification layers provided the\nstate-of-the-art (SOTA) performance on QMNIST, Kuzushiji-MNIST, EMNIST\n(Letters, Digits, and Balanced) datasets. Traditional learning with ImageNet\npre-trained initial weights and SpinalNet classification layers provided the\nSOTA performance on STL-10, Fruits 360, Bird225, and Caltech-101 datasets. The\nscripts of the proposed SpinalNet are available at the following link:\nhttps://github.com/dipuk0506/SpinalNet\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kabir_H/0/1/0/all/0/1\">H M Dipu Kabir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdar_M/0/1/0/all/0/1\">Moloud Abdar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jalali_S/0/1/0/all/0/1\">Seyed Mohammad Jafar Jalali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khosravi_A/0/1/0/all/0/1\">Abbas Khosravi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atiya_A/0/1/0/all/0/1\">Amir F Atiya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nahavandi_S/0/1/0/all/0/1\">Saeid Nahavandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_D/0/1/0/all/0/1\">Dipti Srinivasan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Image Retrieval-based Visual Localization using Kapture. (arXiv:2007.13867v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2007.13867","description":"<p>Visual localization tackles the challenge of estimating the camera pose from\nimages by using correspondence analysis between query images and a map. This\ntask is computation and data intensive which poses challenges on thorough\nevaluation of methods on various datasets. However, in order to further advance\nin the field, we claim that robust visual localization algorithms should be\nevaluated on multiple datasets covering a broad domain variety. To facilitate\nthis, we introduce kapture, a new, flexible, unified data format and toolbox\nfor visual localization and structure-from-motion (SFM). It enables easy usage\nof different datasets as well as efficient and reusable data processing. To\ndemonstrate this, we present a versatile pipeline for visual localization that\nfacilitates the use of different local and global features, 3D data (e.g. depth\nmaps), non-vision sensor data (e.g. IMU, GPS, WiFi), and various processing\nalgorithms. Using multiple configurations of the pipeline, we show the great\nversatility of kapture in our experiments. Furthermore, we evaluate our methods\non eight public datasets where they rank top on all and first on many of them.\nTo foster future research, we release code, models, and all datasets used in\nthis paper in the kapture format open source under a permissive BSD license.\ngithub.com/naver/kapture, github.com/naver/kapture-localization\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Humenberger_M/0/1/0/all/0/1\">Martin Humenberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cabon_Y/0/1/0/all/0/1\">Yohann Cabon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guerin_N/0/1/0/all/0/1\">Nicolas Guerin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morat_J/0/1/0/all/0/1\">Julien Morat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leroy_V/0/1/0/all/0/1\">Vincent Leroy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Revaud_J/0/1/0/all/0/1\">J&#xe9;r&#xf4;me Revaud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rerole_P/0/1/0/all/0/1\">Philippe Rerole</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pion_N/0/1/0/all/0/1\">No&#xe9; Pion</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Souza_C/0/1/0/all/0/1\">Cesar de Souza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Csurka_G/0/1/0/all/0/1\">Gabriela Csurka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DiffusionNet: Discretization Agnostic Learning on Surfaces. (arXiv:2012.00888v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.00888","description":"<p>We introduce a new general-purpose approach to deep learning on 3D surfaces,\nbased on the insight that a simple diffusion layer is highly effective for\nspatial communication. The resulting networks are automatically robust to\nchanges in resolution and sampling of a surface -- a basic property which is\ncrucial for practical applications. Our networks can be discretized on various\ngeometric representations such as triangle meshes or point clouds, and can even\nbe trained on one representation then applied to another. We optimize the\nspatial support of diffusion as a continuous network parameter ranging from\npurely local to totally global, removing the burden of manually choosing\nneighborhood sizes. The only other ingredients in the method are a multi-layer\nperceptron applied independently at each point, and spatial gradient features\nto support directional filters. The resulting networks are simple, robust, and\nefficient. Here, we focus primarily on triangle mesh surfaces, and demonstrate\nstate-of-the-art results for a variety of tasks including surface\nclassification, segmentation, and non-rigid correspondence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharp_N/0/1/0/all/0/1\">Nicholas Sharp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Attaiki_S/0/1/0/all/0/1\">Souhaib Attaiki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crane_K/0/1/0/all/0/1\">Keenan Crane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ovsjanikov_M/0/1/0/all/0/1\">Maks Ovsjanikov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Adversarial Robustness of Multi-Sensor Perception Systems in Self Driving. (arXiv:2101.06784v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.06784","description":"<p>Modern self-driving perception systems have been shown to improve upon\nprocessing complementary inputs such as LiDAR with images. In isolation, 2D\nimages have been found to be extremely vulnerable to adversarial attacks. Yet,\nthere have been limited studies on the adversarial robustness of multi-modal\nmodels that fuse LiDAR features with image features. Furthermore, existing\nworks do not consider physically realizable perturbations that are consistent\nacross the input modalities. In this paper, we showcase practical\nsusceptibilities of multi-sensor detection by placing an adversarial object on\ntop of a host vehicle. We focus on physically realizable and input-agnostic\nattacks as they are feasible to execute in practice, and show that a single\nuniversal adversary can hide different host vehicles from state-of-the-art\nmulti-modal detectors. Our experiments demonstrate that successful attacks are\nprimarily caused by easily corrupted image features. Furthermore, we find that\nin modern sensor fusion methods which project image features into 3D,\nadversarial attacks can exploit the projection process to generate false\npositives across distant regions in 3D. Towards more robust multi-modal\nperception systems, we show that adversarial training with feature denoising\ncan boost robustness to such attacks significantly. However, we find that\nstandard adversarial defenses still struggle to prevent false positives which\nare also caused by inaccurate associations between 3D LiDAR points and 2D\npixels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tu_J/0/1/0/all/0/1\">James Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Huichen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1\">Xinchen Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_M/0/1/0/all/0/1\">Mengye Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_M/0/1/0/all/0/1\">Ming Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bitar_E/0/1/0/all/0/1\">Eilyan Bitar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yumer_E/0/1/0/all/0/1\">Ersin Yumer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Urtasun_R/0/1/0/all/0/1\">Raquel Urtasun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MIN2Net: End-to-End Multi-Task Learning for Subject-Independent Motor Imagery EEG Classification. (arXiv:2102.03814v4 [eess.SP] UPDATED)","link":"http://arxiv.org/abs/2102.03814","description":"<p>Advances in the motor imagery (MI)-based brain-computer interfaces (BCIs)\nallow control of several applications by decoding neurophysiological phenomena,\nwhich are usually recorded by electroencephalography (EEG) using a non-invasive\ntechnique. Despite great advances in MI-based BCI, EEG rhythms are specific to\na subject and various changes over time. These issues point to significant\nchallenges to enhance the classification performance, especially in a\nsubject-independent manner. To overcome these challenges, we propose MIN2Net, a\nnovel end-to-end multi-task learning to tackle this task. We integrate deep\nmetric learning into a multi-task autoencoder to learn a compact and\ndiscriminative latent representation from EEG and perform classification\nsimultaneously. This approach reduces the complexity in pre-processing, results\nin significant performance improvement on EEG classification. Experimental\nresults in a subject-independent manner show that MIN2Net outperforms the\nstate-of-the-art techniques, achieving an F1-score improvement of 6.72%, and\n2.23% on the SMR-BCI, and OpenBMI datasets, respectively. We demonstrate that\nMIN2Net improves discriminative information in the latent representation. This\nstudy indicates the possibility and practicality of using this model to develop\nMI-based BCI applications for new users without the need for calibration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Autthasan_P/0/1/0/all/0/1\">Phairot Autthasan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chaisaen_R/0/1/0/all/0/1\">Rattanaphon Chaisaen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sudhawiyangkul_T/0/1/0/all/0/1\">Thapanun Sudhawiyangkul</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rangpong_P/0/1/0/all/0/1\">Phurin Rangpong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kiatthaveephong_S/0/1/0/all/0/1\">Suktipol Kiatthaveephong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dilokthanakul_N/0/1/0/all/0/1\">Nat Dilokthanakul</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bhakdisongkhram_G/0/1/0/all/0/1\">Gun Bhakdisongkhram</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Phan_H/0/1/0/all/0/1\">Huy Phan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guan_C/0/1/0/all/0/1\">Cuntai Guan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wilaiprasitporn_T/0/1/0/all/0/1\">Theerawit Wilaiprasitporn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Just Noticeable Difference for Deep Machine Vision. (arXiv:2102.08168v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.08168","description":"<p>As an important perceptual characteristic of the Human Visual System (HVS),\nthe Just Noticeable Difference (JND) has been studied for decades with image\nand video processing (e.g., perceptual visual signal compression). However,\nthere is little exploration on the existence of JND for the Deep Machine Vision\n(DMV), although the DMV has made great strides in many machine vision tasks. In\nthis paper, we take an initial attempt, and demonstrate that the DMV has the\nJND, termed as the DMV-JND. We then propose a JND model for the image\nclassification task in the DMV. It has been discovered that the DMV can\ntolerate distorted images with average PSNR of only 9.56dB (the lower the\nbetter), by generating JND via unsupervised learning with the proposed\nDMV-JND-NET. In particular, a semantic-guided redundancy assessment strategy is\ndesigned to restrain the magnitude and spatial distribution of the DMV-JND.\nExperimental results on image classification demonstrate that we successfully\nfind the JND for deep machine vision. Our DMV-JND facilitates a possible\ndirection for DMV-oriented image and video compression, watermarking, quality\nassessment, deep neural network security, and so on.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1\">Jian Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xingxing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_X/0/1/0/all/0/1\">Xin Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Huan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Weisi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1\">Jian Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yao Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quantitative Performance Assessment of CNN Units via Topological Entropy Calculation. (arXiv:2103.09716v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.09716","description":"<p>Identifying the status of individual network units is critical for\nunderstanding the mechanism of convolutional neural networks (CNNs). However,\nit is still challenging to reliably give a general indication of unit status,\nespecially for units in different network models. To this end, we propose a\nnovel method for quantitatively clarifying the status of single unit in CNN\nusing algebraic topological tools. Unit status is indicated via the calculation\nof a defined topological-based entropy, called feature entropy, which measures\nthe degree of chaos of the global spatial pattern hidden in the unit for a\ncategory. In this way, feature entropy could provide an accurate indication of\nstatus for units in different networks with diverse situations like\nweight-rescaling operation. Further, we show that feature entropy decreases as\nthe layer goes deeper and shares almost simultaneous trend with loss during\ntraining. We show that by investigating the feature entropy of units on only\ntraining data, it could give discrimination between networks with different\ngeneralization ability from the view of the effectiveness of feature\nrepresentations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comprehensive Survey of Scene Graphs: Generation and Application. (arXiv:2104.01111v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.01111","description":"<p>Scene graph is a structured representation of a scene that can clearly\nexpress the objects, attributes, and relationships between objects in the\nscene. As computer vision technology continues to develop, people are no longer\nsatisfied with simply detecting and recognizing objects in images; instead,\npeople look forward to a higher level of understanding and reasoning about\nvisual scenes. For example, given an image, we want to not only detect and\nrecognize objects in the image, but also know the relationship between objects\n(visual relationship detection), and generate a text description (image\ncaptioning) based on the image content. Alternatively, we might want the\nmachine to tell us what the little girl in the image is doing (Visual Question\nAnswering (VQA)), or even remove the dog from the image and find similar images\n(image editing and retrieval), etc. These tasks require a higher level of\nunderstanding and reasoning for image vision tasks. The scene graph is just\nsuch a powerful tool for scene understanding. Therefore, scene graphs have\nattracted the attention of a large number of researchers, and related research\nis often cross-modal, complex, and rapidly developing. However, no relatively\nsystematic survey of scene graphs exists at present. To this end, this survey\nconducts a comprehensive investigation of the current scene graph research.\nMore specifically, we first summarized the general definition of the scene\ngraph, then conducted a comprehensive and systematic discussion on the\ngeneration method of the scene graph (SGG) and the SGG with the aid of prior\nknowledge. We then investigated the main applications of scene graphs and\nsummarized the most commonly used datasets. Finally, we provide some insights\ninto the future development of scene graphs. We believe this will be a very\nhelpful foundation for future research on scene graphs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xiaojun Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_P/0/1/0/all/0/1\">Pengzhen Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1\">Pengfei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhihui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaojiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hauptmann_A/0/1/0/all/0/1\">Alex Hauptmann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantically Stealthy Adversarial Attacks against Segmentation Models. (arXiv:2104.01732v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.01732","description":"<p>Segmentation models have been found to be vulnerable to targeted and\nnon-targeted adversarial attacks. However, the resulting segmentation outputs\nare often so damaged that it is easy to spot an attack. In this paper, we\npropose semantically stealthy adversarial attacks which can manipulate targeted\nlabels while preserving non-targeted labels at the same time. One challenge is\nmaking semantically meaningful manipulations across datasets and models.\nAnother challenge is avoiding damaging non-targeted labels. To solve these\nchallenges, we consider each input image as prior knowledge to generate\nperturbations. We also design a special regularizer to help extract features.\nTo evaluate our model's performance, we design three basic attack types, namely\n`vanishing into the context,' `embedding fake labels,' and `displacing target\nobjects.' Our experiments show that our stealthy adversarial model can attack\nsegmentation models with a relatively high success rate on Cityscapes,\nMapillary, and BDD100K. Our framework shows good empirical generalization\nacross datasets and models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhenhua Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chuhua Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crandall_D/0/1/0/all/0/1\">David J. Crandall</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Example Detection for DNN Models: A Review and Experimental Comparison. (arXiv:2105.00203v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.00203","description":"<p>Deep learning (DL) has shown great success in many human-related tasks, which\nhas led to its adoption in many computer vision based applications, such as\nsecurity surveillance systems, autonomous vehicles and healthcare. Such\nsafety-critical applications have to draw their path to success deployment once\nthey have the capability to overcome safety-critical challenges. Among these\nchallenges are the defense against or/and the detection of the adversarial\nexamples (AEs). Adversaries can carefully craft small, often imperceptible,\nnoise called perturbations to be added to the clean image to generate the AE.\nThe aim of AE is to fool the DL model which makes it a potential risk for DL\napplications. Many test-time evasion attacks and countermeasures,i.e., defense\nor detection methods, are proposed in the literature. Moreover, few reviews and\nsurveys were published and theoretically showed the taxonomy of the threats and\nthe countermeasure methods with little focus in AE detection methods. In this\npaper, we focus on image classification task and attempt to provide a survey\nfor detection methods of test-time evasion attacks on neural network\nclassifiers. A detailed discussion for such methods is provided with\nexperimental results for eight state-of-the-art detectors under different\nscenarios on four datasets. We also provide potential challenges and future\nperspectives for this research direction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aldahdooh_A/0/1/0/all/0/1\">Ahmed Aldahdooh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamidouche_W/0/1/0/all/0/1\">Wassim Hamidouche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fezza_S/0/1/0/all/0/1\">Sid Ahmed Fezza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deforges_O/0/1/0/all/0/1\">Olivier Deforges</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sharing Pain: Using Pain Domain Transfer for Video Recognition of Low Grade Orthopedic Pain in Horses. (arXiv:2105.10313v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.10313","description":"<p>Orthopedic disorders are common among horses, often leading to euthanasia,\nwhich often could have been avoided with earlier detection. These conditions\noften create varying degrees of subtle long-term pain. It is challenging to\ntrain a visual pain recognition method with video data depicting such pain,\nsince the resulting pain behavior also is subtle, sparsely appearing, and\nvarying, making it challenging for even an expert human labeller to provide\naccurate ground-truth for the data. We show that a model trained solely on a\ndataset of horses with acute experimental pain (where labeling is less\nambiguous) can aid recognition of the more subtle displays of orthopedic pain.\nMoreover, we present a human expert baseline for the problem, as well as an\nextensive empirical study of various domain transfer methods and of what is\ndetected by the pain recognition method trained on clean experimental pain in\nthe orthopedic dataset. Finally, this is accompanied with a discussion around\nthe challenges posed by real-world animal behavior datasets and how best\npractices can be established for similar fine-grained action recognition tasks.\nOur code is available at https://github.com/sofiabroome/painface-recognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Broome_S/0/1/0/all/0/1\">Sofia Broom&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ask_K/0/1/0/all/0/1\">Katrina Ask</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rashid_M/0/1/0/all/0/1\">Maheen Rashid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andersen_P/0/1/0/all/0/1\">Pia Haubro Andersen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kjellstrom_H/0/1/0/all/0/1\">Hedvig Kjellstr&#xf6;m</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning for Predictive Analytics in Reversible Steganography. (arXiv:2106.06924v2 [cs.MM] UPDATED)","link":"http://arxiv.org/abs/2106.06924","description":"<p>Deep learning is regarded as a promising solution for reversible\nsteganography. The recent development of end-to-end learning has made it\npossible to bypass multiple intermediate stages of steganographic operations\nwith a pair of encoder and decoder neural networks. This framework is, however,\nincapable of guaranteeing perfect reversibility since it is difficult for this\nkind of monolithic machinery, in the form of a black box, to learn the\nintricate logics of reversible computing. A more reliable way to develop a\nlearning-based reversible steganographic scheme is through a divide-and-conquer\nparadigm. Prediction-error modulation is a well-established modular framework\nthat consists of an analytics module and a coding module. The former serves the\npurpose of analysing pixel correlations and predicting pixel intensities, while\nthe latter specialises in reversible coding mechanisms. Given that\nreversibility is governed independently by the coding module, we narrow our\nfocus to the incorporation of neural networks into the analytics module. The\nobjective of this study is to evaluate the impacts of different training\nconfigurations on predictive neural networks and to provide practical insights.\nContext-aware pixel intensity prediction has a central role in reversible\nsteganography and can be perceived as a low-level computer vision task.\nTherefore, instead of reinventing the wheel, we can adopt neural network models\noriginally designed for such computer vision tasks to perform intensity\nprediction. Furthermore, we rigorously investigate the effect of intensity\ninitialisation upon predictive performance and the influence of distributional\nshift in dual-layer prediction. Experimental results show that state-of-the-art\nsteganographic performance can be achieved with advanced neural network models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1\">Ching-Chun Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Sisheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Echizen_I/0/1/0/all/0/1\">Isao Echizen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_V/0/1/0/all/0/1\">Victor Sanchez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chang-Tsun Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-view Image-based Hand Geometry Refinement using Differentiable Monte Carlo Ray Tracing. (arXiv:2107.05509v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.05509","description":"<p>The amount and quality of datasets and tools available in the research field\nof hand pose and shape estimation act as evidence to the significant progress\nthat has been made.However, even the datasets of the highest quality, reported\nto date, have shortcomings in annotation. We propose a refinement approach,\nbased on differentiable ray tracing,and demonstrate how a high-quality publicly\navailable, multi-camera dataset of hands(InterHand2.6M) can become an even\nbetter dataset, with respect to annotation quality. Differentiable ray tracing\nhas not been employed so far to relevant problems and is hereby shown to be\nsuperior to the approximative alternatives that have been employed in the past.\nTo tackle the lack of reliable ground truth, as far as quantitative evaluation\nis concerned, we resort to realistic synthetic data, to show that the\nimprovement we induce is indeed significant. The same becomes evident in real\ndata through visual evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karvounas_G/0/1/0/all/0/1\">Giorgos Karvounas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kyriazis_N/0/1/0/all/0/1\">Nikolaos Kyriazis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oikonomidis_I/0/1/0/all/0/1\">Iason Oikonomidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsoli_A/0/1/0/all/0/1\">Aggeliki Tsoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Argyros_A/0/1/0/all/0/1\">Antonis A. Argyros</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"XCI-Sketch: Extraction of Color Information from Images for Generation of Colored Outlines and Sketches. (arXiv:2108.11554v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.11554","description":"<p>Sketches are a medium to convey a visual scene from an individual's creative\nperspective. The addition of color substantially enhances the overall\nexpressivity of a sketch. This paper proposes two methods to mimic human-drawn\ncolored sketches by utilizing the Contour Drawing Dataset. Our first approach\nrenders colored outline sketches by applying image processing techniques aided\nby k-means color clustering. The second method uses a generative adversarial\nnetwork to develop a model that can generate colored sketches from previously\nunobserved images. We assess the results obtained through quantitative and\nqualitative evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Manushree_V/0/1/0/all/0/1\">V Manushree</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saxena_S/0/1/0/all/0/1\">Sameer Saxena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_P/0/1/0/all/0/1\">Parna Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varma_M/0/1/0/all/0/1\">Manisimha Varma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rathod_H/0/1/0/all/0/1\">Harsh Rathod</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_A/0/1/0/all/0/1\">Ankita Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khose_S/0/1/0/all/0/1\">Sahil Khose</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PnP-DETR: Towards Efficient Visual Analysis with Transformers. (arXiv:2109.07036v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.07036","description":"<p>Recently, DETR pioneered the solution of vision tasks with transformers, it\ndirectly translates the image feature map into the object detection result.\nThough effective, translating the full feature map can be costly due to\nredundant computation on some area like the background. In this work, we\nencapsulate the idea of reducing spatial redundancy into a novel poll and pool\n(PnP) sampling module, with which we build an end-to-end PnP-DETR architecture\nthat adaptively allocates its computation spatially to be more efficient.\nConcretely, the PnP module abstracts the image feature map into fine foreground\nobject feature vectors and a small number of coarse background contextual\nfeature vectors. The transformer models information interaction within the\nfine-coarse feature space and translates the features into the detection\nresult. Moreover, the PnP-augmented model can instantly achieve various desired\ntrade-offs between performance and computation with a single model by varying\nthe sampled feature length, without requiring to train multiple models as\nexisting methods. Thus it offers greater flexibility for deployment in diverse\nscenarios with varying computation constraint. We further validate the\ngeneralizability of the PnP module on panoptic segmentation and the recent\ntransformer-based image recognition model ViT and show consistent efficiency\ngain. We believe our method makes a step for efficient visual analysis with\ntransformers, wherein spatial redundancy is commonly observed. Code will be\navailable at \\url{https://github.com/twangnh/pnp-detr}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Li Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yunpeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jiashi Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Shuicheng Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep DIC: Deep Learning-Based Digital Image Correlation for End-to-End Displacement and Strain Measurement. (arXiv:2110.13720v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2110.13720","description":"<p>Digital image correlation (DIC) has become an industry standard to retrieve\naccurate displacement and strain measurement in tensile testing and other\nmaterial characterization. Though traditional DIC offers a high precision\nestimation of deformation for general tensile testing cases, the prediction\nbecomes unstable at large deformation or when the speckle patterns start to\ntear. In addition, traditional DIC requires a long computation time and often\nproduces a low spatial resolution output affected by filtering and speckle\npattern quality. To address these challenges, we propose a new deep\nlearning-based DIC approach--Deep DIC, in which two convolutional neural\nnetworks, DisplacementNet and StrainNet, are designed to work together for\nend-to-end prediction of displacements and strains. DisplacementNet predicts\nthe displacement field and adaptively tracks a region of interest. StrainNet\npredicts the strain field directly from the image input without relying on the\ndisplacement prediction, which significantly improves the strain prediction\naccuracy. A new dataset generation method is developed to synthesize a\nrealistic and comprehensive dataset, including the generation of speckle\npatterns and the deformation of the speckle image with synthetic displacement\nfields. Though trained on synthetic datasets only, Deep DIC gives highly\nconsistent and comparable predictions of displacement and strain with those\nobtained from commercial DIC software for real experiments, while it\noutperforms commercial software with very robust strain prediction even at\nlarge and localized deformation and varied pattern qualities. In addition, Deep\nDIC is capable of real-time prediction of deformation with a calculation time\ndown to milliseconds.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yang_R/0/1/0/all/0/1\">Ru Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zeng_D/0/1/0/all/0/1\">Danielle Zeng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guo_P/0/1/0/all/0/1\">Ping Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stereoscopic Universal Perturbations across Different Architectures and Datasets. (arXiv:2112.06116v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.06116","description":"<p>We study the effect of adversarial perturbations of images on deep stereo\nmatching networks for the disparity estimation task. We present a method to\ncraft a single set of perturbations that, when added to any stereo image pair\nin a dataset, can fool a stereo network to significantly alter the perceived\nscene geometry. Our perturbation images are \"universal\" in that they not only\ncorrupt estimates of the network on the dataset they are optimized for, but\nalso generalize to stereo networks with different architectures across\ndifferent datasets. We evaluate our approach on multiple public benchmark\ndatasets and show that our perturbations can increase D1-error (akin to fooling\nrate) of state-of-the-art stereo networks from 1% to as much as 87%. We\ninvestigate the effect of perturbations on the estimated scene geometry and\nidentify object classes that are most vulnerable. Our analysis on the\nactivations of registered points between left and right images led us to find\nthat certain architectural components, i.e. deformable convolution and explicit\nmatching, can increase robustness against adversaries. We demonstrate that by\nsimply designing networks with such components, one can reduce the effect of\nadversaries by up to 60.5%, which rivals the robustness of networks fine-tuned\nwith costly adversarial data augmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Berger_Z/0/1/0/all/0/1\">Zachary Berger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_P/0/1/0/all/0/1\">Parth Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tian Yu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1\">Stefano Soatto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1\">Alex Wong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Facial Synthesis: A New Challenge. (arXiv:2112.15439v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.15439","description":"<p>The goal of this paper is to conduct a comprehensive study on the facial\nsketch synthesis (FSS) problem. However, due to the high costs in obtaining\nhand-drawn sketch datasets, there lacks a complete benchmark for assessing the\ndevelopment of FSS algorithms over the last decade. As such, we first introduce\na high-quality dataset for FSS, named FS2K, which consists of 2,104\nimage-sketch pairs spanning three types of sketch styles, image backgrounds,\nlighting conditions, skin colors, and facial attributes. FS2K differs from\nprevious FSS datasets in difficulty, diversity, and scalability, and should\nthus facilitate the progress of FSS research. Second, we present the\nlargest-scale FSS study by investigating 139 classical methods, including 24\nhandcrafted feature based facial sketch synthesis approaches, 37 general\nneural-style transfer methods, 43 deep image-to-image translation methods, and\n35 image-to-sketch approaches. Besides, we elaborate comprehensive experiments\nfor existing 19 cutting-edge models. Third, we present a simple baseline for\nFSS, named FSGAN. With only two straightforward components, i.e., facial-aware\nmasking and style-vector expansion, FSGAN surpasses the performance of all\nprevious state-of-the-art models on the proposed FS2K dataset, by a large\nmargin. Finally, we conclude with lessons learned over the past years, and\npoint out several unsolved challenges. Our open-source code is available at\nhttps://github.com/DengPingFan/FSGAN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1\">Deng-Ping Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Ziling Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_P/0/1/0/all/0/1\">Peng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_X/0/1/0/all/0/1\">Xuebin Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Understanding and Harnessing the Effect of Image Transformation in Adversarial Detection. (arXiv:2201.01080v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.01080","description":"<p>Deep neural networks (DNNs) are threatened by adversarial examples.\nAdversarial detection, which distinguishes adversarial images from benign\nimages, is fundamental for robust DNN-based services. Image transformation is\none of the most effective approaches to detect adversarial examples. During the\nlast few years, a variety of image transformations have been studied and\ndiscussed to design reliable adversarial detectors. In this paper, we\nsystematically synthesize the recent progress on adversarial detection via\nimage transformations with a novel classification method. Then, we conduct\nextensive experiments to test the detection performance of image\ntransformations against state-of-the-art adversarial attacks. Furthermore, we\nreveal that each individual transformation is not capable of detecting\nadversarial examples in a robust way, and propose a DNN-based approach referred\nto as AdvJudge, which combines scores of 9 image transformations. Without\nknowing which individual scores are misleading or not misleading, AdvJudge can\nmake the right judgment, and achieve a significant improvement in detection\naccuracy. We claim that AdvJudge is a more effective adversarial detector than\nthose based on an individual image transformation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1\">Bo Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yuefeng Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Weidong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Peng Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-SRN: Structure-Preserving Super-Resolution Network with Cross Convolution. (arXiv:2201.01458v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2201.01458","description":"<p>It is challenging to restore low-resolution (LR) images to super-resolution\n(SR) images with correct and clear details. Existing deep learning works almost\nneglect the inherent structural information of images, which acts as an\nimportant role for visual perception of SR results. In this paper, we design a\nhierarchical feature exploitation network to probe and preserve structural\ninformation in a multi-scale feature fusion manner. First, we propose a cross\nconvolution upon traditional edge detectors to localize and represent edge\nfeatures. Then, cross convolution blocks (CCBs) are designed with feature\nnormalization and channel attention to consider the inherent correlations of\nfeatures. Finally, we leverage multi-scale feature fusion group (MFFG) to embed\nthe cross convolution blocks and develop the relations of structural features\nin different scales hierarchically, invoking a lightweight structure-preserving\nnetwork named as Cross-SRN. Experimental results demonstrate the Cross-SRN\nachieves competitive or superior restoration performances against the\nstate-of-the-art methods with accurate and clear structural details. Moreover,\nwe set a criterion to select images with rich structural textures. The proposed\nCross-SRN outperforms the state-of-the-art methods on the selected benchmark,\nwhich demonstrates that our network has a significant advantage in preserving\nedges.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yuqing Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jia_Q/0/1/0/all/0/1\">Qi Jia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fan_X/0/1/0/all/0/1\">Xin Fan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1\">Shanshe Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ma_S/0/1/0/all/0/1\">Siwei Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gao_W/0/1/0/all/0/1\">Wen Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Investigation of \"Benford's\" Law Divergence and Machine Learning Techniques for \"Intra-Class\" Separability of Fingerprint Images. (arXiv:2201.01699v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.01699","description":"<p>Protecting a fingerprint database against attackers is very vital in order to\nprotect against false acceptance rate or false rejection rate. A key property\nin distinguishing fingerprint images is by exploiting the characteristics of\nthese different types of fingerprint images. The aim of this paper is to\nperform the classification of fingerprint images using the Ben-ford's law\ndivergence values and machine learning techniques. The usage of these\nBen-ford's law divergence values as features fed into the machine learning\ntechniques has proved to be very effective and efficient in the classification\nof fingerprint images. The effectiveness of our proposed methodology was\ndemonstrated on five datasets, achieving very high classification \"accuracies\"\nof 100% for the Decision Tree and CNN. However, the \"Naive\" Bayes, and Logistic\nRegression achieved \"accuracies\" of 95.95%, and 90.54%, respectively. These\nresults showed that Ben-ford's law features and machine learning techniques\nespecially Decision Tree and CNN can be effectively applied for the\nclassification of fingerprint images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Iorliam_A/0/1/0/all/0/1\">Aamo Iorliam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Emmanuel_O/0/1/0/all/0/1\">Orgem Emmanuel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shehu_Y/0/1/0/all/0/1\">Yahaya I. Shehu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Egocentric 3D Pose Estimation with Third Person Views. (arXiv:2201.02017v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.02017","description":"<p>In this paper, we propose a novel approach to enhance the 3D body pose\nestimation of a person computed from videos captured from a single wearable\ncamera. The key idea is to leverage high-level features linking first- and\nthird-views in a joint embedding space. To learn such embedding space we\nintroduce First2Third-Pose, a new paired synchronized dataset of nearly 2,000\nvideos depicting human activities captured from both first- and third-view\nperspectives. We explicitly consider spatial- and motion-domain features,\ncombined using a semi-Siamese architecture trained in a self-supervised\nfashion. Experimental results demonstrate that the joint multi-view embedded\nspace learned with our dataset is useful to extract discriminatory features\nfrom arbitrary single-view egocentric videos, without needing domain adaptation\nnor knowledge of camera parameters. We achieve significant improvement of\negocentric 3D body pose estimation performance on two unconstrained datasets,\nover three supervised state-of-the-art approaches. Our dataset and code will be\navailable for research purposes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dhamanaskar_A/0/1/0/all/0/1\">Ameya Dhamanaskar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dimiccoli_M/0/1/0/all/0/1\">Mariella Dimiccoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corona_E/0/1/0/all/0/1\">Enric Corona</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pumarola_A/0/1/0/all/0/1\">Albert Pumarola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreno_Noguer_F/0/1/0/all/0/1\">Francesc Moreno-Noguer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-01-09T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}