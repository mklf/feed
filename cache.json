{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.1","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-08-26T01:30:00Z","channels":[{"title":"cs.AI updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.AI","description":"Computer Science -- Artificial Intelligence (cs.AI) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"The Word is Mightier than the Label Learning without Pointillistic Labels using Data Programming. (arXiv:2108.10921v1 [cs.LG])","link":"http://arxiv.org/abs/2108.10921","description":"<p>Most advanced supervised Machine Learning (ML) models rely on vast amounts of\npoint-by-point labelled training examples. Hand-labelling vast amounts of data\nmay be tedious, expensive, and error-prone. Recently, some studies have\nexplored the use of diverse sources of weak supervision to produce competitive\nend model classifiers. In this paper, we survey recent work on weak\nsupervision, and in particular, we investigate the Data Programming (DP)\nframework. Taking a set of potentially noisy heuristics as input, DP assigns\ndenoised probabilistic labels to each data point in a dataset using a\nprobabilistic graphical model of heuristics. We analyze the math fundamentals\nbehind DP and demonstrate the power of it by applying it on two real-world text\nclassification tasks. Furthermore, we compare DP with pointillistic active and\nsemi-supervised learning techniques traditionally applied in data-sparse\nsettings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Chufan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goswami_M/0/1/0/all/0/1\">Mononito Goswami</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reasoning about Counterfactuals and Explanations: Problems, Results and Directions. (arXiv:2108.11004v1 [cs.AI])","link":"http://arxiv.org/abs/2108.11004","description":"<p>There are some recent approaches and results about the use of answer-set\nprogramming for specifying counterfactual interventions on entities under\nclassification, and reasoning about them. These approaches are flexible and\nmodular in that they allow the seamless addition of domain knowledge. Reasoning\nis enabled by query answering from the answer-set program. The programs can be\nused to specify and compute responsibility-based numerical scores as\nattributive explanations for classification results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bertossi_L/0/1/0/all/0/1\">Leopoldo Bertossi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversary agent reinforcement learning for pursuit-evasion. (arXiv:2108.11010v1 [cs.LG])","link":"http://arxiv.org/abs/2108.11010","description":"<p>A reinforcement learning environment with adversary agents is proposed in\nthis work for pursuit-evasion game in the presence of fog of war, which is of\nboth scientific significance and practical importance in aerospace\napplications. One of the most popular learning environments, StarCraft, is\nadopted here and the associated mini-games are analyzed to identify the current\nlimitation for training adversary agents. The key contribution includes the\nanalysis of the potential performance of an agent by incorporating control and\ndifferential game theory into the specific reinforcement learning environment,\nand the development of an adversary agents challenge (SAAC) environment by\nextending the current StarCraft mini-games. The subsequent study showcases the\nuse of this learning environment and the effectiveness of an adversary agent\nfor evasion units. Overall, the proposed SAAC environment should benefit\npursuit-evasion studies with rapidly-emerging reinforcement learning\ntechnologies. Last but not least, the corresponding tutorial code can be found\nat GitHub.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">X. Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tree Decomposed Graph Neural Network. (arXiv:2108.11022v1 [cs.LG])","link":"http://arxiv.org/abs/2108.11022","description":"<p>Graph Neural Networks (GNNs) have achieved significant success in learning\nbetter representations by performing feature propagation and transformation\niteratively to leverage neighborhood information. Nevertheless, iterative\npropagation restricts the information of higher-layer neighborhoods to be\ntransported through and fused with the lower-layer neighborhoods', which\nunavoidably results in feature smoothing between neighborhoods in different\nlayers and can thus compromise the performance, especially on heterophily\nnetworks. Furthermore, most deep GNNs only recognize the importance of\nhigher-layer neighborhoods while yet to fully explore the importance of\nmulti-hop dependency within the context of different layer neighborhoods in\nlearning better representations. In this work, we first theoretically analyze\nthe feature smoothing between neighborhoods in different layers and empirically\ndemonstrate the variance of the homophily level across neighborhoods at\ndifferent layers. Motivated by these analyses, we further propose a tree\ndecomposition method to disentangle neighborhoods in different layers to\nalleviate feature smoothing among these layers. Moreover, we characterize the\nmulti-hop dependency via graph diffusion within our tree decomposition\nformulation to construct Tree Decomposed Graph Neural Network (TDGNN), which\ncan flexibly incorporate information from large receptive fields and aggregate\nthis information utilizing the multi-hop dependency. Comprehensive experiments\ndemonstrate the superior performance of TDGNN on both homophily and heterophily\nnetworks under a variety of node classification settings. Extensive parameter\nanalysis highlights the ability of TDGNN to prevent over-smoothing and\nincorporate features from shallow layers with deeper multi-hop dependencies,\nwhich provides new insights towards deeper graph neural networks. Code of\nTDGNN: <a href=\"http://github.com/YuWVandy/TDGNN\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Derr_T/0/1/0/all/0/1\">Tyler Derr</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Visual Quality of Unrestricted Adversarial Examples with Wavelet-VAE. (arXiv:2108.11032v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11032","description":"<p>Traditional adversarial examples are typically generated by adding\nperturbation noise to the input image within a small matrix norm. In practice,\nun-restricted adversarial attack has raised great concern and presented a new\nthreat to the AI safety. In this paper, we propose a wavelet-VAE structure to\nreconstruct an input image and generate adversarial examples by modifying the\nlatent code. Different from perturbation-based attack, the modifications of the\nproposed method are not limited but imperceptible to human eyes. Experiments\nshow that our method can generate high quality adversarial examples on ImageNet\ndataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiang_W/0/1/0/all/0/1\">Wenzhao Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Shibao Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GRIM: A General, Real-Time Deep Learning Inference Framework for Mobile Devices based on Fine-Grained Structured Weight Sparsity. (arXiv:2108.11033v1 [cs.LG])","link":"http://arxiv.org/abs/2108.11033","description":"<p>It is appealing but challenging to achieve real-time deep neural network\n(DNN) inference on mobile devices because even the powerful modern mobile\ndevices are considered as ``resource-constrained'' when executing large-scale\nDNNs. It necessitates the sparse model inference via weight pruning, i.e., DNN\nweight sparsity, and it is desirable to design a new DNN weight sparsity scheme\nthat can facilitate real-time inference on mobile devices while preserving a\nhigh sparse model accuracy. This paper designs a novel mobile inference\nacceleration framework GRIM that is General to both convolutional neural\nnetworks (CNNs) and recurrent neural networks (RNNs) and that achieves\nReal-time execution and high accuracy, leveraging fine-grained structured\nsparse model Inference and compiler optimizations for Mobiles. We start by\nproposing a new fine-grained structured sparsity scheme through the Block-based\nColumn-Row (BCR) pruning. Based on this new fine-grained structured sparsity,\nour GRIM framework consists of two parts: (a) the compiler optimization and\ncode generation for real-time mobile inference; and (b) the BCR pruning\noptimizations for determining pruning hyperparameters and performing weight\npruning. We compare GRIM with Alibaba MNN, TVM, TensorFlow-Lite, a sparse\nimplementation based on CSR, PatDNN, and ESE (a representative FPGA inference\nacceleration framework for RNNs), and achieve up to 14.08x speedup.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Niu_W/0/1/0/all/0/1\">Wei Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhengang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaolong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_P/0/1/0/all/0/1\">Peiyan Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_G/0/1/0/all/0/1\">Gang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_X/0/1/0/all/0/1\">Xuehai Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xue Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_B/0/1/0/all/0/1\">Bin Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Social Norm Bias: Residual Harms of Fairness-Aware Algorithms. (arXiv:2108.11056v1 [cs.LG])","link":"http://arxiv.org/abs/2108.11056","description":"<p>Many modern learning algorithms mitigate bias by enforcing fairness across\ncoarsely-defined groups related to a sensitive attribute like gender or race.\nHowever, the same algorithms seldom account for the within-group biases that\narise due to the heterogeneity of group members. In this work, we characterize\nSocial Norm Bias (SNoB), a subtle but consequential type of discrimination that\nmay be exhibited by automated decision-making systems, even when these systems\nachieve group fairness objectives. We study this issue through the lens of\ngender bias in occupation classification from biographies. We quantify SNoB by\nmeasuring how an algorithm's predictions are associated with conformity to\ngender norms, which is measured using a machine learning approach. This\nframework reveals that for classification tasks related to male-dominated\noccupations, fairness-aware classifiers favor biographies written in ways that\nalign with masculine gender norms. We compare SNoB across fairness intervention\ntechniques and show that post-processing interventions do not mitigate this\ntype of bias at all.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1\">Myra Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+De_Arteaga_M/0/1/0/all/0/1\">Maria De-Arteaga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mackey_L/0/1/0/all/0/1\">Lester Mackey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalai_A/0/1/0/all/0/1\">Adam Tauman Kalai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Longitudinal Dynamics of Recommender Systems with Agent-Based Modeling and Simulation. (arXiv:2108.11068v1 [cs.IR])","link":"http://arxiv.org/abs/2108.11068","description":"<p>Today's research in recommender systems is largely based on experimental\ndesigns that are static in a sense that they do not consider potential\nlongitudinal effects of providing recommendations to users. In reality,\nhowever, various important and interesting phenomena only emerge or become\nvisible over time, e.g., when a recommender system continuously reinforces the\npopularity of already successful artists on a music streaming site or when\nrecommendations that aim at profit maximization lead to a loss of consumer\ntrust in the long run. In this paper, we discuss how Agent-Based Modeling and\nSimulation (ABM) techniques can be used to study such important longitudinal\ndynamics of recommender systems. To that purpose, we provide an overview of the\nABM principles, outline a simulation framework for recommender systems based on\nthe literature, and discuss various practical research questions that can be\naddressed with such an ABM-based simulation framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Adomavicius_G/0/1/0/all/0/1\">Gediminas Adomavicius</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jannach_D/0/1/0/all/0/1\">Dietmar Jannach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leitner_S/0/1/0/all/0/1\">Stephan Leitner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jingjing Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inductive Matrix Completion Using Graph Autoencoder. (arXiv:2108.11124v1 [cs.LG])","link":"http://arxiv.org/abs/2108.11124","description":"<p>Recently, the graph neural network (GNN) has shown great power in matrix\ncompletion by formulating a rating matrix as a bipartite graph and then\npredicting the link between the corresponding user and item nodes. The majority\nof GNN-based matrix completion methods are based on Graph Autoencoder (GAE),\nwhich considers the one-hot index as input, maps a user (or item) index to a\nlearnable embedding, applies a GNN to learn the node-specific representations\nbased on these learnable embeddings and finally aggregates the representations\nof the target users and its corresponding item nodes to predict missing links.\nHowever, without node content (i.e., side information) for training, the user\n(or item) specific representation can not be learned in the inductive setting,\nthat is, a model trained on one group of users (or items) cannot adapt to new\nusers (or items). To this end, we propose an inductive matrix completion method\nusing GAE (IMC-GAE), which utilizes the GAE to learn both the user-specific (or\nitem-specific) representation for personalized recommendation and local graph\npatterns for inductive matrix completion. Specifically, we design two\ninformative node features and employ a layer-wise node dropout scheme in GAE to\nlearn local graph patterns which can be generalized to unseen data. The main\ncontribution of our paper is the capability to efficiently learn local graph\npatterns in GAE, with good scalability and superior expressiveness compared to\nprevious GNN-based matrix completion methods. Furthermore, extensive\nexperiments demonstrate that our model achieves state-of-the-art performance on\nseveral matrix completion benchmarks. Our official code is publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1\">Wei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chuheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yun Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_L/0/1/0/all/0/1\">Liang Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaonan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_W/0/1/0/all/0/1\">Wanchun Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiaolong Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"YANMTT: Yet Another Neural Machine Translation Toolkit. (arXiv:2108.11126v1 [cs.CL])","link":"http://arxiv.org/abs/2108.11126","description":"<p>In this paper we present our open-source neural machine translation (NMT)\ntoolkit called \"Yet Another Neural Machine Translation Toolkit\" abbreviated as\nYANMTT which is built on top of the Transformers library. Despite the growing\nimportance of sequence to sequence pre-training there surprisingly few, if not\nnone, well established toolkits that allow users to easily do pre-training.\nToolkits such as Fairseq which do allow pre-training, have very large codebases\nand thus they are not beginner friendly. With regards to transfer learning via\nfine-tuning most toolkits do not explicitly allow the user to have control over\nwhat parts of the pre-trained models can be transferred. YANMTT aims to address\nthese issues via the minimum amount of code to pre-train large scale NMT\nmodels, selectively transfer pre-trained parameters and fine-tune them, perform\ntranslation as well as extract representations and attentions for visualization\nand analyses. Apart from these core features our toolkit also provides other\nadvanced functionalities such as but not limited to document/multi-source NMT,\nsimultaneous NMT and model compression via distillation which we believe are\nrelevant to the purpose behind our toolkit.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dabre_R/0/1/0/all/0/1\">Raj Dabre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sumita_E/0/1/0/all/0/1\">Eiichiro Sumita</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bridged Adversarial Training. (arXiv:2108.11135v1 [cs.LG])","link":"http://arxiv.org/abs/2108.11135","description":"<p>Adversarial robustness is considered as a required property of deep neural\nnetworks. In this study, we discover that adversarially trained models might\nhave significantly different characteristics in terms of margin and smoothness,\neven they show similar robustness. Inspired by the observation, we investigate\nthe effect of different regularizers and discover the negative effect of the\nsmoothness regularizer on maximizing the margin. Based on the analyses, we\npropose a new method called bridged adversarial training that mitigates the\nnegative effect by bridging the gap between clean and adversarial examples. We\nprovide theoretical and empirical evidence that the proposed method provides\nstable and better robustness, especially for large perturbations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hoki Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_W/0/1/0/all/0/1\">Woojin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sungyoon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jaewook Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Superpixel-guided Discriminative Low-rank Representation of Hyperspectral Images for Classification. (arXiv:2108.11172v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11172","description":"<p>In this paper, we propose a novel classification scheme for the remotely\nsensed hyperspectral image (HSI), namely SP-DLRR, by comprehensively exploring\nits unique characteristics, including the local spatial information and\nlow-rankness. SP-DLRR is mainly composed of two modules, i.e., the\nclassification-guided superpixel segmentation and the discriminative low-rank\nrepresentation, which are iteratively conducted. Specifically, by utilizing the\nlocal spatial information and incorporating the predictions from a typical\nclassifier, the first module segments pixels of an input HSI (or its\nrestoration generated by the second module) into superpixels. According to the\nresulting superpixels, the pixels of the input HSI are then grouped into\nclusters and fed into our novel discriminative low-rank representation model\nwith an effective numerical solution. Such a model is capable of increasing the\nintra-class similarity by suppressing the spectral variations locally while\npromoting the inter-class discriminability globally, leading to a restored HSI\nwith more discriminative pixels. Experimental results on three benchmark\ndatasets demonstrate the significant superiority of SP-DLRR over\nstate-of-the-art methods, especially for the case with an extremely limited\nnumber of training pixels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shujun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1\">Junhui Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1\">Yuheng Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_S/0/1/0/all/0/1\">Shaohui Mei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Q/0/1/0/all/0/1\">Qian Du</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Models In a Spelling Bee: Language Models Implicitly Learn the Character Composition of Tokens. (arXiv:2108.11193v1 [cs.CL])","link":"http://arxiv.org/abs/2108.11193","description":"<p>Standard pretrained language models operate on sequences of subword tokens\nwithout direct access to the characters that compose each token's string\nrepresentation. We probe the embedding layer of pretrained language models and\nshow that models learn the internal character composition of whole word and\nsubword tokens to a surprising extent, without ever seeing the characters\ncoupled with the tokens. Our results show that the embedding layer of RoBERTa\nholds enough information to accurately spell up to a third of the vocabulary\nand reach high average character ngram overlap on all token types. We further\ntest whether enriching subword models with additional character information can\nimprove language modeling, and observe that this method has a near-identical\nlearning curve as training without spelling-based enrichment. Overall, our\nresults suggest that language modeling objectives incentivize the model to\nimplicitly learn some notion of spelling, and that explicitly teaching the\nmodel how to spell does not enhance its performance on such tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Itzhak_I/0/1/0/all/0/1\">Itay Itzhak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_O/0/1/0/all/0/1\">Omer Levy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Subgoal Search For Complex Reasoning Tasks. (arXiv:2108.11204v1 [cs.AI])","link":"http://arxiv.org/abs/2108.11204","description":"<p>Humans excel in solving complex reasoning tasks through a mental process of\nmoving from one idea to a related one. Inspired by this, we propose Subgoal\nSearch (kSubS) method. Its key component is a learned subgoal generator that\nproduces a diversity of subgoals that are both achievable and closer to the\nsolution. Using subgoals reduces the search space and induces a high-level\nsearch graph suitable for efficient planning. In this paper, we implement kSubS\nusing a transformer-based subgoal module coupled with the classical best-first\nsearch framework. We show that a simple approach of generating $k$-th step\nahead subgoals is surprisingly efficient on three challenging domains: two\npopular puzzle games, Sokoban and the Rubik's Cube, and an inequality proving\nbenchmark INT. kSubS achieves strong results including state-of-the-art on INT\nwithin a modest computational budget.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Czechowski_K/0/1/0/all/0/1\">Konrad Czechowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Odrzygozdz_T/0/1/0/all/0/1\">Tomasz Odrzyg&#xf3;&#x17a;d&#x17a;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zbysinski_M/0/1/0/all/0/1\">Marek Zbysi&#x144;ski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zawalski_M/0/1/0/all/0/1\">Micha&#x142; Zawalski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olejnik_K/0/1/0/all/0/1\">Krzysztof Olejnik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuhuai Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kucinski_L/0/1/0/all/0/1\">&#x141;ukasz Kuci&#x144;ski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milos_P/0/1/0/all/0/1\">Piotr Mi&#x142;o&#x15b;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multiscale Spatio-Temporal Graph Neural Networks for 3D Skeleton-Based Motion Prediction. (arXiv:2108.11244v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11244","description":"<p>We propose a multiscale spatio-temporal graph neural network (MST-GNN) to\npredict the future 3D skeleton-based human poses in an action-category-agnostic\nmanner. The core of MST-GNN is a multiscale spatio-temporal graph that\nexplicitly models the relations in motions at various spatial and temporal\nscales. Different from many previous hierarchical structures, our multiscale\nspatio-temporal graph is built in a data-adaptive fashion, which captures\nnonphysical, yet motion-based relations. The key module of MST-GNN is a\nmultiscale spatio-temporal graph computational unit (MST-GCU) based on the\ntrainable graph structure. MST-GCU embeds underlying features at individual\nscales and then fuses features across scales to obtain a comprehensive\nrepresentation. The overall architecture of MST-GNN follows an encoder-decoder\nframework, where the encoder consists of a sequence of MST-GCUs to learn the\nspatial and temporal features of motions, and the decoder uses a graph-based\nattention gate recurrent unit (GA-GRU) to generate future poses. Extensive\nexperiments are conducted to show that the proposed MST-GNN outperforms\nstate-of-the-art methods in both short and long-term motion prediction on the\ndatasets of Human 3.6M, CMU Mocap and 3DPW, where MST-GNN outperforms previous\nworks by 5.33% and 3.67% of mean angle errors in average for short-term and\nlong-term prediction on Human 3.6M, and by 11.84% and 4.71% of mean angle\nerrors for short-term and long-term prediction on CMU Mocap, and by 1.13% of\nmean angle errors on 3DPW in average, respectively. We further investigate the\nlearned multiscale graphs for interpretability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Maosen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Siheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yangheng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ya Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanfeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Long-term, Short-term and Sudden Event: Trading Volume Movement Prediction with Graph-based Multi-view Modeling. (arXiv:2108.11318v1 [q-fin.ST])","link":"http://arxiv.org/abs/2108.11318","description":"<p>Trading volume movement prediction is the key in a variety of financial\napplications. Despite its importance, there is few research on this topic\nbecause of its requirement for comprehensive understanding of information from\ndifferent sources. For instance, the relation between multiple stocks, recent\ntransaction data and suddenly released events are all essential for\nunderstanding trading market. However, most of the previous methods only take\nthe fluctuation information of the past few weeks into consideration, thus\nyielding poor performance. To handle this issue, we propose a graphbased\napproach that can incorporate multi-view information, i.e., long-term stock\ntrend, short-term fluctuation and sudden events information jointly into a\ntemporal heterogeneous graph. Besides, our method is equipped with deep\ncanonical analysis to highlight the correlations between different perspectives\nof fluctuation for better prediction. Experiment results show that our method\noutperforms strong baselines by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-fin/1/au:+Zhao_L/0/1/0/all/0/1\">Liang Zhao</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Bao_R/0/1/0/all/0/1\">Ruihan Bao</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Harimoto_K/0/1/0/all/0/1\">Keiko Harimoto</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+YunfangWu/0/1/0/all/0/1\">YunfangWu</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Sun_X/0/1/0/all/0/1\">Xu Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Ladder of Causal Distances. (arXiv:2005.02480v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2005.02480","description":"<p>Causal discovery, the task of automatically constructing a causal model from\ndata, is of major significance across the sciences. Evaluating the performance\nof causal discovery algorithms should ideally involve comparing the inferred\nmodels to ground-truth models available for benchmark datasets, which in turn\nrequires a notion of distance between causal models. While such distances have\nbeen proposed previously, they are limited by focusing on graphical properties\nof the causal models being compared. Here, we overcome this limitation by\ndefining distances derived from the causal distributions induced by the models,\nrather than exclusively from their graphical structure. Pearl and Mackenzie\n(2018) have arranged the properties of causal models in a hierarchy called the\n\"ladder of causation\" spanning three rungs: observational, interventional, and\ncounterfactual. Following this organization, we introduce a hierarchy of three\ndistances, one for each rung of the ladder. Our definitions are intuitively\nappealing as well as efficient to compute approximately. We put our causal\ndistances to use by benchmarking standard causal discovery systems on both\nsynthetic and real-world datasets for which ground-truth causal models are\navailable. Finally, we highlight the usefulness of our causal distances by\nbriefly discussing further applications beyond the evaluation of causal\ndiscovery techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peyrard_M/0/1/0/all/0/1\">Maxime Peyrard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+West_R/0/1/0/all/0/1\">Robert West</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Workflow Provenance in the Lifecycle of Scientific Machine Learning. (arXiv:2010.00330v3 [cs.DB] UPDATED)","link":"http://arxiv.org/abs/2010.00330","description":"<p>Machine Learning (ML) has already fundamentally changed several businesses.\nMore recently, it has also been profoundly impacting the computational science\nand engineering domains, like geoscience, climate science, and health science.\nIn these domains, users need to perform comprehensive data analyses combining\nscientific data and ML models to provide for critical requirements, such as\nreproducibility, model explainability, and experiment data understanding.\nHowever, scientific ML is multidisciplinary, heterogeneous, and affected by the\nphysical constraints of the domain, making such analyses even more challenging.\nIn this work, we leverage workflow provenance techniques to build a holistic\nview to support the lifecycle of scientific ML. We contribute with (i)\ncharacterization of the lifecycle and taxonomy for data analyses; (ii) design\nprinciples to build this view, with a W3C PROV compliant data representation\nand a reference system architecture; and (iii) lessons learned after an\nevaluation in an Oil &amp; Gas case using an HPC cluster with 393 nodes and 946\nGPUs. The experiments show that the principles enable queries that integrate\ndomain semantics with ML models while keeping low overhead (&lt;1%), high\nscalability, and an order of magnitude of query acceleration under certain\nworkloads against without our representation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Souza_R/0/1/0/all/0/1\">Renan Souza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azevedo_L/0/1/0/all/0/1\">Leonardo G. Azevedo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lourenco_V/0/1/0/all/0/1\">V&#xed;tor Louren&#xe7;o</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soares_E/0/1/0/all/0/1\">Elton Soares</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thiago_R/0/1/0/all/0/1\">Raphael Thiago</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brandao_R/0/1/0/all/0/1\">Rafael Brand&#xe3;o</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Civitarese_D/0/1/0/all/0/1\">Daniel Civitarese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brazil_E/0/1/0/all/0/1\">Emilio Vital Brazil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreno_M/0/1/0/all/0/1\">Marcio Moreno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valduriez_P/0/1/0/all/0/1\">Patrick Valduriez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mattoso_M/0/1/0/all/0/1\">Marta Mattoso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cerqueira_R/0/1/0/all/0/1\">Renato Cerqueira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Netto_M/0/1/0/all/0/1\">Marco A. S. Netto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Discriminative Feature Learning for Accent Recognition. (arXiv:2011.12461v4 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2011.12461","description":"<p>Accent recognition with deep learning framework is a similar work to deep\nspeaker identification, they're both expected to give the input speech an\nidentifiable representation.\n</p>\n<p>Compared with the individual-level features learned by speaker identification\nnetwork, the deep accent recognition work throws a more challenging point that\nforging group-level accent features for speakers.\n</p>\n<p>In this paper, we borrow and improve the deep speaker identification\nframework to recognize accents, in detail, we adopt Convolutional Recurrent\nNeural Network as front-end encoder and integrate local features using\nRecurrent Neural Network to make an utterance-level accent representation.\n</p>\n<p>Novelly, to address overfitting, we simply add Connectionist Temporal\nClassification based speech recognition auxiliary task during training, and for\nambiguous accent discrimination, we introduce some powerful discriminative loss\nfunctions in face recognition works to enhance the discriminative power of\naccent features.\n</p>\n<p>We show that our proposed network with discriminative training method\n(without data-augment) is significantly ahead of the baseline system on the\naccent classification track in the Accented English Speech Recognition\nChallenge 2020, where the loss function Circle-Loss has achieved the best\ndiscriminative optimization for accent representation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiaopei Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge-aware Prompt-tuning with Synergistic Optimization for Relation Extraction. (arXiv:2104.07650v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07650","description":"<p>Recently, prompt-tuning has achieved promising results on some few-class\nclassification tasks. The core idea of prompt-tuning is to insert text pieces,\ni.e., template, to the input and transform a classification task into a masked\nlanguage modeling problem. However, as for relation extraction, determining the\nappropriate prompt template requires domain expertise, and single label word\nhandcrafted or auto-searched is cumbersome and time-consuming to verify their\neffectiveness in non-few-shot scenarios, which also fails to leverage the\nabundant semantic knowledge in the entities and relation labels. To this end,\nwe focus on incorporating knowledge into prompt-tuning for relation extraction\nand propose a knowledge-aware prompt-tuning with synergistic optimization\n(KNIGHT) approach. Specifically, we inject entity and relation knowledge into\nprompt construction with learnable virtual template words and answer words and\njointly optimize their representation with knowledge constraints. Extensive\nexperimental results on 5 datasets with standard and low-resource settings\ndemonstrate the effectiveness of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yunzhi Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Locate Who You Are: Matching Geo-location to Text for User Identity Linkage. (arXiv:2104.09119v2 [cs.SI] UPDATED)","link":"http://arxiv.org/abs/2104.09119","description":"<p>Nowadays, users are encouraged to activate across multiple online social\nnetworks simultaneously. Anchor link prediction, which aims to reveal the\ncorrespondence among different accounts of the same user across networks, has\nbeen regarded as a fundamental problem for user profiling, marketing,\ncybersecurity, and recommendation. Existing methods mainly address the\nprediction problem by utilizing profile, content, or structural features of\nusers in symmetric ways. However, encouraged by online services, users would\nalso post asymmetric information across networks, such as geo-locations and\ntexts. It leads to an emerged challenge in aligning users with asymmetric\ninformation across networks. Instead of similarity evaluation applied in\nprevious works, we formalize correlation between geo-locations and texts and\npropose a novel anchor link prediction framework for matching users across\nnetworks. Moreover, our model can alleviate the label scarcity problem by\nintroducing external data. Experimental results on real-world datasets show\nthat our approach outperforms existing methods and achieves state-of-the-art\nresults.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1\">Jiangli Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongqing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_H/0/1/0/all/0/1\">Hao Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Huawei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yangyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xueqi Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Agent Routing and Scheduling Through Coalition Formation. (arXiv:2105.00451v2 [cs.MA] UPDATED)","link":"http://arxiv.org/abs/2105.00451","description":"<p>In task allocation for real-time domains, such as disaster response, a\nlimited number of agents is deployed across a large area to carry out numerous\ntasks, each with its prerequisites, profit, time window and workload. To\nmaximize profits while minimizing time penalties, agents need to cooperate by\nforming, disbanding and reforming coalitions. In this paper, we name this\nproblem Multi-Agent Routing and Scheduling through Coalition formation (MARSC)\nand show that it generalizes the important Team Orienteering Problem with Time\nWindows. We propose a binary integer program and an anytime and scalable\nheuristic to solve it. Using public London Fire Brigade records, we create a\ndataset with 347588 tasks and a test framework that simulates the mobilization\nof firefighters. In problems with up to 150 agents and 3000 tasks, our\nheuristic finds solutions up to 3.25 times better than the Earliest Deadline\nFirst approach commonly used in real-time systems. Our results constitute the\nfirst large-scale benchmark for the MARSC problem.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Capezzuto_L/0/1/0/all/0/1\">Luca Capezzuto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tarapore_D/0/1/0/all/0/1\">Danesh Tarapore</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramchurn_S/0/1/0/all/0/1\">Sarvapali D. Ramchurn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Feature Decorrelation in Self-Supervised Learning. (arXiv:2105.00470v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2105.00470","description":"<p>In self-supervised representation learning, a common idea behind most of the\nstate-of-the-art approaches is to enforce the robustness of the representations\nto predefined augmentations. A potential issue of this idea is the existence of\ncompletely collapsed solutions (i.e., constant features), which are typically\navoided implicitly by carefully chosen implementation details. In this work, we\nstudy a relatively concise framework containing the most common components from\nrecent approaches. We verify the existence of complete collapse and discover\nanother reachable collapse pattern that is usually overlooked, namely\ndimensional collapse. We connect dimensional collapse with strong correlations\nbetween axes and consider such connection as a strong motivation for feature\ndecorrelation (i.e., standardizing the covariance matrix). The gains from\nfeature decorrelation are verified empirically to highlight the importance and\nthe potential of this insight.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hua_T/0/1/0/all/0/1\">Tianyu Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenxiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_Z/0/1/0/all/0/1\">Zihui Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1\">Sucheng Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hang Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Episodic Transformer for Vision-and-Language Navigation. (arXiv:2105.06453v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.06453","description":"<p>Interaction and navigation defined by natural language instructions in\ndynamic environments pose significant challenges for neural agents. This paper\nfocuses on addressing two challenges: handling long sequence of subtasks, and\nunderstanding complex human instructions. We propose Episodic Transformer\n(E.T.), a multimodal transformer that encodes language inputs and the full\nepisode history of visual observations and actions. To improve training, we\nleverage synthetic instructions as an intermediate representation that\ndecouples understanding the visual appearance of an environment from the\nvariations of natural language instructions. We demonstrate that encoding the\nhistory with a transformer is critical to solve compositional tasks, and that\npretraining and joint training with synthetic instructions further improve the\nperformance. Our approach sets a new state of the art on the challenging ALFRED\nbenchmark, achieving 38.4% and 8.5% task success rates on seen and unseen test\nsplits.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pashevich_A/0/1/0/all/0/1\">Alexander Pashevich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1\">Cordelia Schmid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Chen Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Intrusion Prevention Policies through Optimal Stopping. (arXiv:2106.07160v5 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2106.07160","description":"<p>We study automated intrusion prevention using reinforcement learning. In a\nnovel approach, we formulate the problem of intrusion prevention as an optimal\nstopping problem. This formulation allows us insight into the structure of the\noptimal policies, which turn out to be threshold based. Since the computation\nof the optimal defender policy using dynamic programming is not feasible for\npractical cases, we approximate the optimal policy through reinforcement\nlearning in a simulation environment. To define the dynamics of the simulation,\nwe emulate the target infrastructure and collect measurements. Our evaluations\nshow that the learned policies are close to optimal and that they indeed can be\nexpressed using thresholds.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hammar_K/0/1/0/all/0/1\">Kim Hammar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stadler_R/0/1/0/all/0/1\">Rolf Stadler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Universal Domain Adaptation in Ordinal Regression. (arXiv:2106.11576v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.11576","description":"<p>We address the problem of universal domain adaptation (UDA) in ordinal\nregression (OR), which attempts to solve classification problems in which\nlabels are not independent, but follow a natural order. We show that the UDA\ntechniques developed for classification and based on the clustering assumption,\nunder-perform in OR settings. We propose a method that complements the OR\nclassifier with an auxiliary task of order learning, which plays the double\nrole of discriminating between common and private instances, and expanding\nclass labels to the private target images via ranking. Combined with\nadversarial domain discrimination, our model is able to address the closed set,\npartial and open set configurations. We evaluate our method on three face age\nestimation datasets, and show that it outperforms the baseline methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chidlovskii_B/0/1/0/all/0/1\">Boris Chidlovskii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadek_A/0/1/0/all/0/1\">Assem Sadek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_C/0/1/0/all/0/1\">Christian Wolf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Machine Learning Characterization of Cancer Patients-Derived Extracellular Vesicles using Vibrational Spectroscopies. (arXiv:2107.10332v3 [q-bio.OT] UPDATED)","link":"http://arxiv.org/abs/2107.10332","description":"<p>The early detection of cancer is a challenging problem in medicine. The blood\nsera of cancer patients are enriched with heterogeneous secretory lipid bound\nextracellular vesicles (EVs), which present a complex repertoire of information\nand biomarkers, representing their cell of origin, that are being currently\nstudied in the field of liquid biopsy and cancer screening. Vibrational\nspectroscopies provide non-invasive approaches for the assessment of structural\nand biophysical properties in complex biological samples. In this study,\nmultiple Raman spectroscopy measurements were performed on the EVs extracted\nfrom the blood sera of 9 patients consisting of four different cancer subtypes\n(colorectal cancer, hepatocellular carcinoma, breast cancer and pancreatic\ncancer) and five healthy patients (controls). FTIR(Fourier Transform Infrared)\nspectroscopy measurements were performed as a complementary approach to Raman\nanalysis, on two of the four cancer subtypes.\n</p>\n<p>The AdaBoost Random Forest Classifier, Decision Trees, and Support Vector\nMachines (SVM) distinguished the baseline corrected Raman spectra of cancer EVs\nfrom those of healthy controls (18 spectra) with a classification accuracy of\ngreater than 90% when reduced to a spectral frequency range of 1800 to 1940\ninverse cm, and subjected to a 0.5 training/testing split. FTIR classification\naccuracy on 14 spectra showed an 80% classification accuracy. Our findings\ndemonstrate that basic machine learning algorithms are powerful tools to\ndistinguish the complex vibrational spectra of cancer patient EVs from those of\nhealthy patients. These experimental methods hold promise as valid and\nefficient liquid biopsy for machine intelligence-assisted early cancer\nscreening.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Uthamacumaran_A/0/1/0/all/0/1\">Abicumaran Uthamacumaran</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Elouatik_S/0/1/0/all/0/1\">Samir Elouatik</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Abdouh_M/0/1/0/all/0/1\">Mohamed Abdouh</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Berteau_Rainville_M/0/1/0/all/0/1\">Michael Berteau-Rainville</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Gao_Z/0/1/0/all/0/1\">Zu-hua Gao</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Arena_G/0/1/0/all/0/1\">Goffredo Arena</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An ASP-based Solution to the Chemotherapy Treatment Scheduling problem. (arXiv:2108.02637v3 [cs.LO] UPDATED)","link":"http://arxiv.org/abs/2108.02637","description":"<p>The problem of scheduling chemotherapy treatments in oncology clinics is a\ncomplex problem, given that the solution has to satisfy (as much as possible)\nseveral requirements such as the cyclic nature of chemotherapy treatment plans,\nmaintaining a constant number of patients, and the availability of resources,\ne.g., treatment time, nurses, and drugs. At the same time, realizing a\nsatisfying schedule is of upmost importance for obtaining the best health\noutcomes. In this paper we first consider a specific instance of the problem\nwhich is employed in the San Martino Hospital in Genova, Italy, and present a\nsolution to the problem based on Answer Set Programming (ASP). Then, we enrich\nthe problem and the related ASP encoding considering further features often\nemployed in other hospitals, desirable also in S. Martino, and/or considered in\nrelated papers. Results of an experimental analysis, conducted on the real data\nprovided by the San Martino Hospital, show that ASP is an effective solving\nmethodology also for this important scheduling problem. Under consideration for\nacceptance in TPLP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dodaro_C/0/1/0/all/0/1\">Carmine Dodaro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galata_G/0/1/0/all/0/1\">Giuseppe Galat&#xe0;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grioni_A/0/1/0/all/0/1\">Andrea Grioni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maratea_M/0/1/0/all/0/1\">Marco Maratea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mochi_M/0/1/0/all/0/1\">Marco Mochi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Porro_I/0/1/0/all/0/1\">Ivan Porro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SERF: Towards better training of deep neural networks using log-Softplus ERror activation Function. (arXiv:2108.09598v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2108.09598","description":"<p>Activation functions play a pivotal role in determining the training dynamics\nand neural network performance. The widely adopted activation function ReLU\ndespite being simple and effective has few disadvantages including the Dying\nReLU problem. In order to tackle such problems, we propose a novel activation\nfunction called Serf which is self-regularized and nonmonotonic in nature. Like\nMish, Serf also belongs to the Swish family of functions. Based on several\nexperiments on computer vision (image classification and object detection) and\nnatural language processing (machine translation, sentiment classification and\nmultimodal entailment) tasks with different state-of-the-art architectures, it\nis observed that Serf vastly outperforms ReLU (baseline) and other activation\nfunctions including both Swish and Mish, with a markedly bigger margin on\ndeeper architectures. Ablation studies further demonstrate that Serf based\narchitectures perform better than those of Swish and Mish in varying scenarios,\nvalidating the effectiveness and compatibility of Serf with varying depth,\ncomplexity, optimizers, learning rates, batch sizes, initializers and dropout\nrates. Finally, we investigate the mathematical relation between Swish and\nSerf, thereby showing the impact of preconditioner function ingrained in the\nfirst derivative of Serf which provides a regularization effect making\ngradients smoother and optimization faster.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nag_S/0/1/0/all/0/1\">Sayan Nag</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharyya_M/0/1/0/all/0/1\">Mayukh Bhattacharyya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"APObind: A Dataset of Ligand Unbound Protein Conformations for Machine Learning Applications in De Novo Drug Design. (arXiv:2108.09926v2 [q-bio.BM] UPDATED)","link":"http://arxiv.org/abs/2108.09926","description":"<p>Protein-ligand complex structures have been utilised to design benchmark\nmachine learning methods that perform important tasks related to drug design\nsuch as receptor binding site detection, small molecule docking and binding\naffinity prediction. However, these methods are usually trained on only ligand\nbound (or holo) conformations of the protein and therefore are not guaranteed\nto perform well when the protein structure is in its native unbound\nconformation (or apo), which is usually the conformation available for a newly\nidentified receptor. A primary reason for this is that the local structure of\nthe binding site usually changes upon ligand binding. To facilitate solutions\nfor this problem, we propose a dataset called APObind that aims to provide apo\nconformations of proteins present in the PDBbind dataset, a popular dataset\nused in drug design. Furthermore, we explore the performance of methods\nspecific to three use cases on this dataset, through which, the importance of\nvalidating them on the APObind dataset is demonstrated.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Aggarwal_R/0/1/0/all/0/1\">Rishal Aggarwal</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Gupta_A/0/1/0/all/0/1\">Akash Gupta</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Priyakumar_U/0/1/0/all/0/1\">U Deva Priyakumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Autoencoder-based Semantic Novelty Detection: Towards Dependable AI-based Systems. (arXiv:2108.10851v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2108.10851","description":"<p>Many autonomous systems, such as driverless taxis, perform safety critical\nfunctions. Autonomous systems employ artificial intelligence (AI) techniques,\nspecifically for the environment perception. Engineers cannot completely test\nor formally verify AI-based autonomous systems. The accuracy of AI-based\nsystems depends on the quality of training data. Thus, novelty detection -\nidentifying data that differ in some respect from the data used for training -\nbecomes a safety measure for system development and operation. In this paper,\nwe propose a new architecture for autoencoder-based semantic novelty detection\nwith two innovations: architectural guidelines for a semantic autoencoder\ntopology and a semantic error calculation as novelty criteria. We demonstrate\nthat such a semantic novelty detection outperforms autoencoder-based novelty\ndetection approaches known from literature by minimizing false negatives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rausch_A/0/1/0/all/0/1\">Andreas Rausch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sedeh_A/0/1/0/all/0/1\">Azarmidokht Motamedi Sedeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Meng Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-08-25T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Artificial Intelligence"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/"}},{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"SimVLM: Simple Visual Language Model Pretraining with Weak Supervision. (arXiv:2108.10904v1 [cs.CV])","link":"http://arxiv.org/abs/2108.10904","description":"<p>With recent progress in joint modeling of visual and textual representations,\nVision-Language Pretraining (VLP) has achieved impressive performance on many\nmultimodal downstream tasks. However, the requirement for expensive annotations\nincluding clean image captions and regional labels limits the scalability of\nexisting approaches, and complicates the pretraining procedure with the\nintroduction of multiple dataset-specific objectives. In this work, we relax\nthese constraints and present a minimalist pretraining framework, named Simple\nVisual Language Model (SimVLM). Unlike prior work, SimVLM reduces the training\ncomplexity by exploiting large-scale weak supervision, and is trained\nend-to-end with a single prefix language modeling objective. Without utilizing\nextra data or task-specific customization, the resulting model significantly\noutperforms previous pretraining methods and achieves new state-of-the-art\nresults on a wide range of discriminative and generative vision-language\nbenchmarks, including VQA (+3.74% vqa-score), NLVR2 (+1.17% accuracy), SNLI-VE\n(+1.37% accuracy) and image captioning tasks (+10.1% average CIDEr score).\nFurthermore, we demonstrate that SimVLM acquires strong generalization and\ntransfer ability, enabling zero-shot behavior including open-ended visual\nquestion answering and cross-modality transfer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zirui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jiahui Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_A/0/1/0/all/0/1\">Adams Wei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Z/0/1/0/all/0/1\">Zihang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1\">Yulia Tsvetkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yuan Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hybrid Multisource Feature Fusion for the Text Clustering. (arXiv:2108.10926v1 [cs.CL])","link":"http://arxiv.org/abs/2108.10926","description":"<p>The text clustering technique is an unsupervised text mining method which are\nused to partition a huge amount of text documents into groups. It has been\nreported that text clustering algorithms are hard to achieve better performance\nthan supervised methods and their clustering performance is highly dependent on\nthe picked text features. Currently, there are many different types of text\nfeature generation algorithms, each of which extracts text features from some\nspecific aspects, such as VSM and distributed word embedding, thus seeking a\nnew way of obtaining features as complete as possible from the corpus is the\nkey to enhance the clustering effects. In this paper, we present a hybrid\nmultisource feature fusion (HMFF) framework comprising three components,\nfeature representation of multimodel, mutual similarity matrices and feature\nfusion, in which we construct mutual similarity matrices for each feature\nsource and fuse discriminative features from mutual similarity matrices by\nreducing dimensionality to generate HMFF features, then k-means clustering\nalgorithm could be configured to partition input samples into groups. The\nexperimental tests show our HMFF framework outperforms other recently published\nalgorithms on 7 of 11 public benchmark datasets and has the leading performance\non the rest 4 benchmark datasets as well. At last, we compare HMFF framework\nwith those competitors on a COVID-19 dataset from the wild with the unknown\ncluster count, which shows the clusters generated by HMFF framework partition\nthose similar samples much closer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiaxuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gui_S/0/1/0/all/0/1\">Shenglin Gui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The State of SLIVAR: What's next for robots, human-robot interaction, and (spoken) dialogue systems?. (arXiv:2108.10931v1 [cs.CL])","link":"http://arxiv.org/abs/2108.10931","description":"<p>We synthesize the reported results and recommendations of recent workshops\nand seminars that convened to discuss open questions within the important\nintersection of robotics, human-robot interaction, and spoken dialogue systems\nresearch. The goal of this growing area of research interest is to enable\npeople to more effectively and naturally communicate with robots. To carry\nforward opportunities networking and discussion towards concrete, potentially\nfundable projects, we encourage interested parties to consider participating in\nfuture virtual and in-person discussions and workshops.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kennington_C/0/1/0/all/0/1\">Casey Kennington</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SN Computer Science: Towards Offensive Language Identification for Tamil Code-Mixed YouTube Comments and Posts. (arXiv:2108.10939v1 [cs.CL])","link":"http://arxiv.org/abs/2108.10939","description":"<p>Offensive Language detection in social media platforms has been an active\nfield of research over the past years. In non-native English spoken countries,\nsocial media users mostly use a code-mixed form of text in their\nposts/comments. This poses several challenges in the offensive content\nidentification tasks, and considering the low resources available for Tamil,\nthe task becomes much harder. The current study presents extensive experiments\nusing multiple deep learning, and transfer learning models to detect offensive\ncontent on YouTube. We propose a novel and flexible approach of selective\ntranslation and transliteration techniques to reap better results from\nfine-tuning and ensembling multilingual transformer networks like BERT, Distil-\nBERT, and XLM-RoBERTa. The experimental results showed that ULMFiT is the best\nmodel for this task. The best performing models were ULMFiT and mBERTBiLSTM for\nthis Tamil code-mix dataset instead of more popular transfer learning models\nsuch as Distil- BERT and XLM-RoBERTa and hybrid deep learning models. The\nproposed model ULMFiT and mBERTBiLSTM yielded good results and are promising\nfor effective offensive speech identification in low-resourced languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vasantharajan_C/0/1/0/all/0/1\">Charangan Vasantharajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thayasivam_U/0/1/0/all/0/1\">Uthayasanker Thayasivam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robustness Evaluation of Entity Disambiguation Using Prior Probes:the Case of Entity Overshadowing. (arXiv:2108.10949v1 [cs.CL])","link":"http://arxiv.org/abs/2108.10949","description":"<p>Entity disambiguation (ED) is the last step of entity linking (EL), when\ncandidate entities are reranked according to the context they appear in. All\ndatasets for training and evaluating models for EL consist of convenience\nsamples, such as news articles and tweets, that propagate the prior probability\nbias of the entity distribution towards more frequently occurring entities. It\nwas previously shown that the performance of the EL systems on such datasets is\noverestimated since it is possible to obtain higher accuracy scores by merely\nlearning the prior. To provide a more adequate evaluation benchmark, we\nintroduce the ShadowLink dataset, which includes 16K short text snippets\nannotated with entity mentions. We evaluate and report the performance of\npopular EL systems on the ShadowLink benchmark. The results show a considerable\ndifference in accuracy between more and less common entities for all of the EL\nsystems under evaluation, demonstrating the effects of prior probability bias\nand entity overshadowing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Provatorova_V/0/1/0/all/0/1\">Vera Provatorova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vakulenko_S/0/1/0/all/0/1\">Svitlana Vakulenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhargav_S/0/1/0/all/0/1\">Samarth Bhargav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanoulas_E/0/1/0/all/0/1\">Evangelos Kanoulas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using BERT Encoding and Sentence-Level Language Model for Sentence Ordering. (arXiv:2108.10986v1 [cs.CL])","link":"http://arxiv.org/abs/2108.10986","description":"<p>Discovering the logical sequence of events is one of the cornerstones in\nNatural Language Understanding. One approach to learn the sequence of events is\nto study the order of sentences in a coherent text. Sentence ordering can be\napplied in various tasks such as retrieval-based Question Answering, document\nsummarization, storytelling, text generation, and dialogue systems.\nFurthermore, we can learn to model text coherence by learning how to order a\nset of shuffled sentences. Previous research has relied on RNN, LSTM, and\nBiLSTM architecture for learning text language models. However, these networks\nhave performed poorly due to the lack of attention mechanisms. We propose an\nalgorithm for sentence ordering in a corpus of short stories. Our proposed\nmethod uses a language model based on Universal Transformers (UT) that captures\nsentences' dependencies by employing an attention mechanism. Our method\nimproves the previous state-of-the-art in terms of Perfect Match Ratio (PMR)\nscore in the ROCStories dataset, a corpus of nearly 100K short human-made\nstories. The proposed model includes three components: Sentence Encoder,\nLanguage Model, and Sentence Arrangement with Brute Force Search. The first\ncomponent generates sentence embeddings using SBERT-WK pre-trained model\nfine-tuned on the ROCStories data. Then a Universal Transformer network\ngenerates a sentence-level language model. For decoding, the network generates\na candidate sentence as the following sentence of the current sentence. We use\ncosine similarity as a scoring function to assign scores to the candidate\nembedding and the embeddings of other sentences in the shuffled set. Then a\nBrute Force Search is employed to maximize the sum of similarities between\npairs of consecutive sentences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Golestani_M/0/1/0/all/0/1\">Melika Golestani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Razavi_S/0/1/0/all/0/1\">Seyedeh Zahra Razavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borhanifard_Z/0/1/0/all/0/1\">Zeinab Borhanifard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tahmasebian_F/0/1/0/all/0/1\">Farnaz Tahmasebian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faili_H/0/1/0/all/0/1\">Hesham Faili</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Natural Language Processing Accurately Categorizes Indications, Findings and Pathology Reports from Multicenter Colonoscopy. (arXiv:2108.11034v1 [cs.CL])","link":"http://arxiv.org/abs/2108.11034","description":"<p>Colonoscopy is used for colorectal cancer (CRC) screening. Extracting details\nof the colonoscopy findings from free text in electronic health records (EHRs)\ncan be used to determine patient risk for CRC and colorectal screening\nstrategies. We developed and evaluated the accuracy of a deep learning model\nframework to extract information for the clinical decision support system to\ninterpret relevant free-text reports, including indications, pathology, and\nfindings notes. The Bio-Bi-LSTM-CRF framework was developed using Bidirectional\nLong Short-term Memory (Bi-LSTM) and Conditional Random Fields (CRF) to extract\nseveral clinical features from these free-text reports including indications\nfor the colonoscopy, findings during the colonoscopy, and pathology of resected\nmaterial. We trained the Bio-Bi-LSTM-CRF and existing Bi-LSTM-CRF models on 80%\nof 4,000 manually annotated notes from 3,867 patients. These clinical notes\nwere from a group of patients over 40 years of age enrolled in four Veterans\nAffairs Medical Centers. A total of 10% of the remaining annotated notes were\nused to train hyperparameter and the remaining 10% were used to evaluate the\naccuracy of our model Bio-Bi-LSTM-CRF and compare to Bi-LSTM-CRF.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vadyala_S/0/1/0/all/0/1\">Shashank Reddy Vadyala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sherer_E/0/1/0/all/0/1\">Eric A. Sherer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How COVID-19 has Impacted American Attitudes Toward China: A Study on Twitter. (arXiv:2108.11040v1 [cs.SI])","link":"http://arxiv.org/abs/2108.11040","description":"<p>Past research has studied social determinants of attitudes toward foreign\ncountries. Confounded by potential endogeneity biases due to unobserved factors\nor reverse causality, the causal impact of these factors on public opinion is\nusually difficult to establish. Using social media data, we leverage the\nsuddenness of the COVID-19 pandemic to examine whether a major global event has\ncausally changed American views of another country. We collate a database of\nmore than 297 million posts on the social media platform Twitter about China or\nCOVID-19 up to June 2020, and we treat tweeting about COVID-19 as a proxy for\nindividual awareness of COVID-19. Using regression discontinuity and\ndifference-in-difference estimation, we find that awareness of COVID-19 causes\na sharp rise in anti-China attitudes. Our work has implications for\nunderstanding how self-interest affects policy preference and how Americans\nview migrant communities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cook_G/0/1/0/all/0/1\">Gavin Cook</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Junming Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yu Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Viola: A Topic Agnostic Generate-and-Rank Dialogue System. (arXiv:2108.11063v1 [cs.CL])","link":"http://arxiv.org/abs/2108.11063","description":"<p>We present Viola, an open-domain dialogue system for spoken conversation that\nuses a topic-agnostic dialogue manager based on a simple generate-and-rank\napproach. Leveraging recent advances of generative dialogue systems powered by\nlarge language models, Viola fetches a batch of response candidates from\nvarious neural dialogue models trained with different datasets and\nknowledge-grounding inputs. Additional responses originating from\ntemplate-based generators are also considered, depending on the user's input\nand detected entities. The hand-crafted generators build on a dynamic knowledge\ngraph injected with rich content that is crawled from the web and automatically\nprocessed on a daily basis. Viola's response ranker is a fine-tuned polyencoder\nthat chooses the best response given the dialogue history. While dedicated\nannotations for the polyencoder alone can indirectly steer it away from\nchoosing problematic responses, we add rule-based safety nets to detect neural\ndegeneration and a dedicated classifier to filter out offensive content. We\nanalyze conversations that Viola took part in for the Alexa Prize Socialbot\nGrand Challenge 4 and discuss the strengths and weaknesses of our approach.\nLastly, we suggest future work with a focus on curating conversation data\nspecifcially for socialbots that will contribute towards a more robust\ndata-driven socialbot.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cho_H/0/1/0/all/0/1\">Hyundong Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shbita_B/0/1/0/all/0/1\">Basel Shbita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shenoy_K/0/1/0/all/0/1\">Kartik Shenoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shuai Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_N/0/1/0/all/0/1\">Nikhil Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pindikanti_H/0/1/0/all/0/1\">Hitesh Pindikanti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jennifer Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+May_J/0/1/0/all/0/1\">Jonathan May</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"YANMTT: Yet Another Neural Machine Translation Toolkit. (arXiv:2108.11126v1 [cs.CL])","link":"http://arxiv.org/abs/2108.11126","description":"<p>In this paper we present our open-source neural machine translation (NMT)\ntoolkit called \"Yet Another Neural Machine Translation Toolkit\" abbreviated as\nYANMTT which is built on top of the Transformers library. Despite the growing\nimportance of sequence to sequence pre-training there surprisingly few, if not\nnone, well established toolkits that allow users to easily do pre-training.\nToolkits such as Fairseq which do allow pre-training, have very large codebases\nand thus they are not beginner friendly. With regards to transfer learning via\nfine-tuning most toolkits do not explicitly allow the user to have control over\nwhat parts of the pre-trained models can be transferred. YANMTT aims to address\nthese issues via the minimum amount of code to pre-train large scale NMT\nmodels, selectively transfer pre-trained parameters and fine-tune them, perform\ntranslation as well as extract representations and attentions for visualization\nand analyses. Apart from these core features our toolkit also provides other\nadvanced functionalities such as but not limited to document/multi-source NMT,\nsimultaneous NMT and model compression via distillation which we believe are\nrelevant to the purpose behind our toolkit.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dabre_R/0/1/0/all/0/1\">Raj Dabre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sumita_E/0/1/0/all/0/1\">Eiichiro Sumita</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Models In a Spelling Bee: Language Models Implicitly Learn the Character Composition of Tokens. (arXiv:2108.11193v1 [cs.CL])","link":"http://arxiv.org/abs/2108.11193","description":"<p>Standard pretrained language models operate on sequences of subword tokens\nwithout direct access to the characters that compose each token's string\nrepresentation. We probe the embedding layer of pretrained language models and\nshow that models learn the internal character composition of whole word and\nsubword tokens to a surprising extent, without ever seeing the characters\ncoupled with the tokens. Our results show that the embedding layer of RoBERTa\nholds enough information to accurately spell up to a third of the vocabulary\nand reach high average character ngram overlap on all token types. We further\ntest whether enriching subword models with additional character information can\nimprove language modeling, and observe that this method has a near-identical\nlearning curve as training without spelling-based enrichment. Overall, our\nresults suggest that language modeling objectives incentivize the model to\nimplicitly learn some notion of spelling, and that explicitly teaching the\nmodel how to spell does not enhance its performance on such tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Itzhak_I/0/1/0/all/0/1\">Itay Itzhak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_O/0/1/0/all/0/1\">Omer Levy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring the Promises of Transformer-Based LMs for the Representation of Normative Claims in the Legal Domain. (arXiv:2108.11215v1 [cs.CL])","link":"http://arxiv.org/abs/2108.11215","description":"<p>In this article, we explore the potential of transformer-based language\nmodels (LMs) to correctly represent normative statements in the legal domain,\ntaking tax law as our use case. In our experiment, we use a variety of LMs as\nbases for both word- and sentence-based clusterers that are then evaluated on a\nsmall, expert-compiled test-set, consisting of real-world samples from tax law\nresearch literature that can be clearly assigned to one of four normative\ntheories. The results of the experiment show that clusterers based on\nsentence-BERT-embeddings deliver the most promising results. Based on this main\nexperiment, we make first attempts at using the best performing models in a\nbootstrapping loop to build classifiers that map normative claims on one of\nthese four normative theories.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gubelmann_R/0/1/0/all/0/1\">Reto Gubelmann</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Hongler_P/0/1/0/all/0/1\">Peter Hongler</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Handschuh_S/0/1/0/all/0/1\">Siegfried Handschuh</a> (1) ((1) University of St.Gallen (HSG))"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ontology-Enhanced Slot Filling. (arXiv:2108.11275v1 [cs.CL])","link":"http://arxiv.org/abs/2108.11275","description":"<p>Slot filling is a fundamental task in dialog state tracking in task-oriented\ndialog systems. In multi-domain task-oriented dialog system, user utterances\nand system responses may mention multiple named entities and attributes values.\nA system needs to select those that are confirmed by the user and fill them\ninto destined slots. One difficulty is that since a dialogue session contains\nmultiple system-user turns, feeding in all the tokens into a deep model such as\nBERT can be challenging due to limited capacity of input word tokens and GPU\nmemory. In this paper, we investigate an ontology-enhanced approach by matching\nthe named entities occurred in all dialogue turns using ontology. The matched\nentities in the previous dialogue turns will be accumulated and encoded as\nadditional inputs to a BERT-based dialogue state tracker. In addition, our\nimprovement includes ontology constraint checking and the correction of slot\nname tokenization. Experimental results showed that our ontology-enhanced\ndialogue state tracker improves the joint goal accuracy (slot F1) from 52.63%\n(91.64%) to 53.91% (92%) on MultiWOZ 2.1 corpus.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yuhao Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tam_Y/0/1/0/all/0/1\">Yik-Cheung Tam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ProoFVer: Natural Logic Theorem Proving for Fact Verification. (arXiv:2108.11357v1 [cs.CL])","link":"http://arxiv.org/abs/2108.11357","description":"<p>We propose ProoFVer, a proof system for fact verification using natural\nlogic. The textual entailment model in ProoFVer is a seq2seq model generating\nvalid natural-logic based logical inferences as its proofs. The generation of\nproofs makes ProoFVer an explainable system. The proof consists of iterative\nlexical mutations of spans in the claim with spans in a set of retrieved\nevidence sentences. Further, each such mutation is marked with an entailment\nrelation using natural logic operators. The veracity of a claim is determined\nsolely based on the sequence of natural logic relations present in the proof.\nBy design, this makes ProoFVer a faithful by construction system that generates\nfaithful explanations. ProoFVer outperforms existing fact-verification models,\nwith more than two percent absolute improvements in performance and robustness.\nIn addition to its explanations being faithful, ProoFVer also scores high on\nrationale extraction, with a five point absolute improvement compared to\nattention-based rationales in existing models. Finally, we find that humans\ncorrectly simulate ProoFVer's decisions more often using the proofs, than the\ndecisions of an existing model that directly use the retrieved evidence for\ndecision making.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Krishna_A/0/1/0/all/0/1\">Amrith Krishna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedel_S/0/1/0/all/0/1\">Sebastian Riedel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vlachos_A/0/1/0/all/0/1\">Andreas Vlachos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Entity Linking: A Survey of Models Based on Deep Learning. (arXiv:2006.00575v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2006.00575","description":"<p>In this survey, we provide a comprehensive description of recent neural\nentity linking (EL) systems developed since 2015 as a result of the \"deep\nlearning revolution\" in NLP. Our goal is to systemize design features of neural\nentity linking systems and compare their performance to the prominent classic\nmethods on common benchmarks. We distill generic architectural components of a\nneural EL system, like candidate generation and entity ranking, and summarize\nprominent methods for each of them. The vast variety of modifications of this\ngeneral neural entity linking architecture are grouped by several common\nthemes: joint entity recognition and linking, models for global linking,\ndomain-independent techniques including zero-shot and distant supervision\nmethods, and cross-lingual approaches. Since many neural models take advantage\nof entity and mention/context embeddings to catch semantic meaning of them, we\nprovide an overview of popular embedding techniques. Finally, we briefly\ndiscuss applications of entity linking, focusing on the recently emerged\nuse-case of enhancing deep pre-trained masked language models based on the\ntransformer architecture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sevgili_O/0/1/0/all/0/1\">Ozge Sevgili</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shelmanov_A/0/1/0/all/0/1\">Artem Shelmanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arkhipov_M/0/1/0/all/0/1\">Mikhail Arkhipov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panchenko_A/0/1/0/all/0/1\">Alexander Panchenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biemann_C/0/1/0/all/0/1\">Chris Biemann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Cross-Modal Pre-Training: A General Strategy for Small Sample Medical Imaging. (arXiv:2010.03060v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2010.03060","description":"<p>A key challenge in training neural networks for a given medical imaging task\nis often the difficulty of obtaining a sufficient number of manually labeled\nexamples. In contrast, textual imaging reports, which are often readily\navailable in medical records, contain rich but unstructured interpretations\nwritten by experts as part of standard clinical practice. We propose using\nthese textual reports as a form of weak supervision to improve the image\ninterpretation performance of a neural network without requiring additional\nmanually labeled examples. We use an image-text matching task to train a\nfeature extractor and then fine-tune it in a transfer learning setting for a\nsupervised task using a small labeled dataset. The end result is a neural\nnetwork that automatically interprets imagery without requiring textual reports\nduring inference. This approach can be applied to any task for which text-image\npairs are readily available. We evaluate our method on three classification\ntasks and find consistent performance improvements, reducing the need for\nlabeled data by 67%-98%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_G/0/1/0/all/0/1\">Gongbo Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Greenwell_C/0/1/0/all/0/1\">Connor Greenwell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoqin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kavuluru_R/0/1/0/all/0/1\">Ramakanth Kavuluru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacobs_N/0/1/0/all/0/1\">Nathan Jacobs</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Telling the What while Pointing to the Where: Multimodal Queries for Image Retrieval. (arXiv:2102.04980v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.04980","description":"<p>Most existing image retrieval systems use text queries as a way for the user\nto express what they are looking for. However, fine-grained image retrieval\noften requires the ability to also express where in the image the content they\nare looking for is. The text modality can only cumbersomely express such\nlocalization preferences, whereas pointing is a more natural fit. In this\npaper, we propose an image retrieval setup with a new form of multimodal\nqueries, where the user simultaneously uses both spoken natural language (the\nwhat) and mouse traces over an empty canvas (the where) to express the\ncharacteristics of the desired target image. We then describe simple\nmodifications to an existing image retrieval model, enabling it to operate in\nthis setup. Qualitative and quantitative experiments show that our model\neffectively takes this spatial guidance into account, and provides\nsignificantly more accurate retrieval results compared to text-only equivalent\nsystems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Changpinyo_S/0/1/0/all/0/1\">Soravit Changpinyo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pont_Tuset_J/0/1/0/all/0/1\">Jordi Pont-Tuset</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrari_V/0/1/0/all/0/1\">Vittorio Ferrari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soricut_R/0/1/0/all/0/1\">Radu Soricut</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Road to Know-Where: An Object-and-Room Informed Sequential BERT for Indoor Vision-Language Navigation. (arXiv:2104.04167v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.04167","description":"<p>Vision-and-Language Navigation (VLN) requires an agent to find a path to a\nremote location on the basis of natural-language instructions and a set of\nphoto-realistic panoramas. Most existing methods take the words in the\ninstructions and the discrete views of each panorama as the minimal unit of\nencoding. However, this requires a model to match different nouns (e.g., TV,\ntable) against the same input view feature. In this work, we propose an\nobject-informed sequential BERT to encode visual perceptions and linguistic\ninstructions at the same fine-grained level, namely objects and words. Our\nsequential BERT also enables the visual-textual clues to be interpreted in\nlight of the temporal context, which is crucial to multi-round VLN tasks.\nAdditionally, we enable the model to identify the relative direction (e.g.,\nleft/right/front/back) of each navigable location and the room type (e.g.,\nbedroom, kitchen) of its current and final navigation goal, as such information\nis widely mentioned in instructions implying the desired next and final\nlocations. We thus enable the model to know-where the objects lie in the\nimages, and to know-where they stand in the scene. Extensive experiments\ndemonstrate the effectiveness compared against several state-of-the-art methods\non three indoor VLN tasks: REVERIE, NDH, and R2R. Project repository:\nhttps://github.com/YuankaiQi/ORIST\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1\">Yuankai Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1\">Zizheng Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1\">Yicong Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Ming-Hsuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hengel_A/0/1/0/all/0/1\">Anton van den Hengel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qi Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge-aware Prompt-tuning with Synergistic Optimization for Relation Extraction. (arXiv:2104.07650v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07650","description":"<p>Recently, prompt-tuning has achieved promising results on some few-class\nclassification tasks. The core idea of prompt-tuning is to insert text pieces,\ni.e., template, to the input and transform a classification task into a masked\nlanguage modeling problem. However, as for relation extraction, determining the\nappropriate prompt template requires domain expertise, and single label word\nhandcrafted or auto-searched is cumbersome and time-consuming to verify their\neffectiveness in non-few-shot scenarios, which also fails to leverage the\nabundant semantic knowledge in the entities and relation labels. To this end,\nwe focus on incorporating knowledge into prompt-tuning for relation extraction\nand propose a knowledge-aware prompt-tuning with synergistic optimization\n(KNIGHT) approach. Specifically, we inject entity and relation knowledge into\nprompt construction with learnable virtual template words and answer words and\njointly optimize their representation with knowledge constraints. Extensive\nexperimental results on 5 datasets with standard and low-resource settings\ndemonstrate the effectiveness of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yunzhi Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Annotated Commodity News Corpus for Event Extraction. (arXiv:2105.08214v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.08214","description":"<p>Commodity News contains a wealth of information such as sum-mary of the\nrecent commodity price movement and notable events that led tothe movement.\nThrough event extraction, useful information extracted fromcommodity news is\nextremely useful in mining for causal relation betweenevents and commodity\nprice movement, which can be used for commodity priceprediction. To facilitate\nthe future research, we introduce a new dataset withthe following information\nidentified and annotated: (i) entities (both nomi-nal and named), (ii) events\n(trigger words and argument roles), (iii) eventmetadata: modality, polarity and\nintensity and (iv) event-event relations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Meisin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soon_L/0/1/0/all/0/1\">Lay-Ki Soon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siew_E/0/1/0/all/0/1\">Eu-Gene Siew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugianto_L/0/1/0/all/0/1\">Ly Fie Sugianto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Laughing Heads: Can Transformers Detect What Makes a Sentence Funny?. (arXiv:2105.09142v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.09142","description":"<p>The automatic detection of humor poses a grand challenge for natural language\nprocessing. Transformer-based systems have recently achieved remarkable results\non this task, but they usually (1)~were evaluated in setups where serious vs\nhumorous texts came from entirely different sources, and (2)~focused on\nbenchmarking performance without providing insights into how the models work.\nWe make progress in both respects by training and analyzing transformer-based\nhumor recognition models on a recently introduced dataset consisting of minimal\npairs of aligned sentences, one serious, the other humorous. We find that,\nalthough our aligned dataset is much harder than previous datasets,\ntransformer-based models recognize the humorous sentence in an aligned pair\nwith high accuracy (78%). In a careful error analysis, we characterize easy vs\nhard instances. Finally, by analyzing attention weights, we obtain important\ninsights into the mechanisms by which transformers recognize humor. Most\nremarkably, we find clear evidence that one single attention head learns to\nrecognize the words that make a test sentence humorous, even without access to\nthis information at training time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peyrard_M/0/1/0/all/0/1\">Maxime Peyrard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borges_B/0/1/0/all/0/1\">Beatriz Borges</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gligoric_K/0/1/0/all/0/1\">Kristina Gligori&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+West_R/0/1/0/all/0/1\">Robert West</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Self-supervised Method for Entity Alignment. (arXiv:2106.09395v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.09395","description":"<p>Entity alignment, aiming to identify equivalent entities across different\nknowledge graphs (KGs), is a fundamental problem for constructing large-scale\nKGs. Over the course of its development, supervision has been considered\nnecessary for accurate alignments. Inspired by the recent progress of\nself-supervised learning, we explore the extent to which we can get rid of\nsupervision for entity alignment. Existing supervised methods for this task\nfocus on pulling each pair of positive (labeled) entities close to each other.\nHowever, our analysis suggests that the learning of entity alignment can\nactually benefit more from pushing sampled (unlabeled) negatives far away than\npulling positive aligned pairs close. We present SelfKG by leveraging this\ndiscovery to design a contrastive learning strategy across two KGs. Extensive\nexperiments on benchmark datasets demonstrate that SelfKG without supervision\ncan match or achieve comparable results with state-of-the-art supervised\nbaselines. The performance of SelfKG demonstrates self-supervised learning\noffers great potential for entity alignment in KGs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_H/0/1/0/all/0/1\">Haoyun Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinghao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zeyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kharlamov_E/0/1/0/all/0/1\">Evgeny Kharlamov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yuxiao Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jie Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are Missing Links Predictable? An Inferential Benchmark for Knowledge Graph Completion. (arXiv:2108.01387v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.01387","description":"<p>We present InferWiki, a Knowledge Graph Completion (KGC) dataset that\nimproves upon existing benchmarks in inferential ability, assumptions, and\npatterns. First, each testing sample is predictable with supportive data in the\ntraining set. To ensure it, we propose to utilize rule-guided train/test\ngeneration, instead of conventional random split. Second, InferWiki initiates\nthe evaluation following the open-world assumption and improves the inferential\ndifficulty of the closed-world assumption, by providing manually annotated\nnegative and unknown triples. Third, we include various inference patterns\n(e.g., reasoning path length and types) for comprehensive evaluation. In\nexperiments, we curate two settings of InferWiki varying in sizes and\nstructures, and apply the construction process on CoDEx as comparative\ndatasets. The results and empirical analyses demonstrate the necessity and\nhigh-quality of InferWiki. Nevertheless, the performance gap among various\ninferential assumptions and patterns presents the difficulty and inspires\nfuture research direction. Our datasets can be found in\nhttps://github.com/TaoMiner/inferwiki\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yixin Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1\">Xiang Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_X/0/1/0/all/0/1\">Xin Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juanzi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yonggang Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hanwang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Smart Bird: Learnable Sparse Attention for Efficient and Effective Transformer. (arXiv:2108.09193v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.09193","description":"<p>Transformer has achieved great success in NLP. However, the quadratic\ncomplexity of the self-attention mechanism in Transformer makes it inefficient\nin handling long sequences. Many existing works explore to accelerate\nTransformers by computing sparse self-attention instead of a dense one, which\nusually attends to tokens at certain positions or randomly selected tokens.\nHowever, manually selected or random tokens may be uninformative for context\nmodeling. In this paper, we propose Smart Bird, which is an efficient and\neffective Transformer with learnable sparse attention. In Smart Bird, we first\ncompute a sketched attention matrix with a single-head low-dimensional\nTransformer, which aims to find potential important interactions between\ntokens. We then sample token pairs based on their probability scores derived\nfrom the sketched attention matrix to generate different sparse attention index\nmatrices for different attention heads. Finally, we select token embeddings\naccording to the index matrices to form the input of sparse attention networks.\nExtensive experiments on six benchmark datasets for different tasks validate\nthe efficiency and effectiveness of Smart Bird in text modeling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chuhan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fangzhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_T/0/1/0/all/0/1\">Tao Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_B/0/1/0/all/0/1\">Binxing Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yongfeng Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-08-25T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"SimVLM: Simple Visual Language Model Pretraining with Weak Supervision. (arXiv:2108.10904v1 [cs.CV])","link":"http://arxiv.org/abs/2108.10904","description":"<p>With recent progress in joint modeling of visual and textual representations,\nVision-Language Pretraining (VLP) has achieved impressive performance on many\nmultimodal downstream tasks. However, the requirement for expensive annotations\nincluding clean image captions and regional labels limits the scalability of\nexisting approaches, and complicates the pretraining procedure with the\nintroduction of multiple dataset-specific objectives. In this work, we relax\nthese constraints and present a minimalist pretraining framework, named Simple\nVisual Language Model (SimVLM). Unlike prior work, SimVLM reduces the training\ncomplexity by exploiting large-scale weak supervision, and is trained\nend-to-end with a single prefix language modeling objective. Without utilizing\nextra data or task-specific customization, the resulting model significantly\noutperforms previous pretraining methods and achieves new state-of-the-art\nresults on a wide range of discriminative and generative vision-language\nbenchmarks, including VQA (+3.74% vqa-score), NLVR2 (+1.17% accuracy), SNLI-VE\n(+1.37% accuracy) and image captioning tasks (+10.1% average CIDEr score).\nFurthermore, we demonstrate that SimVLM acquires strong generalization and\ntransfer ability, enabling zero-shot behavior including open-ended visual\nquestion answering and cross-modality transfer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zirui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jiahui Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_A/0/1/0/all/0/1\">Adams Wei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Z/0/1/0/all/0/1\">Zihang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1\">Yulia Tsvetkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yuan Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Correcting inter-scan motion artefacts in quantitative R1 mapping at 7T. (arXiv:2108.10943v1 [eess.IV])","link":"http://arxiv.org/abs/2108.10943","description":"<p>Purpose: Inter-scan motion is a substantial source of error in $R_1$\nestimation, and can be expected to increase at 7T where $B_1$ fields are more\ninhomogeneous. The established correction scheme does not translate to 7T since\nit requires a body coil reference. Here we introduce two alternatives that\noutperform the established method. Since they compute relative sensitivities\nthey do not require body coil images.\n</p>\n<p>Theory: The proposed methods use coil-combined magnitude images to obtain the\nrelative coil sensitivities. The first method efficiently computes the relative\nsensitivities via a simple ratio; the second by fitting a more sophisticated\ngenerative model.\n</p>\n<p>Methods: $R_1$ maps were computed using the variable flip angle (VFA)\napproach. Multiple datasets were acquired at 3T and 7T, with and without motion\nbetween the acquisition of the VFA volumes. $R_1$ maps were constructed without\ncorrection, with the proposed corrections, and (at 3T) with the previously\nestablished correction scheme.\n</p>\n<p>Results: At 3T, the proposed methods outperform the baseline method.\nInter-scan motion artefacts were also reduced at 7T. However, reproducibility\nonly converged on that of the no motion condition if position-specific transmit\nfield effects were also incorporated.\n</p>\n<p>Conclusion: The proposed methods simplify inter-scan motion correction of\n$R_1$ maps and are applicable at both 3T and 7T, where a body coil is typically\nnot available. The open-source code for all methods is made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Balbastre_Y/0/1/0/all/0/1\">Ya&#xeb;l Balbastre</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Aghaeifar_A/0/1/0/all/0/1\">Ali Aghaeifar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Corbin_N/0/1/0/all/0/1\">Nad&#xe8;ge Corbin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Brudfors_M/0/1/0/all/0/1\">Mikael Brudfors</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ashburner_J/0/1/0/all/0/1\">John Ashburner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Callaghan_M/0/1/0/all/0/1\">Martina F. Callaghan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Field-Guide-Inspired Zero-Shot Learning. (arXiv:2108.10967v1 [cs.CV])","link":"http://arxiv.org/abs/2108.10967","description":"<p>Modern recognition systems require large amounts of supervision to achieve\naccuracy. Adapting to new domains requires significant data from experts, which\nis onerous and can become too expensive. Zero-shot learning requires an\nannotated set of attributes for a novel category. Annotating the full set of\nattributes for a novel category proves to be a tedious and expensive task in\ndeployment. This is especially the case when the recognition domain is an\nexpert domain. We introduce a new field-guide-inspired approach to zero-shot\nannotation where the learner model interactively asks for the most useful\nattributes that define a class. We evaluate our method on classification\nbenchmarks with attribute annotations like CUB, SUN, and AWA2 and show that our\nmodel achieves the performance of a model with full annotations at the cost of\na significantly fewer number of annotations. Since the time of experts is\nprecious, decreasing annotation cost can be very valuable for real-world\ndeployment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mall_U/0/1/0/all/0/1\">Utkarsh Mall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hariharan_B/0/1/0/all/0/1\">Bharath Hariharan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bala_K/0/1/0/all/0/1\">Kavita Bala</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Real-time Indian Sign Language (ISL) Recognition. (arXiv:2108.10970v1 [cs.CV])","link":"http://arxiv.org/abs/2108.10970","description":"<p>This paper presents a system which can recognise hand poses &amp; gestures from\nthe Indian Sign Language (ISL) in real-time using grid-based features. This\nsystem attempts to bridge the communication gap between the hearing and speech\nimpaired and the rest of the society. The existing solutions either provide\nrelatively low accuracy or do not work in real-time. This system provides good\nresults on both the parameters. It can identify 33 hand poses and some gestures\nfrom the ISL. Sign Language is captured from a smartphone camera and its frames\nare transmitted to a remote server for processing. The use of any external\nhardware (such as gloves or the Microsoft Kinect sensor) is avoided, making it\nuser-friendly. Techniques such as Face detection, Object stabilisation and Skin\nColour Segmentation are used for hand detection and tracking. The image is\nfurther subjected to a Grid-based Feature Extraction technique which represents\nthe hand's pose in the form of a Feature Vector. Hand poses are then classified\nusing the k-Nearest Neighbours algorithm. On the other hand, for gesture\nclassification, the motion and intermediate hand poses observation sequences\nare fed to Hidden Markov Model chains corresponding to the 12 pre-selected\ngestures defined in ISL. Using this methodology, the system is able to achieve\nan accuracy of 99.7% for static hand poses, and an accuracy of 97.23% for\ngesture recognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shenoy_K/0/1/0/all/0/1\">Kartik Shenoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dastane_T/0/1/0/all/0/1\">Tejas Dastane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_V/0/1/0/all/0/1\">Varun Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vyavaharkar_D/0/1/0/all/0/1\">Devendra Vyavaharkar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Effective Pixel-Wise Approach for Skin Colour Segmentation Using Pixel Neighbourhood Technique. (arXiv:2108.10971v1 [cs.CV])","link":"http://arxiv.org/abs/2108.10971","description":"<p>This paper presents a novel technique for skin colour segmentation that\novercomes the limitations faced by existing techniques such as Colour Range\nThresholding. Skin colour segmentation is affected by the varied skin colours\nand surrounding lighting conditions, leading to poorskin segmentation for many\ntechniques. We propose a new two stage Pixel Neighbourhood technique that\nclassifies any pixel as skin or non-skin based on its neighbourhood pixels. The\nfirst step calculates the probability of each pixel being skin by passing HSV\nvalues of the pixel to a Deep Neural Network model. In the next step, it\ncalculates the likeliness of pixel being skin using these probabilities of\nneighbouring pixels. This technique performs skin colour segmentation better\nthan the existing techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dastane_T/0/1/0/all/0/1\">Tejas Dastane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_V/0/1/0/all/0/1\">Varun Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shenoy_K/0/1/0/all/0/1\">Kartik Shenoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vyavaharkar_D/0/1/0/all/0/1\">Devendra Vyavaharkar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain Adaptation for Real-World Single View 3D Reconstruction. (arXiv:2108.10972v1 [cs.CV])","link":"http://arxiv.org/abs/2108.10972","description":"<p>Deep learning-based object reconstruction algorithms have shown remarkable\nimprovements over classical methods. However, supervised learning based methods\nperform poorly when the training data and the test data have different\ndistributions. Indeed, most current works perform satisfactorily on the\nsynthetic ShapeNet dataset, but dramatically fail in when presented with real\nworld images. To address this issue, unsupervised domain adaptation can be used\ntransfer knowledge from the labeled synthetic source domain and learn a\nclassifier for the unlabeled real target domain. To tackle this challenge of\nsingle view 3D reconstruction in the real domain, we experiment with a variety\nof domain adaptation techniques inspired by the maximum mean discrepancy (MMD)\nloss, Deep CORAL, and the domain adversarial neural network (DANN). From these\nfindings, we additionally propose a novel architecture which takes advantage of\nthe fact that in this setting, target domain data is unsupervised with regards\nto the 3D model but supervised for class labels. We base our framework off a\nrecent network called pix2vox. Results are performed with ShapeNet as the\nsource domain and domains within the Object Dataset Domain Suite (ODDS) dataset\nas the target, which is a real world multiview, multidomain image dataset. The\ndomains in ODDS vary in difficulty, allowing us to assess notions of domain gap\nsize. Our results are the first in the multiview reconstruction literature\nusing this dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Leung_B/0/1/0/all/0/1\">Brandon Leung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Siddharth Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horodniceanu_A/0/1/0/all/0/1\">Arik Horodniceanu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NeRP: Implicit Neural Representation Learning with Prior Embedding for Sparsely Sampled Image Reconstruction. (arXiv:2108.10991v1 [eess.IV])","link":"http://arxiv.org/abs/2108.10991","description":"<p>Image reconstruction is an inverse problem that solves for a computational\nimage based on sampled sensor measurement. Sparsely sampled image\nreconstruction poses addition challenges due to limited measurements. In this\nwork, we propose an implicit Neural Representation learning methodology with\nPrior embedding (NeRP) to reconstruct a computational image from sparsely\nsampled measurements. The method differs fundamentally from previous deep\nlearning-based image reconstruction approaches in that NeRP exploits the\ninternal information in an image prior, and the physics of the sparsely sampled\nmeasurements to produce a representation of the unknown subject. No large-scale\ndata is required to train the NeRP except for a prior image and sparsely\nsampled measurements. In addition, we demonstrate that NeRP is a general\nmethodology that generalizes to different imaging modalities such as CT and\nMRI. We also show that NeRP can robustly capture the subtle yet significant\nimage changes required for assessing tumor progression.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Shen_L/0/1/0/all/0/1\">Liyue Shen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pauly_J/0/1/0/all/0/1\">John Pauly</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xing_L/0/1/0/all/0/1\">Lei Xing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OOWL500: Overcoming Dataset Collection Bias in the Wild. (arXiv:2108.10992v1 [cs.CV])","link":"http://arxiv.org/abs/2108.10992","description":"<p>The hypothesis that image datasets gathered online \"in the wild\" can produce\nbiased object recognizers, e.g. preferring professional photography or certain\nviewing angles, is studied. A new \"in the lab\" data collection infrastructure\nis proposed consisting of a drone which captures images as it circles around\nobjects. Crucially, the control provided by this setup and the natural camera\nshake inherent to flight mitigate many biases. It's inexpensive and easily\nreplicable nature may also potentially lead to a scalable data collection\neffort by the vision community. The procedure's usefulness is demonstrated by\ncreating a dataset of Objects Obtained With fLight (OOWL). Denoted as OOWL500,\nit contains 120,000 images of 500 objects and is the largest \"in the lab\" image\ndataset available when both number of classes and objects per class are\nconsidered. Furthermore, it has enabled several of new insights on object\nrecognition. First, a novel adversarial attack strategy is proposed, where\nimage perturbations are defined in terms of semantic properties such as camera\nshake and pose. Indeed, experiments have shown that ImageNet has considerable\namounts of pose and professional photography bias. Second, it is used to show\nthat the augmentation of in the wild datasets, such as ImageNet, with in the\nlab data, such as OOWL500, can significantly decrease these biases, leading to\nobject recognizers of improved generalization. Third, the dataset is used to\nstudy questions on \"best procedures\" for dataset collection. It is revealed\nthat data augmentation with synthetic images does not suffice to eliminate in\nthe wild datasets biases, and that camera shake and pose diversity play a more\nimportant role in object recognition robustness than previously thought.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Leung_B/0/1/0/all/0/1\">Brandon Leung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_C/0/1/0/all/0/1\">Chih-Hui Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Persekian_A/0/1/0/all/0/1\">Amir Persekian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orozco_D/0/1/0/all/0/1\">David Orozco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yen Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sandstrom_E/0/1/0/all/0/1\">Erik Sandstrom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasconcelos_N/0/1/0/all/0/1\">Nuno Vasconcelos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Wanderlust: Online Continual Object Detection in the Real World. (arXiv:2108.11005v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11005","description":"<p>Online continual learning from data streams in dynamic environments is a\ncritical direction in the computer vision field. However, realistic benchmarks\nand fundamental studies in this line are still missing. To bridge the gap, we\npresent a new online continual object detection benchmark with an egocentric\nvideo dataset, Objects Around Krishna (OAK). OAK adopts the KrishnaCAM videos,\nan ego-centric video stream collected over nine months by a graduate student.\nOAK provides exhaustive bounding box annotations of 80 video snippets (~17.5\nhours) for 105 object categories in outdoor scenes. The emergence of new object\ncategories in our benchmark follows a pattern similar to what a single person\nmight see in their day-to-day life. The dataset also captures the natural\ndistribution shifts as the person travels to different places. These egocentric\nlong-running videos provide a realistic playground for continual learning\nalgorithms, especially in online embodied settings. We also introduce new\nevaluation metrics to evaluate the model performance and catastrophic\nforgetting and provide baseline studies for online continual object detection.\nWe believe this benchmark will pose new exciting challenges for learning from\nnon-stationary data in continual learning. The OAK dataset and the associated\nbenchmark are released at https://oakdata.github.io/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianren Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_Guan_Y/0/1/0/all/0/1\">Yue Shang-Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Abhinav Gupta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"iDARTS: Improving DARTS by Node Normalization and Decorrelation Discretization. (arXiv:2108.11014v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11014","description":"<p>Differentiable ARchiTecture Search (DARTS) uses a continuous relaxation of\nnetwork representation and dramatically accelerates Neural Architecture Search\n(NAS) by almost thousands of times in GPU-day. However, the searching process\nof DARTS is unstable, which suffers severe degradation when training epochs\nbecome large, thus limiting its application. In this paper, we claim that this\ndegradation issue is caused by the imbalanced norms between different nodes and\nthe highly correlated outputs from various operations. We then propose an\nimproved version of DARTS, namely iDARTS, to deal with the two problems. In the\ntraining phase, it introduces node normalization to maintain the norm balance.\nIn the discretization phase, the continuous architecture is approximated based\non the similarity between the outputs of the node and the decorrelated\noperations rather than the values of the architecture parameters. Extensive\nevaluation is conducted on CIFAR-10 and ImageNet, and the error rates of 2.25\\%\nand 24.7\\% are reported within 0.2 and 1.9 GPU-day for architecture search\nrespectively, which shows its effectiveness. Additional analysis also reveals\nthat iDARTS has the advantage in robustness and generalization over other\nDARTS-based counterparts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huiqun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1\">Ruijie Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">Di Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunhong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Scaling Law for Synthetic-to-Real Transfer: A Measure of Pre-Training. (arXiv:2108.11018v1 [cs.LG])","link":"http://arxiv.org/abs/2108.11018","description":"<p>Synthetic-to-real transfer learning is a framework in which we pre-train\nmodels with synthetically generated images and ground-truth annotations for\nreal tasks. Although synthetic images overcome the data scarcity issue, it\nremains unclear how the fine-tuning performance scales with pre-trained models,\nespecially in terms of pre-training data size. In this study, we collect a\nnumber of empirical observations and uncover the secret. Through experiments,\nwe observe a simple and general scaling law that consistently describes\nlearning curves in various tasks, models, and complexities of synthesized\npre-training data. Further, we develop a theory of transfer learning for a\nsimplified scenario and confirm that the derived generalization bound is\nconsistent with our empirical findings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mikami_H/0/1/0/all/0/1\">Hiroaki Mikami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fukumizu_K/0/1/0/all/0/1\">Kenji Fukumizu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murai_S/0/1/0/all/0/1\">Shogo Murai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suzuki_S/0/1/0/all/0/1\">Shuji Suzuki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kikuchi_Y/0/1/0/all/0/1\">Yuta Kikuchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suzuki_T/0/1/0/all/0/1\">Taiji Suzuki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maeda_S/0/1/0/all/0/1\">Shin-ichi Maeda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayashi_K/0/1/0/all/0/1\">Kohei Hayashi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Layer-wise Customized Weak Segmentation Block and AIoU Loss for Accurate Object Detection. (arXiv:2108.11021v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11021","description":"<p>The anchor-based detectors handle the problem of scale variation by building\nthe feature pyramid and directly setting different scales of anchors on each\ncell in different layers. However, it is difficult for box-wise anchors to\nguide the adaptive learning of scale-specific features in each layer because\nthere is no one-to-one correspondence between box-wise anchors and pixel-level\nfeatures. In order to alleviate the problem, in this paper, we propose a\nscale-customized weak segmentation (SCWS) block at the pixel level for scale\ncustomized object feature learning in each layer. By integrating the SCWS\nblocks into the single-shot detector, a scale-aware object detector (SCOD) is\nconstructed to detect objects of different sizes naturally and accurately.\nFurthermore, the standard location loss neglects the fact that the hard and\neasy samples may be seriously imbalanced. A forthcoming problem is that it is\nunable to get more accurate bounding boxes due to the imbalance. To address\nthis problem, an adaptive IoU (AIoU) loss via a simple yet effective squeeze\noperation is specified in our SCOD. Extensive experiments on PASCAL VOC and MS\nCOCO demonstrate the superiority of our SCOD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Keyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_W/0/1/0/all/0/1\">Wenli Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lang_Q/0/1/0/all/0/1\">Qinghai Lang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_L/0/1/0/all/0/1\">Lingyun Qin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EncoderMI: Membership Inference against Pre-trained Encoders in Contrastive Learning. (arXiv:2108.11023v1 [cs.CR])","link":"http://arxiv.org/abs/2108.11023","description":"<p>Given a set of unlabeled images or (image, text) pairs, contrastive learning\naims to pre-train an image encoder that can be used as a feature extractor for\nmany downstream tasks. In this work, we propose EncoderMI, the first membership\ninference method against image encoders pre-trained by contrastive learning. In\nparticular, given an input and a black-box access to an image encoder,\nEncoderMI aims to infer whether the input is in the training dataset of the\nimage encoder. EncoderMI can be used 1) by a data owner to audit whether its\n(public) data was used to pre-train an image encoder without its authorization\nor 2) by an attacker to compromise privacy of the training data when it is\nprivate/sensitive. Our EncoderMI exploits the overfitting of the image encoder\ntowards its training data. In particular, an overfitted image encoder is more\nlikely to output more (or less) similar feature vectors for two augmented\nversions of an input in (or not in) its training dataset. We evaluate EncoderMI\non image encoders pre-trained on multiple datasets by ourselves as well as the\nContrastive Language-Image Pre-training (CLIP) image encoder, which is\npre-trained on 400 million (image, text) pairs collected from the Internet and\nreleased by OpenAI. Our results show that EncoderMI can achieve high accuracy,\nprecision, and recall. We also explore a countermeasure against EncoderMI via\npreventing overfitting through early stopping. Our results show that it\nachieves trade-offs between accuracy of EncoderMI and utility of the image\nencoder, i.e., it can reduce the accuracy of EncoderMI, but it also incurs\nclassification accuracy loss of the downstream classifiers built based on the\nimage encoder.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hongbin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\">Jinyuan Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_W/0/1/0/all/0/1\">Wenjie Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_N/0/1/0/all/0/1\">Neil Zhenqiang Gong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Visual Quality of Unrestricted Adversarial Examples with Wavelet-VAE. (arXiv:2108.11032v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11032","description":"<p>Traditional adversarial examples are typically generated by adding\nperturbation noise to the input image within a small matrix norm. In practice,\nun-restricted adversarial attack has raised great concern and presented a new\nthreat to the AI safety. In this paper, we propose a wavelet-VAE structure to\nreconstruct an input image and generate adversarial examples by modifying the\nlatent code. Different from perturbation-based attack, the modifications of the\nproposed method are not limited but imperceptible to human eyes. Experiments\nshow that our method can generate high quality adversarial examples on ImageNet\ndataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiang_W/0/1/0/all/0/1\">Wenzhao Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Shibao Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NGC: A Unified Framework for Learning with Open-World Noisy Data. (arXiv:2108.11035v1 [cs.LG])","link":"http://arxiv.org/abs/2108.11035","description":"<p>The existence of noisy data is prevalent in both the training and testing\nphases of machine learning systems, which inevitably leads to the degradation\nof model performance. There have been plenty of works concentrated on learning\nwith in-distribution (IND) noisy labels in the last decade, i.e., some training\nsamples are assigned incorrect labels that do not correspond to their true\nclasses. Nonetheless, in real application scenarios, it is necessary to\nconsider the influence of out-of-distribution (OOD) samples, i.e., samples that\ndo not belong to any known classes, which has not been sufficiently explored\nyet. To remedy this, we study a new problem setup, namely Learning with\nOpen-world Noisy Data (LOND). The goal of LOND is to simultaneously learn a\nclassifier and an OOD detector from datasets with mixed IND and OOD noise. In\nthis paper, we propose a new graph-based framework, namely Noisy Graph Cleaning\n(NGC), which collects clean samples by leveraging geometric structure of data\nand model predictive confidence. Without any additional training effort, NGC\ncan detect and reject the OOD samples based on the learned class prototypes\ndirectly in testing phase. We conduct experiments on multiple benchmarks with\ndifferent types of noise and the results demonstrate the superior performance\nof our method against state of the arts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhi-Fan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_T/0/1/0/all/0/1\">Tong Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jianwen Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_C/0/1/0/all/0/1\">Chaojie Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_M/0/1/0/all/0/1\">Mingqian Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yu-Feng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Localization Uncertainty-Based Attention for Object Detection. (arXiv:2108.11042v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11042","description":"<p>Object detection has been applied in a wide variety of real world scenarios,\nso detection algorithms must provide confidence in the results to ensure that\nappropriate decisions can be made based on their results. Accordingly, several\nstudies have investigated the probabilistic confidence of bounding box\nregression. However, such approaches have been restricted to anchor-based\ndetectors, which use box confidence values as additional screening scores\nduring non-maximum suppression (NMS) procedures. In this paper, we propose a\nmore efficient uncertainty-aware dense detector (UADET) that predicts\nfour-directional localization uncertainties via Gaussian modeling. Furthermore,\na simple uncertainty attention module (UAM) that exploits box confidence maps\nis proposed to improve performance through feature refinement. Experiments\nusing the MS COCO benchmark show that our UADET consistently surpasses baseline\nFCOS, and that our best model, ResNext-64x4d-101-DCN, obtains a single model,\nsingle-scale AP of 48.3% on COCO test-dev, thus achieving the state-of-the-art\namong various object detectors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Sanghun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kunhee Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_E/0/1/0/all/0/1\">Eunseop Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Daijin Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Memory-Augmented Non-Local Attention for Video Super-Resolution. (arXiv:2108.11048v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11048","description":"<p>In this paper, we propose a novel video super-resolution method that aims at\ngenerating high-fidelity high-resolution (HR) videos from low-resolution (LR)\nones. Previous methods predominantly leverage temporal neighbor frames to\nassist the super-resolution of the current frame. Those methods achieve limited\nperformance as they suffer from the challenge in spatial frame alignment and\nthe lack of useful information from similar LR neighbor frames. In contrast, we\ndevise a cross-frame non-local attention mechanism that allows video\nsuper-resolution without frame alignment, leading to be more robust to large\nmotions in the video. In addition, to acquire the information beyond neighbor\nframes, we design a novel memory-augmented attention module to memorize general\nvideo details during the super-resolution training. Experimental results\nindicate that our method can achieve superior performance on large motion\nvideos comparing to the state-of-the-art methods without aligning frames. Our\nsource code will be released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jiyang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jingen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bo_L/0/1/0/all/0/1\">Liefeng Bo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_T/0/1/0/all/0/1\">Tao Mei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding of Kernels in CNN Models by Suppressing Irrelevant Visual Features in Images. (arXiv:2108.11054v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11054","description":"<p>Deep learning models have shown their superior performance in various vision\ntasks. However, the lack of precisely interpreting kernels in convolutional\nneural networks (CNNs) is becoming one main obstacle to wide applications of\ndeep learning models in real scenarios. Although existing interpretation\nmethods may find certain visual patterns which are associated with the\nactivation of a specific kernel, those visual patterns may not be specific or\ncomprehensive enough for interpretation of a specific activation of kernel of\ninterest. In this paper, a simple yet effective optimization method is proposed\nto interpret the activation of any kernel of interest in CNN models. The basic\nidea is to simultaneously preserve the activation of the specific kernel and\nsuppress the activation of all other kernels at the same layer. In this way,\nonly visual information relevant to the activation of the specific kernel is\nremained in the input. Consistent visual information from multiple modified\ninputs would help users understand what kind of features are specifically\nassociated with specific kernel. Comprehensive evaluation shows that the\nproposed method can help better interpret activation of specific kernels than\nwidely used methods, even when two kernels have very similar activation regions\nfrom the same input image.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_J/0/1/0/all/0/1\">Jia-Xin Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_W/0/1/0/all/0/1\">Wanying Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_J/0/1/0/all/0/1\">Jianfei Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1\">Wei Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ruixuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1\">Wei-shi Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Normal Learning in Videos with Attention Prototype Network. (arXiv:2108.11055v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11055","description":"<p>Frame reconstruction (current or future frame) based on Auto-Encoder (AE) is\na popular method for video anomaly detection. With models trained on the normal\ndata, the reconstruction errors of anomalous scenes are usually much larger\nthan those of normal ones. Previous methods introduced the memory bank into AE,\nfor encoding diverse normal patterns across the training videos. However, they\nare memory consuming and cannot cope with unseen new scenarios in the testing\ndata. In this work, we propose a self-attention prototype unit (APU) to encode\nthe normal latent space as prototypes in real time, free from extra memory\ncost. In addition, we introduce circulative attention mechanism to our backbone\nto form a novel feature extracting learner, namely Circulative Attention Unit\n(CAU). It enables the fast adaption capability on new scenes by only consuming\na few iterations of update. Extensive experiments are conducted on various\nbenchmarks. The superior performance over the state-of-the-art demonstrates the\neffectiveness of our method. Our code is available at\nhttps://github.com/huchao-AI/APN/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1\">Chao Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Weijie Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_W/0/1/0/all/0/1\">Weibin Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_S/0/1/0/all/0/1\">Shengxin Lai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Class-level Prototypes for Few-shot Learning. (arXiv:2108.11072v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11072","description":"<p>Few-shot learning aims to recognize new categories using very few labeled\nsamples. Although few-shot learning has witnessed promising development in\nrecent years, most existing methods adopt an average operation to calculate\nprototypes, thus limited by the outlier samples. In this work, we propose a\nsimple yet effective framework for few-shot classification, which can learn to\ngenerate preferable prototypes from few support data, with the help of an\nepisodic prototype generator module. The generated prototype is meant to be\nclose to a certain \\textit{\\targetproto{}} and is less influenced by outlier\nsamples. Extensive experiments demonstrate the effectiveness of this module,\nand our approach gets a significant raise over baseline models, and get a\ncompetitive result compared to previous methods on \\textit{mini}ImageNet,\n\\textit{tiered}ImageNet, and cross-domain (\\textit{mini}ImageNet $\\rightarrow$\nCUB-200-2011) datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_M/0/1/0/all/0/1\">Minglei Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenhai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_C/0/1/0/all/0/1\">Chunhao Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qian Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1\">Tong Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Heredity-aware Child Face Image Generation with Latent Space Disentanglement. (arXiv:2108.11080v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11080","description":"<p>Generative adversarial networks have been widely used in image synthesis in\nrecent years and the quality of the generated image has been greatly improved.\nHowever, the flexibility to control and decouple facial attributes (e.g., eyes,\nnose, mouth) is still limited. In this paper, we propose a novel approach,\ncalled ChildGAN, to generate a child's image according to the images of parents\nwith heredity prior. The main idea is to disentangle the latent space of a\npre-trained generation model and precisely control the face attributes of child\nimages with clear semantics. We use distances between face landmarks as pseudo\nlabels to figure out the most influential semantic vectors of the corresponding\nface attributes by calculating the gradient of latent vectors to pseudo labels.\nFurthermore, we disentangle the semantic vectors by weighting irrelevant\nfeatures and orthogonalizing them with Schmidt Orthogonalization. Finally, we\nfuse the latent vector of the parents by leveraging the disentangled semantic\nvectors under the guidance of biological genetic laws. Extensive experiments\ndemonstrate that our approach outperforms the existing methods with encouraging\nresults.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cui_X/0/1/0/all/0/1\">Xiao Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wengang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weilun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Houqiang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D Face Recognition: A Survey. (arXiv:2108.11082v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11082","description":"<p>Face recognition is one of the most studied research topics in the community.\nIn recent years, the research on face recognition has shifted to using 3D\nfacial surfaces, as more discriminating features can be represented by the 3D\ngeometric information. This survey focuses on reviewing the 3D face recognition\ntechniques developed in the past ten years which are generally categorized into\nconventional methods and deep learning methods. The categorized techniques are\nevaluated using detailed descriptions of the representative works. The\nadvantages and disadvantages of the techniques are summarized in terms of\naccuracy, complexity and robustness to face variation (expression, pose and\nocclusions, etc). The main contribution of this survey is that it\ncomprehensively covers both conventional methods and deep learning methods on\n3D face recognition. In addition, a review of available 3D face databases is\nprovided, along with the discussion of future research challenges and\ndirections.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jing_Y/0/1/0/all/0/1\">Yaping Jing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xuequan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Shang Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Transformer for Single Image Super-Resolution. (arXiv:2108.11084v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11084","description":"<p>Single image super-resolution task has witnessed great strides with the\ndevelopment of deep learning. However, most existing studies focus on building\na more complex neural network with a massive number of layers, bringing heavy\ncomputational cost and memory storage. Recently, as Transformer yields\nbrilliant results in NLP tasks, more and more researchers start to explore the\napplication of Transformer in computer vision tasks. But with the heavy\ncomputational cost and high GPU memory occupation of the vision Transformer,\nthe network can not be designed too deep. To address this problem, we propose a\nnovel Efficient Super-Resolution Transformer (ESRT) for fast and accurate image\nsuper-resolution. ESRT is a hybrid Transformer where a CNN-based SR network is\nfirst designed in the front to extract deep features. Specifically, there are\ntwo backbones for formatting the ESRT: lightweight CNN backbone (LCB) and\nlightweight Transformer backbone (LTB). Among them, LCB is a lightweight SR\nnetwork to extract deep SR features at a low computational cost by dynamically\nadjusting the size of the feature map. LTB is made up of an efficient\nTransformer (ET) with a small GPU memory occupation, which benefited from the\nnovel efficient multi-head attention (EMHA). In EMHA, a feature split module\n(FSM) is proposed to split the long sequence into sub-segments and then these\nsub-segments are applied by attention operation. This module can significantly\ndecrease the GPU memory occupation. Extensive experiments show that our ESRT\nachieves competitive results. Compared with the original Transformer which\noccupies 16057M GPU memory, the proposed ET only occupies 4191M GPU memory with\nbetter performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhisheng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juncheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Linlin Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning From Long-Tailed Data With Noisy Labels. (arXiv:2108.11096v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11096","description":"<p>Class imbalance and noisy labels are the norm rather than the exception in\nmany large-scale classification datasets. Nevertheless, most works in machine\nlearning typically assume balanced and clean data. There have been some recent\nattempts to tackle, on one side, the problem of learning from noisy labels and,\non the other side, learning from long-tailed data. Each group of methods make\nsimplifying assumptions about the other. Due to this separation, the proposed\nsolutions often underperform when both assumptions are violated. In this work,\nwe present a simple two-stage approach based on recent advances in\nself-supervised learning to treat both challenges simultaneously. It consists\nof, first, task-agnostic self-supervised pre-training, followed by\ntask-specific fine-tuning using an appropriate loss. Most significantly, we\nfind that self-supervised learning approaches are effectively able to cope with\nsevere class imbalance. In addition, the resulting learned representations are\nalso remarkably robust to label noise, when fine-tuned with an imbalance- and\nnoise-resistant loss function. We validate our claims with experiments on\nCIFAR-10 and CIFAR-100 augmented with synthetic imbalance and noise, as well as\nthe large-scale inherently noisy Clothing-1M dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karthik_S/0/1/0/all/0/1\">Shyamgopal Karthik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Revaud_J/0/1/0/all/0/1\">J&#xe9;rome Revaud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boris_C/0/1/0/all/0/1\">Chidlovskii Boris</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Monocular Depth Estimation Primed by Salient Point Detection and Normalized Hessian Loss. (arXiv:2108.11098v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11098","description":"<p>Deep neural networks have recently thrived on single image depth estimation.\nThat being said, current developments on this topic highlight an apparent\ncompromise between accuracy and network size. This work proposes an accurate\nand lightweight framework for monocular depth estimation based on a\nself-attention mechanism stemming from salient point detection. Specifically,\nwe utilize a sparse set of keypoints to train a FuSaNet model that consists of\ntwo major components: Fusion-Net and Saliency-Net. In addition, we introduce a\nnormalized Hessian loss term invariant to scaling and shear along the depth\ndirection, which is shown to substantially improve the accuracy. The proposed\nmethod achieves state-of-the-art results on NYU-Depth-v2 and KITTI while using\n3.1-38.4 times smaller model in terms of the number of parameters than baseline\napproaches. Experiments on the SUN-RGBD further demonstrate the\ngeneralizability of the proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huynh_L/0/1/0/all/0/1\">Lam Huynh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pedone_M/0/1/0/all/0/1\">Matteo Pedone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_P/0/1/0/all/0/1\">Phong Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matas_J/0/1/0/all/0/1\">Jiri Matas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahtu_E/0/1/0/all/0/1\">Esa Rahtu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heikkila_J/0/1/0/all/0/1\">Janne Heikkila</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Attributed and Structured Text-to-Face Synthesis. (arXiv:2108.11100v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11100","description":"<p>Generative Adversarial Networks (GANs) have revolutionized image synthesis\nthrough many applications like face generation, photograph editing, and image\nsuper-resolution. Image synthesis using GANs has predominantly been uni-modal,\nwith few approaches that can synthesize images from text or other data modes.\nText-to-image synthesis, especially text-to-face synthesis, has promising use\ncases of robust face-generation from eye witness accounts and augmentation of\nthe reading experience with visual cues. However, only a couple of datasets\nprovide consolidated face data and textual descriptions for text-to-face\nsynthesis. Moreover, these textual annotations are less extensive and\ndescriptive, which reduces the diversity of faces generated from it. This paper\nempirically proves that increasing the number of facial attributes in each\ntextual description helps GANs generate more diverse and real-looking faces. To\nprove this, we propose a new methodology that focuses on using structured\ntextual descriptions. We also consolidate a Multi-Attributed and Structured\nText-to-face (MAST) dataset consisting of high-quality images with structured\ntextual annotations and make it available to researchers to experiment and\nbuild upon. Lastly, we report benchmark Frechet's Inception Distance (FID),\nFacial Semantic Similarity (FSS), and Facial Semantic Distance (FSD) scores for\nthe MAST dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wadhawan_R/0/1/0/all/0/1\">Rohan Wadhawan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drall_T/0/1/0/all/0/1\">Tanuj Drall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Shubham Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraverty_S/0/1/0/all/0/1\">Shampa Chakraverty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Small Objects in Thermal Images Using Single-Shot Detector. (arXiv:2108.11101v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11101","description":"<p>SSD (Single Shot Multibox Detector) is one of the most successful object\ndetectors for its high accuracy and fast speed. However, the features from\nshallow layer (mainly Conv4_3) of SSD lack semantic information, resulting in\npoor performance in small objects. In this paper, we proposed DDSSD (Dilation\nand Deconvolution Single Shot Multibox Detector), an enhanced SSD with a novel\nfeature fusion module which can improve the performance over SSD for small\nobject detection. In the feature fusion module, dilation convolution module is\nutilized to enlarge the receptive field of features from shallow layer and\ndeconvolution module is adopted to increase the size of feature maps from high\nlayer. Our network achieves 79.7% mAP on PASCAL VOC2007 test and 28.3% mmAP on\nMS COCO test-dev at 41 FPS with only 300x300 input using a single Nvidia 1080\nGPU. Especially, for small objects, DDSSD achieves 10.5% on MS COCO and 22.8%\non FLIR thermal dataset, outperforming a lot of state-of-the-art object\ndetection algorithms in both aspects of accuracy and speed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_X/0/1/0/all/0/1\">Xianggong Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Li Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lightweight Monocular Depth with a Novel Neural Architecture Search Method. (arXiv:2108.11105v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11105","description":"<p>This paper presents a novel neural architecture search method, called LiDNAS,\nfor generating lightweight monocular depth estimation models. Unlike previous\nneural architecture search (NAS) approaches, where finding optimized networks\nare computationally highly demanding, the introduced novel Assisted Tabu Search\nleads to efficient architecture exploration. Moreover, we construct the search\nspace on a pre-defined backbone network to balance layer diversity and search\nspace size. The LiDNAS method outperforms the state-of-the-art NAS approach,\nproposed for disparity and depth estimation, in terms of search efficiency and\noutput model performance. The LiDNAS optimized models achieve results superior\nto compact depth estimation state-of-the-art on NYU-Depth-v2, KITTI, and\nScanNet, while being 7%-500% more compact in size, i.e the number of model\nparameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huynh_L/0/1/0/all/0/1\">Lam Huynh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_P/0/1/0/all/0/1\">Phong Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matas_J/0/1/0/all/0/1\">Jiri Matas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahtu_E/0/1/0/all/0/1\">Esa Rahtu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heikkila_J/0/1/0/all/0/1\">Janne Heikkila</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TransFER: Learning Relation-aware Facial Expression Representations with Transformers. (arXiv:2108.11116v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11116","description":"<p>Facial expression recognition (FER) has received increasing interest in\ncomputer vision. We propose the TransFER model which can learn rich\nrelation-aware local representations. It mainly consists of three components:\nMulti-Attention Dropping (MAD), ViT-FER, and Multi-head Self-Attention Dropping\n(MSAD). First, local patches play an important role in distinguishing various\nexpressions, however, few existing works can locate discriminative and diverse\nlocal patches. This can cause serious problems when some patches are invisible\ndue to pose variations or viewpoint changes. To address this issue, the MAD is\nproposed to randomly drop an attention map. Consequently, models are pushed to\nexplore diverse local patches adaptively. Second, to build rich relations\nbetween different local patches, the Vision Transformers (ViT) are used in FER,\ncalled ViT-FER. Since the global scope is used to reinforce each local patch, a\nbetter representation is obtained to boost the FER performance. Thirdly, the\nmulti-head self-attention allows ViT to jointly attend to features from\ndifferent information subspaces at different positions. Given no explicit\nguidance, however, multiple self-attentions may extract similar relations. To\naddress this, the MSAD is proposed to randomly drop one self-attention module.\nAs a result, models are forced to learn rich relations among diverse local\npatches. Our proposed TransFER model outperforms the state-of-the-art methods\non several FER benchmarks, showing its effectiveness and usefulness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xue_F/0/1/0/all/0/1\">Fanglei Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qiangchang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_G/0/1/0/all/0/1\">Guodong Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GlassNet: Label Decoupling-based Three-stream Neural Network for Robust Image Glass Detection. (arXiv:2108.11117v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11117","description":"<p>Most of the existing object detection methods generate poor glass detection\nresults, due to the fact that the transparent glass shares the same appearance\nwith arbitrary objects behind it in an image. Different from traditional deep\nlearning-based wisdoms that simply use the object boundary as auxiliary\nsupervision, we exploit label decoupling to decompose the original labeled\nground-truth (GT) map into an interior-diffusion map and a boundary-diffusion\nmap. The GT map in collaboration with the two newly generated maps breaks the\nimbalanced distribution of the object boundary, leading to improved glass\ndetection quality. We have three key contributions to solve the transparent\nglass detection problem: (1) We propose a three-stream neural network (call\nGlassNet for short) to fully absorb beneficial features in the three maps. (2)\nWe design a multi-scale interactive dilation module to explore a wider range of\ncontextual information. (3) We develop an attention-based boundary-aware\nfeature Mosaic module to integrate multi-modal information. Extensive\nexperiments on the benchmark dataset exhibit clear improvements of our method\nover SOTAs, in terms of both the overall glass detection accuracy and boundary\nclearness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">C. Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_D/0/1/0/all/0/1\">D. Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1\">X. Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1\">D. Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+wei_M/0/1/0/all/0/1\">M. wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">X. Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Y. Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1\">H. Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Product-oriented Machine Translation with Cross-modal Cross-lingual Pre-training. (arXiv:2108.11119v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11119","description":"<p>Translating e-commercial product descriptions, a.k.a product-oriented machine\ntranslation (PMT), is essential to serve e-shoppers all over the world.\nHowever, due to the domain specialty, the PMT task is more challenging than\ntraditional machine translation problems. Firstly, there are many specialized\njargons in the product description, which are ambiguous to translate without\nthe product image. Secondly, product descriptions are related to the image in\nmore complicated ways than standard image descriptions, involving various\nvisual aspects such as objects, shapes, colors or even subjective styles.\nMoreover, existing PMT datasets are small in scale to support the research. In\nthis paper, we first construct a large-scale bilingual product description\ndataset called Fashion-MMT, which contains over 114k noisy and 40k manually\ncleaned description translations with multiple product images. To effectively\nlearn semantic alignments among product images and bilingual texts in\ntranslation, we design a unified product-oriented cross-modal cross-lingual\nmodel (\\upoc~) for pre-training and fine-tuning. Experiments on the Fashion-MMT\nand Multi30k datasets show that our model significantly outperforms the\nstate-of-the-art models even pre-trained on the same dataset. It is also shown\nto benefit more from large-scale noisy data to improve the translation quality.\nWe will release the dataset and codes at\nhttps://github.com/syuqings/Fashion-MMT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yuqing Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shizhe Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1\">Qin Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1\">Wei Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jun Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AutoShape: Real-Time Shape-Aware Monocular 3D Object Detection. (arXiv:2108.11127v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11127","description":"<p>Existing deep learning-based approaches for monocular 3D object detection in\nautonomous driving often model the object as a rotated 3D cuboid while the\nobject's geometric shape has been ignored. In this work, we propose an approach\nfor incorporating the shape-aware 2D/3D constraints into the 3D detection\nframework. Specifically, we employ the deep neural network to learn\ndistinguished 2D keypoints in the 2D image domain and regress their\ncorresponding 3D coordinates in the local 3D object coordinate first. Then the\n2D/3D geometric constraints are built by these correspondences for each object\nto boost the detection performance. For generating the ground truth of 2D/3D\nkeypoints, an automatic model-fitting approach has been proposed by fitting the\ndeformed 3D object model and the object mask in the 2D image. The proposed\nframework has been verified on the public KITTI dataset and the experimental\nresults demonstrate that by using additional geometrical constraints the\ndetection performance has been significantly improved as compared to the\nbaseline method. More importantly, the proposed framework achieves\nstate-of-the-art performance with real time. Data and code will be available at\nhttps://github.com/zongdai/AutoShape\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zongdai Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Dingfu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_F/0/1/0/all/0/1\">Feixiang Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_J/0/1/0/all/0/1\">Jin Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liangjun Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Scene Segmentation for Robotics Applications. (arXiv:2108.11128v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11128","description":"<p>Semantic scene segmentation plays a critical role in a wide range of robotics\napplications, e.g., autonomous navigation. These applications are accompanied\nby specific computational restrictions, e.g., operation on low-power GPUs, at\nsufficient speed, and also for high-resolution input. Existing state-of-the-art\nsegmentation models provide evaluation results under different setups and\nmainly considering high-power GPUs. In this paper, we investigate the behavior\nof the most successful semantic scene segmentation models, in terms of\ndeployment (inference) speed, under various setups (GPUs, input sizes, etc.) in\nthe context of robotics applications. The target of this work is to provide a\ncomparative study of current state-of-the-art segmentation models so as to\nselect the most compliant with the robotics applications requirements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tzelepi_M/0/1/0/all/0/1\">Maria Tzelepi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tefas_A/0/1/0/all/0/1\">Anastasios Tefas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Unified Taxonomy and Multimodal Dataset for Events in Invasion Games. (arXiv:2108.11149v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11149","description":"<p>The automatic detection of events in complex sports games like soccer and\nhandball using positional or video data is of large interest in research and\nindustry. One requirement is a fundamental understanding of underlying\nconcepts, i.e., events that occur on the pitch. Previous work often deals only\nwith so-called low-level events based on well-defined rules such as free kicks,\nfree throws, or goals. High-level events, such as passes, are less frequently\napproached due to a lack of consistent definitions. This introduces a level of\nambiguity that necessities careful validation when regarding event annotations.\nYet, this validation step is usually neglected as the majority of studies adopt\nannotations from commercial providers on private datasets of unknown quality\nand focuses on soccer only. To address these issues, we present (1) a universal\ntaxonomy that covers a wide range of low and high-level events for invasion\ngames and is exemplarily refined to soccer and handball, and (2) release two\nmulti-modal datasets comprising video and positional data with gold-standard\nannotations to foster research in fine-grained and ball-centered event\nspotting. Experiments on human performance demonstrate the robustness of the\nproposed taxonomy, and that disagreements and ambiguities in the annotation\nincrease with the complexity of the event. An I3D model for video\nclassification is adopted for event spotting and reveals the potential for\nbenchmarking. Datasets are available at: https://github.com/mm4spa/eigd\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Biermann_H/0/1/0/all/0/1\">Henrik Biermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theiner_J/0/1/0/all/0/1\">Jonas Theiner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bassek_M/0/1/0/all/0/1\">Manuel Bassek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raabe_D/0/1/0/all/0/1\">Dominik Raabe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Memmert_D/0/1/0/all/0/1\">Daniel Memmert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ewerth_R/0/1/0/all/0/1\">Ralph Ewerth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Duo-SegNet: Adversarial Dual-Views for Semi-Supervised Medical Image Segmentation. (arXiv:2108.11154v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11154","description":"<p>Segmentation of images is a long-standing challenge in medical AI. This is\nmainly due to the fact that training a neural network to perform image\nsegmentation requires a significant number of pixel-level annotated data, which\nis often unavailable. To address this issue, we propose a semi-supervised image\nsegmentation technique based on the concept of multi-view learning. In contrast\nto the previous art, we introduce an adversarial form of dual-view training and\nemploy a critic to formulate the learning problem in multi-view training as a\nmin-max problem. Thorough quantitative and qualitative evaluations on several\ndatasets indicate that our proposed method outperforms state-of-the-art medical\nimage segmentation algorithms consistently and comfortably. The code is\npublicly available at https://github.com/himashi92/Duo-SegNet\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peiris_H/0/1/0/all/0/1\">Himashi Peiris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhaolin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Egan_G/0/1/0/all/0/1\">Gary Egan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harandi_M/0/1/0/all/0/1\">Mehrtash Harandi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarially Robust One-class Novelty Detection. (arXiv:2108.11168v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11168","description":"<p>One-class novelty detectors are trained with examples of a particular class\nand are tasked with identifying whether a query example belongs to the same\nknown class. Most recent advances adopt a deep auto-encoder style architecture\nto compute novelty scores for detecting novel class data. Deep networks have\nshown to be vulnerable to adversarial attacks, yet little focus is devoted to\nstudying the adversarial robustness of deep novelty detectors. In this paper,\nwe first show that existing novelty detectors are susceptible to adversarial\nexamples. We further demonstrate that commonly-used defense approaches for\nclassification tasks have limited effectiveness in one-class novelty detection.\nHence, we need a defense specifically designed for novelty detection. To this\nend, we propose a defense strategy that manipulates the latent space of novelty\ndetectors to improve the robustness against adversarial examples. The proposed\nmethod, referred to as Principal Latent Space (PLS), learns the\nincrementally-trained cascade principal components in the latent space to\nrobustify novelty detectors. PLS can purify latent space against adversarial\nexamples and constrain latent space to exclusively model the known class\ndistribution. We conduct extensive experiments on multiple attacks, datasets\nand novelty detectors, showing that PLS consistently enhances the adversarial\nrobustness of novelty detection models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lo_S/0/1/0/all/0/1\">Shao-Yuan Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oza_P/0/1/0/all/0/1\">Poojan Oza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1\">Vishal M. Patel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Superpixel-guided Discriminative Low-rank Representation of Hyperspectral Images for Classification. (arXiv:2108.11172v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11172","description":"<p>In this paper, we propose a novel classification scheme for the remotely\nsensed hyperspectral image (HSI), namely SP-DLRR, by comprehensively exploring\nits unique characteristics, including the local spatial information and\nlow-rankness. SP-DLRR is mainly composed of two modules, i.e., the\nclassification-guided superpixel segmentation and the discriminative low-rank\nrepresentation, which are iteratively conducted. Specifically, by utilizing the\nlocal spatial information and incorporating the predictions from a typical\nclassifier, the first module segments pixels of an input HSI (or its\nrestoration generated by the second module) into superpixels. According to the\nresulting superpixels, the pixels of the input HSI are then grouped into\nclusters and fed into our novel discriminative low-rank representation model\nwith an effective numerical solution. Such a model is capable of increasing the\nintra-class similarity by suppressing the spectral variations locally while\npromoting the inter-class discriminability globally, leading to a restored HSI\nwith more discriminative pixels. Experimental results on three benchmark\ndatasets demonstrate the significant superiority of SP-DLRR over\nstate-of-the-art methods, especially for the case with an extremely limited\nnumber of training pixels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shujun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1\">Junhui Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1\">Yuheng Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_S/0/1/0/all/0/1\">Shaohui Mei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Q/0/1/0/all/0/1\">Qian Du</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recall@k Surrogate Loss with Large Batches and Similarity Mixup. (arXiv:2108.11179v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11179","description":"<p>Direct optimization, by gradient descent, of an evaluation metric, is not\npossible when it is non-differentiable, which is the case for recall in\nretrieval. In this work, a differentiable surrogate loss for the recall is\nproposed. Using an implementation that sidesteps the hardware constraints of\nthe GPU memory, the method trains with a very large batch size, which is\nessential for metrics computed on the entire retrieval database. It is assisted\nby an efficient mixup approach that operates on pairwise scalar similarities\nand virtually increases the batch size further. When used for deep metric\nlearning, the proposed method achieves state-of-the-art results in several\nimage retrieval benchmarks. For instance-level recognition, the method\noutperforms similar approaches that train using an approximation of average\nprecision. The implementation will be made public.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Patel_Y/0/1/0/all/0/1\">Yash Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tolias_G/0/1/0/all/0/1\">Giorgos Tolias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matas_J/0/1/0/all/0/1\">Jiri Matas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lizard: A Large-Scale Dataset for Colonic Nuclear Instance Segmentation and Classification. (arXiv:2108.11195v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11195","description":"<p>The development of deep segmentation models for computational pathology\n(CPath) can help foster the investigation of interpretable morphological\nbiomarkers. Yet, there is a major bottleneck in the success of such approaches\nbecause supervised deep learning models require an abundance of accurately\nlabelled data. This issue is exacerbated in the field of CPath because the\ngeneration of detailed annotations usually demands the input of a pathologist\nto be able to distinguish between different tissue constructs and nuclei.\nManually labelling nuclei may not be a feasible approach for collecting\nlarge-scale annotated datasets, especially when a single image region can\ncontain thousands of different cells. However, solely relying on automatic\ngeneration of annotations will limit the accuracy and reliability of ground\ntruth. Therefore, to help overcome the above challenges, we propose a\nmulti-stage annotation pipeline to enable the collection of large-scale\ndatasets for histology image analysis, with pathologist-in-the-loop refinement\nsteps. Using this pipeline, we generate the largest known nuclear instance\nsegmentation and classification dataset, containing nearly half a million\nlabelled nuclei in H&amp;E stained colon tissue. We have released the dataset and\nencourage the research community to utilise it to drive forward the development\nof downstream cell-based models in CPath.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Graham_S/0/1/0/all/0/1\">Simon Graham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jahanifar_M/0/1/0/all/0/1\">Mostafa Jahanifar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azam_A/0/1/0/all/0/1\">Ayesha Azam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nimir_M/0/1/0/all/0/1\">Mohammed Nimir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsang_Y/0/1/0/all/0/1\">Yee-Wah Tsang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dodd_K/0/1/0/all/0/1\">Katherine Dodd</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hero_E/0/1/0/all/0/1\">Emily Hero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahota_H/0/1/0/all/0/1\">Harvir Sahota</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tank_A/0/1/0/all/0/1\">Atisha Tank</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benes_K/0/1/0/all/0/1\">Ksenija Benes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wahab_N/0/1/0/all/0/1\">Noorul Wahab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minhas_F/0/1/0/all/0/1\">Fayyaz Minhas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raza_S/0/1/0/all/0/1\">Shan E Ahmed Raza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daly_H/0/1/0/all/0/1\">Hesham El Daly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gopalakrishnan_K/0/1/0/all/0/1\">Kishore Gopalakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snead_D/0/1/0/all/0/1\">David Snead</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajpoot_N/0/1/0/all/0/1\">Nasir Rajpoot</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-domain semantic segmentation with overlapping labels. (arXiv:2108.11224v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11224","description":"<p>Deep supervised models have an unprecedented capacity to absorb large\nquantities of training data. Hence, training on many datasets becomes a method\nof choice towards graceful degradation in unusual scenes. Unfortunately,\ndifferent datasets often use incompatible labels. For instance, the Cityscapes\nroad class subsumes all driving surfaces, while Vistas defines separate classes\nfor road markings, manholes etc. We address this challenge by proposing a\nprincipled method for seamless learning on datasets with overlapping classes\nbased on partial labels and probabilistic loss. Our method achieves competitive\nwithin-dataset and cross-dataset generalization, as well as ability to learn\nvisual concepts which are not separately labeled in any of the training\ndatasets. Experiments reveal competitive or state-of-the-art performance on two\nmulti-domain dataset collections and on the WildDash 2 benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bevandic_P/0/1/0/all/0/1\">Petra Bevandi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orsic_M/0/1/0/all/0/1\">Marin Or&#x161;i&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grubisic_I/0/1/0/all/0/1\">Ivan Grubi&#x161;i&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saric_J/0/1/0/all/0/1\">Josip &#x160;ari&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Segvic_S/0/1/0/all/0/1\">Sini&#x161;a &#x160;egvi&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cell Multi-Bernoulli (Cell-MB) Sensor Control for Multi-object Search-While-Tracking (SWT). (arXiv:2108.11236v1 [eess.SP])","link":"http://arxiv.org/abs/2108.11236","description":"<p>Information driven control can be used to develop intelligent sensors that\ncan optimize their measurement value based on environmental feedback. In object\ntracking applications, sensor actions are chosen based on the expected\nreduction in uncertainty also known as information gain. Random finite set\n(RFS) theory provides a formalism for quantifying and estimating information\ngain in multi-object tracking problems. However, estimating information gain in\nthese applications remains computationally challenging. This paper presents a\nnew tractable approximation of the RFS expected information gain applicable to\nsensor control for multi-object search and tracking. Unlike existing RFS\napproaches, the approximation presented in this paper accounts for noisy\nmeasurements, missed detections, false alarms, and object\nappearance/disappearance. The effectiveness of the information driven sensor\ncontrol is demonstrated through a multi-vehicle search-while-tracking\nexperiment using real video data from a remote optical sensor.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+LeGrand_K/0/1/0/all/0/1\">Keith A. LeGrand</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_P/0/1/0/all/0/1\">Pingping Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ferrari_S/0/1/0/all/0/1\">Silvia Ferrari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multiscale Spatio-Temporal Graph Neural Networks for 3D Skeleton-Based Motion Prediction. (arXiv:2108.11244v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11244","description":"<p>We propose a multiscale spatio-temporal graph neural network (MST-GNN) to\npredict the future 3D skeleton-based human poses in an action-category-agnostic\nmanner. The core of MST-GNN is a multiscale spatio-temporal graph that\nexplicitly models the relations in motions at various spatial and temporal\nscales. Different from many previous hierarchical structures, our multiscale\nspatio-temporal graph is built in a data-adaptive fashion, which captures\nnonphysical, yet motion-based relations. The key module of MST-GNN is a\nmultiscale spatio-temporal graph computational unit (MST-GCU) based on the\ntrainable graph structure. MST-GCU embeds underlying features at individual\nscales and then fuses features across scales to obtain a comprehensive\nrepresentation. The overall architecture of MST-GNN follows an encoder-decoder\nframework, where the encoder consists of a sequence of MST-GCUs to learn the\nspatial and temporal features of motions, and the decoder uses a graph-based\nattention gate recurrent unit (GA-GRU) to generate future poses. Extensive\nexperiments are conducted to show that the proposed MST-GNN outperforms\nstate-of-the-art methods in both short and long-term motion prediction on the\ndatasets of Human 3.6M, CMU Mocap and 3DPW, where MST-GNN outperforms previous\nworks by 5.33% and 3.67% of mean angle errors in average for short-term and\nlong-term prediction on Human 3.6M, and by 11.84% and 4.71% of mean angle\nerrors for short-term and long-term prediction on CMU Mocap, and by 1.13% of\nmean angle errors on 3DPW in average, respectively. We further investigate the\nlearned multiscale graphs for interpretability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Maosen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Siheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yangheng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ya Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanfeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalize then Adapt: Source-Free Domain Adaptive Semantic Segmentation. (arXiv:2108.11249v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11249","description":"<p>Unsupervised domain adaptation (DA) has gained substantial interest in\nsemantic segmentation. However, almost all prior arts assume concurrent access\nto both labeled source and unlabeled target, making them unsuitable for\nscenarios demanding source-free adaptation. In this work, we enable source-free\nDA by partitioning the task into two: a) source-only domain generalization and\nb) source-free target adaptation. Towards the former, we provide theoretical\ninsights to develop a multi-head framework trained with a virtually extended\nmulti-source dataset, aiming to balance generalization and specificity. Towards\nthe latter, we utilize the multi-head framework to extract reliable target\npseudo-labels for self-training. Additionally, we introduce a novel conditional\nprior-enforcing auto-encoder that discourages spatial irregularities, thereby\nenhancing the pseudo-label quality. Experiments on the standard\nGTA5-to-Cityscapes and SYNTHIA-to-Cityscapes benchmarks show our superiority\neven against the non-source-free prior-arts. Further, we show our compatibility\nwith online adaptation enabling deployment in a sequentially changing\nenvironment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kundu_J/0/1/0/all/0/1\">Jogendra Nath Kundu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulkarni_A/0/1/0/all/0/1\">Akshay Kulkarni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Amit Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jampani_V/0/1/0/all/0/1\">Varun Jampani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Babu_R/0/1/0/all/0/1\">R. Venkatesh Babu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"YOLOP: You Only Look Once for Panoptic Driving Perception. (arXiv:2108.11250v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11250","description":"<p>A panoptic driving perception system is an essential part of autonomous\ndriving. A high-precision and real-time perception system can assist the\nvehicle in making the reasonable decision while driving. We present a panoptic\ndriving perception network (YOLOP) to perform traffic object detection,\ndrivable area segmentation and lane detection simultaneously. It is composed of\none encoder for feature extraction and three decoders to handle the specific\ntasks. Our model performs extremely well on the challenging BDD100K dataset,\nachieving state-of-the-art on all three tasks in terms of accuracy and speed.\nBesides, we verify the effectiveness of our multi-task learning model for joint\ntraining via ablative studies. To our best knowledge, this is the first work\nthat can process these three visual perception tasks simultaneously in\nreal-time on an embedded device Jetson TX2(23 FPS) and maintain excellent\naccuracy. To facilitate further research, the source codes and pre-trained\nmodels will be released at https://github.com/hustvl/YOLOP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Dong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_M/0/1/0/all/0/1\">Manwen Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weitian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinggang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep few-shot learning for bi-temporal building change detection. (arXiv:2108.11262v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11262","description":"<p>In real-world applications (e.g., change detection), annotating images is\nvery expensive. To build effective deep learning models in these applications,\ndeep few-shot learning methods have been developed and prove to be a robust\napproach in small training data. The analysis of building change detection from\nhigh spatial resolution remote sensing observations is important research in\nphotogrammetry, computer vision, and remote sensing nowadays, which can be\nwidely used in a variety of real-world applications, such as map updating. As\nmanual high resolution image interpretation is expensive and time-consuming,\nbuilding change detection methods are of high interest. The interest in\ndeveloping building change detection approaches from optical remote sensing\nimages is rapidly increasing due to larger coverages, and lower costs of\noptical images. In this study, we focus on building change detection analysis\non a small set of building change from different regions that sit in several\ncities. In this paper, a new deep few-shot learning method is proposed for\nbuilding change detection using Monte Carlo dropout and remote sensing\nobservations. The setup is based on a small dataset, including bitemporal\noptical images labeled for building change detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khoshboresh_Masouleh_M/0/1/0/all/0/1\">Mehdi Khoshboresh-Masouleh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_Hosseini_R/0/1/0/all/0/1\">Reza Shah-Hosseini</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain Adversarial RetinaNet as a Reference Algorithm for the MItosis DOmain Generalization (MIDOG) Challenge. (arXiv:2108.11269v1 [eess.IV])","link":"http://arxiv.org/abs/2108.11269","description":"<p>Assessing the Mitotic Count has a known high degree of intra- and inter-rater\nvariability. Computer-aided systems have proven to decrease this variability\nand reduce labelling time. These systems, however, are generally highly\ndependent on their training domain and show poor applicability to unseen\ndomains. In histopathology, these domain shifts can result from various\nsources, including different slide scanning systems used to digitize histologic\nsamples. The MItosis DOmain Generalization challenge focuses on this specific\ndomain shift for the task of mitotic figure detection. This work presents a\nmitotic figure detection algorithm developed as a baseline for the challenge,\nbased on domain adversarial training. On the preliminary test set, the\nalgorithm scores an F$_1$ score of 0.7514.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wilm_F/0/1/0/all/0/1\">Frauke Wilm</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Breininger_K/0/1/0/all/0/1\">Katharina Breininger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Aubreville_M/0/1/0/all/0/1\">Marc Aubreville</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Measurement of Hybrid Rocket Solid Fuel Regression Rate for a Slab Burner using Deep Learning. (arXiv:2108.11276v1 [eess.IV])","link":"http://arxiv.org/abs/2108.11276","description":"<p>This study presents an imaging-based deep learning tool to measure the fuel\nregression rate in a 2D slab burner experiment for hybrid rocket fuels. The\nslab burner experiment is designed to verify mechanistic models of reacting\nboundary layer combustion in hybrid rockets by the measurement of fuel\nregression rates. A DSLR camera with a high intensity flash is used to capture\nimages throughout the burn and the images are then used to find the fuel\nboundary to calculate the regression rate. A U-net convolutional neural network\narchitecture is explored to segment the fuel from the experimental images. A\nMonte-Carlo Dropout process is used to quantify the regression rate uncertainty\nproduced from the network. The U-net computed regression rates are compared\nwith values from other techniques from literature and show error less than 10%.\nAn oxidizer flux dependency study is performed and shows the U-net predictions\nof regression rates are accurate and independent of the oxidizer flux, when the\nimages in the training set are not over-saturated. Training with monochrome\nimages is explored and is not successful at predicting the fuel regression rate\nfrom images with high noise. The network is superior at filtering out noise\nintroduced by soot, pitting, and wax deposition on the chamber glass as well as\nthe flame when compared to traditional image processing techniques, such as\nthreshold binary conversion and spatial filtering. U-net consistently provides\nlow error image segmentations to allow accurate computation of the regression\nrate of the fuel.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Surina_G/0/1/0/all/0/1\">Gabriel Surina III</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Georgalis_G/0/1/0/all/0/1\">Georgios Georgalis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Aphale_S/0/1/0/all/0/1\">Siddhant S. Aphale</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Patra_A/0/1/0/all/0/1\">Abani Patra</a>, <a href=\"http://arxiv.org/find/eess/1/au:+DesJardin_P/0/1/0/all/0/1\">Paul E. DesJardin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Feature Highlighting in Noisy RES Data With CycleGAN. (arXiv:2108.11283v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11283","description":"<p>Radio echo sounding (RES) is a common technique used in subsurface glacial\nimaging, which provides insight into the underlying rock and ice. However,\nsystematic noise is introduced into the data during collection, complicating\ninterpretation of the results. Researchers most often use a combination of\nmanual interpretation and filtering techniques to denoise data; however, these\nprocesses are time intensive and inconsistent. Fully Convolutional Networks\nhave been proposed as an automated alternative to identify layer boundaries in\nradargrams. However, they require high-quality manually processed training data\nand struggle to interpolate data in noisy samples (Varshney et al. 2020).\n</p>\n<p>Herein, the authors propose a GAN based model to interpolate layer boundaries\nthrough noise and highlight layers in two-dimensional glacial RES data. In\nreal-world noisy images, filtering often results in loss of data such that\ninterpolating layer boundaries is nearly impossible. Furthermore, traditional\nmachine learning approaches are not suited to this task because of the lack of\npaired data, so we employ an unpaired image-to-image translation model. For\nthis model, we create a synthetic dataset to represent the domain of images\nwith clear, highlighted layers and use an existing real-world RES dataset as\nour noisy domain.\n</p>\n<p>We implement a CycleGAN trained on these two domains to highlight layers in\nnoisy images that can interpolate effectively without significant loss of\nstructure or fidelity. Though the current implementation is not a perfect\nsolution, the model clearly highlights layers in noisy data and allows\nresearchers to determine layer size and position without mathematical\nfiltering, manual processing, or ground-truth images for training. This is\nsignificant because clean images generated by our model enable subsurface\nresearchers to determine glacial layer thickness more efficiently.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khami_N/0/1/0/all/0/1\">Nicholas Khami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Imtiaz_O/0/1/0/all/0/1\">Omar Imtiaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abidi_A/0/1/0/all/0/1\">Akif Abidi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aedavelli_A/0/1/0/all/0/1\">Akash Aedavelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goff_A/0/1/0/all/0/1\">Alan Goff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pisel_J/0/1/0/all/0/1\">Jesse R. Pisel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pyrcz_M/0/1/0/all/0/1\">Michael J. Pyrcz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fully Non-Homogeneous Atmospheric Scattering Modeling with Convolutional Neural Networks for Single Image Dehazing. (arXiv:2108.11292v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11292","description":"<p>In recent years, single image dehazing models (SIDM) based on atmospheric\nscattering model (ASM) have achieved remarkable results. However, it is noted\nthat ASM-based SIDM degrades its performance in dehazing real world hazy images\ndue to the limited modelling ability of ASM where the atmospheric light factor\n(ALF) and the angular scattering coefficient (ASC) are assumed as constants for\none image. Obviously, the hazy images taken in real world cannot always satisfy\nthis assumption. Such generating modelling mismatch between the real-world\nimages and ASM sets up the upper bound of trained ASM-based SIDM for dehazing.\nBearing this in mind, in this study, a new fully non-homogeneous atmospheric\nscattering model (FNH-ASM) is proposed for well modeling the hazy images under\ncomplex conditions where ALF and ASC are pixel dependent. However, FNH-ASM\nbrings difficulty in practical application. In FNH-ASM based SIDM, the\nestimation bias of parameters at different positions lead to different\ndistortion of dehazing result. Hence, in order to reduce the influence of\nparameter estimation bias on dehazing results, two new cost sensitive loss\nfunctions, beta-Loss and D-Loss, are innovatively developed for limiting the\nparameter bias of sensitive positions that have a greater impact on the\ndehazing result. In the end, based on FNH-ASM, an end-to-end CNN-based dehazing\nnetwork, FNHD-Net, is developed, which applies beta-Loss and D-Loss.\nExperimental results demonstrate the effectiveness and superiority of our\nproposed FNHD-Net for dehazing on both synthetic and real-world images. And the\nperformance improvement of our method increases more obviously in dense and\nheterogeneous haze scenes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Cong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuexian Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yong Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Backdoor Attacks on Network Certification via Data Poisoning. (arXiv:2108.11299v1 [cs.LG])","link":"http://arxiv.org/abs/2108.11299","description":"<p>Certifiers for neural networks have made great progress towards provable\nrobustness guarantees against evasion attacks using adversarial examples.\nHowever, introducing certifiers into deep learning systems also opens up new\nattack vectors, which need to be considered before deployment. In this work, we\nconduct the first systematic analysis of training time attacks against\ncertifiers in practical application pipelines, identifying new threat vectors\nthat can be exploited to degrade the overall system. Using these insights, we\ndesign two backdoor attacks against network certifiers, which can drastically\nreduce certified robustness when the backdoor is activated. For example, adding\n1% poisoned data points during training is sufficient to reduce certified\nrobustness by up to 95 percentage points, effectively rendering the certifier\nuseless. We analyze how such novel attacks can compromise the overall system's\nintegrity or availability. Our extensive experiments across multiple datasets,\nmodel architectures, and certifiers demonstrate the wide applicability of these\nattacks. A first investigation into potential defenses shows that current\napproaches only partially mitigate the issue, highlighting the need for new,\nmore specific solutions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lorenz_T/0/1/0/all/0/1\">Tobias Lorenz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwiatkowska_M/0/1/0/all/0/1\">Marta Kwiatkowska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fritz_M/0/1/0/all/0/1\">Mario Fritz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CSG-Stump: A Learning Friendly CSG-Like Representation for Interpretable Shape Parsing. (arXiv:2108.11305v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11305","description":"<p>Generating an interpretable and compact representation of 3D shapes from\npoint clouds is an important and challenging problem. This paper presents\nCSG-Stump Net, an unsupervised end-to-end network for learning shapes from\npoint clouds and discovering the underlying constituent modeling primitives and\noperations as well. At the core is a three-level structure called {\\em\nCSG-Stump}, consisting of a complement layer at the bottom, an intersection\nlayer in the middle, and a union layer at the top. CSG-Stump is proven to be\nequivalent to CSG in terms of representation, therefore inheriting the\ninterpretable, compact and editable nature of CSG while freeing from CSG's\ncomplex tree structures. Particularly, the CSG-Stump has a simple and regular\nstructure, allowing neural networks to give outputs of a constant\ndimensionality, which makes itself deep-learning friendly. Due to these\ncharacteristics of CSG-Stump, CSG-Stump Net achieves superior results compared\nto previous CSG-based methods and generates much more appealing shapes, as\nconfirmed by extensive experiments. Project page:\nhttps://kimren227.github.io/projects/CSGStump/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ren_D/0/1/0/all/0/1\">Daxuan Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Jianmin Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jianfei Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiatong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Haiyong Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1\">Zhongang Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Junzhe Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1\">Liang Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mingyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Haiyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_S/0/1/0/all/0/1\">Shuai Yi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Task Self-Training for Learning General Representations. (arXiv:2108.11353v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11353","description":"<p>Despite the fast progress in training specialized models for various tasks,\nlearning a single general model that works well for many tasks is still\nchallenging for computer vision. Here we introduce multi-task self-training\n(MuST), which harnesses the knowledge in independent specialized teacher models\n(e.g., ImageNet model on classification) to train a single general student\nmodel. Our approach has three steps. First, we train specialized teachers\nindependently on labeled datasets. We then use the specialized teachers to\nlabel an unlabeled dataset to create a multi-task pseudo labeled dataset.\nFinally, the dataset, which now contains pseudo labels from teacher models\ntrained on different datasets/tasks, is then used to train a student model with\nmulti-task learning. We evaluate the feature representations of the student\nmodel on 6 vision tasks including image recognition (classification, detection,\nsegmentation)and 3D geometry estimation (depth and surface normal estimation).\nMuST is scalable with unlabeled or partially labeled datasets and outperforms\nboth specialized supervised models and self-supervised models when training on\nlarge scale datasets. Lastly, we show MuST can improve upon already strong\ncheckpoints trained with billions of examples. The results suggest\nself-training is a promising direction to aggregate labeled and unlabeled\ntraining data for learning general feature representations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghiasi_G/0/1/0/all/0/1\">Golnaz Ghiasi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zoph_B/0/1/0/all/0/1\">Barret Zoph</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cubuk_E/0/1/0/all/0/1\">Ekin D. Cubuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1\">Quoc V. Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Tsung-Yi Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Blind Image Decomposition. (arXiv:2108.11364v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11364","description":"<p>We present and study a novel task named Blind Image Decomposition (BID),\nwhich requires separating a superimposed image into constituent underlying\nimages in a blind setting, that is, both the source components involved in\nmixing as well as the mixing mechanism are unknown. For example, rain may\nconsist of multiple components, such as rain streaks, raindrops, snow, and\nhaze. Rainy images can be treated as an arbitrary combination of these\ncomponents, some of them or all of them. How to decompose superimposed images,\nlike rainy images, into distinct source components is a crucial step towards\nreal-world vision systems. To facilitate research on this new task, we\nconstruct three benchmark datasets, including mixed image decomposition across\nmultiple domains, real-scenario deraining, and joint\nshadow/reflection/watermark removal. Moreover, we propose a simple yet general\nBlind Image Decomposition Network (BIDeN) to serve as a strong baseline for\nfuture work. Experimental results demonstrate the tenability of our benchmarks\nand the effectiveness of BIDeN. Code and project page are available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Junlin Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Weihao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_P/0/1/0/all/0/1\">Pengfei Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Chunyi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1\">Jie Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Armin_M/0/1/0/all/0/1\">Mohammad Ali Armin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petersson_L/0/1/0/all/0/1\">Lars Petersson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongdong Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CDCGen: Cross-Domain Conditional Generation via Normalizing Flows and Adversarial Training. (arXiv:2108.11368v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11368","description":"<p>How to generate conditional synthetic data for a domain without utilizing\ninformation about its labels/attributes? Our work presents a solution to the\nabove question. We propose a transfer learning-based framework utilizing\nnormalizing flows, coupled with both maximum-likelihood and adversarial\ntraining. We model a source domain (labels available) and a target domain\n(labels unavailable) with individual normalizing flows, and perform domain\nalignment to a common latent space using adversarial discriminators. Due to the\ninvertible property of flow models, the mapping has exact cycle consistency. We\nalso learn the joint distribution of the data samples and attributes in the\nsource domain by employing an encoder to map attributes to the latent space via\nadversarial training. During the synthesis phase, given any combination of\nattributes, our method can generate synthetic samples conditioned on them in\nthe target domain. Empirical studies confirm the effectiveness of our method on\nbenchmarked datasets. We envision our method to be particularly useful for\nsynthetic data generation in label-scarce systems by generating non-trivial\naugmentations via attribute transformations. These synthetic samples will\nintroduce more entropy into the label-scarce domain than their geometric and\nphotometric transformation counterparts, helpful for robust downstream tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Das_H/0/1/0/all/0/1\">Hari Prasanna Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_R/0/1/0/all/0/1\">Ryan Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_J/0/1/0/all/0/1\">Japjot Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yu-Wen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spanos_C/0/1/0/all/0/1\">Costas J. Spanos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Meta-Semi: A Meta-learning Approach for Semi-supervised Learning. (arXiv:2007.02394v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2007.02394","description":"<p>Deep learning based semi-supervised learning (SSL) algorithms have led to\npromising results in recent years. However, they tend to introduce multiple\ntunable hyper-parameters, making them less practical in real SSL scenarios\nwhere the labeled data is scarce for extensive hyper-parameter search. In this\npaper, we propose a novel meta-learning based SSL algorithm (Meta-Semi) that\nrequires tuning only one additional hyper-parameter, compared with a standard\nsupervised deep learning algorithm, to achieve competitive performance under\nvarious conditions of SSL. We start by defining a meta optimization problem\nthat minimizes the loss on labeled data through dynamically reweighting the\nloss on unlabeled samples, which are associated with soft pseudo labels during\ntraining. As the meta problem is computationally intensive to solve directly,\nwe propose an efficient algorithm to dynamically obtain the approximate\nsolutions. We show theoretically that Meta-Semi converges to the stationary\npoint of the loss function on labeled data under mild conditions. Empirically,\nMeta-Semi outperforms state-of-the-art SSL algorithms significantly on the\nchallenging semi-supervised CIFAR-100 and STL-10 tasks, and achieves\ncompetitive performance on CIFAR-10 and SVHN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yulin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jiayi Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Shiji Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Gao Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Cross-Modal Pre-Training: A General Strategy for Small Sample Medical Imaging. (arXiv:2010.03060v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2010.03060","description":"<p>A key challenge in training neural networks for a given medical imaging task\nis often the difficulty of obtaining a sufficient number of manually labeled\nexamples. In contrast, textual imaging reports, which are often readily\navailable in medical records, contain rich but unstructured interpretations\nwritten by experts as part of standard clinical practice. We propose using\nthese textual reports as a form of weak supervision to improve the image\ninterpretation performance of a neural network without requiring additional\nmanually labeled examples. We use an image-text matching task to train a\nfeature extractor and then fine-tune it in a transfer learning setting for a\nsupervised task using a small labeled dataset. The end result is a neural\nnetwork that automatically interprets imagery without requiring textual reports\nduring inference. This approach can be applied to any task for which text-image\npairs are readily available. We evaluate our method on three classification\ntasks and find consistent performance improvements, reducing the need for\nlabeled data by 67%-98%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_G/0/1/0/all/0/1\">Gongbo Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Greenwell_C/0/1/0/all/0/1\">Connor Greenwell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoqin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kavuluru_R/0/1/0/all/0/1\">Ramakanth Kavuluru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacobs_N/0/1/0/all/0/1\">Nathan Jacobs</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Variational Feature Disentangling for Fine-Grained Few-Shot Classification. (arXiv:2010.03255v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2010.03255","description":"<p>Fine-grained few-shot recognition often suffers from the problem of training\ndata scarcity for novel categories.The network tends to overfit and does not\ngeneralize well to unseen classes due to insufficient training data. Many\nmethods have been proposed to synthesize additional data to support the\ntraining. In this paper, we focus one enlarging the intra-class variance of the\nunseen class to improve few-shot classification performance. We assume that the\ndistribution of intra-class variance generalizes across the base class and the\nnovel class. Thus, the intra-class variance of the base set can be transferred\nto the novel set for feature augmentation. Specifically, we first model the\ndistribution of intra-class variance on the base set via variational inference.\nThen the learned distribution is transferred to the novel set to generate\nadditional features, which are used together with the original ones to train a\nclassifier. Experimental results show a significant boost over the\nstate-of-the-art methods on the challenging fine-grained few-shot image\nclassification benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jingyi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1\">Hieu Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Mingzhen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Athar_S/0/1/0/all/0/1\">ShahRukh Athar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samaras_D/0/1/0/all/0/1\">Dimitris Samaras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Improved and Interpretable Deep Metric Learning via Attentive Grouping. (arXiv:2011.08877v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.08877","description":"<p>Grouping has been commonly used in deep metric learning for computing diverse\nfeatures. However, current methods are prone to overfitting and lack\ninterpretability. In this work, we propose an improved and interpretable\ngrouping method to be integrated flexibly with any metric learning framework.\nOur method is based on the attention mechanism with a learnable query for each\ngroup. The query is fully trainable and can capture group-specific information\nwhen combined with the diversity loss. An appealing property of our method is\nthat it naturally lends itself interpretability. The attention scores between\nthe learnable query and each spatial position can be interpreted as the\nimportance of that position. We formally show that our proposed grouping method\nis invariant to spatial permutations of features. When used as a module in\nconvolutional neural networks, our method leads to translational invariance. We\nconduct comprehensive experiments to evaluate our method. Our quantitative\nresults indicate that the proposed method outperforms prior methods\nconsistently and significantly across different datasets, evaluation metrics,\nbase models, and loss functions. For the first time to the best of our\nknowledge, our interpretation results clearly demonstrate that the proposed\nmethod enables the learning of distinct and diverse features across groups. The\ncode is available on https://github.com/XinyiXuXD/DGML-master.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xinyi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhengyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_C/0/1/0/all/0/1\">Cheng Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1\">Hao Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shuiwang Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Boosting Monocular Depth Estimation with Lightweight 3D Point Fusion. (arXiv:2012.10296v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.10296","description":"<p>In this paper, we propose enhancing monocular depth estimation by adding 3D\npoints as depth guidance. Unlike existing depth completion methods, our\napproach performs well on extremely sparse and unevenly distributed point\nclouds, which makes it agnostic to the source of the 3D points. We achieve this\nby introducing a novel multi-scale 3D point fusion network that is both\nlightweight and efficient. We demonstrate its versatility on two different\ndepth estimation problems where the 3D points have been acquired with\nconventional structure-from-motion and LiDAR. In both cases, our network\nperforms on par with state-of-the-art depth completion methods and achieves\nsignificantly higher accuracy when only a small number of points is used while\nbeing more compact in terms of the number of parameters. We show that our\nmethod outperforms some contemporary deep learning based multi-view stereo and\nstructure-from-motion methods both in accuracy and in compactness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huynh_L/0/1/0/all/0/1\">Lam Huynh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_P/0/1/0/all/0/1\">Phong Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matas_J/0/1/0/all/0/1\">Jiri Matas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahtu_E/0/1/0/all/0/1\">Esa Rahtu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heikkila_J/0/1/0/all/0/1\">Janne Heikkila</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Memory-Augmented Reinforcement Learning for Image-Goal Navigation. (arXiv:2101.05181v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.05181","description":"<p>In this work, we present a memory-augmented approach for image-goal\nnavigation. Earlier attempts, including RL-based and SLAM-based approaches have\neither shown poor generalization performance, or are heavily-reliant on\npose/depth sensors. Our method uses an attention-based end-to-end model that\nleverages an episodic memory to learn to navigate. First, we train a\nstate-embedding network in a self-supervised fashion, and then use it to embed\npreviously-visited states into the agent's memory. Our navigation policy takes\nadvantage of this information through an attention mechanism. We validate our\napproach with extensive evaluations, and show that our model establishes a new\nstate of the art on the challenging Gibson dataset. Furthermore, we achieve\nthis impressive performance from RGB input alone, without access to additional\ninformation such as position or depth, in stark contrast to related work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mezghani_L/0/1/0/all/0/1\">Lina Mezghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sukhbaatar_S/0/1/0/all/0/1\">Sainbayar Sukhbaatar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lavril_T/0/1/0/all/0/1\">Thibaut Lavril</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maksymets_O/0/1/0/all/0/1\">Oleksandr Maksymets</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batra_D/0/1/0/all/0/1\">Dhruv Batra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bojanowski_P/0/1/0/all/0/1\">Piotr Bojanowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alahari_K/0/1/0/all/0/1\">Karteek Alahari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Coupling innovation method and feasibility analysis of garbage classification. (arXiv:2102.00193v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2102.00193","description":"<p>In order to solve the recent defect in garbage classification - including low\nlevel of intelligence, low accuracy and high cost of equipment, this paper\npresents a series of methods in identification and judgment in intelligent\ngarbage classification, including a material identification based on thermal\nprinciple and non-destructive laser irradiation, another material\nidentification based on optical diffraction and phase analysis, a profile\nidentification which utilizes a scenery thermal image after PCA and histogram\ncorrection, another profile identification which utilizes computer vision with\ninnovated data sets and algorithms. Combining AHP and Bayesian formula, the\npaper innovates a coupling algorithm which helps to make a comprehensive\njudgment of the garbage sort, based on the material and profile identification.\nThis paper also proposes a method for real-time space measurement of garbage\ncans, which based on the characteristics of air as fluid, and analyses the\nfunctions of air cleaning and particle disposing. Instead of the single use of\ngarbage image recognition, this paper provides a comprehensive method to judge\nthe garbage sort by material and profile identifications, which greatly\nenhancing the accuracy and intelligence in garbage classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zizhe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1\">Shaomeng Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mu_J/0/1/0/all/0/1\">Jiabei Mu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Instance and Panoptic Segmentation Using Conditional Convolutions. (arXiv:2102.03026v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.03026","description":"<p>We propose a simple yet effective framework for instance and panoptic\nsegmentation, termed CondInst (conditional convolutions for instance and\npanoptic segmentation). In the literature, top-performing instance segmentation\nmethods typically follow the paradigm of Mask R-CNN and rely on ROI operations\n(typically ROIAlign) to attend to each instance. In contrast, we propose to\nattend to the instances with dynamic conditional convolutions. Instead of using\ninstance-wise ROIs as inputs to the instance mask head of fixed weights, we\ndesign dynamic instance-aware mask heads, conditioned on the instances to be\npredicted. CondInst enjoys three advantages: 1.) Instance and panoptic\nsegmentation are unified into a fully convolutional network, eliminating the\nneed for ROI cropping and feature alignment. 2.) The elimination of the ROI\ncropping also significantly improves the output instance mask resolution. 3.)\nDue to the much improved capacity of dynamically-generated conditional\nconvolutions, the mask head can be very compact (e.g., 3 conv. layers, each\nhaving only 8 channels), leading to significantly faster inference time per\ninstance and making the overall inference time almost constant, irrelevant to\nthe number of instances. We demonstrate a simpler method that can achieve\nimproved accuracy and inference speed on both instance and panoptic\nsegmentation tasks. On the COCO dataset, we outperform a few state-of-the-art\nmethods. We hope that CondInst can be a strong baseline for instance and\npanoptic segmentation. Code is available at: https://git.io/AdelaiDet\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1\">Zhi Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bowen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Telling the What while Pointing to the Where: Multimodal Queries for Image Retrieval. (arXiv:2102.04980v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.04980","description":"<p>Most existing image retrieval systems use text queries as a way for the user\nto express what they are looking for. However, fine-grained image retrieval\noften requires the ability to also express where in the image the content they\nare looking for is. The text modality can only cumbersomely express such\nlocalization preferences, whereas pointing is a more natural fit. In this\npaper, we propose an image retrieval setup with a new form of multimodal\nqueries, where the user simultaneously uses both spoken natural language (the\nwhat) and mouse traces over an empty canvas (the where) to express the\ncharacteristics of the desired target image. We then describe simple\nmodifications to an existing image retrieval model, enabling it to operate in\nthis setup. Qualitative and quantitative experiments show that our model\neffectively takes this spatial guidance into account, and provides\nsignificantly more accurate retrieval results compared to text-only equivalent\nsystems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Changpinyo_S/0/1/0/all/0/1\">Soravit Changpinyo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pont_Tuset_J/0/1/0/all/0/1\">Jordi Pont-Tuset</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrari_V/0/1/0/all/0/1\">Vittorio Ferrari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soricut_R/0/1/0/all/0/1\">Radu Soricut</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigate Indistinguishable Points in Semantic Segmentation of 3D Point Cloud. (arXiv:2103.10339v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.10339","description":"<p>This paper investigates the indistinguishable points (difficult to predict\nlabel) in semantic segmentation for large-scale 3D point clouds. The\nindistinguishable points consist of those located in complex boundary, points\nwith similar local textures but different categories, and points in isolate\nsmall hard areas, which largely harm the performance of 3D semantic\nsegmentation. To address this challenge, we propose a novel Indistinguishable\nArea Focalization Network (IAF-Net), which selects indistinguishable points\nadaptively by utilizing the hierarchical semantic features and enhances\nfine-grained features for points especially those indistinguishable points. We\nalso introduce multi-stage loss to improve the feature representation in a\nprogressive way. Moreover, in order to analyze the segmentation performances of\nindistinguishable areas, we propose a new evaluation metric called\nIndistinguishable Points Based Metric (IPBM). Our IAF-Net achieves the\ncomparable results with state-of-the-art performance on several popular 3D\npoint cloud datasets e.g. S3DIS and ScanNet, and clearly outperforms other\nmethods on IPBM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mingye Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhipeng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Junhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Road to Know-Where: An Object-and-Room Informed Sequential BERT for Indoor Vision-Language Navigation. (arXiv:2104.04167v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.04167","description":"<p>Vision-and-Language Navigation (VLN) requires an agent to find a path to a\nremote location on the basis of natural-language instructions and a set of\nphoto-realistic panoramas. Most existing methods take the words in the\ninstructions and the discrete views of each panorama as the minimal unit of\nencoding. However, this requires a model to match different nouns (e.g., TV,\ntable) against the same input view feature. In this work, we propose an\nobject-informed sequential BERT to encode visual perceptions and linguistic\ninstructions at the same fine-grained level, namely objects and words. Our\nsequential BERT also enables the visual-textual clues to be interpreted in\nlight of the temporal context, which is crucial to multi-round VLN tasks.\nAdditionally, we enable the model to identify the relative direction (e.g.,\nleft/right/front/back) of each navigable location and the room type (e.g.,\nbedroom, kitchen) of its current and final navigation goal, as such information\nis widely mentioned in instructions implying the desired next and final\nlocations. We thus enable the model to know-where the objects lie in the\nimages, and to know-where they stand in the scene. Extensive experiments\ndemonstrate the effectiveness compared against several state-of-the-art methods\non three indoor VLN tasks: REVERIE, NDH, and R2R. Project repository:\nhttps://github.com/YuankaiQi/ORIST\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1\">Yuankai Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1\">Zizheng Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1\">Yicong Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Ming-Hsuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hengel_A/0/1/0/all/0/1\">Anton van den Hengel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qi Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Simple Baseline for Semi-supervised Semantic Segmentation with Strong Data Augmentation. (arXiv:2104.07256v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.07256","description":"<p>Recently, significant progress has been made on semantic segmentation.\nHowever, the success of supervised semantic segmentation typically relies on a\nlarge amount of labelled data, which is time-consuming and costly to obtain.\nInspired by the success of semi-supervised learning methods in image\nclassification, here we propose a simple yet effective semi-supervised learning\nframework for semantic segmentation. We demonstrate that the devil is in the\ndetails: a set of simple design and training techniques can collectively\nimprove the performance of semi-supervised semantic segmentation significantly.\nPrevious works [3, 27] fail to employ strong augmentation in pseudo label\nlearning efficiently, as the large distribution change caused by strong\naugmentation harms the batch normalisation statistics. We design a new batch\nnormalisation, namely distribution-specific batch normalisation (DSBN) to\naddress this problem and demonstrate the importance of strong augmentation for\nsemantic segmentation. Moreover, we design a self correction loss which is\neffective in noise resistance. We conduct a series of ablation studies to show\nthe effectiveness of each component. Our method achieves state-of-the-art\nresults in the semi-supervised settings on the Cityscapes and Pascal VOC\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Jianlong Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yifan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhibin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comprehensive Multi-Modal Interactions for Referring Image Segmentation. (arXiv:2104.10412v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.10412","description":"<p>We investigate Referring Image Segmentation (RIS), which outputs a\nsegmentation map corresponding to the given natural language description. To\nsolve RIS efficiently, we need to understand each word's relationship with\nother words, each region in the image to other regions, and cross-modal\nalignment between linguistic and visual domains. We argue that one of the\nlimiting factors in the recent methods is that they do not handle these\ninteractions simultaneously. To this end, we propose a novel architecture\ncalled JRNet, which uses a Joint Reasoning Module(JRM) to concurrently capture\nthe inter-modal and intra-modal interactions. The output of JRM is passed\nthrough a novel Cross-Modal Multi-Level Fusion (CMMLF) module which further\nrefines the segmentation masks by exchanging contextual information across\nvisual hierarchy through linguistic features acting as a bridge. We present\nthorough ablation studies and validate our approach's performance on four\nbenchmark datasets, showing considerable performance gains over the existing\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jain_K/0/1/0/all/0/1\">Kanishk Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gandhi_V/0/1/0/all/0/1\">Vineet Gandhi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pri3D: Can 3D Priors Help 2D Representation Learning?. (arXiv:2104.11225v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.11225","description":"<p>Recent advances in 3D perception have shown impressive progress in\nunderstanding geometric structures of 3Dshapes and even scenes. Inspired by\nthese advances in geometric understanding, we aim to imbue image-based\nperception with representations learned under geometric constraints. We\nintroduce an approach to learn view-invariant,geometry-aware representations\nfor network pre-training, based on multi-view RGB-D data, that can then be\neffectively transferred to downstream 2D tasks. We propose to employ\ncontrastive learning under both multi-view im-age constraints and\nimage-geometry constraints to encode3D priors into learned 2D representations.\nThis results not only in improvement over 2D-only representation learning on\nthe image-based tasks of semantic segmentation, instance segmentation, and\nobject detection on real-world in-door datasets, but moreover, provides\nsignificant improvement in the low data regime. We show a significant\nimprovement of 6.0% on semantic segmentation on full data as well as 11.9% on\n20% data against baselines on ScanNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1\">Ji Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1\">Saining Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Graham_B/0/1/0/all/0/1\">Benjamin Graham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1\">Angela Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niessner_M/0/1/0/all/0/1\">Matthias Nie&#xdf;ner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploiting Explanations for Model Inversion Attacks. (arXiv:2104.12669v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.12669","description":"<p>The successful deployment of artificial intelligence (AI) in many domains\nfrom healthcare to hiring requires their responsible use, particularly in model\nexplanations and privacy. Explainable artificial intelligence (XAI) provides\nmore information to help users to understand model decisions, yet this\nadditional knowledge exposes additional risks for privacy attacks. Hence,\nproviding explanation harms privacy. We study this risk for image-based model\ninversion attacks and identified several attack architectures with increasing\nperformance to reconstruct private image data from model explanations. We have\ndeveloped several multi-modal transposed CNN architectures that achieve\nsignificantly higher inversion performance than using the target model\nprediction only. These XAI-aware inversion models were designed to exploit the\nspatial knowledge in image explanations. To understand which explanations have\nhigher privacy risk, we analyzed how various explanation types and factors\ninfluence inversion performance. In spite of some models not providing\nexplanations, we further demonstrate increased inversion performance even for\nnon-explainable target models by exploiting explanations of surrogate models\nthrough attention transfer. This method first inverts an explanation from the\ntarget prediction, then reconstructs the target image. These threats highlight\nthe urgent and significant privacy risks of explanations and calls attention\nfor new privacy preservation techniques that balance the dual-requirement for\nAI explainability and privacy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xuejun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wencan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1\">Xiaokui Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_B/0/1/0/all/0/1\">Brian Y. Lim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Feature Decorrelation in Self-Supervised Learning. (arXiv:2105.00470v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2105.00470","description":"<p>In self-supervised representation learning, a common idea behind most of the\nstate-of-the-art approaches is to enforce the robustness of the representations\nto predefined augmentations. A potential issue of this idea is the existence of\ncompletely collapsed solutions (i.e., constant features), which are typically\navoided implicitly by carefully chosen implementation details. In this work, we\nstudy a relatively concise framework containing the most common components from\nrecent approaches. We verify the existence of complete collapse and discover\nanother reachable collapse pattern that is usually overlooked, namely\ndimensional collapse. We connect dimensional collapse with strong correlations\nbetween axes and consider such connection as a strong motivation for feature\ndecorrelation (i.e., standardizing the covariance matrix). The gains from\nfeature decorrelation are verified empirically to highlight the importance and\nthe potential of this insight.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hua_T/0/1/0/all/0/1\">Tianyu Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenxiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_Z/0/1/0/all/0/1\">Zihui Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1\">Sucheng Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hang Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SparseConvMIL: Sparse Convolutional Context-Aware Multiple Instance Learning for Whole Slide Image Classification. (arXiv:2105.02726v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.02726","description":"<p>Multiple instance learning (MIL) is the preferred approach for whole slide\nimage classification. However, most MIL approaches do not exploit the\ninterdependencies of tiles extracted from a whole slide image, which could\nprovide valuable cues for classification. This paper presents a novel MIL\napproach that exploits the spatial relationship of tiles for classifying whole\nslide images. To do so, a sparse map is built from tiles embeddings, and is\nthen classified by a sparse-input CNN. It obtained state-of-the-art performance\nover popular MIL approaches on the classification of cancer subtype involving\n10000 whole slide images. Our results suggest that the proposed approach might\n(i) improve the representation learning of instances and (ii) exploit the\ncontext of instance embeddings to enhance the classification performance. The\ncode of this work is open-source at {github censored for review}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lerousseau_M/0/1/0/all/0/1\">Marvin Lerousseau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vakalopoulou_M/0/1/0/all/0/1\">Maria Vakalopoulou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deutsch_E/0/1/0/all/0/1\">Eric Deutsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paragios_N/0/1/0/all/0/1\">Nikos Paragios</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Episodic Transformer for Vision-and-Language Navigation. (arXiv:2105.06453v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.06453","description":"<p>Interaction and navigation defined by natural language instructions in\ndynamic environments pose significant challenges for neural agents. This paper\nfocuses on addressing two challenges: handling long sequence of subtasks, and\nunderstanding complex human instructions. We propose Episodic Transformer\n(E.T.), a multimodal transformer that encodes language inputs and the full\nepisode history of visual observations and actions. To improve training, we\nleverage synthetic instructions as an intermediate representation that\ndecouples understanding the visual appearance of an environment from the\nvariations of natural language instructions. We demonstrate that encoding the\nhistory with a transformer is critical to solve compositional tasks, and that\npretraining and joint training with synthetic instructions further improve the\nperformance. Our approach sets a new state of the art on the challenging ALFRED\nbenchmark, achieving 38.4% and 8.5% task success rates on seen and unseen test\nsplits.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pashevich_A/0/1/0/all/0/1\">Alexander Pashevich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1\">Cordelia Schmid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Chen Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Physically Plausible Pose Refinement using Fully Differentiable Forces. (arXiv:2105.08196v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.08196","description":"<p>All hand-object interaction is controlled by forces that the two bodies exert\non each other, but little work has been done in modeling these underlying\nforces when doing pose and contact estimation from RGB/RGB-D data. Given the\npose of the hand and object from any pose estimation system, we propose an\nend-to-end differentiable model that refines pose estimates by learning the\nforces experienced by the object at each vertex in its mesh. By matching the\nlearned net force to an estimate of net force based on finite differences of\nposition, this model is able to find forces that accurately describe the\nmovement of the object, while resolving issues like mesh interpenetration and\nlack of contact. Evaluating on the ContactPose dataset, we show this model\nsuccessfully corrects poses and finds contact maps that better match the ground\ntruth, despite not using any RGB or depth image data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Akarsh Kumar</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Vaidya_A/0/1/0/all/0/1\">Aditya R. Vaidya</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Huth_A/0/1/0/all/0/1\">Alexander G. Huth</a> (1) ((1) The University of Texas at Austin)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OpenMatch: Open-set Consistency Regularization for Semi-supervised Learning with Outliers. (arXiv:2105.14148v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.14148","description":"<p>Semi-supervised learning (SSL) is an effective means to leverage unlabeled\ndata to improve a model's performance. Typical SSL methods like FixMatch assume\nthat labeled and unlabeled data share the same label space. However, in\npractice, unlabeled data can contain categories unseen in the labeled set,\ni.e., outliers, which can significantly harm the performance of SSL algorithms.\nTo address this problem, we propose a novel Open-set Semi-Supervised Learning\n(OSSL) approach called OpenMatch. Learning representations of inliers while\nrejecting outliers is essential for the success of OSSL. To this end, OpenMatch\nunifies FixMatch with novelty detection based on one-vs-all (OVA) classifiers.\nThe OVA-classifier outputs the confidence score of a sample being an inlier,\nproviding a threshold to detect outliers. Another key contribution is an\nopen-set soft-consistency regularization loss, which enhances the smoothness of\nthe OVA-classifier with respect to input transformations and greatly improves\noutlier detection. OpenMatch achieves state-of-the-art performance on three\ndatasets, and even outperforms a fully supervised model in detecting outliers\nunseen in unlabeled data on CIFAR10.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saito_K/0/1/0/all/0/1\">Kuniaki Saito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Donghyun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1\">Kate Saenko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Universal Domain Adaptation in Ordinal Regression. (arXiv:2106.11576v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.11576","description":"<p>We address the problem of universal domain adaptation (UDA) in ordinal\nregression (OR), which attempts to solve classification problems in which\nlabels are not independent, but follow a natural order. We show that the UDA\ntechniques developed for classification and based on the clustering assumption,\nunder-perform in OR settings. We propose a method that complements the OR\nclassifier with an auxiliary task of order learning, which plays the double\nrole of discriminating between common and private instances, and expanding\nclass labels to the private target images via ranking. Combined with\nadversarial domain discrimination, our model is able to address the closed set,\npartial and open set configurations. We evaluate our method on three face age\nestimation datasets, and show that it outperforms the baseline methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chidlovskii_B/0/1/0/all/0/1\">Boris Chidlovskii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadek_A/0/1/0/all/0/1\">Assem Sadek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_C/0/1/0/all/0/1\">Christian Wolf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical Object-oriented Spatio-Temporal Reasoning for Video Question Answering. (arXiv:2106.13432v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.13432","description":"<p>Video Question Answering (Video QA) is a powerful testbed to develop new AI\ncapabilities. This task necessitates learning to reason about objects,\nrelations, and events across visual and linguistic domains in space-time.\nHigh-level reasoning demands lifting from associative visual pattern\nrecognition to symbol-like manipulation over objects, their behavior and\ninteractions. Toward reaching this goal we propose an object-oriented reasoning\napproach in that video is abstracted as a dynamic stream of interacting\nobjects. At each stage of the video event flow, these objects interact with\neach other, and their interactions are reasoned about with respect to the query\nand under the overall context of a video. This mechanism is materialized into a\nfamily of general-purpose neural units and their multi-level architecture\ncalled Hierarchical Object-oriented Spatio-Temporal Reasoning (HOSTR) networks.\nThis neural model maintains the objects' consistent lifelines in the form of a\nhierarchically nested spatio-temporal graph. Within this graph, the dynamic\ninteractive object-oriented representations are built up along the video\nsequence, hierarchically abstracted in a bottom-up manner, and converge toward\nthe key information for the correct answer. The method is evaluated on multiple\nmajor Video QA datasets and establishes new state-of-the-arts in these tasks.\nAnalysis into the model's behavior indicates that object-oriented reasoning is\na reliable, interpretable and efficient approach to Video QA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dang_L/0/1/0/all/0/1\">Long Hoang Dang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1\">Thao Minh Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_V/0/1/0/all/0/1\">Vuong Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1\">Truyen Tran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"In-N-Out: Towards Good Initialization for Inpainting and Outpainting. (arXiv:2106.13953v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.13953","description":"<p>In computer vision, recovering spatial information by filling in masked\nregions, e.g., inpainting, has been widely investigated for its usability and\nwide applicability to other various applications: image inpainting, image\nextrapolation, and environment map estimation. Most of them are studied\nseparately depending on the applications. Our focus, however, is on\naccommodating the opposite task, e.g., image outpainting, which would benefit\nthe target applications, e.g., image inpainting. Our self-supervision method,\nIn-N-Out, is summarized as a training approach that leverages the knowledge of\nthe opposite task into the target model. We empirically show that In-N-Out --\nwhich explores the complementary information -- effectively takes advantage\nover the traditional pipelines where only task-specific learning takes place in\ntraining. In experiments, we compare our method to the traditional procedure\nand analyze the effectiveness of our method on different applications: image\ninpainting, image extrapolation, and environment map estimation. For these\ntasks, we demonstrate that In-N-Out consistently improves the performance of\nthe recent works with In-N-Out self-supervision to their training procedure.\nAlso, we show that our approach achieves better results than an existing\ntraining approach for outpainting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jo_C/0/1/0/all/0/1\">Changho Jo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Im_W/0/1/0/all/0/1\">Woobin Im</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Sung-Eui Yoon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Retrieve in Style: Unsupervised Facial Feature Transfer and Retrieval. (arXiv:2107.06256v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.06256","description":"<p>We present Retrieve in Style (RIS), an unsupervised framework for facial\nfeature transfer and retrieval on real images. Recent work shows capabilities\nof transferring local facial features by capitalizing on the disentanglement\nproperty of the StyleGAN latent space. RIS improves existing art on the\nfollowing: 1) Introducing more effective feature disentanglement to allow for\nchallenging transfers (ie, hair, pose) that were not shown possible in SoTA\nmethods. 2) Eliminating the need for per-image hyperparameter tuning, and for\ncomputing a catalog over a large batch of images. 3) Enabling fine-grained face\nretrieval using disentangled facial features (eg, eyes). To our best knowledge,\nthis is the first work to retrieve face images at this fine level. 4)\nDemonstrating robust, natural editing on real images. Our qualitative and\nquantitative analyses show RIS achieves both high-fidelity feature transfers\nand accurate fine-grained retrievals on real images. We also discuss the\nresponsible applications of RIS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chong_M/0/1/0/all/0/1\">Min Jin Chong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_W/0/1/0/all/0/1\">Wen-Sheng Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Abhishek Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Forsyth_D/0/1/0/all/0/1\">David Forsyth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-Shot Domain Adaptation in CT Segmentation by Filtered Back Projection Augmentation. (arXiv:2107.08543v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2107.08543","description":"<p>Domain shift is one of the most salient challenges in medical computer\nvision. Due to immense variability in scanners' parameters and imaging\nprotocols, even images obtained from the same person and the same scanner could\ndiffer significantly. We address variability in computed tomography (CT) images\ncaused by different convolution kernels used in the reconstruction process, the\ncritical domain shift factor in CT. The choice of a convolution kernel affects\npixels' granularity, image smoothness, and noise level. We analyze a dataset of\npaired CT images, where smooth and sharp images were reconstructed from the\nsame sinograms with different kernels, thus providing identical anatomy but\ndifferent style. Though identical predictions are desired, we show that the\nconsistency, measured as the average Dice between predictions on pairs, is just\n0.54. We propose Filtered Back-Projection Augmentation (FBPAug), a simple and\nsurprisingly efficient approach to augment CT images in sinogram space\nemulating reconstruction with different kernels. We apply the proposed method\nin a zero-shot domain adaptation setup and show that the consistency boosts\nfrom 0.54 to 0.92 outperforming other augmentation approaches. Neither specific\npreparation of source domain data nor target domain data is required, so our\npublicly released FBPAug can be used as a plug-and-play module for zero-shot\ndomain adaptation in any CT-based task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Saparov_T/0/1/0/all/0/1\">Talgat Saparov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kurmukov_A/0/1/0/all/0/1\">Anvar Kurmukov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shirokikh_B/0/1/0/all/0/1\">Boris Shirokikh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Belyaev_M/0/1/0/all/0/1\">Mikhail Belyaev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bi-Temporal Semantic Reasoning for the Semantic Change Detection of HR Remote Sensing Images. (arXiv:2108.06103v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.06103","description":"<p>Semantic change detection (SCD) extends the multi-class change detection\n(MCD) task to provide not only the change locations but also the detailed\nland-cover/land-use (LC/LU) categories before and after the observation\nintervals. This fine-grained semantic change information is very useful in many\napplications. Recent studies indicate that the SCD can be modeled through a\ntriple-branch Convolutional Neural Network (CNN), which contains two temporal\nbranches and a change branch. However, in this architecture, the communications\nbetween the temporal branches and the change branch are in-sufficient. To\novercome the limitations in existing methods, we propose a novel CNN\narchitecture for the SCD, where the semantic temporal features are merged in a\ndeep CD unit. Furthermore, we elaborate on this architecture to reason the\nbi-temporal semantic correlations. The resulting Bi-temporal Semantic Reasoning\nNetwork (Bi-SRNet) contains two types of semantic reasoning blocks to reason\nboth single-temporal and cross-temporal semantic correlations, as well as a\nnovel loss function to improve the semantic consistency of change detection\nresults. Experimental results on a benchmark dataset show that the proposed\narchitecture obtains significant accuracy improvements over the existing\napproaches, while the added designs in the Bi-SRNet further improves the\nsegmentation of both semantic categories and the changed areas. The codes in\nthis paper are accessible at: https://github.com/ggsDing/Bi-SRNet\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1\">Lei Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Haitao Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sicong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mou_L/0/1/0/all/0/1\">Lichao Mou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruzzone_L/0/1/0/all/0/1\">Lorenzo Bruzzone</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Activity Recognition for Autism Diagnosis. (arXiv:2108.07917v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.07917","description":"<p>A formal autism diagnosis is an inefficient and lengthy process. Families\noften have to wait years before receiving a diagnosis for their child; some may\nnot receive one at all due to this delay. One approach to this problem is to\nuse digital technologies to detect the presence of behaviors related to autism,\nwhich in aggregate may lead to remote and automated diagnostics. One of the\nstrongest indicators of autism is stimming, which is a set of repetitive,\nself-stimulatory behaviors such as hand flapping, headbanging, and spinning.\nUsing computer vision to detect hand flapping is especially difficult due to\nthe sparsity of public training data in this space and excessive shakiness and\nmotion in such data. Our work demonstrates a novel method that overcomes these\nissues: we use hand landmark detection over time as a feature representation\nwhich is then fed into a Long Short-Term Memory (LSTM) model. We achieve a\nvalidation accuracy and F1 Score of about 72% on detecting whether videos from\nthe Self-Stimulatory Behaviour Dataset (SSBD) contain hand flapping or not. Our\nbest model also predicts accurately on external videos we recorded of ourselves\noutside of the dataset it was trained on. This model uses less than 26,000\nparameters, providing promise for fast deployment into ubiquitous and wearable\ndigital settings for a remote autism diagnosis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lakkapragada_A/0/1/0/all/0/1\">Anish Lakkapragada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Washington_P/0/1/0/all/0/1\">Peter Washington</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wall_D/0/1/0/all/0/1\">Dennis Wall</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Early-exit deep neural networks for distorted images: providing an efficient edge offloading. (arXiv:2108.09343v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.09343","description":"<p>Edge offloading for deep neural networks (DNNs) can be adaptive to the\ninput's complexity by using early-exit DNNs. These DNNs have side branches\nthroughout their architecture, allowing the inference to end earlier in the\nedge. The branches estimate the accuracy for a given input. If this estimated\naccuracy reaches a threshold, the inference ends on the edge. Otherwise, the\nedge offloads the inference to the cloud to process the remaining DNN layers.\nHowever, DNNs for image classification deals with distorted images, which\nnegatively impact the branches' estimated accuracy. Consequently, the edge\noffloads more inferences to the cloud. This work introduces expert side\nbranches trained on a particular distortion type to improve robustness against\nimage distortion. The edge detects the distortion type and selects appropriate\nexpert branches to perform the inference. This approach increases the estimated\naccuracy on the edge, improving the offloading decisions. We validate our\nproposal in a realistic scenario, in which the edge offloads DNN inference to\nAmazon EC2 instances.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pacheco_R/0/1/0/all/0/1\">Roberto G. Pacheco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_F/0/1/0/all/0/1\">Fernanda D.V.R. Oliveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Couto_R/0/1/0/all/0/1\">Rodrigo S. Couto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SERF: Towards better training of deep neural networks using log-Softplus ERror activation Function. (arXiv:2108.09598v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2108.09598","description":"<p>Activation functions play a pivotal role in determining the training dynamics\nand neural network performance. The widely adopted activation function ReLU\ndespite being simple and effective has few disadvantages including the Dying\nReLU problem. In order to tackle such problems, we propose a novel activation\nfunction called Serf which is self-regularized and nonmonotonic in nature. Like\nMish, Serf also belongs to the Swish family of functions. Based on several\nexperiments on computer vision (image classification and object detection) and\nnatural language processing (machine translation, sentiment classification and\nmultimodal entailment) tasks with different state-of-the-art architectures, it\nis observed that Serf vastly outperforms ReLU (baseline) and other activation\nfunctions including both Swish and Mish, with a markedly bigger margin on\ndeeper architectures. Ablation studies further demonstrate that Serf based\narchitectures perform better than those of Swish and Mish in varying scenarios,\nvalidating the effectiveness and compatibility of Serf with varying depth,\ncomplexity, optimizers, learning rates, batch sizes, initializers and dropout\nrates. Finally, we investigate the mathematical relation between Swish and\nSerf, thereby showing the impact of preconditioner function ingrained in the\nfirst derivative of Serf which provides a regularization effect making\ngradients smoother and optimization faster.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nag_S/0/1/0/all/0/1\">Sayan Nag</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharyya_M/0/1/0/all/0/1\">Mayukh Bhattacharyya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The 2nd Anti-UAV Workshop & Challenge: Methods and Results. (arXiv:2108.09909v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.09909","description":"<p>The 2nd Anti-UAV Workshop \\&amp; Challenge aims to encourage research in\ndeveloping novel and accurate methods for multi-scale object tracking. The\nAnti-UAV dataset used for the Anti-UAV Challenge has been publicly released.\nThere are two subsets in the dataset, $i.e.$, the test-dev subset and\ntest-challenge subset. Both subsets consist of 140 thermal infrared video\nsequences, spanning multiple occurrences of multi-scale UAVs. Around 24\nparticipating teams from the globe competed in the 2nd Anti-UAV Challenge. In\nthis paper, we provide a brief summary of the 2nd Anti-UAV Workshop \\&amp;\nChallenge including brief introductions to the top three methods.The submission\nleaderboard will be reopened for researchers that are interested in the\nAnti-UAV challenge. The benchmark dataset and other information can be found\nat: https://anti-uav.github.io/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jian Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Gang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1\">Lei Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_N/0/1/0/all/0/1\">Nana Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Min Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaojuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yong_T/0/1/0/all/0/1\">Ting Yong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yafeng Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yandong Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1\">Shiming Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_G/0/1/0/all/0/1\">Guodong Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-08-25T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}}]}]}