{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.3","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-10-15T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Language Modelling via Learning to Rank. (arXiv:2110.06961v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06961","description":"<p>We consider language modelling (LM) as a multi-label structured prediction\ntask by re-framing training from solely predicting a single ground-truth word\nto ranking a set of words which could continue a given context. To avoid\nannotating top-$k$ ranks, we generate them using pre-trained LMs: GPT-2, BERT,\nand Born-Again models. This leads to a rank-based form of knowledge\ndistillation (KD). We also develop a method using $N$-grams to create a\nnon-probabilistic teacher which generates the ranks without the need of a\npre-trained LM.\n</p>\n<p>We confirm the hypotheses that we can treat LMing as a ranking task and that\nwe can do so without the use of a pre-trained LM. We show that rank-based KD\ngenerally improves perplexity (PPL), often with statistical significance, when\ncompared to Kullback-Leibler-based KD. Surprisingly, given the simplicity of\nthe method, $N$-grams act as competitive teachers and achieve similar\nperformance as using either BERT or a Born-Again model teachers. GPT-2 always\nacts as the best teacher, though, and using it and a Transformer-XL student on\nWiki-02, rank-based KD reduces a cross-entropy baseline from 65.27 to 55.94 and\nagainst a KL-based KD of 56.70.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Frydenlund_A/0/1/0/all/0/1\">Arvid Frydenlund</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_G/0/1/0/all/0/1\">Gagandeep Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudzicz_F/0/1/0/all/0/1\">Frank Rudzicz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Open-Domain Question-Answering for COVID-19 and Other Emergent Domains. (arXiv:2110.06962v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06962","description":"<p>Since late 2019, COVID-19 has quickly emerged as the newest biomedical\ndomain, resulting in a surge of new information. As with other emergent\ndomains, the discussion surrounding the topic has been rapidly changing,\nleading to the spread of misinformation. This has created the need for a public\nspace for users to ask questions and receive credible, scientific answers. To\nfulfill this need, we turn to the task of open-domain question-answering, which\nwe can use to efficiently find answers to free-text questions from a large set\nof documents. In this work, we present such a system for the emergent domain of\nCOVID-19. Despite the small data size available, we are able to successfully\ntrain the system to retrieve answers from a large-scale corpus of published\nCOVID-19 scientific papers. Furthermore, we incorporate effective re-ranking\nand question-answering techniques, such as document diversity and multiple\nanswer spans. Our open-domain question-answering system can further act as a\nmodel for the quick development of similar systems that can be adapted and\nmodified for other developing emergent domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Levy_S/0/1/0/all/0/1\">Sharon Levy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mo_K/0/1/0/all/0/1\">Kevin Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_W/0/1/0/all/0/1\">Wenhan Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FlexiTerm: A more efficient implementation of flexible multi-word term recognition. (arXiv:2110.06981v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06981","description":"<p>Terms are linguistic signifiers of domain-specific concepts. Automated\nrecognition of multi-word terms (MWT) in free text is a sequence labelling\nproblem, which is commonly addressed using supervised machine learning methods.\nTheir need for manual annotation of training data makes it difficult to port\nsuch methods across domains. FlexiTerm, on the other hand, is a fully\nunsupervised method for MWT recognition from domain-specific corpora.\nOriginally implemented in Java as a proof of concept, it did not scale well,\nthus offering little practical value in the context of big data. In this paper,\nwe describe its re-implementation in Python and compare the performance of\nthese two implementations. The results demonstrated major improvements in terms\nof efficiency, which allow FlexiTerm to transition from the proof of concept to\nthe production-grade application.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Spasic_I/0/1/0/all/0/1\">Irena Spasic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bandits Don't Follow Rules: Balancing Multi-Facet Machine Translation with Multi-Armed Bandits. (arXiv:2110.06997v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06997","description":"<p>Training data for machine translation (MT) is often sourced from a multitude\nof large corpora that are multi-faceted in nature, e.g. containing contents\nfrom multiple domains or different levels of quality or complexity. Naturally,\nthese facets do not occur with equal frequency, nor are they equally important\nfor the test scenario at hand. In this work, we propose to optimize this\nbalance jointly with MT model parameters to relieve system developers from\nmanual schedule design. A multi-armed bandit is trained to dynamically choose\nbetween facets in a way that is most beneficial for the MT system. We evaluate\nit on three different multi-facet applications: balancing translationese and\nnatural training data, or data from multiple domains or multiple language\npairs. We find that bandit learning leads to competitive MT systems across\ntasks, and our analysis provides insights into its learned strategies and the\nunderlying data sets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kreutzer_J/0/1/0/all/0/1\">Julia Kreutzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vilar_D/0/1/0/all/0/1\">David Vilar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sokolov_A/0/1/0/all/0/1\">Artem Sokolov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bag-of-Vectors Autoencoders for Unsupervised Conditional Text Generation. (arXiv:2110.07002v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07002","description":"<p>Text autoencoders are often used for unsupervised conditional text generation\nby applying mappings in the latent space to change attributes to the desired\nvalues. Recently, Mai et al. (2020) proposed Emb2Emb, a method to learn these\nmappings in the embedding space of an autoencoder. However, their method is\nrestricted to autoencoders with a single-vector embedding, which limits how\nmuch information can be retained. We address this issue by extending their\nmethod to Bag-of-Vectors Autoencoders (BoV-AEs), which encode the text into a\nvariable-size bag of vectors that grows with the size of the text, as in\nattention-based models. This allows to encode and reconstruct much longer texts\nthan standard autoencoders. Analogous to conventional autoencoders, we propose\nregularization techniques that facilitate learning meaningful operations in the\nlatent space. Finally, we adapt for a training scheme that learns to map an\ninput bag to an output bag, including a novel loss function and neural\narchitecture. Our experimental evaluations on unsupervised sentiment transfer\nand sentence summarization show that our method performs substantially better\nthan a standard autoencoder.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mai_F/0/1/0/all/0/1\">Florian Mai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henderson_J/0/1/0/all/0/1\">James Henderson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comparison of SVD and factorized TDNN approaches for speech to text. (arXiv:2110.07027v1 [cs.SD])","link":"http://arxiv.org/abs/2110.07027","description":"<p>This work concentrates on reducing the RTF and word error rate of a hybrid\nHMM-DNN. Our baseline system uses an architecture with TDNN and LSTM layers. We\nfind this architecture particularly useful for lightly reverberated\nenvironments. However, these models tend to demand more computation than is\ndesirable. In this work, we explore alternate architectures employing singular\nvalue decomposition (SVD) is applied to the TDNN layers to reduce the RTF, as\nwell as to the affine transforms of every LSTM cell. We compare this approach\nwith specifying bottleneck layers similar to those introduced by SVD before\ntraining. Additionally, we reduced the search space of the decoding graph to\nmake it a better fit to operate in real-time applications. We report -61.57%\nrelative reduction in RTF and almost 1% relative decrease in WER for our\narchitecture trained on Fisher data along with reverberated versions of this\ndataset in order to match one of our target test distributions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Michael_J/0/1/0/all/0/1\">Jeffrey Josanne Michael</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goel_N/0/1/0/all/0/1\">Nagendra Kumar Goel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+K_N/0/1/0/all/0/1\">Navneeth K</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robertson_J/0/1/0/all/0/1\">Jonas Robertson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Shravan Mishra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving the Robustness to Variations of Objects and Instructions with a Neuro-Symbolic Approach for Interactive Instruction Following. (arXiv:2110.07031v1 [cs.AI])","link":"http://arxiv.org/abs/2110.07031","description":"<p>An interactive instruction following task has been proposed as a benchmark\nfor learning to map natural language instructions and first-person vision into\nsequences of actions to interact with objects in a 3D simulated environment. We\nfind that an existing end-to-end neural model for this task is not robust to\nvariations of objects and language instructions. We assume that this problem is\ndue to the high sensitiveness of neural feature extraction to small changes in\nvision and language inputs. To mitigate this problem, we propose a\nneuro-symbolic approach that performs reasoning over high-level symbolic\nrepresentations that are robust to small changes in raw inputs. Our experiments\non the ALFRED dataset show that our approach significantly outperforms the\nexisting model by 18, 52, and 73 points in the success rate on the\nToggleObject, PickupObject, and SliceObject subtasks in unseen environments\nrespectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shinoda_K/0/1/0/all/0/1\">Kazutoshi Shinoda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takezawa_Y/0/1/0/all/0/1\">Yuki Takezawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suzuki_M/0/1/0/all/0/1\">Masahiro Suzuki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iwasawa_Y/0/1/0/all/0/1\">Yusuke Iwasawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsuo_Y/0/1/0/all/0/1\">Yutaka Matsuo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Efficient NLP: A Standard Evaluation and A Strong Baseline. (arXiv:2110.07038v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07038","description":"<p>Supersized pre-trained language models have pushed the accuracy of various\nNLP tasks to a new state-of-the-art (SOTA). Rather than pursuing the reachless\nSOTA accuracy, most works are pursuing improvement on other dimensions such as\nefficiency, leading to \"Pareto SOTA\". Different from accuracy, the metric for\nefficiency varies across different studies, making them hard to be fairly\ncompared. To that end, this work presents ELUE (Efficient Language\nUnderstanding Evaluation), a standard evaluation, and a public leaderboard for\nefficient NLP models. ELUE is dedicated to depicting the Pareto Front for\nvarious language understanding tasks, such that it can tell whether and how\nmuch a method achieves Pareto improvement. Along with the benchmark, we also\npre-train and release a strong baseline, ElasticBERT, whose elasticity is both\nstatic and dynamic. ElasticBERT is static in that it allows reducing model\nlayers on demand. ElasticBERT is dynamic in that it selectively executes parts\nof model layers conditioned on the input. We demonstrate the ElasticBERT,\ndespite its simplicity, outperforms or performs on par with SOTA compressed and\nearly exiting models. The ELUE benchmark is publicly available at\n<a href=\"http://eluebenchmark.fastnlp.top/.\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiangyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1\">Tianxiang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Junliang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lingling Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Hao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Zhao Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continual learning using lattice-free MMI for speech recognition. (arXiv:2110.07055v1 [eess.AS])","link":"http://arxiv.org/abs/2110.07055","description":"<p>Continual learning (CL), or domain expansion, recently became a popular topic\nfor automatic speech recognition (ASR) acoustic modeling because practical\nsystems have to be updated frequently in order to work robustly on types of\nspeech not observed during initial training. While sequential adaptation allows\ntuning a system to a new domain, it may result in performance degradation on\nthe old domains due to catastrophic forgetting. In this work we explore\nregularization-based CL for neural network acoustic models trained with the\nlattice-free maximum mutual information (LF-MMI) criterion. We simulate domain\nexpansion by incrementally adapting the acoustic model on different public\ndatasets that include several accents and speaking styles. We investigate two\nwell-known CL techniques, elastic weight consolidation (EWC) and learning\nwithout forgetting (LWF), which aim to reduce forgetting by preserving model\nweights or network outputs. We additionally introduce a sequence-level LWF\nregularization, which exploits posteriors from the denominator graph of LF-MMI\nto further reduce forgetting. Empirical results show that the proposed\nsequence-level LWF can improve the best average word error rate across all\ndomains by up to 9.4% relative compared with using regular LWF.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hadian_H/0/1/0/all/0/1\">Hossein Hadian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gorin_A/0/1/0/all/0/1\">Arseniy Gorin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MIMICause : Defining, identifying and predicting types of causal relationships between biomedical concepts from clinical notes. (arXiv:2110.07090v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07090","description":"<p>Understanding of causal narratives communicated in clinical notes can help\nmake strides towards personalized healthcare. In this work, MIMICause, we\npropose annotation guidelines, develop an annotated corpus and provide baseline\nscores to identify types and direction of causal relations between a pair of\nbiomedical concepts in clinical notes; communicated implicitly or explicitly,\nidentified either in a single sentence or across multiple sentences.\n</p>\n<p>We annotate a total of 2714 de-identified examples sampled from the 2018 n2c2\nshared task dataset and train four different language model based\narchitectures. Annotation based on our guidelines achieved a high\ninter-annotator agreement i.e. Fleiss' kappa score of 0.72 and our model for\nidentification of causal relation achieved a macro F1 score of 0.56 on test\ndata. The high inter-annotator agreement for clinical text shows the quality of\nour annotation guidelines while the provided baseline F1 score sets the\ndirection for future research towards understanding narratives in clinical\ntexts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khetan_V/0/1/0/all/0/1\">Vivek Khetan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rizvi_M/0/1/0/all/0/1\">Md Imbesat Hassan Rizvi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huber_J/0/1/0/all/0/1\">Jessica Huber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bartusiak_P/0/1/0/all/0/1\">Paige Bartusiak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sacaleanu_B/0/1/0/all/0/1\">Bogdan Sacaleanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fano_A/0/1/0/all/0/1\">Andrew Fano</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Identifying Introductions in Podcast Episodes from Automatically Generated Transcripts. (arXiv:2110.07096v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07096","description":"<p>As the volume of long-form spoken-word content such as podcasts explodes,\nmany platforms desire to present short, meaningful, and logically coherent\nsegments extracted from the full content. Such segments can be consumed by\nusers to sample content before diving in, as well as used by the platform to\npromote and recommend content. However, little published work is focused on the\nsegmentation of spoken-word content, where the errors (noise) in transcripts\ngenerated by automatic speech recognition (ASR) services poses many challenges.\nHere we build a novel dataset of complete transcriptions of over 400 podcast\nepisodes, in which we label the position of introductions in each episode.\nThese introductions contain information about the episodes' topics, hosts, and\nguests, providing a valuable summary of the episode content, as it is created\nby the authors. We further augment our dataset with word substitutions to\nincrease the amount of available training data. We train three Transformer\nmodels based on the pre-trained BERT and different augmentation strategies,\nwhich achieve significantly better performance compared with a static embedding\nmodel, showing that it is possible to capture generalized, larger-scale\nstructural information from noisy, loosely-organized speech data. This is\nfurther demonstrated through an analysis of the models' inner architecture. Our\nmethods and dataset can be used to facilitate future work on the\nstructure-based segmentation of spoken-word content.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jing_E/0/1/0/all/0/1\">Elise Jing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schneck_K/0/1/0/all/0/1\">Kristiana Schneck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Egan_D/0/1/0/all/0/1\">Dennis Egan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waterman_S/0/1/0/all/0/1\">Scott A. Waterman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A CLIP-Enhanced Method for Video-Language Understanding. (arXiv:2110.07137v1 [cs.CV])","link":"http://arxiv.org/abs/2110.07137","description":"<p>This technical report summarizes our method for the Video-And-Language\nUnderstanding Evaluation (VALUE) challenge\n(https://value-benchmark.github.io/challenge\\_2021.html). We propose a\nCLIP-Enhanced method to incorporate the image-text pretrained knowledge into\ndownstream video-text tasks. Combined with several other improved designs, our\nmethod outperforms the state-of-the-art by $2.4\\%$ ($57.58$ to $60.00$)\nMeta-Ave score on VALUE benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guohao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_F/0/1/0/all/0/1\">Feng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zhifan Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mind the Style of Text! Adversarial and Backdoor Attacks Based on Text Style Transfer. (arXiv:2110.07139v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07139","description":"<p>Adversarial attacks and backdoor attacks are two common security threats that\nhang over deep learning. Both of them harness task-irrelevant features of data\nin their implementation. Text style is a feature that is naturally irrelevant\nto most NLP tasks, and thus suitable for adversarial and backdoor attacks. In\nthis paper, we make the first attempt to conduct adversarial and backdoor\nattacks based on text style transfer, which is aimed at altering the style of a\nsentence while preserving its meaning. We design an adversarial attack method\nand a backdoor attack method, and conduct extensive experiments to evaluate\nthem. Experimental results show that popular NLP models are vulnerable to both\nadversarial and backdoor attacks based on text style transfer -- the attack\nsuccess rates can exceed 90% without much effort. It reflects the limited\nability of NLP models to handle the feature of text style that has not been\nwidely realized. In addition, the style transfer-based adversarial and backdoor\nattack methods show superiority to baselines in many aspects. All the code and\ndata of this paper can be obtained at https://github.com/thunlp/StyleAttack.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qi_F/0/1/0/all/0/1\">Fanchao Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yangyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xurui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mukai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"bert2BERT: Towards Reusable Pretrained Language Models. (arXiv:2110.07143v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07143","description":"<p>In recent years, researchers tend to pre-train ever-larger language models to\nexplore the upper limit of deep models. However, large language model\npre-training costs intensive computational resources and most of the models are\ntrained from scratch without reusing the existing pre-trained models, which is\nwasteful. In this paper, we propose bert2BERT, which can effectively transfer\nthe knowledge of an existing smaller pre-trained model (e.g., BERT_BASE) to a\nlarge model (e.g., BERT_LARGE) through parameter initialization and\nsignificantly improve the pre-training efficiency of the large model.\nSpecifically, we extend the previous function-preserving on Transformer-based\nlanguage model, and further improve it by proposing advanced knowledge for\nlarge model's initialization. In addition, a two-stage pre-training method is\nproposed to further accelerate the training process. We did extensive\nexperiments on representative PLMs (e.g., BERT and GPT) and demonstrate that\n(1) our method can save a significant amount of training cost compared with\nbaselines including learning from scratch, StackBERT and MSLT; (2) our method\nis generic and applicable to different types of pre-trained models. In\nparticular, bert2BERT saves about 45% and 47% computational cost of\npre-training BERT_BASE and GPT_BASE by reusing the models of almost their half\nsizes. The source code will be publicly available upon publication.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Cheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1\">Yichun Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_L/0/1/0/all/0/1\">Lifeng Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yujia Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fengyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Lingual GenQA: A Language-Agnostic Generative Question Answering Approach for Open-Domain Question Answering. (arXiv:2110.07150v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07150","description":"<p>Open-Retrieval Generative Question Answering (GenQA) is proven to deliver\nhigh-quality, natural-sounding answers in English. In this paper, we present\nthe first generalization of the GenQA approach for the multilingual\nenvironment. To this end, we present the GenTyDiQA dataset, which extends the\nTyDiQA evaluation data (Clark et al., 2020) with natural-sounding, well-formed\nanswers in Arabic, Bengali, English, Japanese, and Russian. For all these\nlanguages, we show that a GenQA sequence-to-sequence-based model outperforms a\nstate-of-the-art Answer Sentence Selection model. We also show that a\nmultilingually-trained model competes with, and in some cases outperforms, its\nmonolingual counterparts. Finally, we show that our system can even compete\nwith strong baselines, even when fed with information from a variety of\nlanguages. Essentially, our system is able to answer a question in any language\nof our language set using information from many languages, making it the first\nLanguage-Agnostic GenQA system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Muller_B/0/1/0/all/0/1\">Benjamin Muller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soldaini_L/0/1/0/all/0/1\">Luca Soldaini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koncel_Kedziorski_R/0/1/0/all/0/1\">Rik Koncel-Kedziorski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lind_E/0/1/0/all/0/1\">Eric Lind</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moschitti_A/0/1/0/all/0/1\">Alessandro Moschitti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Causally Estimating the Sensitivity of Neural NLP Models to Spurious Features. (arXiv:2110.07159v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07159","description":"<p>Recent work finds modern natural language processing (NLP) models relying on\nspurious features for prediction. Mitigating such effects is thus important.\nDespite this need, there is no quantitative measure to evaluate or compare the\neffects of different forms of spurious features in NLP. We address this gap in\nthe literature by quantifying model sensitivity to spurious features with a\ncausal estimand, dubbed CENT, which draws on the concept of average treatment\neffect from the causality literature. By conducting simulations with four\nprominent NLP models -- TextRNN, BERT, RoBERTa and XLNet -- we rank the models\nagainst their sensitivity to artificial injections of eight spurious features.\nWe further hypothesize and validate that models that are more sensitive to a\nspurious feature will be less robust against perturbations with this feature\nduring inference. Conversely, data augmentation with this feature improves\nrobustness to similar perturbations. We find statistically significant inverse\ncorrelations between sensitivity and robustness, providing empirical support\nfor our hypothesis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yunxiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1\">Liangming Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1\">Samson Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kan_M/0/1/0/all/0/1\">Min-Yen Kan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformer over Pre-trained Transformer for Neural Text Segmentation with Enhanced Topic Coherence. (arXiv:2110.07160v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07160","description":"<p>This paper proposes a transformer over transformer framework, called\nTransformer$^2$, to perform neural text segmentation. It consists of two\ncomponents: bottom-level sentence encoders using pre-trained transformers, and\nan upper-level transformer-based segmentation model based on the sentence\nembeddings. The bottom-level component transfers the pre-trained knowledge\nlearned from large external corpora under both single and pair-wise supervised\nNLP tasks to model the sentence embeddings for the documents. Given the\nsentence embeddings, the upper-level transformer is trained to recover the\nsegmentation boundaries as well as the topic labels of each sentence. Equipped\nwith a multi-task loss and the pre-trained knowledge, Transformer$^2$ can\nbetter capture the semantic coherence within the same segments. Our experiments\nshow that (1) Transformer$^2$ manages to surpass state-of-the-art text\nsegmentation models in terms of a commonly-used semantic coherence measure; (2)\nin most cases, both single and pair-wise pre-trained knowledge contribute to\nthe model performance; (3) bottom-level sentence encoders pre-trained on\nspecific languages yield better performance than those pre-trained on specific\ndomains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lo_K/0/1/0/all/0/1\">Kelvin Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yuan Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_W/0/1/0/all/0/1\">Weicong Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Ming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_L/0/1/0/all/0/1\">Lan Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buntine_W/0/1/0/all/0/1\">Wray Buntine</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Attention-Aware Hierarchical Topic Model. (arXiv:2110.07161v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07161","description":"<p>Neural topic models (NTMs) apply deep neural networks to topic modelling.\nDespite their success, NTMs generally ignore two important aspects: (1) only\ndocument-level word count information is utilized for the training, while more\nfine-grained sentence-level information is ignored, and (2) external semantic\nknowledge regarding documents, sentences and words are not exploited for the\ntraining. To address these issues, we propose a variational autoencoder (VAE)\nNTM model that jointly reconstructs the sentence and document word counts using\ncombinations of bag-of-words (BoW) topical embeddings and pre-trained semantic\nembeddings. The pre-trained embeddings are first transformed into a common\nlatent topical space to align their semantics with the BoW embeddings. Our\nmodel also features hierarchical KL divergence to leverage embeddings of each\ndocument to regularize those of their sentences, thereby paying more attention\nto semantically relevant sentences. Both quantitative and qualitative\nexperiments have shown the efficacy of our model in 1) lowering the\nreconstruction errors at both the sentence and document levels, and 2)\ndiscovering more coherent topics from real-world datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yuan Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">He Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Ming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_L/0/1/0/all/0/1\">Lan Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buntine_W/0/1/0/all/0/1\">Wray Buntine</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantically Distributed Robust Optimization for Vision-and-Language Inference. (arXiv:2110.07165v1 [cs.CV])","link":"http://arxiv.org/abs/2110.07165","description":"<p>Analysis of vision-and-language models has revealed their brittleness under\nlinguistic phenomena such as paraphrasing, negation, textual entailment, and\nword substitutions with synonyms or antonyms. While data augmentation\ntechniques have been designed to mitigate against these failure modes, methods\nthat can integrate this knowledge into the training pipeline remain\nunder-explored. In this paper, we present \\textbf{SDRO}, a model-agnostic\nmethod that utilizes a set linguistic transformations in a distributed robust\noptimization setting, along with an ensembling technique to leverage these\ntransformations during inference. Experiments on benchmark datasets with images\n(NLVR$^2$) and video (VIOLIN) demonstrate performance improvements as well as\nrobustness to adversarial attacks. Experiments on binary VQA explore the\ngeneralizability of this method to other V\\&amp;L tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gokhale_T/0/1/0/all/0/1\">Tejas Gokhale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhary_A/0/1/0/all/0/1\">Abhishek Chaudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_P/0/1/0/all/0/1\">Pratyay Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yezhou Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MoFE: Mixture of Factual Experts for Controlling Hallucinations in Abstractive Summarization. (arXiv:2110.07166v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07166","description":"<p>Neural abstractive summarization models are susceptible to generating\nfactually inconsistent content, a phenomenon known as hallucination. This\nlimits the usability and adoption of these systems in real-world applications.\nTo reduce the presence of hallucination, we propose the Mixture of Factual\nExperts (MoFE) model, which combines multiple summarization experts that each\ntarget a specific type of error. We train our experts using reinforcement\nlearning (RL) to minimize the error defined by two factual consistency metrics:\nentity overlap and dependency arc entailment. We construct MoFE by combining\nthe experts using two ensembling strategies (weights and logits) and evaluate\nthem on two summarization datasets (XSUM and CNN/DM). Our experiments on BART\nmodels show that the MoFE improves performance according to both entity overlap\nand dependency arc entailment, without a significant performance drop on\nstandard ROUGE metrics. The performance improvement also transfers to unseen\nfactual consistency metrics, such as question answer-based factuality\nevaluation metric and BERTScore precision with respect to the source document.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choubey_P/0/1/0/all/0/1\">Prafulla Kumar Choubey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vig_J/0/1/0/all/0/1\">Jesse Vig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenhao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajani_N/0/1/0/all/0/1\">Nazneen Fatema Rajani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Context-gloss Augmentation for Improving Word Sense Disambiguation. (arXiv:2110.07174v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07174","description":"<p>The goal of Word Sense Disambiguation (WSD) is to identify the sense of a\npolysemous word in a specific context. Deep-learning techniques using BERT have\nachieved very promising results in the field and different methods have been\nproposed to integrate structured knowledge to enhance performance. At the same\ntime, an increasing number of data augmentation techniques have been proven to\nbe useful for NLP tasks. Building upon previous works leveraging BERT and\nWordNet knowledge, we explore different data augmentation techniques on\ncontext-gloss pairs to improve the performance of WSD. In our experiment, we\nshow that both sentence-level and word-level augmentation methods are effective\nstrategies for WSD. Also, we find out that performance can be improved by\nadding hypernyms' glosses obtained from a lexical knowledge base. We compare\nand analyze different context-gloss augmentation techniques, and the results\nshow that applying back translation on gloss performs the best.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1\">Guan-Ting Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giambi_M/0/1/0/all/0/1\">Manuel Giambi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Symbolic Knowledge Distillation: from General Language Models to Commonsense Models. (arXiv:2110.07178v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07178","description":"<p>The common practice for training commonsense models has gone\nfrom-human-to-corpus-to-machine: humans author commonsense knowledge graphs in\norder to train commonsense models. In this work, we investigate an alternative,\nfrom-machine-to-corpus-to-machine: general language models author these\ncommonsense knowledge graphs to train commonsense models. Our study leads to a\nnew framework, Symbolic Knowledge Distillation. As with prior art in Knowledge\nDistillation (Hinton et al., 2015), our approach uses larger models to teach\nsmaller models. A key difference is that we distill knowledge symbolically-as\ntext-in addition to the neural model. We also distill only one aspect-the\ncommonsense of a general language model teacher, allowing the student to be a\ndifferent type, a commonsense model. Altogether, we show that careful prompt\nengineering and a separately trained critic model allow us to selectively\ndistill high-quality causal commonsense from GPT-3, a general language model.\nEmpirical results demonstrate that, for the first time, a human-authored\ncommonsense knowledge graph is surpassed by our automatically distilled variant\nin all three criteria: quantity, quality, and diversity. In addition, it\nresults in a neural commonsense model that surpasses the teacher model's\ncommonsense capabilities despite its 100x smaller size. We apply this to the\nATOMIC resource, and share our new symbolic knowledge graph and commonsense\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+West_P/0/1/0/all/0/1\">Peter West</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhagavatula_C/0/1/0/all/0/1\">Chandra Bhagavatula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hessel_J/0/1/0/all/0/1\">Jack Hessel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1\">Jena D. Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Liwei Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bras_R/0/1/0/all/0/1\">Ronan Le Bras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Ximing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Welleck_S/0/1/0/all/0/1\">Sean Welleck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting IPA-based Cross-lingual Text-to-speech. (arXiv:2110.07187v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07187","description":"<p>International Phonetic Alphabet (IPA) has been widely used in cross-lingual\ntext-to-speech (TTS) to achieve cross-lingual voice cloning (CL VC). However,\nIPA itself has been understudied in cross-lingual TTS. In this paper, we report\nsome empirical findings of building a cross-lingual TTS model using IPA as\ninputs. Experiments show that the way to process the IPA and suprasegmental\nsequence has a negligible impact on the CL VC performance. Furthermore, we find\nthat using a dataset including one speaker per language to build an IPA-based\nTTS system would fail CL VC since the language-unique IPA and tone/stress\nsymbols could leak the speaker information. In addition, we experiment with\ndifferent combinations of speakers in the training dataset to further\ninvestigate the effect of the number of speakers on the CL VC performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haitong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yue Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Self-Supervision Objectives for Generalizable Coherence Modeling. (arXiv:2110.07198v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07198","description":"<p>Although large-scale pre-trained neural models have shown impressive\nperformances in a variety of tasks, their ability to generate coherent text\nthat appropriately models discourse phenomena is harder to evaluate and less\nunderstood. Given the claims of improved text generation quality across various\nsystems, we consider the coherence evaluation of machine generated text to be\none of the principal applications of coherence models that needs to be\ninvestigated. We explore training data and self-supervision objectives that\nresult in a model that generalizes well across tasks and can be used\noff-the-shelf to perform such evaluations. Prior work in neural coherence\nmodeling has primarily focused on devising new architectures, and trained the\nmodel to distinguish coherent and incoherent text through pairwise\nself-supervision on the permuted documents task. We instead use a basic model\narchitecture and show significant improvements over state of the art within the\nsame training regime. We then design a harder self-supervision objective by\nincreasing the ratio of negative samples within a contrastive learning setup,\nand enhance the model further through automatic hard negative mining coupled\nwith a large global negative queue encoded by a momentum encoder. We show\nempirically that increasing the density of negative samples improves the basic\nmodel, and using a global negative queue further improves and stabilizes the\nmodel while training with hard negative samples. We evaluate the coherence\nmodel on task-independent test sets that resemble real-world use cases and show\nsignificant improvements in coherence evaluations of downstream applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jwalapuram_P/0/1/0/all/0/1\">Prathyusha Jwalapuram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xiang Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SpeechT5: Unified-Modal Encoder-Decoder Pre-training for Spoken Language Processing. (arXiv:2110.07205v1 [eess.AS])","link":"http://arxiv.org/abs/2110.07205","description":"<p>Motivated by the success of T5 (Text-To-Text Transfer Transformer) in\npre-training natural language processing models, we propose a unified-modal\nSpeechT5 framework that explores the encoder-decoder pre-training for\nself-supervised speech/text representation learning. The SpeechT5 framework\nconsists of a shared encoder-decoder network and six modal-specific\n(speech/text) pre/post-nets. After preprocessing the speech/text input through\nthe pre-nets, the shared encoder-decoder network models the sequence to\nsequence transformation, and then the post-nets generate the output in the\nspeech/text modality based on the decoder output. Particularly, SpeechT5 can\npre-train on a large scale of unlabeled speech and text data to improve the\ncapability of the speech and textual modeling. To align the textual and speech\ninformation into a unified semantic space, we propose a cross-modal vector\nquantization method with random mixing-up to bridge speech and text. Extensive\nevaluations on a wide variety of spoken language processing tasks, including\nvoice conversion, automatic speech recognition, text to speech, and speaker\nidentification, show the superiority of the proposed SpeechT5 framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ao_J/0/1/0/all/0/1\">Junyi Ao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_L/0/1/0/all/0/1\">Long Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_S/0/1/0/all/0/1\">Shujie Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ren_S/0/1/0/all/0/1\">Shuo Ren</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_Y/0/1/0/all/0/1\">Yu Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ko_T/0/1/0/all/0/1\">Tom Ko</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Q/0/1/0/all/0/1\">Qing Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wei_Z/0/1/0/all/0/1\">Zhihua Wei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qian_Y/0/1/0/all/0/1\">Yao Qian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Jinyu Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Dual-Attention Neural Network for Pun Location and Using Pun-Gloss Pairs for Interpretation. (arXiv:2110.07209v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07209","description":"<p>Pun location is to identify the punning word (usually a word or a phrase that\nmakes the text ambiguous) in a given short text, and pun interpretation is to\nfind out two different meanings of the punning word. Most previous studies\nadopt limited word senses obtained by WSD(Word Sense Disambiguation) technique\nor pronunciation information in isolation to address pun location. For the task\nof pun interpretation, related work pays attention to various WSD algorithms.\nIn this paper, a model called DANN (Dual-Attentive Neural Network) is proposed\nfor pun location, effectively integrates word senses and pronunciation with\ncontext information to address two kinds of pun at the same time. Furthermore,\nwe treat pun interpretation as a classification task and construct pungloss\npairs as processing data to solve this task. Experiments on the two benchmark\ndatasets show that our proposed methods achieve new state-of-the-art results.\nOur source code is available in the public code repository.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1\">Meirong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1\">Hao Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jianchao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuanbin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_M/0/1/0/all/0/1\">Man Lan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improve Cross-lingual Voice Cloning Using Low-quality Code-switched Data. (arXiv:2110.07210v1 [cs.SD])","link":"http://arxiv.org/abs/2110.07210","description":"<p>Recently, sequence-to-sequence (seq-to-seq) models have been successfully\napplied in text-to-speech (TTS) to synthesize speech for single-language text.\nTo synthesize speech for multiple languages usually requires multi-lingual\nspeech from the target speaker. However, it is both laborious and expensive to\ncollect high-quality multi-lingual TTS data for the target speakers. In this\npaper, we proposed to use low-quality code-switched found data from the\nnon-target speakers to achieve cross-lingual voice cloning for the target\nspeakers. Experiments show that our proposed method can generate high-quality\ncode-switched speech in the target voices in terms of both naturalness and\nspeaker consistency. More importantly, we find that our method can achieve a\ncomparable result to the state-of-the-art (SOTA) performance in cross-lingual\nvoice cloning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haitong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yue Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Causal Transformers Perform Below Chance on Recursive Nested Constructions, Unlike Humans. (arXiv:2110.07240v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07240","description":"<p>Recursive processing is considered a hallmark of human linguistic abilities.\nA recent study evaluated recursive processing in recurrent neural language\nmodels (RNN-LMs) and showed that such models perform below chance level on\nembedded dependencies within nested constructions -- a prototypical example of\nrecursion in natural language. Here, we study if state-of-the-art Transformer\nLMs do any better. We test four different Transformer LMs on two different\ntypes of nested constructions, which differ in whether the embedded (inner)\ndependency is short or long range. We find that Transformers achieve\nnear-perfect performance on short-range embedded dependencies, significantly\nbetter than previous results reported for RNN-LMs and humans. However, on\nlong-range embedded dependencies, Transformers' performance sharply drops below\nchance level. Remarkably, the addition of only three words to the embedded\ndependency caused Transformers to fall from near-perfect to below-chance\nperformance. Taken together, our results reveal Transformers' shortcoming when\nit comes to recursive, structure-based, processing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lakretz_Y/0/1/0/all/0/1\">Yair Lakretz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Desbordes_T/0/1/0/all/0/1\">Th&#xe9;o Desbordes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hupkes_D/0/1/0/all/0/1\">Dieuwke Hupkes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dehaene_S/0/1/0/all/0/1\">Stanislas Dehaene</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Building Chinese Biomedical Language Models via Multi-Level Text Discrimination. (arXiv:2110.07244v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07244","description":"<p>Pre-trained language models (PLMs), such as BERT and GPT, have revolutionized\nthe field of NLP, not only in the general domain but also in the biomedical\ndomain. Most prior efforts in building biomedical PLMs have resorted simply to\ndomain adaptation and focused mainly on English. In this work we introduce\neHealth, a biomedical PLM in Chinese built with a new pre-training framework.\nThis new framework trains eHealth as a discriminator through both token-level\nand sequence-level discrimination. The former is to detect input tokens\ncorrupted by a generator and select their original signals from plausible\ncandidates, while the latter is to further distinguish corruptions of a same\noriginal sequence from those of the others. As such, eHealth can learn language\nsemantics at both the token and sequence levels. Extensive experiments on 11\nChinese biomedical language understanding tasks of various forms verify the\neffectiveness and superiority of our approach. The pre-trained model is\navailable to the public at\n\\url{https://github.com/PaddlePaddle/Research/tree/master/KG/eHealth} and the\ncode will also be released later.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Quan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_S/0/1/0/all/0/1\">Songtai Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Benfeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Y/0/1/0/all/0/1\">Yajuan Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yong Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haifeng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Approach to Mispronunciation Detection and Diagnosis with Acoustic, Phonetic and Linguistic (APL) Embeddings. (arXiv:2110.07274v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07274","description":"<p>Many mispronunciation detection and diagnosis (MD&amp;D) research approaches try\nto exploit both the acoustic and linguistic features as input. Yet the\nimprovement of the performance is limited, partially due to the shortage of\nlarge amount annotated training data at the phoneme level. Phonetic embeddings,\nextracted from ASR models trained with huge amount of word level annotations,\ncan serve as a good representation of the content of input speech, in a\nnoise-robust and speaker-independent manner. These embeddings, when used as\nimplicit phonetic supplementary information, can alleviate the data shortage of\nexplicit phoneme annotations. We propose to utilize Acoustic, Phonetic and\nLinguistic (APL) embedding features jointly for building a more powerful MD\\&amp;D\nsystem. Experimental results obtained on the L2-ARCTIC database show the\nproposed approach outperforms the baseline by 9.93%, 10.13% and 6.17% on the\ndetection accuracy, diagnosis error rate and the F-measure, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_W/0/1/0/all/0/1\">Wenxuan Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_S/0/1/0/all/0/1\">Shaoguang Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soong_F/0/1/0/all/0/1\">Frank Soong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wenshan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1\">Yan Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tien_J/0/1/0/all/0/1\">Jonathan Tien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhiyong Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"P-Adapters: Robustly Extracting Factual Information from Language Models with Diverse Prompts. (arXiv:2110.07280v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07280","description":"<p>Recent work (e.g. LAMA (Petroni et al., 2019)) has found that the quality of\nthe factual information extracted from Large Language Models (LLMs) depends on\nthe prompts used to query them. This inconsistency is problematic because\ndifferent users will query LLMs for the same information using different\nwording, but should receive the same, accurate responses regardless. In this\nwork we aim to address this shortcoming by introducing P-Adapters: lightweight\nmodels that sit between the embedding layer and first attention layer of LLMs.\nThey take LLM embeddings as input and output continuous prompts that are used\nto query the LLM. Additionally, we investigate Mixture of Experts (MoE) models\nthat learn a set of continuous prompts (\"experts\") and select one to query the\nLLM. They require a separate classifier trained on human-annotated data to map\nnatural language prompts to the continuous ones. P-Adapters perform comparably\nto the more complex MoE models in extracting factual information from BERT and\nRoBERTa while eliminating the need for additional annotations. P-Adapters show\nbetween 12-26% absolute improvement in precision and 36-50% absolute\nimprovement in consistency over a baseline of only using natural language\nqueries. Finally, we investigate what makes a P-adapter successful and conclude\nthat access to the LLM's embeddings of the original natural language prompt,\nparticularly the subject of the entity pair being asked about, is a significant\nfactor.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Newman_B/0/1/0/all/0/1\">Benjamin Newman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choubey_P/0/1/0/all/0/1\">Prafulla Kumar Choubey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajani_N/0/1/0/all/0/1\">Nazneen Rajani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LFPT5: A Unified Framework for Lifelong Few-shot Language Learning Based on Prompt Tuning of T5. (arXiv:2110.07298v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07298","description":"<p>Existing approaches to lifelong language learning rely on plenty of labeled\ndata for learning a new task, which is hard to obtain in most real scenarios.\nConsidering that humans can continually learn new tasks from a handful of\nexamples, we expect the models also to be able to generalize well on new\nfew-shot tasks without forgetting the previous ones. In this work, we define\nthis more challenging yet practical problem as Lifelong Few-shot Language\nLearning (LFLL) and propose a unified framework for it based on prompt tuning\nof T5. Our framework called LFPT5 takes full advantage of PT's strong few-shot\nlearning ability, and simultaneously trains the model as a task solver and a\ndata generator. Before learning a new domain of the same task type, LFPT5\ngenerates pseudo (labeled) samples of previously learned domains, and later\ngets trained on those samples to alleviate forgetting of previous knowledge as\nit learns the new domain. In addition, a KL divergence loss is minimized to\nachieve label consistency between the previous and the current model. While\nadapting to a new task type, LFPT5 includes and tunes additional prompt\nembeddings for the new task. With extensive experiments, we demonstrate that\nLFPT5 can be applied to various different types of tasks and significantly\noutperform previous methods in different LFLL settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qin_C/0/1/0/all/0/1\">Chengwei Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Aspect-Sentiment-Multiple-Opinion Triplet Extraction. (arXiv:2110.07303v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07303","description":"<p>Aspect Sentiment Triplet Extraction (ASTE) aims to extract aspect term\n(aspect), sentiment and opinion term (opinion) triplets from sentences and can\ntell a complete story, i.e., the discussed aspect, the sentiment toward the\naspect, and the cause of the sentiment. ASTE is a charming task, however, one\ntriplet extracted by ASTE only includes one opinion of the aspect, but an\naspect in a sentence may have multiple corresponding opinions and one opinion\nonly provides part of the reason why the aspect has this sentiment, as a\nconsequence, some triplets extracted by ASTE are hard to understand, and\nprovide erroneous information for downstream tasks. In this paper, we introduce\na new task, named Aspect Sentiment Multiple Opinions Triplet Extraction\n(ASMOTE). ASMOTE aims to extract aspect, sentiment and multiple opinions\ntriplets. Specifically, one triplet extracted by ASMOTE contains all opinions\nabout the aspect and can tell the exact reason that the aspect has the\nsentiment. We propose an Aspect-Guided Framework (AGF) to address this task.\nAGF first extracts aspects, then predicts their opinions and sentiments.\nMoreover, with the help of the proposed Sequence Labeling Attention(SLA), AGF\nimproves the performance of the sentiment classification using the extracted\nopinions. Experimental results on multiple datasets demonstrate the\neffectiveness of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuncong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_S/0/1/0/all/0/1\">Sheng-hua Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_C/0/1/0/all/0/1\">Cunxiang Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yancheng He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Empirical Investigation of Multi-bridge Multilingual NMT models. (arXiv:2110.07304v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07304","description":"<p>In this paper, we present an extensive investigation of multi-bridge,\nmany-to-many multilingual NMT models (MB-M2M) ie., models trained on\nnon-English language pairs in addition to English-centric language pairs. In\naddition to validating previous work which shows that MB-M2M models can\novercome zeroshot translation problems, our analysis reveals the following\nresults about multibridge models: (1) it is possible to extract a reasonable\namount of parallel corpora between non-English languages for low-resource\nlanguages (2) with limited non-English centric data, MB-M2M models are\ncompetitive with or outperform pivot models, (3) MB-M2M models can outperform\nEnglish-Any models and perform at par with Any-English models, so a single\nmultilingual NMT system can serve all translation directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kunchukuttan_A/0/1/0/all/0/1\">Anoop Kunchukuttan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Solving Aspect Category Sentiment Analysis as a Text Generation Task. (arXiv:2110.07310v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07310","description":"<p>Aspect category sentiment analysis has attracted increasing research\nattention. The dominant methods make use of pre-trained language models by\nlearning effective aspect category-specific representations, and adding\nspecific output layers to its pre-trained representation. We consider a more\ndirect way of making use of pre-trained language models, by casting the ACSA\ntasks into natural language generation tasks, using natural language sentences\nto represent the output. Our method allows more direct use of pre-trained\nknowledge in seq2seq language models by directly following the task setting\nduring pre-training. Experiments on several benchmarks show that our method\ngives the best reported results, having large advantages in few-shot and\nzero-shot settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teng_Z/0/1/0/all/0/1\">Zhiyang Teng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1\">Leyang Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hanmeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WMDecompose: A Framework for Leveraging the Interpretable Properties of Word Mover's Distance in Sociocultural Analysis. (arXiv:2110.07330v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07330","description":"<p>Despite the increasing popularity of NLP in the humanities and social\nsciences, advances in model performance and complexity have been accompanied by\nconcerns about interpretability and explanatory power for sociocultural\nanalysis. One popular model that balances complexity and legibility is Word\nMover's Distance (WMD). Ostensibly adapted for its interpretability, WMD has\nnonetheless been used and further developed in ways which frequently discard\nits most interpretable aspect: namely, the word-level distances required for\ntranslating a set of words into another set of words. To address this apparent\ngap, we introduce WMDecompose: a model and Python library that 1) decomposes\ndocument-level distances into their constituent word-level distances, and 2)\nsubsequently clusters words to induce thematic elements, such that useful\nlexical information is retained and summarized for analysis. To illustrate its\npotential in a social scientific context, we apply it to a longitudinal social\nmedia corpus to explore the interrelationship between conspiracy theories and\nconservative American discourses. Finally, because of the full WMD model's high\ntime-complexity, we additionally suggest a method of sampling document pairs\nfrom large datasets in a reproducible way, with tight bounds that prevent\nextrapolation of unreliable results due to poor sampling practices.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Brunila_M/0/1/0/all/0/1\">Mikael Brunila</a>, <a href=\"http://arxiv.org/find/cs/1/au:+LaViolette_J/0/1/0/all/0/1\">Jack LaViolette</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Plug-Tagger: A Pluggable Sequence Labeling Framework Using Language Models. (arXiv:2110.07331v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07331","description":"<p>Plug-and-play functionality allows deep learning models to adapt well to\ndifferent tasks without requiring any parameters modified. Recently,\nprefix-tuning was shown to be a plug-and-play method on various text generation\ntasks by simply inserting corresponding continuous vectors into the inputs.\nHowever, sequence labeling tasks invalidate existing plug-and-play methods\nsince different label sets demand changes to the architecture of the model\nclassifier. In this work, we propose the use of label word prediction instead\nof classification to totally reuse the architecture of pre-trained models for\nsequence labeling tasks. Specifically, for each task, a label word set is first\nconstructed by selecting a high-frequency word for each class respectively, and\nthen, task-specific vectors are inserted into the inputs and optimized to\nmanipulate the model predictions towards the corresponding label words. As a\nresult, by simply switching the plugin vectors on the input, a frozen\npre-trained language model is allowed to perform different tasks. Experimental\nresults on three sequence labeling tasks show that the performance of the\nproposed method can achieve comparable performance with standard fine-tuning\nwith only 0.1\\% task-specific parameters. In addition, our method is up to 70\ntimes faster than non-plug-and-play methods while switching different tasks\nunder the resource-constrained scenario.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_R/0/1/0/all/0/1\">Ruotian Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gui_T/0/1/0/all/0/1\">Tao Gui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1\">Yiding Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Legal Question Answering Systems. (arXiv:2110.07333v1 [cs.IR])","link":"http://arxiv.org/abs/2110.07333","description":"<p>Many legal professionals think that the explosion of information about local,\nregional, national, and international legislation makes their practice more\ncostly, time-consuming, and even error-prone. The two main reasons for this are\nthat most legislation is usually unstructured, and the tremendous amount and\npace with which laws are released causes information overload in their daily\ntasks. In the case of the legal domain, the research community agrees that a\nsystem allowing to generate automatic responses to legal questions could\nsubstantially impact many practical implications in daily activities. The\ndegree of usefulness is such that even a semi-automatic solution could\nsignificantly help to reduce the workload to be faced. This is mainly because a\nQuestion Answering system could be able to automatically process a massive\namount of legal resources to answer a question or doubt in seconds, which means\nthat it could save resources in the form of effort, money, and time to many\nprofessionals in the legal sector. In this work, we quantitatively and\nqualitatively survey the solutions that currently exist to meet this challenge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Martinez_Gil_J/0/1/0/all/0/1\">Jorge Martinez-Gil</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FILM: Following Instructions in Language with Modular Methods. (arXiv:2110.07342v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07342","description":"<p>Recent methods for embodied instruction following are typically trained\nend-to-end using imitation learning. This requires the use of expert\ntrajectories and low-level language instructions. Such approaches assume\nlearned hidden states will simultaneously integrate semantics from the language\nand vision to perform state tracking, spatial memory, exploration, and\nlong-term planning. In contrast, we propose a modular method with structured\nrepresentations that (1) builds a semantic map of the scene, and (2) performs\nexploration with a semantic search policy, to achieve the natural language\ngoal. Our modular method achieves SOTA performance (24.46%) with a substantial\n(8.17 % absolute) gap from previous work while using less data by eschewing\nboth expert trajectories and low-level instructions. Leveraging low-level\nlanguage, however, can further increase our performance (26.49%). Our findings\nsuggest that an explicit spatial memory and a semantic search policy can\nprovide a stronger and more general representation for state-tracking and\nguidance, even in the absence of expert trajectories or low-level instructions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Min_S/0/1/0/all/0/1\">So Yeon Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaplot_D/0/1/0/all/0/1\">Devendra Singh Chaplot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravikumar_P/0/1/0/all/0/1\">Pradeep Ravikumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bisk_Y/0/1/0/all/0/1\">Yonatan Bisk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1\">Ruslan Salakhutdinov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Music Playlist Title Generation: A Machine-Translation Approach. (arXiv:2110.07354v1 [cs.LG])","link":"http://arxiv.org/abs/2110.07354","description":"<p>We propose a machine-translation approach to automatically generate a\nplaylist title from a set of music tracks. We take a sequence of track IDs as\ninput and a sequence of words in a playlist title as output, adapting the\nsequence-to-sequence framework based on Recurrent Neural Network (RNN) and\nTransformer to the music data. Considering the orderless nature of music tracks\nin a playlist, we propose two techniques that remove the order of the input\nsequence. One is data augmentation by shuffling and the other is deleting the\npositional encoding. We also reorganize the existing music playlist datasets to\ngenerate phrase-level playlist titles. The result shows that the Transformer\nmodels generally outperform the RNN model. Also, removing the order of input\nsequence improves the performance further.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Doh_S/0/1/0/all/0/1\">SeungHeon Doh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Junwon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nam_J/0/1/0/all/0/1\">Juhan Nam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Medically Aware GPT-3 as a Data Generator for Medical Dialogue Summarization. (arXiv:2110.07356v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07356","description":"<p>In medical dialogue summarization, summaries must be coherent and must\ncapture all the medically relevant information in the dialogue. However,\nlearning effective models for summarization require large amounts of labeled\ndata which is especially hard to obtain. We present an algorithm to create\nsynthetic training data with an explicit focus on capturing medically relevant\ninformation. We utilize GPT-3 as the backbone of our algorithm and scale 210\nhuman labeled examples to yield results comparable to using 6400 human labeled\nexamples (~30x) leveraging low-shot learning and an ensemble method. In\ndetailed experiments, we show that this approach produces high quality training\ndata that can further be combined with human labeled data to get summaries that\nare strongly preferable to those produced by models trained on human data alone\nboth in terms of medical accuracy and coherency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chintagunta_B/0/1/0/all/0/1\">Bharath Chintagunta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katariya_N/0/1/0/all/0/1\">Namit Katariya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amatriain_X/0/1/0/all/0/1\">Xavier Amatriain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kannan_A/0/1/0/all/0/1\">Anitha Kannan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Text Mining of COVID-19 Records. (arXiv:2110.07357v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07357","description":"<p>Since the beginning of coronavirus, the disease has spread worldwide and\ndrastically changed many aspects of the human's lifestyle. Twitter as a\npowerful tool can help researchers measure public health in response to\nCOVID-19. According to the high volume of data production on social networks,\nautomated text mining approaches can help search, read and summarize helpful\ninformation. This paper preprocessed the existing medical dataset regarding\nCOVID-19 named CORD-19 and annotated the dataset for supervised classification\ntasks. At this time of the COVID-19 pandemic, we made a preprocessed dataset\nfor the research community. This may contribute towards finding new solutions\nfor some social interventions that COVID-19 has made. The preprocessed version\nof the mentioned dataset is publicly available through Github.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zamini_M/0/1/0/all/0/1\">Mohamad Zamini</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Memory-Based Semantic Parsing. (arXiv:2110.07358v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07358","description":"<p>We present a memory-based model for context-dependent semantic parsing.\nPrevious approaches focus on enabling the decoder to copy or modify the parse\nfrom the previous utterance, assuming there is a dependency between the current\nand previous parses. In this work, we propose to represent contextual\ninformation using an external memory. We learn a context memory controller that\nmanages the memory by maintaining the cumulative meaning of sequential user\nutterances. We evaluate our approach on three semantic parsing benchmarks.\nExperimental results show that our model can better process context-dependent\ninformation and demonstrates improved performance without using task-specific\ndecoders.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jain_P/0/1/0/all/0/1\">Parag Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lapata_M/0/1/0/all/0/1\">Mirella Lapata</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RocketQAv2: A Joint Training Method for Dense Passage Retrieval and Passage Re-ranking. (arXiv:2110.07367v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07367","description":"<p>In various natural language processing tasks, passage retrieval and passage\nre-ranking are two key procedures in finding and ranking relevant information.\nSince both the two procedures contribute to the final performance, it is\nimportant to jointly optimize them in order to achieve mutual improvement. In\nthis paper, we propose a novel joint training approach for dense passage\nretrieval and passage re-ranking. A major contribution is that we introduce the\ndynamic listwise distillation, where we design a unified listwise training\napproach for both the retriever and the re-ranker. During the dynamic\ndistillation, the retriever and the re-ranker can be adaptively improved\naccording to each other's relevance information. We also propose a hybrid data\naugmentation strategy to construct diverse training instances for listwise\ntraining approach. Extensive experiments show the effectiveness of our approach\non both MSMARCO and Natural Questions datasets. Our code is available at\nhttps://github.com/PaddlePaddle/RocketQA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ren_R/0/1/0/all/0/1\">Ruiyang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_Y/0/1/0/all/0/1\">Yingqi Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+She_Q/0/1/0/all/0/1\">Qiaoqiao She</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haifeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transferring Semantic Knowledge Into Language Encoders. (arXiv:2110.07382v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07382","description":"<p>We introduce semantic form mid-tuning, an approach for transferring semantic\nknowledge from semantic meaning representations into transformer-based language\nencoders. In mid-tuning, we learn to align the text of general sentences -- not\ntied to any particular inference task -- and structured semantic\nrepresentations of those sentences. Our approach does not require gold\nannotated semantic representations. Instead, it makes use of automatically\ngenerated semantic representations, such as from off-the-shelf PropBank and\nFrameNet semantic parsers. We show that this alignment can be learned\nimplicitly via classification or directly via triplet loss. Our method yields\nlanguage encoders that demonstrate improved predictive performance across\ninference, reading comprehension, textual similarity, and other semantic tasks\ndrawn from the GLUE, SuperGLUE, and SentEval benchmarks. We evaluate our\napproach on three popular baseline models, where our experimental results and\nanalysis concludes that current pre-trained language models can further benefit\nfrom structured semantic frames with the proposed mid-tuning method, as they\ninject additional task-agnostic knowledge to the encoder, improving the\ngenerated embeddings as well as the linguistic properties of the given model,\nas evident from improvements on a popular sentence embedding toolkit and a\nvariety of probing tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Umair_M/0/1/0/all/0/1\">Mohammad Umair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferraro_F/0/1/0/all/0/1\">Francis Ferraro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Neglected Sibling: Isotropic Gaussian Posterior for VAE. (arXiv:2110.07383v1 [cs.LG])","link":"http://arxiv.org/abs/2110.07383","description":"<p>Deep generative models have been widely used in several areas of NLP, and\nvarious techniques have been proposed to augment them or address their training\nchallenges. In this paper, we propose a simple modification to Variational\nAutoencoders (VAEs) by using an Isotropic Gaussian Posterior (IGP) that allows\nfor better utilisation of their latent representation space. This model avoids\nthe sub-optimal behavior of VAEs related to inactive dimensions in the\nrepresentation space. We provide both theoretical analysis, and empirical\nevidence on various datasets and tasks that show IGP leads to consistent\nimprovement on several quantitative and qualitative grounds, from downstream\ntask performance and sample efficiency to robustness. Additionally, we give\ninsights about the representational properties encouraged by IGP and also show\nthat its gain generalises to image domain as well.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buntine_W/0/1/0/all/0/1\">Wray Buntine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shareghi_E/0/1/0/all/0/1\">Ehsan Shareghi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-shot Controllable Style Transfer for Low-Resource Settings: A Study in Indian Languages. (arXiv:2110.07385v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07385","description":"<p>Style transfer is the task of rewriting an input sentence into a target style\nwhile approximately preserving its content. While most prior literature assumes\naccess to large style-labelled corpora, recent work (Riley et al. 2021) has\nattempted \"few-shot\" style transfer using only 3-10 sentences at inference for\nextracting the target style. In this work we consider one such low resource\nsetting where no datasets are available: style transfer for Indian languages.\nWe find that existing few-shot methods perform this task poorly, with a strong\ntendency to copy inputs verbatim. We push the state-of-the-art for few-shot\nstyle transfer with a new method modeling the stylistic difference between\nparaphrases. When compared to prior work using automatic and human evaluations,\nour model achieves 2-3x better performance and output diversity in formality\ntransfer and code-mixing addition across five Indian languages. Moreover, our\nmethod is better able to control the amount of style transfer using an input\nscalar knob. We report promising qualitative results for several attribute\ntransfer directions, including sentiment transfer, text simplification, gender\nneutralization and text anonymization, all without retraining the model.\nFinally we found model evaluation to be difficult due to the lack of evaluation\ndatasets and metrics for Indian languages. To facilitate further research in\nformality transfer for Indic languages, we crowdsource annotations for 4000\nsentence pairs in four languages, and use this dataset to design our automatic\nevaluation suite.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Krishna_K/0/1/0/all/0/1\">Kalpesh Krishna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nathani_D/0/1/0/all/0/1\">Deepak Nathani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_X/0/1/0/all/0/1\">Xavier Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samanta_B/0/1/0/all/0/1\">Bidisha Samanta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talukdar_P/0/1/0/all/0/1\">Partha Talukdar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Off-the-Shelf Machine Listening and Natural Language Models for Automated Audio Captioning. (arXiv:2110.07410v1 [cs.LG])","link":"http://arxiv.org/abs/2110.07410","description":"<p>Automated audio captioning (AAC) is the task of automatically generating\ntextual descriptions for general audio signals. A captioning system has to\nidentify various information from the input signal and express it with natural\nlanguage. Existing works mainly focus on investigating new methods and try to\nimprove their performance measured on existing datasets. Having attracted\nattention only recently, very few works on AAC study the performance of\nexisting pre-trained audio and natural language processing resources. In this\npaper, we evaluate the performance of off-the-shelf models with a\nTransformer-based captioning approach. We utilize the freely available Clotho\ndataset to compare four different pre-trained machine listening models, four\nword embedding models, and their combinations in many different settings. Our\nevaluation suggests that YAMNet combined with BERT embeddings produces the best\ncaptions. Moreover, in general, fine-tuning pre-trained word embeddings can\nlead to better performance. Finally, we show that sequences of audio embeddings\ncan be processed using a Transformer encoder to produce higher-quality\ncaptions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Weck_B/0/1/0/all/0/1\">Benno Weck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Favory_X/0/1/0/all/0/1\">Xavier Favory</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drossos_K/0/1/0/all/0/1\">Konstantinos Drossos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Serra_X/0/1/0/all/0/1\">Xavier Serra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Simple, Strong and Robust Baseline for Distantly Supervised Relation Extraction. (arXiv:2110.07415v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07415","description":"<p>Distantly supervised relation extraction (DS-RE) is generally framed as a\nmulti-instance multi-label (MI-ML) task, where the optimal aggregation of\ninformation from multiple instances is of key importance. Intra-bag attention\n(Lin et al., 2016) is an example of a popularly used aggregation scheme for\nthis framework. Apart from this scheme, however, there is not much to choose\nfrom in the DS-RE literature as most of the advances in this field are focused\non improving the instance-encoding step rather than the instance-aggregation\nstep. With recent works leveraging large pre-trained language models as\nencoders, the increased capacity of models might allow for more flexibility in\nthe instance-aggregation step. In this work, we explore this hypothesis and\ncome up with a novel aggregation scheme which we call Passage-Att. Under this\naggregation scheme, we combine all instances mentioning an entity pair into a\n\"passage of instances\", which is summarized independently for each relation\nclass. These summaries are used to predict the validity of a potential triple.\nWe show that our Passage-Att with BERT as passage encoder achieves\nstate-of-the-art performance in three different settings (monolingual DS,\nmonolingual DS with manually-annotated test set, multilingual DS).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rathore_V/0/1/0/all/0/1\">Vipul Rathore</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Badola_K/0/1/0/all/0/1\">Kartikeya Badola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mausam/0/1/0/all/0/1\">Mausam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singla_P/0/1/0/all/0/1\">Parag Singla</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Modeling of Social Concepts Evoked by Art Images as Multimodal Frames. (arXiv:2110.07420v1 [cs.CV])","link":"http://arxiv.org/abs/2110.07420","description":"<p>Social concepts referring to non-physical objects--such as revolution,\nviolence, or friendship--are powerful tools to describe, index, and query the\ncontent of visual data, including ever-growing collections of art images from\nthe Cultural Heritage (CH) field. While much progress has been made towards\ncomplete image understanding in computer vision, automatic detection of social\nconcepts evoked by images is still a challenge. This is partly due to the\nwell-known semantic gap problem, worsened for social concepts given their lack\nof unique physical features, and reliance on more unspecific features than\nconcrete concepts. In this paper, we propose the translation of recent\ncognitive theories about social concept representation into a software approach\nto represent them as multimodal frames, by integrating multisensory data. Our\nmethod focuses on the extraction, analysis, and integration of multimodal\nfeatures from visual art material tagged with the concepts of interest. We\ndefine a conceptual model and present a novel ontology for formally\nrepresenting social concepts as multimodal frames. Taking the Tate Gallery's\ncollection as an empirical basis, we experiment our method on a corpus of art\nimages to provide a proof of concept of its potential. We discuss further\ndirections of research, and provide all software, data sources, and results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pandiani_D/0/1/0/all/0/1\">Delfina Sol Martinez Pandiani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Presutti_V/0/1/0/all/0/1\">Valentina Presutti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Model Robustness to User-generated Noisy Texts. (arXiv:2110.07428v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07428","description":"<p>Sensitivity of deep-neural models to input noise is known to be a challenging\nproblem. In NLP, model performance often deteriorates with naturally occurring\nnoise, such as spelling errors. To mitigate this issue, models may leverage\nartificially noised data. However, the amount and type of generated noise has\nso far been determined arbitrarily. We therefore propose to model the errors\nstatistically from grammatical-error-correction corpora. We present a thorough\nevaluation of several state-of-the-art NLP systems' robustness in multiple\nlanguages, with tasks including morpho-syntactic analysis, named entity\nrecognition, neural machine translation, a subset of the GLUE benchmark and\nreading comprehension. We also compare two approaches to address the\nperformance drop: a) training the NLP models with noised data generated by our\nframework; and b) reducing the input noise with external system for natural\nlanguage correction. The code is released at https://github.com/ufal/kazitext.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Naplava_J/0/1/0/all/0/1\">Jakub N&#xe1;plava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Popel_M/0/1/0/all/0/1\">Martin Popel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Straka_M/0/1/0/all/0/1\">Milan Straka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strakova_J/0/1/0/all/0/1\">Jana Strakov&#xe1;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards More Effective and Economic Sparsely-Activated Model. (arXiv:2110.07431v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07431","description":"<p>The sparsely-activated models have achieved great success in natural language\nprocessing through large-scale parameters and relatively low computational\ncost, and gradually become a feasible technique for training and implementing\nextremely large models. Due to the limit of communication cost, activating\nmultiple experts is hardly affordable during training and inference. Therefore,\nprevious work usually activate just one expert at a time to alleviate\nadditional communication cost. Such routing mechanism limits the upper bound of\nmodel performance. In this paper, we first investigate a phenomenon that\nincreasing the number of activated experts can boost the model performance with\nhigher sparse ratio. To increase the number of activated experts without an\nincrease in computational cost, we propose SAM (Switch and Mixture) routing, an\nefficient hierarchical routing mechanism that activates multiple experts in a\nsame device (GPU). Our methods shed light on the training of extremely large\nsparse models and experiments prove that our models can achieve significant\nperformance gain with great efficiency improvement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Hao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_K/0/1/0/all/0/1\">Ke Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_J/0/1/0/all/0/1\">Jianwei Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yongkang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_Z/0/1/0/all/0/1\">Zhaoye Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_Z/0/1/0/all/0/1\">Zhicheng Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zikai Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_R/0/1/0/all/0/1\">Ruofei Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiawen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_E/0/1/0/all/0/1\">Enrui Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yinxia Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1\">Yantao Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Zhao Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Designing Language Technologies for Social Good: The Road not Taken. (arXiv:2110.07444v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07444","description":"<p>Development of speech and language technology for social good (LT4SG),\nespecially those targeted at the welfare of marginalized communities and\nspeakers of low-resource and under-served languages, has been a prominent theme\nof research within NLP, Speech, and the AI communities. Researchers have mostly\nrelied on their individual expertise, experiences or ad hoc surveys for\nprioritization of language technologies that provide social good to the\nend-users. This has been criticized by several scholars who argue that work on\nLT4SG must include the target linguistic communities during the design and\ndevelopment process. However, none of the LT4SG work and their critiques\nsuggest principled techniques for prioritization of the technologies and\nmethods for inclusion of the end-user during the development cycle. Drawing\ninspiration from the fields of Economics, Ethics, Psychology, and Participatory\nDesign, here we chart out a set of methodologies for prioritizing LT4SG that\nare aligned with the end-user preferences. We then analyze several LT4SG\nefforts in light of the proposed methodologies and bring out their hidden\nassumptions and potential pitfalls. While the current study is limited to\nlanguage technologies, we believe that the principles and prioritization\ntechniques highlighted here are applicable more broadly to AI for Social Good.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mukhija_N/0/1/0/all/0/1\">Namrata Mukhija</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choudhury_M/0/1/0/all/0/1\">Monojit Choudhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bali_K/0/1/0/all/0/1\">Kalika Bali</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MReD: A Meta-Review Dataset for Controllable Text Generation. (arXiv:2110.07474v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07474","description":"<p>When directly using existing text generation datasets for controllable\ngeneration, we are facing the problem of not having the domain knowledge and\nthus the aspects that could be controlled are limited.A typical example is when\nusing CNN/Daily Mail dataset for controllable text summarization, there is no\nguided information on the emphasis of summary sentences. A more useful text\ngenerator should leverage both the input text and control variables to guide\nthe generation, which can only be built with deep understanding of the domain\nknowledge. Motivated by this vi-sion, our paper introduces a new text\ngeneration dataset, named MReD. Our new dataset consists of 7,089 meta-reviews\nand all its 45k meta-review sentences are manually annotated as one of the\ncarefully defined 9 categories, including abstract, strength, decision, etc. We\npresent experimental results on start-of-the-art summarization models, and\npropose methods for controlled generation on both extractive and abstractive\nmodels using our annotated data. By exploring various settings and analaysing\nthe model behavior with respect to the control inputs, we demonstrate the\nchallenges and values of our dataset. MReD allows us to have a better\nunderstanding of the meta-review corpora and enlarge the research room for\ncontrollable text generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chenhui Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Liying Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_R/0/1/0/all/0/1\">Ran Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bing_L/0/1/0/all/0/1\">Lidong Bing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1\">Yang You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Query and Extract: Refining Event Extraction as Type-oriented Binary Decoding. (arXiv:2110.07476v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07476","description":"<p>Event extraction is typically modeled as a multi-class classification problem\nwhere both event types and argument roles are treated as atomic symbols. These\napproaches are usually limited to a set of pre-defined types. We propose a\nnovel event extraction framework that takes event types and argument roles as\nnatural language queries to extract candidate triggers and arguments from the\ninput text. With the rich semantics in the queries, our framework benefits from\nthe attention mechanisms to better capture the semantic correlation between the\nevent types or argument roles and the input text. Furthermore, the\nquery-and-extract formulation allows our approach to leverage all available\nevent annotations from various ontologies as a unified model. Experiments on\ntwo public benchmarks, ACE and ERE, demonstrate that our approach achieves\nstate-of-the-art performance on each dataset and significantly outperforms\nexisting methods on zero-shot event extraction. We will make all the programs\npublicly available once the paper is accepted.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sijia Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1\">Mo Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shiyu Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Lichao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Lifu Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Finetuning Large-Scale Pre-trained Language Models for Conversational Recommendation with Knowledge Graph. (arXiv:2110.07477v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07477","description":"<p>In this paper, we present a pre-trained language model (PLM) based framework\ncalled RID for conversational recommender system (CRS). RID finetunes the\nlarge-scale PLMs such as DialoGPT, together with a pre-trained Relational Graph\nConvolutional Network (RGCN) to encode the node representations of an\nitem-oriented knowledge graph. The former aims to generate fluent and diverse\ndialogue responses based on the strong language generation ability of PLMs,\nwhile the latter is to facilitate the item recommendation by learning better\nnode embeddings on the structural knowledge base. To unify two modules of\ndialogue generation and item recommendation into a PLMs-based framework, we\nexpand the generation vocabulary of PLMs to include an extra item vocabulary,\nand introduces a vocabulary pointer to control when to recommend target items\nin the generation process. Extensive experiments on the benchmark dataset\nReDial show RID significantly outperforms the state-of-the-art methods on both\nevaluations of dialogue and recommendation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lingzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Huang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sha_L/0/1/0/all/0/1\">Lei Sha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Can Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_K/0/1/0/all/0/1\">Kam-Fai Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fusing Heterogeneous Factors with Triaffine Mechanism for Nested Named Entity Recognition. (arXiv:2110.07480v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07480","description":"<p>Nested entities are observed in many domains due to their compositionality,\nwhich cannot be easily recognized by the widely-used sequence labeling\nframework. A natural solution is to treat the task as a span classification\nproblem. To increase performance on span representation and classification, it\nis crucial to effectively integrate all useful information of different\nformats, which we refer to heterogeneous factors including tokens, labels,\nboundaries, and related spans. To fuse these heterogeneous factors, we propose\na novel triaffine mechanism including triaffine attention and scoring, which\ninteracts with multiple factors in both the stages of representation and\nclassification. Experiments results show that our proposed method achieves the\nstate-of-the-art F1 scores on four nested NER datasets: ACE2004, ACE2005,\nGENIA, and KBP2017.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zheng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Songfang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Pitfalls of Analyzing Individual Neurons in Language Models. (arXiv:2110.07483v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07483","description":"<p>While many studies have shown that linguistic information is encoded in\nhidden word representations, few have studied individual neurons, to show how\nand in which neurons it is encoded. Among these, the common approach is to use\nan external probe to rank neurons according to their relevance to some\nlinguistic attribute, and to evaluate the obtained ranking using the same probe\nthat produced it. We show two pitfalls in this methodology: 1. It confounds\ndistinct factors: probe quality and ranking quality. We separate them and draw\nconclusions on each. 2. It focuses on encoded information, rather than\ninformation that is used by the model. We show that these are not the same. We\ncompare two recent ranking methods and a simple one we introduce, and evaluate\nthem with regard to both of these aspects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Antverg_O/0/1/0/all/0/1\">Omer Antverg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belinkov_Y/0/1/0/all/0/1\">Yonatan Belinkov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-end Keyword Spotting using Xception-1d. (arXiv:2110.07498v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07498","description":"<p>The field of conversational agents is growing fast and there is an increasing\nneed for algorithms that enhance natural interaction. In this work we show how\nwe achieved state of the art results in the Keyword Spotting field by adapting\nand tweaking the Xception algorithm, which achieved outstanding results in\nseveral computer vision tasks. We obtained about 96\\% accuracy when classifying\naudio clips belonging to 35 different categories, beating human annotation at\nthe most complex tasks proposed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Valles_Perez_I/0/1/0/all/0/1\">Iv&#xe1;n Vall&#xe9;s-P&#xe9;rez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomez_Sanchis_J/0/1/0/all/0/1\">Juan G&#xf3;mez-Sanchis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martinez_Sober_M/0/1/0/all/0/1\">Marcelino Mart&#xed;nez-Sober</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vila_Frances_J/0/1/0/all/0/1\">Joan Vila-Franc&#xe9;s</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Serrano_Lopez_A/0/1/0/all/0/1\">Antonio J. Serrano-L&#xf3;pez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soria_Olivas_E/0/1/0/all/0/1\">Emilio Soria-Olivas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Non-Autoregressive Translation with Layer-Wise Prediction and Deep Supervision. (arXiv:2110.07515v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07515","description":"<p>How do we perform efficient inference while retaining high translation\nquality? Existing neural machine translation models, such as Transformer,\nachieve high performance, but they decode words one by one, which is\ninefficient. Recent non-autoregressive translation models speed up the\ninference, but their quality is still inferior. In this work, we propose DSLP,\na highly efficient and high-performance model for machine translation. The key\ninsight is to train a non-autoregressive Transformer with Deep Supervision and\nfeed additional Layer-wise Predictions. We conducted extensive experiments on\nfour translation tasks (both directions of WMT'14 EN-DE and WMT'16 EN-RO).\nResults show that our approach consistently improves the BLEU scores compared\nwith respective base models. Specifically, our best variant outperforms the\nautoregressive model on three translation tasks, while being 14.8 times more\nefficient in inference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chenyang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaiane_O/0/1/0/all/0/1\">Osmar R. Za&#xef;ane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mou_L/0/1/0/all/0/1\">Lili Mou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SaFeRDialogues: Taking Feedback Gracefully after Conversational Safety Failures. (arXiv:2110.07518v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07518","description":"<p>Current open-domain conversational models can easily be made to talk in\ninadequate ways. Online learning from conversational feedback given by the\nconversation partner is a promising avenue for a model to improve and adapt, so\nas to generate fewer of these safety failures. However, current\nstate-of-the-art models tend to react to feedback with defensive or oblivious\nresponses. This makes for an unpleasant experience and may discourage\nconversation partners from giving feedback in the future. This work proposes\nSaFeRDialogues, a task and dataset of graceful responses to conversational\nfeedback about safety failures. We collect a dataset of 10k dialogues\ndemonstrating safety failures, feedback signaling them, and a response\nacknowledging the feedback. We show how fine-tuning on this dataset results in\nconversations that human raters deem considerably more likely to lead to a\ncivil conversation, without sacrificing engagingness or general conversational\nability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ung_M/0/1/0/all/0/1\">Megan Ung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boureau_Y/0/1/0/all/0/1\">Y-Lan Boureau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comparative Opinion Summarization via Collaborative Decoding. (arXiv:2110.07520v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07520","description":"<p>Opinion summarization focuses on generating summaries that reflect popular\nopinions of multiple reviews for a single entity (e.g., a hotel or a product.)\nWhile generated summaries offer general and concise information about a\nparticular entity, the information may be insufficient to help the user compare\nmultiple entities. Thus, the user may still struggle with the question \"Which\none should I pick?\" In this paper, we propose a {\\em comparative opinion\nsummarization} task, which is to generate two contrastive summaries and one\ncommon summary from two given sets of reviews from different entities. We\ndevelop a comparative summarization framework CoCoSum, which consists of two\nfew-shot summarization models that are jointly used to generate contrastive and\ncommon summaries. Experimental results on a newly created benchmark CoCoTrip\nshow that CoCoSum can produce high-quality contrastive and common summaries\nthan state-of-the-art opinion summarization models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Iso_H/0/1/0/all/0/1\">Hayate Iso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaolan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suhara_Y/0/1/0/all/0/1\">Yoshihiko Suhara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Representation Decoupling for Open-Domain Passage Retrieval. (arXiv:2110.07524v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07524","description":"<p>Training dense passage representations via contrastive learning (CL) has been\nshown effective for Open-Domain Passage Retrieval (ODPR). Recent studies mainly\nfocus on optimizing this CL framework by improving the sampling strategy or\nextra pretraining. Different from previous studies, this work devotes itself to\ninvestigating the influence of conflicts in the widely used CL strategy in\nODPR, motivated by our observation that a passage can be organized by multiple\nsemantically different sentences, thus modeling such a passage as a unified\ndense vector is not optimal. We call such conflicts Contrastive Conflicts. In\nthis work, we propose to solve it with a representation decoupling method, by\ndecoupling the passage representations into contextual sentence-level ones, and\ndesign specific CL strategies to mediate these conflicts. Experiments on widely\nused datasets including Natural Questions, Trivia QA, and SQuAD verify the\neffectiveness of our method, especially on the dataset where the conflicting\nproblem is severe. Our method also presents good transferability across the\ndatasets, which further supports our idea of mediating Contrastive Conflicts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Bohong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhuosheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jinyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Irrationality of Neural Rationale Models. (arXiv:2110.07550v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07550","description":"<p>Neural rationale models are popular for interpretable predictions of NLP\ntasks. In these, a selector extracts segments of the input text, called\nrationales, and passes these segments to a classifier for prediction. Since the\nrationale is the only information accessible to the classifier, it is plausibly\ndefined as the explanation. Is such a characterization unconditionally correct?\nIn this paper, we argue to the contrary, with both philosophical perspectives\nand empirical evidence suggesting that rationale models are, perhaps, less\nrational and interpretable than expected. We call for more rigorous and\ncomprehensive evaluations of these models to ensure desired properties of\ninterpretability are indeed achieved. The code can be found at\nhttps://github.com/yimingz89/Neural-Rationale-Analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yiming Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Booth_S/0/1/0/all/0/1\">Serena Booth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_J/0/1/0/all/0/1\">Julie Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yilun Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BI-RADS BERT & Using Section Tokenization to Understand Radiology Reports. (arXiv:2110.07552v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07552","description":"<p>Radiology reports are the main form of communication between radiologists and\nother clinicians, and contain important information for patient care. However\nin order to use this information for research it is necessary to convert the\nraw text into structured data suitable for analysis. Domain specific contextual\nword embeddings have been shown to achieve impressive accuracy at such natural\nlanguage processing tasks in medicine. In this work we pre-trained a contextual\nembedding BERT model using breast radiology reports and developed a classifier\nthat incorporated the embedding with auxiliary global textual features in order\nto perform a section tokenization task. This model achieved a 98% accuracy at\nsegregating free text reports into sections of information outlined in the\nBreast Imaging Reporting and Data System (BI-RADS) lexicon, a significant\nimprovement over the Classic BERT model without auxiliary information. We then\nevaluated whether using section tokenization improved the downstream extraction\nof the following fields: modality/procedure, previous cancer, menopausal\nstatus, purpose of exam, breast density and background parenchymal enhancement.\nUsing the BERT model pre-trained on breast radiology reports combined with\nsection tokenization resulted in an overall accuracy of 95.9% in field\nextraction. This is a 17% improvement compared to an overall accuracy of 78.9%\nfor field extraction for models without section tokenization and with Classic\nBERT embeddings. Our work shows the strength of using BERT in radiology report\nanalysis and the advantages of section tokenization in identifying key features\nof patient factors recorded in breast radiology reports.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kuling_G/0/1/0/all/0/1\">Grey Kuling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Curpen_D/0/1/0/all/0/1\">Dr. Belinda Curpen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martel_A/0/1/0/all/0/1\">Anne L. Martel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Composable Sparse Fine-Tuning for Cross-Lingual Transfer. (arXiv:2110.07560v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07560","description":"<p>Fine-tuning all parameters of a pre-trained model has become the mainstream\napproach for transfer learning. To increase its efficiency and prevent\ncatastrophic forgetting and interference, techniques like adapters and sparse\nfine-tuning have been developed. Adapters are modular, as they can be combined\nto adapt a model towards different facets of knowledge (e.g., dedicated\nlanguage and/or task adapters). Sparse fine-tuning is expressive, as it\ncontrols the behavior of all model components. In this work, we introduce a new\nfine-tuning method with both these desirable properties. In particular, we\nlearn sparse, real-valued masks based on a simple variant of the Lottery Ticket\nHypothesis. Task-specific masks are obtained from annotated data in a source\nlanguage, and language-specific masks from masked language modeling in a target\nlanguage. Both these masks can then be composed with the pre-trained model.\nUnlike adapter-based fine-tuning, this method neither increases the number of\nparameters at inference time nor alters the original model architecture. Most\nimportantly, it outperforms adapters in zero-shot cross-lingual transfer by a\nlarge margin in a series of multilingual benchmarks, including Universal\nDependencies, MasakhaNER, and AmericasNLI. Based on an in-depth analysis, we\nadditionally find that sparsity is crucial to prevent both 1) interference\nbetween the fine-tunings to be composed and 2) overfitting. We release the code\nand models at https://github.com/cambridgeltl/composable-sft.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ansell_A/0/1/0/all/0/1\">Alan Ansell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ponti_E/0/1/0/all/0/1\">Edoardo Maria Ponti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korhonen_A/0/1/0/all/0/1\">Anna Korhonen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vulic_I/0/1/0/all/0/1\">Ivan Vuli&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Practical Benefits of Feature Feedback Under Distribution Shift. (arXiv:2110.07566v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07566","description":"<p>In attempts to develop sample-efficient algorithms, researcher have explored\nmyriad mechanisms for collecting and exploiting feature feedback, auxiliary\nannotations provided for training (but not test) instances that highlight\nsalient evidence. Examples include bounding boxes around objects and salient\nspans in text. Despite its intuitive appeal, feature feedback has not delivered\nsignificant gains in practical problems as assessed on iid holdout sets.\nHowever, recent works on counterfactually augmented data suggest an alternative\nbenefit of supplemental annotations: lessening sensitivity to spurious patterns\nand consequently delivering gains in out-of-domain evaluations. Inspired by\nthese findings, we hypothesize that while the numerous existing methods for\nincorporating feature feedback have delivered negligible in-sample gains, they\nmay nevertheless generalize better out-of-domain. In experiments addressing\nsentiment analysis, we show that feature feedback methods perform significantly\nbetter on various natural out-of-domain datasets even absent differences on\nin-domain evaluation. By contrast, on natural language inference tasks,\nperformance remains comparable. Finally, we compare those tasks where feature\nfeedback does (and does not) help.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Katakkar_A/0/1/0/all/0/1\">Anurag Katakkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiqin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_C/0/1/0/all/0/1\">Clay H. Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipton_Z/0/1/0/all/0/1\">Zachary C. Lipton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaushik_D/0/1/0/all/0/1\">Divyansh Kaushik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LAGr: Labeling Aligned Graphs for Improving Systematic Generalization in Semantic Parsing. (arXiv:2110.07572v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07572","description":"<p>Semantic parsing is the task of producing a structured meaning representation\nfor natural language utterances or questions. Recent research has pointed out\nthat the commonly-used sequence-to-sequence (seq2seq) semantic parsers struggle\nto generalize systematically, i.e. to handle examples that require recombining\nknown knowledge in novel settings. In this work, we show that better systematic\ngeneralization can be achieved by producing the meaning representation (MR)\ndirectly as a graph and not as a sequence. To this end we propose LAGr, the\nLabeling Aligned Graphs algorithm that produces semantic parses by predicting\nnode and edge labels for a complete multi-layer input-aligned graph. The\nstrongly-supervised LAGr algorithm requires aligned graphs as inputs, whereas\nweakly-supervised LAGr infers alignments for originally unaligned target graphs\nusing an approximate MAP inference procedure. On the COGS and CFQ compositional\ngeneralization benchmarks the strongly- and weakly- supervised LAGr algorithms\nachieve significant improvements upon the baseline seq2seq parsers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jambor_D/0/1/0/all/0/1\">Dora Jambor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahdanau_D/0/1/0/all/0/1\">Dzmitry Bahdanau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Delphi: Towards Machine Ethics and Norms. (arXiv:2110.07574v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07574","description":"<p>What would it take to teach a machine to behave ethically? While broad\nethical rules may seem straightforward to state (\"thou shalt not kill\"),\napplying such rules to real-world situations is far more complex. For example,\nwhile \"helping a friend\" is generally a good thing to do, \"helping a friend\nspread fake news\" is not. We identify four underlying challenges towards\nmachine ethics and norms: (1) an understanding of moral precepts and social\nnorms; (2) the ability to perceive real-world situations visually or by reading\nnatural language descriptions; (3) commonsense reasoning to anticipate the\noutcome of alternative actions in different contexts; (4) most importantly, the\nability to make ethical judgments given the interplay between competing values\nand their grounding in different contexts (e.g., the right to freedom of\nexpression vs. preventing the spread of fake news).\n</p>\n<p>Our paper begins to address these questions within the deep learning\nparadigm. Our prototype model, Delphi, demonstrates strong promise of\nlanguage-based commonsense moral reasoning, with up to 92.1% accuracy vetted by\nhumans. This is in stark contrast to the zero-shot performance of GPT-3 of\n52.3%, which suggests that massive scale alone does not endow pre-trained\nneural language models with human values. Thus, we present Commonsense Norm\nBank, a moral textbook customized for machines, which compiles 1.7M examples of\npeople's ethical judgments on a broad spectrum of everyday situations. In\naddition to the new resources and baseline performances for future research,\nour study provides new insights that lead to several important open research\nquestions: differentiating between universal human values and personal values,\nmodeling different moral frameworks, and explainable, consistent approaches to\nmachine ethics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Liwei Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1\">Jena D. Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhagavatula_C/0/1/0/all/0/1\">Chandra Bhagavatula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bras_R/0/1/0/all/0/1\">Ronan Le Bras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Forbes_M/0/1/0/all/0/1\">Maxwell Forbes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borchardt_J/0/1/0/all/0/1\">Jon Borchardt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jenny Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Etzioni_O/0/1/0/all/0/1\">Oren Etzioni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sap_M/0/1/0/all/0/1\">Maarten Sap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spoken ObjectNet: A Bias-Controlled Spoken Caption Dataset. (arXiv:2110.07575v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07575","description":"<p>Visually-grounded spoken language datasets can enable models to learn\ncross-modal correspondences with very weak supervision. However, modern\naudio-visual datasets contain biases that undermine the real-world performance\nof models trained on that data. We introduce Spoken ObjectNet, which is\ndesigned to remove some of these biases and provide a way to better evaluate\nhow effectively models will perform in real-world scenarios. This dataset\nexpands upon ObjectNet, which is a bias-controlled image dataset that features\nsimilar image classes to those present in ImageNet. We detail our data\ncollection pipeline, which features several methods to improve caption quality,\nincluding automated language model checks. Lastly, we show baseline results on\nimage retrieval and audio retrieval tasks. These results show that models\ntrained on other datasets and then evaluated on Spoken ObjectNet tend to\nperform poorly due to biases in other datasets that the models have learned. We\nalso show evidence that the performance decrease is due to the dataset\ncontrols, and not the transfer setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Palmer_I/0/1/0/all/0/1\">Ian Palmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rouditchenko_A/0/1/0/all/0/1\">Andrew Rouditchenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barbu_A/0/1/0/all/0/1\">Andrei Barbu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katz_B/0/1/0/all/0/1\">Boris Katz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glass_J/0/1/0/all/0/1\">James Glass</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UniPELT: A Unified Framework for Parameter-Efficient Language Model Tuning. (arXiv:2110.07577v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07577","description":"<p>Conventional fine-tuning of pre-trained language models tunes all model\nparameters and stores a full model copy for each downstream task, which has\nbecome increasingly infeasible as the model size grows larger. Recent\nparameter-efficient language model tuning (PELT) methods manage to match the\nperformance of fine-tuning with much fewer trainable parameters and perform\nespecially well when the training data is limited. However, different PELT\nmethods may perform rather differently on the same task, making it nontrivial\nto select the most appropriate method for a specific task, especially\nconsidering the fast-growing number of new PELT methods and downstream tasks.\nIn light of model diversity and the difficulty of model selection, we propose a\nunified framework, UniPELT, which incorporates different PELT methods as\nsubmodules and learns to activate the ones that best suit the current data or\ntask setup. Remarkably, on the GLUE benchmark, UniPELT consistently achieves\n1~3pt gains compared to the best individual PELT method that it incorporates\nand even outperforms fine-tuning under different setups. Moreover, UniPELT\noften surpasses the upper bound when taking the best performance of all its\nsubmodules used individually on each task, indicating that a mixture of\nmultiple PELT methods may be inherently more effective than single methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1\">Yuning Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathias_L/0/1/0/all/0/1\">Lambert Mathias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_R/0/1/0/all/0/1\">Rui Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Almahairi_A/0/1/0/all/0/1\">Amjad Almahairi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1\">Hao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yih_W/0/1/0/all/0/1\">Wen-tau Yih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khabsa_M/0/1/0/all/0/1\">Madian Khabsa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-Shot Dense Retrieval with Momentum Adversarial Domain Invariant Representations. (arXiv:2110.07581v1 [cs.IR])","link":"http://arxiv.org/abs/2110.07581","description":"<p>Dense retrieval (DR) methods conduct text retrieval by first encoding texts\nin the embedding space and then matching them by nearest neighbor search. This\nrequires strong locality properties from the representation space, i.e, the\nclose allocations of each small group of relevant texts, which are hard to\ngeneralize to domains without sufficient training data. In this paper, we aim\nto improve the generalization ability of DR models from source training domains\nwith rich supervision signals to target domains without any relevant labels, in\nthe zero-shot setting. To achieve that, we propose Momentum adversarial Domain\nInvariant Representation learning (MoDIR), which introduces a momentum method\nin the DR training process to train a domain classifier distinguishing source\nversus target, and then adversarially updates the DR encoder to learn domain\ninvariant representations. Our experiments show that MoDIR robustly outperforms\nits baselines on 10+ ranking datasets from the BEIR benchmark in the zero-shot\nsetup, with more than 10% relative gains on datasets with enough sensitivity\nfor DR models' evaluation. Source code of this paper will be released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xin_J/0/1/0/all/0/1\">Ji Xin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Chenyan Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_A/0/1/0/all/0/1\">Ashwin Srinivasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Ankita Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jose_D/0/1/0/all/0/1\">Damien Jose</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bennett_P/0/1/0/all/0/1\">Paul N. Bennett</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Explanations Be Useful for Calibrating Black Box Models?. (arXiv:2110.07586v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07586","description":"<p>One often wants to take an existing, trained NLP model and use it on data\nfrom a new domain. While fine-tuning or few-shot learning can be used to adapt\nthe base model, there is no one simple recipe to getting these working;\nmoreover, one may not have access to the original model weights if it is\ndeployed as a black box. To this end, we study how to improve a black box\nmodel's performance on a new domain given examples from the new domain by\nleveraging explanations of the model's behavior. Our approach first extracts a\nset of features combining human intuition about the task with model\nattributions generated by black box interpretation techniques, and then uses a\nsimple model to calibrate or rerank the model's predictions based on the\nfeatures. We experiment with our method on two tasks, extractive question\nanswering and natural language inference, covering adaptation from several\npairs of domains. The experimental results across all the domain pairs show\nthat explanations are useful for calibrating these models. We show that the\ncalibration features transfer to some extent between tasks and shed light on\nhow to effectively use them.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1\">Xi Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrett_G/0/1/0/all/0/1\">Greg Durrett</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Speech Toxicity Analysis: A New Spoken Language Processing Task. (arXiv:2110.07592v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07592","description":"<p>Toxic speech, also known as hate speech, is regarded as one of the crucial\nissues plaguing online social media today. Most recent work on toxic speech\ndetection is constrained to the modality of text with no existing work on\ntoxicity detection from spoken utterances. In this paper, we propose a new\nSpoken Language Processing task of detecting toxicity from spoken speech. We\nintroduce DeToxy, the first publicly available toxicity annotated dataset for\nEnglish speech, sourced from various openly available speech databases,\nconsisting of over 2 million utterances. Finally, we also provide analysis on\nhow a spoken speech corpus annotated for toxicity can help facilitate the\ndevelopment of E2E models which better capture various prosodic cues in speech,\nthereby boosting toxicity classification on spoken utterances.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Sreyan Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lepcha_S/0/1/0/all/0/1\">Samden Lepcha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sakshi_S/0/1/0/all/0/1\">S Sakshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1\">Rajiv Ratn Shah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Compressibility of Distributed Document Representations. (arXiv:2110.07595v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07595","description":"<p>Contemporary natural language processing (NLP) revolves around learning from\nlatent document representations, generated either implicitly by neural language\nmodels or explicitly by methods such as doc2vec or similar. One of the key\nproperties of the obtained representations is their dimension. Whilst the\ncommonly adopted dimensions of 256 and 768 offer sufficient performance on many\ntasks, it is many times unclear whether the default dimension is the most\nsuitable choice for the subsequent downstream learning tasks. Furthermore,\nrepresentation dimensions are seldom subject to hyperparameter tuning due to\ncomputational constraints. The purpose of this paper is to demonstrate that a\nsurprisingly simple and efficient recursive compression procedure can be\nsufficient to both significantly compress the initial representation, but also\npotentially improve its performance when considering the task of text\nclassification. Having smaller and less noisy representations is the desired\nproperty during deployment, as orders of magnitude smaller models can\nsignificantly reduce the computational overload and with it the deployment\ncosts. We propose CoRe, a straightforward, representation learner-agnostic\nframework suitable for representation compression. The CoRe's performance is\nshowcased and studied on a collection of 17 real-life corpora from biomedical,\nnews, social media, and literary domains. We explored CoRe's behavior when\nconsidering contextual and non-contextual document representations, different\ncompression levels, and 9 different compression algorithms. Current results\nbased on more than 100,000 compression experiments indicate that recursive\nSingular Value Decomposition offers a very good trade-off between the\ncompression efficiency and performance, making CoRe useful in many existing,\nrepresentation-dependent NLP pipelines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Skrlj_B/0/1/0/all/0/1\">Bla&#x17e; &#x160;krlj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petkovic_M/0/1/0/all/0/1\">Matej Petkovi&#x10d;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Retrieval-guided Counterfactual Generation for QA. (arXiv:2110.07596v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07596","description":"<p>Deep NLP models have been shown to learn spurious correlations, leaving them\nbrittle to input perturbations. Recent work has shown that counterfactual or\ncontrastive data -- i.e. minimally perturbed inputs -- can reveal these\nweaknesses, and that data augmentation using counterfactuals can help\nameliorate them. Proposed techniques for generating counterfactuals rely on\nhuman annotations, perturbations based on simple heuristics, and meaning\nrepresentation frameworks. We focus on the task of creating counterfactuals for\nquestion answering, which presents unique challenges related to world\nknowledge, semantic diversity, and answerability. To address these challenges,\nwe develop a Retrieve-Generate-Filter(RGF) technique to create counterfactual\nevaluation and training data with minimal human supervision. Using an\nopen-domain QA framework and question generation model trained on original task\ndata, we create counterfactuals that are fluent, semantically diverse, and\nautomatically labeled. Data augmentation with RGF counterfactuals improves\nperformance on out-of-domain and challenging evaluation sets over and above\nexisting methods, in both the reading comprehension and open-domain QA\nsettings. Moreover, we find that RGF data leads to significant improvements in\na model's robustness to local perturbations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Paranjape_B/0/1/0/all/0/1\">Bhargavi Paranjape</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lamm_M/0/1/0/all/0/1\">Matthew Lamm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tenney_I/0/1/0/all/0/1\">Ian Tenney</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks. (arXiv:2110.07602v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07602","description":"<p>Prompt tuning, which only tunes continuous prompts with a frozen language\nmodel, substantially reduces per-task storage and memory usage at training.\nHowever, in the context of NLU, prior work and our results reveal that existing\nmethods of prompt tuning do not perform well for normal-sized pre-trained\nmodels and for hard sequence tasks, indicating lack of universality. We present\na novel empirical finding that properly-optimized prompt tuning can be\nuniversally effective across a wide range of model scales and NLU tasks, where\nit matches the performance of fine-tuning while having only 0.1\\%-3\\% tuned\nparameters. Our method P-Tuning v2 is not a new method but a version of\nprefix-tuning \\cite{li2021prefix} optimized and adapted for NLU. Given the\nuniversality and simplicity of P-Tuning v2, we believe it can serve as an\nalternative for fine-tuning and a strong baseline for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_K/0/1/0/all/0/1\">Kaixuan Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yicheng Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Z/0/1/0/all/0/1\">Zhengxiao Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhilin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jie Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sub-word Level Lip Reading With Visual Attention. (arXiv:2110.07603v1 [cs.CV])","link":"http://arxiv.org/abs/2110.07603","description":"<p>The goal of this paper is to learn strong lip reading models that can\nrecognise speech in silent videos. Most prior works deal with the open-set\nvisual speech recognition problem by adapting existing automatic speech\nrecognition techniques on top of trivially pooled visual features. Instead, in\nthis paper we focus on the unique challenges encountered in lip reading and\npropose tailored solutions. To that end we make the following contributions:\n(1) we propose an attention-based pooling mechanism to aggregate visual speech\nrepresentations; (2) we use sub-word units for lip reading for the first time\nand show that this allows us to better model the ambiguities of the task; (3)\nwe propose a training pipeline that balances the lip reading performance with\nother key factors such as data and compute efficiency. Following the above, we\nobtain state-of-the-art results on the challenging LRS2 and LRS3 benchmarks\nwhen training on public datasets, and even surpass models trained on\nlarge-scale industrial datasets by using an order of magnitude less data. Our\nbest model achieves 22.6% word error rate on the LRS2 dataset, a performance\nunprecedented for lip reading models, significantly reducing the performance\ngap between lip reading and automatic speech recognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+R_P/0/1/0/all/0/1\">Prajwal K R</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Afouras_T/0/1/0/all/0/1\">Triantafyllos Afouras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1\">Andrew Zisserman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Political Text Scaling Meets Computational Semantics. (arXiv:1904.06217v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/1904.06217","description":"<p>During the last fifteen years, automatic text scaling has become one of the\nkey tools of the Text as Data community in political science. Prominent text\nscaling algorithms, however, rely on the assumption that latent positions can\nbe captured just by leveraging the information about word frequencies in\ndocuments under study. We challenge this traditional view and present a new,\nsemantically aware text scaling algorithm, SemScale, which combines recent\ndevelopments in the area of computational linguistics with unsupervised\ngraph-based clustering. We conduct an extensive quantitative analysis over a\ncollection of speeches from the European Parliament in five different languages\nand from two different legislative terms, and show that a scaling approach\nrelying on semantic document representations is often better at capturing known\nunderlying political dimensions than the established frequency-based (i.e.,\nsymbolic) scaling method. We further validate our findings through a series of\nexperiments focused on text preprocessing and feature selection, document\nrepresentation, scaling of party manifestos, and a supervised extension of our\nalgorithm. To catalyze further research on this new branch of text scaling\nmethods, we release a Python implementation of SemScale with all included data\nsets and evaluation procedures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nanni_F/0/1/0/all/0/1\">Federico Nanni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glavas_G/0/1/0/all/0/1\">Goran Glavas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rehbein_I/0/1/0/all/0/1\">Ines Rehbein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ponzetto_S/0/1/0/all/0/1\">Simone Paolo Ponzetto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stuckenschmidt_H/0/1/0/all/0/1\">Heiner Stuckenschmidt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BAS: An Answer Selection Method Using BERT Language Model. (arXiv:1911.01528v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/1911.01528","description":"<p>In recent years, Question Answering systems have become more popular and\nwidely used by users. Despite the increasing popularity of these systems, the\ntheir performance is not even sufficient for textual data and requires further\nresearch. These systems consist of several parts that one of them is the Answer\nSelection component. This component detects the most relevant answer from a\nlist of candidate answers. The methods presented in previous researches have\nattempted to provide an independent model to undertake the answer-selection\ntask. An independent model cannot comprehend the syntactic and semantic\nfeatures of questions and answers with a small training dataset. To fill this\ngap, language models can be employed in implementing the answer selection part.\nThis action enables the model to have a better understanding of the language in\norder to understand questions and answers better than previous works. In this\nresearch, we will present the \"BAS\" (BERT Answer Selection) that uses the BERT\nlanguage model to comprehend language. The empirical results of applying the\nmodel on the TrecQA Raw, TrecQA Clean, and WikiQA datasets demonstrate that\nusing a robust language model such as BERT can enhance the performance. Using a\nmore robust classifier also enhances the effect of the language model on the\nanswer selection component. The results demonstrate that language comprehension\nis an essential requirement in natural language processing tasks such as\nanswer-selection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mozafari_J/0/1/0/all/0/1\">Jamshid Mozafari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fatemi_A/0/1/0/all/0/1\">Afsaneh Fatemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nematbakhsh_M/0/1/0/all/0/1\">Mohammad Ali Nematbakhsh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explaining Deep Neural Networks. (arXiv:2010.01496v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.01496","description":"<p>Deep neural networks are becoming more and more popular due to their\nrevolutionary success in diverse areas, such as computer vision, natural\nlanguage processing, and speech recognition. However, the decision-making\nprocesses of these models are generally not interpretable to users. In various\ndomains, such as healthcare, finance, or law, it is critical to know the\nreasons behind a decision made by an artificial intelligence system. Therefore,\nseveral directions for explaining neural models have recently been explored. In\nthis thesis, I investigate two major directions for explaining deep neural\nnetworks. The first direction consists of feature-based post-hoc explanatory\nmethods, that is, methods that aim to explain an already trained and fixed\nmodel (post-hoc), and that provide explanations in terms of input features,\nsuch as tokens for text and superpixels for images (feature-based). The second\ndirection consists of self-explanatory neural models that generate natural\nlanguage explanations, that is, models that have a built-in module that\ngenerates explanations for the predictions of the model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Camburu_O/0/1/0/all/0/1\">Oana-Maria Camburu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Synthetic Data Augmentation for Zero-Shot Cross-Lingual Question Answering. (arXiv:2010.12643v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.12643","description":"<p>Coupled with the availability of large scale datasets, deep learning\narchitectures have enabled rapid progress on the Question Answering task.\nHowever, most of those datasets are in English, and the performances of\nstate-of-the-art multilingual models are significantly lower when evaluated on\nnon-English data. Due to high data collection costs, it is not realistic to\nobtain annotated data for each language one desires to support.\n</p>\n<p>We propose a method to improve the Cross-lingual Question Answering\nperformance without requiring additional annotated data, leveraging Question\nGeneration models to produce synthetic samples in a cross-lingual fashion. We\nshow that the proposed method allows to significantly outperform the baselines\ntrained on English data only. We report a new state-of-the-art on four\nmultilingual datasets: MLQA, XQuAD, SQuAD-it and PIAF (fr).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Riabi_A/0/1/0/all/0/1\">Arij Riabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scialom_T/0/1/0/all/0/1\">Thomas Scialom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keraron_R/0/1/0/all/0/1\">Rachel Keraron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sagot_B/0/1/0/all/0/1\">Beno&#xee;t Sagot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seddah_D/0/1/0/all/0/1\">Djam&#xe9; Seddah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Staiano_J/0/1/0/all/0/1\">Jacopo Staiano</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On-the-Fly Attention Modulation for Neural Generation. (arXiv:2101.00371v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2101.00371","description":"<p>Despite considerable advancements with deep neural language models (LMs),\nneural text generation still suffers from degeneration: the generated text is\nrepetitive, generic, self-contradictory, and often lacks commonsense. Our\nanalyses on sentence-level attention patterns in LMs reveal that neural\ndegeneration may be associated with insufficient learning of task-specific\ncharacteristics by the attention mechanism. This finding motivates on-the-fly\nattention modulation -- a simple but effective method that enables the\ninjection of priors into attention computation during inference. Automatic and\nhuman evaluation results on three text generation benchmarks demonstrate that\nattention modulation helps LMs generate text with enhanced fluency, creativity,\nand commonsense reasoning, in addition to significantly reduce sentence-level\nrepetition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yue Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhagavatula_C/0/1/0/all/0/1\">Chandra Bhagavatula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Ximing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1\">Jena D. Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bosselut_A/0/1/0/all/0/1\">Antoine Bosselut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheung_J/0/1/0/all/0/1\">Jackie Chi Kit Cheung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Characterizing Partisan Political Narrative Frameworks about COVID-19 on Twitter. (arXiv:2103.06960v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.06960","description":"<p>The COVID-19 pandemic is a global crisis that has been testing every society\nand exposing the critical role of local politics in crisis response. In the\nUnited States, there has been a strong partisan divide between the Democratic\nand Republican party's narratives about the pandemic which resulted in\npolarization of individual behaviors and divergent policy adoption across\nregions. As shown in this case, as well as in most major social issues,\nstrongly polarized narrative frameworks facilitate such narratives. To\nunderstand polarization and other social chasms, it is critical to dissect\nthese diverging narratives. Here, taking the Democratic and Republican\npolitical social media posts about the pandemic as a case study, we demonstrate\nthat a combination of computational methods can provide useful insights into\nthe different contexts, framing, and characters and relationships that\nconstruct their narrative frameworks which individual posts source from.\nLeveraging a dataset of tweets from elite politicians in the U.S., we found\nthat the Democrats' narrative tends to be more concerned with the pandemic as\nwell as financial and social support, while the Republicans discuss more about\nother political entities such as China. We then perform an automatic framing\nanalysis to characterize the ways in which they frame their narratives, where\nwe found that the Democrats emphasize the government's role in responding to\nthe pandemic, and the Republicans emphasize the roles of individuals and\nsupport for small businesses. Finally, we present a semantic role analysis that\nuncovers the important characters and relationships in their narratives as well\nas how they facilitate a membership categorization process. Our findings\nconcretely expose the gaps in the \"elusive consensus\" between the two parties.\nOur methodologies may be applied to computationally study narratives in various\ndomains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jing_E/0/1/0/all/0/1\">Elise Jing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahn_Y/0/1/0/all/0/1\">Yong-Yeol Ahn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Misinfo Reaction Frames: Reasoning about Readers' Reactions to News Headlines. (arXiv:2104.08790v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08790","description":"<p>Even to a simple and short news headline, readers react in a multitude of\nways: cognitively (e.g., inferring the writer's intent), emotionally (e.g.,\nfeeling distrust), and behaviorally (e.g., sharing the news with their\nfriends). Such reactions are instantaneous and yet complex, as they rely on\nfactors that go beyond interpreting the factual content the news headline.\nInstead, understanding reactions require pragmatic understanding of the news\nheadline, including broader background knowledge about contentious news topics\nas well as commonsense reasoning about people's intents and emotional\nreactions. We propose Misinfo Reaction Frames, a pragmatic formalism for\nmodeling how readers might react to a news headline cognitively, emotionally,\nand behaviorally. We also introduce a Misinfo Reaction Frames corpus, a dataset\nof over 200k news headline annotated with crowdsourced reactions focusing on\nglobal crises: the Covid-19 pandemic, climate change, and cancer. Empirical\nresults confirm that it is indeed possible to learn the prominent patterns of\nreaders' reactions to news headlines. We also find a potentially positive use\ncase of our model; When we present our model generated inferences to people, we\nfind that the machine inferences can increase readers' trust in real news while\ndecreasing their trust in misinformation. Our work demonstrates the feasibility\nand the importance of pragmatic inferences of news to help enhance AI-guided\nmisinformation detection and mitigation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gabriel_S/0/1/0/all/0/1\">Saadia Gabriel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hallinan_S/0/1/0/all/0/1\">Skyler Hallinan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sap_M/0/1/0/all/0/1\">Maarten Sap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_P/0/1/0/all/0/1\">Pemi Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roesner_F/0/1/0/all/0/1\">Franziska Roesner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1\">Eunsol Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond Voice Activity Detection: Hybrid Audio Segmentation for Direct Speech Translation. (arXiv:2104.11710v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2104.11710","description":"<p>The audio segmentation mismatch between training data and those seen at\nrun-time is a major problem in direct speech translation. Indeed, while systems\nare usually trained on manually segmented corpora, in real use cases they are\noften presented with continuous audio requiring automatic (and sub-optimal)\nsegmentation. After comparing existing techniques (VAD-based, fixed-length and\nhybrid segmentation methods), in this paper we propose enhanced hybrid\nsolutions to produce better results without sacrificing latency. Through\nexperiments on different domains and language pairs, we show that our methods\noutperform all the other techniques, reducing by at least 30% the gap between\nthe traditional VAD-based approach and optimal manual segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gaido_M/0/1/0/all/0/1\">Marco Gaido</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Negri_M/0/1/0/all/0/1\">Matteo Negri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cettolo_M/0/1/0/all/0/1\">Mauro Cettolo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turchi_M/0/1/0/all/0/1\">Marco Turchi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detection of Emotions in Hindi-English Code Mixed Text Data. (arXiv:2105.09226v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.09226","description":"<p>In recent times, we have seen an increased use of text chat for communication\non social networks and smartphones. This particularly involves the use of\nHindi-English code-mixed text which contains words which are not recognized in\nEnglish vocabulary. We have worked on detecting emotions in these mixed data\nand classify the sentences in human emotions which are angry, fear, happy or\nsad. We have used state of the art natural language processing models and\ncompared their performance on the dataset comprising sentences in this mixed\ndata. The dataset was collected and annotated from sources and then used to\ntrain the models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_D/0/1/0/all/0/1\">Divyansh Singh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RobeCzech: Czech RoBERTa, a monolingual contextualized language representation model. (arXiv:2105.11314v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.11314","description":"<p>We present RobeCzech, a monolingual RoBERTa language representation model\ntrained on Czech data. RoBERTa is a robustly optimized Transformer-based\npretraining approach. We show that RobeCzech considerably outperforms\nequally-sized multilingual and Czech-trained contextualized language\nrepresentation models, surpasses current state of the art in all five evaluated\nNLP tasks and reaches state-of-the-art results in four of them. The RobeCzech\nmodel is released publicly at https://hdl.handle.net/11234/1-3691 and\nhttps://huggingface.co/ufal/robeczech-base.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Straka_M/0/1/0/all/0/1\">Milan Straka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naplava_J/0/1/0/all/0/1\">Jakub N&#xe1;plava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strakova_J/0/1/0/all/0/1\">Jana Strakov&#xe1;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samuel_D/0/1/0/all/0/1\">David Samuel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Stable Classifiers by Transferring Unstable Features. (arXiv:2106.07847v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.07847","description":"<p>While unbiased machine learning models are essential for many applications,\nbias is a human-defined concept that can vary across tasks. Given only\ninput-label pairs, algorithms may lack sufficient information to distinguish\nstable (causal) features from unstable (spurious) features. However, related\ntasks often share similar biases -- an observation we may leverage to develop\nstable classifiers in the transfer setting. In this work, we explicitly inform\nthe target classifier about unstable features in the source tasks.\nSpecifically, we derive a representation that encodes the unstable features by\ncontrasting different data environments in the source task. We achieve\nrobustness by clustering data of the target task according to this\nrepresentation and minimizing the worst-case risk across these clusters. We\nevaluate our method on both text and image classifications. Empirical results\ndemonstrate that our algorithm is able to maintain robustness on the target\ntask, outperforming the best baseline by 22.9% in absolute accuracy across 12\ntransfer settings. Our code is available at https://github.com/YujiaBao/Tofu.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bao_Y/0/1/0/all/0/1\">Yujia Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shiyu Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barzilay_R/0/1/0/all/0/1\">Regina Barzilay</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ConveRT for FAQ Answering. (arXiv:2108.00719v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.00719","description":"<p>Knowledgeable FAQ chatbots are a valuable resource to any organization. While\npowerful and efficient retrieval-based models exist for English, it is rarely\nthe case for other languages for which the same amount of training data is not\navailable. In this paper, we propose a novel pre-training procedure to adapt\nConveRT, an English conversational retriever model, to other languages with\nless training data available. We apply it for the first time to the task of\nDutch FAQ answering related to the COVID-19 vaccine. We show it performs better\nthan an open-source alternative in both a low-data regime and a high-data\nregime.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bruyn_M/0/1/0/all/0/1\">Maxime De Bruyn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lotfi_E/0/1/0/all/0/1\">Ehsan Lotfi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buhmann_J/0/1/0/all/0/1\">Jeska Buhmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daelemans_W/0/1/0/all/0/1\">Walter Daelemans</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Monolingual versus Multilingual BERTology for Vietnamese Extractive Multi-Document Summarization. (arXiv:2108.13741v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.13741","description":"<p>Recent researches have demonstrated that BERT shows potential in a wide range\nof natural language processing tasks. It is adopted as an encoder for many\nstate-of-the-art automatic summarizing systems, which achieve excellent\nperformance. However, so far, there is not much work done for Vietnamese. In\nthis paper, we showcase how BERT can be implemented for extractive text\nsummarization in Vietnamese on multi-document. We introduce a novel comparison\nbetween different multilingual and monolingual BERT models. The experiment\nresults indicate that monolingual models produce promising results compared to\nother multilingual models and previous text summarizing models for Vietnamese.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+To_H/0/1/0/all/0/1\">Huy Quoc To</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Kiet Van Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1\">Ngan Luu-Thuy Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1\">Anh Gia-Tuan Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The VoicePrivacy 2020 Challenge: Results and findings. (arXiv:2109.00648v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.00648","description":"<p>This paper presents the results and analyses stemming from the first\nVoicePrivacy 2020 Challenge which focuses on developing anonymization solutions\nfor speech technology. We provide a systematic overview of the challenge design\nwith an analysis of submitted systems and evaluation results. In particular, we\ndescribe the voice anonymization task and datasets used for system development\nand evaluation. Also, we present different attack models and the associated\nobjective and subjective evaluation metrics. We introduce two anonymization\nbaselines and provide a summary description of the anonymization systems\ndeveloped by the challenge participants. We report objective and subjective\nevaluation results for baseline and submitted systems. In addition, we present\nexperimental results for alternative privacy metrics and attack models\ndeveloped as a part of the post-evaluation analysis. Finally, we summarize our\ninsights and observations that will influence the design of the next\nVoicePrivacy challenge edition and some directions for future voice\nanonymization research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tomashenko_N/0/1/0/all/0/1\">Natalia Tomashenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vincent_E/0/1/0/all/0/1\">Emmanuel Vincent</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patino_J/0/1/0/all/0/1\">Jose Patino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_B/0/1/0/all/0/1\">Brij Mohan Lal Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noe_P/0/1/0/all/0/1\">Paul-Gauthier No&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nautsch_A/0/1/0/all/0/1\">Andreas Nautsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Evans_N/0/1/0/all/0/1\">Nicholas Evans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamagishi_J/0/1/0/all/0/1\">Junichi Yamagishi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OBrien_B/0/1/0/all/0/1\">Benjamin O&#x27;Brien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chanclu_A/0/1/0/all/0/1\">Ana&#xef;s Chanclu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bonastre_J/0/1/0/all/0/1\">Jean-Fran&#xe7;ois Bonastre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Todisco_M/0/1/0/all/0/1\">Massimiliano Todisco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maouche_M/0/1/0/all/0/1\">Mohamed Maouche</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Phrase-BERT: Improved Phrase Embeddings from BERT with an Application to Corpus Exploration. (arXiv:2109.06304v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.06304","description":"<p>Phrase representations derived from BERT often do not exhibit complex phrasal\ncompositionality, as the model relies instead on lexical similarity to\ndetermine semantic relatedness. In this paper, we propose a contrastive\nfine-tuning objective that enables BERT to produce more powerful phrase\nembeddings. Our approach (Phrase-BERT) relies on a dataset of diverse phrasal\nparaphrases, which is automatically generated using a paraphrase generation\nmodel, as well as a large-scale dataset of phrases in context mined from the\nBooks3 corpus. Phrase-BERT outperforms baselines across a variety of\nphrase-level similarity tasks, while also demonstrating increased lexical\ndiversity between nearest neighbors in the vector space. Finally, as a case\nstudy, we show that Phrase-BERT embeddings can be easily integrated with a\nsimple autoencoder to build a phrase-based neural topic model that interprets\ntopics as mixtures of words and phrases by performing a nearest neighbor search\nin the embedding space. Crowdsourced evaluations demonstrate that this\nphrase-based topic model produces more coherent and meaningful topics than\nbaseline word and phrase-level topic models, further validating the utility of\nPhrase-BERT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shufan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thompson_L/0/1/0/all/0/1\">Laure Thompson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyyer_M/0/1/0/all/0/1\">Mohit Iyyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NOAHQA: Numerical Reasoning with Interpretable Graph Question Answering Dataset. (arXiv:2109.10604v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.10604","description":"<p>While diverse question answering (QA) datasets have been proposed and\ncontributed significantly to the development of deep learning models for QA\ntasks, the existing datasets fall short in two aspects. First, we lack QA\ndatasets covering complex questions that involve answers as well as the\nreasoning processes to get the answers. As a result, the state-of-the-art QA\nresearch on numerical reasoning still focuses on simple calculations and does\nnot provide the mathematical expressions or evidences justifying the answers.\nSecond, the QA community has contributed much effort to improving the\ninterpretability of QA models. However, these models fail to explicitly show\nthe reasoning process, such as the evidence order for reasoning and the\ninteractions between different pieces of evidence. To address the above\nshortcomings, we introduce NOAHQA, a conversational and bilingual QA dataset\nwith questions requiring numerical reasoning with compound mathematical\nexpressions. With NOAHQA, we develop an interpretable reasoning graph as well\nas the appropriate evaluation metric to measure the answer quality. We evaluate\nthe state-of-the-art QA models trained using existing QA datasets on NOAHQA and\nshow that the best among them can only achieve 55.5 exact match scores, while\nthe human performance is 89.7. We also present a new QA model for generating a\nreasoning graph where the reasoning graph metric still has a large gap compared\nwith that of humans, e.g., 28 scores.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qiyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Sicheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuohang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jing Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_E/0/1/0/all/0/1\">Ee-Peng Lim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Breaking BERT: Understanding its Vulnerabilities for Biomedical Named Entity Recognition through Adversarial Attack. (arXiv:2109.11308v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.11308","description":"<p>Biomedical named entity recognition (NER) is a key task in the extraction of\ninformation from biomedical literature and electronic health records. For this\ntask, both generic and biomedical BERT models are widely used. Robustness of\nthese models is vital for medical applications, such as automated medical\ndecision making. In this paper we investigate the vulnerability of BERT models\nto variation in input data for NER through adversarial attack. Since\nadversarial attack methods for NER are sparse, we propose two black-box methods\nfor NER based on existing methods for classification tasks. Experimental\nresults show that the original as well as the biomedical BERT models are highly\nvulnerable to entity replacement: They can be fooled in 89.2 to 99.4% of the\ncases to mislabel previously correct entities. BERT models are also vulnerable\nto variation in the entity context with 20.2 to 45.0% of entities predicted\ncompletely wrong and another 29.3 to 53.3% of entities predicted wrong\npartially. Often a single change is sufficient to fool the model. BERT models\nseem most vulnerable to changes in the local context of entities. Of the\nbiomedical BERT models, the vulnerability of BioBERT is comparable to the\noriginal BERT model whereas SciBERT is even more vulnerable. Our results chart\nthe vulnerabilities of BERT models for biomedical NER and emphasize the\nimportance of further research into uncovering and reducing these weaknesses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dirkson_A/0/1/0/all/0/1\">Anne Dirkson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verberne_S/0/1/0/all/0/1\">Suzan Verberne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kraaij_W/0/1/0/all/0/1\">Wessel Kraaij</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AES Systems Are Both Overstable And Oversensitive: Explaining Why And Proposing Defenses. (arXiv:2109.11728v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.11728","description":"<p>Deep-learning based Automatic Essay Scoring (AES) systems are being actively\nused by states and language testing agencies alike to evaluate millions of\ncandidates for life-changing decisions ranging from college applications to\nvisa approvals. However, little research has been put to understand and\ninterpret the black-box nature of deep-learning based scoring algorithms.\nPrevious studies indicate that scoring models can be easily fooled. In this\npaper, we explore the reason behind their surprising adversarial brittleness.\nWe utilize recent advances in interpretability to find the extent to which\nfeatures such as coherence, content, vocabulary, and relevance are important\nfor automated scoring mechanisms. We use this to investigate the\noversensitivity i.e., large change in output score with a little change in\ninput essay content) and overstability i.e., little change in output scores\nwith large changes in input essay content) of AES. Our results indicate that\nautoscoring models, despite getting trained as \"end-to-end\" models with rich\ncontextual embeddings such as BERT, behave like bag-of-words models. A few\nwords determine the essay score without the requirement of any context making\nthe model largely overstable. This is in stark contrast to recent probing\nstudies on pre-trained representation learning models, which show that rich\nlinguistic features such as parts-of-speech and morphology are encoded by them.\nFurther, we also find that the models have learnt dataset biases, making them\noversensitive. To deal with these issues, we propose detection-based protection\nmodels that can detect oversensitivity and overstability causing samples with\nhigh accuracies. We find that our proposed models are able to detect unusual\nattribution patterns and flag adversarial samples successfully.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singla_Y/0/1/0/all/0/1\">Yaman Kumar Singla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parekh_S/0/1/0/all/0/1\">Swapnil Parekh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Somesh Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Jessy Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1\">Rajiv Ratn Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Changyou Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual AMR Parsing with Noisy Knowledge Distillation. (arXiv:2109.15196v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.15196","description":"<p>We study multilingual AMR parsing from the perspective of knowledge\ndistillation, where the aim is to learn and improve a multilingual AMR parser\nby using an existing English parser as its teacher. We constrain our\nexploration in a strict multilingual setting: there is but one model to parse\nall different languages including English. We identify that noisy input and\nprecise output are the key to successful distillation. Together with extensive\npre-training, we obtain an AMR parser whose performances surpass all previously\npublished results on four different foreign languages, including German,\nSpanish, Italian, and Chinese, by large margins (up to 18.8 \\textsc{Smatch}\npoints on Chinese and on average 11.3 \\textsc{Smatch} points). Our parser also\nachieves comparable performance on English to the latest state-of-the-art\nEnglish-only parser.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1\">Deng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_J/0/1/0/all/0/1\">Jackie Chun-Sing Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bing_L/0/1/0/all/0/1\">Lidong Bing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_W/0/1/0/all/0/1\">Wai Lam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large-scale ASR Domain Adaptation using Self- and Semi-supervised Learning. (arXiv:2110.00165v4 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2110.00165","description":"<p>Self- and semi-supervised learning methods have been actively investigated to\nreduce labeled training data or enhance the model performance. However, the\napproach mostly focus on in-domain performance for public datasets. In this\nstudy, we utilize the combination of self- and semi-supervised learning methods\nto solve unseen domain adaptation problem in a large-scale production setting\nfor online ASR model. This approach demonstrates that using the source domain\ndata with a small fraction of the target domain data (3%) can recover the\nperformance gap compared to a full data baseline: relative 13.5% WER\nimprovement for target domain data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hwang_D/0/1/0/all/0/1\">Dongseong Hwang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Misra_A/0/1/0/all/0/1\">Ananya Misra</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huo_Z/0/1/0/all/0/1\">Zhouyuan Huo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Siddhartha_N/0/1/0/all/0/1\">Nikhil Siddhartha</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Garg_S/0/1/0/all/0/1\">Shefali Garg</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qiu_D/0/1/0/all/0/1\">David Qiu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sim_K/0/1/0/all/0/1\">Khe Chai Sim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Strohman_T/0/1/0/all/0/1\">Trevor Strohman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Beaufays_F/0/1/0/all/0/1\">Fran&#xe7;oise Beaufays</a>, <a href=\"http://arxiv.org/find/eess/1/au:+He_Y/0/1/0/all/0/1\">Yanzhang He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Speech Technology for Everyone: Automatic Speech Recognition for Non-Native English with Transfer Learning. (arXiv:2110.00678v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2110.00678","description":"<p>To address the performance gap of English ASR models on L2 English speakers,\nwe evaluate fine-tuning of pretrained wav2vec 2.0 models (Baevski et al., 2020;\nXu et al., 2021) on L2-ARCTIC, a non-native English speech corpus (Zhao et al.,\n2018) under different training settings. We compare \\textbf{(a)} models trained\nwith a combination of diverse accents to ones trained with only specific\naccents and \\textbf{(b)} results from different single-accent models. Our\nexperiments demonstrate the promise of developing ASR models for non-native\nEnglish speakers, even with small amounts of L2 training data and even without\na language model. Our models also excel in the zero-shot setting where we train\non multiple L2 datasets and test on a blind L2 test set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Shibano_T/0/1/0/all/0/1\">Toshiko Shibano</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1\">Xinyi Zhang</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Li_M/0/1/0/all/0/1\">Mia Taige Li</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Cho_H/0/1/0/all/0/1\">Haejin Cho</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Sullivan_P/0/1/0/all/0/1\">Peter Sullivan</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Abdul_Mageed_M/0/1/0/all/0/1\">Muhammad Abdul-Mageed</a> (1) ((1) University of British Columbia)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LaoPLM: Pre-trained Language Models for Lao. (arXiv:2110.05896v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.05896","description":"<p>Trained on the large corpus, pre-trained language models (PLMs) can capture\ndifferent levels of concepts in context and hence generate universal language\nrepresentations. They can benefit multiple downstream natural language\nprocessing (NLP) tasks. Although PTMs have been widely used in most NLP\napplications, especially for high-resource languages such as English, it is\nunder-represented in Lao NLP research. Previous work on Lao has been hampered\nby the lack of annotated datasets and the sparsity of language resources. In\nthis work, we construct a text classification dataset to alleviate the\nresource-scare situation of the Lao language. We additionally present the first\ntransformer-based PTMs for Lao with four versions: BERT-small, BERT-base,\nELECTRA-small and ELECTRA-base, and evaluate it over two downstream tasks:\npart-of-speech tagging and text classification. Experiments demonstrate the\neffectiveness of our Lao models. We will release our models and datasets to the\ncommunity, hoping to facilitate the future development of Lao NLP applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_N/0/1/0/all/0/1\">Nankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yingwen Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chuwei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Ziyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Shengyi Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Time Masking for Temporal Language Models. (arXiv:2110.06366v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.06366","description":"<p>Our world is constantly evolving, and so is the content on the web.\nConsequently, our languages, often said to mirror the world, are dynamic in\nnature. However, most current contextual language models are static and cannot\nadapt to changes over time. In this work, we propose a temporal contextual\nlanguage model called TempoBERT, which uses time as an additional context of\ntexts. Our technique is based on modifying texts with temporal information and\nperforming time masking - specific masking for the supplementary time\ninformation. We leverage our approach for the tasks of semantic change\ndetection and sentence time prediction, experimenting on diverse datasets in\nterms of time, size, genre, and language. Our extensive evaluation shows that\nboth tasks benefit from exploiting time masking.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rosin_G/0/1/0/all/0/1\">Guy D. Rosin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guy_I/0/1/0/all/0/1\">Ido Guy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radinsky_K/0/1/0/all/0/1\">Kira Radinsky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mengzi: Towards Lightweight yet Ingenious Pre-trained Models for Chinese. (arXiv:2110.06696v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.06696","description":"<p>Although pre-trained models (PLMs) have achieved remarkable improvements in a\nwide range of NLP tasks, they are expensive in terms of time and resources.\nThis calls for the study of training more efficient models with less\ncomputation but still ensures impressive performance. Instead of pursuing a\nlarger scale, we are committed to developing lightweight yet more powerful\nmodels trained with equal or less computation and friendly to rapid deployment.\nThis technical report releases our pre-trained model called Mengzi, which\nstands for a family of discriminative, generative, domain-specific, and\nmultimodal pre-trained model variants, capable of a wide range of language and\nvision tasks. Compared with public Chinese PLMs, Mengzi is simple but more\npowerful. Our lightweight model has achieved new state-of-the-art results on\nthe widely-used CLUE benchmark with our optimized pre-training and fine-tuning\ntechniques. Without modifying the model architecture, our model can be easily\nemployed as an alternative to existing PLMs. Our sources are available at\nhttps://github.com/Langboat/Mengzi.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhuosheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hanqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Keming Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yuhang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_J/0/1/0/all/0/1\">Jingyun Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yulong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Ming Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-10-14T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Considering user agreement in learning to predict the aesthetic quality. (arXiv:2110.06956v1 [cs.CV])","link":"http://arxiv.org/abs/2110.06956","description":"<p>How to robustly rank the aesthetic quality of given images has been a\nlong-standing ill-posed topic. Such challenge stems mainly from the diverse\nsubjective opinions of different observers about the varied types of content.\nThere is a growing interest in estimating the user agreement by considering the\nstandard deviation of the scores, instead of only predicting the mean aesthetic\nopinion score. Nevertheless, when comparing a pair of contents, few studies\nconsider how confident are we regarding the difference in the aesthetic scores.\nIn this paper, we thus propose (1) a re-adapted multi-task attention network to\npredict both the mean opinion score and the standard deviation in an end-to-end\nmanner; (2) a brand-new confidence interval ranking loss that encourages the\nmodel to focus on image-pairs that are less certain about the difference of\ntheir aesthetic scores. With such loss, the model is encouraged to learn the\nuncertainty of the content that is relevant to the diversity of observers'\nopinions, i.e., user disagreement. Extensive experiments have demonstrated that\nthe proposed multi-task aesthetic model achieves state-of-the-art performance\non two different types of aesthetic datasets, i.e., AVA and TMGA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ling_S/0/1/0/all/0/1\">Suiyi Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pastor_A/0/1/0/all/0/1\">Andreas Pastor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Junle Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callet_P/0/1/0/all/0/1\">Patrick Le Callet</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking the Representational Continuity: Towards Unsupervised Continual Learning. (arXiv:2110.06976v1 [cs.LG])","link":"http://arxiv.org/abs/2110.06976","description":"<p>Continual learning (CL) aims to learn a sequence of tasks without forgetting\nthe previously acquired knowledge. However, recent advances in continual\nlearning are restricted to supervised continual learning (SCL) scenarios.\nConsequently, they are not scalable to real-world applications where the data\ndistribution is often biased and unannotated. In this work, we focus on\nunsupervised continual learning (UCL), where we learn the feature\nrepresentations on an unlabelled sequence of tasks and show that reliance on\nannotated data is not necessary for continual learning. We conduct a systematic\nstudy analyzing the learned feature representations and show that unsupervised\nvisual representations are surprisingly more robust to catastrophic forgetting,\nconsistently achieve better performance, and generalize better to\nout-of-distribution tasks than SCL. Furthermore, we find that UCL achieves a\nsmoother loss landscape through qualitative analysis of the learned\nrepresentations and learns meaningful feature representations. Additionally, we\npropose Lifelong Unsupervised Mixup (LUMP), a simple yet effective technique\nthat leverages the interpolation between the current task and previous tasks'\ninstances to alleviate catastrophic forgetting for unsupervised\nrepresentations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Madaan_D/0/1/0/all/0/1\">Divyam Madaan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1\">Jaehong Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuanchun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yunxin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Sung Ju Hwang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ADMM-DAD net: a deep unfolding network for analysis compressed sensing. (arXiv:2110.06986v1 [cs.IT])","link":"http://arxiv.org/abs/2110.06986","description":"<p>In this paper, we propose a new deep unfolding neural network based on the\nADMM algorithm for analysis Compressed Sensing. The proposed network jointly\nlearns a redundant analysis operator for sparsification and reconstructs the\nsignal of interest. We compare our proposed network with a state-of-the-art\nunfolded ISTA decoder, that also learns an orthogonal sparsifier. Moreover, we\nconsider not only image, but also speech datasets as test examples.\nComputational experiments demonstrate that our proposed network outperforms the\nstate-of-the-art deep unfolding networks, consistently for both real-world\nimage and speech datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kouni_V/0/1/0/all/0/1\">Vasiliki Kouni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paraskevopoulos_G/0/1/0/all/0/1\">Georgios Paraskevopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rauhut_H/0/1/0/all/0/1\">Holger Rauhut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alexandropoulos_G/0/1/0/all/0/1\">George C. Alexandropoulos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scaling Laws for the Few-Shot Adaptation of Pre-trained Image Classifiers. (arXiv:2110.06990v1 [cs.LG])","link":"http://arxiv.org/abs/2110.06990","description":"<p>Empirical science of neural scaling laws is a rapidly growing area of\nsignificant importance to the future of machine learning, particularly in the\nlight of recent breakthroughs achieved by large-scale pre-trained models such\nas GPT-3, CLIP and DALL-e. Accurately predicting the neural network performance\nwith increasing resources such as data, compute and model size provides a more\ncomprehensive evaluation of different approaches across multiple scales, as\nopposed to traditional point-wise comparisons of fixed-size models on\nfixed-size benchmarks, and, most importantly, allows for focus on the\nbest-scaling, and thus most promising in the future, approaches. In this work,\nwe consider a challenging problem of few-shot learning in image classification,\nespecially when the target data distribution in the few-shot phase is different\nfrom the source, training, data distribution, in a sense that it includes new\nimage classes not encountered during training. Our current main goal is to\ninvestigate how the amount of pre-training data affects the few-shot\ngeneralization performance of standard image classifiers. Our key observations\nare that (1) such performance improvements are well-approximated by power laws\n(linear log-log plots) as the training set size increases, (2) this applies to\nboth cases of target data coming from either the same or from a different\ndomain (i.e., new classes) as the training data, and (3) few-shot performance\non new classes converges at a faster rate than the standard classification\nperformance on previously seen classes. Our findings shed new light on the\nrelationship between scale and generalization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Prato_G/0/1/0/all/0/1\">Gabriele Prato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guiroy_S/0/1/0/all/0/1\">Simon Guiroy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caballero_E/0/1/0/all/0/1\">Ethan Caballero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rish_I/0/1/0/all/0/1\">Irina Rish</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandar_S/0/1/0/all/0/1\">Sarath Chandar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Top 3 in FG 2021 Families In the Wild Kinship Verification Challenge. (arXiv:2110.07020v1 [cs.CV])","link":"http://arxiv.org/abs/2110.07020","description":"<p>Kinship verification is the task of determining whether a parent-child,\nsibling, or grandparent-grandchild relationship exists between two people and\nis important in social media applications, forensic investigations, finding\nmissing children, and reuniting families. We demonstrate high quality kinship\nverification by participating in the FG 2021 Recognizing Families in the Wild\nchallenge which provides the largest publicly available dataset in the field.\nOur approach is among the top 3 winning entries in the competition. We ensemble\nmodels written by both human experts and OpenAI Codex. We make our models and\ncode publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Junyi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strome_M/0/1/0/all/0/1\">Maxwell Benjamin Strome</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jenkins_I/0/1/0/all/0/1\">Ian Jenkins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_P/0/1/0/all/0/1\">Parker Williams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_B/0/1/0/all/0/1\">Bo Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaning Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Roman Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bagri_V/0/1/0/all/0/1\">Vaibhav Bagri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_N/0/1/0/all/0/1\">Newman Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drori_I/0/1/0/all/0/1\">Iddo Drori</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data Incubation -- Synthesizing Missing Data for Handwriting Recognition. (arXiv:2110.07040v1 [cs.CV])","link":"http://arxiv.org/abs/2110.07040","description":"<p>In this paper, we demonstrate how a generative model can be used to build a\nbetter recognizer through the control of content and style. We are building an\nonline handwriting recognizer from a modest amount of training samples. By\ntraining our controllable handwriting synthesizer on the same data, we can\nsynthesize handwriting with previously underrepresented content (e.g., URLs and\nemail addresses) and style (e.g., cursive and slanted). Moreover, we propose a\nframework to analyze a recognizer that is trained with a mixture of real and\nsynthetic training data. We use the framework to optimize data synthesis and\ndemonstrate significant improvement on handwriting recognition over a model\ntrained on real data only. Overall, we achieve a 66% reduction in Character\nError Rate.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1\">Jen-Hao Rick Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bresler_M/0/1/0/all/0/1\">Martin Bresler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chherawala_Y/0/1/0/all/0/1\">Youssouf Chherawala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Delaye_A/0/1/0/all/0/1\">Adrien Delaye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deselaers_T/0/1/0/all/0/1\">Thomas Deselaers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dixon_R/0/1/0/all/0/1\">Ryan Dixon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuzel_O/0/1/0/all/0/1\">Oncel Tuzel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"High-throughput Phenotyping of Nematode Cysts. (arXiv:2110.07057v1 [eess.IV])","link":"http://arxiv.org/abs/2110.07057","description":"<p>The beet cyst nematode (BCN) Heterodera schachtii is a plant pest responsible\nfor crop loss on a global scale. Here, we introduce a high-throughput system\nbased on computer vision that allows quantifying BCN infestation and\ncharacterizing nematode cysts through phenotyping. After recording microscopic\nimages of soil extracts in a standardized setting, an instance segmentation\nalgorithm serves to detect nematode cysts in these samples. Going beyond fast\nand precise cyst counting, the image-based approach enables quantification of\ncyst density and phenotyping of morphological features of cysts under different\nconditions, providing the basis for high-throughput applications in agriculture\nand plant breeding research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chen_L/0/1/0/all/0/1\">Long Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Daub_M/0/1/0/all/0/1\">Matthias Daub</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Luigs_H/0/1/0/all/0/1\">Hans-Georg Luigs</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jansen_M/0/1/0/all/0/1\">Marcus Jansen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Strauch_M/0/1/0/all/0/1\">Martin Strauch</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Merhof_D/0/1/0/all/0/1\">Dorit Merhof</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ego4D: Around the World in 3,000 Hours of Egocentric Video. (arXiv:2110.07058v1 [cs.CV])","link":"http://arxiv.org/abs/2110.07058","description":"<p>We introduce Ego4D, a massive-scale egocentric video dataset and benchmark\nsuite. It offers 3,025 hours of daily-life activity video spanning hundreds of\nscenarios (household, outdoor, workplace, leisure, etc.) captured by 855 unique\ncamera wearers from 74 worldwide locations and 9 different countries. The\napproach to collection is designed to uphold rigorous privacy and ethics\nstandards with consenting participants and robust de-identification procedures\nwhere relevant. Ego4D dramatically expands the volume of diverse egocentric\nvideo footage publicly available to the research community. Portions of the\nvideo are accompanied by audio, 3D meshes of the environment, eye gaze, stereo,\nand/or synchronized videos from multiple egocentric cameras at the same event.\nFurthermore, we present a host of new benchmark challenges centered around\nunderstanding the first-person visual experience in the past (querying an\nepisodic memory), present (analyzing hand-object manipulation, audio-visual\nconversation, and social interactions), and future (forecasting activities). By\npublicly sharing this massive annotated dataset and benchmark suite, we aim to\npush the frontier of first-person perception. Project page:\nhttps://ego4d-data.org/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Grauman_K/0/1/0/all/0/1\">Kristen Grauman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Westbury_A/0/1/0/all/0/1\">Andrew Westbury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Byrne_E/0/1/0/all/0/1\">Eugene Byrne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chavis_Z/0/1/0/all/0/1\">Zachary Chavis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Furnari_A/0/1/0/all/0/1\">Antonino Furnari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Girdhar_R/0/1/0/all/0/1\">Rohit Girdhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamburger_J/0/1/0/all/0/1\">Jackson Hamburger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Hao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Miao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xingyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_M/0/1/0/all/0/1\">Miguel Martin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagarajan_T/0/1/0/all/0/1\">Tushar Nagarajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radosavovic_I/0/1/0/all/0/1\">Ilija Radosavovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramakrishnan_S/0/1/0/all/0/1\">Santhosh Kumar Ramakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryan_F/0/1/0/all/0/1\">Fiona Ryan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_J/0/1/0/all/0/1\">Jayant Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wray_M/0/1/0/all/0/1\">Michael Wray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mengmeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_E/0/1/0/all/0/1\">Eric Zhongcong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_S/0/1/0/all/0/1\">Siddhant Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batra_D/0/1/0/all/0/1\">Dhruv Batra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cartillier_V/0/1/0/all/0/1\">Vincent Cartillier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crane_S/0/1/0/all/0/1\">Sean Crane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Do_T/0/1/0/all/0/1\">Tien Do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doulaty_M/0/1/0/all/0/1\">Morrie Doulaty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erapalli_A/0/1/0/all/0/1\">Akshay Erapalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feichtenhofer_C/0/1/0/all/0/1\">Christoph Feichtenhofer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fragomeni_A/0/1/0/all/0/1\">Adriano Fragomeni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Q/0/1/0/all/0/1\">Qichen Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fuegen_C/0/1/0/all/0/1\">Christian Fuegen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gebreselasie_A/0/1/0/all/0/1\">Abrham Gebreselasie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_C/0/1/0/all/0/1\">Cristina Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hillis_J/0/1/0/all/0/1\">James Hillis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuhua Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yifei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_W/0/1/0/all/0/1\">Wenqi Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khoo_W/0/1/0/all/0/1\">Weslie Khoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolar_J/0/1/0/all/0/1\">Jachym Kolar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kottur_S/0/1/0/all/0/1\">Satwik Kottur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Anurag Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Landini_F/0/1/0/all/0/1\">Federico Landini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanghao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenqiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mangalam_K/0/1/0/all/0/1\">Karttikeya Mangalam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Modhugu_R/0/1/0/all/0/1\">Raghava Modhugu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Munro_J/0/1/0/all/0/1\">Jonathan Munro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murrell_T/0/1/0/all/0/1\">Tullie Murrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nishiyasu_T/0/1/0/all/0/1\">Takumi Nishiyasu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Price_W/0/1/0/all/0/1\">Will Price</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Puentes_P/0/1/0/all/0/1\">Paola Ruiz Puentes</a>, et al. (32 additional authors not shown)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Subspace Regularizers for Few-Shot Class Incremental Learning. (arXiv:2110.07059v1 [cs.CV])","link":"http://arxiv.org/abs/2110.07059","description":"<p>Few-shot class incremental learning -- the problem of updating a trained\nclassifier to discriminate among an expanded set of classes with limited\nlabeled data -- is a key challenge for machine learning systems deployed in\nnon-stationary environments. Existing approaches to the problem rely on complex\nmodel architectures and training procedures that are difficult to tune and\nre-use. In this paper, we present an extremely simple approach that enables the\nuse of ordinary logistic regression classifiers for few-shot incremental\nlearning. The key to this approach is a new family of subspace regularization\nschemes that encourage weight vectors for new classes to lie close to the\nsubspace spanned by the weights of existing classes. When combined with\npretrained convolutional feature extractors, logistic regression models trained\nwith subspace regularization outperform specialized, state-of-the-art\napproaches to few-shot incremental image classification by up to 22% on the\nminiImageNet dataset. Because of its simplicity, subspace regularization can be\nstraightforwardly extended to incorporate additional background information\nabout the new classes (including class names and descriptions specified in\nnatural language); these further improve accuracy by up to 2%. Our results show\nthat simple geometric regularization of class representations offers an\neffective tool for continual learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Akyurek_A/0/1/0/all/0/1\">Afra Feyza Aky&#xfc;rek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akyurek_E/0/1/0/all/0/1\">Ekin Aky&#xfc;rek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wijaya_D/0/1/0/all/0/1\">Derry Wijaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andreas_J/0/1/0/all/0/1\">Jacob Andreas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast Hand Detection in Collaborative Learning Environments. (arXiv:2110.07070v1 [cs.CV])","link":"http://arxiv.org/abs/2110.07070","description":"<p>Long-term object detection requires the integration of frame-based results\nover several seconds. For non-deformable objects, long-term detection is often\naddressed using object detection followed by video tracking. Unfortunately,\ntracking is inapplicable to objects that undergo dramatic changes in appearance\nfrom frame to frame. As a related example, we study hand detection over long\nvideo recordings in collaborative learning environments. More specifically, we\ndevelop long-term hand detection methods that can deal with partial occlusions\nand dramatic changes in appearance.\n</p>\n<p>Our approach integrates object-detection, followed by time projections,\nclustering, and small region removal to provide effective hand detection over\nlong videos. The hand detector achieved average precision (AP) of 72% at 0.5\nintersection over union (IoU). The detection results were improved to 81% by\nusing our optimized approach for data augmentation. The method runs at 4.7x the\nreal-time with AP of 81% at 0.5 intersection over the union. Our method reduced\nthe number of false-positive hand detections by 80% by improving IoU ratios\nfrom 0.2 to 0.5. The overall hand detection system runs at 4x real-time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Teeparthi_S/0/1/0/all/0/1\">Sravani Teeparthi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jatla_V/0/1/0/all/0/1\">Venkatesh Jatla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pattichis_M/0/1/0/all/0/1\">Marios S. Pattichis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pattichis_S/0/1/0/all/0/1\">Sylvia Celedon Pattichis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+LopezLeiva_C/0/1/0/all/0/1\">Carlos LopezLeiva</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Impact of Spatiotemporal Augmentations on Self-Supervised Audiovisual Representation Learning. (arXiv:2110.07082v1 [cs.CV])","link":"http://arxiv.org/abs/2110.07082","description":"<p>Contrastive learning of auditory and visual perception has been extremely\nsuccessful when investigated individually. However, there are still major\nquestions on how we could integrate principles learned from both domains to\nattain effective audiovisual representations. In this paper, we present a\ncontrastive framework to learn audiovisual representations from unlabeled\nvideos. The type and strength of augmentations utilized during self-supervised\npre-training play a crucial role for contrastive frameworks to work\nsufficiently. Hence, we extensively investigate composition of temporal\naugmentations suitable for learning audiovisual representations; we find lossy\nspatio-temporal transformations that do not corrupt the temporal coherency of\nvideos are the most effective. Furthermore, we show that the effectiveness of\nthese transformations scales with higher temporal resolution and stronger\ntransformation intensity. Compared to self-supervised models pre-trained on\nonly sampling-based temporal augmentation, self-supervised models pre-trained\nwith our temporal augmentations lead to approximately 6.5% gain on linear\nclassifier performance on AVE dataset. Lastly, we show that despite their\nsimplicity, our proposed transformations work well across self-supervised\nlearning frameworks (SimSiam, MoCoV3, etc), and benchmark audiovisual dataset\n(AVE).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Al_Tahan_H/0/1/0/all/0/1\">Haider Al-Tahan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohsenzadeh_Y/0/1/0/all/0/1\">Yalda Mohsenzadeh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comprehensive Study on Torchvision Pre-trained Models for Fine-grained Inter-species Classification. (arXiv:2110.07097v1 [cs.CV])","link":"http://arxiv.org/abs/2110.07097","description":"<p>This study aims to explore different pre-trained models offered in the\nTorchvision package which is available in the PyTorch library. And investigate\ntheir effectiveness on fine-grained images classification. Transfer Learning is\nan effective method of achieving extremely good performance with insufficient\ntraining data. In many real-world situations, people cannot collect sufficient\ndata required to train a deep neural network model efficiently. Transfer\nLearning models are pre-trained on a large data set, and can bring a good\nperformance on smaller datasets with significantly lower training time.\nTorchvision package offers us many models to apply the Transfer Learning on\nsmaller datasets. Therefore, researchers may need a guideline for the selection\nof a good model. We investigate Torchvision pre-trained models on four\ndifferent data sets: 10 Monkey Species, 225 Bird Species, Fruits 360, and\nOxford 102 Flowers. These data sets have images of different resolutions, class\nnumbers, and different achievable accuracies. We also apply their usual\nfully-connected layer and the Spinal fully-connected layer to investigate the\neffectiveness of SpinalNet. The Spinal fully-connected layer brings better\nperformance in most situations. We apply the same augmentation for different\nmodels for the same data set for a fair comparison. This paper may help future\nComputer Vision researchers in choosing a proper Transfer Learning model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Albardi_F/0/1/0/all/0/1\">Feras Albardi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kabir_H/0/1/0/all/0/1\">H M Dipu Kabir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhuiyan_M/0/1/0/all/0/1\">Md Mahbub Islam Bhuiyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kebria_P/0/1/0/all/0/1\">Parham M. Kebria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khosravi_A/0/1/0/all/0/1\">Abbas Khosravi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nahavandi_S/0/1/0/all/0/1\">Saeid Nahavandi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video-based cattle identification and action recognition. (arXiv:2110.07103v1 [cs.CV])","link":"http://arxiv.org/abs/2110.07103","description":"<p>We demonstrate a working prototype for the monitoring of cow welfare by\nautomatically analysing the animal behaviours. Deep learning models have been\ndeveloped and tested with videos acquired in a farm, and a precision of 81.2\\%\nhas been achieved for cow identification. An accuracy of 84.4\\% has been\nachieved for the detection of drinking events, and 94.4\\% for the detection of\ngrazing events. Experimental results show that the proposed deep learning\nmethod can be used to identify the behaviours of individual animals to enable\nautomated farm provenance. Our raw and ground-truth dataset will be released as\nthe first public video dataset for cow identification and action recognition.\nRecommendations for further development are also provided.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1\">Chuong Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dadong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Richter_K/0/1/0/all/0/1\">Karl Von Richter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valencia_P/0/1/0/all/0/1\">Philip Valencia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alvarenga_F/0/1/0/all/0/1\">Flavio A. P. Alvarenga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bishop_Hurley_G/0/1/0/all/0/1\">Gregory Bishop-Hurley</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly Supervised Semantic Segmentation by Pixel-to-Prototype Contrast. (arXiv:2110.07110v1 [cs.CV])","link":"http://arxiv.org/abs/2110.07110","description":"<p>Though image-level weakly supervised semantic segmentation (WSSS) has\nachieved great progress with Class Activation Map (CAM) as the cornerstone, the\nlarge supervision gap between classification and segmentation still hampers the\nmodel to generate more complete and precise pseudo masks for segmentation.\n</p>\n<p>In this study, we explore two implicit but intuitive constraints, i.e.,\ncross-view feature semantic consistency and intra(inter)-class\ncompactness(dispersion), to narrow the supervision gap.\n</p>\n<p>To this end, we propose two novel pixel-to-prototype contrast regularization\nterms that are conducted cross different views and within per single view of an\nimage, respectively. Besides, we adopt two sample mining strategies, named\nsemi-hard prototype mining and hard pixel sampling, to better leverage hard\nexamples while reducing incorrect contrasts caused due to the absence of\nprecise pixel-wise labels.\n</p>\n<p>Our method can be seamlessly incorporated into existing WSSS models without\nany changes to the base network and does not incur any extra inference burden.\nExperiments on standard benchmark show that our method consistently improves\ntwo strong baselines by large margins, demonstrating the effectiveness of our\nmethod. Specifically, built on top of SEAM, we improve the initial seed mIoU on\nPASCAL VOC 2012 from 55.4% to 61.5%. Moreover, armed with our method, we\nincrease the segmentation mIoU of EPS from 70.8% to 73.6%, achieving new\nstate-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Ye Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1\">Zehua Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qingjie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunhong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Nuisance-Label Supervision: Robustness Improvement by Free Labels. (arXiv:2110.07118v1 [cs.CV])","link":"http://arxiv.org/abs/2110.07118","description":"<p>In this paper, we present a Nuisance-label Supervision (NLS) module, which\ncan make models more robust to nuisance factor variations. Nuisance factors are\nthose irrelevant to a task, and an ideal model should be invariant to them. For\nexample, an activity recognition model should perform consistently regardless\nof the change of clothes and background. But our experiments show existing\nmodels are far from this capability. So we explicitly supervise a model with\nnuisance labels to make extracted features less dependent on nuisance factors.\nAlthough the values of nuisance factors are rarely annotated, we demonstrate\nthat besides existing annotations, nuisance labels can be acquired freely from\ndata augmentation and synthetic data. Experiments show consistent improvement\nin robustness towards image corruption and appearance change in action\nrecognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xinyue Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_W/0/1/0/all/0/1\">Weichao Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1\">Zihao Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1\">Alan Yuille</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Brittle interpretations: The Vulnerability of TCAV and Other Concept-based Explainability Tools to Adversarial Attack. (arXiv:2110.07120v1 [cs.LG])","link":"http://arxiv.org/abs/2110.07120","description":"<p>Methods for model explainability have become increasingly critical for\ntesting the fairness and soundness of deep learning. A number of explainability\ntechniques have been developed which use a set of examples to represent a\nhuman-interpretable concept in a model's activations. In this work we show that\nthese explainability methods can suffer the same vulnerability to adversarial\nattacks as the models they are meant to analyze. We demonstrate this phenomenon\non two well-known concept-based approaches to the explainability of deep\nlearning models: TCAV and faceted feature visualization. We show that by\ncarefully perturbing the examples of the concept that is being investigated, we\ncan radically change the output of the interpretability method, e.g. showing\nthat stripes are not an important factor in identifying images of a zebra. Our\nwork highlights the fact that in safety-critical applications, there is need\nfor security around not only the machine learning pipeline but also the model\ninterpretation process.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Brown_D/0/1/0/all/0/1\">Davis Brown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kvinge_H/0/1/0/all/0/1\">Henry Kvinge</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Region Semantically Aligned Network for Zero-Shot Learning. (arXiv:2110.07130v1 [cs.CV])","link":"http://arxiv.org/abs/2110.07130","description":"<p>Zero-shot learning (ZSL) aims to recognize unseen classes based on the\nknowledge of seen classes. Previous methods focused on learning direct\nembeddings from global features to the semantic space in hope of knowledge\ntransfer from seen classes to unseen classes. However, an unseen class shares\nlocal visual features with a set of seen classes and leveraging global visual\nfeatures makes the knowledge transfer ineffective. To tackle this problem, we\npropose a Region Semantically Aligned Network (RSAN), which maps local features\nof unseen classes to their semantic attributes. Instead of using global\nfeatures which are obtained by an average pooling layer after an image encoder,\nwe directly utilize the output of the image encoder which maintains local\ninformation of the image. Concretely, we obtain each attribute from a specific\nregion of the output and exploit these attributes for recognition. As a result,\nthe knowledge of seen classes can be successfully transferred to unseen classes\nin a region-bases manner. In addition, we regularize the image encoder through\nattribute regression with a semantic knowledge to extract robust and\nattribute-related visual features. Experiments on several standard ZSL datasets\nreveal the benefit of the proposed RSAN method, outperforming state-of-the-art\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Ziyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gou_Y/0/1/0/all/0/1\">Yunhao Gou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jingjing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yang Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A CLIP-Enhanced Method for Video-Language Understanding. (arXiv:2110.07137v1 [cs.CV])","link":"http://arxiv.org/abs/2110.07137","description":"<p>This technical report summarizes our method for the Video-And-Language\nUnderstanding Evaluation (VALUE) challenge\n(https://value-benchmark.github.io/challenge\\_2021.html). We propose a\nCLIP-Enhanced method to incorporate the image-text pretrained knowledge into\ndownstream video-text tasks. Combined with several other improved designs, our\nmethod outperforms the state-of-the-art by $2.4\\%$ ($57.58$ to $60.00$)\nMeta-Ave score on VALUE benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guohao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_F/0/1/0/all/0/1\">Feng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zhifan Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Data-Driven Nuclei Segmentation For Histology Images. (arXiv:2110.07147v1 [eess.IV])","link":"http://arxiv.org/abs/2110.07147","description":"<p>An unsupervised data-driven nuclei segmentation method for histology images,\ncalled CBM, is proposed in this work. CBM consists of three modules applied in\na block-wise manner: 1) data-driven color transform for energy compaction and\ndimension reduction, 2) data-driven binarization, and 3) incorporation of\ngeometric priors with morphological processing. CBM comes from the first letter\nof the three modules - \"Color transform\", \"Binarization\" and \"Morphological\nprocessing\". Experiments on the MoNuSeg dataset validate the effectiveness of\nthe proposed CBM method. CBM outperforms all other unsupervised methods and\noffers a competitive standing among supervised models based on the Aggregated\nJaccard Index (AJI) metric.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Magoulianitis_V/0/1/0/all/0/1\">Vasileios Magoulianitis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Han_P/0/1/0/all/0/1\">Peida Han</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_Y/0/1/0/all/0/1\">Yijing Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kuo_C/0/1/0/all/0/1\">C.-C. Jay Kuo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepSSM: A Blueprint for Image-to-Shape Deep Learning Models. (arXiv:2110.07152v1 [cs.CV])","link":"http://arxiv.org/abs/2110.07152","description":"<p>Statistical shape modeling (SSM) characterizes anatomical variations in a\npopulation of shapes generated from medical images. SSM requires consistent\nshape representation across samples in shape cohort. Establishing this\nrepresentation entails a processing pipeline that includes anatomy\nsegmentation, re-sampling, registration, and non-linear optimization. These\nshape representations are then used to extract low-dimensional shape\ndescriptors that facilitate subsequent analyses in different applications.\nHowever, the current process of obtaining these shape descriptors from imaging\ndata relies on human and computational resources, requiring domain expertise\nfor segmenting anatomies of interest. Moreover, this same taxing pipeline needs\nto be repeated to infer shape descriptors for new image data using a\npre-trained/existing shape model. Here, we propose DeepSSM, a deep\nlearning-based framework for learning the functional mapping from images to\nlow-dimensional shape descriptors and their associated shape representations,\nthereby inferring statistical representation of anatomy directly from 3D\nimages. Once trained using an existing shape model, DeepSSM circumvents the\nheavy and manual pre-processing and segmentation and significantly improves the\ncomputational time, making it a viable solution for fully end-to-end SSM\napplications. In addition, we introduce a model-based data-augmentation\nstrategy to address data scarcity. Finally, this paper presents and analyzes\ntwo different architectural variants of DeepSSM with different loss functions\nusing three medical datasets and their downstream clinical application.\nExperiments showcase that DeepSSM performs comparably or better to the\nstate-of-the-art SSM both quantitatively and on application-driven downstream\ntasks. Therefore, DeepSSM aims to provide a comprehensive blueprint for deep\nlearning-based image-to-shape models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhalodia_R/0/1/0/all/0/1\">Riddhish Bhalodia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elhabian_S/0/1/0/all/0/1\">Shireen Elhabian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adams_J/0/1/0/all/0/1\">Jadie Adams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_W/0/1/0/all/0/1\">Wenzheng Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kavan_L/0/1/0/all/0/1\">Ladislav Kavan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Whitaker_R/0/1/0/all/0/1\">Ross Whitaker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantically Distributed Robust Optimization for Vision-and-Language Inference. (arXiv:2110.07165v1 [cs.CV])","link":"http://arxiv.org/abs/2110.07165","description":"<p>Analysis of vision-and-language models has revealed their brittleness under\nlinguistic phenomena such as paraphrasing, negation, textual entailment, and\nword substitutions with synonyms or antonyms. While data augmentation\ntechniques have been designed to mitigate against these failure modes, methods\nthat can integrate this knowledge into the training pipeline remain\nunder-explored. In this paper, we present \\textbf{SDRO}, a model-agnostic\nmethod that utilizes a set linguistic transformations in a distributed robust\noptimization setting, along with an ensembling technique to leverage these\ntransformations during inference. Experiments on benchmark datasets with images\n(NLVR$^2$) and video (VIOLIN) demonstrate performance improvements as well as\nrobustness to adversarial attacks. Experiments on binary VQA explore the\ngeneralizability of this method to other V\\&amp;L tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gokhale_T/0/1/0/all/0/1\">Tejas Gokhale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhary_A/0/1/0/all/0/1\">Abhishek Chaudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_P/0/1/0/all/0/1\">Pratyay Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yezhou Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SGoLAM: Simultaneous Goal Localization and Mapping for Multi-Object Goal Navigation. (arXiv:2110.07171v1 [cs.CV])","link":"http://arxiv.org/abs/2110.07171","description":"<p>We present SGoLAM, short for simultaneous goal localization and mapping,\nwhich is a simple and efficient algorithm for Multi-Object Goal navigation.\nGiven an agent equipped with an RGB-D camera and a GPS/Compass sensor, our\nobjective is to have the agent navigate to a sequence of target objects in\nrealistic 3D environments. Our pipeline fully leverages the strength of\nclassical approaches for visual navigation, by decomposing the problem into two\nkey components: mapping and goal localization. The mapping module converts the\ndepth observations into an occupancy map, and the goal localization module\nmarks the locations of goal objects. The agent's policy is determined using the\ninformation provided by the two modules: if a current goal is found, plan\ntowards the goal and otherwise, perform exploration. As our approach does not\nrequire any training of neural networks, it could be used in an off-the-shelf\nmanner, and amenable for fast generalization in new, unseen environments.\nNonetheless, our approach performs on par with the state-of-the-art\nlearning-based approaches. SGoLAM is ranked 2nd in the CVPR 2021 MultiON\n(Multi-Object Goal Navigation) challenge. We have made our code publicly\navailable at \\emph{https://github.com/eunsunlee/SGoLAM}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Junho Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_E/0/1/0/all/0/1\">Eun Sun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Mingi Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Donsu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Young Min Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial examples by perturbing high-level features in intermediate decoder layers. (arXiv:2110.07182v1 [cs.CV])","link":"http://arxiv.org/abs/2110.07182","description":"<p>We propose a novel method for creating adversarial examples. Instead of\nperturbing pixels, we use an encoder-decoder representation of the input image\nand perturb intermediate layers in the decoder. This changes the high-level\nfeatures provided by the generative model. Therefore, our perturbation\npossesses semantic meaning, such as a longer beak or green tints. We formulate\nthis task as an optimization problem by minimizing the Wasserstein distance\nbetween the adversarial and initial images under a misclassification\nconstraint. We employ the projected gradient method with a simple inexact\nprojection. Due to the projection, all iterations are feasible, and our method\nalways generates adversarial images. We perform numerical experiments on the\nMNIST and ImageNet datasets in both targeted and untargeted settings. We\ndemonstrate that our adversarial images are much less vulnerable to\nsteganographic defence techniques than pixel-based attacks. Moreover, we show\nthat our method modifies key features such as edges and that defence techniques\nbased on adversarial training are vulnerable to our attacks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cermak_V/0/1/0/all/0/1\">Vojt&#x11b;ch &#x10c;erm&#xe1;k</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adam_L/0/1/0/all/0/1\">Luk&#xe1;&#x161; Adam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Domain Adaptation for Visual Navigation with Global Map Consistency. (arXiv:2110.07184v1 [cs.CV])","link":"http://arxiv.org/abs/2110.07184","description":"<p>We propose a light-weight, self-supervised adaptation for a visual navigation\nagent to generalize to unseen environment. Given an embodied agent trained in a\nnoiseless environment, our objective is to transfer the agent to a noisy\nenvironment where actuation and odometry sensor noise is present. Our method\nencourages the agent to maximize the consistency between the global maps\ngenerated at different time steps in a round-trip trajectory. The proposed task\nis completely self-supervised, not requiring any supervision from ground-truth\npose data or explicit noise model. In addition, optimization of the task\nobjective is extremely light-weight, as training terminates within a few\nminutes on a commodity GPU. Our experiments show that the proposed task helps\nthe agent to successfully transfer to new, noisy environments. The transferred\nagent exhibits improved localization and mapping accuracy, further leading to\nenhanced performance in downstream visual navigation tasks. Moreover, we\ndemonstrate test-time adaptation with our self-supervised task to show its\npotential applicability in real-world deployment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_E/0/1/0/all/0/1\">Eun Sun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Junho Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Young Min Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-supervised Multi-task Learning for Semantics and Depth. (arXiv:2110.07197v1 [cs.CV])","link":"http://arxiv.org/abs/2110.07197","description":"<p>Multi-Task Learning (MTL) aims to enhance the model generalization by sharing\nrepresentations between related tasks for better performance. Typical MTL\nmethods are jointly trained with the complete multitude of ground-truths for\nall tasks simultaneously. However, one single dataset may not contain the\nannotations for each task of interest. To address this issue, we propose the\nSemi-supervised Multi-Task Learning (SemiMTL) method to leverage the available\nsupervisory signals from different datasets, particularly for semantic\nsegmentation and depth estimation tasks. To this end, we design an adversarial\nlearning scheme in our semi-supervised training by leveraging unlabeled data to\noptimize all the task branches simultaneously and accomplish all tasks across\ndatasets with partial annotations. We further present a domain-aware\ndiscriminator structure with various alignment formulations to mitigate the\ndomain discrepancy issue among datasets. Finally, we demonstrate the\neffectiveness of the proposed method to learn across different datasets on\nchallenging street view and remote sensing benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yufeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsai_Y/0/1/0/all/0/1\">Yi-Hsuan Tsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hung_W/0/1/0/all/0/1\">Wei-Chih Hung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1\">Wenrui Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shuo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Ming-Hsuan Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Coarse to Fine: Video Retrieval before Moment Localization. (arXiv:2110.07201v1 [cs.CV])","link":"http://arxiv.org/abs/2110.07201","description":"<p>The current state-of-the-art methods for video corpus moment retrieval (VCMR)\noften use similarity-based feature alignment approach for the sake of\nconvenience and speed. However, late fusion methods like cosine similarity\nalignment are unable to make full use of the information from both query texts\nand videos. In this paper, we combine feature alignment with feature fusion to\npromote the performance on VCMR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1\">Zijian Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Huanyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jingyu Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unrolled Variational Bayesian Algorithm for Image Blind Deconvolution. (arXiv:2110.07202v1 [cs.CV])","link":"http://arxiv.org/abs/2110.07202","description":"<p>In this paper, we introduce a variational Bayesian algorithm (VBA) for image\nblind deconvolution. Our generic framework incorporates smoothness priors on\nthe unknown blur/image and possible affine constraints (e.g., sum to one) on\nthe blur kernel. One of our main contributions is the integration of VBA within\na neural network paradigm, following an unrolling methodology. The proposed\narchitecture is trained in a supervised fashion, which allows us to optimally\nset two key hyperparameters of the VBA model and lead to further improvements\nin terms of resulting visual quality. Various experiments involving\ngrayscale/color images and diverse kernel shapes, are performed. The numerical\nexamples illustrate the high performance of our approach when compared to\nstate-of-the-art techniques based on optimization, Bayesian estimation, or deep\nlearning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yunshi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chouzenoux_E/0/1/0/all/0/1\">Emilie Chouzenoux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pesquet_J/0/1/0/all/0/1\">Jean-Christophe Pesquet</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Task-Driven Deep Image Enhancement Network for Autonomous Driving in Bad Weather. (arXiv:2110.07206v1 [cs.CV])","link":"http://arxiv.org/abs/2110.07206","description":"<p>Visual perception in autonomous driving is a crucial part of a vehicle to\nnavigate safely and sustainably in different traffic conditions. However, in\nbad weather such as heavy rain and haze, the performance of visual perception\nis greatly affected by several degrading effects. Recently, deep learning-based\nperception methods have addressed multiple degrading effects to reflect\nreal-world bad weather cases but have shown limited success due to 1) high\ncomputational costs for deployment on mobile devices and 2) poor relevance\nbetween image enhancement and visual perception in terms of the model ability.\nTo solve these issues, we propose a task-driven image enhancement network\nconnected to the high-level vision task, which takes in an image corrupted by\nbad weather as input. Specifically, we introduce a novel low memory network to\nreduce most of the layer connections of dense blocks for less memory and\ncomputational cost while maintaining high performance. We also introduce a new\ntask-driven training strategy to robustly guide the high-level task model\nsuitable for both high-quality restoration of images and highly accurate\nperception. Experiment results demonstrate that the proposed method improves\nthe performance among lane and 2D object detection, and depth estimation\nlargely under adverse weather in terms of both low memory and accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Younkwan Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeon_J/0/1/0/all/0/1\">Jihyo Jeon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_Y/0/1/0/all/0/1\">Yeongmin Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeon_B/0/1/0/all/0/1\">Byunggwan Jeon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeon_M/0/1/0/all/0/1\">Moongu Jeon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HUMAN4D: A Human-Centric Multimodal Dataset for Motions and Immersive Media. (arXiv:2110.07235v1 [cs.CV])","link":"http://arxiv.org/abs/2110.07235","description":"<p>We introduce HUMAN4D, a large and multimodal 4D dataset that contains a\nvariety of human activities simultaneously captured by a professional\nmarker-based MoCap, a volumetric capture and an audio recording system. By\ncapturing 2 female and $2$ male professional actors performing various\nfull-body movements and expressions, HUMAN4D provides a diverse set of motions\nand poses encountered as part of single- and multi-person daily, physical and\nsocial activities (jumping, dancing, etc.), along with multi-RGBD (mRGBD),\nvolumetric and audio data. Despite the existence of multi-view color datasets\ncaptured with the use of hardware (HW) synchronization, to the best of our\nknowledge, HUMAN4D is the first and only public resource that provides\nvolumetric depth maps with high synchronization precision due to the use of\nintra- and inter-sensor HW-SYNC. Moreover, a spatio-temporally aligned scanned\nand rigged 3D character complements HUMAN4D to enable joint research on\ntime-varying and high-quality dynamic meshes. We provide evaluation baselines\nby benchmarking HUMAN4D with state-of-the-art human pose estimation and 3D\ncompression methods. For the former, we apply 2D and 3D pose estimation\nalgorithms both on single- and multi-view data cues. For the latter, we\nbenchmark open-source 3D codecs on volumetric data respecting online volumetric\nvideo encoding and steady bit-rates. Furthermore, qualitative and quantitative\nvisual comparison between mesh-based volumetric data reconstructed in different\nqualities showcases the available options with respect to 4D representations.\nHUMAN4D is introduced to the computer vision and graphics research communities\nto enable joint research on spatio-temporally aligned pose, volumetric, mRGBD\nand audio data cues. The dataset and its code are available\nhttps://tofis.github.io/myurls/human4d.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chatzitofis_n/0/1/0/all/0/1\">nargyros Chatzitofis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saroglou_L/0/1/0/all/0/1\">Leonidas Saroglou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boutis_P/0/1/0/all/0/1\">Prodromos Boutis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drakoulis_P/0/1/0/all/0/1\">Petros Drakoulis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zioulis_N/0/1/0/all/0/1\">Nikolaos Zioulis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subramanyam_S/0/1/0/all/0/1\">Shishir Subramanyam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kevelham_B/0/1/0/all/0/1\">Bart Kevelham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charbonnier_C/0/1/0/all/0/1\">Caecilia Charbonnier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cesar_P/0/1/0/all/0/1\">Pablo Cesar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zarpalas_D/0/1/0/all/0/1\">Dimitrios Zarpalas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kollias_S/0/1/0/all/0/1\">Stefanos Kollias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daras_P/0/1/0/all/0/1\">Petros Daras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Point Cloud Filtering: A Non-Local Position Based Approach. (arXiv:2110.07253v1 [cs.CV])","link":"http://arxiv.org/abs/2110.07253","description":"<p>Existing position based point cloud filtering methods can hardly preserve\nsharp geometric features. In this paper, we rethink point cloud filtering from\na non-learning non-local non-normal perspective, and propose a novel position\nbased approach for feature-preserving point cloud filtering. Unlike normal\nbased techniques, our method does not require the normal information. The core\nidea is to first design a similarity metric to search the non-local similar\npatches of a queried local patch. We then map the non-local similar patches\ninto a canonical space and aggregate the non-local information. The aggregated\noutcome (i.e. coordinate) will be inversely mapped into the original space. Our\nmethod is simple yet effective. Extensive experiments validate our method, and\nshow that it generally outperforms position based methods (deep learning and\nnon-learning), and generates better or comparable outcomes to normal based\ntechniques (deep learning and non-learning).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jinxi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jincen Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xuequan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Meili Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepMoCap: Deep Optical Motion Capture Using Multiple Depth Sensors and Retro-Reflectors. (arXiv:2110.07283v1 [cs.CV])","link":"http://arxiv.org/abs/2110.07283","description":"<p>In this paper, a marker-based, single-person optical motion capture method\n(DeepMoCap) is proposed using multiple spatio-temporally aligned infrared-depth\nsensors and retro-reflective straps and patches (reflectors). DeepMoCap\nexplores motion capture by automatically localizing and labeling reflectors on\ndepth images and, subsequently, on 3D space. Introducing a non-parametric\nrepresentation to encode the temporal correlation among pairs of colorized\ndepthmaps and 3D optical flow frames, a multi-stage Fully Convolutional Network\n(FCN) architecture is proposed to jointly learn reflector locations and their\ntemporal dependency among sequential frames. The extracted reflector 2D\nlocations are spatially mapped in 3D space, resulting in robust 3D optical data\nextraction. The subject's motion is efficiently captured by applying a\ntemplate-based fitting technique on the extracted optical data. Two datasets\nhave been created and made publicly available for evaluation purposes; one\ncomprising multi-view depth and 3D optical flow annotated images (DMC2.5D), and\na second, consisting of spatio-temporally aligned multi-view depth images along\nwith skeleton, inertial and ground truth MoCap data (DMC3D). The FCN model\noutperforms its competitors on the DMC2.5D dataset using 2D Percentage of\nCorrect Keypoints (PCK) metric, while the motion capture outcome is evaluated\nagainst RGB-D and inertial data fusion approaches on DMC3D, outperforming the\nnext best method by 4.5% in total 3D PCK accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chatzitofis_A/0/1/0/all/0/1\">Anargyros Chatzitofis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zarpalas_D/0/1/0/all/0/1\">Dimitrios Zarpalas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kollias_S/0/1/0/all/0/1\">Stefanos Kollias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daras_P/0/1/0/all/0/1\">Petros Daras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"View Vertically: A Hierarchical Network for Trajectory Prediction via Fourier Spectrums. (arXiv:2110.07288v1 [cs.CV])","link":"http://arxiv.org/abs/2110.07288","description":"<p>Learning to understand and predict future motions or behaviors for agents\nlike humans and robots are critical to various autonomous platforms, such as\nbehavior analysis, robot navigation, and self-driving cars. Intrinsic factors\nsuch as agents' diversified personalities and decision-making styles bring rich\nand diverse changes and multi-modal characteristics to their future plannings.\nBesides, the extrinsic interactive factors have also brought rich and varied\nchanges to their trajectories. Previous methods mostly treat trajectories as\ntime sequences, and reach great prediction performance. In this work, we try to\nfocus on agents' trajectories in another view, i.e., the Fourier spectrums, to\nexplore their future behavior rules in a novel hierarchical way. We propose the\nTransformer-based V model, which concatenates two continuous keypoints\nestimation and spectrum interpolation sub-networks, to model and predict\nagents' trajectories with spectrums in the keypoints and interactions levels\nrespectively. Experimental results show that V outperforms most of current\nstate-of-the-art methods on ETH-UCY and SDD trajectories dataset for about 15\\%\nquantitative improvements, and performs better qualitative results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wong_C/0/1/0/all/0/1\">Conghao Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_B/0/1/0/all/0/1\">Beihao Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_Z/0/1/0/all/0/1\">Ziming Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Q/0/1/0/all/0/1\">Qinmu Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_X/0/1/0/all/0/1\">Xinge You</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ClonalNet: Classifying Better by Focusing on Confusing Categories. (arXiv:2110.07307v1 [cs.CV])","link":"http://arxiv.org/abs/2110.07307","description":"<p>Existing neural classification networks predominately adopt one-hot encoding\ndue to its simplicity in representing categorical data. However, the one-hot\nrepresentation neglects inter-category correlations, which may result in poor\ngeneralization. Herein, we observe that a pre-trained baseline network has paid\nattention to the target image region even though it incorrectly predicts the\nimage, revealing which categories confuse the baseline. This observation\nmotivates us to consider inter-category correlations. Therefore, we propose a\nclonal network, named ClonalNet, which learns to discriminate between confusing\ncategories derived from the pre-trained baseline. The ClonalNet architecture\ncan be identical or smaller than the baseline architecture. When identical,\nClonalNet is a clonal version of the baseline but does not share weights. When\nsmaller, the training process of ClonalNet resembles that of the standard\nknowledge distillation. The difference from knowledge distillation is that we\ndesign a focusing-picking loss to optimize ClonalNet. This novel loss enforces\nClonalNet to concentrate on confusing categories and make more confident\npredictions on ground-truth labels with the baseline reference. Experiments\nshow that ClonalNet significantly outperforms baseline networks and knowledge\ndistillation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Hui-Liang Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modeling dynamic target deformation in camera calibration. (arXiv:2110.07322v1 [cs.CV])","link":"http://arxiv.org/abs/2110.07322","description":"<p>Most approaches to camera calibration rely on calibration targets of\nwell-known geometry. During data acquisition, calibration target and camera\nsystem are typically moved w.r.t. each other, to allow image coverage and\nperspective versatility. We show that moving the target can lead to small\ntemporary deformations of the target, which can introduce significant errors\ninto the calibration result. While static inaccuracies of calibration targets\nhave been addressed in previous works, to our knowledge, none of the existing\napproaches can capture time-varying, dynamic deformations. To achieve\nhigh-accuracy calibrations despite moving the target, we propose a way to\nexplicitly model dynamic target deformations in camera calibration. This is\nachieved by using a low-dimensional deformation model with only few parameters\nper image, which can be optimized jointly with target poses and intrinsics. We\ndemonstrate the effectiveness of modeling dynamic deformations using different\ncalibration targets and show its significance in a structure-from-motion\napplication.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hagemann_A/0/1/0/all/0/1\">Annika Hagemann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knorr_M/0/1/0/all/0/1\">Moritz Knorr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stiller_C/0/1/0/all/0/1\">Christoph Stiller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-center, multi-vendor automated segmentation of left ventricular anatomy in contrast-enhanced MRI. (arXiv:2110.07360v1 [eess.IV])","link":"http://arxiv.org/abs/2110.07360","description":"<p>Accurate delineation of the left ventricular boundaries in late\ngadolinium-enhanced magnetic resonance imaging (LGE-MRI) is an essential step\nfor scar tissue quantification and patient-specific assessment of myocardial\ninfarction. Many deep-learning techniques have been proposed to perform\nautomatic segmentations of the left ventricle (LV) in LGE-MRI showing\nsegmentations as accurate as those obtained by expert cardiologists. Thus far,\nthe existing models have been overwhelmingly developed and evaluated with\nLGE-MRI datasets from single clinical centers. However, in practice, LGE-MRI\nimages vary significantly between clinical centers within and across countries,\nin particular due to differences in the MRI scanners, imaging conditions,\ncontrast injection protocols and local clinical practise. This work\ninvestigates for the first time multi-center and multi-vendor LV segmentation\nin LGE-MRI, by proposing, implementing and evaluating in detail several\nstrategies to enhance model generalizability across clinical cites. These\ninclude data augmentation to artificially augment the image variability in the\ntraining sample, image harmonization to align the distributions of LGE-MRI\nimages across centers, and transfer learning to adjust existing single-center\nmodels to unseen images from new clinical sites. The results obtained based on\na new multi-center LGE-MRI dataset acquired in four clinical centers in Spain,\nFrance and China, show that the combination of data augmentation and transfer\nlearning can lead to single-center models that generalize well to new clinical\ncenters not included in the original training. The proposed framework shows the\npotential for developing clinical tools for automated LV segmentation in\nLGE-MRI that can be deployed in multiple clinical centers across distinct\ngeographical locations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Sendra_Balcells_C/0/1/0/all/0/1\">Carla Sendra-Balcells</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Campello_V/0/1/0/all/0/1\">V&#xed;ctor M. Campello</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Martin_Isla_C/0/1/0/all/0/1\">Carlos Mart&#xed;n-Isla</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Medel_D/0/1/0/all/0/1\">David Vilades Medel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Descalzo_M/0/1/0/all/0/1\">Mart&#xed;n Lu&#xed;s Descalzo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guala_A/0/1/0/all/0/1\">Andrea Guala</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Palomares_J/0/1/0/all/0/1\">Jos&#xe9; F. Rodr&#xed;guez Palomares</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lekadir_K/0/1/0/all/0/1\">Karim Lekadir</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multiple Style Transfer via Variational AutoEncoder. (arXiv:2110.07375v1 [cs.CV])","link":"http://arxiv.org/abs/2110.07375","description":"<p>Modern works on style transfer focus on transferring style from a single\nimage. Recently, some approaches study multiple style transfer; these, however,\nare either too slow or fail to mix multiple styles. We propose ST-VAE, a\nVariational AutoEncoder for latent space-based style transfer. It performs\nmultiple style transfer by projecting nonlinear styles to a linear latent\nspace, enabling to merge styles via linear interpolation before transferring\nthe new style to the content image. To evaluate ST-VAE, we experiment on COCO\nfor single and multiple style transfer. We also present a case study revealing\nthat ST-VAE outperforms other methods while being faster, flexible, and setting\na new path for multiple style transfer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhi-Song Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalogeiton_V/0/1/0/all/0/1\">Vicky Kalogeiton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cani_M/0/1/0/all/0/1\">Marie-Paule Cani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain Adaptation on Semantic Segmentation with Separate Affine Transformation in Batch Normalization. (arXiv:2110.07376v1 [cs.CV])","link":"http://arxiv.org/abs/2110.07376","description":"<p>In recent years, unsupervised domain adaptation (UDA) for semantic\nsegmentation has brought many researchers'attention. Many of them take an\napproach to design a complex system so as to better align the gap between\nsource and target domain. Instead, we focus on the very basic structure of the\ndeep neural network, Batch Normalization, and propose to replace the Sharing\nAffine Transformation with our proposed Separate Affine Transformation (SEAT).\nThe proposed SEAT is simple, easily implemented and easy to integrate into\nexisting adversarial learning based UDA methods. Also, to further improve the\nadaptation quality, we introduce multi level adaptation by adding the\nlower-level features to the higher-level ones before feeding them to the\ndiscriminator, without adding extra discriminator like others. Experiments show\nthat the proposed methods is less complex without losing performance accuracy\nwhen compared with other UDA methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Junhao Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_W/0/1/0/all/0/1\">Woonsok Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Safer Transportation: a self-supervised learning approach for traffic video deraining. (arXiv:2110.07379v1 [cs.CV])","link":"http://arxiv.org/abs/2110.07379","description":"<p>Video monitoring of traffic is useful for traffic management and control,\ntraffic counting, and traffic law enforcement. However, traffic monitoring\nduring inclement weather such as rain is a challenging task because video\nquality is corrupted by streaks of falling rain on the video image, and this\nhinders reliable characterization not only of the road environment but also of\nroad-user behavior during such adverse weather events. This study proposes a\ntwo-stage self-supervised learning method to remove rain streaks in traffic\nvideos. The first and second stages address intra- and inter-frame noise,\nrespectively. The results indicated that the model exhibits satisfactory\nperformance in terms of the image visual quality and the Peak Signal-Noise\nRatio value.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zong_S/0/1/0/all/0/1\">Shuya Zong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Sikai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labi_S/0/1/0/all/0/1\">Samuel Labi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reason induced visual attention for explainable autonomous driving. (arXiv:2110.07380v1 [cs.CV])","link":"http://arxiv.org/abs/2110.07380","description":"<p>Deep learning (DL) based computer vision (CV) models are generally considered\nas black boxes due to poor interpretability. This limitation impedes efficient\ndiagnoses or predictions of system failure, thereby precluding the widespread\ndeployment of DLCV models in safety-critical tasks such as autonomous driving.\nThis study is motivated by the need to enhance the interpretability of DL model\nin autonomous driving and therefore proposes an explainable DL-based framework\nthat generates textual descriptions of the driving environment and makes\nappropriate decisions based on the generated descriptions. The proposed\nframework imitates the learning process of human drivers by jointly modeling\nthe visual input (images) and natural language, while using the language to\ninduce the visual attention in the image. The results indicate strong\nexplainability of autonomous driving decisions obtained by focusing on relevant\nfeatures from visual inputs. Furthermore, the output attention maps enhance the\ninterpretability of the model not only by providing meaningful explanation to\nthe model behavior but also by identifying the weakness of and potential\nimprovement directions for the model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Sikai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Jiqian Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_R/0/1/0/all/0/1\">Runjia Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yujie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labi_S/0/1/0/all/0/1\">Samuel Labi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Learning by Estimating Twin Class Distributions. (arXiv:2110.07402v1 [cs.CV])","link":"http://arxiv.org/abs/2110.07402","description":"<p>We present TWIST, a novel self-supervised representation learning method by\nclassifying large-scale unlabeled datasets in an end-to-end way. We employ a\nsiamese network terminated by a softmax operation to produce twin class\ndistributions of two augmented images. Without supervision, we enforce the\nclass distributions of different augmentations to be consistent. In the\nmeantime, we regularize the class distributions to make them sharp and diverse.\nSpecifically, we minimize the entropy of the distribution for each sample to\nmake the class prediction for each sample assertive and maximize the entropy of\nthe mean distribution to make the predictions of different samples diverse. In\nthis way, TWIST can naturally avoid the trivial solutions without specific\ndesigns such as asymmetric network, stop-gradient operation, or momentum\nencoder. Different from the clustering-based methods which alternate between\nclustering and learning, our method is a single learning process guided by a\nunified loss function. As a result, TWIST outperforms state-of-the-art methods\non a wide range of tasks, including unsupervised classification, linear\nclassification, semi-supervised learning, transfer learning, and some dense\nprediction tasks such as detection and segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Feng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_T/0/1/0/all/0/1\">Tao Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rufeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Huaping Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RGB-D Image Inpainting Using Generative Adversarial Network with a Late Fusion Approach. (arXiv:2110.07413v1 [cs.CV])","link":"http://arxiv.org/abs/2110.07413","description":"<p>Diminished reality is a technology that aims to remove objects from video\nimages and fills in the missing region with plausible pixels. Most conventional\nmethods utilize the different cameras that capture the same scene from\ndifferent viewpoints to allow regions to be removed and restored. In this\npaper, we propose an RGB-D image inpainting method using generative adversarial\nnetwork, which does not require multiple cameras. Recently, an RGB image\ninpainting method has achieved outstanding results by employing a generative\nadversarial network. However, RGB inpainting methods aim to restore only the\ntexture of the missing region and, therefore, does not recover geometric\ninformation (i.e, 3D structure of the scene). We expand conventional image\ninpainting method to RGB-D image inpainting to jointly restore the texture and\ngeometry of missing regions from a pair of RGB and depth images. Inspired by\nother tasks that use RGB and depth images (e.g., semantic segmentation and\nobject detection), we propose late fusion approach that exploits the advantage\nof RGB and depth information each other. The experimental results verify the\neffectiveness of our proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fujii_R/0/1/0/all/0/1\">Ryo Fujii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hachiuma_R/0/1/0/all/0/1\">Ryo Hachiuma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saito_H/0/1/0/all/0/1\">Hideo Saito</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Modeling of Social Concepts Evoked by Art Images as Multimodal Frames. (arXiv:2110.07420v1 [cs.CV])","link":"http://arxiv.org/abs/2110.07420","description":"<p>Social concepts referring to non-physical objects--such as revolution,\nviolence, or friendship--are powerful tools to describe, index, and query the\ncontent of visual data, including ever-growing collections of art images from\nthe Cultural Heritage (CH) field. While much progress has been made towards\ncomplete image understanding in computer vision, automatic detection of social\nconcepts evoked by images is still a challenge. This is partly due to the\nwell-known semantic gap problem, worsened for social concepts given their lack\nof unique physical features, and reliance on more unspecific features than\nconcrete concepts. In this paper, we propose the translation of recent\ncognitive theories about social concept representation into a software approach\nto represent them as multimodal frames, by integrating multisensory data. Our\nmethod focuses on the extraction, analysis, and integration of multimodal\nfeatures from visual art material tagged with the concepts of interest. We\ndefine a conceptual model and present a novel ontology for formally\nrepresenting social concepts as multimodal frames. Taking the Tate Gallery's\ncollection as an empirical basis, we experiment our method on a corpus of art\nimages to provide a proof of concept of its potential. We discuss further\ndirections of research, and provide all software, data sources, and results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pandiani_D/0/1/0/all/0/1\">Delfina Sol Martinez Pandiani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Presutti_V/0/1/0/all/0/1\">Valentina Presutti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Possibilistic Fuzzy Local Information C-Means with Automated Feature Selection for Seafloor Segmentation. (arXiv:2110.07433v1 [cs.CV])","link":"http://arxiv.org/abs/2110.07433","description":"<p>The Possibilistic Fuzzy Local Information C-Means (PFLICM) method is\npresented as a technique to segment side-look synthetic aperture sonar (SAS)\nimagery into distinct regions of the sea-floor. In this work, we investigate\nand present the results of an automated feature selection approach for SAS\nimage segmentation. The chosen features and resulting segmentation from the\nimage will be assessed based on a select quantitative clustering validity\ncriterion and the subset of the features that reach a desired threshold will be\nused for the segmentation process.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peeples_J/0/1/0/all/0/1\">Joshua Peeples</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suen_D/0/1/0/all/0/1\">Daniel Suen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zare_A/0/1/0/all/0/1\">Alina Zare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keller_J/0/1/0/all/0/1\">James Keller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inverse Problems Leveraging Pre-trained Contrastive Representations. (arXiv:2110.07439v1 [cs.LG])","link":"http://arxiv.org/abs/2110.07439","description":"<p>We study a new family of inverse problems for recovering representations of\ncorrupted data. We assume access to a pre-trained representation learning\nnetwork R(x) that operates on clean images, like CLIP. The problem is to\nrecover the representation of an image R(x), if we are only given a corrupted\nversion A(x), for some known forward operator A. We propose a supervised\ninversion method that uses a contrastive objective to obtain excellent\nrepresentations for highly corrupted images. Using a linear probe on our robust\nrepresentations, we achieve a higher accuracy than end-to-end supervised\nbaselines when classifying images with various types of distortions, including\nblurring, additive noise, and random pixel masking. We evaluate on a subset of\nImageNet and observe that our method is robust to varying levels of distortion.\nOur method outperforms end-to-end baselines even with a fraction of the labeled\ndata in a wide range of forward operators.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ravula_S/0/1/0/all/0/1\">Sriram Ravula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smyrnis_G/0/1/0/all/0/1\">Georgios Smyrnis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1\">Matt Jordan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dimakis_A/0/1/0/all/0/1\">Alexandros G. Dimakis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Capacity of Group-invariant Linear Readouts from Equivariant Representations: How Many Objects can be Linearly Classified Under All Possible Views?. (arXiv:2110.07472v1 [cs.LG])","link":"http://arxiv.org/abs/2110.07472","description":"<p>Equivariance has emerged as a desirable property of representations of\nobjects subject to identity-preserving transformations that constitute a group,\nsuch as translations and rotations. However, the expressivity of a\nrepresentation constrained by group equivariance is still not fully understood.\nWe address this gap by providing a generalization of Cover's Function Counting\nTheorem that quantifies the number of linearly separable and group-invariant\nbinary dichotomies that can be assigned to equivariant representations of\nobjects. We find that the fraction of separable dichotomies is determined by\nthe dimension of the space that is fixed by the group action. We show how this\nrelation extends to operations such as convolutions, element-wise\nnonlinearities, and global and local pooling. While other operations do not\nchange the fraction of separable dichotomies, local pooling decreases the\nfraction, despite being a highly nonlinear operation. Finally, we test our\ntheory on intermediate representations of randomly initialized and fully\ntrained convolutional neural networks and find perfect agreement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Farrell_M/0/1/0/all/0/1\">Matthew Farrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bordelon_B/0/1/0/all/0/1\">Blake Bordelon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trivedi_S/0/1/0/all/0/1\">Shubhendu Trivedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pehlevan_C/0/1/0/all/0/1\">Cengiz Pehlevan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simple Baseline for Single Human Motion Forecasting. (arXiv:2110.07495v1 [cs.CV])","link":"http://arxiv.org/abs/2110.07495","description":"<p>Global human motion forecasting is important in many fields, which is the\ncombination of global human trajectory prediction and local human pose\nprediction. Visual and social information are often used to boost model\nperformance, however, they may consume too much computational resource. In this\npaper, we establish a simple but effective baseline for single human motion\nforecasting without visual and social information, equipped with useful\ntraining tricks. Our method \"futuremotion_ICCV21\" outperforms existing methods\nby a large margin on SoMoF benchmark. We hope our work provide new ideas for\nfuture research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chenxi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunfeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zixuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhiwen Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TDACNN: Target-domain-free Domain Adaptation Convolutional Neural Network for Drift Compensation in Gas Sensors. (arXiv:2110.07509v1 [q-bio.QM])","link":"http://arxiv.org/abs/2110.07509","description":"<p>Sensor drift is a long-existing unpredictable problem that deteriorates the\nperformance of gaseous substance recognition, calling for an antidrift domain\nadaptation algorithm. However, the prerequisite for traditional methods to\nachieve fine results is to have data from both nondrift distributions (source\ndomain) and drift distributions (target domain) for domain alignment, which is\nusually unrealistic and unachievable in real-life scenarios. To compensate for\nthis, in this paper, deep learning based on a target-domain-free domain\nadaptation convolutional neural network (TDACNN) is proposed. The main concept\nis that CNNs extract not only the domain-specific features of samples but also\nthe domain-invariant features underlying both the source and target domains.\nMaking full use of these various levels of embedding features can lead to\ncomprehensive utilization of different levels of characteristics, thus\nachieving drift compensation by the extracted intermediate features between two\ndomains. In the TDACNN, a flexible multibranch backbone with a multiclassifier\nstructure is proposed under the guidance of bionics, which utilizes multiple\nembedding features comprehensively without involving target domain data during\ntraining. A classifier ensemble method based on maximum mean discrepancy (MMD)\nis proposed to evaluate all the classifiers jointly based on the credibility of\nthe pseudolabel. To optimize network training, an additive angular margin\nsoftmax loss with parameter dynamic adjustment is utilized. Experiments on two\ndrift datasets under different settings demonstrate the superiority of TDACNN\ncompared with several state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuelin Zhang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Yan_J/0/1/0/all/0/1\">Jia Yan</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Wanga_Z/0/1/0/all/0/1\">Zehuan Wanga</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Peng_X/0/1/0/all/0/1\">Xiaoyan Peng</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Tian_Y/0/1/0/all/0/1\">Yutong Tian</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Duan_S/0/1/0/all/0/1\">Shukai Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Proposal Extension with Sequential Network for Weakly Supervised Object Detection. (arXiv:2110.07511v1 [cs.CV])","link":"http://arxiv.org/abs/2110.07511","description":"<p>Weakly supervised object detection (WSOD) has attracted more and more\nattention since it only uses image-level labels and can save huge annotation\ncosts. Most of the WSOD methods use Multiple Instance Learning (MIL) as their\nbasic framework, which regard it as an instance classification problem.\nHowever, these methods based on MIL tends to converge only on the most\ndiscriminate regions of different instances, rather than their corresponding\ncomplete regions, that is, insufficient integrity. Inspired by the habit of\nobserving things by the human, we propose a new method by comparing the initial\nproposals and the extension ones to optimize those initial proposals.\nSpecifically, we propose one new strategy for WSOD by involving contrastive\nproposal extension (CPE), which consists of multiple directional contrastive\nproposal extensions (D-CPE), and each D-CPE contains encoders based on LSTM\nnetwork and corresponding decoders. %\\textcolor{red}{with temporal network}.\n</p>\n<p>Firstly, the boundary of initial proposals in MIL is extended to different\npositions according to well-designed sequential order. Then, CPE compares the\nextended proposal and the initial proposal by extracting the feature semantics\nof them using the encoders, and calculates the integrity of the initial\nproposal to optimize the score of the initial proposal.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lv_P/0/1/0/all/0/1\">Pei Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Suqi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_T/0/1/0/all/0/1\">Tianran Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Haohan Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1\">Lisha Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1\">Haoyi Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mingliang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Changsheng Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spoken ObjectNet: A Bias-Controlled Spoken Caption Dataset. (arXiv:2110.07575v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07575","description":"<p>Visually-grounded spoken language datasets can enable models to learn\ncross-modal correspondences with very weak supervision. However, modern\naudio-visual datasets contain biases that undermine the real-world performance\nof models trained on that data. We introduce Spoken ObjectNet, which is\ndesigned to remove some of these biases and provide a way to better evaluate\nhow effectively models will perform in real-world scenarios. This dataset\nexpands upon ObjectNet, which is a bias-controlled image dataset that features\nsimilar image classes to those present in ImageNet. We detail our data\ncollection pipeline, which features several methods to improve caption quality,\nincluding automated language model checks. Lastly, we show baseline results on\nimage retrieval and audio retrieval tasks. These results show that models\ntrained on other datasets and then evaluated on Spoken ObjectNet tend to\nperform poorly due to biases in other datasets that the models have learned. We\nalso show evidence that the performance decrease is due to the dataset\ncontrols, and not the transfer setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Palmer_I/0/1/0/all/0/1\">Ian Palmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rouditchenko_A/0/1/0/all/0/1\">Andrew Rouditchenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barbu_A/0/1/0/all/0/1\">Andrei Barbu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katz_B/0/1/0/all/0/1\">Boris Katz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glass_J/0/1/0/all/0/1\">James Glass</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Temporal 3D Human Pose Estimation with Pseudo-Labels. (arXiv:2110.07578v1 [cs.CV])","link":"http://arxiv.org/abs/2110.07578","description":"<p>We present a simple, yet effective, approach for self-supervised 3D human\npose estimation. Unlike the prior work, we explore the temporal information\nnext to the multi-view self-supervision. During training, we rely on\ntriangulating 2D body pose estimates of a multiple-view camera system. A\ntemporal convolutional neural network is trained with the generated 3D\nground-truth and the geometric multi-view consistency loss, imposing\ngeometrical constraints on the predicted 3D body skeleton. During inference,\nour model receives a sequence of 2D body pose estimates from a single-view to\npredict the 3D body pose for each of them. An extensive evaluation shows that\nour method achieves state-of-the-art performance in the Human3.6M and\nMPI-INF-3DHP benchmarks. Our code and models are publicly available at\n\\url{https://github.com/vru2020/TM_HPE/}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bouazizi_A/0/1/0/all/0/1\">Arij Bouazizi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kressel_U/0/1/0/all/0/1\">Ulrich Kressel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belagiannis_V/0/1/0/all/0/1\">Vasileios Belagiannis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Playing for 3D Human Recovery. (arXiv:2110.07588v1 [cs.CV])","link":"http://arxiv.org/abs/2110.07588","description":"<p>Image- and video-based 3D human recovery (i.e. pose and shape estimation)\nhave achieved substantial progress. However, due to the prohibitive cost of\nmotion capture, existing datasets are often limited in scale and diversity,\nwhich hinders the further development of more powerful models. In this work, we\nobtain massive human sequences as well as their 3D ground truths by playing\nvideo games. Specifically, we contribute, GTA-Human, a mega-scale and\nhighly-diverse 3D human dataset generated with the GTA-V game engine. With a\nrich set of subjects, actions, and scenarios, GTA-Human serves as both an\neffective training source. Notably, the \"unreasonable effectiveness of data\"\nphenomenon is validated in 3D human recovery using our game-playing data. A\nsimple frame-based baseline trained on GTA-Human already outperforms more\nsophisticated methods by a large margin; for video-based methods, GTA-Human\ndemonstrates superiority over even the in-domain training set. We extend our\nstudy to larger models to observe the same consistent improvements, and the\nstudy on supervision signals suggests the rich collection of SMPL annotations\nis key. Furthermore, equipped with the diverse annotations in GTA-Human, we\nsystematically investigate the performance of various methods under a wide\nspectrum of real-world variations, e.g. camera angles, poses, and occlusions.\nWe hope our work could pave way for scaling up 3D human recovery to the real\nworld.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1\">Zhongang Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mingyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Jiawei Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1\">Chen Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_D/0/1/0/all/0/1\">Daxuan Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiatong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhengyu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Haiyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_S/0/1/0/all/0/1\">Shuai Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1\">Chen Change Loy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sub-word Level Lip Reading With Visual Attention. (arXiv:2110.07603v1 [cs.CV])","link":"http://arxiv.org/abs/2110.07603","description":"<p>The goal of this paper is to learn strong lip reading models that can\nrecognise speech in silent videos. Most prior works deal with the open-set\nvisual speech recognition problem by adapting existing automatic speech\nrecognition techniques on top of trivially pooled visual features. Instead, in\nthis paper we focus on the unique challenges encountered in lip reading and\npropose tailored solutions. To that end we make the following contributions:\n(1) we propose an attention-based pooling mechanism to aggregate visual speech\nrepresentations; (2) we use sub-word units for lip reading for the first time\nand show that this allows us to better model the ambiguities of the task; (3)\nwe propose a training pipeline that balances the lip reading performance with\nother key factors such as data and compute efficiency. Following the above, we\nobtain state-of-the-art results on the challenging LRS2 and LRS3 benchmarks\nwhen training on public datasets, and even surpass models trained on\nlarge-scale industrial datasets by using an order of magnitude less data. Our\nbest model achieves 22.6% word error rate on the LRS2 dataset, a performance\nunprecedented for lip reading models, significantly reducing the performance\ngap between lip reading and automatic speech recognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+R_P/0/1/0/all/0/1\">Prajwal K R</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Afouras_T/0/1/0/all/0/1\">Triantafyllos Afouras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1\">Andrew Zisserman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NeRS: Neural Reflectance Surfaces for Sparse-view 3D Reconstruction in the Wild. (arXiv:2110.07604v1 [cs.CV])","link":"http://arxiv.org/abs/2110.07604","description":"<p>Recent history has seen a tremendous growth of work exploring implicit\nrepresentations of geometry and radiance, popularized through Neural Radiance\nFields (NeRF). Such works are fundamentally based on a (implicit) {\\em\nvolumetric} representation of occupancy, allowing them to model diverse scene\nstructure including translucent objects and atmospheric obscurants. But because\nthe vast majority of real-world scenes are composed of well-defined surfaces,\nwe introduce a {\\em surface} analog of such implicit models called Neural\nReflectance Surfaces (NeRS). NeRS learns a neural shape representation of a\nclosed surface that is diffeomorphic to a sphere, guaranteeing water-tight\nreconstructions. Even more importantly, surface parameterizations allow NeRS to\nlearn (neural) bidirectional surface reflectance functions (BRDFs) that\nfactorize view-dependent appearance into environmental illumination, diffuse\ncolor (albedo), and specular \"shininess.\" Finally, rather than illustrating our\nresults on synthetic scenes or controlled in-the-lab capture, we assemble a\nnovel dataset of multi-view images from online marketplaces for selling goods.\nSuch \"in-the-wild\" multi-view image sets pose a number of challenges, including\na small number of views with unknown/rough camera estimates. We demonstrate\nthat surface-based neural reconstructions enable learning from such data,\noutperforming volumetric neural rendering-based reconstructions. We hope that\nNeRS serves as a first step toward building scalable, high-quality libraries of\nreal-world shape, materials, and illumination. The project page with code and\nvideo visualizations can be found at\nhttps://jasonyzhang.com/ners}{jasonyzhang.com/ners.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jason Y. Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1\">Gengshan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tulsiani_S/0/1/0/all/0/1\">Shubham Tulsiani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanan_D/0/1/0/all/0/1\">Deva Ramanan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Plan-Recognition-Driven Attention Modeling for Visual Recognition. (arXiv:1812.00301v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1812.00301","description":"<p>Human visual recognition of activities or external agents involves an\ninterplay between high-level plan recognition and low-level perception. Given\nthat, a natural question to ask is: can low-level perception be improved by\nhigh-level plan recognition? We formulate the problem of leveraging recognized\nplans to generate better top-down attention maps\n\\cite{gazzaniga2009,baluch2011} to improve the perception performance. We call\nthese top-down attention maps specifically as plan-recognition-driven attention\nmaps. To address this problem, we introduce the Pixel Dynamics Network. Pixel\nDynamics Network serves as an observation model, which predicts next states of\nobject points at each pixel location given observation of pixels and\npixel-level action feature. This is like internally learning a pixel-level\ndynamics model. Pixel Dynamics Network is a kind of Convolutional Neural\nNetwork (ConvNet), with specially-designed architecture. Therefore, Pixel\nDynamics Network could take the advantage of parallel computation of ConvNets,\nwhile learning the pixel-level dynamics model. We further prove the equivalence\nbetween Pixel Dynamics Network as an observation model, and the belief update\nin partially observable Markov decision process (POMDP) framework. We evaluate\nour Pixel Dynamics Network in event recognition tasks. We build an event\nrecognition system, ER-PRN, which takes Pixel Dynamics Network as a subroutine,\nto recognize events based on observations augmented by plan-recognition-driven\nattention.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zha_Y/0/1/0/all/0/1\">Yantian Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yikang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tianshu Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kambhampati_S/0/1/0/all/0/1\">Subbarao Kambhampati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Baoxin Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Drone-based RGB-Infrared Cross-Modality Vehicle Detection via Uncertainty-Aware Learning. (arXiv:2003.02437v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2003.02437","description":"<p>Drone-based vehicle detection aims at finding the vehicle locations and\ncategories in an aerial image. It empowers smart city traffic management and\ndisaster rescue. Researchers have made mount of efforts in this area and\nachieved considerable progress. Nevertheless, it is still a challenge when the\nobjects are hard to distinguish, especially in low light conditions. To tackle\nthis problem, we construct a large-scale drone-based RGB-Infrared vehicle\ndetection dataset, termed DroneVehicle. Our DroneVehicle collects 28, 439\nRGB-Infrared image pairs, covering urban roads, residential areas, parking\nlots, and other scenarios from day to night. Due to the great gap between RGB\nand infrared images, cross-modal images provide both effective information and\nredundant information. To address this dilemma, we further propose an\nuncertainty-aware cross-modality vehicle detection (UA-CMDet) framework to\nextract complementary information from cross-modal images, which can\nsignificantly improve the detection performance in low light conditions. An\nuncertainty-aware module (UAM) is designed to quantify the uncertainty weights\nof each modality, which is calculated by the cross-modal Intersection over\nUnion (IoU) and the RGB illumination value. Furthermore, we design an\nillumination-aware cross-modal non-maximum suppression algorithm to better\nintegrate the modal-specific information in the inference phase. Extensive\nexperiments on the DroneVehicle dataset demonstrate the flexibility and\neffectiveness of the proposed method for crossmodality vehicle detection. The\ndataset can be download from https://github.com/VisDrone/DroneVehicle.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yiming Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_B/0/1/0/all/0/1\">Bing Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_P/0/1/0/all/0/1\">Pengfei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1\">Qinghua Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Spatial-Temporal Attentive Network with Spatial Continuity for Trajectory Prediction. (arXiv:2003.06107v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2003.06107","description":"<p>It remains challenging to automatically predict the multi-agent trajectory\ndue to multiple interactions including agent to agent interaction and scene to\nagent interaction. Although recent methods have achieved promising performance,\nmost of them just consider spatial influence of the interactions and ignore the\nfact that temporal influence always accompanies spatial influence. Moreover,\nthose methods based on scene information always require extra segmented scene\nimages to generate multiple socially acceptable trajectories. To solve these\nlimitations, we propose a novel model named spatial-temporal attentive network\nwith spatial continuity (STAN-SC). First, spatial-temporal attention mechanism\nis presented to explore the most useful and important information. Second, we\nconduct a joint feature sequence based on the sequence and instant state\ninformation to make the generative trajectories keep spatial continuity.\nExperiments are performed on the two widely used ETH-UCY datasets and\ndemonstrate that the proposed model achieves state-of-the-art prediction\naccuracy and handles more complex scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xia_B/0/1/0/all/0/1\">Beihao Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Conghao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Q/0/1/0/all/0/1\">Qinmu Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_X/0/1/0/all/0/1\">Xinge You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Robustness of Deep Sensor Fusion Models. (arXiv:2006.13192v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2006.13192","description":"<p>We experimentally study the robustness of deep camera-LiDAR fusion\narchitectures for 2D object detection in autonomous driving. First, we find\nthat the fusion model is usually both more accurate, and more robust against\nsingle-source attacks than single-sensor deep neural networks. Furthermore, we\nshow that without adversarial training, early fusion is more robust than late\nfusion, whereas the two perform similarly after adversarial training. However,\nwe note that single-channel adversarial training of deep fusion is often\ndetrimental even to robustness. Moreover, we observe cross-channel\nexternalities, where single-channel adversarial training reduces robustness to\nattacks on the other channel. Additionally, we observe that the choice of\nadversarial model in adversarial training is critical: using attacks restricted\nto cars' bounding boxes is more effective in adversarial training and exhibits\nless significant cross-channel externalities. Finally, we find that\njoint-channel adversarial training helps mitigate many of the issues above, but\ndoes not significantly boost adversarial robustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shaojie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakrabarti_A/0/1/0/all/0/1\">Ayan Chakrabarti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vorobeychik_Y/0/1/0/all/0/1\">Yevgeniy Vorobeychik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spatial-Angular Attention Network for Light Field Reconstruction. (arXiv:2007.02252v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2007.02252","description":"<p>Typical learning-based light field reconstruction methods demand in\nconstructing a large receptive field by deepening the network to capture\ncorrespondences between input views. In this paper, we propose a\nspatial-angular attention network to perceive correspondences in the light\nfield non-locally, and reconstruction high angular resolution light field in an\nend-to-end manner. Motivated by the non-local attention mechanism, a\nspatial-angular attention module specifically for the high-dimensional light\nfield data is introduced to compute the responses from all the positions in the\nepipolar plane for each pixel in the light field, and generate an attention map\nthat captures correspondences along the angular dimension. We then propose a\nmulti-scale reconstruction structure to efficiently implement the non-local\nattention in the low spatial scale, while also preserving the high frequency\ncomponents in the high spatial scales. Extensive experiments demonstrate the\nsuperior performance of the proposed spatial-angular attention network for\nreconstructing sparsely-sampled light fields with non-Lambertian effects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wu_G/0/1/0/all/0/1\">Gaochang Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yingqian Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yebin Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fang_L/0/1/0/all/0/1\">Lu Fang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chai_T/0/1/0/all/0/1\">Tianyou Chai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unseen Object Instance Segmentation for Robotic Environments. (arXiv:2007.08073v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2007.08073","description":"<p>In order to function in unstructured environments, robots need the ability to\nrecognize unseen objects. We take a step in this direction by tackling the\nproblem of segmenting unseen object instances in tabletop environments.\nHowever, the type of large-scale real-world dataset required for this task\ntypically does not exist for most robotic settings, which motivates the use of\nsynthetic data. Our proposed method, UOIS-Net, separately leverages synthetic\nRGB and synthetic depth for unseen object instance segmentation. UOIS-Net is\ncomprised of two stages: first, it operates only on depth to produce object\ninstance center votes in 2D or 3D and assembles them into rough initial masks.\nSecondly, these initial masks are refined using RGB. Surprisingly, our\nframework is able to learn from synthetic RGB-D data where the RGB is\nnon-photorealistic. To train our method, we introduce a large-scale synthetic\ndataset of random objects on tabletops. We show that our method can produce\nsharp and accurate segmentation masks, outperforming state-of-the-art methods\non unseen object instance segmentation. We also show that our method can\nsegment unseen objects for robot grasping.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1\">Christopher Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_Y/0/1/0/all/0/1\">Yu Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mousavian_A/0/1/0/all/0/1\">Arsalan Mousavian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fox_D/0/1/0/all/0/1\">Dieter Fox</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LT4REC:A Lottery Ticket Hypothesis Based Multi-task Practice for Video Recommendation System. (arXiv:2008.09872v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2008.09872","description":"<p>Click-through rate prediction (CTR) and post-click conversion rate prediction\n(CVR) play key roles across all industrial ranking systems, such as\nrecommendation systems, online advertising, and search engines. Different from\nthe extensive research on CTR, there is much less research on CVR estimation,\nwhose main challenge is extreme data sparsity with one or two orders of\nmagnitude reduction in the number of samples than CTR. People try to solve this\nproblem with the paradigm of multi-task learning with the sufficient samples of\nCTR, but the typical hard sharing method can't effectively solve this problem,\nbecause it is difficult to analyze which parts of network components can be\nshared and which parts are in conflict, i.e., there is a large inaccuracy with\nartificially designed neurons sharing. In this paper, we model CVR in a\nbrand-new method by adopting the lottery-ticket-hypothesis-based sparse sharing\nmulti-task learning, which can automatically and flexibly learn which neuron\nweights to be shared without artificial experience. Experiments on the dataset\ngathered from traffic logs of Tencent video's recommendation system demonstrate\nthat sparse sharing in the CVR model significantly outperforms competitive\nmethods. Due to the nature of weight sparsity in sparse sharing, it can also\nsignificantly reduce computational complexity and memory usage which are very\nimportant in the industrial recommendation system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1\">Xuanji Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huabin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuzhen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_X/0/1/0/all/0/1\">Xing Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Pei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1\">Chaosheng Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_N/0/1/0/all/0/1\">Nian Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xirong Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain-invariant Similarity Activation Map Contrastive Learning for Retrieval-based Long-term Visual Localization. (arXiv:2009.07719v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2009.07719","description":"<p>Visual localization is a crucial component in the application of mobile robot\nand autonomous driving. Image retrieval is an efficient and effective technique\nin image-based localization methods. Due to the drastic variability of\nenvironmental conditions, e.g. illumination, seasonal and weather changes,\nretrieval-based visual localization is severely affected and becomes a\nchallenging problem. In this work, a general architecture is first formulated\nprobabilistically to extract domain invariant feature through multi-domain\nimage translation. And then a novel gradient-weighted similarity activation\nmapping loss (Grad-SAM) is incorporated for finer localization with high\naccuracy. We also propose a new adaptive triplet loss to boost the contrastive\nlearning of the embedding in a self-supervised manner. The final coarse-to-fine\nimage retrieval pipeline is implemented as the sequential combination of models\nwithout and with Grad-SAM loss. Extensive experiments have been conducted to\nvalidate the effectiveness of the proposed approach on the CMUSeasons dataset.\nThe strong generalization ability of our approach is verified on RobotCar\ndataset using models pre-trained on urban part of CMU-Seasons dataset. Our\nperformance is on par with or even outperforms the state-of-the-art image-based\nlocalization baselines in medium or high precision, especially under the\nchallenging environments with illumination variance, vegetation and night-time\nimages. The code and pretrained models are available on\nhttps://github.com/HanjiangHu/DISAM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Hanjiang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hesheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhe Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weidong Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Point Cloud Pre-Training via Occlusion Completion. (arXiv:2010.01089v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2010.01089","description":"<p>We describe a simple pre-training approach for point clouds. It works in\nthree steps: 1. Mask all points occluded in a camera view; 2. Learn an\nencoder-decoder model to reconstruct the occluded points; 3. Use the encoder\nweights as initialisation for downstream point cloud tasks. We find that even\nwhen we construct a single pre-training dataset (from ModelNet40), this\npre-training method improves accuracy across different datasets and encoders,\non a wide range of downstream tasks. Specifically, we show that our method\noutperforms previous pre-training methods in object classification, and both\npart-based and semantic segmentation tasks. We study the pre-trained features\nand find that they lead to wide downstream minima, have high transformation\ninvariance, and have activations that are highly correlated with part labels.\nCode and data are available at: https://github.com/hansen7/OcCo\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hanchen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_X/0/1/0/all/0/1\">Xiangyu Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lasenby_J/0/1/0/all/0/1\">Joan Lasenby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kusner_M/0/1/0/all/0/1\">Matthew J. Kusner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Point Transformer. (arXiv:2011.00931v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.00931","description":"<p>In this work, we present Point Transformer, a deep neural network that\noperates directly on unordered and unstructured point sets. We design Point\nTransformer to extract local and global features and relate both\nrepresentations by introducing the local-global attention mechanism, which aims\nto capture spatial point relations and shape information. For that purpose, we\npropose SortNet, as part of the Point Transformer, which induces input\npermutation invariance by selecting points based on a learned score. The output\nof Point Transformer is a sorted and permutation invariant feature list that\ncan directly be incorporated into common computer vision applications. We\nevaluate our approach on standard classification and part segmentation\nbenchmarks to demonstrate competitive results compared to the prior work. Code\nis publicly available at: https://github.com/engelnico/point-transformer\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Engel_N/0/1/0/all/0/1\">Nico Engel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belagiannis_V/0/1/0/all/0/1\">Vasileios Belagiannis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dietmayer_K/0/1/0/all/0/1\">Klaus Dietmayer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Physics-aware Inference of Cloth Deformation for Monocular Human Performance Capture. (arXiv:2011.12866v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.12866","description":"<p>Recent monocular human performance capture approaches have shown compelling\ndense tracking results of the full body from a single RGB camera. However,\nexisting methods either do not estimate clothing at all or model cloth\ndeformation with simple geometric priors instead of taking into account the\nunderlying physical principles. This leads to noticeable artifacts in their\nreconstructions, e.g. baked-in wrinkles, implausible deformations that\nseemingly defy gravity, and intersections between cloth and body. To address\nthese problems, we propose a person-specific, learning-based method that\nintegrates a simulation layer into the training process to provide for the\nfirst time physics supervision in the context of weakly supervised deep\nmonocular human performance capture. We show how integrating physics into the\ntraining process improves the learned cloth deformations, allows modeling\nclothing as a separate piece of geometry, and largely reduces cloth-body\nintersections. Relying only on weak 2D multi-view supervision during training,\nour approach leads to a significant improvement over current state-of-the-art\nmethods and is thus a clear step towards realistic monocular capture of the\nentire deforming surface of a clothed human.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yue Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Habermann_M/0/1/0/all/0/1\">Marc Habermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomaszewski_B/0/1/0/all/0/1\">Bernhard Thomaszewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coros_S/0/1/0/all/0/1\">Stelian Coros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beeler_T/0/1/0/all/0/1\">Thabo Beeler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1\">Christian Theobalt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Co-mining: Self-Supervised Learning for Sparsely Annotated Object Detection. (arXiv:2012.01950v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.01950","description":"<p>Object detectors usually achieve promising results with the supervision of\ncomplete instance annotations. However, their performance is far from\nsatisfactory with sparse instance annotations. Most existing methods for\nsparsely annotated object detection either re-weight the loss of hard negative\nsamples or convert the unlabeled instances into ignored regions to reduce the\ninterference of false negatives. We argue that these strategies are\ninsufficient since they can at most alleviate the negative effect caused by\nmissing annotations. In this paper, we propose a simple but effective\nmechanism, called Co-mining, for sparsely annotated object detection. In our\nCo-mining, two branches of a Siamese network predict the pseudo-label sets for\neach other. To enhance multi-view learning and better mine unlabeled instances,\nthe original image and corresponding augmented image are used as the inputs of\ntwo branches of the Siamese network, respectively. Co-mining can serve as a\ngeneral training mechanism applied to most of modern object detectors.\nExperiments are performed on MS COCO dataset with three different sparsely\nannotated settings using two typical frameworks: anchor-based detector\nRetinaNet and anchor-free detector FCOS. Experimental results show that our\nCo-mining with RetinaNet achieves 1.4%~2.1% improvements compared with\ndifferent baselines and surpasses existing methods under the same sparsely\nannotated setting. Code is available at\nhttps://github.com/megvii-research/Co-mining.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tiancai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Tong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Jiale Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangyu Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unleashing the Power of Contrastive Self-Supervised Visual Models via Contrast-Regularized Fine-Tuning. (arXiv:2102.06605v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.06605","description":"<p>Contrastive self-supervised learning (CSL) has attracted increasing attention\nfor model pre-training via unlabeled data. The resulted CSL models provide\ninstance-discriminative visual features that are uniformly scattered in the\nfeature space. During deployment, the common practice is to directly fine-tune\nCSL models with cross-entropy, which however may not be the best strategy in\npractice. Although cross-entropy tends to separate inter-class features, the\nresulting models still have limited capability for reducing intra-class feature\nscattering that exists in CSL models. In this paper, we investigate whether\napplying contrastive learning to fine-tuning would bring further benefits, and\nanalytically find that optimizing the contrastive loss benefits both\ndiscriminative representation learning and model optimization during\nfine-tuning. Inspired by these findings, we propose Contrast-regularized tuning\n(Core-tuning), a new approach for fine-tuning CSL models. Instead of simply\nadding the contrastive loss to the objective of fine-tuning, Core-tuning\nfurther applies a novel hard pair mining strategy for more effective\ncontrastive fine-tuning, as well as smoothing the decision boundary to better\nexploit the learned discriminative feature space. Extensive experiments on\nimage classification and semantic segmentation verify the effectiveness of\nCore-tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yifan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hooi_B/0/1/0/all/0/1\">Bryan Hooi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1\">Dapeng Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jian Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jiashi Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spectral Reconstruction and Disparity from Spatio-Spectrally Coded Light Fields via Multi-Task Deep Learning. (arXiv:2103.10179v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.10179","description":"<p>We present a novel method to reconstruct a spectral central view and its\naligned disparity map from spatio-spectrally coded light fields. Since we do\nnot reconstruct an intermediate full light field from the coded measurement, we\nrefer to this as principal reconstruction. The coded light fields correspond to\nthose captured by a light field camera in the unfocused design with a\nspectrally coded microlens array. In this application, the spectrally coded\nlight field camera can be interpreted as a single-shot spectral depth camera.\n</p>\n<p>We investigate several multi-task deep learning methods and propose a new\nauxiliary loss-based training strategy to enhance the reconstruction\nperformance. The results are evaluated using a synthetic as well as a new\nreal-world spectral light field dataset that we captured using a custom-built\ncamera. The results are compared to state-of-the art compressed sensing\nreconstruction and disparity estimation.\n</p>\n<p>We achieve a high reconstruction quality for both synthetic and real-world\ncoded light fields. The disparity estimation quality is on par with or even\noutperforms state-of-the-art disparity estimation from uncoded RGB light\nfields.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schambach_M/0/1/0/all/0/1\">Maximilian Schambach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jiayang Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heizmann_M/0/1/0/all/0/1\">Michael Heizmann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D Point Cloud Registration with Multi-Scale Architecture and Unsupervised Transfer Learning. (arXiv:2103.14533v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.14533","description":"<p>We propose a method for generalizing deep learning for 3D point cloud\nregistration on new, totally different datasets. It is based on two components,\nMS-SVConv and UDGE. Using Multi-Scale Sparse Voxel Convolution, MS-SVConv is a\nfast deep neural network that outputs the descriptors from point clouds for 3D\nregistration between two scenes. UDGE is an algorithm for transferring deep\nnetworks on unknown datasets in a unsupervised way. The interest of the\nproposed method appears while using the two components, MS-SVConv and UDGE,\ntogether as a whole, which leads to state-of-the-art results on real world\nregistration datasets such as 3DMatch, ETH and TUM. The code is publicly\navailable at https://github.com/humanpose1/MS-SVConv .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Horache_S/0/1/0/all/0/1\">Sofiane Horache</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deschaud_J/0/1/0/all/0/1\">Jean-Emmanuel Deschaud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goulette_F/0/1/0/all/0/1\">Fran&#xe7;ois Goulette</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Self-supervised Correspondence Learning: A Video Frame-level Similarity Perspective. (arXiv:2103.17263v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.17263","description":"<p>Learning a good representation for space-time correspondence is the key for\nvarious computer vision tasks, including tracking object bounding boxes and\nperforming video object pixel segmentation. To learn generalizable\nrepresentation for correspondence in large-scale, a variety of self-supervised\npretext tasks are proposed to explicitly perform object-level or patch-level\nsimilarity learning. Instead of following the previous literature, we propose\nto learn correspondence using Video Frame-level Similarity (VFS) learning, i.e,\nsimply learning from comparing video frames. Our work is inspired by the recent\nsuccess in image-level contrastive learning and similarity learning for visual\nrecognition. Our hypothesis is that if the representation is good for\nrecognition, it requires the convolutional features to find correspondence\nbetween similar objects or parts. Our experiments show surprising results that\nVFS surpasses state-of-the-art self-supervised approaches for both OTB visual\nobject tracking and DAVIS video object segmentation. We perform detailed\nanalysis on what matters in VFS and reveals new properties on image and frame\nlevel similarity learning. Project page with code is available at\nhttps://jerryxu.net/VFS\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jiarui Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaolong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Direct-PoseNet: Absolute Pose Regression with Photometric Consistency. (arXiv:2104.04073v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.04073","description":"<p>We present a relocalization pipeline, which combines an absolute pose\nregression (APR) network with a novel view synthesis based direct matching\nmodule, offering superior accuracy while maintaining low inference time. Our\ncontribution is twofold: i) we design a direct matching module that supplies a\nphotometric supervision signal to refine the pose regression network via\ndifferentiable rendering; ii) we modify the rotation representation from the\nclassical quaternion to SO(3) in pose regression, removing the need for\nbalancing rotation and translation loss terms. As a result, our network\nDirect-PoseNet achieves state-of-the-art performance among all other\nsingle-image APR methods on the 7-Scenes benchmark and the LLFF dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shuai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zirui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prisacariu_V/0/1/0/all/0/1\">Victor Prisacariu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Advanced Deep Networks for 3D Mitochondria Instance Segmentation. (arXiv:2104.07961v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.07961","description":"<p>Mitochondria instance segmentation from electron microscopy (EM) images has\nseen notable progress since the introduction of deep learning methods. In this\npaper, we propose two advanced deep networks, named Res-UNet-R and Res-UNet-H,\nfor 3D mitochondria instance segmentation from Rat and Human samples.\nSpecifically, we design a simple yet effective anisotropic convolution block\nand deploy a multi-scale training strategy, which together boost the\nsegmentation performance. Moreover, we enhance the generalizability of the\ntrained models on the test set by adding a denoising operation as\npre-processing. In the Large-scale 3D Mitochondria Instance Segmentation\nChallenge at ISBI 2021, our method ranks the 1st place. Code is available at\nhttps://github.com/Limingxing00/MitoEM2021-Challenge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mingxing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaoyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yueyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Z/0/1/0/all/0/1\">Zhiwei Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Soft Expectation and Deep Maximization for Image Feature Detection. (arXiv:2104.10291v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.10291","description":"<p>Central to the application of many multi-view geometry algorithms is the\nextraction of matching points between multiple viewpoints, enabling classical\ntasks such as camera pose estimation and 3D reconstruction. Many approaches\nthat characterize these points have been proposed based on hand-tuned\nappearance models or data-driven learning methods. We propose Soft Expectation\nand Deep Maximization (SEDM), an iterative unsupervised learning process that\ndirectly optimizes the repeatability of the features by posing the problem in a\nsimilar way to expectation maximization (EM). We found convergence to be\nreliable and the new model to be more lighting invariant and better at localize\nthe underlying 3D points in a scene, improving SfM quality when compared to\nother state of the art deep learning detectors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mai_A/0/1/0/all/0/1\">Alexander Mai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1\">Allen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meyer_D/0/1/0/all/0/1\">Dominique E. Meyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quantization of Deep Neural Networks for Accurate Edge Computing. (arXiv:2104.12046v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.12046","description":"<p>Deep neural networks (DNNs) have demonstrated their great potential in recent\nyears, exceeding the per-formance of human experts in a wide range of\napplications. Due to their large sizes, however, compressiontechniques such as\nweight quantization and pruning are usually applied before they can be\naccommodated onthe edge. It is generally believed that quantization leads to\nperformance degradation, and plenty of existingworks have explored quantization\nstrategies aiming at minimum accuracy loss. In this paper, we argue\nthatquantization, which essentially imposes regularization on weight\nrepresentations, can sometimes help toimprove accuracy. We conduct\ncomprehensive experiments on three widely used applications: fully con-nected\nnetwork (FCN) for biomedical image segmentation, convolutional neural network\n(CNN) for imageclassification on ImageNet, and recurrent neural network (RNN)\nfor automatic speech recognition, and experi-mental results show that\nquantization can improve the accuracy by 1%, 1.95%, 4.23% on the three\napplicationsrespectively with 3.5x-6.4x memory reduction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wentao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_H/0/1/0/all/0/1\">Hailong Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_J/0/1/0/all/0/1\">Jian Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chutong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Q/0/1/0/all/0/1\">Qing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianchen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yiyu Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Meiping Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiaowe Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ResT: An Efficient Transformer for Visual Recognition. (arXiv:2105.13677v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.13677","description":"<p>This paper presents an efficient multi-scale vision Transformer, called ResT,\nthat capably served as a general-purpose backbone for image recognition. Unlike\nexisting Transformer methods, which employ standard Transformer blocks to\ntackle raw images with a fixed resolution, our ResT have several advantages:\n(1) A memory-efficient multi-head self-attention is built, which compresses the\nmemory by a simple depth-wise convolution, and projects the interaction across\nthe attention-heads dimension while keeping the diversity ability of\nmulti-heads; (2) Position encoding is constructed as spatial attention, which\nis more flexible and can tackle with input images of arbitrary size without\ninterpolation or fine-tune; (3) Instead of the straightforward tokenization at\nthe beginning of each stage, we design the patch embedding as a stack of\noverlapping convolution operation with stride on the 2D-reshaped token map. We\ncomprehensively validate ResT on image classification and downstream tasks.\nExperimental results show that the proposed ResT can outperform the recently\nstate-of-the-art backbones by a large margin, demonstrating the potential of\nResT as strong backbones. The code and models will be made publicly available\nat https://github.com/wofmanaf/ResT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qinglong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yubin Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Seamless and High-Performance Out-of-Distribution Detection Approach Simply Replacing the SoftMax Loss. (arXiv:2105.14399v6 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2105.14399","description":"<p>Current out-of-distribution detection approaches usually present special\nrequirements (e.g., collecting outlier data and hyperparameter validation) and\nproduce side effects (e.g., classification accuracy drop and slow/inefficient\ninferences). Recently, entropic out-of-distribution detection has been proposed\nas a seamless approach (i.e., a solution that avoids all of the previously\nmentioned drawbacks). The entropic out-of-distribution detection solution uses\nthe IsoMax loss for training and the entropic score for out-of-distribution\ndetection. The IsoMax loss works as a SoftMax loss drop-in replacement because\nswapping the SoftMax loss with the IsoMax loss requires no changes in the\nmodel's architecture or training procedures/hyperparameters. In this paper, we\nperform what we call an isometrization of the distances used in the IsoMax\nloss. Additionally, we propose replacing the entropic score with the minimum\ndistance score. Experiments showed that these simple modifications increase\nout-of-distribution detection performance while keeping the solution seamless.\nBesides being competitive with or outperforming all major current approaches,\nthe proposed solution avoids all their current limitations in addition to being\nmuch easier to use because only a simple loss replacement for training the\nneural network is required.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Macedo_D/0/1/0/all/0/1\">David Mac&#xea;do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ludermir_T/0/1/0/all/0/1\">Teresa Ludermir</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Neural Network Robustness via Persistency of Excitation. (arXiv:2106.02078v4 [stat.ML] UPDATED)","link":"http://arxiv.org/abs/2106.02078","description":"<p>Improving adversarial robustness of neural networks remains a major\nchallenge. Fundamentally, training a neural network via gradient descent is a\nparameter estimation problem. In adaptive control, maintaining persistency of\nexcitation (PoE) is integral to ensuring convergence of parameter estimates in\ndynamical systems to their true values. We show that parameter estimation with\ngradient descent can be modeled as a sampling of an adaptive linear\ntime-varying continuous system. Leveraging this model, and with inspiration\nfrom Model-Reference Adaptive Control (MRAC), we prove a sufficient condition\nto constrain gradient descent updates to reference persistently excited\ntrajectories converging to the true parameters. The sufficient condition is\nachieved when the learning rate is less than the inverse of the Lipschitz\nconstant of the gradient of loss function. We provide an efficient technique\nfor estimating the corresponding Lipschitz constant in practice using extreme\nvalue theory. Our experimental results in both standard and adversarial\ntraining illustrate that networks trained with the PoE-motivated learning rate\nschedule have similar clean accuracy but are significantly more robust to\nadversarial attacks than models trained using current state-of-the-art\nheuristics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Sridhar_K/0/1/0/all/0/1\">Kaustubh Sridhar</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sokolsky_O/0/1/0/all/0/1\">Oleg Sokolsky</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lee_I/0/1/0/all/0/1\">Insup Lee</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Weimer_J/0/1/0/all/0/1\">James Weimer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Stable Classifiers by Transferring Unstable Features. (arXiv:2106.07847v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.07847","description":"<p>While unbiased machine learning models are essential for many applications,\nbias is a human-defined concept that can vary across tasks. Given only\ninput-label pairs, algorithms may lack sufficient information to distinguish\nstable (causal) features from unstable (spurious) features. However, related\ntasks often share similar biases -- an observation we may leverage to develop\nstable classifiers in the transfer setting. In this work, we explicitly inform\nthe target classifier about unstable features in the source tasks.\nSpecifically, we derive a representation that encodes the unstable features by\ncontrasting different data environments in the source task. We achieve\nrobustness by clustering data of the target task according to this\nrepresentation and minimizing the worst-case risk across these clusters. We\nevaluate our method on both text and image classifications. Empirical results\ndemonstrate that our algorithm is able to maintain robustness on the target\ntask, outperforming the best baseline by 22.9% in absolute accuracy across 12\ntransfer settings. Our code is available at https://github.com/YujiaBao/Tofu.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bao_Y/0/1/0/all/0/1\">Yujia Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shiyu Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barzilay_R/0/1/0/all/0/1\">Regina Barzilay</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving On-Screen Sound Separation for Open-Domain Videos with Audio-Visual Self-Attention. (arXiv:2106.09669v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2106.09669","description":"<p>We introduce a state-of-the-art audio-visual on-screen sound separation\nsystem which is capable of learning to separate sounds and associate them with\non-screen objects by looking at in-the-wild videos. We identify limitations of\nprevious work on audio-visual on-screen sound separation, including the\nsimplicity and coarse resolution of spatio-temporal attention, and poor\nconvergence of the audio separation model. Our proposed model addresses these\nissues using cross-modal and self-attention modules that capture audio-visual\ndependencies at a finer resolution over time, and by unsupervised pre-training\nof audio separation model. These improvements allow the model to generalize to\na much wider set of unseen videos. We also show a robust way to further improve\nthe generalization capability of our models by calibrating the probabilities of\nour audio-visual on-screen classifier, using only a small amount of in-domain\nvideos labeled for their on-screen presence. For evaluation and semi-supervised\ntraining, we collected human annotations of on-screen audio from a large\ndatabase of in-the-wild videos (YFCC100m). Our results show marked improvements\nin on-screen separation performance, in more general conditions than previous\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tzinis_E/0/1/0/all/0/1\">Efthymios Tzinis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wisdom_S/0/1/0/all/0/1\">Scott Wisdom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Remez_T/0/1/0/all/0/1\">Tal Remez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hershey_J/0/1/0/all/0/1\">John R. Hershey</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pre-training also Transfers Non-Robustness. (arXiv:2106.10989v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.10989","description":"<p>Pre-training has enabled state-of-the-art results on many tasks. In spite of\nits recognized contribution to generalization, we observed in this study that\npre-training also transfers adversarial non-robustness from pre-trained model\ninto fine-tuned model in the downstream tasks. Using image classification as an\nexample, we first conducted experiments on various datasets and network\nbackbones to uncover the adversarial non-robustness in fine-tuned model.\nFurther analysis was conducted on examining the learned knowledge of fine-tuned\nmodel and standard model, and revealed that the reason leading to the\nnon-robustness is the non-robust features transferred from pre-trained model.\nFinally, we analyzed the preference for feature learning of the pre-trained\nmodel, explored the factors influencing robustness, and introduced a simple\nrobust pre-traning solution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiaming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sang_J/0/1/0/all/0/1\">Jitao Sang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_Q/0/1/0/all/0/1\">Qi Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yunfan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Huiwen Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jian Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Ensembling with No Overhead for either Training or Testing: The All-Round Blessings of Dynamic Sparsity. (arXiv:2106.14568v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.14568","description":"<p>Recent works on sparse neural networks have demonstrated the possibility to\ntrain a sparse subnetwork independently from scratch, to match the performance\nof its corresponding dense network. However, identifying such sparse\nsubnetworks (winning tickets) either involves a costly iterative\ntrain-prune-retrain process (e.g., Lottery Ticket Hypothesis) or an\nover-extended training time (e.g., Dynamic Sparse Training). In this work, we\ndraw a unique connection between sparse neural network training and the deep\nensembling technique, yielding a novel ensemble learning framework called\nFreeTickets. Instead of starting from a dense network, FreeTickets randomly\ninitializes a sparse subnetwork and then trains the subnetwork while\ndynamically adjusting its sparse mask, resulting in many diverse sparse\nsubnetworks throughout the training process. FreeTickets is defined as the\nensemble of these sparse subnetworks freely obtained during this one-pass,\nsparse-to-sparse training, which uses only a fraction of the computational\nresources required by the vanilla dense training. Moreover, despite being an\nensemble of models, FreeTickets has even fewer parameters and training FLOPs\ncompared to a single dense model: this seemingly counter-intuitive outcome is\ndue to the high sparsity of each subnetwork. FreeTickets is observed to\ndemonstrate a significant all-round improvement compared to standard dense\nbaselines, in prediction accuracy, uncertainty estimation, robustness, and\nefficiency. FreeTickets easily outperforms the naive deep ensemble with\nResNet50 on ImageNet using only a quarter of the training FLOPs required by the\nlatter. Our results provide insights into the strength of sparse neural\nnetworks and suggest that the benefits of sparsity go way beyond the usually\nexpected inference efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shiwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianlong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atashgahi_Z/0/1/0/all/0/1\">Zahra Atashgahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaohan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sokar_G/0/1/0/all/0/1\">Ghada Sokar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mocanu_E/0/1/0/all/0/1\">Elena Mocanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pechenizkiy_M/0/1/0/all/0/1\">Mykola Pechenizkiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mocanu_D/0/1/0/all/0/1\">Decebal Constantin Mocanu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating Attention Mechanism in 3D Point Cloud Object Detection. (arXiv:2108.00620v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.00620","description":"<p>Object detection in three-dimensional (3D) space attracts much interest from\nacademia and industry since it is an essential task in AI-driven applications\nsuch as robotics, autonomous driving, and augmented reality. As the basic\nformat of 3D data, the point cloud can provide detailed geometric information\nabout the objects in the original 3D space. However, due to 3D data's sparsity\nand unorderedness, specially designed networks and modules are needed to\nprocess this type of data. Attention mechanism has achieved impressive\nperformance in diverse computer vision tasks; however, it is unclear how\nattention modules would affect the performance of 3D point cloud object\ndetection and what sort of attention modules could fit with the inherent\nproperties of 3D data. This work investigates the role of the attention\nmechanism in 3D point cloud object detection and provides insights into the\npotential of different attention modules. To achieve that, we comprehensively\ninvestigate classical 2D attentions, novel 3D attentions, including the latest\npoint cloud transformers on SUN RGB-D and ScanNetV2 datasets. Based on the\ndetailed experiments and analysis, we conclude the effects of different\nattention modules. This paper is expected to serve as a reference source for\nbenefiting attention-embedded 3D point cloud object detection. The code and\ntrained models are available at:\nhttps://github.com/ShiQiu0419/attentions_in_3D_detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_S/0/1/0/all/0/1\">Shi Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yunfan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anwar_S/0/1/0/all/0/1\">Saeed Anwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chongyi Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ME-PCN: Point Completion Conditioned on Mask Emptiness. (arXiv:2108.08187v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.08187","description":"<p>Point completion refers to completing the missing geometries of an object\nfrom incomplete observations. Main-stream methods predict the missing shapes by\ndecoding a global feature learned from the input point cloud, which often leads\nto deficient results in preserving topology consistency and surface details. In\nthis work, we present ME-PCN, a point completion network that leverages\n`emptiness' in 3D shape space. Given a single depth scan, previous methods\noften encode the occupied partial shapes while ignoring the empty regions (e.g.\nholes) in depth maps. In contrast, we argue that these `emptiness' clues\nindicate shape boundaries that can be used to improve topology representation\nand detail granularity on surfaces. Specifically, our ME-PCN encodes both the\noccupied point cloud and the neighboring `empty points'. It estimates\ncoarse-grained but complete and reasonable surface points in the first stage,\nfollowed by a refinement stage to produce fine-grained surface details.\nComprehensive experiments verify that our ME-PCN presents better qualitative\nand quantitative performance against the state-of-the-art. Besides, we further\nprove that our `emptiness' design is lightweight and easy to embed in existing\nmethods, which shows consistent effectiveness in improving the CD and EMD\nscores.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gong_B/0/1/0/all/0/1\">Bingchen Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_Y/0/1/0/all/0/1\">Yinyu Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yiqun Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xiaoguang Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yizhou Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Real-time Bangla License Plate Recognition System for Low Resource Video-based Applications. (arXiv:2108.08339v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.08339","description":"<p>Automatic License Plate Recognition systems aim to provide a solution for\ndetecting, localizing, and recognizing license plate characters from vehicles\nappearing in video frames. However, deploying such systems in the real world\nrequires real-time performance in low-resource environments. In our paper, we\npropose a two-stage detection pipeline paired with Vision API that provides\nreal-time inference speed along with consistently accurate detection and\nrecognition performance. We used a haar-cascade classifier as a filter on top\nof our backbone MobileNet SSDv2 detection model. This reduces inference time by\nonly focusing on high confidence detections and using them for recognition. We\nalso impose a temporal frame separation strategy to distinguish between\nmultiple vehicle license plates in the same clip. Furthermore, there are no\npublicly available Bangla license plate datasets, for which we created an image\ndataset and a video dataset containing license plates in the wild. We trained\nour models on the image dataset and achieved an AP(0.5) score of 86% and tested\nour pipeline on the video dataset and observed reasonable detection and\nrecognition performance (82.7% detection rate, and 60.8% OCR F1 score) with\nreal-time processing speed (27.2 frames per second).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ashrafee_A/0/1/0/all/0/1\">Alif Ashrafee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1\">Akib Mohammed Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Irbaz_M/0/1/0/all/0/1\">Mohammad Sabik Irbaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasim_M/0/1/0/all/0/1\">MD Abdullah Al Nasim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Oriented Object Detection in Aerial Images Based on Area Ratio of Parallelogram. (arXiv:2109.10187v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.10187","description":"<p>Rotated object detection is a challenging task in aerial images since the\nobjects in aerial images are displayed in arbitrary directions and are\nfrequently densely packed. Although considerable progress has been made, there\nare still challenges that existing regression-based rotation detectors suffer\nfrom the representation ambiguity. In this paper, we propose a simple,\npractical framework to optimize the bounding box regression for rotating\nobjects. Rather than directly regressing the five parameters (coordinates of\nthe central point, width, height, and rotation angle) or the four vertices, we\nemploy the area ratio of the parallelogram (ARP) to describe a multi-oriented\nobject accurately. Specifically, ARP regresses coordinates of the center point,\nheight, and width of the oriented object's minimum circumscribed rectangle and\nthree area ratios. It may facilitate learning offset and avoid the issue of\nangular periodicity or label points sequence for oriented objects. To further\nremedy the confusion issue of nearly horizontal objects, the area ratio between\nthe object and its minimal circumscribed rectangle has been used to guide the\nselection of horizontal or oriented detection for each object. The rotation\nefficient IOU loss (R-EIOU) connects the flat bounding box with the three area\nratios and improves the accuracy of the rotating bounding. Experimental results\non remote sensing datasets, including HRSC2016, DOTA, and UCAS-AOD, show that\nour method achieves superior detection performance than many state-of-the-art\napproaches. The code and model will be coming with the paper published.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xinyi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Mi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiangping Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ou_L/0/1/0/all/0/1\">Linlin Ou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learn then Test: Calibrating Predictive Algorithms to Achieve Risk Control. (arXiv:2110.01052v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.01052","description":"<p>We introduce Learn then Test, a framework for calibrating machine learning\nmodels so that their predictions satisfy explicit, finite-sample statistical\nguarantees regardless of the underlying model and (unknown) data-generating\ndistribution. The framework addresses, among other examples, false discovery\nrate control in multi-label classification, intersection-over-union control in\ninstance segmentation, and the simultaneous control of the type-1 error of\noutlier detection and confidence set coverage in classification or regression.\nTo accomplish this, we solve a key technical challenge: the control of\narbitrary risks that are not necessarily monotonic. Our main insight is to\nreframe the risk-control problem as multiple hypothesis testing, enabling\ntechniques and mathematical arguments different from those in the previous\nliterature. We use our framework to provide new calibration methods for several\ncore machine learning tasks with detailed worked examples in computer vision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Angelopoulos_A/0/1/0/all/0/1\">Anastasios N. Angelopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bates_S/0/1/0/all/0/1\">Stephen Bates</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Candes_E/0/1/0/all/0/1\">Emmanuel J. Cand&#xe8;s</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1\">Michael I. Jordan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_L/0/1/0/all/0/1\">Lihua Lei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MovingFashion: a Benchmark for the Video-to-Shop Challenge. (arXiv:2110.02627v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.02627","description":"<p>Retrieving clothes which are worn in social media videos (Instagram, TikTok)\nis the latest frontier of e-fashion, referred to as \"video-to-shop\" in the\ncomputer vision literature. In this paper we present MovingFashion, the first\npublicly available dataset to cope with this challenge. MovingFashion is\ncomposed of 14855 social videos, each one of them associated to e-commerce\n\"shop\" images where the corresponding clothing items are clearly portrayed. In\naddition, we present a network for retrieving the shop images in this scenario,\ndubbed SEAM Match-RCNN. The model is trained by image-to-video domain\nadaptation, allowing to use video sequences where only their association with a\nshop image is given, eliminating the need of millions of annotated bounding\nboxes. SEAM Match-RCNN builds an embedding, where an attention-based weighted\nsum of few frames (10) of a social video is enough to individuate the correct\nproduct within the first 5 retrieved items in a 14K+ shop element gallery with\nan accuracy of 80%. This provides the best performance on MovingFashion,\ncomparing exhaustively against the related state-of-the-art approaches and\nalternative baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Godi_M/0/1/0/all/0/1\">Marco Godi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joppi_C/0/1/0/all/0/1\">Christian Joppi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skenderi_G/0/1/0/all/0/1\">Geri Skenderi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cristani_M/0/1/0/all/0/1\">Marco Cristani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Unlearning of Backdoors via Implicit Hypergradient. (arXiv:2110.03735v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.03735","description":"<p>We propose a minimax formulation for removing backdoors from a given poisoned\nmodel based on a small set of clean data. This formulation encompasses much of\nprior work on backdoor removal. We propose the Implicit Bacdoor Adversarial\nUnlearning (I-BAU) algorithm to solve the minimax. Unlike previous work, which\nbreaks down the minimax into separate inner and outer problems, our algorithm\nutilizes the implicit hypergradient to account for the interdependence between\ninner and outer optimization. We theoretically analyze its convergence and the\ngeneralizability of the robustness gained by solving minimax on clean data to\nunseen test data. In our evaluation, we compare I-BAU with six state-of-art\nbackdoor defenses on seven backdoor attacks over two datasets and various\nattack settings, including the common setting where the attacker targets one\nclass as well as important but underexplored settings where multiple classes\nare targeted. I-BAU's performance is comparable to and most often significantly\nbetter than the best baseline. Particularly, its performance is more robust to\nthe variation on triggers, attack settings, poison ratio, and clean data size.\nMoreover, I-BAU requires less computation to take effect; particularly, it is\nmore than $13\\times$ faster than the most efficient baseline in the\nsingle-target attack setting. Furthermore, it can remain effective in the\nextreme case where the defender can only access 100 clean samples -- a setting\nwhere all the baselines fail to produce acceptable results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1\">Yi Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Si Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_W/0/1/0/all/0/1\">Won Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Z/0/1/0/all/0/1\">Z. Morley Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_M/0/1/0/all/0/1\">Ming Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1\">Ruoxi Jia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Architectural Ingredients of Adversarially Robust Deep Neural Networks. (arXiv:2110.03825v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.03825","description":"<p>Deep neural networks (DNNs) are known to be vulnerable to adversarial\nattacks. A range of defense methods have been proposed to train adversarially\nrobust DNNs, among which adversarial training has demonstrated promising\nresults. However, despite preliminary understandings developed for adversarial\ntraining, it is still not clear, from the architectural perspective, what\nconfigurations can lead to more robust DNNs. In this paper, we address this gap\nvia a comprehensive investigation on the impact of network width and depth on\nthe robustness of adversarially trained DNNs. Specifically, we make the\nfollowing key observations: 1) more parameters (higher model capacity) does not\nnecessarily help adversarial robustness; 2) reducing capacity at the last stage\n(the last group of blocks) of the network can actually improve adversarial\nrobustness; and 3) under the same parameter budget, there exists an optimal\narchitectural configuration for adversarial robustness. We also provide a\ntheoretical analysis explaning why such network configuration can help\nrobustness. These architectural insights can help design adversarially robust\nDNNs. Code is available at \\url{https://github.com/HanxunH/RobustWRN}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hanxun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yisen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erfani_S/0/1/0/all/0/1\">Sarah Monazam Erfani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1\">Quanquan Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bailey_J/0/1/0/all/0/1\">James Bailey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xingjun Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Hybrid Spatial-temporal Deep Learning Architecture for Lane Detection. (arXiv:2110.04079v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.04079","description":"<p>Reliable and accurate lane detection is of vital importance for the safe\nperformance of Lane Keeping Assistance and Lane Departure Warning systems.\nHowever, under certain challenging peculiar circumstances, it is difficult to\nget satisfactory performance in accurately detecting the lanes from one single\nimage which is often the case in current literature. Since lane markings are\ncontinuous lines, the lanes that are difficult to be accurately detected in the\nsingle current image can potentially be better deduced if information from\nprevious frames is incorporated. This study proposes a novel hybrid\nspatial-temporal sequence-to-one deep learning architecture making full use of\nthe spatial-temporal information in multiple continuous image frames to detect\nlane markings in the very last current frame. Specifically, the hybrid model\nintegrates the single image feature extraction module with the spatial\nconvolutional neural network (SCNN) embedded for excavating spatial features\nand relationships in one single image, the spatial-temporal feature integration\nmodule with spatial-temporal recurrent neural network (ST-RNN), which can\ncapture the spatial-temporal correlations and time dependencies among image\nsequences, and the encoder-decoder structure, which makes this image\nsegmentation problem work in an end-to-end supervised learning format.\nExtensive experiments reveal that the proposed model can effectively handle\nchallenging driving scenes and outperforms available state-of-the-art methods\nwith a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yongqi Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patil_S/0/1/0/all/0/1\">Sandeep Patil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arem_B/0/1/0/all/0/1\">Bart van Arem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farah_H/0/1/0/all/0/1\">Haneen Farah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Closer Look at Prototype Classifier for Few-shot Image Classification. (arXiv:2110.05076v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.05076","description":"<p>The prototypical network is a prototype classifier based on meta-learning and\nis widely used for few-shot learning because it classifies unseen examples by\nconstructing class-specific prototypes without adjusting hyper-parameters\nduring meta-testing. Interestingly, recent research has attracted a lot of\nattention, showing that a linear classifier with fine-tuning, which does not\nuse a meta-learning algorithm, performs comparably with the prototypical\nnetwork. However, fine-tuning requires additional hyper-parameters when\nadapting a model to a new environment. In addition, although the purpose of\nfew-shot learning is to enable the model to quickly adapt to a new environment,\nfine-tuning needs to be applied every time a new class appears, making fast\nadaptation difficult. In this paper, we analyze how a prototype classifier\nworks equally well without fine-tuning and meta-learning. We experimentally\nfound that directly using the feature vector extracted using standard\npre-trained models to construct a prototype classifier in meta-testing does not\nperform as well as the prototypical network and linear classifiers with\nfine-tuning and feature vectors of pre-trained models. Thus, we derive a novel\ngeneralization bound for the prototypical network and show that focusing on the\nvariance of the norm of a feature vector can improve performance. We\nexperimentally investigated several normalization methods for minimizing the\nvariance of the norm and found that the same performance can be obtained by\nusing the L2 normalization and embedding space transformation without\nfine-tuning or meta-learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hou_M/0/1/0/all/0/1\">Mingcheng Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sato_I/0/1/0/all/0/1\">Issei Sato</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLIP4Caption ++: Multi-CLIP for Video Caption. (arXiv:2110.05204v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.05204","description":"<p>This report describes our solution to the VALUE Challenge 2021 in the\ncaptioning task. Our solution, named CLIP4Caption++, is built on\nX-Linear/X-Transformer, which is an advanced model with encoder-decoder\narchitecture. We make the following improvements on the proposed\nCLIP4Caption++: We employ an advanced encoder-decoder model architecture\nX-Transformer as our main framework and make the following improvements: 1) we\nutilize three strong pre-trained CLIP models to extract the text-related\nappearance visual features. 2) we adopt the TSN sampling strategy for data\nenhancement. 3) we involve the video subtitle information to provide richer\nsemantic information. 3) we introduce the subtitle information, which fuses\nwith the visual features as guidance. 4) we design word-level and\nsentence-level ensemble strategies. Our proposed method achieves 86.5, 148.4,\n64.5 CIDEr scores on VATEX, YC2C, and TVC datasets, respectively, which shows\nthe superior performance of our proposed CLIP4Caption++ on all three datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_M/0/1/0/all/0/1\">Mingkang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhanyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Zhaoyang Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_F/0/1/0/all/0/1\">Fengyun Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dian Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ByteTrack: Multi-Object Tracking by Associating Every Detection Box. (arXiv:2110.06864v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.06864","description":"<p>Multi-object tracking (MOT) aims at estimating bounding boxes and identities\nof objects in videos. Most methods obtain identities by associating detection\nboxes whose scores are higher than a threshold. The objects with low detection\nscores, e.g. occluded objects, are simply thrown away, which brings\nnon-negligible true object missing and fragmented trajectories. To solve this\nproblem, we present a simple, effective and generic association method, called\nBYTE, tracking BY associaTing Every detection box instead of only the high\nscore ones. For the low score detection boxes, we utilize their similarities\nwith tracklets to recover true objects and filter out the background\ndetections. We apply BYTE to 9 different state-of-the-art trackers and achieve\nconsistent improvement on IDF1 score ranging from 1 to 10 points. To put\nforwards the state-of-the-art performance of MOT, we design a simple and strong\ntracker, named ByteTrack. For the first time, we achieve 80.3 MOTA, 77.3 IDF1\nand 63.1 HOTA on the test set of MOT17 with 30 FPS running speed on a single\nV100 GPU. The source code, pre-trained models with deploy versions and\ntutorials of applying to other trackers are released at\nhttps://github.com/ifzhang/ByteTrack.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yifu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_P/0/1/0/all/0/1\">Peize Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dongdong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zehuan Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinggang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-10-14T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","dc":"http://purl.org/dc/elements/1.1/","content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}}]}]}