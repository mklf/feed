{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-02-15T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Metadata-Induced Contrastive Learning for Zero-Shot Multi-Label Text Classification. (arXiv:2202.05932v1 [cs.CL])","link":"http://arxiv.org/abs/2202.05932","description":"<p>Large-scale multi-label text classification (LMTC) aims to associate a\ndocument with its relevant labels from a large candidate set. Most existing\nLMTC approaches rely on massive human-annotated training data, which are often\ncostly to obtain and suffer from a long-tailed label distribution (i.e., many\nlabels occur only a few times in the training set). In this paper, we study\nLMTC under the zero-shot setting, which does not require any annotated\ndocuments with labels and only relies on label surface names and descriptions.\nTo train a classifier that calculates the similarity score between a document\nand a label, we propose a novel metadata-induced contrastive learning (MICoL)\nmethod. Different from previous text-based contrastive learning techniques,\nMICoL exploits document metadata (e.g., authors, venues, and references of\nresearch papers), which are widely available on the Web, to derive similar\ndocument-document pairs. Experimental results on two large-scale datasets show\nthat: (1) MICoL significantly outperforms strong zero-shot text classification\nand contrastive learning baselines; (2) MICoL is on par with the\nstate-of-the-art supervised metadata-aware LMTC method trained on 10K-200K\nlabeled documents; and (3) MICoL tends to predict more infrequent labels than\nsupervised methods, thus alleviates the deteriorated performance on long-tailed\nlabels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zhihong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chieh-Han Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_B/0/1/0/all/0/1\">Boya Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_J/0/1/0/all/0/1\">Junheng Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Ye-Yi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kuansan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-supervised New Event Type Induction and Description via Contrastive Loss-Enforced Batch Attention. (arXiv:2202.05943v1 [cs.CL])","link":"http://arxiv.org/abs/2202.05943","description":"<p>Most event extraction methods have traditionally relied on an annotated set\nof event types. However, creating event ontologies and annotating supervised\ntraining data are expensive and time-consuming. Previous work has proposed\nsemi-supervised approaches which leverage seen (annotated) types to learn how\nto automatically discover new event types. State-of-the-art methods, both\nsemi-supervised or fully unsupervised, use a form of reconstruction loss on\nspecific tokens in a context. In contrast, we present a novel approach to\nsemi-supervised new event type induction using a masked contrastive loss, which\nlearns similarities between event mentions by enforcing an attention mechanism\nover the data minibatch. We further disentangle the discovered clusters by\napproximating the underlying manifolds in the data, which allows us to increase\nnormalized mutual information and Fowlkes-Mallows scores by over 20% absolute.\nBuilding on these clustering results, we extend our approach to two new tasks:\npredicting the type name of the discovered clusters and linking them to\nFrameNet frames.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Edwards_C/0/1/0/all/0/1\">Carl Edwards</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Contextual Coherence in Variational Personalized and Empathetic Dialogue Agents. (arXiv:2202.05971v1 [cs.CL])","link":"http://arxiv.org/abs/2202.05971","description":"<p>In recent years, latent variable models, such as the Conditional Variational\nAuto Encoder (CVAE), have been applied to both personalized and empathetic\ndialogue generation. Prior work have largely focused on generating diverse\ndialogue responses that exhibit persona consistency and empathy. However, when\nit comes to the contextual coherence of the generated responses, there is still\nroom for improvement. Hence, to improve the contextual coherence, we propose a\nnovel Uncertainty Aware CVAE (UA-CVAE) framework. The UA-CVAE framework\ninvolves approximating and incorporating the aleatoric uncertainty during\nresponse generation. We apply our framework to both personalized and empathetic\ndialogue generation. Empirical results show that our framework significantly\nimproves the contextual coherence of the generated response. Additionally, we\nintroduce a novel automatic metric for measuring contextual coherence, which\nwas found to correlate positively with human judgement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jing Yang Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kong Aik Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_W/0/1/0/all/0/1\">Woon Seng Gan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A multi-task semi-supervised framework for Text2Graph & Graph2Text. (arXiv:2202.06041v1 [cs.CL])","link":"http://arxiv.org/abs/2202.06041","description":"<p>The Artificial Intelligence industry regularly develops applications that\nmostly rely on Knowledge Bases, a data repository about specific, or general,\ndomains, usually represented in a graph shape. Similar to other databases, they\nface two main challenges: information ingestion and information retrieval. We\napproach these challenges by jointly learning graph extraction from text and\ntext generation from graphs. The proposed solution, a T5 architecture, is\ntrained in a multi-task semi-supervised environment, with our collected\nnon-parallel data, following a cycle training regime. Experiments on WebNLG\ndataset show that our approach surpasses unsupervised state-of-the-art results\nin text-to-graph and graph-to-text. More relevantly, our framework is more\nconsistent across seen and unseen domains than supervised models. The resulting\nmodel can be easily trained in any new domain with non-parallel data, by simply\nadding text and graphs about it, in our cycle framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Domingo_O/0/1/0/all/0/1\">Oriol Domingo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Costa_jussa_M/0/1/0/all/0/1\">Marta R. Costa-juss&#xe0;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Escolano_C/0/1/0/all/0/1\">Carlos Escolano</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"USTED: Improving ASR with a Unified Speech and Text Encoder-Decoder. (arXiv:2202.06045v1 [cs.CL])","link":"http://arxiv.org/abs/2202.06045","description":"<p>Improving end-to-end speech recognition by incorporating external text data\nhas been a longstanding research topic. There has been a recent focus on\ntraining E2E ASR models that get the performance benefits of external text data\nwithout incurring the extra cost of evaluating an external language model at\ninference time. In this work, we propose training ASR model jointly with a set\nof text-to-text auxiliary tasks with which it shares a decoder and parts of the\nencoder. When we jointly train ASR and masked language model with the 960-hour\nLibrispeech and Opensubtitles data respectively, we observe WER reductions of\n16% and 20% on test-other and test-clean respectively over an ASR-only baseline\nwithout any extra cost at inference time, and reductions of 6% and 8% compared\nto a stronger MUTE-L baseline which trains the decoder with the same text data\nas our model. We achieve further improvements when we train masked language\nmodel on Librispeech data or when we use machine translation as the auxiliary\ntask, without significantly sacrificing performance on the task itself.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yusuf_B/0/1/0/all/0/1\">Bolaji Yusuf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gandhe_A/0/1/0/all/0/1\">Ankur Gandhe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sokolov_A/0/1/0/all/0/1\">Alex Sokolov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic-Oriented Unlabeled Priming for Large-Scale Language Models. (arXiv:2202.06133v1 [cs.CL])","link":"http://arxiv.org/abs/2202.06133","description":"<p>Due to the high costs associated with finetuning large language models,\nvarious recent works propose to adapt them to specific tasks without any\nparameter updates through in-context learning. Unfortunately, for in-context\nlearning there is currently no way to leverage unlabeled data, which is often\nmuch easier to obtain in large quantities than labeled examples. In this work,\nwe therefore investigate ways to make use of unlabeled examples to improve the\nzero-shot performance of pretrained language models without any finetuning: We\nintroduce Semantic-Oriented Unlabeled Priming (SOUP), a method that classifies\nexamples by retrieving semantically similar unlabeled examples, assigning\nlabels to them in a zero-shot fashion, and then using them for in-context\nlearning. We also propose bag-of-contexts priming, a new priming strategy that\nis more suitable for our setting and enables the usage of more examples than\nfit into the context window.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yanchen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schick_T/0/1/0/all/0/1\">Timo Schick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ultra-fine Entity Typing with Indirect Supervision from Natural Language Inference. (arXiv:2202.06167v1 [cs.CL])","link":"http://arxiv.org/abs/2202.06167","description":"<p>The task of ultra-fine entity typing (UFET) seeks to predict diverse and\nfree-form words or phrases that describe the appropriate types of entities\nmentioned in sentences. A key challenge for this task lies in the large amount\nof types and the scarcity of annotated data per type. Existing systems\nformulate the task as a multi-way classification problem and train directly or\ndistantly supervised classifiers. This causes two issues: (i) the classifiers\ndo not capture the type semantics since types are often converted into indices;\n(ii) systems developed in this way are limited to predicting within a\npre-defined type set, and often fall short of generalizing to types that are\nrarely seen or unseen in training. This work presents LITE, a new approach that\nformulates entity typing as a natural language inference (NLI) problem, making\nuse of (i) the indirect supervision from NLI to infer type information\nmeaningfully represented as textual hypotheses and alleviate the data scarcity\nissue, as well as (ii) a learning-to-rank objective to avoid the pre-defining\nof a type set. Experiments show that, with limited training data, LITE obtains\nstate-of-the-art performance on the UFET task. In addition, LITE demonstrates\nits strong generalizability, by not only yielding best results on other\nfine-grained entity typing benchmarks, more importantly, a pre-trained LITE\nsystem works well on new data containing unseen types.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bangzheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_W/0/1/0/all/0/1\">Wenpeng Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Muhao Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StoryBuddy: A Human-AI Collaborative Chatbot for Parent-Child Interactive Storytelling with Flexible Parental Involvement. (arXiv:2202.06205v1 [cs.HC])","link":"http://arxiv.org/abs/2202.06205","description":"<p>Despite its benefits for children's skill development and parent-child\nbonding, many parents do not often engage in interactive storytelling by having\nstory-related dialogues with their child due to limited availability or\nchallenges in coming up with appropriate questions. While recent advances made\nAI generation of questions from stories possible, the fully-automated approach\nexcludes parent involvement, disregards educational goals, and underoptimizes\nfor child engagement. Informed by need-finding interviews and participatory\ndesign (PD) results, we developed StoryBuddy, an AI-enabled system for parents\nto create interactive storytelling experiences. StoryBuddy's design highlighted\nthe need for accommodating dynamic user needs between the desire for parent\ninvolvement and parent-child bonding and the goal of minimizing parent\nintervention when busy. The PD revealed varied assessment and educational goals\nof parents, which StoryBuddy addressed by supporting configuring question types\nand tracking child progress. A user study validated StoryBuddy's usability and\nsuggested design insights for future parent-AI collaboration systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Ying Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanhao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_B/0/1/0/all/0/1\">Bingsheng Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ritchie_D/0/1/0/all/0/1\">Daniel Ritchie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tongshuang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1\">Mo Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dakuo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Toby Jia-Jun Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uni-Retriever: Towards Learning The Unified Embedding Based Retriever in Bing Sponsored Search. (arXiv:2202.06212v1 [cs.IR])","link":"http://arxiv.org/abs/2202.06212","description":"<p>Embedding based retrieval (EBR) is a fundamental building block in many web\napplications. However, EBR in sponsored search is distinguished from other\ngeneric scenarios and technically challenging due to the need of serving\nmultiple retrieval purposes: firstly, it has to retrieve high-relevance ads,\nwhich may exactly serve user's search intent; secondly, it needs to retrieve\nhigh-CTR ads so as to maximize the overall user clicks. In this paper, we\npresent a novel representation learning framework Uni-Retriever developed for\nBing Search, which unifies two different training modes knowledge distillation\nand contrastive learning to realize both required objectives. On one hand, the\ncapability of making high-relevance retrieval is established by distilling\nknowledge from the ``relevance teacher model''. On the other hand, the\ncapability of making high-CTR retrieval is optimized by learning to\ndiscriminate user's clicked ads from the entire corpus. The two training modes\nare jointly performed as a multi-objective learning process, such that the ads\nof high relevance and CTR can be favored by the generated embeddings. Besides\nthe learning strategy, we also elaborate our solution for EBR serving pipeline\nbuilt upon the substantially optimized DiskANN, where massive-scale EBR can be\nperformed with competitive time and memory efficiency, and accomplished in\nhigh-quality. We make comprehensive offline and online experiments to evaluate\nthe proposed techniques, whose findings may provide useful insights for the\nfuture development of EBR systems. Uni-Retriever has been mainstreamed as the\nmajor retrieval path in Bing's production thanks to the notable improvements on\nthe representation and EBR serving quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianjin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1\">Weihao Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_S/0/1/0/all/0/1\">Shitao Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_R/0/1/0/all/0/1\">Ruicheng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1\">Yingxia Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Hao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hanqing Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_P/0/1/0/all/0/1\">Premkumar Srinivasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_D/0/1/0/all/0/1\">Denvy Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xing Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emotion Based Hate Speech Detection using Multimodal Learning. (arXiv:2202.06218v1 [cs.LG])","link":"http://arxiv.org/abs/2202.06218","description":"<p>In recent years, monitoring hate speech and offensive language on social\nmedia platforms has become paramount due to its widespread usage among all age\ngroups, races, and ethnicities. Consequently, there have been substantial\nresearch efforts towards automated detection of such content using Natural\nLanguage Processing (NLP). While successfully filtering textual data, no\nresearch has focused on detecting hateful content in multimedia data. With\nincreased ease of data storage and the exponential growth of social media\nplatforms, multimedia content proliferates the internet as much as text data.\nNevertheless, it escapes the automatic filtering systems. Hate speech and\noffensiveness can be detected in multimedia primarily via three modalities,\ni.e., visual, acoustic, and verbal. Our preliminary study concluded that the\nmost essential features in classifying hate speech would be the speaker's\nemotional state and its influence on the spoken words, therefore limiting our\ncurrent research to these modalities. This paper proposes the first multimodal\ndeep learning framework to combine the auditory features representing emotion\nand the semantic features to detect hateful content. Our results demonstrate\nthat incorporating emotional attributes leads to significant improvement over\ntext-based models in detecting hateful multimedia content. This paper also\npresents a new Hate Speech Detection Video Dataset (HSDVD) collected for the\npurpose of multimodal learning as no such dataset exists today.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rana_A/0/1/0/all/0/1\">Aneri Rana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1\">Sonali Jha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PQuAD: A Persian Question Answering Dataset. (arXiv:2202.06219v1 [cs.CL])","link":"http://arxiv.org/abs/2202.06219","description":"<p>We present Persian Question Answering Dataset (PQuAD), a crowdsourced reading\ncomprehension dataset on Persian Wikipedia articles. It includes 80,000\nquestions along with their answers, with 25% of the questions being\nadversarially unanswerable. We examine various properties of the dataset to\nshow the diversity and the level of its difficulty as an MRC benchmark. By\nreleasing this dataset, we aim to ease research on Persian reading\ncomprehension and development of Persian question answering systems. Our\nexperiments on different state-of-the-art pre-trained contextualized language\nmodels show 74.8% Exact Match (EM) and 87.6% F1-score that can be used as the\nbaseline results for further research on Persian QA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Darvishi_K/0/1/0/all/0/1\">Kasra Darvishi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahbodagh_N/0/1/0/all/0/1\">Newsha Shahbodagh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbasiantaeb_Z/0/1/0/all/0/1\">Zahra Abbasiantaeb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Momtazi_S/0/1/0/all/0/1\">Saeedeh Momtazi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Depression Classification Using Articulatory Coordination Features And Hierarchical Attention Based Text Embeddings. (arXiv:2202.06238v1 [eess.AS])","link":"http://arxiv.org/abs/2202.06238","description":"<p>Multimodal depression classification has gained immense popularity over the\nrecent years. We develop a multimodal depression classification system using\narticulatory coordination features extracted from vocal tract variables and\ntext transcriptions obtained from an automatic speech recognition tool that\nyields improvements of area under the receiver operating characteristics curve\ncompared to uni-modal classifiers (7.5% and 13.7% for audio and text\nrespectively). We show that in the case of limited training data, a\nsegment-level classifier can first be trained to then obtain a session-wise\nprediction without hindering the performance, using a multi-stage convolutional\nrecurrent neural network. A text model is trained using a Hierarchical\nAttention Network (HAN). The multimodal system is developed by combining\nembeddings from the session-level audio model and the HAN text model\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Seneviratne_N/0/1/0/all/0/1\">Nadee Seneviratne</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Espy_Wilson_C/0/1/0/all/0/1\">Carol Espy-Wilson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Simplified Variant of G\\\"odel's Ontological Argument. (arXiv:2202.06264v1 [cs.LO])","link":"http://arxiv.org/abs/2202.06264","description":"<p>A simplified variant of G\\\"odel's ontological argument is presented. The\nsimplified argument is valid already in basic modal logics K or KT, it does not\nsuffer from modal collapse, and it avoids the rather complex predicates of\nessence (Ess.) and necessary existence (NE) as used by G\\\"odel. The variant\npresented has been obtained as a side result of a series of theory\nsimplification experiments conducted in interaction with a modern proof\nassistant system. The starting point for these experiments was the computer\nencoding of G\\\"odel's argument, and then automated reasoning techniques were\nsystematically applied to arrive at the simplified variant presented. The\npresented work thus exemplifies a fruitful human-computer interaction in\ncomputational metaphysics. Whether the presented result increases or decreases\nthe attractiveness and persuasiveness of the ontological argument is a question\nI would like to pass on to philosophy and theology.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Benzmuller_C/0/1/0/all/0/1\">Christoph Benzm&#xfc;ller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Incremental user embedding modeling for personalized text classification. (arXiv:2202.06369v1 [cs.LG])","link":"http://arxiv.org/abs/2202.06369","description":"<p>Individual user profiles and interaction histories play a significant role in\nproviding customized experiences in real-world applications such as chatbots,\nsocial media, retail, and education. Adaptive user representation learning by\nutilizing user personalized information has become increasingly challenging due\nto ever-growing history data. In this work, we propose an incremental user\nembedding modeling approach, in which embeddings of user's recent interaction\nhistories are dynamically integrated into the accumulated history vectors via a\ntransformer encoder. This modeling paradigm allows us to create generalized\nuser representations in a consecutive manner and also alleviate the challenges\nof data management. We demonstrate the effectiveness of this approach by\napplying it to a personalized multi-class classification task based on the\nReddit dataset, and achieve 9% and 30% relative improvement on prediction\naccuracy over a baseline system for two experiment settings through appropriate\ncomment history encoding and task modeling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lian_R/0/1/0/all/0/1\">Ruixue Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Che-Wei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yuqing Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1\">Qilong Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Chengyuan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1\">Chenlei Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scaling Laws Under the Microscope: Predicting Transformer Performance from Small Scale Experiments. (arXiv:2202.06387v1 [cs.CL])","link":"http://arxiv.org/abs/2202.06387","description":"<p>Neural scaling laws define a predictable relationship between a model's\nparameter count and its performance after training in the form of a power law.\nHowever, most research to date has not explicitly investigated whether scaling\nlaws can be used to accelerate model development. In this work, we perform such\nan empirical investigation across a wide range of language understanding tasks,\nstarting from models with as few as 10K parameters, and evaluate downstream\nperformance across 9 language understanding tasks. We find that scaling laws\nemerge at finetuning time in some NLP tasks, and that they can also be\nexploited for debugging convergence when training large models. Moreover, for\ntasks where scaling laws exist, they can be used to predict the performance of\nlarger models, which enables effective model selection. However, revealing\nscaling laws requires careful hyperparameter tuning and multiple runs for the\npurpose of uncertainty estimation, which incurs additional overhead, partially\noffsetting the computational benefits.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ivgi_M/0/1/0/all/0/1\">Maor Ivgi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carmon_Y/0/1/0/all/0/1\">Yair Carmon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berant_J/0/1/0/all/0/1\">Jonathan Berant</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformer-based Approaches for Legal Text Processing. (arXiv:2202.06397v1 [cs.CL])","link":"http://arxiv.org/abs/2202.06397","description":"<p>In this paper, we introduce our approaches using Transformer-based models for\ndifferent problems of the COLIEE 2021 automatic legal text processing\ncompetition. Automated processing of legal documents is a challenging task\nbecause of the characteristics of legal documents as well as the limitation of\nthe amount of data. With our detailed experiments, we found that\nTransformer-based pretrained language models can perform well with automated\nlegal text processing problems with appropriate approaches. We describe in\ndetail the processing steps for each task such as problem formulation, data\nprocessing and augmentation, pretraining, finetuning. In addition, we introduce\nto the community two pretrained models that take advantage of parallel\ntranslations in legal domain, NFSP and NMSP. In which, NFSP achieves the\nstate-of-the-art result in Task 5 of the competition. Although the paper\nfocuses on technical reporting, the novelty of its approaches can also be an\nuseful reference in automated legal document processing using Transformer-based\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Ha-Thanh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_M/0/1/0/all/0/1\">Minh-Phuong Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vuong_T/0/1/0/all/0/1\">Thi-Hai-Yen Vuong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_M/0/1/0/all/0/1\">Minh-Quan Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_M/0/1/0/all/0/1\">Minh-Chau Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dang_T/0/1/0/all/0/1\">Tran-Binh Dang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_V/0/1/0/all/0/1\">Vu Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_L/0/1/0/all/0/1\">Le-Minh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Satoh_K/0/1/0/all/0/1\">Ken Satoh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distribution augmentation for low-resource expressive text-to-speech. (arXiv:2202.06409v1 [eess.AS])","link":"http://arxiv.org/abs/2202.06409","description":"<p>This paper presents a novel data augmentation technique for text-to-speech\n(TTS), that allows to generate new (text, audio) training examples without\nrequiring any additional data. Our goal is to increase diversity of text\nconditionings available during training. This helps to reduce overfitting,\nespecially in low-resource settings. Our method relies on substituting text and\naudio fragments in a way that preserves syntactical correctness. We take\nadditional measures to ensure that synthesized speech does not contain\nartifacts caused by combining inconsistent audio samples. The perceptual\nevaluations show that our method improves speech quality over a number of\ndatasets, speakers, and TTS architectures. We also demonstrate that it greatly\nimproves robustness of attention-based TTS models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lajszczak_M/0/1/0/all/0/1\">Mateusz Lajszczak</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Prasad_A/0/1/0/all/0/1\">Animesh Prasad</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Korlaar_A/0/1/0/all/0/1\">Arent van Korlaar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bollepalli_B/0/1/0/all/0/1\">Bajibabu Bollepalli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bonafonte_A/0/1/0/all/0/1\">Antonio Bonafonte</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Joly_A/0/1/0/all/0/1\">Arnaud Joly</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nicolis_M/0/1/0/all/0/1\">Marco Nicolis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Moinet_A/0/1/0/all/0/1\">Alexis Moinet</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Drugman_T/0/1/0/all/0/1\">Thomas Drugman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wood_T/0/1/0/all/0/1\">Trevor Wood</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sokolova_E/0/1/0/all/0/1\">Elena Sokolova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extending Neural Keyword Extraction with TF-IDF tagset matching. (arXiv:2102.00472v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2102.00472","description":"<p>Keyword extraction is the task of identifying words (or multi-word\nexpressions) that best describe a given document and serve in news portals to\nlink articles of similar topics. In this work we develop and evaluate our\nmethods on four novel data sets covering less represented, morphologically-rich\nlanguages in European news media industry (Croatian, Estonian, Latvian and\nRussian). First, we perform evaluation of two supervised neural\ntransformer-based methods (TNT-KID and BERT+BiLSTM CRF) and compare them to a\nbaseline TF-IDF based unsupervised approach. Next, we show that by combining\nthe keywords retrieved by both neural transformer based methods and extending\nthe final set of keywords with an unsupervised TF-IDF based technique, we can\ndrastically improve the recall of the system, making it appropriate to be used\nas a recommendation system in the media house environment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Koloski_B/0/1/0/all/0/1\">Boshko Koloski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pollak_S/0/1/0/all/0/1\">Senja Pollak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skrlj_B/0/1/0/all/0/1\">Bla&#x17e; &#x160;krlj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martinc_M/0/1/0/all/0/1\">Matej Martinc</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Pre-trained Language Models Contain Human-like Biases of What is Right and Wrong to Do. (arXiv:2103.11790v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.11790","description":"<p>Artificial writing is permeating our lives due to recent advances in\nlarge-scale, transformer-based language models (LMs) such as BERT, its\nvariants, GPT-2/3, and others. Using them as pre-trained models and fine-tuning\nthem for specific tasks, researchers have extended state of the art for many\nNLP tasks and shown that they capture not only linguistic knowledge but also\nretain general knowledge implicitly present in the data. Unfortunately, LMs\ntrained on unfiltered text corpora suffer from degenerated and biased\nbehaviour. While this is well established, we show that recent LMs also contain\nhuman-like biases of what is right and wrong to do, some form of ethical and\nmoral norms of the society -- they bring a \"moral direction\" to surface. That\nis, we show that these norms can be captured geometrically by a direction,\nwhich can be computed, e.g., by a PCA, in the embedding space, reflecting well\nthe agreement of phrases to social norms implicitly expressed in the training\ntexts and providing a path for attenuating or even preventing toxic\ndegeneration in LMs. Being able to rate the (non-)normativity of arbitrary\nphrases without explicitly training the LM for this task, we demonstrate the\ncapabilities of the \"moral direction\" for guiding (even other) LMs towards\nproducing normative text and showcase it on RealToxicityPrompts testbed,\npreventing the neural toxic degeneration in GPT-2.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schramowski_P/0/1/0/all/0/1\">Patrick Schramowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turan_C/0/1/0/all/0/1\">Cigdem Turan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andersen_N/0/1/0/all/0/1\">Nico Andersen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rothkopf_C/0/1/0/all/0/1\">Constantin A. Rothkopf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1\">Kristian Kersting</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Statistical Model of Word Rank Evolution. (arXiv:2107.09948v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.09948","description":"<p>The availability of large linguistic data sets enables data-driven approaches\nto study linguistic change. The Google Books corpus unigram frequency data set\nis used to investigate the word rank dynamics in eight languages. We observed\nthe rank changes of the unigrams from 1900 to 2008 and compared it to a\nWright-Fisher inspired model that we developed for our analysis. The model\nsimulates a neutral evolutionary process with the restriction of having no\ndisappearing and added words. This work explains the mathematical framework of\nthe model - written as a Markov Chain with multinomial transition probabilities\n- to show how frequencies of words change in time. From our observations in the\ndata and our model, word rank stability shows two types of characteristics: (1)\nthe increase/decrease in ranks are monotonic, or (2) the rank stays the same.\nBased on our model, high-ranked words tend to be more stable while low-ranked\nwords tend to be more volatile. Some words change in ranks in two ways: (a) by\nan accumulation of small increasing/decreasing rank changes in time and (b) by\nshocks of increase/decrease in ranks. Most words in all of the languages we\nhave looked at are rank stable, but not as stable as a neutral model would\npredict. The stopwords and Swadesh words are observed to be rank stable across\neight languages indicating linguistic conformity in established languages.\nThese signatures suggest unigram frequencies in all languages have changed in a\nmanner inconsistent with a purely neutral evolutionary process.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Quijano_A/0/1/0/all/0/1\">Alex John Quijano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dale_R/0/1/0/all/0/1\">Rick Dale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sindi_S/0/1/0/all/0/1\">Suzanne Sindi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understand me, if you refer to Aspect Knowledge: Knowledge-aware Gated Recurrent Memory Network. (arXiv:2108.02352v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.02352","description":"<p>Aspect-level sentiment classification (ASC) aims to predict the fine-grained\nsentiment polarity towards a given aspect mentioned in a review. Despite recent\nadvances in ASC, enabling machines to preciously infer aspect sentiments is\nstill challenging. This paper tackles two challenges in ASC: (1) due to lack of\naspect knowledge, aspect representation derived in prior works is inadequate to\nrepresent aspect's exact meaning and property information; (2) prior works only\ncapture either local syntactic information or global relational information,\nthus missing either one of them leads to insufficient syntactic information. To\ntackle these challenges, we propose a novel ASC model which not only end-to-end\nembeds and leverages aspect knowledge but also marries the two kinds of\nsyntactic information and lets them compensate for each other. Our model\nincludes three key components: (1) a knowledge-aware gated recurrent memory\nnetwork recurrently integrates dynamically summarized aspect knowledge; (2) a\ndual syntax graph network combines both kinds of syntactic information to\ncomprehensively capture sufficient syntactic information; (3) a knowledge\nintegrating gate re-enhances the final representation with further needed\naspect knowledge; (4) an aspect-to-context attention mechanism aggregates the\naspect-related semantics from all hidden states into the final representation.\nExperimental results on several benchmark datasets demonstrate the\neffectiveness of our model, which overpass previous state-of-the-art models by\nlarge margins in terms of both Accuracy and Macro-F1.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xing_B/0/1/0/all/0/1\">Bowen Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsang_I/0/1/0/all/0/1\">Ivor W. Tsang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AR-BERT: Aspect-relation enhanced Aspect-level Sentiment Classification with Multi-modal Explanations. (arXiv:2108.11656v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.11656","description":"<p>Aspect level sentiment classification (ALSC) is a difficult problem with\nstate-of-the-art models showing less than 80% macro-F1 score on benchmark\ndatasets. Existing models do not incorporate information on aspect-aspect\nrelations in knowledge graphs (KGs), e.g. DBpedia. Two main challenges stem\nfrom inaccurate disambiguation of aspects to KG entities, and the inability to\nlearn aspect representations from the large KGs in joint training with ALSC\nmodels. We propose AR-BERT, a novel two-level global-local entity embedding\nscheme that allows efficient joint training of KG-based aspect embeddings and\nALSC models. A novel incorrect disambiguation detection technique addresses the\nproblem of inaccuracy in aspect disambiguation. We also introduce the problem\nof determining mode significance in multi-modal explanation generation, and\npropose a two step solution. The proposed methods show a consistent improvement\nof 2.5 - 4.1 percentage points, over the recent BERT-based baselines on\nbenchmark datasets. The code is available at\nhttps://github.com/mainuliitkgp/AR-BERT.git.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Islam_S/0/1/0/all/0/1\">Sk Mainul Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_S/0/1/0/all/0/1\">Sourangshu Bhattacharya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast-Slow Transformer for Visually Grounding Speech. (arXiv:2109.08186v3 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2109.08186","description":"<p>We present Fast-Slow Transformer for Visually Grounding Speech, or FaST-VGS.\nFaST-VGS is a Transformer-based model for learning the associations between raw\nspeech waveforms and visual images. The model unifies dual-encoder and\ncross-attention architectures into a single model, reaping the superior\nretrieval speed of the former along with the accuracy of the latter. FaST-VGS\nachieves state-of-the-art speech-image retrieval accuracy on benchmark\ndatasets, and its learned representations exhibit strong performance on the\nZeroSpeech 2021 phonetic and semantic tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Peng_P/0/1/0/all/0/1\">Puyuan Peng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Harwath_D/0/1/0/all/0/1\">David Harwath</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"JuriBERT: A Masked-Language Model Adaptation for French Legal Text. (arXiv:2110.01485v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.01485","description":"<p>Language models have proven to be very useful when adapted to specific\ndomains. Nonetheless, little research has been done on the adaptation of\ndomain-specific BERT models in the French language. In this paper, we focus on\ncreating a language model adapted to French legal text with the goal of helping\nlaw professionals. We conclude that some specific tasks do not benefit from\ngeneric language models pre-trained on large amounts of data. We explore the\nuse of smaller architectures in domain-specific sub-languages and their\nbenefits for French legal text. We prove that domain-specific pre-trained\nmodels can perform better than their equivalent generalised ones in the legal\ndomain. Finally, we release JuriBERT, a new set of BERT models adapted to the\nFrench legal domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Douka_S/0/1/0/all/0/1\">Stella Douka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdine_H/0/1/0/all/0/1\">Hadi Abdine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vazirgiannis_M/0/1/0/all/0/1\">Michalis Vazirgiannis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamdani_R/0/1/0/all/0/1\">Rajaa El Hamdani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amariles_D/0/1/0/all/0/1\">David Restrepo Amariles</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Neurons Invariant to Sentence Structural Changes in Neural Machine Translation. (arXiv:2110.03067v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.03067","description":"<p>To gain insight into the role neurons play, we study the activation patterns\ncorresponding to meaning preserving paraphrases (e.g., active-passive). We\ncompile a dataset of controlled syntactic paraphrases in English with their\nreference German translations and demonstrate our model-agnostic approach on\nthe Transformer translation model. First, we identify neurons that correlate\nacross paraphrases and examine the observed correlation for possible confounds.\nAlthough lower-level components (e.g., position embeddings) are found as the\ncause of similar activations, we identify no localizable set of neurons that\nspecifically encode these paraphrases. We further manipulate neuron activations\nto influence translation towards a particular syntactic form. We find that a\nsimple value shift is effective, and more so when many neurons are modified.\nThis may suggest that complex syntactic constructions are indeed encoded in the\nmodel. We conclude by discussing how to better manipulate it using the\ncorrelations we first obtained.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Patel_G/0/1/0/all/0/1\">Gal Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choshen_L/0/1/0/all/0/1\">Leshem Choshen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abend_O/0/1/0/all/0/1\">Omri Abend</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sm{\\aa}prat: DialoGPT for Natural Language Generation of Swedish Dialogue by Transfer Learning. (arXiv:2110.06273v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.06273","description":"<p>Building open-domain conversational systems (or chatbots) that produce\nconvincing responses is a recognized challenge. Recent state-of-the-art (SoTA)\ntransformer-based models for the generation of natural language dialogue have\ndemonstrated impressive performance in simulating human-like, single-turn\nconversations in English. This work investigates, by an empirical study, the\npotential for transfer learning of such models to Swedish language. DialoGPT,\nan English language pre-trained model, is adapted by training on three\ndifferent Swedish language conversational datasets obtained from publicly\navailable sources. Perplexity score (an automated intrinsic language model\nmetric) and surveys by human evaluation were used to assess the performances of\nthe fine-tuned models, with results that indicate that the capacity for\ntransfer learning can be exploited with considerable success. Human evaluators\nasked to score the simulated dialogue judged over 57% of the chatbot responses\nto be human-like for the model trained on the largest (Swedish) dataset. We\nprovide the demos and model checkpoints of our English and Swedish chatbots on\nthe HuggingFace platform for public use.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Adewumi_T/0/1/0/all/0/1\">Tosin Adewumi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brannvall_R/0/1/0/all/0/1\">Rickard Br&#xe4;nnvall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abid_N/0/1/0/all/0/1\">Nosheen Abid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pahlavan_M/0/1/0/all/0/1\">Maryam Pahlavan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabry_S/0/1/0/all/0/1\">Sana Sabah Sabry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liwicki_F/0/1/0/all/0/1\">Foteini Liwicki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liwicki_M/0/1/0/all/0/1\">Marcus Liwicki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comparative Explanations of Recommendations. (arXiv:2111.00670v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2111.00670","description":"<p>As recommendation is essentially a comparative (or ranking) process, a good\nexplanation should illustrate to users why an item is believed to be better\nthan another, i.e., comparative explanations about the recommended items.\nIdeally, after reading the explanations, a user should reach the same ranking\nof items as the system's. Unfortunately, little research attention has yet been\npaid on such comparative explanations.\n</p>\n<p>In this work, we develop an extract-and-refine architecture to explain the\nrelative comparisons among a set of ranked items from a recommender system. For\neach recommended item, we first extract one sentence from its associated\nreviews that best suits the desired comparison against a set of reference\nitems. Then this extracted sentence is further articulated with respect to the\ntarget user through a generative model to better explain why the item is\nrecommended. We design a new explanation quality metric based on BLEU to guide\nthe end-to-end training of the extraction and refinement components, which\navoids generation of generic content. Extensive offline evaluations on two\nlarge recommendation benchmark datasets and serious user studies against an\narray of state-of-the-art explainable recommendation algorithms demonstrate the\nnecessity of comparative explanations and the effectiveness of our solution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1\">Aobo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1\">Nan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_R/0/1/0/all/0/1\">Renqin Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_H/0/1/0/all/0/1\">Hongbo Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongning Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discourse Comprehension: A Question Answering Framework to Represent Sentence Connections. (arXiv:2111.00701v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.00701","description":"<p>While there has been substantial progress in text comprehension through\nsimple factoid question answering, more holistic comprehension of a discourse\nstill presents a major challenge. Someone critically reflecting on a text as\nthey read it will pose curiosity-driven, often open-ended questions, which\nreflect deep understanding of the content and require complex reasoning to\nanswer. A key challenge in building and evaluating models for this type of\ndiscourse comprehension is the lack of annotated data, especially since finding\nanswers to such questions (which may not be answered at all) requires high\ncognitive load for annotators over long documents. This paper presents a novel\nparadigm that enables scalable data collection targeting the comprehension of\nnews documents, viewing these questions through the lens of discourse. The\nresulting corpus, DCQA (Discourse Comprehension by Question Answering),\nconsists of 22,430 question-answer pairs across 607 English documents. DCQA\ncaptures both discourse and semantic links between sentences in the form of\nfree-form, open-ended questions. On an evaluation set that we annotated on\nquestions from the INQUISITIVE dataset, we show that DCQA provides valuable\nsupervision for answering open-ended questions. We additionally design\npre-training methods utilizing existing question-answering resources, and use\nsynthetic data to accommodate unanswerable questions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ko_W/0/1/0/all/0/1\">Wei-Jen Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dalton_C/0/1/0/all/0/1\">Cutter Dalton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simmons_M/0/1/0/all/0/1\">Mark Simmons</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fisher_E/0/1/0/all/0/1\">Eliza Fisher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrett_G/0/1/0/all/0/1\">Greg Durrett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Jessy Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How News Evolves? Modeling News Text and Coverage using Graphs and Hawkes Process. (arXiv:2112.03008v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.03008","description":"<p>Monitoring news content automatically is an important problem. The news\ncontent, unlike traditional text, has a temporal component. However, few works\nhave explored the combination of natural language processing and dynamic system\nmodels. One reason is that it is challenging to mathematically model the\nnuances of natural language. In this paper, we discuss how we built a novel\ndataset of news articles collected over time. Then, we present a method of\nconverting news text collected over time to a sequence of directed\nmulti-graphs, which represent semantic triples (Subject -&gt; Predicate}\n-&gt;Object). We model the dynamics of specific topological changes in these\ngraphs using a set of multivariate count series, which we fit the discrete-time\nHawkes process. With our real-world data, we show that the multivariate time\nseries contain both dynamic information of how many articles/words were\npublished each day and semantic information of the content of the articles.\nThis yields novel insights into how news events are covered. We show with the\nexperiment that our approach can be used to infer from a sequence of news\narticles if the articles were published by major or entertainment news outlets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Honggen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">June Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Automated Error Analysis: Learning to Characterize Errors. (arXiv:2201.05017v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.05017","description":"<p>Characterizing the patterns of errors that a system makes helps researchers\nfocus future development on increasing its accuracy and robustness. We propose\na novel form of \"meta learning\" that automatically learns interpretable rules\nthat characterize the types of errors that a system makes, and demonstrate\nthese rules' ability to help understand and improve two NLP systems. Our\napproach works by collecting error cases on validation data, extracting\nmeta-features describing these samples, and finally learning rules that\ncharacterize errors using these features. We apply our approach to VilBERT, for\nVisual Question Answering, and RoBERTa, for Common Sense Question Answering.\nOur system learns interpretable rules that provide insights into systemic\nerrors these systems make on the given tasks. Using these insights, we are also\nable to \"close the loop\" and modestly improve performance of these systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_T/0/1/0/all/0/1\">Tong Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Shivang Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mooney_R/0/1/0/all/0/1\">Raymond J. Mooney</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Clinical-Longformer and Clinical-BigBird: Transformers for long clinical sequences. (arXiv:2201.11838v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.11838","description":"<p>Transformers-based models, such as BERT, have dramatically improved the\nperformance for various natural language processing tasks. The clinical\nknowledge enriched model, namely ClinicalBERT, also achieved state-of-the-art\nresults when performed on clinical named entity recognition and natural\nlanguage inference tasks. One of the core limitations of these transformers is\nthe substantial memory consumption due to their full self-attention mechanism.\nTo overcome this, long sequence transformer models, e.g. Longformer and\nBigBird, were proposed with the idea of sparse attention mechanism to reduce\nthe memory usage from quadratic to the sequence length to a linear scale. These\nmodels extended the maximum input sequence length from 512 to 4096, which\nenhanced the ability of modeling long-term dependency and consequently achieved\noptimal results in a variety of tasks. Inspired by the success of these long\nsequence transformer models, we introduce two domain enriched language models,\nnamely Clinical-Longformer and Clinical-BigBird, which are pre-trained from\nlarge-scale clinical corpora. We evaluate both pre-trained models using 10\nbaseline tasks including named entity recognition, question answering, and\ndocument classification tasks. The results demonstrate that Clinical-Longformer\nand Clinical-BigBird consistently and significantly outperform ClinicalBERT as\nwell as other short-sequence transformers in all downstream tasks. We have made\nour source code available at\n[https://github.com/luoyuanlab/Clinical-Longformer] the pre-trained models\navailable for public download at:\n[https://huggingface.co/yikuan8/Clinical-Longformer].\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yikuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wehbe_R/0/1/0/all/0/1\">Ramsey M. Wehbe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_F/0/1/0/all/0/1\">Faraz S. Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hanyin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yuan Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Retrieval-Augmented Text Generation. (arXiv:2202.01110v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.01110","description":"<p>Recently, retrieval-augmented text generation attracted increasing attention\nof the computational linguistics community. Compared with conventional\ngeneration models, retrieval-augmented text generation has remarkable\nadvantages and particularly has achieved state-of-the-art performance in many\nNLP tasks. This paper aims to conduct a survey about retrieval-augmented text\ngeneration. It firstly highlights the generic paradigm of retrieval-augmented\ngeneration, and then it reviews notable approaches according to different tasks\nincluding dialogue response generation, machine translation, and other\ngeneration tasks. Finally, it points out some important directions on top of\nrecent methods to facilitate future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Huayang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yixuan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1\">Deng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lemao Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"No Parameters Left Behind: Sensitivity Guided Adaptive Learning Rate for Training Large Transformer Models. (arXiv:2202.02664v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.02664","description":"<p>Recent research has shown the existence of significant redundancy in large\nTransformer models. One can prune the redundant parameters without\nsignificantly sacrificing the generalization performance. However, we question\nwhether the redundant parameters could have contributed more if they were\nproperly trained. To answer this question, we propose a novel training strategy\nthat encourages all parameters to be trained sufficiently. Specifically, we\nadaptively adjust the learning rate for each parameter according to its\nsensitivity, a robust gradient-based measure reflecting this parameter's\ncontribution to the model performance. A parameter with low sensitivity is\nredundant, and we improve its fitting by increasing its learning rate. In\ncontrast, a parameter with high sensitivity is well-trained, and we regularize\nit by decreasing its learning rate to prevent further overfitting. We conduct\nextensive experiments on natural language understanding, neural machine\ntranslation, and image classification to demonstrate the effectiveness of the\nproposed schedule. Analysis shows that the proposed schedule indeed reduces the\nredundancy and improves generalization performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1\">Chen Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Haoming Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_S/0/1/0/all/0/1\">Simiao Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1\">Pengcheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaodong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tuo Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Weakly-Supervised Text Spotting using a Multi-Task Transformer. (arXiv:2202.05508v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.05508","description":"<p>Text spotting end-to-end methods have recently gained attention in the\nliterature due to the benefits of jointly optimizing the text detection and\nrecognition components. Existing methods usually have a distinct separation\nbetween the detection and recognition branches, requiring exact annotations for\nthe two tasks. We introduce TextTranSpotter (TTS), a transformer-based approach\nfor text spotting and the first text spotting framework which may be trained\nwith both fully- and weakly-supervised settings. By learning a single latent\nrepresentation per word detection, and using a novel loss function based on the\nHungarian loss, our method alleviates the need for expensive localization\nannotations. Trained with only text transcription annotations on real data, our\nweakly-supervised method achieves competitive performance with previous\nstate-of-the-art fully-supervised methods. When trained in a fully-supervised\nmanner, TextTranSpotter shows state-of-the-art results on multiple benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kittenplon_Y/0/1/0/all/0/1\">Yair Kittenplon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lavi_I/0/1/0/all/0/1\">Inbal Lavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fogel_S/0/1/0/all/0/1\">Sharon Fogel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bar_Y/0/1/0/all/0/1\">Yarin Bar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manmatha_R/0/1/0/all/0/1\">R. Manmatha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perona_P/0/1/0/all/0/1\">Pietro Perona</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What Does it Mean for a Language Model to Preserve Privacy?. (arXiv:2202.05520v2 [stat.ML] UPDATED)","link":"http://arxiv.org/abs/2202.05520","description":"<p>Natural language reflects our private lives and identities, making its\nprivacy concerns as broad as those of real life. Language models lack the\nability to understand the context and sensitivity of text, and tend to memorize\nphrases present in their training sets. An adversary can exploit this tendency\nto extract training data. Depending on the nature of the content and the\ncontext in which this data was collected, this could violate expectations of\nprivacy. Thus there is a growing interest in techniques for training language\nmodels that preserve privacy. In this paper, we discuss the mismatch between\nthe narrow assumptions made by popular data protection techniques (data\nsanitization and differential privacy), and the broadness of natural language\nand of privacy as a social norm. We argue that existing protection methods\ncannot guarantee a generic and meaningful notion of privacy for language\nmodels. We conclude that language models should be trained on text data which\nwas explicitly produced for public use.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Brown_H/0/1/0/all/0/1\">Hannah Brown</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lee_K/0/1/0/all/0/1\">Katherine Lee</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Mireshghallah_F/0/1/0/all/0/1\">Fatemehsadat Mireshghallah</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Shokri_R/0/1/0/all/0/1\">Reza Shokri</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Tramer_F/0/1/0/all/0/1\">Florian Tram&#xe8;r</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"White-Box Attacks on Hate-speech BERT Classifiers in German with Explicit and Implicit Character Level Defense. (arXiv:2202.05778v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.05778","description":"<p>In this work, we evaluate the adversarial robustness of BERT models trained\non German Hate Speech datasets. We also complement our evaluation with two\nnovel white-box character and word level attacks thereby contributing to the\nrange of attacks available. Furthermore, we also perform a comparison of two\nnovel character-level defense strategies and evaluate their robustness with one\nanother.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1\">Shahrukh Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahid_M/0/1/0/all/0/1\">Mahnoor Shahid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_N/0/1/0/all/0/1\">Navdeeppal Singh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-02-14T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Motion Correction and Volumetric Reconstruction for Fetal Functional Magnetic Resonance Imaging Data. (arXiv:2202.05863v1 [cs.CV])","link":"http://arxiv.org/abs/2202.05863","description":"<p>Motion correction is an essential preprocessing step in functional Magnetic\nResonance Imaging (fMRI) of the fetal brain with the aim to remove artifacts\ncaused by fetal movement and maternal breathing and consequently to suppress\nerroneous signal correlations. Current motion correction approaches for fetal\nfMRI choose a single 3D volume from a specific acquisition timepoint with least\nmotion artefacts as reference volume, and perform interpolation for the\nreconstruction of the motion corrected time series. The results can suffer, if\nno low-motion frame is available, and if reconstruction does not exploit any\nassumptions about the continuity of the fMRI signal. Here, we propose a novel\nframework, which estimates a high-resolution reference volume by using\noutlier-robust motion correction, and by utilizing Huber L2 regularization for\nintra-stack volumetric reconstruction of the motion-corrected fetal brain fMRI.\nWe performed an extensive parameter study to investigate the effectiveness of\nmotion estimation and present in this work benchmark metrics to quantify the\neffect of motion correction and regularised volumetric reconstruction\napproaches on functional connectivity computations. We demonstrate the proposed\nframework's ability to improve functional connectivity estimates,\nreproducibility and signal interpretability, which is clinically highly\ndesirable for the establishment of prognostic noninvasive imaging biomarkers.\nThe motion correction and volumetric reconstruction framework is made available\nas an open-source package of NiftyMIC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sobotka_D/0/1/0/all/0/1\">Daniel Sobotka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ebner_M/0/1/0/all/0/1\">Michael Ebner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_E/0/1/0/all/0/1\">Ernst Schwartz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nenning_K/0/1/0/all/0/1\">Karl-Heinz Nenning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taymourtash_A/0/1/0/all/0/1\">Athena Taymourtash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vercauteren_T/0/1/0/all/0/1\">Tom Vercauteren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ourselin_S/0/1/0/all/0/1\">Sebastien Ourselin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasprian_G/0/1/0/all/0/1\">Gregor Kasprian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prayer_D/0/1/0/all/0/1\">Daniela Prayer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Langs_G/0/1/0/all/0/1\">Georg Langs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Licandro_R/0/1/0/all/0/1\">Roxane Licandro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-level Latent Space Structuring for Generative Control. (arXiv:2202.05910v1 [cs.CV])","link":"http://arxiv.org/abs/2202.05910","description":"<p>Truncation is widely used in generative models for improving the quality of\nthe generated samples, at the expense of reducing their diversity. We propose\nto leverage the StyleGAN generative architecture to devise a new truncation\ntechnique, based on a decomposition of the latent space into clusters, enabling\ncustomized truncation to be performed at multiple semantic levels. We do so by\nlearning to re-generate W-space, the extended intermediate latent space of\nStyleGAN, using a learnable mixture of Gaussians, while simultaneously training\na classifier to identify, for each latent vector, the cluster that it belongs\nto. The resulting truncation scheme is more faithful to the original\nuntruncated samples and allows a better trade-off between quality and\ndiversity. We compare our method to other truncation approaches for StyleGAN,\nboth qualitatively and quantitatively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Katzir_O/0/1/0/all/0/1\">Oren Katzir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perepelook_V/0/1/0/all/0/1\">Vicky Perepelook</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lischinski_D/0/1/0/all/0/1\">Dani Lischinski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_Or_D/0/1/0/all/0/1\">Daniel Cohen-Or</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Signatures -- Learning Invariants of Planar Curves. (arXiv:2202.05922v1 [cs.CV])","link":"http://arxiv.org/abs/2202.05922","description":"<p>We propose a learning paradigm for numerical approximation of differential\ninvariants of planar curves. Deep neural-networks' (DNNs) universal\napproximation properties are utilized to estimate geometric measures. The\nproposed framework is shown to be a preferable alternative to axiomatic\nconstructions. Specifically, we show that DNNs can learn to overcome\ninstabilities and sampling artifacts and produce numerically-stable signatures\nfor curves subject to a given group of transformations in the plane. We compare\nthe proposed schemes to alternative state-of-the-art axiomatic constructions of\ngroup invariant arc-lengths and curvatures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Velich_R/0/1/0/all/0/1\">Roy Velich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kimmel_R/0/1/0/all/0/1\">Ron Kimmel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting out-of-context objects using contextual cues. (arXiv:2202.05930v1 [cs.CV])","link":"http://arxiv.org/abs/2202.05930","description":"<p>This paper presents an approach to detect out-of-context (OOC) objects in an\nimage. Given an image with a set of objects, our goal is to determine if an\nobject is inconsistent with the scene context and detect the OOC object with a\nbounding box. In this work, we consider commonly explored contextual relations\nsuch as co-occurrence relations, the relative size of an object with respect to\nother objects, and the position of the object in the scene. We posit that\ncontextual cues are useful to determine object labels for in-context objects\nand inconsistent context cues are detrimental to determining object labels for\nout-of-context objects. To realize this hypothesis, we propose a graph\ncontextual reasoning network (GCRN) to detect OOC objects. GCRN consists of two\nseparate graphs to predict object labels based on the contextual cues in the\nimage: 1) a representation graph to learn object features based on the\nneighboring objects and 2) a context graph to explicitly capture contextual\ncues from the neighboring objects. GCRN explicitly captures the contextual cues\nto improve the detection of in-context objects and identify objects that\nviolate contextual relations. In order to evaluate our approach, we create a\nlarge-scale dataset by adding OOC object instances to the COCO images. We also\nevaluate on recent OCD benchmark. Our results show that GCRN outperforms\ncompetitive baselines in detecting OOC objects and correctly detecting\nin-context objects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Acharya_M/0/1/0/all/0/1\">Manoj Acharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_A/0/1/0/all/0/1\">Anirban Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koneripalli_K/0/1/0/all/0/1\">Kaushik Koneripalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1\">Susmit Jha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanan_C/0/1/0/all/0/1\">Christopher Kanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Divakaran_A/0/1/0/all/0/1\">Ajay Divakaran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain-Invariant Proposals based on a Balanced Domain Classifier for Object Detection. (arXiv:2202.05941v1 [cs.CV])","link":"http://arxiv.org/abs/2202.05941","description":"<p>Object recognition from images means to automatically find object(s) of\ninterest and to return their category and location information. Benefiting from\nresearch on deep learning, like convolutional neural networks~(CNNs) and\ngenerative adversarial networks, the performance in this field has been\nimproved significantly, especially when training and test data are drawn from\nsimilar distributions. However, mismatching distributions, i.e., domain shifts,\nlead to a significant performance drop. In this paper, we build\ndomain-invariant detectors by learning domain classifiers via adversarial\ntraining. Based on the previous works that align image and instance level\nfeatures, we mitigate the domain shift further by introducing a domain\nadaptation component at the region level within Faster \\mbox{R-CNN}. We embed a\ndomain classification network in the region proposal network~(RPN) using\nadversarial learning. The RPN can now generate accurate region proposals in\ndifferent domains by effectively aligning the features between them. To\nmitigate the unstable convergence during the adversarial learning, we introduce\na balanced domain classifier as well as a network learning rate adjustment\nstrategy. We conduct comprehensive experiments using four standard datasets.\nThe results demonstrate the effectiveness and robustness of our object\ndetection approach in domain shift scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhize Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaofeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Tong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xuebin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_L/0/1/0/all/0/1\">Le Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lixiang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weise_T/0/1/0/all/0/1\">Thomas Weise</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Open-set Adversarial Defense with Clean-Adversarial Mutual Learning. (arXiv:2202.05953v1 [cs.CV])","link":"http://arxiv.org/abs/2202.05953","description":"<p>Open-set recognition and adversarial defense study two key aspects of deep\nlearning that are vital for real-world deployment. The objective of open-set\nrecognition is to identify samples from open-set classes during testing, while\nadversarial defense aims to robustify the network against images perturbed by\nimperceptible adversarial noise. This paper demonstrates that open-set\nrecognition systems are vulnerable to adversarial samples. Furthermore, this\npaper shows that adversarial defense mechanisms trained on known classes are\nunable to generalize well to open-set samples. Motivated by these observations,\nwe emphasize the necessity of an Open-Set Adversarial Defense (OSAD) mechanism.\nThis paper proposes an Open-Set Defense Network with Clean-Adversarial Mutual\nLearning (OSDN-CAML) as a solution to the OSAD problem. The proposed network\ndesigns an encoder with dual-attentive feature-denoising layers coupled with a\nclassifier to learn a noise-free latent feature representation, which\nadaptively removes adversarial noise guided by channel and spatial-wise\nattentive filters. Several techniques are exploited to learn a noise-free and\ninformative latent feature space with the aim of improving the performance of\nadversarial defense and open-set recognition. First, we incorporate a decoder\nto ensure that clean images can be well reconstructed from the obtained latent\nfeatures. Then, self-supervision is used to ensure that the latent features are\ninformative enough to carry out an auxiliary task. Finally, to exploit more\ncomplementary knowledge from clean image classification to facilitate feature\ndenoising and search for a more generalized local minimum for open-set\nrecognition, we further propose clean-adversarial mutual learning, where a peer\nnetwork (classifying clean images) is further introduced to mutually learn with\nthe classifier (classifying adversarial images).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shao_R/0/1/0/all/0/1\">Rui Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perera_P/0/1/0/all/0/1\">Pramuditha Perera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuen_P/0/1/0/all/0/1\">Pong C. Yuen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1\">Vishal M. Patel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Audio-Visual Fusion Layers for Event Type Aware Video Recognition. (arXiv:2202.05961v1 [cs.CV])","link":"http://arxiv.org/abs/2202.05961","description":"<p>Human brain is continuously inundated with the multisensory information and\ntheir complex interactions coming from the outside world at any given moment.\nSuch information is automatically analyzed by binding or segregating in our\nbrain. While this task might seem effortless for human brains, it is extremely\nchallenging to build a machine that can perform similar tasks since complex\ninteractions cannot be dealt with single type of integration but requires more\nsophisticated approaches. In this paper, we propose a new model to address the\nmultisensory integration problem with individual event-specific layers in a\nmulti-task learning scheme. Unlike previous works where single type of fusion\nis used, we design event-specific layers to deal with different audio-visual\nrelationship tasks, enabling different ways of audio-visual formation.\nExperimental results show that our event-specific layers can discover unique\nproperties of the audio-visual relationships in the videos. Moreover, although\nour network is formulated with single labels, it can output additional true\nmulti-labels to represent the given videos. We demonstrate that our proposed\nframework also exposes the modality bias of the video data category-wise and\ndataset-wise manner in popular benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Senocak_A/0/1/0/all/0/1\">Arda Senocak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Junsik Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_T/0/1/0/all/0/1\">Tae-Hyun Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryu_H/0/1/0/all/0/1\">Hyeonggon Ryu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dingzeyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kweon_I/0/1/0/all/0/1\">In So Kweon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Low-light Image Enhancement by Retinex Based Algorithm Unrolling and Adjustment. (arXiv:2202.05972v1 [cs.CV])","link":"http://arxiv.org/abs/2202.05972","description":"<p>Motivated by their recent advances, deep learning techniques have been widely\napplied to low-light image enhancement (LIE) problem. Among which, Retinex\ntheory based ones, mostly following a decomposition-adjustment pipeline, have\ntaken an important place due to its physical interpretation and promising\nperformance. However, current investigations on Retinex based deep learning are\nstill not sufficient, ignoring many useful experiences from traditional\nmethods. Besides, the adjustment step is either performed with simple image\nprocessing techniques, or by complicated networks, both of which are\nunsatisfactory in practice. To address these issues, we propose a new deep\nlearning framework for the LIE problem. The proposed framework contains a\ndecomposition network inspired by algorithm unrolling, and adjustment networks\nconsidering both global brightness and local brightness sensitivity. By virtue\nof algorithm unrolling, both implicit priors learned from data and explicit\npriors borrowed from traditional methods can be embedded in the network,\nfacilitate to better decomposition. Meanwhile, the consideration of global and\nlocal brightness can guide designing simple yet effective network modules for\nadjustment. Besides, to avoid manually parameter tuning, we also propose a\nself-supervised fine-tuning strategy, which can always guarantee a promising\nperformance. Experiments on a series of typical LIE datasets demonstrated the\neffectiveness of the proposed method, both quantitatively and visually, as\ncompared with existing methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xinyi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Q/0/1/0/all/0/1\">Qi Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1\">Qian Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qang_H/0/1/0/all/0/1\">Hong Qang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_D/0/1/0/all/0/1\">Deyu Meng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncalibrated Models Can Improve Human-AI Collaboration. (arXiv:2202.05983v1 [cs.AI])","link":"http://arxiv.org/abs/2202.05983","description":"<p>In many practical applications of AI, an AI model is used as a decision aid\nfor human users. The AI provides advice that a human (sometimes) incorporates\ninto their decision-making process. The AI advice is often presented with some\nmeasure of \"confidence\" that the human can use to calibrate how much they\ndepend on or trust the advice. In this paper, we demonstrate that presenting AI\nmodels as more confident than they actually are, even when the original AI is\nwell-calibrated, can improve human-AI performance (measured as the accuracy and\nconfidence of the human's final prediction after seeing the AI advice). We\nfirst learn a model for how humans incorporate AI advice using data from\nthousands of human interactions. This enables us to explicitly estimate how to\ntransform the AI's prediction confidence, making the AI uncalibrated, in order\nto improve the final human prediction. We empirically validate our results\nacross four different tasks -- dealing with images, text and tabular data --\ninvolving hundreds of human participants. We further support our findings with\nsimulation analysis. Our findings suggest the importance of and a framework for\njointly optimizing the human-AI system as opposed to the standard paradigm of\noptimizing the AI model alone.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vodrahalli_K/0/1/0/all/0/1\">Kailas Vodrahalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gerstenberg_T/0/1/0/all/0/1\">Tobias Gerstenberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1\">James Zou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RSINet: Inpainting Remotely Sensed Images Using Triple GAN Framework. (arXiv:2202.05988v1 [cs.CV])","link":"http://arxiv.org/abs/2202.05988","description":"<p>We tackle the problem of image inpainting in the remote sensing domain.\nRemote sensing images possess high resolution and geographical variations, that\nrender the conventional inpainting methods less effective. This further entails\nthe requirement of models with high complexity to sufficiently capture the\nspectral, spatial and textural nuances within an image, emerging from its high\nspatial variability. To this end, we propose a novel inpainting method that\nindividually focuses on each aspect of an image such as edges, colour and\ntexture using a task specific GAN. Moreover, each individual GAN also\nincorporates the attention mechanism that explicitly extracts the spectral and\nspatial features. To ensure consistent gradient flow, the model uses residual\nlearning paradigm, thus simultaneously working with high and low level\nfeatures. We evaluate our model, alongwith previous state of the art models, on\nthe two well known remote sensing datasets, Open Cities AI and Earth on Canvas,\nand achieve competitive performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Advait Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tamboli_D/0/1/0/all/0/1\">Dipesh Tamboli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pande_S/0/1/0/all/0/1\">Shivam Pande</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_B/0/1/0/all/0/1\">Biplab Banerjee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-direction and Multi-scale Pyramid in Transformer for Video-based Pedestrian Retrieval. (arXiv:2202.06014v1 [cs.CV])","link":"http://arxiv.org/abs/2202.06014","description":"<p>In video surveillance, pedestrian retrieval (also called person\nre-identification) is a critical task. This task aims to retrieve the\npedestrian of interest from non-overlapping cameras. Recently,\ntransformer-based models have achieved significant progress for this task.\nHowever, these models still suffer from ignoring fine-grained, part-informed\ninformation. This paper proposes a multi-direction and multi-scale Pyramid in\nTransformer (PiT) to solve this problem. In transformer-based architecture,\neach pedestrian image is split into many patches. Then, these patches are fed\nto transformer layers to obtain the feature representation of this image. To\nexplore the fine-grained information, this paper proposes to apply vertical\ndivision and horizontal division on these patches to generate\ndifferent-direction human parts. These parts provide more fine-grained\ninformation. To fuse multi-scale feature representation, this paper presents a\npyramid structure containing global-level information and many pieces of\nlocal-level information from different scales. The feature pyramids of all the\npedestrian images from the same video are fused to form the final\nmulti-direction and multi-scale feature representation. Experimental results on\ntwo challenging video-based benchmarks, MARS and iLIDS-VID, show the proposed\nPiT achieves state-of-the-art performance. Extensive ablation studies\ndemonstrate the superiority of the proposed pyramid structure. The code is\navailable at https://git.openi.org.cn/zangxh/PiT.git.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zang_X/0/1/0/all/0/1\">Xianghao Zang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Ge Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1\">Wei Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fun Selfie Filters in Face Recognition: Impact Assessment and Removal. (arXiv:2202.06022v1 [cs.CV])","link":"http://arxiv.org/abs/2202.06022","description":"<p>This work investigates the impact of fun selfie filters, which are frequently\nused to modify selfies, on face recognition systems. Based on a qualitative\nassessment and classification of freely available mobile applications, ten\nrelevant fun selfie filters are selected to create a database. To this end, the\nselected filters are automatically applied to face images of public face image\ndatabases. Different state-of-the-art methods are used to evaluate the\ninfluence of fun selfie filters on the performance of face detection using\ndlib, RetinaFace, and a COTS method, sample quality estimated by FaceQNet and\nMagFace, and recognition accuracy employing ArcFace and a COTS algorithm. The\nobtained results indicate that selfie filters negatively affect face\nrecognition modules, especially if fun selfie filters cover a large region of\nthe face, where the mouth, nose, and eyes are covered. To mitigate such\nunwanted effects, a GAN-based selfie filter removal algorithm is proposed which\nconsists of a segmentation module, a perceptual network, and a generation\nmodule. In a cross-database experiment the application of the presented selfie\nfilter removal technique has shown to significantly improve the biometric\nperformance of the underlying face recognition systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Botezatu_C/0/1/0/all/0/1\">Cristian Botezatu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ibsen_M/0/1/0/all/0/1\">Mathias Ibsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rathgeb_C/0/1/0/all/0/1\">Christian Rathgeb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Busch_C/0/1/0/all/0/1\">Christoph Busch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-end Reinforcement Learning of Robotic Manipulation with Robust Keypoints Representation. (arXiv:2202.06027v1 [cs.RO])","link":"http://arxiv.org/abs/2202.06027","description":"<p>We present an end-to-end Reinforcement Learning(RL) framework for robotic\nmanipulation tasks, using a robust and efficient keypoints representation. The\nproposed method learns keypoints from camera images as the state\nrepresentation, through a self-supervised autoencoder architecture. The\nkeypoints encode the geometric information, as well as the relationship of the\ntool and target in a compact representation to ensure efficient and robust\nlearning. After keypoints learning, the RL step then learns the robot motion\nfrom the extracted keypoints state representation. The keypoints and RL\nlearning processes are entirely done in the simulated environment. We\ndemonstrate the effectiveness of the proposed method on robotic manipulation\ntasks including grasping and pushing, in different scenarios. We also\ninvestigate the generalization capability of the trained model. In addition to\nthe robust keypoints representation, we further apply domain randomization and\nadversarial training examples to achieve zero-shot sim-to-real transfer in\nreal-world robotic manipulation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianying Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Puang_E/0/1/0/all/0/1\">En Yen Puang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Marcus Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jing_W/0/1/0/all/0/1\">Wei Jing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OctAttention: Octree-based Large-scale Contexts Model for Point Cloud Compression. (arXiv:2202.06028v1 [cs.CV])","link":"http://arxiv.org/abs/2202.06028","description":"<p>In point cloud compression, sufficient contexts are significant for modeling\nthe point cloud distribution. However, the contexts gathered by the previous\nvoxel-based methods decrease when handling sparse point clouds. To address this\nproblem, we propose a multiple-contexts deep learning framework called\nOctAttention employing the octree structure, a memory-efficient representation\nfor point clouds. Our approach encodes octree symbol sequences in a lossless\nway by gathering the information of sibling and ancestor nodes. Expressly, we\nfirst represent point clouds with octree to reduce spatial redundancy, which is\nrobust for point clouds with different resolutions. We then design a\nconditional entropy model with a large receptive field that models the sibling\nand ancestor contexts to exploit the strong dependency among the neighboring\nnodes and employ an attention mechanism to emphasize the correlated nodes in\nthe context. Furthermore, we introduce a mask operation during training and\ntesting to make a trade-off between encoding time and performance. Compared to\nthe previous state-of-the-art works, our approach obtains a 10%-35% BD-Rate\ngain on the LiDAR benchmark (e.g. SemanticKITTI) and object point cloud dataset\n(e.g. MPEG 8i, MVUB), and saves 95% coding time compared to the voxel-based\nbaseline. The code is available at https://github.com/zb12138/OctAttention.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1\">Chunyang Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Ge Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_R/0/1/0/all/0/1\">Rui Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1\">Wei Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shan Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Depth-Cooperated Trimodal Network for Video Salient Object Detection. (arXiv:2202.06060v1 [cs.CV])","link":"http://arxiv.org/abs/2202.06060","description":"<p>Depth can provide useful geographical cues for salient object detection\n(SOD), and has been proven helpful in recent RGB-D SOD methods. However,\nexisting video salient object detection (VSOD) methods only utilize\nspatiotemporal information and seldom exploit depth information for detection.\nIn this paper, we propose a depth-cooperated trimodal network, called DCTNet\nfor VSOD, which is a pioneering work to incorporate depth information to assist\nVSOD. To this end, we first generate depth from RGB frames, and then propose an\napproach to treat the three modalities unequally. Specifically, a multi-modal\nattention module (MAM) is designed to model multi-modal long-range dependencies\nbetween the main modality (RGB) and the two auxiliary modalities (depth,\noptical flow). We also introduce a refinement fusion module (RFM) to suppress\nnoises in each modality and select useful information dynamically for further\nfeature refinement. Lastly, a progressive fusion strategy is adopted after the\nrefined features to achieve final cross-modal fusion. Experiments on five\nbenchmark datasets demonstrate the superiority of our depth-cooperated model\nagainst 12 state-of-the-art methods, and the necessity of depth is also\nvalidated.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yukang Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_D/0/1/0/all/0/1\">Dingyao Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_K/0/1/0/all/0/1\">Keren Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1\">Qijun Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Classification of Microscopy Images of Breast Tissue: Region Duplication based Self-Supervision vs. Off-the Shelf Deep Representations. (arXiv:2202.06073v1 [eess.IV])","link":"http://arxiv.org/abs/2202.06073","description":"<p>Breast cancer is one of the leading causes of female mortality in the world.\nThis can be reduced when diagnoses are performed at the early stages of\nprogression. Further, the efficiency of the process can be significantly\nimproved with computer aided diagnosis. Deep learning based approaches have\nbeen successfully applied to achieve this. One of the limiting factors for\ntraining deep networks in a supervised manner is the dependency on large\namounts of expert annotated data. In reality, large amounts of unlabelled data\nand only small amounts of expert annotated data are available. In such\nscenarios, transfer learning approaches and self-supervised learning (SSL)\nbased approaches can be leveraged. In this study, we propose a novel\nself-supervision pretext task to train a convolutional neural network (CNN) and\nextract domain specific features. This method was compared with deep features\nextracted using pre-trained CNNs such as DenseNet-121 and ResNet-50 trained on\nImageNet. Additionally, two types of patch-combination methods were introduced\nand compared with majority voting. The methods were validated on the BACH\nmicroscopy images dataset. Results indicated that the best performance of 99%\nsensitivity was achieved for the deep features extracted using ResNet50 with\nconcatenation of patch-level embedding. Preliminary results of SSL to extract\ndomain specific features indicated that with just 15% of unlabelled data a high\nsensitivity of 94% can be achieved for a four class classification of\nmicroscopy images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ravi_A/0/1/0/all/0/1\">Aravind Ravi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Indication as Prior Knowledge for Multimodal Disease Classification in Chest Radiographs with Transformers. (arXiv:2202.06076v1 [cs.CV])","link":"http://arxiv.org/abs/2202.06076","description":"<p>When a clinician refers a patient for an imaging exam, they include the\nreason (e.g. relevant patient history, suspected disease) in the scan request;\nthis appears as the indication field in the radiology report. The\ninterpretation and reporting of the image are substantially influenced by this\nrequest text, steering the radiologist to focus on particular aspects of the\nimage. We use the indication field to drive better image classification, by\ntaking a transformer network which is unimodally pre-trained on text (BERT) and\nfine-tuning it for multimodal classification of a dual image-text input. We\nevaluate the method on the MIMIC-CXR dataset, and present ablation studies to\ninvestigate the effect of the indication field on the classification\nperformance. The experimental results show our approach achieves 87.8 average\nmicro AUROC, outperforming the state-of-the-art methods for unimodal (84.4) and\nmultimodal (86.0) classification. Our code is available at\nhttps://github.com/jacenkow/mmbt.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jacenkow_G/0/1/0/all/0/1\">Grzegorz Jacenk&#xf3;w</a>, <a href=\"http://arxiv.org/find/cs/1/au:+ONeil_A/0/1/0/all/0/1\">Alison Q. O&#x27;Neil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsaftaris_S/0/1/0/all/0/1\">Sotirios A. Tsaftaris</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text and Image Guided 3D Avatar Generation and Manipulation. (arXiv:2202.06079v1 [cs.CV])","link":"http://arxiv.org/abs/2202.06079","description":"<p>The manipulation of latent space has recently become an interesting topic in\nthe field of generative models. Recent research shows that latent directions\ncan be used to manipulate images towards certain attributes. However,\ncontrolling the generation process of 3D generative models remains a challenge.\nIn this work, we propose a novel 3D manipulation method that can manipulate\nboth the shape and texture of the model using text or image-based prompts such\nas 'a young face' or 'a surprised face'. We leverage the power of Contrastive\nLanguage-Image Pre-training (CLIP) model and a pre-trained 3D GAN model\ndesigned to generate face avatars, and create a fully differentiable rendering\npipeline to manipulate meshes. More specifically, our method takes an input\nlatent code and modifies it such that the target attribute specified by a text\nor image prompt is present or enhanced, while leaving other attributes largely\nunaffected. Our method requires only 5 minutes per manipulation, and we\ndemonstrate the effectiveness of our approach with extensive results and\ncomparisons.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Canfes_Z/0/1/0/all/0/1\">Zehranaz Canfes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atasoy_M/0/1/0/all/0/1\">M. Furkan Atasoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dirik_A/0/1/0/all/0/1\">Alara Dirik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yanardag_P/0/1/0/all/0/1\">Pinar Yanardag</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recognition-free Question Answering on Handwritten Document Collections. (arXiv:2202.06080v1 [cs.CV])","link":"http://arxiv.org/abs/2202.06080","description":"<p>In recent years, considerable progress has been made in the research area of\nQuestion Answering (QA) on document images. Current QA approaches from the\nDocument Image Analysis community are mainly focusing on machine-printed\ndocuments and perform rather limited on handwriting. This is mainly due to the\nreduced recognition performance on handwritten documents. To tackle this\nproblem, we propose a recognition-free QA approach, especially designed for\nhandwritten document image collections. We present a robust document retrieval\nmethod, as well as two QA models. Our approaches outperform the\nstate-of-the-art recognition-free models on the challenging BenthamQA and\nHW-SQuAD datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tuselmann_O/0/1/0/all/0/1\">Oliver T&#xfc;selmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_F/0/1/0/all/0/1\">Friedrich M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_F/0/1/0/all/0/1\">Fabian Wolf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fink_G/0/1/0/all/0/1\">Gernot A. Fink</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NeuVV: Neural Volumetric Videos with Immersive Rendering and Editing. (arXiv:2202.06088v1 [cs.CV])","link":"http://arxiv.org/abs/2202.06088","description":"<p>Some of the most exciting experiences that Metaverse promises to offer, for\ninstance, live interactions with virtual characters in virtual environments,\nrequire real-time photo-realistic rendering. 3D reconstruction approaches to\nrendering, active or passive, still require extensive cleanup work to fix the\nmeshes or point clouds. In this paper, we present a neural volumography\ntechnique called neural volumetric video or NeuVV to support immersive,\ninteractive, and spatial-temporal rendering of volumetric video contents with\nphoto-realism and in real-time. The core of NeuVV is to efficiently encode a\ndynamic neural radiance field (NeRF) into renderable and editable primitives.\nWe introduce two types of factorization schemes: a hyper-spherical harmonics\n(HH) decomposition for modeling smooth color variations over space and time and\na learnable basis representation for modeling abrupt density and color changes\ncaused by motion. NeuVV factorization can be integrated into a Video Octree\n(VOctree) analogous to PlenOctree to significantly accelerate training while\nreducing memory overhead. Real-time NeuVV rendering further enables a class of\nimmersive content editing tools. Specifically, NeuVV treats each VOctree as a\nprimitive and implements volume-based depth ordering and alpha blending to\nrealize spatial-temporal compositions for content re-purposing. For example, we\ndemonstrate positioning varied manifestations of the same performance at\ndifferent 3D locations with different timing, adjusting color/texture of the\nperformer's clothing, casting spotlight shadows and synthesizing distance\nfalloff lighting, etc, all at an interactive speed. We further develop a hybrid\nneural-rasterization rendering framework to support consumer-level VR headsets\nso that the aforementioned volumetric video viewing and editing, for the first\ntime, can be conducted immersively in virtual 3D space.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiakai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xinhang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_F/0/1/0/all/0/1\">Fuqiang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Minzhang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1\">Haizhao Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Boyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jingyi Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Review of Deep Learning-based Approaches for Deepfake Content Detection. (arXiv:2202.06095v1 [cs.CV])","link":"http://arxiv.org/abs/2202.06095","description":"<p>The fast-spreading information over the internet is essential to support the\nrapid supply of numerous public utility services and entertainment to users.\nSocial networks and online media paved the way for modern,\ntimely-communication-fashion and convenient access to all types of information.\nHowever, it also provides new chances for ill use of the massive amount of\navailable data, such as spreading fake content to manipulate public opinion.\nDetection of counterfeit content has raised attention in the last few years for\nthe advances in deepfake generation. The rapid growth of machine learning\ntechniques, particularly deep learning, can predict fake content in several\napplication domains, including fake image and video manipulation. This paper\npresents a comprehensive review of recent studies for deepfake content\ndetection using deep learning-based approaches. We aim to broaden the\nstate-of-the-art research by systematically reviewing the different categories\nof fake content detection. Furthermore, we report the advantages and drawbacks\nof the examined works and future directions towards the issues and shortcomings\nstill unsolved on deepfake detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Passos_L/0/1/0/all/0/1\">Leandro A. Passos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jodas_D/0/1/0/all/0/1\">Danilo Jodas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Costa_K/0/1/0/all/0/1\">Kelton A. P. da Costa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Junior_L/0/1/0/all/0/1\">Luis A. Souza J&#xfa;nior</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colombo_D/0/1/0/all/0/1\">Danilo Colombo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papa_J/0/1/0/all/0/1\">Jo&#xe3;o Paulo Papa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-supervised Medical Image Segmentation via Geometry-aware Consistency Training. (arXiv:2202.06104v1 [eess.IV])","link":"http://arxiv.org/abs/2202.06104","description":"<p>The performance of supervised deep learning methods for medical image\nsegmentation is often limited by the scarcity of labeled data. As a promising\nresearch direction, semi-supervised learning addresses this dilemma by\nleveraging unlabeled data information to assist the learning process. In this\npaper, a novel geometry-aware semi-supervised learning framework is proposed\nfor medical image segmentation, which is a consistency-based method.\nConsidering that the hard-to-segment regions are mainly located around the\nobject boundary, we introduce an auxiliary prediction task to learn the global\ngeometric information. Based on the geometric constraint, the ambiguous\nboundary regions are emphasized through an exponentially weighted strategy for\nthe model training to better exploit both labeled and unlabeled data. In\naddition, a dual-view network is designed to perform segmentation from\ndifferent perspectives and reduce the prediction uncertainty. The proposed\nmethod is evaluated on the public left atrium benchmark dataset and improves\nfully supervised method by 8.7% in Dice with 10% labeled images, while 4.3%\nwith 20% labeled images. Meanwhile, our framework outperforms six\nstate-of-the-art semi-supervised segmentation methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Liu_Z/0/1/0/all/0/1\">Zihang Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_C/0/1/0/all/0/1\">Chunhui Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Breast Cancer Detection using Histopathological Images. (arXiv:2202.06109v1 [eess.IV])","link":"http://arxiv.org/abs/2202.06109","description":"<p>Cancer is one of the most common and fatal diseases in the world. Breast\ncancer affects one in every eight women and one in every eight hundred men.\nHence, our prime target should be early detection of cancer because the early\ndetection of cancer can be helpful to cure cancer effectively. Therefore, we\npropose a saliency detection system with the help of advanced deep learning\ntechniques, such that the machine will be taught to emulate actions of\npathologists for localization of diagnostically pertinent regions. We study\nidentification of five diagnostic categories of breast cancer by training a CNN\n(VGG16, ResNet architecture). We have used BreakHis dataset to train our model.\nWe focus on both detection and classification of cancerous regions in\nhistopathology images. The diagnostically relevant regions are salient. The\ndetection system will be available as an open source web application which can\nbe used by pathologists and medical institutions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Maan_J/0/1/0/all/0/1\">Jitendra Maan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maan_H/0/1/0/all/0/1\">Harsh Maan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-task Deep Learning for Cerebrovascular Disease Classification and MRI-to-PET Translation. (arXiv:2202.06142v1 [eess.IV])","link":"http://arxiv.org/abs/2202.06142","description":"<p>Accurate quantification of cerebral blood flow (CBF) is essential for the\ndiagnosis and assessment of cerebrovascular diseases such as Moyamoya, carotid\nstenosis, aneurysms, and stroke. Positron emission tomography (PET) is\ncurrently regarded as the gold standard for the measurement of CBF in the human\nbrain. PET imaging, however, is not widely available because of its prohibitive\ncosts, use of ionizing radiation, and logistical challenges, which require a\nco-localized cyclotron to deliver the 2 min half-life Oxygen-15 radioisotope.\nMagnetic resonance imaging (MRI), in contrast, is more readily available and\ndoes not involve ionizing radiation. In this study, we propose a multi-task\nlearning framework for brain MRI-to-PET translation and disease diagnosis. The\nproposed framework comprises two prime networks: (1) an attention-based 3D\nencoder-decoder convolutional neural network (CNN) that synthesizes\nhigh-quality PET CBF maps from multi-contrast MRI images, and (2) a multi-scale\n3D CNN that identifies the brain disease corresponding to the input MRI images.\nOur multi-task framework yields promising results on the task of MRI-to-PET\ntranslation, achieving an average structural similarity index (SSIM) of 0.94\nand peak signal-to-noise ratio (PSNR) of 38dB on a cohort of 120 subjects. In\naddition, we show that integrating multiple MRI modalities can improve the\nclinical diagnosis of brain diseases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hussein_R/0/1/0/all/0/1\">Ramy Hussein</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_M/0/1/0/all/0/1\">Moss Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shin_D/0/1/0/all/0/1\">David Shin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guo_J/0/1/0/all/0/1\">Jia Guo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_K/0/1/0/all/0/1\">Kevin T. Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Armindo_R/0/1/0/all/0/1\">Rui D. Armindo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Davidzon_G/0/1/0/all/0/1\">Guido Davidzon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Moseley_M/0/1/0/all/0/1\">Michael Moseley</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zaharchuk_G/0/1/0/all/0/1\">Greg Zaharchuk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"InfraredTags: Embedding Invisible AR Markers and Barcodes Using Low-Cost, Infrared-Based 3D Printing and Imaging Tools. (arXiv:2202.06165v1 [cs.HC])","link":"http://arxiv.org/abs/2202.06165","description":"<p>Existing approaches for embedding unobtrusive tags inside 3D objects require\neither complex fabrication or high-cost imaging equipment. We present\nInfraredTags, which are 2D markers and barcodes imperceptible to the naked eye\nthat can be 3D printed as part of objects, and detected rapidly by low-cost\nnear-infrared cameras. We achieve this by printing objects from an\ninfrared-transmitting filament, which infrared cameras can see through, and by\nhaving air gaps inside for the tag's bits, which appear at a different\nintensity in the infrared image.\n</p>\n<p>We built a user interface that facilitates the integration of common tags (QR\ncodes, ArUco markers) with the object geometry to make them 3D printable as\nInfraredTags. We also developed a low-cost infrared imaging module that\naugments existing mobile devices and decodes tags using our image processing\npipeline. Our evaluation shows that the tags can be detected with little\nnear-infrared illumination (0.2lux) and from distances as far as 250cm. We\ndemonstrate how our method enables various applications, such as object\ntracking and embedding metadata for augmented reality and tangible\ninteractions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dogan_M/0/1/0/all/0/1\">Mustafa Doga Dogan</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Taka_A/0/1/0/all/0/1\">Ahmad Taka</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Lu_M/0/1/0/all/0/1\">Michael Lu</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yunyi Zhu</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Akshat Kumar</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Aakar Gupta</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Mueller_S/0/1/0/all/0/1\">Stefanie Mueller</a> (1) ((1) MIT CSAIL, Cambridge, MA, USA, (2) Facebook Reality Labs, Redmond, WA, USA)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Source-Free Progressive Graph Learning for Open-Set Domain Adaptation. (arXiv:2202.06174v1 [cs.CV])","link":"http://arxiv.org/abs/2202.06174","description":"<p>Open-set domain adaptation (OSDA) has gained considerable attention in many\nvisual recognition tasks. However, most existing OSDA approaches are limited\ndue to three main reasons, including: (1) the lack of essential theoretical\nanalysis of generalization bound, (2) the reliance on the coexistence of source\nand target data during adaptation, and (3) failing to accurately estimate the\nuncertainty of model predictions. We propose a Progressive Graph Learning (PGL)\nframework that decomposes the target hypothesis space into the shared and\nunknown subspaces, and then progressively pseudo-labels the most confident\nknown samples from the target domain for hypothesis adaptation. Moreover, we\ntackle a more realistic source-free open-set domain adaptation (SF-OSDA)\nsetting that makes no assumption about the coexistence of source and target\ndomains, and introduce a balanced pseudo-labeling (BP-L) strategy in a\ntwo-stage framework, namely SF-PGL. Different from PGL that applies a\nclass-agnostic constant threshold for all target samples for pseudo-labeling,\nthe SF-PGL model uniformly selects the most confident target instances from\neach category at a fixed ratio. The confidence thresholds in each class are\nregarded as the 'uncertainty' of learning the semantic information, which are\nthen used to weigh the classification loss in the adaptation step. We conducted\nunsupervised and semi-supervised OSDA and SF-OSDA experiments on the benchmark\nimage classification and action recognition datasets. Additionally, we find\nthat balanced pseudo-labeling plays a significant role in improving\ncalibration, which makes the trained model less prone to over-confident or\nunder-confident predictions on the target data. Source code is available at\nhttps://github.com/Luoyadan/SF-PGL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yadan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zijian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhuoxiao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baktashmotlagh_M/0/1/0/all/0/1\">Mahsa Baktashmotlagh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lip movements information disentanglement for lip sync. (arXiv:2202.06198v1 [cs.CV])","link":"http://arxiv.org/abs/2202.06198","description":"<p>The lip movements information is critical for many audio-visual tasks.\nHowever, extracting lip movements information from videos is challenging, as it\ncan be easily perturbed by factors like personal identities and head poses.\nThis paper proposes utilizing the parametric 3D face model to disentangle lip\nmovements information explicitly. Building on top of the recent 3D face\nreconstruction advances, we firstly offer a method that can consistently\ndisentangle expression information, where the lip movements information lies.\nThen we demonstrate that once the influences of perturbing factors are\nalleviated by synthesizing faces with the disentangled lip movements\ninformation, the lip-sync task can be done better with much fewer data.\nFinally, we show its effectiveness in the wild by testing it on an unseen\ndataset for the active speaker detection task and achieving competitive\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chun Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Disentanglement with Tensor Product Representations on the Torus. (arXiv:2202.06201v1 [cs.LG])","link":"http://arxiv.org/abs/2202.06201","description":"<p>The current methods for learning representations with auto-encoders almost\nexclusively employ vectors as the latent representations. In this work, we\npropose to employ a tensor product structure for this purpose. This way, the\nobtained representations are naturally disentangled. In contrast to the\nconventional variations methods, which are targeted toward normally distributed\nfeatures, the latent space in our representation is distributed uniformly over\na set of unit circles. We argue that the torus structure of the latent space\ncaptures the generative factors effectively. We employ recent tools for\nmeasuring unsupervised disentanglement, and in an extensive set of experiments\ndemonstrate the advantage of our method in terms of disentanglement,\ncompleteness, and informativeness. The code for our proposed method is\navailable at https://github.com/rotmanmi/Unsupervised-Disentanglement-Torus.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rotman_M/0/1/0/all/0/1\">Michael Rotman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dekel_A/0/1/0/all/0/1\">Amit Dekel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gur_S/0/1/0/all/0/1\">Shir Gur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oz_Y/0/1/0/all/0/1\">Yaron Oz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_L/0/1/0/all/0/1\">Lior Wolf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Deepfake On Unrestricted Media: Generation And Detection. (arXiv:2202.06228v1 [cs.CV])","link":"http://arxiv.org/abs/2202.06228","description":"<p>Recent advances in deep learning have led to substantial improvements in\ndeepfake generation, resulting in fake media with a more realistic appearance.\nAlthough deepfake media have potential application in a wide range of areas and\nare drawing much attention from both the academic and industrial communities,\nit also leads to serious social and criminal concerns. This chapter explores\nthe evolution of and challenges in deepfake generation and detection. It also\ndiscusses possible ways to improve the robustness of deepfake detection for a\nwide variety of media (e.g., in-the-wild images and videos). Finally, it\nsuggests a focus for future fake media research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1\">Trung-Nghia Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Huy H Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamagishi_J/0/1/0/all/0/1\">Junichi Yamagishi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Echizen_I/0/1/0/all/0/1\">Isao Echizen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FairStyle: Debiasing StyleGAN2 with Style Channel Manipulations. (arXiv:2202.06240v1 [cs.CV])","link":"http://arxiv.org/abs/2202.06240","description":"<p>Recent advances in generative adversarial networks have shown that it is\npossible to generate high-resolution and hyperrealistic images. However, the\nimages produced by GANs are only as fair and representative as the datasets on\nwhich they are trained. In this paper, we propose a method for directly\nmodifying a pre-trained StyleGAN2 model that can be used to generate a balanced\nset of images with respect to one (e.g., eyeglasses) or more attributes (e.g.,\ngender and eyeglasses). Our method takes advantage of the style space of the\nStyleGAN2 model to perform disentangled control of the target attributes to be\ndebiased. Our method does not require training additional models and directly\ndebiases the GAN model, paving the way for its use in various downstream\napplications. Our experiments show that our method successfully debiases the\nGAN model within a few minutes without compromising the quality of the\ngenerated images. To promote fair generative models, we share the code and\ndebiased models at <a href=\"http://catlab-team.github.io/fairstyle.\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karakas_C/0/1/0/all/0/1\">Cemre Karakas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dirik_A/0/1/0/all/0/1\">Alara Dirik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yalcinkaya_E/0/1/0/all/0/1\">Eylul Yalcinkaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yanardag_P/0/1/0/all/0/1\">Pinar Yanardag</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Privacy protection based on mask template. (arXiv:2202.06250v1 [cs.CR])","link":"http://arxiv.org/abs/2202.06250","description":"<p>Powerful recognition algorithms are widely used in the Internet or important\nmedical systems, which poses a serious threat to personal privacy. Although the\nlaw provides for diversity protection, e.g. The General Data Protection\nRegulation (GDPR) in Europe and Articles 1032 to 1039 of the civil code in\nChina. However, as an important privacy disclosure event, biometric data is\noften hidden, which is difficult for the owner to detect and trace to the\nsource. Human biometrics generally exist in images. In order to avoid the\ndisclosure of personal privacy, we should prevent unauthorized recognition\nalgorithms from acquiring the real features of the original image.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yu Bai</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1\">Guangmin Sun</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jie Liu</a> (1) ((1) Beijing University of Technology,(2) Beijing Friendship Hospital)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Autonomous Drone Swarm Navigation and Multi-target Tracking in 3D Environments with Dynamic Obstacles. (arXiv:2202.06253v1 [cs.RO])","link":"http://arxiv.org/abs/2202.06253","description":"<p>Autonomous modeling of artificial swarms is necessary because manual creation\nis a time intensive and complicated procedure which makes it impractical. An\nautonomous approach employing deep reinforcement learning is presented in this\nstudy for swarm navigation. In this approach, complex 3D environments with\nstatic and dynamic obstacles and resistive forces (like linear drag, angular\ndrag, and gravity) are modeled to track multiple dynamic targets. Moreover,\nreward functions for robust swarm formation and target tracking are devised for\nlearning complex swarm behaviors. Since the number of agents is not fixed and\nhas only the partial observance of the environment, swarm formation and\nnavigation become challenging. In this regard, the proposed strategy consists\nof three main phases to tackle the aforementioned challenges: 1) A methodology\nfor dynamic swarm management, 2) Avoiding obstacles, Finding the shortest path\ntowards the targets, 3) Tracking the targets and Island modeling. The dynamic\nswarm management phase translates basic sensory input to high level commands to\nenhance swarm navigation and decentralized setup while maintaining the swarms\nsize fluctuations. While, in the island modeling, the swarm can split into\nindividual subswarms according to the number of targets, conversely, these\nsubswarms may join to form a single huge swarm, giving the swarm ability to\ntrack multiple targets. Customized state of the art policy based deep\nreinforcement learning algorithms are employed to achieve significant results.\nThe promising results show that our proposed strategy enhances swarm navigation\nand can track multiple static and dynamic targets in complex dynamic\nenvironments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qamar_S/0/1/0/all/0/1\">Suleman Qamar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1\">Saddam Hussain Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arshad_M/0/1/0/all/0/1\">Muhammad Arif Arshad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qamar_M/0/1/0/all/0/1\">Maryam Qamar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1\">Asifullah Khan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RandomSEMO: Normality Learning Of Moving Objects For Video Anomaly Detection. (arXiv:2202.06256v1 [cs.CV])","link":"http://arxiv.org/abs/2202.06256","description":"<p>Recent anomaly detection algorithms have shown powerful performance by\nadopting frame predicting autoencoders. However, these methods face two\nchallenging circumstances. First, they are likely to be trained to be\nexcessively powerful, generating even abnormal frames well, which leads to\nfailure in detecting anomalies. Second, they are distracted by the large number\nof objects captured in both foreground and background. To solve these problems,\nwe propose a novel superpixel-based video data transformation technique named\nRandom Superpixel Erasing on Moving Objects (RandomSEMO) and Moving Object Loss\n(MOLoss), built on top of a simple lightweight autoencoder. RandomSEMO is\napplied to the moving object regions by randomly erasing their superpixels. It\nenforces the network to pay attention to the foreground objects and learn the\nnormal features more effectively, rather than simply predicting the future\nframe. Moreover, MOLoss urges the model to focus on learning normal objects\ncaptured within RandomSEMO by amplifying the loss on the pixels near the moving\nobjects. The experimental results show that our model outperforms\nstate-of-the-arts on three benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1\">Chaewon Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Minhyeok Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_M/0/1/0/all/0/1\">MyeongAh Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sangyoun Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LTSP: Long-Term Slice Propagation for Accurate Airway Segmentation. (arXiv:2202.06260v1 [eess.IV])","link":"http://arxiv.org/abs/2202.06260","description":"<p>Purpose: Bronchoscopic intervention is a widely-used clinical technique for\npulmonary diseases, which requires an accurate and topological complete airway\nmap for its localization and guidance. The airway map could be extracted from\nchest computed tomography (CT) scans automatically by airway segmentation\nmethods. Due to the complex tree-like structure of the airway, preserving its\ntopology completeness while maintaining the segmentation accuracy is a\nchallenging task.\n</p>\n<p>Methods: In this paper, a long-term slice propagation (LTSP) method is\nproposed for accurate airway segmentation from pathological CT scans. We also\ndesign a two-stage end-to-end segmentation framework utilizing the LTSP method\nin the decoding process. Stage 1 is used to generate a coarse feature map by an\nencoder-decoder architecture. Stage 2 is to adopt the proposed LTSP method for\nexploiting the continuity information and enhancing the weak airway features in\nthe coarse feature map. The final segmentation result is predicted from the\nrefined feature map.\n</p>\n<p>Results: Extensive experiments were conducted to evaluate the performance of\nthe proposed method on 70 clinical CT scans. The results demonstrate the\nconsiderable improvements of the proposed method compared to some\nstate-of-the-art methods as most breakages are eliminated and more tiny bronchi\nare detected. The ablation studies further confirm the effectiveness of the\nconstituents of the proposed method.\n</p>\n<p>Conclusion: Slice continuity information is beneficial to accurate airway\nsegmentation. Furthermore, by propagating the long-term slice feature, the\nairway topology connectivity is preserved with overall segmentation accuracy\nmaintained.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wu_Y/0/1/0/all/0/1\">Yangqian Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_M/0/1/0/all/0/1\">Minghui Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_W/0/1/0/all/0/1\">Weihao Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_H/0/1/0/all/0/1\">Hao Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_J/0/1/0/all/0/1\">Jiasheng Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gu_Y/0/1/0/all/0/1\">Yun Gu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LighTN: Light-weight Transformer Network for Performance-overhead Tradeoff in Point Cloud Downsampling. (arXiv:2202.06263v1 [cs.CV])","link":"http://arxiv.org/abs/2202.06263","description":"<p>Compared with traditional task-irrelevant downsampling methods, task-oriented\nneural networks have shown improved performance in point cloud downsampling\nrange. Recently, Transformer family of networks has shown a more powerful\nlearning capacity in visual tasks. However, Transformer-based architectures\npotentially consume too many resources which are usually worthless for low\noverhead task networks in downsampling range. This paper proposes a novel\nlight-weight Transformer network (LighTN) for task-oriented point cloud\ndownsampling, as an end-to-end and plug-and-play solution. In LighTN, a\nsingle-head self-correlation module is presented to extract refined global\ncontextual features, where three projection matrices are simultaneously\neliminated to save resource overhead, and the output of symmetric matrix\nsatisfies the permutation invariant. Then, we design a novel downsampling loss\nfunction to guide LighTN focuses on critical point cloud regions with more\nuniform distribution and prominent points coverage. Furthermore, We introduce a\nfeed-forward network scaling mechanism to enhance the learnable capacity of\nLighTN according to the expand-reduce strategy. The result of extensive\nexperiments on classification and registration tasks demonstrates LighTN can\nachieve state-of-the-art performance with limited resource overhead.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yi Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cen_Y/0/1/0/all/0/1\">Yigang Cen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_B/0/1/0/all/0/1\">Bowen Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yidong Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improve Deep Image Inpainting by Emphasizing the Complexity of Missing Regions. (arXiv:2202.06266v1 [cs.CV])","link":"http://arxiv.org/abs/2202.06266","description":"<p>Deep image inpainting research mainly focuses on constructing various neural\nnetwork architectures or imposing novel optimization objectives. However, on\nthe one hand, building a state-of-the-art deep inpainting model is an extremely\ncomplex task, and on the other hand, the resulting performance gains are\nsometimes very limited. We believe that besides the frameworks of inpainting\nmodels, lightweight traditional image processing techniques, which are often\noverlooked, can actually be helpful to these deep models. In this paper, we\nenhance the deep image inpainting models with the help of classical image\ncomplexity metrics. A knowledge-assisted index composed of missingness\ncomplexity and forward loss is presented to guide the batch selection in the\ntraining procedure. This index helps find samples that are more conducive to\noptimization in each iteration and ultimately boost the overall inpainting\nperformance. The proposed approach is simple and can be plugged into many deep\ninpainting models by changing only a few lines of code. We experimentally\ndemonstrate the improvements for several recently developed image inpainting\nmodels on various datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yufeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Cong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Min Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BViT: Broad Attention based Vision Transformer. (arXiv:2202.06268v1 [cs.CV])","link":"http://arxiv.org/abs/2202.06268","description":"<p>Recent works have demonstrated that transformer can achieve promising\nperformance in computer vision, by exploiting the relationship among image\npatches with self-attention. While they only consider the attention in a single\nfeature layer, but ignore the complementarity of attention in different levels.\nIn this paper, we propose the broad attention to improve the performance by\nincorporating the attention relationship of different layers for vision\ntransformer, which is called BViT. The broad attention is implemented by broad\nconnection and parameter-free attention. Broad connection of each transformer\nlayer promotes the transmission and integration of information for BViT.\nWithout introducing additional trainable parameters, parameter-free attention\njointly focuses on the already available attention information in different\nlayers for extracting useful information and building their relationship.\nExperiments on image classification tasks demonstrate that BViT delivers\nstate-of-the-art accuracy of 74.8\\%/81.6\\% top-1 accuracy on ImageNet with\n5M/22M parameters. Moreover, we transfer BViT to downstream object recognition\nbenchmarks to achieve 98.9\\% and 89.9\\% on CIFAR10 and CIFAR100 respectively\nthat exceed ViT with fewer parameters. For the generalization test, the broad\nattention in Swin Transformer and T2T-ViT also bring an improvement of more\nthan 1\\%. To sum up, broad attention is promising to promote the performance of\nattention based models. Code and pre-trained models are available at\nhttps://github.com/DRL-CASIA/Broad_ViT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_N/0/1/0/all/0/1\">Nannan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yaran Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Weifan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1\">Zixiang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Dongbin Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Natural Image Stitching Using Depth Maps. (arXiv:2202.06276v1 [cs.CV])","link":"http://arxiv.org/abs/2202.06276","description":"<p>Natural image stitching (NIS) aims to create one natural-looking mosaic from\ntwo overlapping images that capture a same 3D scene from different viewing\npositions. Challenges inevitably arise when the scene is non-planar and the\ncamera baseline is wide, since parallax becomes not negligible in such cases.\nIn this paper, we propose a novel NIS method using depth maps, which generates\nnatural-looking mosaics against parallax in both overlapping and\nnon-overlapping regions. Firstly, we estimate a pixel-to-pixel transformation\nbased on feature matches and their depth values. Then, we draw a triangulation\nof the target image and estimate multiple local homographies, one per triangle,\nbased on the locations of their vertices and the rectified depth values.\nFinally, the warping image is composited by the backward mapping of piece-wise\nhomographies. Experimental results demonstrate that the proposed method not\nonly provides accurate alignment in the overlapping regions, but also virtual\nnaturalness in the non-overlapping region.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liao_T/0/1/0/all/0/1\">Tianli Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_N/0/1/0/all/0/1\">Nan Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-Reference Image Restoration for Under-Display Camera of UAV. (arXiv:2202.06283v1 [cs.CV])","link":"http://arxiv.org/abs/2202.06283","description":"<p>The exposed cameras of UAV can shake, shift, or even malfunction under the\ninfluence of harsh weather, while the add-on devices (Dupont lines) are very\nvulnerable to damage.\n</p>\n<p>We can place a low-cost T-OLED overlay around the camera to protect it, but\nthis would also introduce image degradation issues.\n</p>\n<p>In particular, the temperature variations in the atmosphere can create mist\nthat adsorbs to the T-OLED, which can cause secondary disasters (i.e., more\nsevere image degradation) during the UAV's filming process.\n</p>\n<p>To solve the image degradation problem caused by overlaying T-OLEDs, in this\npaper we propose a new method to enhance the visual experience by enhancing the\ntexture and color of images.\n</p>\n<p>Specifically, our method trains a lightweight network to estimate a low-rank\naffine grid on the input image, and then utilizes the grid to enhance the input\nimage at block granularity.\n</p>\n<p>The advantages of our method are that no reference image is required and the\nloss function is developed from visual experience.\n</p>\n<p>In addition, our model can perform high-quality recovery of images of\narbitrary resolution in real time.\n</p>\n<p>In the end, the limitations of our model and the collected datasets\n(including the daytime and nighttime scenes) are discussed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zhuoran Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1\">Xiuyi Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1\">Yunliang Zhuang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Motion Sickness Modeling with Visual Vertical Estimation and Its Application to Autonomous Personal Mobility Vehicles. (arXiv:2202.06299v1 [cs.HC])","link":"http://arxiv.org/abs/2202.06299","description":"<p>Passengers (drivers) of level 3-5 autonomous personal mobility vehicles\n(APMV) and cars can perform non-driving tasks, such as reading books and\nsmartphones, while driving. It has been pointed out that such activities may\nincrease motion sickness. Many studies have been conducted to build\ncountermeasures, of which various computational motion sickness models have\nbeen developed. Many of these are based on subjective vertical conflict (SVC)\ntheory, which describes vertical changes in direction sensed by human sensory\norgans vs. those expected by the central nervous system. Such models are\nexpected to be applied to autonomous driving scenarios. However, no current\ncomputational model can integrate visual vertical information with vestibular\nsensations.\n</p>\n<p>We proposed a 6 DoF SVC-VV model which add a visually perceived vertical\nblock into a conventional six-degrees-of-freedom SVC model to predict VV\ndirections from image data simulating the visual input of a human. Hence, a\nsimple image-based VV estimation method is proposed.\n</p>\n<p>As the validation of the proposed model, this paper focuses on describing the\nfact that the motion sickness increases as a passenger reads a book while using\nan AMPV, assuming that visual vertical (VV) plays an important role. In the\nstatic experiment, it is demonstrated that the estimated VV by the proposed\nmethod accurately described the gravitational acceleration direction with a low\nmean absolute deviation. In addition, the results of the driving experiment\nusing an APMV demonstrated that the proposed 6 DoF SVC-VV model could describe\nthat the increased motion sickness experienced when the VV and gravitational\nacceleration directions were different.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hailong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inoue_S/0/1/0/all/0/1\">Shota Inoue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wada_T/0/1/0/all/0/1\">Takahiro Wada</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Graph Learning for Spatially-Varying Indoor Lighting Prediction. (arXiv:2202.06300v1 [cs.CV])","link":"http://arxiv.org/abs/2202.06300","description":"<p>Lighting prediction from a single image is becoming increasingly important in\nmany vision and augmented reality (AR) applications in which shading and shadow\nconsistency between virtual and real objects should be guaranteed. However,\nthis is a notoriously ill-posed problem, especially for indoor scenarios,\nbecause of the complexity of indoor luminaires and the limited information\ninvolved in 2D images. In this paper, we propose a graph learning-based\nframework for indoor lighting estimation. At its core is a new lighting model\n(dubbed DSGLight) based on depth-augmented Spherical Gaussians (SG) and a Graph\nConvolutional Network (GCN) that infers the new lighting representation from a\nsingle LDR image of limited field-of-view. Our lighting model builds 128 evenly\ndistributed SGs over the indoor panorama, where each SG encoding the lighting\nand the depth around that node. The proposed GCN then learns the mapping from\nthe input image to DSGLight. Compared with existing lighting models, our\nDSGLight encodes both direct lighting and indirect environmental lighting more\nfaithfully and compactly. It also makes network training and inference more\nstable. The estimated depth distribution enables temporally stable shading and\nshadows under spatially-varying lighting. Through thorough experiments, we show\nthat our method obviously outperforms existing methods both qualitatively and\nquantitatively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1\">Jiayang Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jie Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_C/0/1/0/all/0/1\">Chenchen Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhenyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhen He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Piaopiao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yanwen Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Fine-tuning for Backdoor Defense: Connect Adversarial Examples to Triggered Samples. (arXiv:2202.06312v1 [cs.CV])","link":"http://arxiv.org/abs/2202.06312","description":"<p>Deep neural networks (DNNs) are known to be vulnerable to backdoor attacks,\ni.e., a backdoor trigger planted at training time, the infected DNN model would\nmisclassify any testing sample embedded with the trigger as target label. Due\nto the stealthiness of backdoor attacks, it is hard either to detect or erase\nthe backdoor from infected models. In this paper, we propose a new Adversarial\nFine-Tuning (AFT) approach to erase backdoor triggers by leveraging adversarial\nexamples of the infected model. For an infected model, we observe that its\nadversarial examples have similar behaviors as its triggered samples. Based on\nsuch observation, we design the AFT to break the foundation of the backdoor\nattack (i.e., the strong correlation between a trigger and a target label). We\nempirically show that, against 5 state-of-the-art backdoor attacks, AFT can\neffectively erase the backdoor triggers without obvious performance degradation\non clean samples, which significantly outperforms existing defense methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mu_B/0/1/0/all/0/1\">Bingxu Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Le Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_Z/0/1/0/all/0/1\">Zhenxing Niu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Data Augmentation Method for Fully Automatic Brain Tumor Segmentation. (arXiv:2202.06344v1 [eess.IV])","link":"http://arxiv.org/abs/2202.06344","description":"<p>Automatic segmentation of glioma and its subregions is of great significance\nfor diagnosis, treatment and monitoring of disease. In this paper, an\naugmentation method, called TensorMixup, was proposed and applied to the three\ndimensional U-Net architecture for brain tumor segmentation. The main ideas\nincluded that first, two image patches with size of 128 in three dimensions\nwere selected according to glioma information of ground truth labels from the\nmagnetic resonance imaging data of any two patients with the same modality.\nNext, a tensor in which all elements were independently sampled from Beta\ndistribution was used to mix the image patches. Then the tensor was mapped to a\nmatrix which was used to mix the one-hot encoded labels of the above image\npatches. Therefore, a new image and its one-hot encoded label were synthesized.\nFinally, the new data was used to train the model which could be used to\nsegment glioma. The experimental results show that the mean accuracy of Dice\nscores are 91.32%, 85.67%, and 82.20% respectively on the whole tumor, tumor\ncore, and enhancing tumor segmentation, which proves that the proposed\nTensorMixup is feasible and effective for brain tumor segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ji_Y/0/1/0/all/0/1\">Yarong Ji</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiao_H/0/1/0/all/0/1\">Hongbing Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diverse facial inpainting guided by exemplars. (arXiv:2202.06358v1 [cs.CV])","link":"http://arxiv.org/abs/2202.06358","description":"<p>Facial image inpainting is a task of filling visually realistic and\nsemantically meaningful contents for missing or masked pixels in a face image.\nAlthough existing methods have made significant progress in achieving high\nvisual quality, the controllable diversity of facial image inpainting remains\nan open problem in this field. This paper introduces EXE-GAN, a novel diverse\nand interactive facial inpainting framework, which can not only preserve the\nhigh-quality visual effect of the whole image but also complete the face image\nwith exemplar-like facial attributes. The proposed facial inpainting is\nachieved based on generative adversarial networks by leveraging the global\nstyle of input image, the stochastic style, and the exemplar style of example\nimage. A novel attribute similarity metric is introduced to encourage networks\nto learn the style of facial attributes from the exemplar in a self-supervised\nway. To guarantee the natural transition across the boundary of inpainted\nregions, a novel spatial variant gradient backpropagation technique is designed\nto adjust the loss gradients based on the spatial location. A variety of\nexperimental results and comparisons on public CelebA-HQ and FFHQ datasets are\npresented to demonstrate the superiority of the proposed method in terms of\nboth the quality and diversity in facial inpainting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Wanglong Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hanli Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xianta Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1\">Xiaogang Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Min Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_J/0/1/0/all/0/1\">Jiankai Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_K/0/1/0/all/0/1\">Kaijie Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Perspective Deformation in X-Ray Transmission Imaging. (arXiv:2202.06366v1 [eess.IV])","link":"http://arxiv.org/abs/2202.06366","description":"<p>In cone-beam X-ray transmission imaging, due to the divergence of X-rays,\nimaged structures with different depths have different magnification factors on\nan X-ray detector, which results in perspective deformation. Perspective\ndeformation causes difficulty in direct, accurate geometric assessments of\nanatomical structures. In this work, to reduce perspective deformation in X-ray\nimages acquired from regular cone-beam computed tomography (CBCT) systems, we\ninvestigate on learning perspective deformation, i.e., converting perspective\nprojections into orthogonal projections. Directly converting a single\nperspective projection image into an orthogonal projection image is extremely\nchallenging due to the lack of depth information. Therefore, we propose to\nutilize one additional perspective projection, a complementary (180-degree) or\northogonal (90-degree) view, to provide a certain degree of depth information.\nFurthermore, learning perspective deformation in different spatial domains is\ninvestigated. Our proposed method is evaluated on numerical spherical bead\nphantoms as well as patients' chest and head X-ray data. The experiments on\nnumerical bead phantom data demonstrate that learning perspective deformation\nin polar coordinates has significant advantages over learning in Cartesian\ncoordinates, as root-mean-square error (RMSE) decreases from 5.31 to 1.40,\nwhile learning in log-polar coordinates has no further considerable improvement\n(RMSE = 1.85). In addition, using a complementary view (RMSE = 1.40) is better\nthan an orthogonal view (RMSE = 3.87). The experiments on patients' chest and\nhead data demonstrate that learning perspective deformation using dual\ncomplementary views is also applicable in anatomical X-ray data, allowing\naccurate cardiothoracic ratio measurements in chest X-ray images and\ncephalometric analysis in synthetic cephalograms from cone-beam X-ray\nprojections.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1\">Yixing Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maier_A/0/1/0/all/0/1\">Andreas Maier</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fietkau_R/0/1/0/all/0/1\">Rainer Fietkau</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bert_C/0/1/0/all/0/1\">Christoph Bert</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Putz_F/0/1/0/all/0/1\">Florian Putz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Omnifont Persian OCR System Using Primitives. (arXiv:2202.06371v1 [cs.CV])","link":"http://arxiv.org/abs/2202.06371","description":"<p>In this paper, we introduce a model-based omnifont Persian OCR system. The\nsystem uses a set of 8 primitive elements as structural features for\nrecognition. First, the scanned document is preprocessed. After normalizing the\npreprocessed image, text rows and sub-words are separated and then thinned.\nAfter recognition of dots in sub-words, strokes are extracted and primitive\nelements of each sub-word are recognized using the strokes. Finally, the\nprimitives are compared with a predefined set of character identification\nvectors in order to identify sub-word characters. The separation and\nrecognition steps of the system are concurrent, eliminating unavoidable errors\nof independent separation of letters. The system has been tested on documents\nwith 14 standard Persian fonts in 6 sizes. The achieved precision is 97.06%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Keipour_A/0/1/0/all/0/1\">Azarakhsh Keipour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eshghi_M/0/1/0/all/0/1\">Mohammad Eshghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghadikolaei_S/0/1/0/all/0/1\">Sina Mohammadzadeh Ghadikolaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammadi_N/0/1/0/all/0/1\">Negin Mohammadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ensafi_S/0/1/0/all/0/1\">Shahab Ensafi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Deep Learning Techniques for the Analysis of COVID-19 and their usability for Detecting Omicron. (arXiv:2202.06372v1 [eess.IV])","link":"http://arxiv.org/abs/2202.06372","description":"<p>The Coronavirus (COVID-19) outbreak in December 2019 has become an ongoing\nthreat to humans worldwide, creating a health crisis that infected millions of\nlives, as well as devastating the global economy. Deep learning (DL) techniques\nhave proved helpful in analysis and delineation of infectious regions in\nradiological images in a timely manner. This paper makes an in-depth survey of\nDL techniques and draws a taxonomy based on diagnostic strategies and learning\napproaches. DL techniques are systematically categorized into classification,\nsegmentation, and multi-stage approaches for COVID-19 diagnosis at image and\nregion level analysis. Each category includes pre-trained and custom-made\nConvolutional Neural Network architectures for detecting COVID-19 infection in\nradiographic imaging modalities; X-Ray, and Computer Tomography (CT).\nFurthermore, a discussion is made on challenges in developing diagnostic\ntechniques in pandemic, cross-platform interoperability, and examining imaging\nmodality, in addition to reviewing methodologies and performance measures used\nin these techniques. This survey provides an insight into promising areas of\nresearch in DL for analyzing radiographic images and thus, may further\naccelerate the research in designing of customized DL based diagnostic tools\nfor effectively dealing with new variants of COVID-19 and emerging challenges.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Khan_A/0/1/0/all/0/1\">Asifullah Khan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khan_S/0/1/0/all/0/1\">Saddam Hussain Khan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Saif_M/0/1/0/all/0/1\">Mahrukh Saif</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Batool_A/0/1/0/all/0/1\">Asiya Batool</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sohail_A/0/1/0/all/0/1\">Anabia Sohail</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khan_M/0/1/0/all/0/1\">Muhammad Waleed Khan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scheduling Techniques for Liver Segmentation: ReduceLRonPlateau Vs OneCycleLR. (arXiv:2202.06373v1 [cs.CV])","link":"http://arxiv.org/abs/2202.06373","description":"<p>Machine learning and computer vision techniques have influenced many fields\nincluding the biomedical one. The aim of this paper is to investigate the\nimportant concept of schedulers in manipulating the learning rate (LR), for the\nliver segmentation task, throughout the training process, focusing on the newly\ndevised OneCycleLR against the ReduceLRonPlateau. A dataset, published in 2018\nand produced by the Medical Segmentation Decathlon Challenge organizers, called\nTask 8 Hepatic Vessel (MSDC-T8) has been used for testing and validation. The\nreported results that have the same number of maximum epochs (75), and are the\naverage of 5-fold cross-validation, indicate that ReduceLRonPlateau converges\nfaster while maintaining a similar or even better loss score on the validation\nset when compared to OneCycleLR. The epoch at which the peak LR occurs perhaps\nshould be made early for the OneCycleLR such that the super-convergence feature\ncan be observed. Moreover, the overall results outperform the state-of-the-art\nresults from the researchers who published the liver masks for this dataset. To\nconclude, both schedulers are suitable for medical segmentation challenges,\nespecially the MSDC-T8 dataset, and can be used confidently in rapidly\nconverging the validation loss with a minimal number of epochs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Al_Kababji_A/0/1/0/all/0/1\">Ayman Al-Kababji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bensaali_F/0/1/0/all/0/1\">Faycal Bensaali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dakua_S/0/1/0/all/0/1\">Sarada Prasad Dakua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Sound Localization in the Wild by Cross-Modal Interference Erasing. (arXiv:2202.06406v1 [cs.CV])","link":"http://arxiv.org/abs/2202.06406","description":"<p>The task of audio-visual sound source localization has been well studied\nunder constrained scenes, where the audio recordings are clean. However, in\nreal-world scenarios, audios are usually contaminated by off-screen sound and\nbackground noise. They will interfere with the procedure of identifying desired\nsources and building visual-sound connections, making previous studies\nnon-applicable. In this work, we propose the Interference Eraser (IEr)\nframework, which tackles the problem of audio-visual sound source localization\nin the wild. The key idea is to eliminate the interference by redefining and\ncarving discriminative audio representations. Specifically, we observe that the\nprevious practice of learning only a single audio representation is\ninsufficient due to the additive nature of audio signals. We thus extend the\naudio representation with our Audio-Instance-Identifier module, which clearly\ndistinguishes sounding instances when audio signals of different volumes are\nunevenly mixed. Then we erase the influence of the audible but off-screen\nsounds and the silent but visible objects by a Cross-modal Referrer module with\ncross-modality distillation. Quantitative and qualitative evaluations\ndemonstrate that our proposed framework achieves superior results on sound\nlocalization tasks, especially under real-world scenarios. Code is available at\nhttps://github.com/alvinliu0/Visual-Sound-Localization-in-the-Wild.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_R/0/1/0/all/0/1\">Rui Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1\">Di Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Weiyao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Bolei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xiaowei Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical Point Cloud Encoding and Decoding with Lightweight Self-Attention based Model. (arXiv:2202.06407v1 [cs.CV])","link":"http://arxiv.org/abs/2202.06407","description":"<p>In this paper we present SA-CNN, a hierarchical and lightweight\nself-attention based encoding and decoding architecture for representation\nlearning of point cloud data. The proposed SA-CNN introduces convolution and\ntransposed convolution stacks to capture and generate contextual information\namong unordered 3D points. Following conventional hierarchical pipeline, the\nencoding process extracts feature in local-to-global manner, while the decoding\nprocess generates feature and point cloud in coarse-to-fine, multi-resolution\nstages. We demonstrate that SA-CNN is capable of a wide range of applications,\nnamely classification, part segmentation, reconstruction, shape retrieval, and\nunsupervised classification. While achieving the state-of-the-art or comparable\nperformance in the benchmarks, SA-CNN maintains its model complexity several\norder of magnitude lower than the others. In term of qualitative results, we\nvisualize the multi-stage point cloud reconstructions and latent walks on rigid\nobjects as well as deformable non-rigid human and robot models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Puang_E/0/1/0/all/0/1\">En Yen Puang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hongyuan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jing_W/0/1/0/all/0/1\">Wei Jing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Human Action Recognition and Prediction: A Survey. (arXiv:1806.11230v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1806.11230","description":"<p>Derived from rapid advances in computer vision and machine learning, video\nanalysis tasks have been moving from inferring the present state to predicting\nthe future state. Vision-based action recognition and prediction from videos\nare such tasks, where action recognition is to infer human actions (present\nstate) based upon complete action executions, and action prediction to predict\nhuman actions (future state) based upon incomplete action executions. These two\ntasks have become particularly prevalent topics recently because of their\nexplosively emerging real-world applications, such as visual surveillance,\nautonomous driving vehicle, entertainment, and video retrieval, etc. Many\nattempts have been devoted in the last a few decades in order to build a robust\nand effective framework for action recognition and prediction. In this paper,\nwe survey the complete state-of-the-art techniques in action recognition and\nprediction. Existing models, popular algorithms, technical difficulties,\npopular action databases, evaluation protocols, and promising future directions\nare also provided with systematic discussions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kong_Y/0/1/0/all/0/1\">Yu Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yun Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Modal Distillation for RGB-Depth Person Re-Identification. (arXiv:1810.11641v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1810.11641","description":"<p>Person re-identification is a key challenge for surveillance across multiple\nsensors. Prompted by the advent of powerful deep learning models for visual\nrecognition, and inexpensive RGB-D cameras and sensor-rich mobile robotic\nplatforms, e.g. self-driving vehicles, we investigate the relatively unexplored\nproblem of cross-modal re-identification of persons between RGB (color) and\ndepth images. The considerable divergence in data distributions across\ndifferent sensor modalities introduces additional challenges to the typical\ndifficulties like distinct viewpoints, occlusions, and pose and illumination\nvariation. While some work has investigated re-identification across RGB and\ninfrared, we take inspiration from successes in transfer learning from RGB to\ndepth in object detection tasks. Our main contribution is a novel method for\ncross-modal distillation for robust person re-identification, which learns a\nshared feature representation space of person's appearance in both RGB and\ndepth images. In addition, we propose a cross-modal attention mechanism where\nthe gating signal from one modality can dynamically activate the most\ndiscriminant CNN filters of the other modality. The proposed distillation\nmethod is compared to conventional and deep learning approaches proposed for\nother cross-domain re-identification tasks. Results obtained on the public BIWI\nand RobotPKU datasets indicate that the proposed method can significantly\noutperform the state-of-the-art approaches by up to 16.1% in mean Average\nPrecision (mAP), demonstrating the benefit of the distillation paradigm. The\nexperimental results also indicate that using cross-modal attention allows to\nimprove recognition accuracy considerably with respect to the proposed\ndistillation method and relevant state-of-the-art approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hafner_F/0/1/0/all/0/1\">Frank Hafner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhuiyan_A/0/1/0/all/0/1\">Amran Bhuiyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kooij_J/0/1/0/all/0/1\">Julian F. P. Kooij</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Granger_E/0/1/0/all/0/1\">Eric Granger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Concise and Effective Network for 3D Human Modeling from Orthogonal Silhouettes. (arXiv:1912.11616v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1912.11616","description":"<p>In this paper, we revisit the problem of 3D human modeling from two\northogonal silhouettes of individuals (i.e., front and side views). Different\nfrom our prior work, a supervised learning approach based on convolutional\nneural network (CNN) is investigated to solve the problem by establishing a\nmapping function that can effectively extract features from two silhouettes and\nfuse them into coefficients in the shape space of human bodies. A new CNN\nstructure is proposed in our work to exact not only the discriminative features\nof front and side views and also their mixed features for the mapping function.\n3D human models with high accuracy are synthesized from coefficients generated\nby the mapping function. Existing CNN approaches for 3D human modeling usually\nlearn a large number of parameters (from 8.5M to 355.4M) from two binary\nimages. Differently, we investigate a new network architecture and conduct the\nsamples on silhouettes as input. As a consequence, more accurate models can be\ngenerated by our network with only 2.4M coefficients. The training of our\nnetwork is conducted on samples obtained by augmenting a publicly accessible\ndataset. Learning transfer by using datasets with a smaller number of scanned\nmodels is applied to our network to enable the function of generating results\nwith gender-oriented (or geographical) patterns.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiuping Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhixin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Charlie C.L. Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards High Performance Low Complexity Calibration in Appearance Based Gaze Estimation. (arXiv:2001.09284v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2001.09284","description":"<p>Appearance-based gaze estimation from RGB images provides relatively\nunconstrained gaze tracking. We have previously proposed a gaze decomposition\nmethod that decomposes the gaze angle into the sum of a subject-independent\ngaze estimate from the image and a subject-dependent bias. This paper extends\nthat work with a more complete characterization of the interplay between the\ncomplexity of the calibration dataset and estimation accuracy. We analyze the\neffect of the number of gaze targets, the number of images used per gaze target\nand the number of head positions in calibration data using a new NISLGaze\ndataset, which is well suited for analyzing these effects as it includes more\ndiversity in head positions and orientations for each subject than other\ndatasets. A better understanding of these factors enables low complexity high\nperformance calibration. Our results indicate that using only a single gaze\ntarget and single head position is sufficient to achieve high quality\ncalibration, outperforming state-of-the-art methods by more than 6.3%. One of\nthe surprising findings is that the same estimator yields the best performance\nboth with and without calibration. To better understand the reasons, we provide\na new theoretical analysis that specifies the conditions under which this can\nbe expected.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhaokang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1\">Bertram E. Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cyclic Differentiable Architecture Search. (arXiv:2006.10724v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2006.10724","description":"<p>Differentiable ARchiTecture Search, i.e., DARTS, has drawn great attention in\nneural architecture search. It tries to find the optimal architecture in a\nshallow search network and then measures its performance in a deep evaluation\nnetwork. The independent optimization of the search and evaluation networks,\nhowever, leaves room for potential improvement by allowing interaction between\nthe two networks. To address the problematic optimization issue, we propose new\njoint optimization objectives and a novel Cyclic Differentiable ARchiTecture\nSearch framework, dubbed CDARTS. Considering the structure difference, CDARTS\nbuilds a cyclic feedback mechanism between the search and evaluation networks\nwith introspective distillation. First, the search network generates an initial\narchitecture for evaluation, and the weights of the evaluation network are\noptimized. Second, the architecture weights in the search network are further\noptimized by the label supervision in classification, as well as the\nregularization from the evaluation network through feature distillation.\nRepeating the above cycle results in joint optimization of the search and\nevaluation networks and thus enables the evolution of the architecture to fit\nthe final evaluation network. The experiments and analysis on CIFAR, ImageNet\nand NAS-Bench-201 demonstrate the effectiveness of the proposed approach over\nthe state-of-the-art ones. Specifically, in the DARTS search space, we achieve\n97.52% top-1 accuracy on CIFAR10 and 76.3% top-1 accuracy on ImageNet. In the\nchain-structured search space, we achieve 78.2% top-1 accuracy on ImageNet,\nwhich is 1.1% higher than EfficientNet-B0. Our code and models are publicly\navailable at https://github.com/researchmm/CDARTS.git.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hongyuan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Houwen Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jianlong Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_H/0/1/0/all/0/1\">Hao Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_H/0/1/0/all/0/1\">Haibin Ling</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Computer Vision and Normalizing Flow-Based Defect Detection. (arXiv:2012.06737v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.06737","description":"<p>Visual defect detection is critical to ensure the quality of most products.\nHowever, the majority of small and medium-sized manufacturing enterprises still\nrely on tedious and error-prone human manual inspection. The main reasons\ninclude: 1) the existing automated visual defect detection systems require\naltering production assembly lines, which is time consuming and expensive 2)\nthe existing systems require manually collecting defective samples and labeling\nthem for a comparison-based algorithm or training a machine learning model.\nThis introduces a heavy burden for small and medium-sized manufacturing\nenterprises as defects do not happen often and are difficult and time-consuming\nto collect. Furthermore, we cannot exhaustively collect or define all defect\ntypes as any new deviation from acceptable products are defects. In this paper,\nwe overcome these challenges and design a three-stage plug-and-play fully\nautomated unsupervised 360-degree defect detection system. In our system,\nproducts are freely placed on an unaltered assembly line and receive 360 degree\nvisual inspection with multiple cameras from different angles. As such, the\nimages collected from real-world product assembly lines contain lots of\nbackground noise. The products face different angles. The product sizes vary\ndue to the distance to cameras. All these make defect detection much more\ndifficult. Our system use object detection, background subtraction and\nunsupervised normalizing flow-based defect detection techniques to tackle these\ndifficulties. Experiments show our system can achieve 0.90 AUROC in a\nreal-world non-altered drinkware production assembly line.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kuang_Z/0/1/0/all/0/1\">Zijian Kuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tie_X/0/1/0/all/0/1\">Xinran Tie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ying_L/0/1/0/all/0/1\">Lihang Ying</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_S/0/1/0/all/0/1\">Shi Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Iterative Facial Image Inpainting Based on an Encoder-Generator Architecture. (arXiv:2101.07036v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2101.07036","description":"<p>Facial image inpainting is a challenging problem as it requires generating\nnew pixels that include semantic information for masked key components in a\nface, e.g., eyes and nose. Recently, remarkable methods have been proposed in\nthis field. Most of these approaches use encoder-decoder architectures and have\ndifferent limitations such as allowing unique results for a given image and a\nparticular mask. Alternatively, some optimization-based approaches generate\npromising results using different masks with generator networks. However, these\napproaches are computationally more expensive. In this paper, we propose an\nefficient solution to the facial image inpainting problem using the Cyclic\nReverse Generator (CRG) architecture, which provides an encoder-generator\nmodel. We use the encoder to embed a given image to the generator space and\nincrementally inpaint the masked regions until a plausible image is generated;\nwe trained a discriminator model to assess the quality of the generated images\nduring the iterations and determine the convergence. After the generation\nprocess, for the post-processing, we utilize a Unet model that we trained\nspecifically for this task to remedy the artifacts close to the mask\nboundaries. We empirically observed that only a few iterations are sufficient\nto generate realistic images with the proposed model. Since the models are not\ntrained for particular mask types, our method allows applying sketch-based\ninpaintings, using a variety of mask types, and producing multiple and diverse\nresults. We compared our method with the state-of-the-art models both\nquantitatively and qualitatively, and observed that our method can compete with\nthe other models in all mask types; it is particularly better in images where\nlarger masks are utilized. Our code, dataset and models are available at:\nhttps://github.com/yahyadogan72/iterative facial image inpainting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Dogan_Y/0/1/0/all/0/1\">Yahya Dogan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Keles_H/0/1/0/all/0/1\">Hacer Yalim Keles</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-source Pseudo-label Learning of Semantic Segmentation for the Scene Recognition of Agricultural Mobile Robots. (arXiv:2102.06386v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.06386","description":"<p>This paper describes a novel method of training a semantic segmentation model\nfor scene recognition of agricultural mobile robots exploiting publicly\navailable datasets of outdoor scenes that are different from the target\ngreenhouse environments. Semantic segmentation models require abundant labels\ngiven by tedious manual annotation. A method to work around it is unsupervised\ndomain adaptation (UDA) that transfers knowledge from labeled source datasets\nto unlabeled target datasets. However, the effectiveness of existing methods is\nnot well studied in adaptation between heterogeneous environments, such as\nurban scenes and greenhouses. In this paper, we propose a method to train a\nsemantic segmentation model for greenhouse images without manually labeled\ndatasets of greenhouse images. The core of our idea is to use multiple rich\nimage datasets of different environments with segmentation labels to generate\npseudo-labels for the target images to effectively transfer the knowledge from\nmultiple sources and realize a precise training of semantic segmentation. Along\nwith the pseudo-label generation, we introduce state-of-the-art methods to deal\nwith noise in the pseudo-labels to further improve the performance. We\ndemonstrate in experiments with multiple greenhouse datasets that our proposed\nmethod improves the performance compared to the single-source baselines and an\nexisting approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Matsuzaki_S/0/1/0/all/0/1\">Shigemichi Matsuzaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miura_J/0/1/0/all/0/1\">Jun Miura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masuzawa_H/0/1/0/all/0/1\">Hiroaki Masuzawa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Gait Recognition: A Survey. (arXiv:2102.09546v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.09546","description":"<p>Gait recognition is an appealing biometric modality which aims to identify\nindividuals based on the way they walk. Deep learning has reshaped the research\nlandscape in this area since 2015 through the ability to automatically learn\ndiscriminative representations. Gait recognition methods based on deep learning\nnow dominate the state-of-the-art in the field and have fostered real-world\napplications. In this paper, we present a comprehensive overview of\nbreakthroughs and recent developments in gait recognition with deep learning,\nand cover broad topics including datasets, test protocols, state-of-the-art\nsolutions, challenges, and future research directions. We first review the\ncommonly used gait datasets along with the principles designed for evaluating\nthem. We then propose a novel taxonomy made up of four separate dimensions\nnamely body representation, temporal representation, feature representation,\nand neural architecture, to help characterize and organize the research\nlandscape and literature in this area. Following our proposed taxonomy, a\ncomprehensive survey of gait recognition methods using deep learning is\npresented with discussions on their performances, characteristics, advantages,\nand limitations. We conclude this survey with a discussion on current\nchallenges and mention a number of promising directions for future research in\ngait recognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sepas_Moghaddam_A/0/1/0/all/0/1\">Alireza Sepas-Moghaddam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Etemad_A/0/1/0/all/0/1\">Ali Etemad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Disentangled Representation by Exploiting Pretrained Generative Models: A Contrastive Learning View. (arXiv:2102.10543v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.10543","description":"<p>From the intuitive notion of disentanglement, the image variations\ncorresponding to different factors should be distinct from each other, and the\ndisentangled representation should reflect those variations with separate\ndimensions. To discover the factors and learn disentangled representation,\nprevious methods typically leverage an extra regularization term when learning\nto generate realistic images. However, the term usually results in a trade-off\nbetween disentanglement and generation quality. For the generative models\npretrained without any disentanglement term, the generated images show\nsemantically meaningful variations when traversing along different directions\nin the latent space. Based on this observation, we argue that it is possible to\nmitigate the trade-off by $(i)$ leveraging the pretrained generative models\nwith high generation quality, $(ii)$ focusing on discovering the traversal\ndirections as factors for disentangled representation learning. To achieve\nthis, we propose Disentaglement via Contrast (DisCo) as a framework to model\nthe variations based on the target disentangled representations, and contrast\nthe variations to jointly discover disentangled directions and learn\ndisentangled representations. DisCo achieves the state-of-the-art disentangled\nrepresentation learning and distinct direction discovering, given pretrained\nnon-disentangled generative models including GAN, VAE, and Flow. Source code is\nat https://github.com/xrenaa/DisCo.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xuanchi Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Tao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuwang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1\">Wenjun Zeng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Representational Invariances for Data-Efficient Action Recognition. (arXiv:2103.16565v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.16565","description":"<p>Data augmentation is a ubiquitous technique for improving image\nclassification when labeled data is scarce. Constraining the model predictions\nto be invariant to diverse data augmentations effectively injects the desired\nrepresentational invariances to the model (e.g., invariance to photometric\nvariations) and helps improve accuracy. Compared to image data, the appearance\nvariations in videos are far more complex due to the additional temporal\ndimension. Yet, data augmentation methods for videos remain under-explored.\nThis paper investigates various data augmentation strategies that capture\ndifferent video invariances, including photometric, geometric, temporal, and\nactor/scene augmentations. When integrated with existing semi-supervised\nlearning frameworks, we show that our data augmentation strategy leads to\npromising performance on the Kinetics-100/400, Mini-Something-v2, UCF-101, and\nHMDB-51 datasets in the low-label regime. We also validate our data\naugmentation strategy in the fully supervised setting and demonstrate improved\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuliang Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jinwoo Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qitong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jia-Bin Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D Convolutional Neural Networks for Stalled Brain Capillary Detection. (arXiv:2104.01687v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2104.01687","description":"<p>Adequate blood supply is critical for normal brain function. Brain\nvasculature dysfunctions such as stalled blood flow in cerebral capillaries are\nassociated with cognitive decline and pathogenesis in Alzheimer's disease.\nRecent advances in imaging technology enabled generation of high-quality 3D\nimages that can be used to visualize stalled blood vessels. However,\nlocalization of stalled vessels in 3D images is often required as the first\nstep for downstream analysis, which can be tedious, time-consuming and\nerror-prone, when done manually. Here, we describe a deep learning-based\napproach for automatic detection of stalled capillaries in brain images based\non 3D convolutional neural networks. Our networks employed custom 3D data\naugmentations and were used weight transfer from pre-trained 2D models for\ninitialization. We used an ensemble of several 3D models to produce the winning\nsubmission to the Clog Loss: Advance Alzheimer's Research with Stall Catchers\nmachine learning competition that challenged the participants with classifying\nblood vessels in 3D image stacks as stalled or flowing. In this setting, our\napproach outperformed other methods and demonstrated state-of-the-art results,\nachieving 0.85 Matthews correlation coefficient, 85% sensitivity, and 99.3%\nspecificity. The source code for our solution is made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Solovyev_R/0/1/0/all/0/1\">Roman Solovyev</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kalinin_A/0/1/0/all/0/1\">Alexandr A. Kalinin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gabruseva_T/0/1/0/all/0/1\">Tatiana Gabruseva</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lung Cancer Diagnosis Using Deep Attention Based on Multiple Instance Learning and Radiomics. (arXiv:2104.14655v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2104.14655","description":"<p>Early diagnosis of lung cancer is a key intervention for the treatment of\nlung cancer computer aided diagnosis (CAD) can play a crucial role. However,\nmost published CAD methods treat lung cancer diagnosis as a lung nodule\nclassification problem, which does not reflect clinical practice, where\nclinicians diagnose a patient based on a set of images of nodules, instead of\none specific nodule. Besides, the low interpretability of the output provided\nby these methods presents an important barrier for their adoption. In this\narticle, we treat lung cancer diagnosis as a multiple instance learning (MIL)\nproblem in order to better reflect the diagnosis process in the clinical\nsetting and for the higher interpretability of the output. We chose radiomics\nas the source of input features and deep attention-based MIL as the\nclassification algorithm.The attention mechanism provides higher\ninterpretability by estimating the importance of each instance in the set for\nthe final diagnosis.In order to improve the model's performance in a small\nimbalanced dataset, we introduce a new bag simulation method for MIL.The\nresults show that our method can achieve a mean accuracy of 0.807 with a\nstandard error of the mean (SEM) of 0.069, a recall of 0.870 (SEM 0.061), a\npositive predictive value of 0.928 (SEM 0.078), a negative predictive value of\n0.591 (SEM 0.155) and an area under the curve (AUC) of 0.842 (SEM 0.074),\noutperforming other MIL methods.Additional experiments show that the proposed\noversampling strategy significantly improves the model's performance. In\naddition, our experiments show that our method provides an indication of the\nimportance of each nodule in determining the diagnosis, which combined with the\nwell-defined radiomic features, make the results more interpretable and\nacceptable for doctors and patients.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chen_J/0/1/0/all/0/1\">Junhua Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zeng_H/0/1/0/all/0/1\">Haiyan Zeng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_C/0/1/0/all/0/1\">Chong Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shi_Z/0/1/0/all/0/1\">Zhenwei Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dekker_A/0/1/0/all/0/1\">Andre Dekker</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wee_L/0/1/0/all/0/1\">Leonard Wee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bermejo_I/0/1/0/all/0/1\">Inigo Bermejo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Blurs Behave Like Ensembles: Spatial Smoothings to Improve Accuracy, Uncertainty, and Robustness. (arXiv:2105.12639v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2105.12639","description":"<p>Neural network ensembles, such as Bayesian neural networks (BNNs), have shown\nsuccess in the areas of uncertainty estimation and robustness. However, a\ncrucial challenge prohibits their use in practice. BNNs require a large number\nof predictions to produce reliable results, leading to a significant increase\nin computational cost. To alleviate this issue, we propose spatial smoothing, a\nmethod that spatially ensembles neighboring feature map points of convolutional\nneural networks. By simply adding a few blur layers to the models, we\nempirically show that spatial smoothing improves accuracy, uncertainty\nestimation, and robustness of BNNs across a whole range of ensemble sizes. In\nparticular, BNNs incorporating spatial smoothing achieve high predictive\nperformance merely with a handful of ensembles. Moreover, this method also can\nbe applied to canonical deterministic neural networks to improve the\nperformances. A number of evidences suggest that the improvements can be\nattributed to the stabilized feature maps and the smoothing of the loss\nlandscape. In addition, we provide a fundamental explanation for prior works -\nnamely, global average pooling, pre-activation, and ReLU6 - by addressing them\nas special cases of spatial smoothing. These not only enhance accuracy, but\nalso improve uncertainty estimation and robustness by making the loss landscape\nsmoother in the same manner as spatial smoothing. The code is available at\nhttps://github.com/xxxnell/spatial-smoothing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_N/0/1/0/all/0/1\">Namuk Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Songkuk Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Real-time and Light-weight Line Segment Detection. (arXiv:2106.00186v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.00186","description":"<p>Previous deep learning-based line segment detection (LSD) suffers from the\nimmense model size and high computational cost for line prediction. This\nconstrains them from real-time inference on computationally restricted\nenvironments. In this paper, we propose a real-time and light-weight line\nsegment detector for resource-constrained environments named Mobile LSD\n(M-LSD). We design an extremely efficient LSD architecture by minimizing the\nbackbone network and removing the typical multi-module process for line\nprediction found in previous methods. To maintain competitive performance with\na light-weight network, we present novel training schemes: Segments of Line\nsegment (SoL) augmentation, matching and geometric loss. SoL augmentation\nsplits a line segment into multiple subparts, which are used to provide\nauxiliary line data during the training process. Moreover, the matching and\ngeometric loss allow a model to capture additional geometric cues. Compared\nwith TP-LSD-Lite, previously the best real-time LSD method, our model\n(M-LSD-tiny) achieves competitive performance with 2.5% of model size and an\nincrease of 130.5% in inference speed on GPU. Furthermore, our model runs at\n56.8 FPS and 48.6 FPS on the latest Android and iPhone mobile devices,\nrespectively. To the best of our knowledge, this is the first real-time deep\nLSD available on mobile devices. Our code is available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_G/0/1/0/all/0/1\">Geonmo Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_B/0/1/0/all/0/1\">Byungsoo Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Go_S/0/1/0/all/0/1\">SeoungHyun Go</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sung-Hyun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jingeun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_M/0/1/0/all/0/1\">Minchul Shin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"P2T: Pyramid Pooling Transformer for Scene Understanding. (arXiv:2106.12011v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.12011","description":"<p>Recently, the vision transformer has achieved great successes by pushing the\nstate-of-the-arts of various vision tasks. One of the most challenging problems\nin the vision transformer is that the large sequence length of image tokens\nleads to high computational cost (quadratic complexity). A popular solution to\nthis problem is to use a single pooling operation to reduce the sequence\nlength. This paper considers how to improve existing vision transformers where\nthe pooled feature extracted by a single pooling operation seems less powerful.\nTo this end, we note that pyramid pooling has been demonstrated to be effective\nin various vision tasks owing to its powerful ability in context abstraction.\nHowever, pyramid pooling has not been explored in backbone network design. To\nbridge this gap, we propose to adapt pyramid pooling to Multi-Head\nSelf-Attention (MHSA) in the vision transformer, simultaneously reducing the\nsequence length and capturing powerful contextual features. Plugged with our\npooling-based MHSA, we build a universal vision transformer backbone, dubbed\nPyramid Pooling Transformer (P2T). Extensive experiments demonstrate that, when\napplied P2T as the backbone network, it shows substantial superiority in\nvarious vision tasks such as image classification, semantic segmentation,\nobject detection, and instance segmentation, compared to previous CNN- and\ntransformer-based networks. The code will be released at\nhttps://github.com/yuhuan-wu/P2T.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yu-Huan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_X/0/1/0/all/0/1\">Xin Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1\">Ming-Ming Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"No-Reference Quality Assessment for 3D Colored Point Cloud and Mesh Models. (arXiv:2107.02041v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.02041","description":"<p>To improve the viewer's Quality of Experience (QoE) and optimize computer\ngraphics applications, 3D model quality assessment (3D-QA) has become an\nimportant task in the multimedia area. Point cloud and mesh are the two most\nwidely used digital representation formats of 3D models, the visual quality of\nwhich is quite sensitive to lossy operations like simplification and\ncompression. Therefore, many related studies such as point cloud quality\nassessment (PCQA) and mesh quality assessment (MQA) have been carried out to\nmeasure the visual quality degradations of 3D models. However, a large part of\nprevious studies utilize full-reference (FR) metrics, which indicates they can\nnot predict the quality level with the absence of the reference 3D model.\nFurthermore, few 3D-QA metrics consider color information, which significantly\nrestricts their effectiveness and scope of application. In this paper, we\npropose a no-reference (NR) quality assessment metric for colored 3D models\nrepresented by both point cloud and mesh. First, we project the 3D models from\n3D space into quality-related geometry and color feature domains. Then, the 3D\nnatural scene statistics (3D-NSS) and entropy are utilized to extract\nquality-aware features. Finally, machine learning is employed to regress the\nquality-aware features into visual quality scores. Our method is validated on\nthe colored point cloud quality assessment database (SJTU-PCQA), the Waterloo\npoint cloud assessment database (WPC), and the colored mesh quality assessment\ndatabase (CMDM). The experimental results show that the proposed method\noutperforms most compared NR 3D-QA metrics with competitive computational\nresources and greatly reduces the performance gap with the state-of-the-art FR\n3D-QA metrics. The code of the proposed model is publicly available now to\nfacilitate further research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zicheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Wei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_X/0/1/0/all/0/1\">Xiongkuo Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Wei Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_G/0/1/0/all/0/1\">Guangtao Zhai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Representation Learning Does Not Generalize Strongly Within the Same Domain. (arXiv:2107.08221v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2107.08221","description":"<p>An important component for generalization in machine learning is to uncover\nunderlying latent factors of variation as well as the mechanism through which\neach factor acts in the world. In this paper, we test whether 17 unsupervised,\nweakly supervised, and fully supervised representation learning approaches\ncorrectly infer the generative factors of variation in simple datasets\n(dSprites, Shapes3D, MPI3D) from controlled environments, and on our\ncontributed CelebGlow dataset. In contrast to prior robustness work that\nintroduces novel factors of variation during test time, such as blur or other\n(un)structured noise, we here recompose, interpolate, or extrapolate only\nexisting factors of variation from the training data set (e.g., small and\nmedium-sized objects during training and large objects during testing). Models\nthat learn the correct mechanism should be able to generalize to this\nbenchmark. In total, we train and test 2000+ models and observe that all of\nthem struggle to learn the underlying mechanism regardless of supervision\nsignal and architectural bias. Moreover, the generalization capabilities of all\ntested models drop significantly as we move from artificial datasets towards\nmore realistic real-world datasets. Despite their inability to identify the\ncorrect mechanism, the models are quite modular as their ability to infer other\nin-distribution factors remains fairly stable, providing only a single factor\nis out-of-distribution. These results point to an important yet understudied\nproblem of learning mechanistic models of observations that can facilitate\ngeneralization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schott_L/0/1/0/all/0/1\">Lukas Schott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kugelgen_J/0/1/0/all/0/1\">Julius von K&#xfc;gelgen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trauble_F/0/1/0/all/0/1\">Frederik Tr&#xe4;uble</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gehler_P/0/1/0/all/0/1\">Peter Gehler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Russell_C/0/1/0/all/0/1\">Chris Russell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bethge_M/0/1/0/all/0/1\">Matthias Bethge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Locatello_F/0/1/0/all/0/1\">Francesco Locatello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brendel_W/0/1/0/all/0/1\">Wieland Brendel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MCDAL: Maximum Classifier Discrepancy for Active Learning. (arXiv:2107.11049v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.11049","description":"<p>Recent state-of-the-art active learning methods have mostly leveraged\nGenerative Adversarial Networks (GAN) for sample acquisition; however, GAN is\nusually known to suffer from instability and sensitivity to hyper-parameters.\nIn contrast to these methods, we propose in this paper a novel active learning\nframework that we call Maximum Classifier Discrepancy for Active Learning\n(MCDAL) which takes the prediction discrepancies between multiple classifiers.\nIn particular, we utilize two auxiliary classification layers that learn\ntighter decision boundaries by maximizing the discrepancies among them.\nIntuitively, the discrepancies in the auxiliary classification layers'\npredictions indicate the uncertainty in the prediction. In this regard, we\npropose a novel method to leverage the classifier discrepancies for the\nacquisition function for active learning. We also provide an interpretation of\nour idea in relation to existing GAN based active learning methods and domain\nadaptation frameworks. Moreover, we empirically demonstrate the utility of our\napproach where the performance of our approach exceeds the state-of-the-art\nmethods on several image classification and semantic segmentation datasets in\nactive learning setups.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1\">Jae Won Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Dong-Jin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_Y/0/1/0/all/0/1\">Yunjae Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kweon_I/0/1/0/all/0/1\">In So Kweon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NODEO: A Neural Ordinary Differential Equation Based Optimization Framework for Deformable Image Registration. (arXiv:2108.03443v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.03443","description":"<p>Deformable image registration (DIR), aiming to find spatial correspondence\nbetween images, is one of the most critical problems in the domain of medical\nimage analysis. In this paper, we present a novel, generic, and accurate\ndiffeomorphic image registration framework that utilizes neural ordinary\ndifferential equations (NODEs). We model each voxel as a moving particle and\nconsider the set of all voxels in a 3D image as a high-dimensional dynamical\nsystem whose trajectory determines the targeted deformation field. Our method\nleverages deep neural networks for their expressive power in modeling dynamical\nsystems, and simultaneously optimizes for a dynamical system between the image\npairs and the corresponding transformation. Our formulation allows various\nconstraints to be imposed along the transformation to maintain desired\nregularities. Our experiment results show that our method outperforms the\nbenchmarks under various metrics. Additionally, we demonstrate the feasibility\nto expand our framework to register multiple image sets using a unified form of\ntransformation,which could possibly serve a wider range of applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yifan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiahao_T/0/1/0/all/0/1\">Tom Z.Jiahao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiancong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yushkevich_P/0/1/0/all/0/1\">Paul A.Yushkevich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_M/0/1/0/all/0/1\">M.Ani Hsieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gee_J/0/1/0/all/0/1\">James C.Gee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Polyp-PVT: Polyp Segmentation with Pyramid Vision Transformers. (arXiv:2108.06932v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.06932","description":"<p>Most polyp segmentation methods use CNNs as their backbone, leading to two\nkey issues when exchanging information between the encoder and decoder: 1)\ntaking into account the differences in contribution between different-level\nfeatures; and 2) designing an effective mechanism for fusing these features.\nDifferent from existing CNN-based methods, we adopt a transformer encoder,\nwhich learns more powerful and robust representations. In addition, considering\nthe image acquisition influence and elusive properties of polyps, we introduce\nthree novel modules, including a cascaded fusion module (CFM), a camouflage\nidentification module (CIM), a and similarity aggregation module (SAM). Among\nthese, the CFM is used to collect the semantic and location information of\npolyps from high-level features, while the CIM is applied to capture polyp\ninformation disguised in low-level features. With the help of the SAM, we\nextend the pixel features of the polyp area with high-level semantic position\ninformation to the entire polyp area, thereby effectively fusing cross-level\nfeatures. The proposed model, named \\ourmodel, effectively suppresses noises in\nthe features and significantly improves their expressive capabilities.\nExtensive experiments on five widely adopted datasets show that the proposed\nmodel is more robust to various challenging situations (e.g. appearance\nchanges, small objects) than existing methods, and achieves the new\nstate-of-the-art performance. The proposed model is available at\nhttps://github.com/DengPingFan/Polyp-PVT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_B/0/1/0/all/0/1\">Bo Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenhai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinpeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1\">Deng-Ping Fan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Attentive Deep Neural Network for Exposing GAN-generated Faces. (arXiv:2109.02167v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.02167","description":"<p>GAN-based techniques that generate and synthesize realistic faces have caused\nsevere social concerns and security problems. Existing methods for detecting\nGAN-generated faces can perform well on limited public datasets. However,\nimages from existing public datasets do not represent real-world scenarios well\nenough in terms of view variations and data distributions (where real faces\nlargely outnumber synthetic faces). The state-of-the-art methods do not\ngeneralize well in real-world problems and lack the interpretability of\ndetection results. Performance of existing GAN-face detection models degrades\nsignificantly when facing imbalanced data distributions. To address these\nshortcomings, we propose a robust, attentive, end-to-end network that can spot\nGAN-generated faces by analyzing their eye inconsistencies. Specifically, our\nmodel learns to identify inconsistent eye components by localizing and\ncomparing the iris artifacts between the two eyes automatically. Our deep\nnetwork addresses the imbalance learning issues by considering the AUC loss and\nthe traditional cross-entropy loss jointly. Comprehensive evaluations of the\nFFHQ dataset in terms of both balanced and imbalanced scenarios demonstrate the\nsuperiority of the proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Hui Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1\">Ming-Ching Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1\">Siwei Lyu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Local Domains for Image-to-Image Translation. (arXiv:2109.04468v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.04468","description":"<p>Image-to-image (i2i) networks struggle to capture local changes because they\ndo not affect the global scene structure. For example, translating from highway\nscenes to offroad, i2i networks easily focus on global color features but\nignore obvious traits for humans like the absence of lane markings. In this\npaper, we leverage human knowledge about spatial domain characteristics which\nwe refer to as 'local domains' and demonstrate its benefit for image-to-image\ntranslation. Relying on a simple geometrical guidance, we train a patch-based\nGAN on few source data and hallucinate a new unseen domain which subsequently\neases transfer learning to target. We experiment on three tasks ranging from\nunstructured environments to adverse weather. Our comprehensive evaluation\nsetting shows we are able to generate realistic translations, with minimal\npriors, and training only on a few images. Furthermore, when trained on our\ntranslations images we show that all tested proxy tasks are significantly\nimproved, without ever seeing target domain at training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+DellEva_A/0/1/0/all/0/1\">Anthony Dell&#x27;Eva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pizzati_F/0/1/0/all/0/1\">Fabio Pizzati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bertozzi_M/0/1/0/all/0/1\">Massimo Bertozzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charette_R/0/1/0/all/0/1\">Raoul de Charette</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A survey on deep learning approaches for breast cancer diagnosis. (arXiv:2109.08853v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2109.08853","description":"<p>Deep learning has introduced several learning-based methods to recognize\nbreast tumours and presents high applicability in breast cancer diagnostics. It\nhas presented itself as a practical installment in Computer-Aided Diagnostic\n(CAD) systems to further assist radiologists in diagnostics for different\nmodalities. A deep learning network trained on images provided by hospitals or\npublic databases can perform classification, detection, and segmentation of\nlesion types. Significant progress has been made in recognizing tumours on 2D\nimages but recognizing 3D images remains a frontier so far. The interconnection\nof deep learning networks between different fields of study help propels\ndiscoveries for more efficient, accurate, and robust networks. In this review\npaper, the following topics will be explored: (i) theory and application of\ndeep learning, (ii) progress of 2D, 2.5D, and 3D CNN approaches in breast\ntumour recognition from a performance metric perspective, and (iii) challenges\nfaced in CNN approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kwong_T/0/1/0/all/0/1\">Timothy Kwong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mazaheri_S/0/1/0/all/0/1\">Samaneh Mazaheri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gradient Step Denoiser for convergent Plug-and-Play. (arXiv:2110.03220v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.03220","description":"<p>Plug-and-Play methods constitute a class of iterative algorithms for imaging\nproblems where regularization is performed by an off-the-shelf denoiser.\nAlthough Plug-and-Play methods can lead to tremendous visual performance for\nvarious image problems, the few existing convergence guarantees are based on\nunrealistic (or suboptimal) hypotheses on the denoiser, or limited to strongly\nconvex data terms. In this work, we propose a new type of Plug-and-Play\nmethods, based on half-quadratic splitting, for which the denoiser is realized\nas a gradient descent step on a functional parameterized by a deep neural\nnetwork. Exploiting convergence results for proximal gradient descent\nalgorithms in the non-convex setting, we show that the proposed Plug-and-Play\nalgorithm is a convergent iterative scheme that targets stationary points of an\nexplicit global functional. Besides, experiments show that it is possible to\nlearn such a deep denoiser while not compromising the performance in comparison\nto other state-of-the-art deep denoisers used in Plug-and-Play schemes. We\napply our proximal gradient algorithm to various ill-posed inverse problems,\ne.g. deblurring, super-resolution and inpainting. For all these applications,\nnumerical results empirically confirm the convergence results. Experiments also\nshow that this new algorithm reaches state-of-the-art performance, both\nquantitatively and qualitatively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hurault_S/0/1/0/all/0/1\">Samuel Hurault</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leclaire_A/0/1/0/all/0/1\">Arthur Leclaire</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papadakis_N/0/1/0/all/0/1\">Nicolas Papadakis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MoDeRNN: Towards Fine-grained Motion Details for Spatiotemporal Predictive Learning. (arXiv:2110.12978v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.12978","description":"<p>Spatiotemporal predictive learning (ST-PL) aims at predicting the subsequent\nframes via limited observed sequences, and it has broad applications in the\nreal world. However, learning representative spatiotemporal features for\nprediction is challenging. Moreover, chaotic uncertainty among consecutive\nframes exacerbates the difficulty in long-term prediction. This paper\nconcentrates on improving prediction quality by enhancing the correspondence\nbetween the previous context and the current state. We carefully design Detail\nContext Block (DCB) to extract fine-grained details and improve the isolated\ncorrelation between upper context state and current input state. We integrate\nDCB with standard ConvLSTM and introduce Motion Details RNN (MoDeRNN) to\ncapture fine-grained spatiotemporal features and improve the expression of\nlatent states of RNNs to achieve significant quality. Experiments on Moving\nMNIST and Typhoon datasets demonstrate the effectiveness of the proposed\nmethod. MoDeRNN outperforms existing state-of-the-art techniques qualitatively\nand quantitatively with lower computation loads.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chai_Z/0/1/0/all/0/1\">Zenghao Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhengzhuo Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1\">Chun Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Integrated Pipeline of Segmentation Guided Classification of Breast Cancer from Ultrasound Images. (arXiv:2110.14013v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2110.14013","description":"<p>Breast cancer has become a symbol of tremendous concern in the modern world,\nas it is one of the major causes of cancer mortality worldwide. In this regard,\nbreast ultrasonography images are frequently utilized by doctors to diagnose\nbreast cancer at an early stage. However, the complex artifacts and heavily\nnoised breast ultrasonography images make diagnosis a great challenge.\nFurthermore, the ever-increasing number of patients being screened for breast\ncancer necessitates the use of automated end-to-end technology for highly\naccurate diagnosis at a low cost and in a short time. In this concern, to\ndevelop an end-to-end integrated pipeline for breast ultrasonography image\nclassification, we conducted an exhaustive analysis of image preprocessing\nmethods such as K Means++ and SLIC, as well as four transfer learning models\nsuch as VGG16, VGG19, DenseNet121, and ResNet50. With a Dice-coefficient score\nof 63.4 in the segmentation stage and accuracy and an F1-Score (Benign) of\n73.72 percent and 78.92 percent in the classification stage, the combination of\nSLIC, UNET, and VGG16 outperformed all other integrated combinations. Finally,\nwe have proposed an end to end integrated automated pipelining framework which\nincludes preprocessing with SLIC to capture super-pixel features from the\ncomplex artifact of ultrasonography images, complementing semantic segmentation\nwith modified U-Net, leading to breast tumor classification using a transfer\nlearning approach with a pre-trained VGG16 and a densely connected neural\nnetwork. The proposed automated pipeline can be effectively implemented to\nassist medical practitioners in making more accurate and timely diagnoses of\nbreast cancer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Inan_M/0/1/0/all/0/1\">Muhammad Sakib Khan Inan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Alam_F/0/1/0/all/0/1\">Fahim Irfan Alam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hasan_R/0/1/0/all/0/1\">Rizwan Hasan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Residual Quantity in Percentage of Factory Machines Using Computer Vision and Mathematical Methods. (arXiv:2111.05080v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.05080","description":"<p>Computer vision has been thriving since AI development was gaining thrust.\nUsing deep learning techniques has been the most popular way which computer\nscientists thought the solution of. However, deep learning techniques tend to\nshow lower performance than manual processing. Using deep learning is not\nalways the answer to a problem related to computer vision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seunghyeon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryoo_J/0/1/0/all/0/1\">Jihoon Ryoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dongyeob Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Youngho Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploiting Robust Unsupervised Video Person Re-identification. (arXiv:2111.05170v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.05170","description":"<p>Unsupervised video person re-identification (reID) methods usually depend on\nglobal-level features. And many supervised reID methods employed local-level\nfeatures and achieved significant performance improvements. However, applying\nlocal-level features to unsupervised methods may introduce an unstable\nperformance. To improve the performance stability for unsupervised video reID,\nthis paper introduces a general scheme fusing part models and unsupervised\nlearning. In this scheme, the global-level feature is divided into equal\nlocal-level feature. A local-aware module is employed to explore the poentials\nof local-level feature for unsupervised learning. A global-aware module is\nproposed to overcome the disadvantages of local-level features. Features from\nthese two modules are fused to form a robust feature representation for each\ninput image. This feature representation has the advantages of local-level\nfeature without suffering from its disadvantages. Comprehensive experiments are\nconducted on three benchmarks, including PRID2011, iLIDS-VID, and\nDukeMTMC-VideoReID, and the results demonstrate that the proposed approach\nachieves state-of-the-art performance. Extensive ablation studies demonstrate\nthe effectiveness and robustness of proposed scheme, local-aware module and\nglobal-aware module. The code and generated features are available at\nhttps://github.com/deropty/uPMnet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zang_X/0/1/0/all/0/1\">Xianghao Zang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Ge Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1\">Wei Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_X/0/1/0/all/0/1\">Xiujun Shu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Probability Estimation. (arXiv:2111.10734v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2111.10734","description":"<p>Reliable probability estimation is of crucial importance in many real-world\napplications where there is inherent uncertainty, such as weather forecasting,\nmedical prognosis, or collision avoidance in autonomous vehicles.\nProbability-estimation models are trained on observed outcomes (e.g. whether it\nhas rained or not, or whether a patient has died or not), because the\nground-truth probabilities of the events of interest are typically unknown. The\nproblem is therefore analogous to binary classification, with the important\ndifference that the objective is to estimate probabilities rather than\npredicting the specific outcome. The goal of this work is to investigate\nprobability estimation from high-dimensional data using deep neural networks.\nThere exist several methods to improve the probabilities generated by these\nmodels but they mostly focus on classification problems where the probabilities\nare related to model uncertainty. In the case of problems with inherent\nuncertainty, it is challenging to evaluate performance without access to\nground-truth probabilities. To address this, we build a synthetic dataset to\nstudy and compare different computable metrics. We evaluate existing methods on\nthe synthetic data as well as on three real-world probability estimation tasks,\nall of which involve inherent uncertainty. We also give a theoretical analysis\nof a model for high-dimensional probability estimation which reproduces several\nof the phenomena evinced in our experiments. Finally, we propose a new method\nfor probability estimation using neural networks, which modifies the training\nprocess to promote output probabilities that are consistent with empirical\nprobabilities computed from the data. The method outperforms existing\napproaches on most metrics on the simulated as well as real-world data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaku_A/0/1/0/all/0/1\">Aakash Kaku</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Weicheng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leibovich_M/0/1/0/all/0/1\">Matan Leibovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohan_S/0/1/0/all/0/1\">Sreyas Mohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Boyang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Haoxiang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zanna_L/0/1/0/all/0/1\">Laure Zanna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Razavian_N/0/1/0/all/0/1\">Narges Razavian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niles_Weed_J/0/1/0/all/0/1\">Jonathan Niles-Weed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandez_Granda_C/0/1/0/all/0/1\">Carlos Fernandez-Granda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Handwritten Mathematical Expression Recognition via Attention Aggregation based Bi-directional Mutual Learning. (arXiv:2112.03603v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.03603","description":"<p>Handwritten mathematical expression recognition aims to automatically\ngenerate LaTeX sequences from given images. Currently, attention-based\nencoder-decoder models are widely used in this task. They typically generate\ntarget sequences in a left-to-right (L2R) manner, leaving the right-to-left\n(R2L) contexts unexploited. In this paper, we propose an Attention aggregation\nbased Bi-directional Mutual learning Network (ABM) which consists of one shared\nencoder and two parallel inverse decoders (L2R and R2L). The two decoders are\nenhanced via mutual distillation, which involves one-to-one knowledge transfer\nat each training step, making full use of the complementary information from\ntwo inverse directions. Moreover, in order to deal with mathematical symbols in\ndiverse scales, an Attention Aggregation Module (AAM) is proposed to\neffectively integrate multi-scale coverage attentions. Notably, in the\ninference phase, given that the model already learns knowledge from two inverse\ndirections, we only use the L2R branch for inference, keeping the original\nparameter size and inference speed. Extensive experiments demonstrate that our\nproposed approach achieves the recognition accuracy of 56.85 % on CROHME 2014,\n52.92 % on CROHME 2016, and 53.96 % on CROHME 2019 without data augmentation\nand model ensembling, substantially outperforming the state-of-the-art methods.\nThe source code is available in https://github.com/XH-B/ABM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bian_X/0/1/0/all/0/1\">Xiaohang Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1\">Bo Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xin_X/0/1/0/all/0/1\">Xiaozhe Xin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianwu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_X/0/1/0/all/0/1\">Xuefeng Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanfeng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adverse Weather Image Translation with Asymmetric and Uncertainty-aware GAN. (arXiv:2112.04283v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.04283","description":"<p>Adverse weather image translation belongs to the unsupervised image-to-image\n(I2I) translation task which aims to transfer adverse condition domain (eg,\nrainy night) to standard domain (eg, day). It is a challenging task because\nimages from adverse domains have some artifacts and insufficient information.\nRecently, many studies employing Generative Adversarial Networks (GANs) have\nachieved notable success in I2I translation but there are still limitations in\napplying them to adverse weather enhancement. Symmetric architecture based on\nbidirectional cycle-consistency loss is adopted as a standard framework for\nunsupervised domain transfer methods. However, it can lead to inferior\ntranslation result if the two domains have imbalanced information. To address\nthis issue, we propose a novel GAN model, i.e., AU-GAN, which has an asymmetric\narchitecture for adverse domain translation. We insert a proposed feature\ntransfer network (${T}$-net) in only a normal domain generator (i.e., rainy\nnight-&gt; day) to enhance encoded features of the adverse domain image. In\naddition, we introduce asymmetric feature matching for disentanglement of\nencoded features. Finally, we propose uncertainty-aware cycle-consistency loss\nto address the regional uncertainty of a cyclic reconstructed image. We\ndemonstrate the effectiveness of our method by qualitative and quantitative\ncomparisons with state-of-the-art models. Codes are available at\nhttps://github.com/jgkwak95/AU-GAN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kwak_J/0/1/0/all/0/1\">Jeong-gi Kwak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Youngsaeng Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuanming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_D/0/1/0/all/0/1\">Dongsik Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Donghyeon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_H/0/1/0/all/0/1\">Hanseok Ko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hybrid Atlas Building with Deep Registration Priors. (arXiv:2112.06406v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.06406","description":"<p>Registration-based atlas building often poses computational challenges in\nhigh-dimensional image spaces. In this paper, we introduce a novel hybrid atlas\nbuilding algorithm that fast estimates atlas from large-scale image datasets\nwith much reduced computational cost. In contrast to previous approaches that\niteratively perform registration tasks between an estimated atlas and\nindividual images, we propose to use learned priors of registration from\npre-trained neural networks. This newly developed hybrid framework features\nseveral advantages of (i) providing an efficient way of atlas building without\nlosing the quality of results, and (ii) offering flexibility in utilizing a\nwide variety of deep learning based registration methods. We demonstrate the\neffectiveness of this proposed model on 3D brain magnetic resonance imaging\n(MRI) scans.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_N/0/1/0/all/0/1\">Nian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Miaomiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guixu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yaxin Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chaomin Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Roadside Lidar Vehicle Detection and Tracking Using Range And Intensity Background Subtraction. (arXiv:2201.04756v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.04756","description":"<p>In this paper, we present the solution of roadside LiDAR object detection\nusing a combination of two unsupervised learning algorithms. The 3D point\nclouds data are firstly converted into spherical coordinates and filled into\nthe elevation-azimuth matrix using a hash function. After that, the raw LiDAR\ndata were rearranged into new data structures to store the information of\nrange, azimuth, and intensity. Then, the Dynamic Mode Decomposition method is\napplied for decomposing the LiDAR data into low-rank backgrounds and sparse\nforegrounds based on intensity channel pattern recognition. The Coarse Fine\nTriangle Algorithm (CFTA) automatically finds the dividing value to separate\nthe moving targets from static background according to range information. After\nintensity and range background subtraction, the foreground moving objects will\nbe detected using a density-based detector and encoded into the state-space\nmodel for tracking. The output of the proposed model includes vehicle\ntrajectories that can enable many mobility and safety applications. The method\nwas validated at both path and point levels against a commercial traffic data\ncollection platform and outperformed the state-of-the-art. In contrast to the\nprevious methods that process directly on the scattered and discrete point\nclouds, the dynamic classification method can establish the less sophisticated\nlinear relationship of the 3D measurement data, which captures the\nspatial-temporal structure that we often desire.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianya Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_P/0/1/0/all/0/1\">Peter J. Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Face Detection in Extreme Conditions: A Machine-learning Approach. (arXiv:2201.06220v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.06220","description":"<p>Face detection in unrestricted conditions has been a trouble for years due to\nvarious expressions, brightness, and coloration fringing. Recent studies show\nthat deep learning knowledge of strategies can acquire spectacular performance\ninside the identification of different gadgets and patterns. This face\ndetection in unconstrained surroundings is difficult due to various poses,\nilluminations, and occlusions. Figuring out someone with a picture has been\npopularized through the mass media. However, it's miles less sturdy to\nfingerprint or retina scanning. The latest research shows that deep mastering\ntechniques can gain mind-blowing performance on those two responsibilities. In\nthis paper, I recommend a deep cascaded multi-venture framework that exploits\nthe inherent correlation among them to boost up their performance. In\nparticular, my framework adopts a cascaded shape with 3 layers of cautiously\ndesigned deep convolutional networks that expect face and landmark region in a\ncoarse-to-fine way. Besides, within the gaining knowledge of the procedure, I\npropose a new online tough sample mining method that can enhance the\nperformance robotically without manual pattern choice.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hashmi_S/0/1/0/all/0/1\">Sameer Aqib Hashmi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reading-strategy Inspired Visual Representation Learning for Text-to-Video Retrieval. (arXiv:2201.09168v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.09168","description":"<p>This paper aims for the task of text-to-video retrieval, where given a query\nin the form of a natural-language sentence, it is asked to retrieve videos\nwhich are semantically relevant to the given query, from a great number of\nunlabeled videos. The success of this task depends on cross-modal\nrepresentation learning that projects both videos and sentences into common\nspaces for semantic similarity computation. In this work, we concentrate on\nvideo representation learning, an essential component for text-to-video\nretrieval. Inspired by the reading strategy of humans, we propose a\nReading-strategy Inspired Visual Representation Learning (RIVRL) to represent\nvideos, which consists of two branches: a previewing branch and an\nintensive-reading branch. The previewing branch is designed to briefly capture\nthe overview information of videos, while the intensive-reading branch is\ndesigned to obtain more in-depth information. Moreover, the intensive-reading\nbranch is aware of the video overview captured by the previewing branch. Such\nholistic information is found to be useful for the intensive-reading branch to\nextract more fine-grained features. Extensive experiments on three datasets are\nconducted, where our model RIVRL achieves a new state-of-the-art on TGIF and\nVATEX. Moreover, on MSR-VTT, our model using two video features shows\ncomparable performance to the state-of-the-art using seven video features and\neven outperforms models pre-trained on the large-scale HowTo100M dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Jianfeng Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yabing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xianke Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_X/0/1/0/all/0/1\">Xiaoye Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xirong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yuan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xun Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"POTHER: Patch-Voted Deep Learning-based Chest X-ray Bias Analysis for COVID-19 Detection. (arXiv:2201.09360v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2201.09360","description":"<p>A critical step in the fight against COVID-19, which continues to have a\ncatastrophic impact on peoples lives, is the effective screening of patients\npresented in the clinics with severe COVID-19 symptoms. Chest radiography is\none of the promising screening approaches. Many studies reported detecting\nCOVID-19 in chest X-rays accurately using deep learning. A serious limitation\nof many published approaches is insufficient attention paid to explaining\ndecisions made by deep learning models. Using explainable artificial\nintelligence methods, we demonstrate that model decisions may rely on\nconfounding factors rather than medical pathology. After an analysis of\npotential confounding factors found on chest X-ray images, we propose a novel\nmethod to minimise their negative impact. We show that our proposed method is\nmore robust than previous attempts to counter confounding factors such as ECG\nleads in chest X-rays that often influence model classification decisions. In\naddition to being robust, our method achieves results comparable to the\nstate-of-the-art. The source code and pre-trained weights are publicly\navailable (https://github.com/tomek1911/POTHER).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Szczepanski_T/0/1/0/all/0/1\">Tomasz Szczepa&#x144;ski</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sitek_A/0/1/0/all/0/1\">Arkadiusz Sitek</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Trzcinski_T/0/1/0/all/0/1\">Tomasz Trzci&#x144;ski</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Plotka_S/0/1/0/all/0/1\">Szymon P&#x142;otka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beam Stack Search-based reconstruction of unhealthy coronary artery wall segmentations in CCTA-CPR scans. (arXiv:2201.10424v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2201.10424","description":"<p>The estimation of the coronary artery wall boundaries in CCTA scans is a\ncostly but essential task in the diagnosis of cardiac diseases. To automatize\nthis task, deep learning-based image segmentation methods are commonly used.\nHowever, in the case of coronary artery wall, even state-of-the-art\nsegmentation methods fail to produce an accurate boundary in the presence of\nplaques and bifurcations. Post-processing reconstruction methods have been\nproposed to further refine segmentation results, but when applying\ngeneral-purpose reconstruction to artery wall segmentations, they fail to\nreproduce the wide variety of boundary shapes. In this paper, we propose a\nnovel method for reconstructing coronary artery wall segmentations, the Tube\nBeam Stack Search (TBSS). By leveraging the voxel shape of adjacent slices in a\nCPR volume, our TBSS is capable of finding the most plausible path of the\nartery wall. Similarly to the original Beam Stack Search, TBSS navigates along\nthe voxel probabilities output by the segmentation method, reconstructing the\ninner and outer artery walls. Finally, skeletonization is applied on the TBSS\nreconstructions to eliminate noise and produce more refined segmentations.\nAlso, since our method does not require learning a model, the lack of annotated\ndata is not a limitation. We evaluated our method on a dataset of coronary CT\nangiography with curved planar reconstruction (CCTA-CPR) of 92 arteries.\nExperimental results show that our method outperforms the state-of-the-art work\nin reconstruction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Tejero_de_Pablos_A/0/1/0/all/0/1\">Antonio Tejero-de-Pablos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yamane_H/0/1/0/all/0/1\">Hiroaki Yamane</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kurose_Y/0/1/0/all/0/1\">Yusuke Kurose</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Iho_J/0/1/0/all/0/1\">Junichi Iho</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tokunaga_Y/0/1/0/all/0/1\">Youji Tokunaga</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Horie_M/0/1/0/all/0/1\">Makoto Horie</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nishizawa_K/0/1/0/all/0/1\">Keisuke Nishizawa</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hayashi_Y/0/1/0/all/0/1\">Yusaku Hayashi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Koyama_Y/0/1/0/all/0/1\">Yasushi Koyama</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Harada_T/0/1/0/all/0/1\">Tatsuya Harada</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Low Can We Go? Pixel Annotation for Semantic Segmentation. (arXiv:2201.10448v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.10448","description":"<p>How many labeled pixels are needed to segment an image, without any prior\nknowledge? We conduct an experiment to answer this question.\n</p>\n<p>In our experiment, an Oracle is using Active Learning to train a network from\nscratch. The Oracle has access to the entire label map of the image, but the\ngoal is to reveal as little pixel labels to the network as possible. We find\nthat, on average, the Oracle needs to reveal (i.e., annotate) less than 0.1% of\nthe pixels in order to train a network. The network can then label all pixels\nin the image at an accuracy of more than 98%.\n</p>\n<p>Based on this single-image-annotation experiment, we design an experiment to\nquickly annotate an entire data set. In the data set level experiment the\nOracle trains a new network for each image from scratch. The network can then\nbe used to create pseudo-labels, which are the network predicted labels of the\nunlabeled pixels, for the entire image. Only then, a data set level network is\ntrained from scratch on all the pseudo-labeled images at once.\n</p>\n<p>We repeat both image level and data set level experiments on two, very\ndifferent, real-world data sets, and find that it is possible to reach the\nperformance of a fully annotated data set using a fraction of the annotation\ncost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kigli_D/0/1/0/all/0/1\">Daniel Kigli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shamir_A/0/1/0/all/0/1\">Ariel Shamir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avidan_S/0/1/0/all/0/1\">Shai Avidan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning-Based Framework for Camera Calibration with Distortion Correction and High Precision Feature Detection. (arXiv:2202.00158v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.00158","description":"<p>Camera calibration is a crucial technique which significantly influences the\nperformance of many robotic systems. Robustness and high precision have always\nbeen the pursuit of diverse calibration methods. State-of-the-art calibration\ntechniques based on classical Zhang's method, however, still suffer from\nenvironmental noise, radial lens distortion and sub-optimal parameter\nestimation. Therefore, in this paper, we propose a hybrid camera calibration\nframework which combines learning-based approaches with traditional methods to\nhandle these bottlenecks. In particular, this framework leverages\nlearning-based approaches to perform efficient distortion correction and robust\nchessboard corner coordinate encoding. For sub-pixel accuracy of corner\ndetection, a specially-designed coordinate decoding algorithm with embed\noutlier rejection mechanism is proposed. To avoid sub-optimal estimation\nresults, we improve the traditional parameter estimation by RANSAC algorithm\nand achieve stable results. Compared with two widely-used camera calibration\ntoolboxes, experiment results on both real and synthetic datasets manifest the\nbetter robustness and higher precision of the proposed framework. The massive\nsynthetic dataset is the basis of our framework's decent performance and will\nbe publicly available along with the code at\nhttps://github.com/Easonyesheng/CCS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yesheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_D/0/1/0/all/0/1\">Dahong Qian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fully Online Meta-Learning Without Task Boundaries. (arXiv:2202.00263v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2202.00263","description":"<p>While deep networks can learn complex functions such as classifiers,\ndetectors, and trackers, many applications require models that continually\nadapt to changing input distributions, changing tasks, and changing\nenvironmental conditions. Indeed, this ability to continuously accrue knowledge\nand use past experience to learn new tasks quickly in continual settings is one\nof the key properties of an intelligent system. For complex and\nhigh-dimensional problems, simply updating the model continually with standard\nlearning algorithms such as gradient descent may result in slow adaptation.\nMeta-learning can provide a powerful tool to accelerate adaptation yet is\nconventionally studied in batch settings. In this paper, we study how\nmeta-learning can be applied to tackle online problems of this nature,\nsimultaneously adapting to changing tasks and input distributions and\nmeta-training the model in order to adapt more quickly in the future. Extending\nmeta-learning into the online setting presents its own challenges, and although\nseveral prior methods have studied related problems, they generally require a\ndiscrete notion of tasks, with known ground-truth task boundaries. Such methods\ntypically adapt to each task in sequence, resetting the model between tasks,\nrather than adapting continuously across tasks. In many real-world settings,\nsuch discrete boundaries are unavailable, and may not even exist. To address\nthese settings, we propose a Fully Online Meta-Learning (FOML) algorithm, which\ndoes not require any ground truth knowledge about the task boundaries and stays\nfully online without resetting back to pre-trained weights. Our experiments\nshow that FOML was able to learn new tasks faster than the state-of-the-art\nonline learning methods on Rainbow-MNIST, CIFAR100 and CELEBA datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rajasegaran_J/0/1/0/all/0/1\">Jathushan Rajasegaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1\">Chelsea Finn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1\">Sergey Levine</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HRBF-Fusion: Accurate 3D reconstruction from RGB-D data using on-the-fly implicits. (arXiv:2202.01829v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.01829","description":"<p>Reconstruction of high-fidelity 3D objects or scenes is a fundamental\nresearch problem. Recent advances in RGB-D fusion have demonstrated the\npotential of producing 3D models from consumer-level RGB-D cameras. However,\ndue to the discrete nature and limited resolution of their surface\nrepresentations (e.g., point- or voxel-based), existing approaches suffer from\nthe accumulation of errors in camera tracking and distortion in the\nreconstruction, which leads to an unsatisfactory 3D reconstruction. In this\npaper, we present a method using on-the-fly implicits of Hermite Radial Basis\nFunctions (HRBFs) as a continuous surface representation for camera tracking in\nan existing RGB-D fusion framework. Furthermore, curvature estimation and\nconfidence evaluation are coherently derived from the inherent surface\nproperties of the on-the-fly HRBF implicits, which devote to a data fusion with\nbetter quality. We argue that our continuous but on-the-fly surface\nrepresentation can effectively mitigate the impact of noise with its robustness\nand constrain the reconstruction with inherent surface smoothness when being\ncompared with discrete representations. Experimental results on various\nreal-world and synthetic datasets demonstrate that our HRBF-fusion outperforms\nthe state-of-the-art approaches in terms of tracking robustness and\nreconstruction accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yabin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nan_L/0/1/0/all/0/1\">Liangliang Nan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Laishui Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Charlie C.L. Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D Object Detection from Images for Autonomous Driving: A Survey. (arXiv:2202.02980v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.02980","description":"<p>3D object detection from images, one of the fundamental and challenging\nproblems in autonomous driving, has received increasing attention from both\nindustry and academia in recent years. Benefiting from the rapid development of\ndeep learning technologies, image-based 3D detection has achieved remarkable\nprogress. Particularly, more than 200 works have studied this problem from 2015\nto 2021, encompassing a broad spectrum of theories, algorithms, and\napplications. However, to date no recent survey exists to collect and organize\nthis knowledge. In this paper, we fill this gap in the literature and provide\nthe first comprehensive survey of this novel and continuously growing research\nfield, summarizing the most commonly used pipelines for image-based 3D\ndetection and deeply analyzing each of their components. Additionally, we also\npropose two new taxonomies to organize the state-of-the-art methods into\ndifferent categories, with the intent of providing a more systematic review of\nexisting methods and facilitating fair comparisons with future works. In\nretrospect of what has been achieved so far, we also analyze the current\nchallenges in the field and discuss future directions for image-based 3D\ndetection research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xinzhu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1\">Wanli Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simonelli_A/0/1/0/all/0/1\">Andrea Simonelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ricci_E/0/1/0/all/0/1\">Elisa Ricci</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reasoning for Complex Data through Ensemble-based Self-Supervised Learning. (arXiv:2202.03126v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.03126","description":"<p>Self-supervised learning deals with problems that have little or no available\nlabeled data. Recent work has shown impressive results when underlying classes\nhave significant semantic differences. One important dataset in which this\ntechnique thrives is ImageNet, as intra-class distances are substantially lower\nthan inter-class distances. However, this is not the case for several critical\ntasks, and general self-supervised learning methods fail to learn\ndiscriminative features when classes have closer semantics, thus requiring more\nrobust strategies. We propose a strategy to tackle this problem, and to enable\nlearning from unlabeled data even when samples from different classes are not\nprominently diverse. We approach the problem by leveraging a novel\nensemble-based clustering strategy where clusters derived from different\nconfigurations are combined to generate a better grouping for the data samples\nin a fully-unsupervised way. This strategy allows clusters with different\ndensities and higher variability to emerge, which in turn reduces intra-class\ndiscrepancies, without requiring the burden of finding an optimal configuration\nper dataset. We also consider different Convolutional Neural Networks to\ncompute distances between samples. We refine these distances by performing\ncontext analysis and group them to capture complementary information. We\nconsider two applications to validate our pipeline: Person Re-Identification\nand Text Authorship Verification. These are challenging applications\nconsidering that classes are semantically close to each other and that training\nand test sets have disjoint identities. Our method is robust across different\nmodalities and outperforms state-of-the-art results with a fully-unsupervised\nsolution without any labeling or human intervention.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bertocco_G/0/1/0/all/0/1\">Gabriel Bertocco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theophilo_A/0/1/0/all/0/1\">Ant&#xf4;nio Theophilo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andalo_F/0/1/0/all/0/1\">Fernanda Andal&#xf3;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rocha_A/0/1/0/all/0/1\">Anderson Rocha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Phase-Stretch Adaptive Gradient-Field Extractor (PAGE). (arXiv:2202.03570v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2202.03570","description":"<p>Phase-Stretch Adaptive Gradient-Field Extractor (PAGE) is an edge detection\nalgorithm that is inspired by physics of electromagnetic diffraction and\ndispersion. A computational imaging algorithm, it identifies edges, their\norientations and sharpness in a digital image where the image brightness\nchanges abruptly. Edge detection is a basic operation performed by the eye and\nis crucial to visual perception. PAGE embeds an original image into a set of\nfeature maps that can be used for object representation and classification. The\nalgorithm performs exceptionally well as an edge and texture extractor in low\nlight level and low contrast images. This manuscript is prepared to support the\nopen-source code which is being simultaneously made available within the GitHub\nrepository\nhttps://github.com/JalaliLabUCLA/Phase-Stretch-Adaptive-Gradient-field-Extractor/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+MacPhee_C/0/1/0/all/0/1\">Callen MacPhee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Suthar_M/0/1/0/all/0/1\">Madhuri Suthar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jalali_B/0/1/0/all/0/1\">Bahram Jalali</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-level Contrastive Learning and Consistency Constraint for Semi-supervised Medical Image Segmentation. (arXiv:2202.04074v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2202.04074","description":"<p>Semi-supervised learning (SSL), which aims at leveraging a few labeled images\nand a large number of unlabeled images for network training, is beneficial for\nrelieving the burden of data annotation in medical image segmentation.\nAccording to the experience of medical imaging experts, local attributes such\nas texture, luster and smoothness are very important factors for identifying\ntarget objects like lesions and polyps in medical images. Motivated by this, we\npropose a cross-level contrastive learning scheme to enhance representation\ncapacity for local features in semi-supervised medical image segmentation.\nCompared to existing image-wise, patch-wise and point-wise contrastive learning\nalgorithms, our devised method is capable of exploring more complex similarity\ncues, namely the relational characteristics between global and local patch-wise\nrepresentations. Additionally, for fully making use of cross-level semantic\nrelations, we devise a novel consistency constraint that compares the\npredictions of patches against those of the full image. With the help of the\ncross-level contrastive learning and consistency constraint, the unlabelled\ndata can be effectively explored to improve segmentation performance on two\nmedical image datasets for polyp and skin lesion segmentation respectively.\nCode of our approach is available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhao_X/0/1/0/all/0/1\">Xinkai Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fang_C/0/1/0/all/0/1\">Chaowei Fang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fan_D/0/1/0/all/0/1\">De-Jun Fan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_X/0/1/0/all/0/1\">Xutao Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gao_F/0/1/0/all/0/1\">Feng Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_G/0/1/0/all/0/1\">Guanbin Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FILM: Frame Interpolation for Large Motion. (arXiv:2202.04901v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.04901","description":"<p>We present a frame interpolation algorithm that synthesizes multiple\nintermediate frames from two input images with large in-between motion. Recent\nmethods use multiple networks to estimate optical flow or depth and a separate\nnetwork dedicated to frame synthesis. This is often complex and requires scarce\noptical flow or depth ground-truth. In this work, we present a single unified\nnetwork, distinguished by a multi-scale feature extractor that shares weights\nat all scales, and is trainable from frames alone. To synthesize crisp and\npleasing frames, we propose to optimize our network with the Gram matrix loss\nthat measures the correlation difference between feature maps. Our approach\noutperforms state-of-the-art methods on the Xiph large motion benchmark. We\nalso achieve higher scores on Vimeo-90K, Middlebury and UCF101, when comparing\nto methods that use perceptual losses. We study the effect of weight sharing\nand of training with datasets of increasing motion range. Finally, we\ndemonstrate our model's effectiveness in synthesizing high quality and\ntemporally coherent videos on a challenging near-duplicate photos dataset.\nCodes and pre-trained models are available at\nhttps://github.com/google-research/frame-interpolation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Reda_F/0/1/0/all/0/1\">Fitsum Reda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kontkanen_J/0/1/0/all/0/1\">Janne Kontkanen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tabellion_E/0/1/0/all/0/1\">Eric Tabellion</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_D/0/1/0/all/0/1\">Deqing Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pantofaru_C/0/1/0/all/0/1\">Caroline Pantofaru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Curless_B/0/1/0/all/0/1\">Brian Curless</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OWL (Observe, Watch, Listen): Localizing Actions in Egocentric Video via Audiovisual Temporal Context. (arXiv:2202.04947v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.04947","description":"<p>Temporal action localization (TAL) is an important task extensively explored\nand improved for third-person videos in recent years. Recent efforts have been\nmade to perform fine-grained temporal localization on first-person videos.\nHowever, current TAL methods only use visual signals, neglecting the audio\nmodality that exists in most videos and that shows meaningful action\ninformation in egocentric videos. In this work, we take a deep look into the\neffectiveness of audio in detecting actions in egocentric videos and introduce\na simple-yet-effective approach via Observing, Watching, and Listening (OWL) to\nleverage audio-visual information and context for egocentric TAL. For doing\nthat, we: 1) compare and study different strategies for where and how to fuse\nthe two modalities; 2) propose a transformer-based model to incorporate\ntemporal audio-visual context. Our experiments show that our approach achieves\nstate-of-the-art performance on EPIC-KITCHENS-100.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ramazanova_M/0/1/0/all/0/1\">Merey Ramazanova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Escorcia_V/0/1/0/all/0/1\">Victor Escorcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heilbron_F/0/1/0/all/0/1\">Fabian Caba Heilbron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bernard Ghanem</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Equivariance Regularization for Image Reconstruction. (arXiv:2202.05062v2 [math.OC] UPDATED)","link":"http://arxiv.org/abs/2202.05062","description":"<p>In this work, we propose Regularization-by-Equivariance (REV), a novel\nstructure-adaptive regularization scheme for solving imaging inverse problems\nunder incomplete measurements. This regularization scheme utilizes the\nequivariant structure in the physics of the measurements -- which is prevalent\nin many inverse problems such as tomographic image reconstruction -- to\nmitigate the ill-poseness of the inverse problem. Our proposed scheme can be\napplied in a plug-and-play manner alongside with any classic first-order\noptimization algorithm such as the accelerated gradient descent/FISTA for\nsimplicity and fast convergence. The numerical experiments in sparse-view X-ray\nCT image reconstruction tasks demonstrate the effectiveness of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/math/1/au:+Tang_J/0/1/0/all/0/1\">Junqi Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Incremental Learning of Structured Memory via Closed-Loop Transcription. (arXiv:2202.05411v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.05411","description":"<p>This work proposes a minimal computational model for learning a structured\nmemory of multiple object classes in an incremental setting. Our approach is\nbased on establishing a closed-loop transcription between multiple classes and\ntheir corresponding subspaces, known as a linear discriminative representation,\nin a low-dimensional feature space. Our method is both simpler and more\nefficient than existing approaches to incremental learning, in terms of model\nsize, storage, and computation: it requires only a single, fixed-capacity\nautoencoding network with a feature space that is used for both discriminative\nand generative purposes. All network parameters are optimized simultaneously\nwithout architectural manipulations, by solving a constrained minimax game\nbetween the encoding and decoding maps over a single rate reduction-based\nobjective. Experimental results show that our method can effectively alleviate\ncatastrophic forgetting, achieving significantly better performance than prior\nwork for both generative and discriminative purposes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tong_S/0/1/0/all/0/1\">Shengbang Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1\">Xili Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Ziyang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mingyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_B/0/1/0/all/0/1\">Brent Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yi Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Weakly-Supervised Text Spotting using a Multi-Task Transformer. (arXiv:2202.05508v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.05508","description":"<p>Text spotting end-to-end methods have recently gained attention in the\nliterature due to the benefits of jointly optimizing the text detection and\nrecognition components. Existing methods usually have a distinct separation\nbetween the detection and recognition branches, requiring exact annotations for\nthe two tasks. We introduce TextTranSpotter (TTS), a transformer-based approach\nfor text spotting and the first text spotting framework which may be trained\nwith both fully- and weakly-supervised settings. By learning a single latent\nrepresentation per word detection, and using a novel loss function based on the\nHungarian loss, our method alleviates the need for expensive localization\nannotations. Trained with only text transcription annotations on real data, our\nweakly-supervised method achieves competitive performance with previous\nstate-of-the-art fully-supervised methods. When trained in a fully-supervised\nmanner, TextTranSpotter shows state-of-the-art results on multiple benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kittenplon_Y/0/1/0/all/0/1\">Yair Kittenplon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lavi_I/0/1/0/all/0/1\">Inbal Lavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fogel_S/0/1/0/all/0/1\">Sharon Fogel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bar_Y/0/1/0/all/0/1\">Yarin Bar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manmatha_R/0/1/0/all/0/1\">R. Manmatha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perona_P/0/1/0/all/0/1\">Pietro Perona</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video-driven Neural Physically-based Facial Asset for Production. (arXiv:2202.05592v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.05592","description":"<p>Production-level workflows for producing convincing 3D dynamic human faces\nhave long relied on a disarray of labor-intensive tools for geometry and\ntexture generation, motion capture and rigging, and expression synthesis.\nRecent neural approaches automate individual components but the corresponding\nlatent representations cannot provide artists explicit controls as in\nconventional tools. In this paper, we present a new learning-based,\nvideo-driven approach for generating dynamic facial geometries with\nhigh-quality physically-based assets. Two key components are well-structured\nlatent spaces due to dense temporal samplings from videos and explicit facial\nexpression controls to regulate the latent spaces. For data collection, we\nconstruct a hybrid multiview-photometric capture stage, coupling with an\nultra-fast video camera to obtain raw 3D facial assets. We then model the\nfacial expression, geometry and physically-based textures using separate VAEs\nwith a global MLP-based expression mapping across the latent spaces, to\npreserve characteristics across respective attributes while maintaining\nexplicit controls over geometry and texture. We also introduce to model the\ndelta information as wrinkle maps for physically-base textures, achieving\nhigh-quality rendering of dynamic textures. We demonstrate our approach in\nhigh-fidelity performer-specific facial capture and cross-identity facial\nmotion retargeting. In addition, our neural asset along with fast adaptation\nschemes can also be deployed to handle in-the-wild videos. Besides, we motivate\nthe utility of our explicit facial disentangle strategy by providing promising\nphysically-based editing results like geometry and material editing or winkle\ntransfer with high realism. Comprehensive experiments show that our technique\nprovides higher accuracy and visual fidelity than previous video-driven facial\nreconstruction and animation methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Longwen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_C/0/1/0/all/0/1\">Chuxiao Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qixuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hongyang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_R/0/1/0/all/0/1\">Ruixiang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jingyi Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vector Quantized Bayesian Neural Network Inference for Data Streams. (arXiv:1907.05911v3 [cs.LG] CROSS LISTED)","link":"http://arxiv.org/abs/1907.05911","description":"<p>Bayesian neural networks (BNN) can estimate the uncertainty in predictions,\nas opposed to non-Bayesian neural networks (NNs). However, BNNs have been far\nless widely used than non-Bayesian NNs in practice since they need iterative NN\nexecutions to predict a result for one data, and it gives rise to prohibitive\ncomputational cost. This computational burden is a critical problem when\nprocessing data streams with low-latency. To address this problem, we propose a\nnovel model VQ-BNN, which approximates BNN inference for data streams. In order\nto reduce the computational burden, VQ-BNN inference predicts NN only once and\ncompensates the result with previously memorized predictions. To be specific,\nVQ-BNN inference for data streams is given by temporal exponential smoothing of\nrecent predictions. The computational cost of this model is almost the same as\nthat of non-Bayesian NNs. Experiments including semantic segmentation on\nreal-world data show that this model performs significantly faster than BNNs\nwhile estimating predictive results comparable to or superior to the results of\nBNNs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_N/0/1/0/all/0/1\">Namuk Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_T/0/1/0/all/0/1\">Taekyu Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Songkuk Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Coded ResNeXt: a network for designing disentangled information paths. (arXiv:2202.05343v1 [cs.CV] CROSS LISTED)","link":"http://arxiv.org/abs/2202.05343","description":"<p>To avoid treating neural networks as highly complex black boxes, the deep\nlearning research community has tried to build interpretable models allowing\nhumans to understand the decisions taken by the model. Unfortunately, the focus\nis mostly on manipulating only the very high-level features associated with the\nlast layers. In this work, we look at neural network architectures for\nclassification in a more general way and introduce an algorithm which defines\nbefore the training the paths of the network through which the per-class\ninformation flows. We show that using our algorithm we can extract a lighter\nsingle-purpose binary classifier for a particular class by removing the\nparameters that do not participate in the predefined information path of that\nclass, which is approximately 60% of the total parameters. Notably, leveraging\ncoding theory to design the information paths enables us to use intermediate\nnetwork layers for making early predictions without having to evaluate the full\nnetwork. We demonstrate that a slightly modified ResNeXt model, trained with\nour algorithm, can achieve higher classification accuracy on CIFAR-10/100 and\nImageNet than the original ResNeXt, while having all the aforementioned\nproperties.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Avranas_A/0/1/0/all/0/1\">Apostolos Avranas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kountouris_M/0/1/0/all/0/1\">Marios Kountouris</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-02-14T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}