{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-07-25T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Active Data Pattern Extraction Attacks on Generative Language Models. (arXiv:2207.10802v1 [cs.CR])","link":"http://arxiv.org/abs/2207.10802","description":"<p>With the wide availability of large pre-trained language model checkpoints,\nsuch as GPT-2 and BERT, the recent trend has been to fine-tune them on a\ndownstream task to achieve the state-of-the-art performance with a small\ncomputation overhead. One natural example is the Smart Reply application where\na pre-trained model is fine-tuned for suggesting a number of responses given a\nquery message. In this work, we set out to investigate potential information\nleakage vulnerabilities in a typical Smart Reply pipeline and show that it is\npossible for an adversary, having black-box or gray-box access to a Smart Reply\nmodel, to extract sensitive user information present in the training data. We\nfurther analyse the privacy impact of specific components, e.g. the decoding\nstrategy, pertained to this application through our attack settings. We explore\npotential mitigation strategies and demonstrate how differential privacy can be\na strong defense mechanism to such data extraction attacks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jayaraman_B/0/1/0/all/0/1\">Bargav Jayaraman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_E/0/1/0/all/0/1\">Esha Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inan_H/0/1/0/all/0/1\">Huseyin Inan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chase_M/0/1/0/all/0/1\">Melissa Chase</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1\">Sambuddha Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1\">Wei Dai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ASR Error Detection via Audio-Transcript entailment. (arXiv:2207.10849v1 [cs.CL])","link":"http://arxiv.org/abs/2207.10849","description":"<p>Despite improved performances of the latest Automatic Speech Recognition\n(ASR) systems, transcription errors are still unavoidable. These errors can\nhave a considerable impact in critical domains such as healthcare, when used to\nhelp with clinical documentation. Therefore, detecting ASR errors is a critical\nfirst step in preventing further error propagation to downstream applications.\nTo this end, we propose a novel end-to-end approach for ASR error detection\nusing audio-transcript entailment. To the best of our knowledge, we are the\nfirst to frame this problem as an end-to-end entailment task between the audio\nsegment and its corresponding transcript segment. Our intuition is that there\nshould be a bidirectional entailment between audio and transcript when there is\nno recognition error and vice versa. The proposed model utilizes an acoustic\nencoder and a linguistic encoder to model the speech and transcript\nrespectively. The encoded representations of both modalities are fused to\npredict the entailment. Since doctor-patient conversations are used in our\nexperiments, a particular emphasis is placed on medical terms. Our proposed\nmodel achieves classification error rates (CER) of 26.2% on all transcription\nerrors and 23% on medical errors specifically, leading to improvements upon a\nstrong baseline by 12% and 15.4%, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meripo_N/0/1/0/all/0/1\">Nimshi Venkat Meripo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konam_S/0/1/0/all/0/1\">Sandeep Konam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Two-Stage Fine-Tuning: A Novel Strategy for Learning Class-Imbalanced Data. (arXiv:2207.10858v1 [cs.CL])","link":"http://arxiv.org/abs/2207.10858","description":"<p>Classification on long-tailed distributed data is a challenging problem,\nwhich suffers from serious class-imbalance and hence poor performance on tail\nclasses with only a few samples. Owing to this paucity of samples, learning on\nthe tail classes is especially challenging for the fine-tuning when\ntransferring a pretrained model to a downstream task. In this work, we present\na simple modification of standard fine-tuning to cope with these challenges.\nSpecifically, we propose a two-stage fine-tuning: we first fine-tune the final\nlayer of the pretrained model with class-balanced reweighting loss, and then we\nperform the standard fine-tuning. Our modification has several benefits: (1) it\nleverages pretrained representations by only fine-tuning a small portion of the\nmodel parameters while keeping the rest untouched; (2) it allows the model to\nlearn an initial representation of the specific task; and importantly (3) it\nprotects the learning of tail classes from being at a disadvantage during the\nmodel updating. We conduct extensive experiments on synthetic datasets of both\ntwo-class and multi-class tasks of text classification as well as a real-world\napplication to ADME (i.e., absorption, distribution, metabolism, and excretion)\nsemantic labeling. The experimental results show that the proposed two-stage\nfine-tuning outperforms both fine-tuning with conventional loss and fine-tuning\nwith a reweighting loss on the above datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+ValizadehAslani_T/0/1/0/all/0/1\">Taha ValizadehAslani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yiwen Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_P/0/1/0/all/0/1\">Ping Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_M/0/1/0/all/0/1\">Meng Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Liang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1\">Hualou Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Assessing mortality prediction through different representation models based on concepts extracted from clinical notes. (arXiv:2207.10872v1 [cs.CL])","link":"http://arxiv.org/abs/2207.10872","description":"<p>Recent years have seen particular interest in using electronic medical\nrecords (EMRs) for secondary purposes to enhance the quality and safety of\nhealthcare delivery. EMRs tend to contain large amounts of valuable clinical\nnotes. Learning of embedding is a method for converting notes into a format\nthat makes them comparable. Transformer-based representation models have\nrecently made a great leap forward. These models are pre-trained on large\nonline datasets to understand natural language texts effectively. The quality\nof a learning embedding is influenced by how clinical notes are used as input\nto representation models. A clinical note has several sections with different\nlevels of information value. It is also common for healthcare providers to use\ndifferent expressions for the same concept. Existing methods use clinical notes\ndirectly or with an initial preprocessing as input to representation models.\nHowever, to learn a good embedding, we identified the most essential clinical\nnotes section. We then mapped the extracted concepts from selected sections to\nthe standard names in the Unified Medical Language System (UMLS). We used the\nstandard phrases corresponding to the unique concepts as input for clinical\nmodels. We performed experiments to measure the usefulness of the learned\nembedding vectors in the task of hospital mortality prediction on a subset of\nthe publicly available Medical Information Mart for Intensive Care (MIMIC-III)\ndataset. According to the experiments, clinical transformer-based\nrepresentation models produced better results with getting input generated by\nstandard names of extracted unique concepts compared to other input formats.\nThe best-performing models were BioBERT, PubMedBERT, and UmlsBERT,\nrespectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Memarzadeh_H/0/1/0/all/0/1\">Hoda Memarzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghadiri_N/0/1/0/all/0/1\">Nasser Ghadiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahreza_M/0/1/0/all/0/1\">Maryam Lotfi Shahreza</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Level Fine-Tuning, Data Augmentation, and Few-Shot Learning for Specialized Cyber Threat Intelligence. (arXiv:2207.11076v1 [cs.CR])","link":"http://arxiv.org/abs/2207.11076","description":"<p>Gathering cyber threat intelligence from open sources is becoming\nincreasingly important for maintaining and achieving a high level of security\nas systems become larger and more complex. However, these open sources are\noften subject to information overload. It is therefore useful to apply machine\nlearning models that condense the amount of information to what is necessary.\nYet, previous studies and applications have shown that existing classifiers are\nnot able to extract specific information about emerging cybersecurity events\ndue to their low generalization ability. Therefore, we propose a system to\novercome this problem by training a new classifier for each new incident. Since\nthis requires a lot of labelled data using standard training methods, we\ncombine three different low-data regime techniques - transfer learning, data\naugmentation, and few-shot learning - to train a high-quality classifier from\nvery few labelled instances. We evaluated our approach using a novel dataset\nderived from the Microsoft Exchange Server data breach of 2021 which was\nlabelled by three experts. Our findings reveal an increase in F1 score of more\nthan 21 points compared to standard training methods and more than 18 points\ncompared to a state-of-the-art method in few-shot learning. Furthermore, the\nclassifier trained with this method and 32 instances is only less than 5 F1\nscore points worse than a classifier trained with 1800 instances.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bayer_M/0/1/0/all/0/1\">Markus Bayer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frey_T/0/1/0/all/0/1\">Tobias Frey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reuter_C/0/1/0/all/0/1\">Christian Reuter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lagrangian Method for Q-Function Learning (with Applications to Machine Translation). (arXiv:2207.11161v1 [cs.LG])","link":"http://arxiv.org/abs/2207.11161","description":"<p>This paper discusses a new approach to the fundamental problem of learning\noptimal Q-functions. In this approach, optimal Q-functions are formulated as\nsaddle points of a nonlinear Lagrangian function derived from the classic\nBellman optimality equation. The paper shows that the Lagrangian enjoys strong\nduality, in spite of its nonlinearity, which paves the way to a general\nLagrangian method to Q-function learning. As a demonstration, the paper\ndevelops an imitation learning algorithm based on the duality theory, and\napplies the algorithm to a state-of-the-art machine translation benchmark. The\npaper then turns to demonstrate a symmetry breaking phenomenon regarding the\noptimality of the Lagrangian saddle points, which justifies a largely\noverlooked direction in developing the Lagrangian method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bojun_H/0/1/0/all/0/1\">Huang Bojun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Target-Driven Structured Transformer Planner for Vision-Language Navigation. (arXiv:2207.11201v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11201","description":"<p>Vision-language navigation is the task of directing an embodied agent to\nnavigate in 3D scenes with natural language instructions. For the agent,\ninferring the long-term navigation target from visual-linguistic clues is\ncrucial for reliable path planning, which, however, has rarely been studied\nbefore in literature. In this article, we propose a Target-Driven Structured\nTransformer Planner (TD-STP) for long-horizon goal-guided and room layout-aware\nnavigation. Specifically, we devise an Imaginary Scene Tokenization mechanism\nfor explicit estimation of the long-term target (even located in unexplored\nenvironments). In addition, we design a Structured Transformer Planner which\nelegantly incorporates the explored room layout into a neural attention\narchitecture for structured and global planning. Experimental results\ndemonstrate that our TD-STP substantially improves previous best methods'\nsuccess rate by 2% and 5% on the test set of R2R and REVERIE benchmarks,\nrespectively. Our code is available at https://github.com/YushengZhao/TD-STP .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yusheng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jinyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Chen Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenguan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lirong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1\">Haibing Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_H/0/1/0/all/0/1\">Huaxia Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Si Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Twitmo: A Twitter Data Topic Modeling and Visualization Package for R. (arXiv:2207.11236v1 [cs.IR])","link":"http://arxiv.org/abs/2207.11236","description":"<p>We present Twitmo, a package that provides a broad range of methods to\ncollect, pre-process, analyze and visualize geo-tagged Twitter data. Twitmo\nenables the user to collect geo-tagged Tweets from Twitter and and provides a\ncomprehensive and user-friendly toolbox to generate topic distributions from\nLatent Dirichlet Allocations (LDA), correlated topic models (CTM) and\nstructural topic models (STM). Functions are included for pre-processing of\ntext, model building and prediction. In addition, one of the innovations of the\npackage is the automatic pooling of Tweets into longer pseudo-documents using\nhashtags and cosine similarities for better topic coherence. The package\nadditionally comes with functionality to visualize collected data sets and\nfitted models in static as well as interactive ways and offers built-in support\nfor model visualizations via LDAvis providing great convenience for researchers\nin this area. The Twitmo package is an innovative toolbox that can be used to\nanalyze public discourse of various topics, political parties or persons of\ninterest in space and time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Buchmuller_A/0/1/0/all/0/1\">Andreas Buchm&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kant_G/0/1/0/all/0/1\">Gillian Kant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weisser_C/0/1/0/all/0/1\">Christoph Weisser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Safken_B/0/1/0/all/0/1\">Benjamin S&#xe4;fken</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kis_Katos_K/0/1/0/all/0/1\">Krisztina Kis-Katos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kneib_T/0/1/0/all/0/1\">Thomas Kneib</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Panoptic Scene Graph Generation. (arXiv:2207.11247v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11247","description":"<p>Existing research addresses scene graph generation (SGG) -- a critical\ntechnology for scene understanding in images -- from a detection perspective,\ni.e., objects are detected using bounding boxes followed by prediction of their\npairwise relationships. We argue that such a paradigm causes several problems\nthat impede the progress of the field. For instance, bounding box-based labels\nin current datasets usually contain redundant classes like hairs, and leave out\nbackground information that is crucial to the understanding of context. In this\nwork, we introduce panoptic scene graph generation (PSG), a new problem task\nthat requires the model to generate a more comprehensive scene graph\nrepresentation based on panoptic segmentations rather than rigid bounding\nboxes. A high-quality PSG dataset, which contains 49k well-annotated\noverlapping images from COCO and Visual Genome, is created for the community to\nkeep track of its progress. For benchmarking, we build four two-stage\nbaselines, which are modified from classic methods in SGG, and two one-stage\nbaselines called PSGTR and PSGFormer, which are based on the efficient\nTransformer-based detector, i.e., DETR. While PSGTR uses a set of queries to\ndirectly learn triplets, PSGFormer separately models the objects and relations\nin the form of queries from two Transformer decoders, followed by a\nprompting-like relation-object matching mechanism. In the end, we share\ninsights on open challenges and future directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jingkang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ang_Y/0/1/0/all/0/1\">Yi Zhe Ang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zujin Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kaiyang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wayne Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transforming Wikipedia into Augmented Data for Query-Focused Summarization. (arXiv:1911.03324v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/1911.03324","description":"<p>The limited size of existing query-focused summarization datasets renders\ntraining data-driven summarization models challenging. Meanwhile, the manual\nconstruction of a query-focused summarization corpus is costly and\ntime-consuming. In this paper, we use Wikipedia to automatically collect a\nlarge query-focused summarization dataset (named WIKIREF) of more than 280, 000\nexamples, which can serve as a means of data augmentation. We also develop a\nBERT-based query-focused summarization model (Q-BERT) to extract sentences from\nthe documents as summaries. To better adapt a huge model containing millions of\nparameters to tiny benchmarks, we identify and fine-tune only a sparse\nsubnetwork, which corresponds to a small fraction of the whole model\nparameters. Experimental results on three DUC benchmarks show that the model\npre-trained on WIKIREF has already achieved reasonable performance. After\nfine-tuning on the specific benchmark datasets, the model with data\naugmentation outperforms strong comparison systems. Moreover, both our proposed\nQ-BERT model and subnetwork fine-tuning further improve the model performance.\nThe dataset is publicly available at https://aka.ms/wikiref.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Haichao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1\">Li Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1\">Bing Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Ting Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Socially Intelligent Agents with Mental State Transition and Human Utility. (arXiv:2103.07011v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.07011","description":"<p>Building a socially intelligent agent involves many challenges. One of which\nis to track the agent's mental state transition and teach the agent to make\ndecisions guided by its value like a human. Towards this end, we propose to\nincorporate mental state simulation and value modeling into dialogue agents.\nFirst, we build a hybrid mental state parser that extracts information from\nboth the dialogue and event observations and maintains a graphical\nrepresentation of the agent's mind; Meanwhile, the transformer-based value\nmodel learns human preferences from the human value dataset, ValueNet.\nEmpirical results show that the proposed model attains state-of-the-art\nperformance on the dialogue/action/emotion prediction task in the fantasy\ntext-adventure game dataset, LIGHT. We also show example cases to demonstrate:\n(i) how the proposed mental state parser can assist the agent's decision by\ngrounding on the context like locations and objects, and (ii) how the value\nmodel can help the agent make decisions based on its personal priorities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_L/0/1/0/all/0/1\">Liang Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yizhou Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yuan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1\">Pan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1\">Weiyan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Song-Chun Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data Augmentation in Natural Language Processing: A Novel Text Generation Approach for Long and Short Text Classifiers. (arXiv:2103.14453v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.14453","description":"<p>In many cases of machine learning, research suggests that the development of\ntraining data might have a higher relevance than the choice and modelling of\nclassifiers themselves. Thus, data augmentation methods have been developed to\nimprove classifiers by artificially created training data. In NLP, there is the\nchallenge of establishing universal rules for text transformations which\nprovide new linguistic patterns. In this paper, we present and evaluate a text\ngeneration method suitable to increase the performance of classifiers for long\nand short texts. We achieved promising improvements when evaluating short as\nwell as long text tasks with the enhancement by our text generation method.\nEspecially with regard to small data analytics, additive accuracy gains of up\nto 15.53% and 3.56% are achieved within a constructed low data regime, compared\nto the no augmentation baseline and another data augmentation technique. As the\ncurrent track of these constructed regimes is not universally applicable, we\nalso show major improvements in several real world low data tasks (up to +4.84\nF1-score). Since we are evaluating the method from many perspectives (in total\n11 datasets), we also observe situations where the method might not be\nsuitable. We discuss implications and patterns for the successful application\nof our approach on different types of datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bayer_M/0/1/0/all/0/1\">Markus Bayer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaufhold_M/0/1/0/all/0/1\">Marc-Andr&#xe9; Kaufhold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buchhold_B/0/1/0/all/0/1\">Bj&#xf6;rn Buchhold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keller_M/0/1/0/all/0/1\">Marcel Keller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dallmeyer_J/0/1/0/all/0/1\">J&#xf6;rg Dallmeyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reuter_C/0/1/0/all/0/1\">Christian Reuter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Code Structure Guided Transformer for Source Code Summarization. (arXiv:2104.09340v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.09340","description":"<p>Code summaries help developers comprehend programs and reduce their time to\ninfer the program functionalities during software maintenance. Recent efforts\nresort to deep learning techniques such as sequence-to-sequence models for\ngenerating accurate code summaries, among which Transformer-based approaches\nhave achieved promising performance. However, effectively integrating the code\nstructure information into the Transformer is under-explored in this task\ndomain. In this paper, we propose a novel approach named SG-Trans to\nincorporate code structural properties into Transformer. Specifically, we\ninject the local symbolic information (e.g., code tokens and statements) and\nglobal syntactic structure (e.g., data flow graph) into the self-attention\nmodule of Transformer as inductive bias. To further capture the hierarchical\ncharacteristics of code, the local information and global structure are\ndesigned to distribute in the attention heads of lower layers and high layers\nof Transformer. Extensive evaluation shows the superior performance of SG-Trans\nover the state-of-the-art approaches. Compared with the best-performing\nbaseline, SG-Trans still improves 1.4% and 2.0% in terms of METEOR score, a\nmetric widely used for measuring generation quality, respectively on two\nbenchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Shuzheng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Cuiyun Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yulan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_J/0/1/0/all/0/1\">Jichuan Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_L/0/1/0/all/0/1\">Lun Yiu Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_X/0/1/0/all/0/1\">Xin Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_M/0/1/0/all/0/1\">Michael R. Lyu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Data Augmentation for Text Classification. (arXiv:2107.03158v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.03158","description":"<p>Data augmentation, the artificial creation of training data for machine\nlearning by transformations, is a widely studied research field across machine\nlearning disciplines. While it is useful for increasing a model's\ngeneralization capabilities, it can also address many other challenges and\nproblems, from overcoming a limited amount of training data, to regularizing\nthe objective, to limiting the amount data used to protect privacy. Based on a\nprecise description of the goals and applications of data augmentation and a\ntaxonomy for existing works, this survey is concerned with data augmentation\nmethods for textual classification and aims to provide a concise and\ncomprehensive overview for researchers and practitioners. Derived from the\ntaxonomy, we divide more than 100 methods into 12 different groupings and give\nstate-of-the-art references expounding which methods are highly promising by\nrelating them to each other. Finally, research perspectives that may constitute\na building block for future work are provided.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bayer_M/0/1/0/all/0/1\">Markus Bayer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaufhold_M/0/1/0/all/0/1\">Marc-Andr&#xe9; Kaufhold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reuter_C/0/1/0/all/0/1\">Christian Reuter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attention-based Aspect Reasoning for Knowledge Base Question Answering on Clinical Notes. (arXiv:2108.00513v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.00513","description":"<p>Question Answering (QA) in clinical notes has gained a lot of attention in\nthe past few years. Existing machine reading comprehension approaches in\nclinical domain can only handle questions about a single block of clinical\ntexts and fail to retrieve information about multiple patients and their\nclinical notes. To handle more complex questions, we aim at creating knowledge\nbase from clinical notes to link different patients and clinical notes, and\nperforming knowledge base question answering (KBQA). Based on the expert\nannotations available in the n2c2 dataset, we first created the ClinicalKBQA\ndataset that includes around 9K QA pairs and covers questions about seven\nmedical topics using more than 300 question templates. Then, we investigated an\nattention-based aspect reasoning (AAR) method for KBQA and analyzed the impact\nof different aspects of answers (e.g., entity, type, path, and context) for\nprediction. The AAR method achieves better performance due to the well-designed\nencoder and attention mechanism. From our experiments, we find that both\naspects, type and path, enable the model to identify answers satisfying the\ngeneral conditions and produce lower precision and higher recall. On the other\nhand, the aspects, entity and context, limit the answers by node-specific\ninformation and lead to higher precision and lower recall.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Ping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_T/0/1/0/all/0/1\">Tian Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_K/0/1/0/all/0/1\">Khushbu Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choudhury_S/0/1/0/all/0/1\">Sutanay Choudhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_C/0/1/0/all/0/1\">Chandan K. Reddy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analyzing and Mitigating Interference in Neural Architecture Search. (arXiv:2108.12821v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.12821","description":"<p>Weight sharing is a popular approach to reduce the cost of neural\narchitecture search (NAS) by reusing the weights of shared operators from\npreviously trained child models. However, the rank correlation between the\nestimated accuracy and ground truth accuracy of those child models is low due\nto the interference among different child models caused by weight sharing. In\nthis paper, we investigate the interference issue by sampling different child\nmodels and calculating the gradient similarity of shared operators, and\nobserve: 1) the interference on a shared operator between two child models is\npositively correlated with the number of different operators; 2) the\ninterference is smaller when the inputs and outputs of the shared operator are\nmore similar. Inspired by these two observations, we propose two approaches to\nmitigate the interference: 1) MAGIC-T: rather than randomly sampling child\nmodels for optimization, we propose a gradual modification scheme by modifying\none operator between adjacent optimization steps to minimize the interference\non the shared operators; 2) MAGIC-A: forcing the inputs and outputs of the\noperator across all child models to be similar to reduce the interference.\nExperiments on a BERT search space verify that mitigating interference via each\nof our proposed methods improves the rank correlation of super-pet and\ncombining both methods can achieve better results. Our discovered architecture\noutperforms RoBERTa$_{\\rm base}$ by 1.1 and 0.6 points and ELECTRA$_{\\rm base}$\nby 1.6 and 1.1 points on the dev and test set of GLUE benchmark. Extensive\nresults on the BERT compression, reading comprehension and ImageNet task\ndemonstrate the effectiveness and generality of our proposed methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xu Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1\">Kaitao Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_R/0/1/0/all/0/1\">Renqian Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leng_Y/0/1/0/all/0/1\">Yichong Leng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jian Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BigSSL: Exploring the Frontier of Large-Scale Semi-Supervised Learning for Automatic Speech Recognition. (arXiv:2109.13226v3 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2109.13226","description":"<p>We summarize the results of a host of efforts using giant automatic speech\nrecognition (ASR) models pre-trained using large, diverse unlabeled datasets\ncontaining approximately a million hours of audio. We find that the combination\nof pre-training, self-training and scaling up model size greatly increases data\nefficiency, even for extremely large tasks with tens of thousands of hours of\nlabeled data. In particular, on an ASR task with 34k hours of labeled data, by\nfine-tuning an 8 billion parameter pre-trained Conformer model we can match\nstate-of-the-art (SoTA) performance with only 3% of the training data and\nsignificantly improve SoTA with the full training set. We also report on the\nuniversal benefits gained from using big pre-trained and self-trained models\nfor a large set of downstream tasks that cover a wide range of speech domains\nand span multiple orders of magnitudes of dataset sizes, including obtaining\nSoTA performance on many public benchmarks. In addition, we utilize the learned\nrepresentation of pre-trained networks to achieve SoTA results on non-ASR\ntasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Park_D/0/1/0/all/0/1\">Daniel S. Park</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Han_W/0/1/0/all/0/1\">Wei Han</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qin_J/0/1/0/all/0/1\">James Qin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gulati_A/0/1/0/all/0/1\">Anmol Gulati</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shor_J/0/1/0/all/0/1\">Joel Shor</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jansen_A/0/1/0/all/0/1\">Aren Jansen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Y/0/1/0/all/0/1\">Yuanzhong Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1\">Yanping Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1\">Shibo Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_Z/0/1/0/all/0/1\">Zongwei Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ma_M/0/1/0/all/0/1\">Min Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chan_W/0/1/0/all/0/1\">William Chan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_J/0/1/0/all/0/1\">Jiahui Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yongqiang Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cao_L/0/1/0/all/0/1\">Liangliang Cao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sim_K/0/1/0/all/0/1\">Khe Chai Sim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ramabhadran_B/0/1/0/all/0/1\">Bhuvana Ramabhadran</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sainath_T/0/1/0/all/0/1\">Tara N. Sainath</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Beaufays_F/0/1/0/all/0/1\">Fran&#xe7;oise Beaufays</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Z/0/1/0/all/0/1\">Zhifeng Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Le_Q/0/1/0/all/0/1\">Quoc V. Le</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chiu_C/0/1/0/all/0/1\">Chung-Cheng Chiu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pang_R/0/1/0/all/0/1\">Ruoming Pang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_Y/0/1/0/all/0/1\">Yonghui Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NLP From Scratch Without Large-Scale Pretraining: A Simple and Efficient Framework. (arXiv:2111.04130v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.04130","description":"<p>Pretrained language models have become the standard approach for many NLP\ntasks due to strong performance, but they are very expensive to train. We\npropose a simple and efficient learning framework, TLM, that does not rely on\nlarge-scale pretraining. Given some labeled task data and a large general\ncorpus, TLM uses task data as queries to retrieve a tiny subset of the general\ncorpus and jointly optimizes the task objective and the language modeling\nobjective from scratch. On eight classification datasets in four domains, TLM\nachieves results better than or similar to pretrained language models (e.g.,\nRoBERTa-Large) while reducing the training FLOPs by two orders of magnitude.\nWith high accuracy and efficiency, we hope TLM will contribute to democratizing\nNLP and expediting its development.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_X/0/1/0/all/0/1\">Xingcheng Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yanan Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaocong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhilin Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prompt Tuning GPT-2 language model for parameter-efficient domain adaptation of ASR systems. (arXiv:2112.08718v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.08718","description":"<p>Automatic Speech Recognition (ASR) systems have found their use in numerous\nindustrial applications in very diverse domains creating a need to adapt to new\ndomains with small memory and deployment overhead. In this work, we introduce\ndomain-prompts, a methodology that involves training a small number of domain\nembedding parameters to prime a Transformer-based Language Model (LM) to a\nparticular domain. Using this domain-adapted LM for rescoring ASR hypotheses\ncan achieve 7-13% WER reduction for a new domain with just 1000 unlabeled\ntextual domain-specific sentences. This improvement is comparable or even\nbetter than fully fine-tuned models even though just 0.02% of the parameters of\nthe base LM are updated. Additionally, our method is deployment-friendly as the\nlearnt domain embeddings are prefixed to the input to the model rather than\nchanging the base model architecture. Therefore, our method is an ideal choice\nfor on-the-fly adaptation of LMs used in ASR systems to progressively scale it\nto new domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dingliwal_S/0/1/0/all/0/1\">Saket Dingliwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shenoy_A/0/1/0/all/0/1\">Ashish Shenoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bodapati_S/0/1/0/all/0/1\">Sravan Bodapati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gandhe_A/0/1/0/all/0/1\">Ankur Gandhe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gadde_R/0/1/0/all/0/1\">Ravi Teja Gadde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirchhoff_K/0/1/0/all/0/1\">Katrin Kirchhoff</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Optimization of a Real-Time Wavelet-Based Algorithm for Improving Speech Intelligibility. (arXiv:2202.02545v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2202.02545","description":"<p>The optimization of a wavelet-based algorithm to improve speech\nintelligibility along with the full data set and results are reported. The\ndiscrete-time speech signal is split into frequency sub-bands via a multi-level\ndiscrete wavelet transform. Various gains are applied to the sub-band signals\nbefore they are recombined to form a modified version of the speech. The\nsub-band gains are adjusted while keeping the overall signal energy unchanged,\nand the speech intelligibility under various background interference and\nsimulated hearing loss conditions is enhanced and evaluated objectively and\nquantitatively using Google Speech-to-Text transcription. A universal set of\nsub-band gains can work over a range of noise-to-signal ratios up to 4.8 dB.\nFor noise-free speech, overall intelligibility is improved, and the Google\ntranscription accuracy is increased by 16.9 percentage points on average and\n86.7 maximum by reallocating the spectral energy toward the mid-frequency\nsub-bands. For speech already corrupted by noise, improving intelligibility is\nchallenging but still realizable with an increased transcription accuracy of\n9.5 percentage points on average and 71.4 maximum. The proposed algorithm is\nimplementable for real-time speech processing and comparatively simpler than\nprevious algorithms. Potential applications include speech enhancement, hearing\naids, machine listening, and a better understanding of speech intelligibility.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kang_T/0/1/0/all/0/1\">Tianqu Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dinh_A/0/1/0/all/0/1\">Anh-Dung Dinh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Binghong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_T/0/1/0/all/0/1\">Tianyuan Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yijia Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chau_K/0/1/0/all/0/1\">Kevin Chau</a> (Hong Kong University of Science and Technology)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"It Takes Two Flints to Make a Fire: Multitask Learning of Neural Relation and Explanation Classifiers. (arXiv:2204.11424v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.11424","description":"<p>We propose an explainable approach for relation extraction that mitigates the\ntension between generalization and explainability by jointly training for the\ntwo goals. Our approach uses a multi-task learning architecture, which jointly\ntrains a classifier for relation extraction, and a sequence model that labels\nwords in the context of the relation that explain the decisions of the relation\nclassifier. We also convert the model outputs to rules to bring global\nexplanations to this approach. This sequence model is trained using a hybrid\nstrategy: supervised, when supervision from pre-existing patterns is available,\nand semi-supervised otherwise. In the latter situation, we treat the sequence\nmodel's labels as latent variables, and learn the best assignment that\nmaximizes the performance of the relation classifier. We evaluate the proposed\napproach on the two datasets and show that the sequence model provides labels\nthat serve as accurate explanations for the relation classifier's decisions,\nand, importantly, that the joint training generally improves the performance of\nthe relation classifier. We also evaluate the performance of the generated\nrules and show that the new rules are great add-on to the manual rules and\nbring the rule-based system much closer to the neural models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1\">Zheng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Surdeanu_M/0/1/0/all/0/1\">Mihai Surdeanu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TEMOS: Generating diverse human motions from textual descriptions. (arXiv:2204.14109v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.14109","description":"<p>We address the problem of generating diverse 3D human motions from textual\ndescriptions. This challenging task requires joint modeling of both modalities:\nunderstanding and extracting useful human-centric information from the text,\nand then generating plausible and realistic sequences of human poses. In\ncontrast to most previous work which focuses on generating a single,\ndeterministic, motion from a textual description, we design a variational\napproach that can produce multiple diverse human motions. We propose TEMOS, a\ntext-conditioned generative model leveraging variational autoencoder (VAE)\ntraining with human motion data, in combination with a text encoder that\nproduces distribution parameters compatible with the VAE latent space. We show\nthe TEMOS framework can produce both skeleton-based animations as in prior\nwork, as well more expressive SMPL body motions. We evaluate our approach on\nthe KIT Motion-Language benchmark and, despite being relatively\nstraightforward, demonstrate significant improvements over the state of the\nart. Code and models are available on our webpage.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Petrovich_M/0/1/0/all/0/1\">Mathis Petrovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Black_M/0/1/0/all/0/1\">Michael J. Black</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varol_G/0/1/0/all/0/1\">G&#xfc;l Varol</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Dialogue Representations from Consecutive Utterances. (arXiv:2205.13568v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.13568","description":"<p>Learning high-quality dialogue representations is essential for solving a\nvariety of dialogue-oriented tasks, especially considering that dialogue\nsystems often suffer from data scarcity. In this paper, we introduce Dialogue\nSentence Embedding (DSE), a self-supervised contrastive learning method that\nlearns effective dialogue representations suitable for a wide range of dialogue\ntasks. DSE learns from dialogues by taking consecutive utterances of the same\ndialogue as positive pairs for contrastive learning. Despite its simplicity,\nDSE achieves significantly better representation capability than other dialogue\nrepresentation and universal sentence representation models. We evaluate DSE on\nfive downstream dialogue tasks that examine dialogue representation at\ndifferent semantic granularities. Experiments in few-shot and zero-shot\nsettings show that DSE outperforms baselines by a large margin. For example, it\nachieves 13% average performance improvement over the strongest unsupervised\nbaseline in 1-shot intent classification on 6 datasets. We also provide\nanalyses on the benefits and limitations of our model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhihan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dejiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_W/0/1/0/all/0/1\">Wei Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dingwall_N/0/1/0/all/0/1\">Nicholas Dingwall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaofei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arnold_A/0/1/0/all/0/1\">Andrew O. Arnold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_B/0/1/0/all/0/1\">Bing Xiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-LexSum: Real-World Summaries of Civil Rights Lawsuits at Multiple Granularities. (arXiv:2206.10883v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.10883","description":"<p>With the advent of large language models, methods for abstractive\nsummarization have made great strides, creating potential for use in\napplications to aid knowledge workers processing unwieldy document collections.\nOne such setting is the Civil Rights Litigation Clearinghouse (CRLC)\n(https://clearinghouse.net),which posts information about large-scale civil\nrights lawsuits, serving lawyers, scholars, and the general public. Today,\nsummarization in the CRLC requires extensive training of lawyers and law\nstudents who spend hours per case understanding multiple relevant documents in\norder to produce high-quality summaries of key events and outcomes. Motivated\nby this ongoing real-world summarization effort, we introduce Multi-LexSum, a\ncollection of 9,280 expert-authored summaries drawn from ongoing CRLC writing.\nMulti-LexSum presents a challenging multi-document summarization task given the\nlength of the source documents, often exceeding two hundred pages per case.\nFurthermore, Multi-LexSum is distinct from other datasets in its multiple\ntarget summaries, each at a different granularity (ranging from one-sentence\n\"extreme\" summaries to multi-paragraph narrations of over five hundred words).\nWe present extensive analysis demonstrating that despite the high-quality\nsummaries in the training data (adhering to strict content and style\nguidelines), state-of-the-art summarization models perform poorly on this task.\nWe release Multi-LexSum for further research in summarization methods as well\nas to facilitate development of applications to assist in the CRLC's mission at\nhttps://multilexsum.github.io.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zejiang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_K/0/1/0/all/0/1\">Kyle Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Lauren Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dahlberg_N/0/1/0/all/0/1\">Nathan Dahlberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schlanger_M/0/1/0/all/0/1\">Margo Schlanger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Downey_D/0/1/0/all/0/1\">Doug Downey</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Democratizing Ethical Assessment of Natural Language Generation Models. (arXiv:2207.10576v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2207.10576","description":"<p>Natural language generation models are computer systems that generate\ncoherent language when prompted with a sequence of words as context. Despite\ntheir ubiquity and many beneficial applications, language generation models\nalso have the potential to inflict social harms by generating discriminatory\nlanguage, hateful speech, profane content, and other harmful material. Ethical\nassessment of these models is therefore critical. But it is also a challenging\ntask, requiring an expertise in several specialized domains, such as\ncomputational linguistics and social justice. While significant strides have\nbeen made by the research community in this domain, accessibility of such\nethical assessments to the wider population is limited due to the high entry\nbarriers. This article introduces a new tool to democratize and standardize\nethical assessment of natural language generation models: Tool for Ethical\nAssessment of Language generation models (TEAL), a component of Credo AI Lens,\nan open-source assessment framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rasekh_A/0/1/0/all/0/1\">Amin Rasekh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eisenberg_I/0/1/0/all/0/1\">Ian Eisenberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"STOP: A dataset for Spoken Task Oriented Semantic Parsing. (arXiv:2207.10643v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2207.10643","description":"<p>End-to-end spoken language understanding (SLU) predicts intent directly from\naudio using a single model. It promises to improve the performance of assistant\nsystems by leveraging acoustic information lost in the intermediate textual\nrepresentation and preventing cascading errors from Automatic Speech\nRecognition (ASR). Further, having one unified model has efficiency advantages\nwhen deploying assistant systems on-device. However, the limited number of\npublic audio datasets with semantic parse labels hinders the research progress\nin this area. In this paper, we release the Spoken Task-Oriented semantic\nParsing (STOP) dataset, the largest and most complex SLU dataset to be publicly\navailable. Additionally, we define low-resource splits to establish a benchmark\nfor improving SLU when limited labeled data is available. Furthermore, in\naddition to the human-recorded audio, we are releasing a TTS-generated version\nto benchmark the performance for low-resource domain adaptation of end-to-end\nSLU systems. Initial experimentation show end-to-end SLU models performing\nslightly worse than their cascaded counterparts, which we hope encourages\nfuture work in this direction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tomasello_P/0/1/0/all/0/1\">Paden Tomasello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1\">Akshat Shrivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lazar_D/0/1/0/all/0/1\">Daniel Lazar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_P/0/1/0/all/0/1\">Po-Chun Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_D/0/1/0/all/0/1\">Duc Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sagar_A/0/1/0/all/0/1\">Adithya Sagar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elkahky_A/0/1/0/all/0/1\">Ali Elkahky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Copet_J/0/1/0/all/0/1\">Jade Copet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Wei-Ning Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mordechay_Y/0/1/0/all/0/1\">Yossef Mordechay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Algayres_R/0/1/0/all/0/1\">Robin Algayres</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tu Ahn Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dupoux_E/0/1/0/all/0/1\">Emmanuel Dupoux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1\">Abdelrahman Mohamed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-07-24T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"R2P: A Deep Learning Model from mmWave Radar to Point Cloud. (arXiv:2207.10690v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10690","description":"<p>Recent research has shown the effectiveness of mmWave radar sensing for\nobject detection in low visibility environments, which makes it an ideal\ntechnique in autonomous navigation systems. In this paper, we introduce Radar\nto Point Cloud (R2P), a deep learning model that generates smooth, dense, and\nhighly accurate point cloud representation of a 3D object with fine geometry\ndetails, based on rough and sparse point clouds with incorrect points obtained\nfrom mmWave radar. These input point clouds are converted from the 2D depth\nimages that are generated from raw mmWave radar sensor data, characterized by\ninconsistency, and orientation and shape errors. R2P utilizes an architecture\nof two sequential deep learning encoder-decoder blocks to extract the essential\nfeatures of those radar-based input point clouds of an object when observed\nfrom multiple viewpoints, and to ensure the internal consistency of a generated\noutput point cloud and its accurate and detailed shape reconstruction of the\noriginal object. We implement R2P to replace Stage 2 of our recently proposed\n3DRIMR (3D Reconstruction and Imaging via mmWave Radar) system. Our experiments\ndemonstrate the significant performance improvement of R2P over the popular\nexisting methods such as PointNet, PCN, and the original 3DRIMR design.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yue Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Honggang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhuoming Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Benyuan Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Synthetic Dataset Generation for Adversarial Machine Learning Research. (arXiv:2207.10719v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10719","description":"<p>Existing adversarial example research focuses on digitally inserted\nperturbations on top of existing natural image datasets. This construction of\nadversarial examples is not realistic because it may be difficult, or even\nimpossible, for an attacker to deploy such an attack in the real-world due to\nsensing and environmental effects. To better understand adversarial examples\nagainst cyber-physical systems, we propose approximating the real-world through\nsimulation. In this paper we describe our synthetic dataset generation tool\nthat enables scalable collection of such a synthetic dataset with realistic\nadversarial examples. We use the CARLA simulator to collect such a dataset and\ndemonstrate simulated attacks that undergo the same environmental transforms\nand processing as real-world images. Our tools have been used to collect\ndatasets to help evaluate the efficacy of adversarial examples, and can be\nfound at https://github.com/carla-simulator/carla/pull/4992.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiruo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Shibani Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cornelius_C/0/1/0/all/0/1\">Cory Cornelius</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Busho_C/0/1/0/all/0/1\">Colin Busho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1\">Mike Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paul_A/0/1/0/all/0/1\">Anindya Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_J/0/1/0/all/0/1\">Jason Martin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fusing Frame and Event Vision for High-speed Optical Flow for Edge Application. (arXiv:2207.10720v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10720","description":"<p>Optical flow computation with frame-based cameras provides high accuracy but\nthe speed is limited either by the model size of the algorithm or by the frame\nrate of the camera. This makes it inadequate for high-speed applications. Event\ncameras provide continuous asynchronous event streams overcoming the frame-rate\nlimitation. However, the algorithms for processing the data either borrow frame\nlike setup limiting the speed or suffer from lower accuracy. We fuse the\ncomplementary accuracy and speed advantages of the frame and event-based\npipelines to provide high-speed optical flow while maintaining a low error\nrate. Our bio-mimetic network is validated with the MVSEC dataset showing 19%\nerror degradation at 4x speed up. We then demonstrate the system with a\nhigh-speed drone flight scenario where a high-speed event camera computes the\nflow even before the optical camera sees the drone making it suited for\napplications like tracking and segmentation. This work shows the fundamental\ntrade-offs in frame-based processing may be overcome by fusing data from other\nmodalities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lele_A/0/1/0/all/0/1\">Ashwin Sanjay Lele</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raychowdhury_A/0/1/0/all/0/1\">Arijit Raychowdhury</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Irrelevant Pixels are Everywhere: Find and Exclude Them for More Efficient Computer Vision. (arXiv:2207.10741v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10741","description":"<p>Computer vision is often performed using Convolutional Neural Networks\n(CNNs). CNNs are compute-intensive and challenging to deploy on\npower-contrained systems such as mobile and Internet-of-Things (IoT) devices.\nCNNs are compute-intensive because they indiscriminately compute many features\non all pixels of the input image. We observe that, given a computer vision\ntask, images often contain pixels that are irrelevant to the task. For example,\nif the task is looking for cars, pixels in the sky are not very useful.\nTherefore, we propose that a CNN be modified to only operate on relevant pixels\nto save computation and energy. We propose a method to study three popular\ncomputer vision datasets, finding that 48% of pixels are irrelevant. We also\npropose the focused convolution to modify a CNN's convolutional layers to\nreject the pixels that are marked irrelevant. On an embedded device, we observe\nno loss in accuracy, while inference latency, energy consumption, and\nmultiply-add count are all reduced by about 45%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tung_C/0/1/0/all/0/1\">Caleb Tung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goel_A/0/1/0/all/0/1\">Abhinav Goel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiao Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eliopoulos_N/0/1/0/all/0/1\">Nicholas Eliopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amobi_E/0/1/0/all/0/1\">Emmanuel Amobi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thiruvathukal_G/0/1/0/all/0/1\">George K. Thiruvathukal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhary_V/0/1/0/all/0/1\">Vipin Chaudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yung-Hsiang Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DEVIANT: Depth EquiVarIAnt NeTwork for Monocular 3D Object Detection. (arXiv:2207.10758v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10758","description":"<p>Modern neural networks use building blocks such as convolutions that are\nequivariant to arbitrary 2D translations. However, these vanilla blocks are not\nequivariant to arbitrary 3D translations in the projective manifold. Even then,\nall monocular 3D detectors use vanilla blocks to obtain the 3D coordinates, a\ntask for which the vanilla blocks are not designed for. This paper takes the\nfirst step towards convolutions equivariant to arbitrary 3D translations in the\nprojective manifold. Since the depth is the hardest to estimate for monocular\ndetection, this paper proposes Depth EquiVarIAnt NeTwork (DEVIANT) built with\nexisting scale equivariant steerable blocks. As a result, DEVIANT is\nequivariant to the depth translations in the projective manifold whereas\nvanilla networks are not. The additional depth equivariance forces the DEVIANT\nto learn consistent depth estimates, and therefore, DEVIANT achieves\nstate-of-the-art monocular 3D detection results on KITTI and Waymo datasets in\nthe image-only category and performs competitively to methods using extra\ninformation. Moreover, DEVIANT works better than vanilla networks in\ncross-dataset evaluation. Code and models at\nhttps://github.com/abhi1kumar/DEVIANT\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Abhinav Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brazil_G/0/1/0/all/0/1\">Garrick Brazil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corona_E/0/1/0/all/0/1\">Enrique Corona</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parchami_A/0/1/0/all/0/1\">Armin Parchami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaoming Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TIDEE: Tidying Up Novel Rooms using Visuo-Semantic Commonsense Priors. (arXiv:2207.10761v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10761","description":"<p>We introduce TIDEE, an embodied agent that tidies up a disordered scene based\non learned commonsense object placement and room arrangement priors. TIDEE\nexplores a home environment, detects objects that are out of their natural\nplace, infers plausible object contexts for them, localizes such contexts in\nthe current scene, and repositions the objects. Commonsense priors are encoded\nin three modules: i) visuo-semantic detectors that detect out-of-place objects,\nii) an associative neural graph memory of objects and spatial relations that\nproposes plausible semantic receptacles and surfaces for object repositions,\nand iii) a visual search network that guides the agent's exploration for\nefficiently localizing the receptacle-of-interest in the current scene to\nreposition the object. We test TIDEE on tidying up disorganized scenes in the\nAI2THOR simulation environment. TIDEE carries out the task directly from pixel\nand raw depth input without ever having observed the same room beforehand,\nrelying only on priors learned from a separate set of training houses. Human\nevaluations on the resulting room reorganizations show TIDEE outperforms\nablative versions of the model that do not use one or more of the commonsense\npriors. On a related room rearrangement benchmark that allows the agent to view\nthe goal state prior to rearrangement, a simplified version of our model\nsignificantly outperforms a top-performing method by a large margin. Code and\ndata are available at the project website: https://tidee-agent.github.io/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sarch_G/0/1/0/all/0/1\">Gabriel Sarch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1\">Zhaoyuan Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harley_A/0/1/0/all/0/1\">Adam W. Harley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schydlo_P/0/1/0/all/0/1\">Paul Schydlo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tarr_M/0/1/0/all/0/1\">Michael J. Tarr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1\">Saurabh Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fragkiadaki_K/0/1/0/all/0/1\">Katerina Fragkiadaki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MeshLoc: Mesh-Based Visual Localization. (arXiv:2207.10762v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10762","description":"<p>Visual localization, i.e., the problem of camera pose estimation, is a\ncentral component of applications such as autonomous robots and augmented\nreality systems. A dominant approach in the literature, shown to scale to large\nscenes and to handle complex illumination and seasonal changes, is based on\nlocal features extracted from images. The scene representation is a sparse\nStructure-from-Motion point cloud that is tied to a specific local feature.\nSwitching to another feature type requires an expensive feature matching step\nbetween the database images used to construct the point cloud. In this work, we\nthus explore a more flexible alternative based on dense 3D meshes that does not\nrequire features matching between database images to build the scene\nrepresentation. We show that this approach can achieve state-of-the-art\nresults. We further show that surprisingly competitive results can be obtained\nwhen extracting features on renderings of these meshes, without any neural\nrendering stage, and even when rendering raw scene geometry without color or\ntexture. Our results show that dense 3D model-based representations are a\npromising alternative to existing representations and point to interesting and\nchallenging directions for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Panek_V/0/1/0/all/0/1\">Vojtech Panek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kukelova_Z/0/1/0/all/0/1\">Zuzana Kukelova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sattler_T/0/1/0/all/0/1\">Torsten Sattler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Interpretable Video Super-Resolution via Alternating Optimization. (arXiv:2207.10765v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10765","description":"<p>In this paper, we study a practical space-time video super-resolution (STVSR)\nproblem which aims at generating a high-framerate high-resolution sharp video\nfrom a low-framerate low-resolution blurry video. Such problem often occurs\nwhen recording a fast dynamic event with a low-framerate and low-resolution\ncamera, and the captured video would suffer from three typical issues: i)\nmotion blur occurs due to object/camera motions during exposure time; ii)\nmotion aliasing is unavoidable when the event temporal frequency exceeds the\nNyquist limit of temporal sampling; iii) high-frequency details are lost\nbecause of the low spatial sampling rate. These issues can be alleviated by a\ncascade of three separate sub-tasks, including video deblurring, frame\ninterpolation, and super-resolution, which, however, would fail to capture the\nspatial and temporal correlations among video sequences. To address this, we\npropose an interpretable STVSR framework by leveraging both model-based and\nlearning-based methods. Specifically, we formulate STVSR as a joint video\ndeblurring, frame interpolation, and super-resolution problem, and solve it as\ntwo sub-problems in an alternate way. For the first sub-problem, we derive an\ninterpretable analytical solution and use it as a Fourier data transform layer.\nThen, we propose a recurrent video enhancement layer for the second sub-problem\nto further recover high-frequency details. Extensive experiments demonstrate\nthe superiority of our method in terms of quantitative metrics and visual\nquality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Jiezhang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jingyun Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenguan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yulun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Focused Decoding Enables 3D Anatomical Detection by Transformers. (arXiv:2207.10774v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10774","description":"<p>Detection Transformers represent end-to-end object detection approaches based\non a Transformer encoder-decoder architecture, exploiting the attention\nmechanism for global relation modeling. Although Detection Transformers deliver\nresults on par with or even superior to their highly optimized CNN-based\ncounterparts operating on 2D natural images, their success is closely coupled\nto access to a vast amount of training data. This, however, restricts the\nfeasibility of employing Detection Transformers in the medical domain, as\naccess to annotated data is typically limited. To tackle this issue and\nfacilitate the advent of medical Detection Transformers, we propose a novel\nDetection Transformer for 3D anatomical structure detection, dubbed Focused\nDecoder. Focused Decoder leverages information from an anatomical region atlas\nto simultaneously deploy query anchors and restrict the cross-attention's field\nof view to regions of interest, which allows for a precise focus on relevant\nanatomical structures. We evaluate our proposed approach on two publicly\navailable CT datasets and demonstrate that Focused Decoder not only provides\nstrong detection results and thus alleviates the need for a vast amount of\nannotated data but also exhibits exceptional and highly intuitive\nexplainability of results via attention weights. Code for Focused Decoder is\navailable in our medical Vision Transformer library\ngithub.com/bwittmann/transoar.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wittmann_B/0/1/0/all/0/1\">Bastian Wittmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navarro_F/0/1/0/all/0/1\">Fernando Navarro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shit_S/0/1/0/all/0/1\">Suprosanna Shit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menze_B/0/1/0/all/0/1\">Bjoern Menze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Auto-regressive Image Synthesis with Integrated Quantization. (arXiv:2207.10776v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10776","description":"<p>Deep generative models have achieved conspicuous progress in realistic image\nsynthesis with multifarious conditional inputs, while generating diverse yet\nhigh-fidelity images remains a grand challenge in conditional image generation.\nThis paper presents a versatile framework for conditional image generation\nwhich incorporates the inductive bias of CNNs and powerful sequence modeling of\nauto-regression that naturally leads to diverse image generation. Instead of\nindependently quantizing the features of multiple domains as in prior research,\nwe design an integrated quantization scheme with a variational regularizer that\nmingles the feature discretization in multiple domains, and markedly boosts the\nauto-regressive modeling performance. Notably, the variational regularizer\nenables to regularize feature distributions in incomparable latent spaces by\npenalizing the intra-domain variations of distributions. In addition, we design\na Gumbel sampling strategy that allows to incorporate distribution uncertainty\ninto the auto-regressive training procedure. The Gumbel sampling substantially\nmitigates the exposure bias that often incurs misalignment between the training\nand inference stages and severely impairs the inference performance. Extensive\nexperiments over multiple conditional image generation tasks show that our\nmethod achieves superior diverse image generation performance qualitatively and\nquantitatively as compared with the state-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhan_F/0/1/0/all/0/1\">Fangneng Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yingchen Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1\">Rongliang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiahui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_K/0/1/0/all/0/1\">Kaiwen Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Changgong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shijian Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An advanced combination of semi-supervised Normalizing Flow & Yolo (YoloNF) to detect and recognize vehicle license plates. (arXiv:2207.10777v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10777","description":"<p>Fully Automatic License Plate Recognition (ALPR) has been a frequent research\ntopic due to several practical applications. However, many of the current\nsolutions are still not robust enough in real situations, commonly depending on\nmany constraints. This paper presents a robust and efficient ALPR system based\non the state-of-the-art YOLO object detector and Normalizing flows. The model\nuses two new strategies. Firstly, a two-stage network using YOLO and a\nnormalization flow-based model for normalization to detect Licenses Plates (LP)\nand recognize the LP with numbers and Arabic characters. Secondly, Multi-scale\nimage transformations are implemented to provide a solution to the problem of\nthe YOLO cropped LP detection including significant background noise.\nFurthermore, extensive experiments are led on a new dataset with realistic\nscenarios, we introduce a larger public annotated dataset collected from\nMoroccan plates. We demonstrate that our proposed model can learn on a small\nnumber of samples free of single or multiple characters. The dataset will also\nbe made publicly available to encourage further studies and research on plate\ndetection and recognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Oublal_K/0/1/0/all/0/1\">Khalid Oublal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1\">Xinyi Dai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Strategising template-guided needle placement for MR-targeted prostate biopsy. (arXiv:2207.10784v1 [cs.LG])","link":"http://arxiv.org/abs/2207.10784","description":"<p>Clinically significant prostate cancer has a better chance to be sampled\nduring ultrasound-guided biopsy procedures, if suspected lesions found in\npre-operative magnetic resonance (MR) images are used as targets. However, the\ndiagnostic accuracy of the biopsy procedure is limited by the\noperator-dependent skills and experience in sampling the targets, a sequential\ndecision making process that involves navigating an ultrasound probe and\nplacing a series of sampling needles for potentially multiple targets. This\nwork aims to learn a reinforcement learning (RL) policy that optimises the\nactions of continuous positioning of 2D ultrasound views and biopsy needles\nwith respect to a guiding template, such that the MR targets can be sampled\nefficiently and sufficiently. We first formulate the task as a Markov decision\nprocess (MDP) and construct an environment that allows the targeting actions to\nbe performed virtually for individual patients, based on their anatomy and\nlesions derived from MR images. A patient-specific policy can thus be\noptimised, before each biopsy procedure, by rewarding positive sampling in the\nMDP environment. Experiment results from fifty four prostate cancer patients\nshow that the proposed RL-learned policies obtained a mean hit rate of 93% and\nan average cancer core length of 11 mm, which compared favourably to two\nalternative baseline strategies designed by humans, without hand-engineered\nrewards that directly maximise these clinically relevant metrics. Perhaps more\ninterestingly, it is found that the RL agents learned strategies that were\nadaptive to the lesion size, where spread of the needles was prioritised for\nsmaller lesions. Such a strategy has not been previously reported or commonly\nadopted in clinical practice, but led to an overall superior targeting\nperformance when compared with intuitively designed strategies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gayo_I/0/1/0/all/0/1\">Iani JMB Gayo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saeed_S/0/1/0/all/0/1\">Shaheer U. Saeed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barratt_D/0/1/0/all/0/1\">Dean C. Barratt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clarkson_M/0/1/0/all/0/1\">Matthew J. Clarkson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yipeng Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inductive and Transductive Few-Shot Video Classification via Appearance and Temporal Alignments. (arXiv:2207.10785v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10785","description":"<p>We present a novel method for few-shot video classification, which performs\nappearance and temporal alignments. In particular, given a pair of query and\nsupport videos, we conduct appearance alignment via frame-level feature\nmatching to achieve the appearance similarity score between the videos, while\nutilizing temporal order-preserving priors for obtaining the temporal\nsimilarity score between the videos. Moreover, we introduce a few-shot video\nclassification framework that leverages the above appearance and temporal\nsimilarity scores across multiple steps, namely prototype-based training and\ntesting as well as inductive and transductive prototype refinement. To the best\nof our knowledge, our work is the first to explore transductive few-shot video\nclassification. Extensive experiments on both Kinetics and Something-Something\nV2 datasets show that both appearance and temporal alignments are crucial for\ndatasets with temporal order sensitivity such as Something-Something V2. Our\napproach achieves similar or better results than previous methods on both\ndatasets. Our code is available at https://github.com/VinAIResearch/fsvc-ata.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Khoi D. Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_Q/0/1/0/all/0/1\">Quoc-Huy Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Khoi Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_B/0/1/0/all/0/1\">Binh-Son Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_R/0/1/0/all/0/1\">Rang Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Test-Time Adaptation via Self-Training with Nearest Neighbor Information. (arXiv:2207.10792v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10792","description":"<p>Adapting trained classifiers using only online test data is important since\nit is difficult to access training data or future test data during test time.\nOne of the popular approaches for test-time adaptation is self-training, which\nfine-tunes the trained classifiers using the classifier predictions of the test\ndata as pseudo labels. However, under the test-time domain shift, self-training\nmethods have a limitation that learning with inaccurate pseudo labels greatly\ndegrades the performance of the adapted classifiers. To overcome this\nlimitation, we propose a novel test-time adaptation method Test-time Adaptation\nvia Self-Training with nearest neighbor information (TAST). Based on the idea\nthat a test data and its nearest neighbors in the embedding space of the\ntrained classifier are more likely to have the same label, we adapt the trained\nclassifier with the following two steps: (1) generate the pseudo label for the\ntest data using its nearest neighbors from a set composed of previous test\ndata, and (2) fine-tune the trained classifier with the pseudo label. Our\nexperiments on two standard benchmarks, i.e., domain generalization and image\ncorruption benchmarks, show that TAST outperforms the current state-of-the-art\ntest-time adaptation methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jang_M/0/1/0/all/0/1\">Minguk Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_S/0/1/0/all/0/1\">Sae-Young Chung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neuroimaging Feature Extraction using a Neural Network Classifier for Imaging Genetics. (arXiv:2207.10794v1 [q-bio.QM])","link":"http://arxiv.org/abs/2207.10794","description":"<p>A major issue in the association of genes to neuroimaging phenotypes is the\nhigh dimension of both genetic data and neuroimaging data. In this article, we\ntackle the latter problem with an eye toward developing solutions that are\nrelevant for disease prediction. Supported by a vast literature on the\npredictive power of neural networks, our proposed solution uses neural networks\nto extract from neuroimaging data features that are relevant for predicting\nAlzheimer's Disease (AD) for subsequent relation to genetics. Our\nneuroimaging-genetic pipeline is comprised of image processing, neuroimaging\nfeature extraction and genetic association steps. We propose a neural network\nclassifier for extracting neuroimaging features that are related with disease\nand a multivariate Bayesian group sparse regression model for genetic\nassociation. We compare the predictive power of these features to expert\nselected features and take a closer look at the SNPs identified with the new\nneuroimaging features.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Beaulac_C/0/1/0/all/0/1\">C&#xe9;dric Beaulac</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Wu_S/0/1/0/all/0/1\">Sidi Wu</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Gibson_E/0/1/0/all/0/1\">Erin Gibson</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Miranda_M/0/1/0/all/0/1\">Michelle F. Miranda</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Cao_J/0/1/0/all/0/1\">Jiguo Cao</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Rocha_L/0/1/0/all/0/1\">Leno Rocha</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Beg_M/0/1/0/all/0/1\">Mirza Faisal Beg</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Nathoo_F/0/1/0/all/0/1\">Farouk S. Nathoo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Just Rotate it: Deploying Backdoor Attacks via Rotation Transformation. (arXiv:2207.10825v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10825","description":"<p>Recent works have demonstrated that deep learning models are vulnerable to\nbackdoor poisoning attacks, where these attacks instill spurious correlations\nto external trigger patterns or objects (e.g., stickers, sunglasses, etc.). We\nfind that such external trigger signals are unnecessary, as highly effective\nbackdoors can be easily inserted using rotation-based image transformation. Our\nmethod constructs the poisoned dataset by rotating a limited amount of objects\nand labeling them incorrectly; once trained with it, the victim's model will\nmake undesirable predictions during run-time inference. It exhibits a\nsignificantly high attack success rate while maintaining clean performance\nthrough comprehensive empirical studies on image classification and object\ndetection tasks. Furthermore, we evaluate standard data augmentation techniques\nand four different backdoor defenses against our attack and find that none of\nthem can serve as a consistent mitigation approach. Our attack can be easily\ndeployed in the real world since it only requires rotating the object, as we\nshow in both image classification and object detection applications. Overall,\nour work highlights a new, simple, physically realizable, and highly effective\nvector for backdoor attacks. Our video demo is available at\nhttps://youtu.be/6JIF8wnX34M.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianhao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sehwag_V/0/1/0/all/0/1\">Vikash Sehwag</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahloujifar_S/0/1/0/all/0/1\">Saeed Mahloujifar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mittal_P/0/1/0/all/0/1\">Prateek Mittal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-shot Image Generation Using Discrete Content Representation. (arXiv:2207.10833v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10833","description":"<p>Few-shot image generation and few-shot image translation are two related\ntasks, both of which aim to generate new images for an unseen category with\nonly a few images. In this work, we make the first attempt to adapt few-shot\nimage translation method to few-shot image generation task. Few-shot image\ntranslation disentangles an image into style vector and content map. An unseen\nstyle vector can be combined with different seen content maps to produce\ndifferent images. However, it needs to store seen images to provide content\nmaps and the unseen style vector may be incompatible with seen content maps. To\nadapt it to few-shot image generation task, we learn a compact dictionary of\nlocal content vectors via quantizing continuous content maps into discrete\ncontent maps instead of storing seen images. Furthermore, we model the\nautoregressive distribution of discrete content map conditioned on style\nvector, which can alleviate the incompatibility between content map and style\nvector. Qualitative and quantitative results on three real datasets demonstrate\nthat our model can produce images of higher diversity and fidelity for unseen\ncategories than previous methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1\">Yan Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_L/0/1/0/all/0/1\">Li Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianfu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liqing Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncertainty-aware Multi-modal Learning via Cross-modal Random Network Prediction. (arXiv:2207.10851v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10851","description":"<p>Multi-modal learning focuses on training models by equally combining multiple\ninput data modalities during the prediction process. However, this equal\ncombination can be detrimental to the prediction accuracy because different\nmodalities are usually accompanied by varying levels of uncertainty. Using such\nuncertainty to combine modalities has been studied by a couple of approaches,\nbut with limited success because these approaches are either designed to deal\nwith specific classification or segmentation problems and cannot be easily\ntranslated into other tasks, or suffer from numerical instabilities. In this\npaper, we propose a new Uncertainty-aware Multi-modal Learner that estimates\nuncertainty by measuring feature density via Cross-modal Random Network\nPrediction (CRNP). CRNP is designed to require little adaptation to translate\nbetween different prediction tasks, while having a stable training process.\nFrom a technical point of view, CRNP is the first approach to explore random\nnetwork prediction to estimate uncertainty and to combine multi-modal data.\nExperiments on two 3D multi-modal medical image segmentation tasks and three 2D\nmulti-modal computer vision classification tasks show the effectiveness,\nadaptability and robustness of CRNP. Also, we provide an extensive discussion\non different fusion functions and visualization to validate the proposed model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianpeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuanhong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Congbo Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avery_J/0/1/0/all/0/1\">Jodie Avery</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hull_L/0/1/0/all/0/1\">Louise Hull</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carneiro_G/0/1/0/all/0/1\">Gustavo Carneiro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spatio-Temporal Deformable Attention Network for Video Deblurring. (arXiv:2207.10852v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10852","description":"<p>The key success factor of the video deblurring methods is to compensate for\nthe blurry pixels of the mid-frame with the sharp pixels of the adjacent video\nframes. Therefore, mainstream methods align the adjacent frames based on the\nestimated optical flows and fuse the alignment frames for restoration. However,\nthese methods sometimes generate unsatisfactory results because they rarely\nconsider the blur levels of pixels, which may introduce blurry pixels from\nvideo frames. Actually, not all the pixels in the video frames are sharp and\nbeneficial for deblurring. To address this problem, we propose the\nspatio-temporal deformable attention network (STDANet) for video delurring,\nwhich extracts the information of sharp pixels by considering the pixel-wise\nblur levels of the video frames. Specifically, STDANet is an encoder-decoder\nnetwork combined with the motion estimator and spatio-temporal deformable\nattention (STDA) module, where motion estimator predicts coarse optical flows\nthat are used as base offsets to find the corresponding sharp pixels in STDA\nmodule. Experimental results indicate that the proposed STDANet performs\nfavorably against state-of-the-art methods on the GoPro, DVD, and BSD datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Huicong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1\">Haozhe Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1\">Hongxun Yao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prototype-Guided Continual Adaptation for Class-Incremental Unsupervised Domain Adaptation. (arXiv:2207.10856v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10856","description":"<p>This paper studies a new, practical but challenging problem, called\nClass-Incremental Unsupervised Domain Adaptation (CI-UDA), where the labeled\nsource domain contains all classes, but the classes in the unlabeled target\ndomain increase sequentially. This problem is challenging due to two\ndifficulties. First, source and target label sets are inconsistent at each time\nstep, which makes it difficult to conduct accurate domain alignment. Second,\nprevious target classes are unavailable in the current step, resulting in the\nforgetting of previous knowledge. To address this problem, we propose a novel\nPrototype-guided Continual Adaptation (ProCA) method, consisting of two\nsolution strategies. 1) Label prototype identification: we identify target\nlabel prototypes by detecting shared classes with cumulative prediction\nprobabilities of target samples. 2) Prototype-based alignment and replay: based\non the identified label prototypes, we align both domains and enforce the model\nto retain previous knowledge. With these two strategies, ProCA is able to adapt\nthe source model to a class-incremental unlabeled target domain effectively.\nExtensive experiments demonstrate the effectiveness and superiority of ProCA in\nresolving CI-UDA. The source code is available at\nhttps://github.com/Hongbin98/ProCA.git\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hongbin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yifan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Z/0/1/0/all/0/1\">Zhen Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_S/0/1/0/all/0/1\">Shuaicheng Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1\">Chuang Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yanxia Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1\">Mingkui Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Geodesic-Former: a Geodesic-Guided Few-shot 3D Point Cloud Instance Segmenter. (arXiv:2207.10859v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10859","description":"<p>This paper introduces a new problem in 3D point cloud: few-shot instance\nsegmentation. Given a few annotated point clouds exemplified a target class,\nour goal is to segment all instances of this target class in a query point\ncloud. This problem has a wide range of practical applications where point-wise\ninstance segmentation annotation is prohibitively expensive to collect. To\naddress this problem, we present Geodesic-Former -- the first geodesic-guided\ntransformer for 3D point cloud instance segmentation. The key idea is to\nleverage the geodesic distance to tackle the density imbalance of LiDAR 3D\npoint clouds. The LiDAR 3D point clouds are dense near the object surface and\nsparse or empty elsewhere making the Euclidean distance less effective to\ndistinguish different objects. The geodesic distance, on the other hand, is\nmore suitable since it encodes the scene's geometry which can be used as a\nguiding signal for the attention mechanism in a transformer decoder to generate\nkernels representing distinct features of instances. These kernels are then\nused in a dynamic convolution to obtain the final instance masks. To evaluate\nGeodesic-Former on the new task, we propose new splits of the two common 3D\npoint cloud instance segmentation datasets: ScannetV2 and S3DIS.\nGeodesic-Former consistently outperforms strong baselines adapted from\nstate-of-the-art 3D point cloud instance segmentation approaches with a\nsignificant margin. Code is available at\nhttps://github.com/VinAIResearch/GeoFormer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ngo_T/0/1/0/all/0/1\">Tuan Ngo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Khoi Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Higher Adversarial Susceptibility of Contrastive Self-Supervised Learning. (arXiv:2207.10862v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10862","description":"<p>Contrastive self-supervised learning (CSL) has managed to match or surpass\nthe performance of supervised learning in image and video classification.\nHowever, it is still largely unknown if the nature of the representation\ninduced by the two learning paradigms is similar. We investigate this under the\nlens of adversarial robustness. Our analytical treatment of the problem reveals\nintrinsic higher sensitivity of CSL over supervised learning. It identifies the\nuniform distribution of data representation over a unit hypersphere in the CSL\nrepresentation space as the key contributor to this phenomenon. We establish\nthat this increases model sensitivity to input perturbations in the presence of\nfalse negatives in the training data. Our finding is supported by extensive\nexperiments for image and video classification using adversarial perturbations\nand other input corruptions. Building on the insights, we devise strategies\nthat are simple, yet effective in improving model robustness with CSL training.\nWe demonstrate up to 68% reduction in the performance gap between adversarially\nattacked CSL and its supervised counterpart. Finally, we contribute to robust\nCSL paradigm by incorporating our findings in adversarial self-supervised\nlearning. We demonstrate an average gain of about 5% over two different\nstate-of-the-art methods in this domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1\">Rohit Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akhtar_N/0/1/0/all/0/1\">Naveed Akhtar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mian_A/0/1/0/all/0/1\">Ajmal Mian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1\">Mubarak Shah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cost Aggregation with 4D Convolutional Swin Transformer for Few-Shot Segmentation. (arXiv:2207.10866v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10866","description":"<p>This paper presents a novel cost aggregation network, called Volumetric\nAggregation with Transformers (VAT), for few-shot segmentation. The use of\ntransformers can benefit correlation map aggregation through self-attention\nover a global receptive field. However, the tokenization of a correlation map\nfor transformer processing can be detrimental, because the discontinuity at\ntoken boundaries reduces the local context available near the token edges and\ndecreases inductive bias. To address this problem, we propose a 4D\nConvolutional Swin Transformer, where a high-dimensional Swin Transformer is\npreceded by a series of small-kernel convolutions that impart local context to\nall pixels and introduce convolutional inductive bias. We additionally boost\naggregation performance by applying transformers within a pyramidal structure,\nwhere aggregation at a coarser level guides aggregation at a finer level. Noise\nin the transformer output is then filtered in the subsequent decoder with the\nhelp of the query's appearance embedding. With this model, a new\nstate-of-the-art is set for all the standard benchmarks in few-shot\nsegmentation. It is shown that VAT attains state-of-the-art performance for\nsemantic correspondence as well, where cost aggregation also plays a central\nrole.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hong_S/0/1/0/all/0/1\">Sunghwan Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1\">Seokju Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nam_J/0/1/0/all/0/1\">Jisu Nam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Stephen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seungryong Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Optimizing Image Compression via Joint Learning with Denoising. (arXiv:2207.10869v1 [eess.IV])","link":"http://arxiv.org/abs/2207.10869","description":"<p>High levels of noise usually exist in today's captured images due to the\nrelatively small sensors equipped in the smartphone cameras, where the noise\nbrings extra challenges to lossy image compression algorithms. Without the\ncapacity to tell the difference between image details and noise, general image\ncompression methods allocate additional bits to explicitly store the undesired\nimage noise during compression and restore the unpleasant noisy image during\ndecompression. Based on the observations, we optimize the image compression\nalgorithm to be noise-aware as joint denoising and compression to resolve the\nbits misallocation problem. The key is to transform the original noisy images\nto noise-free bits by eliminating the undesired noise during compression, where\nthe bits are later decompressed as clean images. Specifically, we propose a\nnovel two-branch, weight-sharing architecture with plug-in feature denoisers to\nallow a simple and effective realization of the goal with little computational\ncost. Experimental results show that our method gains a significant improvement\nover the existing baseline methods on both the synthetic and real-world\ndatasets. Our source code is available at\nhttps://github.com/felixcheng97/DenoiseCompression.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Cheng_K/0/1/0/all/0/1\">Ka Leong Cheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xie_Y/0/1/0/all/0/1\">Yueqi Xie</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Q/0/1/0/all/0/1\">Qifeng Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Ensemble Approach for Multiple Emotion Descriptors Estimation Using Multi-task Learning. (arXiv:2207.10878v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10878","description":"<p>This paper illustrates our submission method to the fourth Affective Behavior\nAnalysis in-the-Wild (ABAW) Competition. The method is used for the Multi-Task\nLearning Challenge. Instead of using only face information, we employ full\ninformation from a provided dataset containing face and the context around the\nface. We utilized the InceptionNet V3 model to extract deep features then we\napplied the attention mechanism to refine the features. After that, we put\nthose features into the transformer block and multi-layer perceptron networks\nto get the final multiple kinds of emotion. Our model predicts arousal and\nvalence, classifies the emotional expression and estimates the action units\nsimultaneously. The proposed system achieves the performance of 0.917 on the\nMTL Challenge validation dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Haider_I/0/1/0/all/0/1\">Irfan Haider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_M/0/1/0/all/0/1\">Minh-Trieu Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Soo-Hyung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hyung-Jeong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Guee-Sang Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"My View is the Best View: Procedure Learning from Egocentric Videos. (arXiv:2207.10883v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10883","description":"<p>Procedure learning involves identifying the key-steps and determining their\nlogical order to perform a task. Existing approaches commonly use third-person\nvideos for learning the procedure, making the manipulated object small in\nappearance and often occluded by the actor, leading to significant errors. In\ncontrast, we observe that videos obtained from first-person (egocentric)\nwearable cameras provide an unobstructed and clear view of the action. However,\nprocedure learning from egocentric videos is challenging because (a) the camera\nview undergoes extreme changes due to the wearer's head motion, and (b) the\npresence of unrelated frames due to the unconstrained nature of the videos. Due\nto this, current state-of-the-art methods' assumptions that the actions occur\nat approximately the same time and are of the same duration, do not hold.\nInstead, we propose to use the signal provided by the temporal correspondences\nbetween key-steps across videos. To this end, we present a novel\nself-supervised Correspond and Cut (CnC) framework for procedure learning. CnC\nidentifies and utilizes the temporal correspondences between the key-steps\nacross multiple videos to learn the procedure. Our experiments show that CnC\noutperforms the state-of-the-art on the benchmark ProceL and CrossTask datasets\nby 5.2% and 6.3%, respectively. Furthermore, for procedure learning using\negocentric videos, we propose the EgoProceL dataset consisting of 62 hours of\nvideos captured by 130 subjects performing 16 tasks. The source code and the\ndataset are available on the project page https://sid2697.github.io/egoprocel/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bansal_S/0/1/0/all/0/1\">Siddhant Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arora_C/0/1/0/all/0/1\">Chetan Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jawahar_C/0/1/0/all/0/1\">C.V. Jawahar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"XAI based Performance Preserving Adaptive Image Compression for Efficient Satellite Communication. (arXiv:2207.10885v1 [eess.IV])","link":"http://arxiv.org/abs/2207.10885","description":"<p>In the era of multinational cooperation, gathering and analyzing the\nsatellite images are getting easier and more important. Typical procedure of\nthe satellite image analysis include transmission of the bulky image data from\nsatellite to the ground producing significant overhead. To reduce the amount of\nthe transmission overhead while making no harm to the analysis result, we\npropose a novel image compression scheme RDIC in this paper. RDIC is a\nreasoning based image compression scheme that compresses an image according to\nthe pixel importance score acquired from the analysis model itself. From the\nexperimental results we showed that our RDIC scheme successfully captures the\nimportant regions in an image showing high compression rate and low accuracy\nloss.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lee_K/0/1/0/all/0/1\">KyungChae Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FairGRAPE: Fairness-aware GRAdient Pruning mEthod for Face Attribute Classification. (arXiv:2207.10888v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10888","description":"<p>Existing pruning techniques preserve deep neural networks' overall ability to\nmake correct predictions but may also amplify hidden biases during the\ncompression process. We propose a novel pruning method, Fairness-aware GRAdient\nPruning mEthod (FairGRAPE), that minimizes the disproportionate impacts of\npruning on different sub-groups. Our method calculates the per-group importance\nof each model weight and selects a subset of weights that maintain the relative\nbetween-group total importance in pruning. The proposed method then prunes\nnetwork edges with small importance values and repeats the procedure by\nupdating importance values. We demonstrate the effectiveness of our method on\nfour different datasets, FairFace, UTKFace, CelebA, and ImageNet, for the tasks\nof face attribute classification where our method reduces the disparity in\nperformance degradation by up to 90% compared to the state-of-the-art pruning\nalgorithms. Our method is substantially more effective in a setting with a high\npruning rate (99%). The code and dataset used in the experiments are available\nat https://github.com/Bernardo1998/FairGRAPE\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xiaofeng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seungbae Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joo_J/0/1/0/all/0/1\">Jungseock Joo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bi-directional Contrastive Learning for Domain Adaptive Semantic Segmentation. (arXiv:2207.10892v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10892","description":"<p>We present a novel unsupervised domain adaptation method for semantic\nsegmentation that generalizes a model trained with source images and\ncorresponding ground-truth labels to a target domain. A key to domain adaptive\nsemantic segmentation is to learn domain-invariant and discriminative features\nwithout target ground-truth labels. To this end, we propose a bi-directional\npixel-prototype contrastive learning framework that minimizes intra-class\nvariations of features for the same object class, while maximizing inter-class\nvariations for different ones, regardless of domains. Specifically, our\nframework aligns pixel-level features and a prototype of the same object class\nin target and source images (i.e., positive pairs), respectively, sets them\napart for different classes (i.e., negative pairs), and performs the alignment\nand separation processes toward the other direction with pixel-level features\nin the source image and a prototype in the target image. The cross-domain\nmatching encourages domain-invariant feature representations, while the\nbidirectional pixel-prototype correspondences aggregate features for the same\nobject class, providing discriminative features. To establish training pairs\nfor contrastive learning, we propose to generate dynamic pseudo labels of\ntarget images using a non-parametric label transfer, that is, pixel-prototype\ncorrespondences across different domains. We also present a calibration method\ncompensating class-wise domain biases of prototypes gradually during training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Geon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eom_C/0/1/0/all/0/1\">Chanho Eom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_W/0/1/0/all/0/1\">Wonkyung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_H/0/1/0/all/0/1\">Hyekang Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ham_B/0/1/0/all/0/1\">Bumsub Ham</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D Random Occlusion and Multi-Layer Projection for Deep Multi-Camera Pedestrian Localization. (arXiv:2207.10895v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10895","description":"<p>Although deep-learning based methods for monocular pedestrian detection have\nmade great progress, they are still vulnerable to heavy occlusions. Using\nmulti-view information fusion is a potential solution but has limited\napplications, due to the lack of annotated training samples in existing\nmulti-view datasets, which increases the risk of overfitting. To address this\nproblem, a data augmentation method is proposed to randomly generate 3D\ncylinder occlusions, on the ground plane, which are of the average size of\npedestrians and projected to multiple views, to relieve the impact of\noverfitting in the training. Moreover, the feature map of each view is\nprojected to multiple parallel planes at different heights, by using\nhomographies, which allows the CNNs to fully utilize the features across the\nheight of each pedestrian to infer the locations of pedestrians on the ground\nplane. The proposed 3DROM method has a greatly improved performance in\ncomparison with the state-of-the-art deep-learning based methods for multi-view\npedestrian detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_R/0/1/0/all/0/1\">Rui Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Ming Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yuyao Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_J/0/1/0/all/0/1\">Jeremy S. Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xi Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Modeling of Future Context for Image Captioning. (arXiv:2207.10897v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10897","description":"<p>Existing approaches to image captioning usually generate the sentence\nword-by-word from left to right, with the constraint of conditioned on local\ncontext including the given image and history generated words. There have been\nmany studies target to make use of global information during decoding, e.g.,\niterative refinement. However, it is still under-explored how to effectively\nand efficiently incorporate the future context. To respond to this issue,\ninspired by that Non-Autoregressive Image Captioning (NAIC) can leverage\ntwo-side relation with modified mask operation, we aim to graft this advance to\nthe conventional Autoregressive Image Captioning (AIC) model while maintaining\nthe inference efficiency without extra time cost. Specifically, AIC and NAIC\nmodels are first trained combined with shared visual encoders, forcing the\nvisual encoder to contain sufficient and valid future context; then the AIC\nmodel is encouraged to capture the causal dynamics of cross-layer interchanging\nfrom NAIC model on its unconfident words, which follows a teacher-student\nparadigm and optimized with the distribution calibration training objective.\nEmpirical evidences demonstrate that our proposed approach clearly surpass the\nstate-of-the-art baselines in both automatic metrics and human evaluations on\nthe MS COCO benchmark. The source code is available at:\nhttps://github.com/feizc/Future-Caption.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fei_Z/0/1/0/all/0/1\">Zhengcong Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Junshi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xiaoming Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xiaolin Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Decoupled Adversarial Contrastive Learning for Self-supervised Adversarial Robustness. (arXiv:2207.10899v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10899","description":"<p>Adversarial training (AT) for robust representation learning and\nself-supervised learning (SSL) for unsupervised representation learning are two\nactive research fields. Integrating AT into SSL, multiple prior works have\naccomplished a highly significant yet challenging task: learning robust\nrepresentation without labels. A widely used framework is adversarial\ncontrastive learning which couples AT and SSL, and thus constitute a very\ncomplex optimization problem. Inspired by the divide-and-conquer philosophy, we\nconjecture that it might be simplified as well as improved by solving two\nsub-problems: non-robust SSL and pseudo-supervised AT. This motivation shifts\nthe focus of the task from seeking an optimal integrating strategy for a\ncoupled problem to finding sub-solutions for sub-problems. With this said, this\nwork discards prior practices of directly introducing AT to SSL frameworks and\nproposed a two-stage framework termed Decoupled Adversarial Contrastive\nLearning (DeACL). Extensive experimental results demonstrate that our DeACL\nachieves SOTA self-supervised adversarial robustness while significantly\nreducing the training time, which validates its effectiveness and efficiency.\nMoreover, our DeACL constitutes a more explainable solution, and its success\nalso bridges the gap with semi-supervised AT for exploiting unlabeled samples\nfor robust representation learning. The code is publicly accessible at\nhttps://github.com/pantheon5100/DeACL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chaoning Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chenshuang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_A/0/1/0/all/0/1\">Axi Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jiu Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_C/0/1/0/all/0/1\">Chang D. Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kweon_I/0/1/0/all/0/1\">In So Kweon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DBQ-SSD: Dynamic Ball Query for Efficient 3D Object Detection. (arXiv:2207.10909v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10909","description":"<p>Many point-based 3D detectors adopt point-feature sampling strategies to drop\nsome points for efficient inference. These strategies are typically based on\nfixed and handcrafted rules, making difficult to handle complicated scenes.\nDifferent from them, we propose a Dynamic Ball Query (DBQ) network to\nadaptively select a subset of input points according to the input features, and\nassign the feature transform with suitable receptive field for each selected\npoint. It can be embedded into some state-of-the-art 3D detectors and trained\nin an end-to-end manner, which significantly reduces the computational cost.\nExtensive experiments demonstrate that our method can reduce latency by 30%-60%\non KITTI and Waymo datasets. Specifically, the inference speed of our detector\ncan reach 162 FPS and 30 FPS with negligible performance degradation on KITTI\nand Waymo datasets, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jinrong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1\">Lin Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Songtao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zeming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoping Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Hongbin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1\">Nanning Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Optimization of Forcemyography Sensor Placement for Arm Movement Recognition. (arXiv:2207.10915v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10915","description":"<p>How to design an optimal wearable device for human movement recognition is\nvital to reliable and accurate human-machine collaboration. Previous works\nmainly fabricate wearable devices heuristically. Instead, this paper raises an\nacademic question: can we design an optimization algorithm to optimize the\nfabrication of wearable devices such as figuring out the best sensor\narrangement automatically? Specifically, this work focuses on optimizing the\nplacement of Forcemyography (FMG) sensors for FMG armbands in the application\nof arm movement recognition. Firstly, based on graph theory, the armband is\nmodeled considering sensors' signals and connectivity. Then, a Graph-based\nArmband Modeling Network (GAM-Net) is introduced for arm movement recognition.\nAfterward, the sensor placement optimization for FMG armbands is formulated and\nan optimization algorithm with greedy local search is proposed. To study the\neffectiveness of our optimization algorithm, a dataset for mechanical\nmaintenance tasks using FMG armbands with 16 sensors is collected. Our\nexperiments show that using only 4 sensors optimized with our algorithm can\nhelp maintain a comparable recognition accuracy to using all sensors. Finally,\nthe optimized sensor placement result is verified from a physiological view.\nThis work would like to shed light on the automatic fabrication of wearable\ndevices considering downstream tasks, such as human biological signal\ncollection and movement recognition. Our code and dataset are available at\nhttps://github.com/JerryX1110/IROS22-FMG-Sensor-Optimization\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiaohao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Z/0/1/0/all/0/1\">Zihao Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Huaxin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruichao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_Z/0/1/0/all/0/1\">Zihan Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1\">Bin Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PLD-SLAM: A Real-Time Visual SLAM Using Points and Line Segments in Dynamic Scenes. (arXiv:2207.10916v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10916","description":"<p>In this paper, we consider the problems in the practical application of\nvisual simultaneous localization and mapping (SLAM). With the popularization\nand application of the technology in wide scope, the practicability of SLAM\nsystem has become a new hot topic after the accuracy and robustness, e.g., how\nto keep the stability of the system and achieve accurate pose estimation in the\nlow-texture and dynamic environment, and how to improve the universality and\nreal-time performance of the system in the real scenes, etc. This paper\nproposes a real-time stereo indirect visual SLAM system, PLD-SLAM, which\ncombines point and line features, and avoid the impact of dynamic objects in\nhighly dynamic environments. We also present a novel global gray similarity\n(GGS) algorithm to achieve reasonable keyframe selection and efficient loop\nclosure detection (LCD). Benefiting from the GGS, PLD-SLAM can realize\nreal-time accurate pose estimation in most real scenes without pre-training and\nloading a huge feature dictionary model. To verify the performance of the\nproposed system, we compare it with existing state-of-the-art (SOTA) methods on\nthe public datasets KITTI, EuRoC MAV, and the indoor stereo datasets provided\nby us, etc. The experiments show that the PLD-SLAM has better real-time\nperformance while ensuring stability and accuracy in most scenarios. In\naddition, through the analysis of the experimental results of the GGS, we can\nfind it has excellent performance in the keyframe selection and LCD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">BaoSheng Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Long-tailed Instance Segmentation using Gumbel Optimized Loss. (arXiv:2207.10936v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10936","description":"<p>Major advancements have been made in the field of object detection and\nsegmentation recently. However, when it comes to rare categories, the\nstate-of-the-art methods fail to detect them, resulting in a significant\nperformance gap between rare and frequent categories. In this paper, we\nidentify that Sigmoid or Softmax functions used in deep detectors are a major\nreason for low performance and are sub-optimal for long-tailed detection and\nsegmentation. To address this, we develop a Gumbel Optimized Loss (GOL), for\nlong-tailed detection and segmentation. It aligns with the Gumbel distribution\nof rare classes in imbalanced datasets, considering the fact that most classes\nin long-tailed detection have low expected probability. The proposed GOL\nsignificantly outperforms the best state-of-the-art method by 1.1% on AP , and\nboosts the overall segmentation by 9.0% and detection by 8.0%, particularly\nimproving detection of rare classes by 20.3%, compared to Mask-RCNN, on LVIS\ndataset. Code available at: https://github.com/kostas1515/GOL\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alexandridis_K/0/1/0/all/0/1\">Konstantinos Panagiotis Alexandridis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jiankang Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1\">Anh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1\">Shan Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dense RGB-D-Inertial SLAM with Map Deformations. (arXiv:2207.10940v1 [cs.RO])","link":"http://arxiv.org/abs/2207.10940","description":"<p>While dense visual SLAM methods are capable of estimating dense\nreconstructions of the environment, they suffer from a lack of robustness in\ntheir tracking step, especially when the optimisation is poorly initialised.\nSparse visual SLAM systems have attained high levels of accuracy and robustness\nthrough the inclusion of inertial measurements in a tightly-coupled fusion.\nInspired by this performance, we propose the first tightly-coupled dense\nRGB-D-inertial SLAM system.\n</p>\n<p>Our system has real-time capability while running on a GPU. It jointly\noptimises for the camera pose, velocity, IMU biases and gravity direction while\nbuilding up a globally consistent, fully dense surfel-based 3D reconstruction\nof the environment. Through a series of experiments on both synthetic and real\nworld datasets, we show that our dense visual-inertial SLAM system is more\nrobust to fast motions and periods of low texture and low geometric variation\nthan a related RGB-D-only SLAM system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Laidlow_T/0/1/0/all/0/1\">Tristan Laidlow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bloesch_M/0/1/0/all/0/1\">Michael Bloesch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenbin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leutenegger_S/0/1/0/all/0/1\">Stefan Leutenegger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Local Aggregation Network with Adaptive Clusterer for Anomaly Detection. (arXiv:2207.10948v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10948","description":"<p>Existing methods for anomaly detection based on memory-augmented autoencoder\n(AE) have the following drawbacks: (1) Establishing a memory bank requires\nadditional memory space. (2) The fixed number of prototypes from subjective\nassumptions ignores the data feature differences and diversity. To overcome\nthese drawbacks, we introduce DLAN-AC, a Dynamic Local Aggregation Network with\nAdaptive Clusterer, for anomaly detection. First, The proposed DLAN can\nautomatically learn and aggregate high-level features from the AE to obtain\nmore representative prototypes, while freeing up extra memory space. Second,\nThe proposed AC can adaptively cluster video data to derive initial prototypes\nwith prior information. In addition, we also propose a dynamic redundant\nclustering strategy (DRCS) to enable DLAN for automatically eliminating feature\nclusters that do not contribute to the construction of prototypes. Extensive\nexperiments on benchmarks demonstrate that DLAN-AC outperforms most existing\nmethods, validating the effectiveness of our method. Our code is publicly\navailable at https://github.com/Beyond-Zw/DLAN-AC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhiwei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_P/0/1/0/all/0/1\">Peng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaotao Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scale dependant layer for self-supervised nuclei encoding. (arXiv:2207.10950v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10950","description":"<p>Recent developments in self-supervised learning give us the possibility to\nfurther reduce human intervention in multi-step pipelines where the focus\nevolves around particular objects of interest. In the present paper, the focus\nlays in the nuclei in histopathology images. In particular we aim at extracting\ncellular information in an unsupervised manner for a downstream task. As nuclei\npresent themselves in a variety of sizes, we propose a new Scale-dependant\nconvolutional layer to bypass scaling issues when resizing nuclei. On three\nnuclei datasets, we benchmark the following methods: handcrafted, pre-trained\nResNet, supervised ResNet and self-supervised features. We show that the\nproposed convolution layer boosts performance and that this layer combined with\nBarlows-Twins allows for better nuclei encoding compared to the supervised\nparadigm in the low sample setting and outperforms all other proposed\nunsupervised methods. In addition, we extend the existing TNBC dataset to\nincorporate nuclei class annotation in order to enrich and publicly release a\nsmall sample setting dataset for nuclei segmentation and classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Naylor_P/0/1/0/all/0/1\">Peter Naylor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsai_Y/0/1/0/all/0/1\">Yao-Hung Hubert Tsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lae_M/0/1/0/all/0/1\">Marick La&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamada_M/0/1/0/all/0/1\">Makoto Yamada</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision-based Human Fall Detection Systems using Deep Learning: A Review. (arXiv:2207.10952v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10952","description":"<p>Human fall is one of the very critical health issues, especially for elders\nand disabled people living alone. The number of elder populations is increasing\nsteadily worldwide. Therefore, human fall detection is becoming an effective\ntechnique for assistive living for those people. For assistive living, deep\nlearning and computer vision have been used largely. In this review article, we\ndiscuss deep learning (DL)-based state-of-the-art non-intrusive (vision-based)\nfall detection techniques. We also present a survey on fall detection benchmark\ndatasets. For a clear understanding, we briefly discuss different metrics which\nare used to evaluate the performance of the fall detection systems. This\narticle also gives a future direction on vision-based human fall detection\ntechniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alam_E/0/1/0/all/0/1\">Ekram Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sufian_A/0/1/0/all/0/1\">Abu Sufian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dutta_P/0/1/0/all/0/1\">Paramartha Dutta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leo_M/0/1/0/all/0/1\">Marco Leo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visible and Near Infrared Image Fusion Based on Texture Information. (arXiv:2207.10953v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10953","description":"<p>Multi-sensor fusion is widely used in the environment perception system of\nthe autonomous vehicle. It solves the interference caused by environmental\nchanges and makes the whole driving system safer and more reliable. In this\npaper, a novel visible and near-infrared fusion method based on texture\ninformation is proposed to enhance unstructured environmental images. It aims\nat the problems of artifact, information loss and noise in traditional visible\nand near infrared image fusion methods. Firstly, the structure information of\nthe visible image (RGB) and the near infrared image (NIR) after texture removal\nis obtained by relative total variation (RTV) calculation as the base layer of\nthe fused image; secondly, a Bayesian classification model is established to\ncalculate the noise weight and the noise information and the noise information\nin the visible image is adaptively filtered by joint bilateral filter; finally,\nthe fused image is acquired by color space conversion. The experimental results\ndemonstrate that the proposed algorithm can preserve the spectral\ncharacteristics and the unique information of visible and near-infrared images\nwithout artifacts and color distortion, and has good robustness as well as\npreserving the unique texture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guanyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1\">Beichen Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1\">Yuehan Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Faster VoxelPose: Real-time 3D Human Pose Estimation by Orthographic Projection. (arXiv:2207.10955v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10955","description":"<p>While the voxel-based methods have achieved promising results for\nmulti-person 3D pose estimation from multi-cameras, they suffer from heavy\ncomputation burdens, especially for large scenes. We present Faster VoxelPose\nto address the challenge by re-projecting the feature volume to the three\ntwo-dimensional coordinate planes and estimating X, Y, Z coordinates from them\nseparately. To that end, we first localize each person by a 3D bounding box by\nestimating a 2D box and its height based on the volume features projected to\nthe xy-plane and z-axis, respectively. Then for each person, we estimate\npartial joint coordinates from the three coordinate planes separately which are\nthen fused to obtain the final 3D pose. The method is free from costly 3D-CNNs\nand improves the speed of VoxelPose by ten times and meanwhile achieves\ncompetitive accuracy as the state-of-the-art methods, proving its potential in\nreal-time applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Hang Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wentao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chunyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1\">Rujie Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yizhou Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"QueryProp: Object Query Propagation for High-Performance Video Object Detection. (arXiv:2207.10959v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10959","description":"<p>Video object detection has been an important yet challenging topic in\ncomputer vision. Traditional methods mainly focus on designing the image-level\nor box-level feature propagation strategies to exploit temporal information.\nThis paper argues that with a more effective and efficient feature propagation\nframework, video object detectors can gain improvement in terms of both\naccuracy and speed. For this purpose, this paper studies object-level feature\npropagation, and proposes an object query propagation (QueryProp) framework for\nhigh-performance video object detection. The proposed QueryProp contains two\npropagation strategies: 1) query propagation is performed from sparse key\nframes to dense non-key frames to reduce the redundant computation on non-key\nframes; 2) query propagation is performed from previous key frames to the\ncurrent key frame to improve feature representation by temporal context\nmodeling. To further facilitate query propagation, an adaptive propagation gate\nis designed to achieve flexible key frame selection. We conduct extensive\nexperiments on the ImageNet VID dataset. QueryProp achieves comparable accuracy\nwith state-of-the-art methods and strikes a decent accuracy/speed trade-off.\nCode is available at https://github.com/hf1995/QueryProp.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_F/0/1/0/all/0/1\">Fei He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_N/0/1/0/all/0/1\">Naiyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\">Jian Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kaiqi Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Principal Geodesic Analysis of Merge Trees (and Persistence Diagrams). (arXiv:2207.10960v1 [cs.GR])","link":"http://arxiv.org/abs/2207.10960","description":"<p>This paper presents a computational framework for the Principal Geodesic\nAnalysis of merge trees (MT-PGA), a novel adaptation of the celebrated\nPrincipal Component Analysis (PCA) framework [87] to the Wasserstein metric\nspace of merge trees [92]. We formulate MT-PGA computation as a constrained\noptimization problem, aiming at adjusting a basis of orthogonal geodesic axes,\nwhile minimizing a fitting energy. We introduce an efficient, iterative\nalgorithm which exploits shared-memory parallelism, as well as an analytic\nexpression of the fitting energy gradient, to ensure fast iterations. Our\napproach also trivially extends to extremum persistence diagrams. Extensive\nexperiments on public ensembles demonstrate the efficiency of our approach -\nwith MT-PGA computations in the orders of minutes for the largest examples. We\nshow the utility of our contributions by extending to merge trees two typical\nPCA applications. First, we apply MT-PGA to data reduction and reliably\ncompress merge trees by concisely representing them by their first coordinates\nin the MT-PGA basis. Second, we present a dimensionality reduction framework\nexploiting the first two directions of the MT-PGA basis to generate\ntwo-dimensional layouts of the ensemble. We augment these layouts with\npersistence correlation views, enabling global and local visual inspections of\nthe feature variability in the ensemble. In both applications, quantitative\nexperiments assess the relevance of our framework. Finally, we provide a\nlightweight C++ implementation that can be used to reproduce our results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pont_M/0/1/0/all/0/1\">Mathieu Pont</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vidal_J/0/1/0/all/0/1\">Jules Vidal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tierny_J/0/1/0/all/0/1\">Julien Tierny</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Opportunistic hip fracture risk prediction in Men from X-ray: Findings from the Osteoporosis in Men (MrOS) Study. (arXiv:2207.10970v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10970","description":"<p>Osteoporosis is a common disease that increases fracture risk. Hip fractures,\nespecially in elderly people, lead to increased morbidity, decreased quality of\nlife and increased mortality. Being a silent disease before fracture,\nosteoporosis often remains undiagnosed and untreated. Areal bone mineral\ndensity (aBMD) assessed by dual-energy X-ray absorptiometry (DXA) is the\ngold-standard method for osteoporosis diagnosis and hence also for future\nfracture prediction (prognostic). However, the required special equipment is\nnot broadly available everywhere, in particular not to patients in developing\ncountries. We propose a deep learning classification model (FORM) that can\ndirectly predict hip fracture risk from either plain radiographs (X-ray) or 2D\nprojection images of computed tomography (CT) data. Our method is fully\nautomated and therefore well suited for opportunistic screening settings,\nidentifying high risk patients in a broader population without additional\nscreening. FORM was trained and evaluated on X-rays and CT projections from the\nOsteoporosis in Men (MrOS) study. 3108 X-rays (89 incident hip fractures) or\n2150 CTs (80 incident hip fractures) with a 80/20 split were used. We show that\nFORM can correctly predict the 10-year hip fracture risk with a validation AUC\nof 81.44 +- 3.11% / 81.04 +- 5.54% (mean +- STD) including additional\ninformation like age, BMI, fall history and health background across a 5-fold\ncross validation on the X-ray and CT cohort, respectively. Our approach\nsignificantly (p &lt; 0.01) outperforms previous methods like Cox\nProportional-Hazards Model and \\frax with 70.19 +- 6.58 and 74.72 +- 7.21\nrespectively on the X-ray cohort. Our model outperform on both cohorts hip aBMD\nbased predictions. We are confident that FORM can contribute on improving\nosteoporosis diagnosis at an early stage.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schmarje_L/0/1/0/all/0/1\">Lars Schmarje</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reinhold_S/0/1/0/all/0/1\">Stefan Reinhold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Damm_T/0/1/0/all/0/1\">Timo Damm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orwoll_E/0/1/0/all/0/1\">Eric Orwoll</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gluer_C/0/1/0/all/0/1\">Claus-C. Gl&#xfc;er</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koch_R/0/1/0/all/0/1\">Reinhard Koch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Human Kinematics by Modeling Temporal Correlations between Joints for Video-based Human Pose Estimation. (arXiv:2207.10971v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10971","description":"<p>Estimating human poses from videos is critical in human-computer interaction.\nBy precisely estimating human poses, the robot can provide an appropriate\nresponse to the human. Most existing approaches use the optical flow, RNNs, or\nCNNs to extract temporal features from videos. Despite the positive results of\nthese attempts, most of them only straightforwardly integrate features along\nthe temporal dimension, ignoring temporal correlations between joints. In\ncontrast to previous methods, we propose a plug-and-play kinematics modeling\nmodule (KMM) based on the domain-cross attention mechanism to model the\ntemporal correlation between joints across different frames explicitly.\nSpecifically, the proposed KMM models the temporal correlation between any two\njoints by calculating their temporal similarity. In this way, KMM can learn the\nmotion cues of each joint. Using the motion cues (temporal domain) and\nhistorical positions of joints (spatial domain), KMM can infer the initial\npositions of joints in the current frame in advance. In addition, we present a\nkinematics modeling network (KIMNet) based on the KMM for obtaining the final\npositions of joints by combining pose features and initial positions of joints.\nBy explicitly modeling temporal correlations between joints, KIMNet can infer\nthe occluded joints at present according to all joints at the previous moment.\nFurthermore, the KMM is achieved through an attention mechanism, which allows\nit to maintain the high resolution of features. Therefore, it can transfer rich\nhistorical pose information to the current frame, which provides effective pose\ninformation for locating occluded joints. Our approach achieves\nstate-of-the-art results on two standard video-based pose estimation\nbenchmarks. Moreover, the proposed KIMNet shows some robustness to the\nocclusion, demonstrating the effectiveness of the proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dang_Y/0/1/0/all/0/1\">Yonghao Dang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1\">Jianqin Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shaojie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiping Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yanzhu Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NeurAR: Neural Uncertainty for Autonomous 3D Reconstruction. (arXiv:2207.10985v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10985","description":"<p>Implicit neural representations have shown compelling results in offline 3D\nreconstruction and also recently demonstrated the potential for online SLAM\nsystems. However, applying them to autonomous 3D reconstruction, where robots\nare required to explore a scene and plan a view path for the reconstruction,\nhas not been studied. In this paper, we explore for the first time the\npossibility of using implicit neural representations for autonomous 3D scene\nreconstruction by addressing two key challenges: 1) seeking a criterion to\nmeasure the quality of the candidate viewpoints for the view planning based on\nthe new representations, and 2) learning the criterion from data that can\ngeneralize to different scenes instead of hand-crafting one. For the first\nchallenge, a proxy of Peak Signal-to-Noise Ratio (PSNR) is proposed to quantify\na viewpoint quality. The proxy is acquired by treating the color of a spatial\npoint in a scene as a random variable under a Gaussian distribution rather than\na deterministic one; the variance of the distribution quantifies the\nuncertainty of the reconstruction and composes the proxy. For the second\nchallenge, the proxy is optimized jointly with the parameters of an implicit\nneural network for the scene. With the proposed view quality criterion, we can\nthen apply the new representations to autonomous 3D reconstruction. Our method\ndemonstrates significant improvements on various metrics for the rendered image\nquality and the geometry quality of the reconstructed 3D models when compared\nwith variants using TSDF or reconstruction without view planning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ran_Y/0/1/0/all/0/1\">Yunlong Ran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_J/0/1/0/all/0/1\">Jing Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Shibo He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lincheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yingfeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Gimhee Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiming Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1\">Qi Ye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-shot Object Counting and Detection. (arXiv:2207.10988v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10988","description":"<p>We tackle a new task of few-shot object counting and detection. Given a few\nexemplar bounding boxes of a target object class, we seek to count and detect\nall objects of the target class. This task shares the same supervision as the\nfew-shot object counting but additionally outputs the object bounding boxes\nalong with the total object count. To address this challenging problem, we\nintroduce a novel two-stage training strategy and a novel uncertainty-aware\nfew-shot object detector: Counting-DETR. The former is aimed at generating\npseudo ground-truth bounding boxes to train the latter. The latter leverages\nthe pseudo ground-truth provided by the former but takes the necessary steps to\naccount for the imperfection of pseudo ground-truth. To validate the\nperformance of our method on the new task, we introduce two new datasets named\nFSCD-147 and FSCD-LVIS. Both datasets contain images with complex scenes,\nmultiple object classes per image, and a huge variation in object shapes,\nsizes, and appearance. Our proposed approach outperforms very strong baselines\nadapted from few-shot object counting and few-shot object detection with a\nlarge margin in both counting and detection metrics. The code and models are\navailable at \\url{https://github.com/VinAIResearch/Counting-DETR}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Thanh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pham_C/0/1/0/all/0/1\">Chau Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Khoi Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoai_M/0/1/0/all/0/1\">Minh Hoai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Taguchi based Design of Sequential Convolution Neural Network for Classification of Defective Fasteners. (arXiv:2207.10992v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10992","description":"<p>Fasteners play a critical role in securing various parts of machinery.\nDeformations such as dents, cracks, and scratches on the surface of fasteners\nare caused by material properties and incorrect handling of equipment during\nproduction processes. As a result, quality control is required to ensure safe\nand reliable operations. The existing defect inspection method relies on manual\nexamination, which consumes a significant amount of time, money, and other\nresources; also, accuracy cannot be guaranteed due to human error. Automatic\ndefect detection systems have proven impactful over the manual inspection\ntechnique for defect analysis. However, computational techniques such as\nconvolutional neural networks (CNN) and deep learning-based approaches are\nevolutionary methods. By carefully selecting the design parameter values, the\nfull potential of CNN can be realised. Using Taguchi-based design of\nexperiments and analysis, an attempt has been made to develop a robust\nautomatic system in this study. The dataset used to train the system has been\ncreated manually for M14 size nuts having two labeled classes: Defective and\nNon-defective. There are a total of 264 images in the dataset. The proposed\nsequential CNN comes up with a 96.3% validation accuracy, 0.277 validation loss\nat 0.001 learning rate.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kaur_M/0/1/0/all/0/1\">Manjeet Kaur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chauhan_K/0/1/0/all/0/1\">Krishan Kumar Chauhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aggarwal_T/0/1/0/all/0/1\">Tanya Aggarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bharadwaj_P/0/1/0/all/0/1\">Pushkar Bharadwaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vig_R/0/1/0/all/0/1\">Renu Vig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ihianle_I/0/1/0/all/0/1\">Isibor Kennedy Ihianle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_G/0/1/0/all/0/1\">Garima Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Owa_K/0/1/0/all/0/1\">Kayode Owa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Generalized Non-Rigid Multimodal Biomedical Image Registration from Generic Point Set Data. (arXiv:2207.10994v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10994","description":"<p>Free Point Transformer (FPT) has been proposed as a data-driven, non-rigid\npoint set registration approach using deep neural networks. As FPT does not\nassume constraints based on point vicinity or correspondence, it may be trained\nsimply and in a flexible manner by minimizing an unsupervised loss based on the\nChamfer Distance. This makes FPT amenable to real-world medical imaging\napplications where ground-truth deformations may be infeasible to obtain, or in\nscenarios where only a varying degree of completeness in the point sets to be\naligned is available. To test the limit of the correspondence finding ability\nof FPT and its dependency on training data sets, this work explores the\ngeneralizability of the FPT from well-curated non-medical data sets to medical\nimaging data sets. First, we train FPT on the ModelNet40 dataset to demonstrate\nits effectiveness and the superior registration performance of FPT over\niterative and learning-based point set registration methods. Second, we\ndemonstrate superior performance in rigid and non-rigid registration and\nrobustness to missing data. Last, we highlight the interesting generalizability\nof the ModelNet-trained FPT by registering reconstructed freehand ultrasound\nscans of the spine and generic spine models without additional training,\nwhereby the average difference to the ground truth curvatures is 1.3 degrees,\nacross 13 patients.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baum_Z/0/1/0/all/0/1\">Zachary MC Baum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ungi_T/0/1/0/all/0/1\">Tamas Ungi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schlenger_C/0/1/0/all/0/1\">Christopher Schlenger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yipeng Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barratt_D/0/1/0/all/0/1\">Dean C Barratt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Meta-Registration: Learning Test-Time Optimization for Single-Pair Image Registration. (arXiv:2207.10996v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10996","description":"<p>Neural networks have been proposed for medical image registration by\nlearning, with a substantial amount of training data, the optimal\ntransformations between image pairs. These trained networks can further be\noptimized on a single pair of test images - known as test-time optimization.\nThis work formulates image registration as a meta-learning algorithm. Such\nnetworks can be trained by aligning the training image pairs while\nsimultaneously improving test-time optimization efficacy; tasks which were\npreviously considered two independent training and optimization processes. The\nproposed meta-registration is hypothesized to maximize the efficiency and\neffectiveness of the test-time optimization in the \"outer\" meta-optimization of\nthe networks. For image guidance applications that often are time-critical yet\nlimited in training data, the potentially gained speed and accuracy are\ncompared with classical registration algorithms, registration networks without\nmeta-learning, and single-pair optimization without test-time optimization\ndata. Experiments are presented in this paper using clinical transrectal\nultrasound image data from 108 prostate cancer patients. These experiments\ndemonstrate the effectiveness of a meta-registration protocol, which yields\nsignificantly improved performance relative to existing learning-based methods.\nFurthermore, the meta-registration achieves comparable results to classical\niterative methods in a fraction of the time, owing to its rapid test-time\noptimization process.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baum_Z/0/1/0/all/0/1\">Zachary MC Baum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yipeng Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barratt_D/0/1/0/all/0/1\">Dean C Barratt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rapid Lung Ultrasound COVID-19 Severity Scoring with Resource-Efficient Deep Feature Extraction. (arXiv:2207.10998v1 [eess.IV])","link":"http://arxiv.org/abs/2207.10998","description":"<p>Artificial intelligence-based analysis of lung ultrasound imaging has been\ndemonstrated as an effective technique for rapid diagnostic decision support\nthroughout the COVID-19 pandemic. However, such techniques can require days- or\nweeks-long training processes and hyper-parameter tuning to develop intelligent\ndeep learning image analysis models. This work focuses on leveraging\n'off-the-shelf' pre-trained models as deep feature extractors for scoring\ndisease severity with minimal training time. We propose using pre-trained\ninitializations of existing methods ahead of simple and compact neural networks\nto reduce reliance on computational capacity. This reduction of computational\ncapacity is of critical importance in time-limited or resource-constrained\ncircumstances, such as the early stages of a pandemic. On a dataset of 49\npatients, comprising over 20,000 images, we demonstrate that the use of\nexisting methods as feature extractors results in the effective classification\nof COVID-19-related pneumonia severity while requiring only minutes of training\ntime. Our methods can achieve an accuracy of over 0.93 on a 4-level severity\nscore scale and provides comparable per-patient region and global scores\ncompared to expert annotated ground truths. These results demonstrate the\ncapability for rapid deployment and use of such minimally-adapted methods for\nprogress monitoring, patient stratification and management in clinical practice\nfor COVID-19 patients, and potentially in other respiratory diseases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Raillard_P/0/1/0/all/0/1\">Pierre Raillard</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cristoni_L/0/1/0/all/0/1\">Lorenzo Cristoni</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Walden_A/0/1/0/all/0/1\">Andrew Walden</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lazzari_R/0/1/0/all/0/1\">Roberto Lazzari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pulimood_T/0/1/0/all/0/1\">Thomas Pulimood</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Grandjean_L/0/1/0/all/0/1\">Louis Grandjean</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wheeler_Kingshott_C/0/1/0/all/0/1\">Claudia AM Gandini Wheeler-Kingshott</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_Y/0/1/0/all/0/1\">Yipeng Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Baum_Z/0/1/0/all/0/1\">Zachary MC Baum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"POP: Mining POtential Performance of new fashion products via webly cross-modal query expansion. (arXiv:2207.11001v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11001","description":"<p>We propose a data-centric pipeline able to generate exogenous observation\ndata for the New Fashion Product Performance Forecasting (NFPPF) problem, i.e.,\npredicting the performance of a brand-new clothing probe with no available past\nobservations. Our pipeline manufactures the missing past starting from a\nsingle, available image of the clothing probe. It starts by expanding textual\ntags associated with the image, querying related fashionable or unfashionable\nimages uploaded on the web at a specific time in the past. A binary classifier\nis robustly trained on these web images by confident learning, to learn what\nwas fashionable in the past and how much the probe image conforms to this\nnotion of fashionability. This compliance produces the POtential Performance\n(POP) time series, indicating how performing the probe could have been if it\nwere available earlier. POP proves to be highly predictive for the probe's\nfuture performance, ameliorating the sales forecasts of all state-of-the-art\nmodels on the recent VISUELLE fast-fashion dataset. We also show that POP\nreflects the ground-truth popularity of new styles (ensembles of clothing\nitems) on the Fashion Forward benchmark, demonstrating that our webly-learned\nsignal is a truthful expression of popularity, accessible by everyone and\ngeneralizable to any time of analysis. Forecasting code, data and the POP time\nseries are available at:\nhttps://github.com/HumaticsLAB/POP-Mining-POtential-Performance\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Joppi_C/0/1/0/all/0/1\">Christian Joppi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skenderi_G/0/1/0/all/0/1\">Geri Skenderi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cristani_M/0/1/0/all/0/1\">Marco Cristani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fact sheet: Automatic Self-Reported Personality Recognition Track. (arXiv:2207.11012v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11012","description":"<p>We propose an informed baseline to help disentangle the various contextual\nfactors of influence in this type of case studies. For this purpose, we\nanalysed the correlation between the given metadata and the self-assigned\npersonality trait scores and developed a model based solely on this\ninformation. Further, we compared the performance of this informed baseline\nwith models based on state-of-the-art visual, linguistic and audio features.\nFor the present dataset, a model trained solely on simple metadata features\n(age, gender and number of sessions) proved to have superior or similar\nperformance when compared with simple audio, linguistic or visual\nfeatures-based systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pessanha_F/0/1/0/all/0/1\">Francisca Pessanha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sogancioglu_G/0/1/0/all/0/1\">Gizem Sogancioglu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Open video data sharing in developmental and behavioural science. (arXiv:2207.11020v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11020","description":"<p>Video recording is a widely used method for documenting infant and child\nbehaviours in research and clinical practice. Video data has rarely been shared\ndue to ethical concerns of confidentiality, although the need of shared\nlarge-scaled datasets remains increasing. This demand is even more imperative\nwhen data-driven computer-based approaches are involved, such as screening\ntools to complement clinical assessments. To share data while abiding by\nprivacy protection rules, a critical question arises whether efforts at data\nde-identification reduce data utility? We addressed this question by showcasing\nthe Prechtl's general movements assessment (GMA), an established and globally\npractised video-based diagnostic tool in early infancy for detecting\nneurological deficits, such as cerebral palsy. To date, no shared\nexpert-annotated large data repositories for infant movement analyses exist.\nSuch datasets would massively benefit training and recalibration of human\nassessors and the development of computer-based approaches. In the current\nstudy, sequences from a prospective longitudinal infant cohort with a total of\n19451 available general movements video snippets were randomly selected for\nhuman clinical reasoning and computer-based analysis. We demonstrated for the\nfirst time that pseudonymisation by face-blurring video recordings is a viable\napproach. The video redaction did not affect classification accuracy for either\nhuman assessors or computer vision methods, suggesting an adequate and\neasy-to-apply solution for sharing movement video data. We call for further\nexplorations into efficient and privacy rule-conforming approaches for\ndeidentifying video data in scientific and clinical fields beyond movement\nassessments. These approaches shall enable sharing and merging stand-alone\nvideo datasets into large data pools to advance science and public health.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Marschik_P/0/1/0/all/0/1\">Peter B Marschik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulvicius_T/0/1/0/all/0/1\">Tomas Kulvicius</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flugge_S/0/1/0/all/0/1\">Sarah Fl&#xfc;gge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Widmann_C/0/1/0/all/0/1\">Claudius Widmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nielsen_Saines_K/0/1/0/all/0/1\">Karin Nielsen-Saines</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schulte_Ruther_M/0/1/0/all/0/1\">Martin Schulte-R&#xfc;ther</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huning_B/0/1/0/all/0/1\">Britta H&#xfc;ning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bolte_S/0/1/0/all/0/1\">Sven B&#xf6;lte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poustka_L/0/1/0/all/0/1\">Luise Poustka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sigafoos_J/0/1/0/all/0/1\">Jeff Sigafoos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Worgotter_F/0/1/0/all/0/1\">Florentin W&#xf6;rg&#xf6;tter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Einspieler_C/0/1/0/all/0/1\">Christa Einspieler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dajie Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Custom Structure Preservation in Face Aging. (arXiv:2207.11025v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11025","description":"<p>In this work, we propose a novel architecture for face age editing that can\nproduce structural modifications while maintaining relevant details present in\nthe original image. We disentangle the style and content of the input image and\npropose a new decoder network that adopts a style-based strategy to combine the\nstyle and content representations of the input image while conditioning the\noutput on the target age. We go beyond existing aging methods allowing users to\nadjust the degree of structure preservation in the input image during\ninference. To this purpose, we introduce a masking mechanism, the CUstom\nStructure Preservation module, that distinguishes relevant regions in the input\nimage from those that should be discarded. CUSP requires no additional\nsupervision. Finally, our quantitative and qualitative analysis which include a\nuser study, show that our method outperforms prior art and demonstrates the\neffectiveness of our strategy regarding image editing and adjustable structure\npreservation. Code and pretrained models are available at\nhttps://github.com/guillermogotre/CUSP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gomez_Trenado_G/0/1/0/all/0/1\">Guillermo Gomez-Trenado</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Lathuiliere_S/0/1/0/all/0/1\">St&#xe9;phane Lathuili&#xe8;re</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Mesejo_P/0/1/0/all/0/1\">Pablo Mesejo</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Cordon_O/0/1/0/all/0/1\">&#xd3;scar Cord&#xf3;n</a> (1) ((1) DaSCI research institute, DECSAI, University of Granada, Granada, Spain, (2) LTCI, T&#xe9;l&#xe9;com-Paris, Intitute Polytechnique de Paris, Palaiseau, France)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MobileDenseNet: A new approach to object detection on mobile devices. (arXiv:2207.11031v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11031","description":"<p>Object detection problem solving has developed greatly within the past few\nyears. There is a need for lighter models in instances where hardware\nlimitations exist, as well as a demand for models to be tailored to mobile\ndevices. In this article, we will assess the methods used when creating\nalgorithms that address these issues. The main goal of this article is to\nincrease accuracy in state-of-the-art algorithms while maintaining speed and\nreal-time efficiency. The most significant issues in one-stage object detection\npertains to small objects and inaccurate localization. As a solution, we\ncreated a new network by the name of MobileDenseNet suitable for embedded\nsystems. We also developed a light neck FCPNLite for mobile devices that will\naid with the detection of small objects. Our research revealed that very few\npapers cited necks in embedded systems. What differentiates our network from\nothers is our use of concatenation features. A small yet significant change to\nthe head of the network amplified accuracy without increasing speed or limiting\nparameters. In short, our focus on the challenging CoCo and Pascal VOC datasets\nwere 24.8 and 76.8 in percentage terms respectively - a rate higher than that\nrecorded by other state-of-the-art systems thus far. Our network is able to\nincrease accuracy while maintaining real-time efficiency on mobile devices. We\ncalculated operational speed on Pixel 3 (Snapdragon 845) to 22.8 fps. The\nsource code of this research is available on\nhttps://github.com/hajizadeh/MobileDenseNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hajizadeh_M/0/1/0/all/0/1\">Mohammad Hajizadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabokrou_M/0/1/0/all/0/1\">Mohammad Sabokrou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahmani_A/0/1/0/all/0/1\">Adel Rahmani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GesSure -- A Robust Face-Authentication enabled Dynamic Gesture Recognition GUI Application. (arXiv:2207.11033v1 [cs.HC])","link":"http://arxiv.org/abs/2207.11033","description":"<p>Using physical interactive devices like mouse and keyboards hinders\nnaturalistic human-machine interaction and increases the probability of surface\ncontact during a pandemic. Existing gesture-recognition systems do not possess\nuser authentication, making them unreliable. Static gestures in current\ngesture-recognition technology introduce long adaptation periods and reduce\nuser compatibility. Our technology places a strong emphasis on user recognition\nand safety. We use meaningful and relevant gestures for task operation,\nresulting in a better user experience. This paper aims to design a robust,\nface-verification-enabled gesture recognition system that utilizes a graphical\nuser interface and primarily focuses on security through user recognition and\nauthorization. The face model uses MTCNN and FaceNet to verify the user, and\nour LSTM-CNN architecture for gesture recognition, achieving an accuracy of 95%\nwith five classes of gestures. The prototype developed through our research has\nsuccessfully executed context-dependent tasks like save, print, control\nvideo-player operations and exit, and context-free operating system tasks like\nsleep, shut-down, and unlock intuitively. Our application and dataset are\navailable as open source.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jha_A/0/1/0/all/0/1\">Ankit Jha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shenwai_I/0/1/0/all/0/1\">Ishita Pratham G. Shenwai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batra_A/0/1/0/all/0/1\">Ayush Batra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kotian_S/0/1/0/all/0/1\">Siddharth Kotian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Modi_P/0/1/0/all/0/1\">Piyush Modi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Graph Spatio-Spectral Total Variation Model for Hyperspectral Image Denoising. (arXiv:2207.11050v1 [eess.IV])","link":"http://arxiv.org/abs/2207.11050","description":"<p>The spatio-spectral total variation (SSTV) model has been widely used as an\neffective regularization of hyperspectral images (HSI) for various applications\nsuch as mixed noise removal. However, since SSTV computes local spatial\ndifferences uniformly, it is difficult to remove noise while preserving complex\nspatial structures with fine edges and textures, especially in situations of\nhigh noise intensity. To solve this problem, we propose a new TV-type\nregularization called Graph-SSTV (GSSTV), which generates a graph explicitly\nreflecting the spatial structure of the target HSI from noisy HSIs and\nincorporates a weighted spatial difference operator designed based on this\ngraph. Furthermore, we formulate the mixed noise removal problem as a convex\noptimization problem involving GSSTV and develop an efficient algorithm based\non the primal-dual splitting method to solve this problem. Finally, we\ndemonstrate the effectiveness of GSSTV compared with existing HSI\nregularization models through experiments on mixed noise removal. The source\ncode will be available at https://www.mdi.c.titech.ac.jp/publications/gsstv.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Takemoto_S/0/1/0/all/0/1\">Shingo Takemoto</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Naganuma_K/0/1/0/all/0/1\">Kazuki Naganuma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ono_S/0/1/0/all/0/1\">Shunsuke Ono</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D Interacting Hand Pose Estimation by Hand De-occlusion and Removal. (arXiv:2207.11061v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11061","description":"<p>Estimating 3D interacting hand pose from a single RGB image is essential for\nunderstanding human actions. Unlike most previous works that directly predict\nthe 3D poses of two interacting hands simultaneously, we propose to decompose\nthe challenging interacting hand pose estimation task and estimate the pose of\neach hand separately. In this way, it is straightforward to take advantage of\nthe latest research progress on the single-hand pose estimation system.\nHowever, hand pose estimation in interacting scenarios is very challenging, due\nto (1) severe hand-hand occlusion and (2) ambiguity caused by the homogeneous\nappearance of hands. To tackle these two challenges, we propose a novel Hand\nDe-occlusion and Removal (HDR) framework to perform hand de-occlusion and\ndistractor removal. We also propose the first large-scale synthetic amodal hand\ndataset, termed Amodal InterHand Dataset (AIH), to facilitate model training\nand promote the development of the related research. Experiments show that the\nproposed method significantly outperforms previous state-of-the-art interacting\nhand pose estimation approaches. Codes and data are available at\nhttps://github.com/MengHao666/HDR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meng_H/0/1/0/all/0/1\">Hao Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_S/0/1/0/all/0/1\">Sheng Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wentao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1\">Chen Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Mengxiang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1\">Wanli Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RealFlow: EM-based Realistic Optical Flow Dataset Generation from Videos. (arXiv:2207.11075v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11075","description":"<p>Obtaining the ground truth labels from a video is challenging since the\nmanual annotation of pixel-wise flow labels is prohibitively expensive and\nlaborious. Besides, existing approaches try to adapt the trained model on\nsynthetic datasets to authentic videos, which inevitably suffers from domain\ndiscrepancy and hinders the performance for real-world applications. To solve\nthese problems, we propose RealFlow, an Expectation-Maximization based\nframework that can create large-scale optical flow datasets directly from any\nunlabeled realistic videos. Specifically, we first estimate optical flow\nbetween a pair of video frames, and then synthesize a new image from this pair\nbased on the predicted flow. Thus the new image pairs and their corresponding\nflows can be regarded as a new training set. Besides, we design a Realistic\nImage Pair Rendering (RIPR) module that adopts softmax splatting and\nbi-directional hole filling techniques to alleviate the artifacts of the image\nsynthesis. In the E-step, RIPR renders new images to create a large quantity of\ntraining data. In the M-step, we utilize the generated training data to train\nan optical flow network, which can be used to estimate optical flows in the\nnext E-step. During the iterative learning steps, the capability of the flow\nnetwork is gradually improved, so is the accuracy of the flow, as well as the\nquality of the synthesized dataset. Experimental results show that RealFlow\noutperforms previous dataset generation methods by a considerably large margin.\nMoreover, based on the generated dataset, our approach achieves\nstate-of-the-art performance on two standard benchmarks compared with both\nsupervised and unsupervised optical flow methods. Our code and dataset are\navailable at https://github.com/megvii-research/RealFlow\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1\">Yunhui Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_K/0/1/0/all/0/1\">Kunming Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_A/0/1/0/all/0/1\">Ao Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiangyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1\">Haoqiang Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_G/0/1/0/all/0/1\">Guiming Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shuaicheng Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Facial Expression Recognition using Vanilla ViT backbones with MAE Pretraining. (arXiv:2207.11081v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11081","description":"<p>Humans usually convey emotions voluntarily or involuntarily by facial\nexpressions. Automatically recognizing the basic expression (such as happiness,\nsadness, and neutral) from a facial image, i.e., facial expression recognition\n(FER), is extremely challenging and attracts much research interests. Large\nscale datasets and powerful inference models have been proposed to address the\nproblem. Though considerable progress has been made, most of the state of the\narts employing convolutional neural networks (CNNs) or elaborately modified\nVision Transformers (ViTs) depend heavily on upstream supervised pretraining.\nTransformers are taking place the domination of CNNs in more and more computer\nvision tasks. But they usually need much more data to train, since they use\nless inductive biases compared with CNNs. To explore whether a vanilla ViT\nwithout extra training samples from upstream tasks is able to achieve\ncompetitive accuracy, we use a plain ViT with MAE pretraining to perform the\nFER task. Specifically, we first pretrain the original ViT as a Masked\nAutoencoder (MAE) on a large facial expression dataset without expression\nlabels. Then, we fine-tune the ViT on popular facial expression datasets with\nexpression labels. The presented method is quite competitive with 90.22\\% on\nRAF-DB, 61.73\\% on AfectNet and can serve as a simple yet strong ViT-based\nbaseline for FER studies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Ziyang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Speech-Aware Perceptual 3D Facial Expression Reconstruction from Videos. (arXiv:2207.11094v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11094","description":"<p>The recent state of the art on monocular 3D face reconstruction from image\ndata has made some impressive advancements, thanks to the advent of Deep\nLearning. However, it has mostly focused on input coming from a single RGB\nimage, overlooking the following important factors: a) Nowadays, the vast\nmajority of facial image data of interest do not originate from single images\nbut rather from videos, which contain rich dynamic information. b) Furthermore,\nthese videos typically capture individuals in some form of verbal communication\n(public talks, teleconferences, audiovisual human-computer interactions,\ninterviews, monologues/dialogues in movies, etc). When existing 3D face\nreconstruction methods are applied in such videos, the artifacts in the\nreconstruction of the shape and motion of the mouth area are often severe,\nsince they do not match well with the speech audio.\n</p>\n<p>To overcome the aforementioned limitations, we present the first method for\nvisual speech-aware perceptual reconstruction of 3D mouth expressions. We do\nthis by proposing a \"lipread\" loss, which guides the fitting process so that\nthe elicited perception from the 3D reconstructed talking head resembles that\nof the original video footage. We demonstrate that, interestingly, the lipread\nloss is better suited for 3D reconstruction of mouth movements compared to\ntraditional landmark losses, and even direct 3D supervision. Furthermore, the\ndevised method does not rely on any text transcriptions or corresponding audio,\nrendering it ideal for training in unlabeled datasets. We verify the efficiency\nof our method through exhaustive objective evaluations on three large-scale\ndatasets, as well as subjective evaluation with two web-based user studies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Filntisis_P/0/1/0/all/0/1\">Panagiotis P. Filntisis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Retsinas_G/0/1/0/all/0/1\">George Retsinas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paraperas_Papantoniou_F/0/1/0/all/0/1\">Foivos Paraperas-Papantoniou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katsamanis_A/0/1/0/all/0/1\">Athanasios Katsamanis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roussos_A/0/1/0/all/0/1\">Anastasios Roussos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maragos_P/0/1/0/all/0/1\">Petros Maragos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-temporal speckle reduction with self-supervised deep neural networks. (arXiv:2207.11095v1 [eess.IV])","link":"http://arxiv.org/abs/2207.11095","description":"<p>Speckle filtering is generally a prerequisite to the analysis of synthetic\naperture radar (SAR) images. Tremendous progress has been achieved in the\ndomain of single-image despeckling. Latest techniques rely on deep neural\nnetworks to restore the various structures and textures peculiar to SAR images.\nThe availability of time series of SAR images offers the possibility of\nimproving speckle filtering by combining different speckle realizations over\nthe same area. The supervised training of deep neural networks requires\nground-truth speckle-free images. Such images can only be obtained indirectly\nthrough some form of averaging, by spatial or temporal integration, and are\nimperfect. Given the potential of very high quality restoration reachable by\nmulti-temporal speckle filtering, the limitations of ground-truth images need\nto be circumvented. We extend a recent self-supervised training strategy for\nsingle-look complex SAR images, called MERLIN, to the case of multi-temporal\nfiltering. This requires modeling the sources of statistical dependencies in\nthe spatial and temporal dimensions as well as between the real and imaginary\ncomponents of the complex amplitudes. Quantitative analysis on datasets with\nsimulated speckle indicates a clear improvement of speckle reduction when\nadditional SAR images are included. Our method is then applied to stacks of\nTerraSAR-X images and shown to outperform competing multi-temporal speckle\nfiltering approaches. The code of the trained models is made freely available\non the\n$\\href{https://gitlab.telecom-paris.fr/ring/multi-temporal-merlin/}{\\text{GitLab}}$\nof the IMAGES team of the LTCI Lab, T\\'el\\'ecom Paris Institut Polytechnique de\nParis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Meraoumia_I/0/1/0/all/0/1\">In&#xe8;s Meraoumia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dalsasso_E/0/1/0/all/0/1\">Emanuele Dalsasso</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Denis_L/0/1/0/all/0/1\">Lo&#xef;c Denis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Abergel_R/0/1/0/all/0/1\">R&#xe9;my Abergel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tupin_F/0/1/0/all/0/1\">Florence Tupin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-Shot Video Captioning with Evolving Pseudo-Tokens. (arXiv:2207.11100v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11100","description":"<p>We introduce a zero-shot video captioning method that employs two frozen\nnetworks: the GPT-2 language model and the CLIP image-text matching model. The\nmatching score is used to steer the language model toward generating a sentence\nthat has a high average matching score to a subset of the video frames. Unlike\nzero-shot image captioning methods, our work considers the entire sentence at\nonce. This is achieved by optimizing, during the generation process, part of\nthe prompt from scratch, by modifying the representation of all other tokens in\nthe prompt, and by repeating the process iteratively, gradually improving the\nspecificity and comprehensiveness of the generated sentence. Our experiments\nshow that the generated captions are coherent and display a broad range of\nreal-world knowledge. Our code is available at:\nhttps://github.com/YoadTew/zero-shot-video-to-text\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tewel_Y/0/1/0/all/0/1\">Yoad Tewel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shalev_Y/0/1/0/all/0/1\">Yoav Shalev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nadler_R/0/1/0/all/0/1\">Roy Nadler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_I/0/1/0/all/0/1\">Idan Schwartz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_L/0/1/0/all/0/1\">Lior Wolf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Physiology-based simulation of the retinal vasculature enables annotation-free segmentation of OCT angiographs. (arXiv:2207.11102v1 [eess.IV])","link":"http://arxiv.org/abs/2207.11102","description":"<p>Optical coherence tomography angiography (OCTA) can non-invasively image the\neye's circulatory system. In order to reliably characterize the retinal\nvasculature, there is a need to automatically extract quantitative metrics from\nthese images. The calculation of such biomarkers requires a precise semantic\nsegmentation of the blood vessels. However, deep-learning-based methods for\nsegmentation mostly rely on supervised training with voxel-level annotations,\nwhich are costly to obtain. In this work, we present a pipeline to synthesize\nlarge amounts of realistic OCTA images with intrinsically matching ground truth\nlabels; thereby obviating the need for manual annotation of training data. Our\nproposed method is based on two novel components: 1) a physiology-based\nsimulation that models the various retinal vascular plexuses and 2) a suite of\nphysics-based image augmentations that emulate the OCTA image acquisition\nprocess including typical artifacts. In extensive benchmarking experiments, we\ndemonstrate the utility of our synthetic data by successfully training retinal\nvessel segmentation algorithms. Encouraged by our method's competitive\nquantitative and superior qualitative performance, we believe that it\nconstitutes a versatile tool to advance the quantitative analysis of OCTA\nimages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Menten_M/0/1/0/all/0/1\">Martin J. Menten</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Paetzold_J/0/1/0/all/0/1\">Johannes C. Paetzold</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dima_A/0/1/0/all/0/1\">Alina Dima</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Menze_B/0/1/0/all/0/1\">Bjoern H. Menze</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Knier_B/0/1/0/all/0/1\">Benjamin Knier</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rueckert_D/0/1/0/all/0/1\">Daniel Rueckert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeVIS: Making Deformable Transformers Work for Video Instance Segmentation. (arXiv:2207.11103v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11103","description":"<p>Video Instance Segmentation (VIS) jointly tackles multi-object detection,\ntracking, and segmentation in video sequences. In the past, VIS methods\nmirrored the fragmentation of these subtasks in their architectural design,\nhence missing out on a joint solution. Transformers recently allowed to cast\nthe entire VIS task as a single set-prediction problem. Nevertheless, the\nquadratic complexity of existing Transformer-based methods requires long\ntraining times, high memory requirements, and processing of low-single-scale\nfeature maps. Deformable attention provides a more efficient alternative but\nits application to the temporal domain or the segmentation task have not yet\nbeen explored.\n</p>\n<p>In this work, we present Deformable VIS (DeVIS), a VIS method which\ncapitalizes on the efficiency and performance of deformable Transformers. To\nreason about all VIS subtasks jointly over multiple frames, we present temporal\nmulti-scale deformable attention with instance-aware object queries. We further\nintroduce a new image and video instance mask head with multi-scale features,\nand perform near-online video processing with multi-cue clip tracking. DeVIS\nreduces memory as well as training time requirements, and achieves\nstate-of-the-art results on the YouTube-VIS 2021, as well as the challenging\nOVIS dataset.\n</p>\n<p>Code is available at https://github.com/acaelles97/DeVIS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Caelles_A/0/1/0/all/0/1\">Adri&#xe0; Caelles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meinhardt_T/0/1/0/all/0/1\">Tim Meinhardt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Braso_G/0/1/0/all/0/1\">Guillem Bras&#xf3;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leal_Taixe_L/0/1/0/all/0/1\">Laura Leal-Taix&#xe9;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast strategies for multi-temporal speckle reduction of Sentinel-1 GRD images. (arXiv:2207.11111v1 [eess.IV])","link":"http://arxiv.org/abs/2207.11111","description":"<p>Reducing speckle and limiting the variations of the physical parameters in\nSynthetic Aperture Radar (SAR) images is often a key-step to fully exploit the\npotential of such data. Nowadays, deep learning approaches produce state of the\nart results in single-image SAR restoration. Nevertheless, huge multi-temporal\nstacks are now often available and could be efficiently exploited to further\nimprove image quality. This paper explores two fast strategies employing a\nsingle-image despeckling algorithm, namely SAR2SAR, in a multi-temporal\nframework. The first one is based on Quegan filter and replaces the local\nreflectivity pre-estimation by SAR2SAR. The second one uses SAR2SAR to suppress\nspeckle from a ratio image encoding the multi-temporal information under the\nform of a \"super-image\", i.e. the temporal arithmetic mean of a time series.\nExperimental results on Sentinel-1 GRD data show that these two multi-temporal\nstrategies provide improved filtering results while adding a limited\ncomputational cost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Meraoumia_I/0/1/0/all/0/1\">In&#xe8;s Meraoumia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dalsasso_E/0/1/0/all/0/1\">Emanuele Dalsasso</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Denis_L/0/1/0/all/0/1\">Lo&#xef;c Denis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tupin_F/0/1/0/all/0/1\">Florence Tupin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking the Reference-based Distinctive Image Captioning. (arXiv:2207.11118v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11118","description":"<p>Distinctive Image Captioning (DIC) -- generating distinctive captions that\ndescribe the unique details of a target image -- has received considerable\nattention over the last few years. A recent DIC work proposes to generate\ndistinctive captions by comparing the target image with a set of\nsemantic-similar reference images, i.e., reference-based DIC (Ref-DIC). It aims\nto make the generated captions can tell apart the target and reference images.\nUnfortunately, reference images used by existing Ref-DIC works are easy to\ndistinguish: these reference images only resemble the target image at\nscene-level and have few common objects, such that a Ref-DIC model can\ntrivially generate distinctive captions even without considering the reference\nimages. To ensure Ref-DIC models really perceive the unique objects (or\nattributes) in target images, we first propose two new Ref-DIC benchmarks.\nSpecifically, we design a two-stage matching mechanism, which strictly controls\nthe similarity between the target and reference images at object-/attribute-\nlevel (vs. scene-level). Secondly, to generate distinctive captions, we develop\na strong Transformer-based Ref-DIC baseline, dubbed as TransDIC. It not only\nextracts visual features from the target image, but also encodes the\ndifferences between objects in the target and reference images. Finally, for\nmore trustworthy benchmarking, we propose a new evaluation metric named\nDisCIDEr for Ref-DIC, which evaluates both the accuracy and distinctiveness of\nthe generated captions. Experimental results demonstrate that our TransDIC can\ngenerate distinctive captions. Besides, it outperforms several state-of-the-art\nmodels on the two new benchmarks over different metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1\">Yangjun Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Long Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zhihong Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhimeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1\">Jian Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jun Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Graph-Based Feature Normalization for Facial Expression Recognition. (arXiv:2207.11123v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11123","description":"<p>Facial Expression Recognition (FER) suffers from data uncertainties caused by\nambiguous facial images and annotators' subjectiveness, resulting in excursive\nsemantic and feature covariate shifting problem. Existing works usually correct\nmislabeled data by estimating noise distribution, or guide network training\nwith knowledge learned from clean data, neglecting the associative relations of\nexpressions. In this work, we propose an Adaptive Graph-based Feature\nNormalization (AGFN) method to protect FER models from data uncertainties by\nnormalizing feature distributions with the association of expressions.\nSpecifically, we propose a Poisson graph generator to adaptively construct\ntopological graphs for samples in each mini-batches via a sampling process, and\ncorrespondingly design a coordinate descent strategy to optimize proposed\nnetwork. Our method outperforms state-of-the-art works with accuracies of\n91.84% and 91.11% on the benchmark datasets FERPlus and RAF-DB, respectively,\nand when the percentage of mislabeled data increases (e.g., to 20%), our\nnetwork surpasses existing works significantly by 3.38% and 4.52%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yangtao Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qingqing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1\">Yujie Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VTrackIt: A Synthetic Self-Driving Dataset with Infrastructure and Pooled Vehicle Information. (arXiv:2207.11146v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11146","description":"<p>Artificial intelligence solutions for Autonomous Vehicles (AVs) have been\ndeveloped using publicly available datasets such as Argoverse, ApolloScape,\nLevel5, and NuScenes. One major limitation of these datasets is the absence of\ninfrastructure and/or pooled vehicle information like lane line type, vehicle\nspeed, traffic signs, and intersections. Such information is necessary and not\ncomplementary to eliminating high-risk edge cases. The rapid advancements in\nVehicle-to-Infrastructure and Vehicle-to-Vehicle technologies show promise that\ninfrastructure and pooled vehicle information will soon be accessible in near\nreal-time. Taking a leap in the future, we introduce the first comprehensive\nsynthetic dataset with intelligent infrastructure and pooled vehicle\ninformation for advancing the next generation of AVs, named VTrackIt. We also\nintroduce the first deep learning model (InfraGAN) for trajectory predictions\nthat considers such information. Our experiments with InfraGAN show that the\ncomprehensive information offered by VTrackIt reduces the number of high-risk\nedge cases. The VTrackIt dataset is available upon request under the Creative\nCommons CC BY-NC-SA 4.0 license at <a href=\"http://vtrackit.irda.club.\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Savargaonkar_M/0/1/0/all/0/1\">Mayuresh Savargaonkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chehade_A/0/1/0/all/0/1\">Abdallah Chehade</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"InfiniteNature-Zero: Learning Perpetual View Generation of Natural Scenes from Single Images. (arXiv:2207.11148v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11148","description":"<p>We present a method for learning to generate unbounded flythrough videos of\nnatural scenes starting from a single view, where this capability is learned\nfrom a collection of single photographs, without requiring camera poses or even\nmultiple views of each scene. To achieve this, we propose a novel\nself-supervised view generation training paradigm, where we sample and\nrendering virtual camera trajectories, including cyclic ones, allowing our\nmodel to learn stable view generation from a collection of single views. At\ntest time, despite never seeing a video during training, our approach can take\na single image and generate long camera trajectories comprised of hundreds of\nnew views with realistic and diverse content. We compare our approach with\nrecent state-of-the-art supervised view generation methods that require posed\nmulti-view videos and demonstrate superior performance and synthesis quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhengqi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qianqian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snavely_N/0/1/0/all/0/1\">Noah Snavely</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanazawa_A/0/1/0/all/0/1\">Angjoo Kanazawa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Soft Contrastive Learning. (arXiv:2207.11163v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11163","description":"<p>Self-supervised learning has recently achieved great success in\nrepresentation learning without human annotations. The dominant method -- that\nis contrastive learning, is generally based on instance discrimination tasks,\ni.e., individual samples are treated as independent categories. However,\npresuming all the samples are different contradicts the natural grouping of\nsimilar samples in common visual datasets, e.g., multiple views of the same\ndog. To bridge the gap, this paper proposes an adaptive method that introduces\nsoft inter-sample relations, namely Adaptive Soft Contrastive Learning (ASCL).\nMore specifically, ASCL transforms the original instance discrimination task\ninto a multi-instance soft discrimination task, and adaptively introduces\ninter-sample relations. As an effective and concise plug-in module for existing\nself-supervised learning frameworks, ASCL achieves the best performance on\nseveral benchmarks in terms of both performance and efficiency. Code is\navailable at https://github.com/MrChenFeng/ASCL_ICPR2022.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_C/0/1/0/all/0/1\">Chen Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patras_I/0/1/0/all/0/1\">Ioannis Patras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"METER-ML: A Multi-sensor Earth Observation Benchmark for Automated Methane Source Mapping. (arXiv:2207.11166v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11166","description":"<p>Reducing methane emissions is essential for mitigating global warming. To\nattribute methane emissions to their sources, a comprehensive dataset of\nmethane source infrastructure is necessary. Recent advancements with deep\nlearning on remotely sensed imagery have the potential to identify the\nlocations and characteristics of methane sources, but there is a substantial\nlack of publicly available data to enable machine learning researchers and\npractitioners to build automated mapping approaches. To help fill this gap, we\nconstruct a multi-sensor dataset called METER-ML containing 86,625\ngeoreferenced NAIP, Sentinel-1, and Sentinel-2 images in the U.S. labeled for\nthe presence or absence of methane source facilities including concentrated\nanimal feeding operations, coal mines, landfills, natural gas processing\nplants, oil refineries and petroleum terminals, and wastewater treatment\nplants. We experiment with a variety of models that leverage different spatial\nresolutions, spatial footprints, image products, and spectral bands. We find\nthat our best model achieves an area under the precision recall curve of 0.915\nfor identifying concentrated animal feeding operations and 0.821 for oil\nrefineries and petroleum terminals on an expert-labeled test set, suggesting\nthe potential for large-scale mapping. We make METER-ML freely available at\nhttps://stanfordmlgroup.github.io/projects/meter-ml/ to support future work on\nautomated methane source mapping.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1\">Bryan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lui_N/0/1/0/all/0/1\">Nicholas Lui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Irvin_J/0/1/0/all/0/1\">Jeremy Irvin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_J/0/1/0/all/0/1\">Jimmy Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tadwalkar_S/0/1/0/all/0/1\">Sahil Tadwalkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chenghao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_Z/0/1/0/all/0/1\">Zutao Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Frankie Y. Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_A/0/1/0/all/0/1\">Andrew Y. Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jackson_R/0/1/0/all/0/1\">Robert B. Jackson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Few-Shot Object Detection on a Multi-Domain Benchmark. (arXiv:2207.11169v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11169","description":"<p>Most existing works on few-shot object detection (FSOD) focus on a setting\nwhere both pre-training and few-shot learning datasets are from a similar\ndomain. However, few-shot algorithms are important in multiple domains; hence\nevaluation needs to reflect the broad applications. We propose a Multi-dOmain\nFew-Shot Object Detection (MoFSOD) benchmark consisting of 10 datasets from a\nwide range of domains to evaluate FSOD algorithms. We comprehensively analyze\nthe impacts of freezing layers, different architectures, and different\npre-training datasets on FSOD performance. Our empirical results show several\nkey factors that have not been explored in previous works: 1) contrary to\nprevious belief, on a multi-domain benchmark, fine-tuning (FT) is a strong\nbaseline for FSOD, performing on par or better than the state-of-the-art (SOTA)\nalgorithms; 2) utilizing FT as the baseline allows us to explore multiple\narchitectures, and we found them to have a significant impact on down-stream\nfew-shot tasks, even with similar pre-training performances; 3) by decoupling\npre-training and few-shot learning, MoFSOD allows us to explore the impact of\ndifferent pre-training datasets, and the right choice can boost the performance\nof the down-stream tasks significantly. Based on these findings, we list\npossible avenues of investigation for improving FSOD performance and propose\ntwo simple modifications to existing algorithms that lead to SOTA performance\non the MoFSOD benchmark. The code is available at\nhttps://github.com/amazon-research/few-shot-object-detection-benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kibok Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_S/0/1/0/all/0/1\">Satyaki Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1\">Zhaowei Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swaminathan_G/0/1/0/all/0/1\">Gurumurthy Swaminathan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravichandran_A/0/1/0/all/0/1\">Avinash Ravichandran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dabeer_O/0/1/0/all/0/1\">Onkar Dabeer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Training Certifiably Robust Neural Networks Against Semantic Perturbations. (arXiv:2207.11177v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11177","description":"<p>Semantic image perturbations, such as scaling and rotation, have been shown\nto easily deceive deep neural networks (DNNs). Hence, training DNNs to be\ncertifiably robust to these perturbations is critical. However, no prior work\nhas been able to incorporate the objective of deterministic semantic robustness\ninto the training procedure, as existing deterministic semantic verifiers are\nexceedingly slow. To address these challenges, we propose Certified Semantic\nTraining (CST), the first training framework for deterministic certified\nrobustness against semantic image perturbations. Our framework leverages a\nnovel GPU-optimized verifier that, unlike existing works, is fast enough for\nuse in training. Our results show that networks trained via CST consistently\nachieve both better provable semantic robustness and clean accuracy, compared\nto networks trained via baselines based on existing works.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1\">Rem Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laurel_J/0/1/0/all/0/1\">Jacob Laurel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Misailovic_S/0/1/0/all/0/1\">Sasa Misailovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_G/0/1/0/all/0/1\">Gagandeep Singh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Faceted Distillation of Base-Novel Commonality for Few-shot Object Detection. (arXiv:2207.11184v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11184","description":"<p>Most of existing methods for few-shot object detection follow the fine-tuning\nparadigm, which potentially assumes that the class-agnostic generalizable\nknowledge can be learned and transferred implicitly from base classes with\nabundant samples to novel classes with limited samples via such a two-stage\ntraining strategy. However, it is not necessarily true since the object\ndetector can hardly distinguish between class-agnostic knowledge and\nclass-specific knowledge automatically without explicit modeling. In this work\nwe propose to learn three types of class-agnostic commonalities between base\nand novel classes explicitly: recognition-related semantic commonalities,\nlocalization-related semantic commonalities and distribution commonalities. We\ndesign a unified distillation framework based on a memory bank, which is able\nto perform distillation of all three types of commonalities jointly and\nefficiently. Extensive experiments demonstrate that our method can be readily\nintegrated into most of existing fine-tuning based methods and consistently\nimprove the performance by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shuang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pei_W/0/1/0/all/0/1\">Wenjie Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_D/0/1/0/all/0/1\">Dianwen Mei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Fanglin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1\">Jiandong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_G/0/1/0/all/0/1\">Guangming Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to identify cracks on wind turbine blade surfaces using drone-based inspection images. (arXiv:2207.11186v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11186","description":"<p>Wind energy is expected to be one of the leading ways to achieve the goals of\nthe Paris Agreement but it in turn heavily depends on effective management of\nits operations and maintenance (O&amp;M) costs. Blade failures account for\none-third of all O&amp;M costs thus making accurate detection of blade damages,\nespecially cracks, very important for sustained operations and cost savings.\nTraditionally, damage inspection has been a completely manual process thus\nmaking it subjective, error-prone, and time-consuming. Hence in this work, we\nbring more objectivity, scalability, and repeatability in our damage inspection\nprocess, using deep learning, to miss fewer cracks. We build a deep learning\nmodel trained on a large dataset of blade damages, collected by our drone-based\ninspection, to correctly detect cracks. Our model is already in production and\nhas processed more than a million damages with a recall of 0.96. We also focus\non model interpretability using class activation maps to get a peek into the\nmodel workings. The model not only performs as good as human experts but also\nbetter in certain tricky cases. Thus, in this work, we aim to increase wind\nenergy adoption by decreasing one of its major hurdles - the O\\&amp;M costs\nresulting from missing blade failures like cracks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Iyer_A/0/1/0/all/0/1\">Akshay Iyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_L/0/1/0/all/0/1\">Linh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khushu_S/0/1/0/all/0/1\">Shweta Khushu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised-RCNN for Medical Image Segmentation with Limited Data Annotation. (arXiv:2207.11191v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11191","description":"<p>Many successful methods developed for medical image analysis that are based\non machine learning use supervised learning approaches, which often require\nlarge datasets annotated by experts to achieve high accuracy. However, medical\ndata annotation is time-consuming and expensive, especially for segmentation\ntasks. To solve the problem of learning with limited labeled medical image\ndata, an alternative deep learning training strategy based on self-supervised\npretraining on unlabeled MRI scans is proposed in this work. Our pretraining\napproach first, randomly applies different distortions to random areas of\nunlabeled images and then predicts the type of distortions and loss of\ninformation. To this aim, an improved version of Mask-RCNN architecture has\nbeen adapted to localize the distortion location and recover the original image\npixels. The effectiveness of the proposed method for segmentation tasks in\ndifferent pre-training and fine-tuning scenarios is evaluated based on the\nOsteoarthritis Initiative dataset. Using this self-supervised pretraining\nmethod improved the Dice score by 20% compared to training from scratch. The\nproposed self-supervised learning is simple, effective, and suitable for\ndifferent ranges of medical image analysis tasks including anomaly detection,\nsegmentation, and classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Felfeliyan_B/0/1/0/all/0/1\">Banafshe Felfeliyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hareendranathan_A/0/1/0/all/0/1\">Abhilash Hareendranathan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuntze_G/0/1/0/all/0/1\">Gregor Kuntze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cornell_D/0/1/0/all/0/1\">David Cornell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Forkert_N/0/1/0/all/0/1\">Nils D. Forkert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaremko_J/0/1/0/all/0/1\">Jacob L. Jaremko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ronsky_J/0/1/0/all/0/1\">Janet L. Ronsky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Progressive Deblurring of Diffusion Models for Coarse-to-Fine Image Synthesis. (arXiv:2207.11192v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11192","description":"<p>Recently, diffusion models have shown remarkable results in image synthesis\nby gradually removing noise and amplifying signals. Although the simple\ngenerative process surprisingly works well, is this the best way to generate\nimage data? For instance, despite the fact that human perception is more\nsensitive to the low frequencies of an image, diffusion models themselves do\nnot consider any relative importance of each frequency component. Therefore, to\nincorporate the inductive bias for image data, we propose a novel generative\nprocess that synthesizes images in a coarse-to-fine manner. First, we\ngeneralize the standard diffusion models by enabling diffusion in a rotated\ncoordinate system with different velocities for each component of the vector.\nWe further propose a blur diffusion as a special case, where each frequency\ncomponent of an image is diffused at different speeds. Specifically, the\nproposed blur diffusion consists of a forward process that blurs an image and\nadds noise gradually, after which a corresponding reverse process deblurs an\nimage and removes noise progressively. Experiments show that the proposed model\noutperforms the previous method in FID on LSUN bedroom and church datasets.\nCode is available at https://github.com/sangyun884/blur-diffusion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sangyun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_H/0/1/0/all/0/1\">Hyungjin Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jaehyeon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jong Chul Ye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Target-Driven Structured Transformer Planner for Vision-Language Navigation. (arXiv:2207.11201v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11201","description":"<p>Vision-language navigation is the task of directing an embodied agent to\nnavigate in 3D scenes with natural language instructions. For the agent,\ninferring the long-term navigation target from visual-linguistic clues is\ncrucial for reliable path planning, which, however, has rarely been studied\nbefore in literature. In this article, we propose a Target-Driven Structured\nTransformer Planner (TD-STP) for long-horizon goal-guided and room layout-aware\nnavigation. Specifically, we devise an Imaginary Scene Tokenization mechanism\nfor explicit estimation of the long-term target (even located in unexplored\nenvironments). In addition, we design a Structured Transformer Planner which\nelegantly incorporates the explored room layout into a neural attention\narchitecture for structured and global planning. Experimental results\ndemonstrate that our TD-STP substantially improves previous best methods'\nsuccess rate by 2% and 5% on the test set of R2R and REVERIE benchmarks,\nrespectively. Our code is available at https://github.com/YushengZhao/TD-STP .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yusheng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jinyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Chen Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenguan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lirong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1\">Haibing Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_H/0/1/0/all/0/1\">Huaxia Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Si Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Human Treelike Tubular Structure Segmentation: A Comprehensive Review and Future Perspectives. (arXiv:2207.11203v1 [eess.IV])","link":"http://arxiv.org/abs/2207.11203","description":"<p>Various structures in human physiology follow a treelike morphology, which\noften expresses complexity at very fine scales. Examples of such structures are\nintrathoracic airways, retinal blood vessels, and hepatic blood vessels. Large\ncollections of 2D and 3D images have been made available by medical imaging\nmodalities such as magnetic resonance imaging (MRI), computed tomography (CT),\nOptical coherence tomography (OCT) and ultrasound in which the spatial\narrangement can be observed. Segmentation of these structures in medical\nimaging is of great importance since the analysis of the structure provides\ninsights into disease diagnosis, treatment planning, and prognosis. Manually\nlabelling extensive data by radiologists is often time-consuming and\nerror-prone. As a result, automated or semi-automated computational models have\nbecome a popular research field of medical imaging in the past two decades, and\nmany have been developed to date. In this survey, we aim to provide a\ncomprehensive review of currently publicly available datasets, segmentation\nalgorithms, and evaluation metrics. In addition, current challenges and future\nresearch directions are discussed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tang_Z/0/1/0/all/0/1\">Zeyu Tang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nan_Y/0/1/0/all/0/1\">Yang Nan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_G/0/1/0/all/0/1\">Guang Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Divide and Conquer: 3D Point Cloud Instance Segmentation With Point-Wise Binarization. (arXiv:2207.11209v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11209","description":"<p>Instance segmentation on point clouds is crucially important for 3D scene\nunderstanding. Distance clustering is commonly used in state-of-the-art methods\n(SOTAs), which is typically effective but does not perform well in segmenting\nadjacent objects with the same semantic label (especially when they share\nneighboring points). Due to the uneven distribution of offset points, these\nexisting methods can hardly cluster all instance points. To this end, we design\na novel divide and conquer strategy and propose an end-to-end network named\nPBNet that binarizes each point and clusters them separately to segment\ninstances. PBNet divides offset instance points into two categories: high and\nlow density points (HPs vs.LPs), which are then conquered separately. Adjacent\nobjects can be clearly separated by removing LPs, and then be completed and\nrefined by assigning LPs via a neighbor voting method. To further reduce\nclustering errors, we develop an iterative merging algorithm based on mean size\nto aggregate fragment instances. Experiments on ScanNetV2 and S3DIS datasets\nindicate the superiority of our model. In particular, PBNet achieves so far the\nbest AP50 and AP25 on the ScanNetV2 official benchmark challenge (Validation\nSet) while demonstrating high efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Weiguang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yuyao Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chaolong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jianan Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kaizhu Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Predictive Performance and Calibration by Weight Fusion in Semantic Segmentation. (arXiv:2207.11211v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11211","description":"<p>Averaging predictions of a deep ensemble of networks is apopular and\neffective method to improve predictive performance andcalibration in various\nbenchmarks and Kaggle competitions. However, theruntime and training cost of\ndeep ensembles grow linearly with the size ofthe ensemble, making them\nunsuitable for many applications. Averagingensemble weights instead of\npredictions circumvents this disadvantageduring inference and is typically\napplied to intermediate checkpoints ofa model to reduce training cost. Albeit\neffective, only few works haveimproved the understanding and the performance of\nweight averaging.Here, we revisit this approach and show that a simple weight\nfusion (WF)strategy can lead to a significantly improved predictive performance\nandcalibration. We describe what prerequisites the weights must meet interms of\nweight space, functional space and loss. Furthermore, we presenta new test\nmethod (called oracle test) to measure the functional spacebetween weights. We\ndemonstrate the versatility of our WF strategy acrossstate of the art\nsegmentation CNNs and Transformers as well as real worlddatasets such as\nBDD100K and Cityscapes. We compare WF with similarapproaches and show our\nsuperiority for in- and out-of-distribution datain terms of predictive\nperformance and calibration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Samann_T/0/1/0/all/0/1\">Timo S&#xe4;mann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hammam_A/0/1/0/all/0/1\">Ahmed Mostafa Hammam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bursuc_A/0/1/0/all/0/1\">Andrei Bursuc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stiller_C/0/1/0/all/0/1\">Christoph Stiller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gross_H/0/1/0/all/0/1\">Horst-Michael Gro&#xdf;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Target Identification and Bayesian Model Averaging with Probabilistic Hierarchical Factor Probabilities. (arXiv:2207.11212v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11212","description":"<p>Target detection in hyperspectral imagery is the process of locating pixels\nfrom an image which are likely to contain target, typically done by comparing\none or more spectra for the desired target material to each pixel in the image.\nTarget identification is the process of target detection incorporating an\nadditional process to identify more specifically the material that is present\nin each pixel that scored high in detection. Detection is generally a 2-class\nproblem of target vs. background, and identification is a many class problem\nincluding target, background, and additional know materials. The identification\nprocess we present is probabilistic and hierarchical which provides\ntransparency to the process and produces trustworthy output. In this paper we\nshow that target identification has a much lower false alarm rate than\ndetection alone, and provide a detailed explanation of a robust identification\nmethod using probabilistic hierarchical classification that handles the vague\ncategories of materials that depend on users which are different than the\nspecific physical categories of chemical constituents. Identification is often\ndone by comparing mixtures of materials including the target spectra to\nmixtures of materials that do not include the target spectra, possibly with\nother steps. (band combinations, feature checking, background removal, etc.)\nStandard linear regression does not handle these problems well because the\nnumber of regressors (identification spectra) is greater than the number of\nfeature variables (bands), and there are multiple correlated spectra. Our\nproposed method handles these challenges efficiently and provides additional\nimportant practical information in the form of hierarchical probabilities\ncomputed from Bayesian model averaging.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Basener_W/0/1/0/all/0/1\">William Basener</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-Shot Class-Incremental Learning via Entropy-Regularized Data-Free Replay. (arXiv:2207.11213v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11213","description":"<p>Few-shot class-incremental learning (FSCIL) has been proposed aiming to\nenable a deep learning system to incrementally learn new classes with limited\ndata. Recently, a pioneer claims that the commonly used replay-based method in\nclass-incremental learning (CIL) is ineffective and thus not preferred for\nFSCIL. This has, if truth, a significant influence on the fields of FSCIL. In\nthis paper, we show through empirical results that adopting the data replay is\nsurprisingly favorable. However, storing and replaying old data can lead to a\nprivacy concern. To address this issue, we alternatively propose using\ndata-free replay that can synthesize data by a generator without accessing real\ndata. In observing the the effectiveness of uncertain data for knowledge\ndistillation, we impose entropy regularization in the generator training to\nencourage more uncertain examples. Moreover, we propose to relabel the\ngenerated data with one-hot-like labels. This modification allows the network\nto learn by solely minimizing the cross-entropy loss, which mitigates the\nproblem of balancing different objectives in the conventional knowledge\ndistillation approach. Finally, we show extensive experimental results and\nanalysis on CIFAR-100, miniImageNet and CUB-200 to demonstrate the\neffectiveness of our proposed one.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Huan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_L/0/1/0/all/0/1\">Li Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chi_Z/0/1/0/all/0/1\">Zhixiang Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yuanhao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jin Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain Generalization for Activity Recognition via Adaptive Feature Fusion. (arXiv:2207.11221v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11221","description":"<p>Human activity recognition requires the efforts to build a generalizable\nmodel using the training datasets with the hope to achieve good performance in\ntest datasets. However, in real applications, the training and testing datasets\nmay have totally different distributions due to various reasons such as\ndifferent body shapes, acting styles, and habits, damaging the model's\ngeneralization performance. While such a distribution gap can be reduced by\nexisting domain adaptation approaches, they typically assume that the test data\ncan be accessed in the training stage, which is not realistic. In this paper,\nwe consider a more practical and challenging scenario: domain-generalized\nactivity recognition (DGAR) where the test dataset \\emph{cannot} be accessed\nduring training. To this end, we propose \\emph{Adaptive Feature Fusion for\nActivity Recognition~(AFFAR)}, a domain generalization approach that learns to\nfuse the domain-invariant and domain-specific representations to improve the\nmodel's generalization performance. AFFAR takes the best of both worlds where\ndomain-invariant representations enhance the transferability across domains and\ndomain-specific representations leverage the model discrimination power from\neach domain. Extensive experiments on three public HAR datasets show its\neffectiveness. Furthermore, we apply AFFAR to a real application, i.e., the\ndiagnosis of Children's Attention Deficit Hyperactivity Disorder~(ADHD), which\nalso demonstrates the superiority of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qin_X/0/1/0/all/0/1\">Xin Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jindong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yiqiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Wang Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xinlong Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Forest and Water Bodies Segmentation Through Satellite Images Using U-Net. (arXiv:2207.11222v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11222","description":"<p>Global environment monitoring is a task that requires additional attention in\nthe contemporary rapid climate change environment. This includes monitoring the\nrate of deforestation and areas affected by flooding. Satellite imaging has\ngreatly helped monitor the earth, and deep learning techniques have helped to\nautomate this monitoring process. This paper proposes a solution for observing\nthe area covered by the forest and water. To achieve this task UNet model has\nbeen proposed, which is an image segmentation model. The model achieved a\nvalidation accuracy of 82.55% and 82.92% for the segmentation of areas covered\nby forest and water, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Filatov_D/0/1/0/all/0/1\">Dmytro Filatov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yar_G/0/1/0/all/0/1\">Ghulam Nabi Ahmad Hassan Yar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improved $\\alpha$-GAN architecture for generating 3D connected volumes with an application to radiosurgery treatment planning. (arXiv:2207.11223v1 [eess.IV])","link":"http://arxiv.org/abs/2207.11223","description":"<p>Generative Adversarial Networks (GANs) have gained significant attention in\nseveral computer vision tasks for generating high-quality synthetic data.\nVarious medical applications including diagnostic imaging and radiation therapy\ncan benefit greatly from synthetic data generation due to data scarcity in the\ndomain. However, medical image data is typically kept in 3D space, and\ngenerative models suffer from the curse of dimensionality issues in generating\nsuch synthetic data. In this paper, we investigate the potential of GANs for\ngenerating connected 3D volumes. We propose an improved version of 3D\n$\\alpha$-GAN by incorporating various architectural enhancements. On a\nsynthetic dataset of connected 3D spheres and ellipsoids, our model can\ngenerate fully connected 3D shapes with similar geometrical characteristics to\nthat of training data. We also show that our 3D GAN model can successfully\ngenerate high-quality 3D tumor volumes and associated treatment specifications\n(e.g., isocenter locations). Similar moment invariants to the training data as\nwell as fully connected 3D shapes confirm that improved 3D $\\alpha$-GAN\nimplicitly learns the training data distribution, and generates\nrealistic-looking samples. The capability of improved 3D $\\alpha$-GAN makes it\na valuable source for generating synthetic medical image data that can help\nfuture research in this domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Mohammadjafari_S/0/1/0/all/0/1\">Sanaz Mohammadjafari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cevik_M/0/1/0/all/0/1\">Mucahit Cevik</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Basar_A/0/1/0/all/0/1\">Ayse Basar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large-Kernel Attention for 3D Medical Image Segmentation. (arXiv:2207.11225v1 [eess.IV])","link":"http://arxiv.org/abs/2207.11225","description":"<p>Automatic segmentation of multiple organs and tumors from 3D medical images\nsuch as magnetic resonance imaging (MRI) and computed tomography (CT) scans\nusing deep learning methods can aid in diagnosing and treating cancer. However,\norgans often overlap and are complexly connected, characterized by extensive\nanatomical variation and low contrast. In addition, the diversity of tumor\nshape, location, and appearance, coupled with the dominance of background\nvoxels, makes accurate 3D medical image segmentation difficult. In this paper,\na novel large-kernel (LK) attention module is proposed to address these\nproblems to achieve accurate multi-organ segmentation and tumor segmentation.\nThe advantages of convolution and self-attention are combined in the proposed\nLK attention module, including local contextual information, long-range\ndependence, and channel adaptation. The module also decomposes the LK\nconvolution to optimize the computational cost and can be easily incorporated\ninto FCNs such as U-Net. Comprehensive ablation experiments demonstrated the\nfeasibility of convolutional decomposition and explored the most efficient and\neffective network design. Among them, the best Mid-type LK attention-based\nU-Net network was evaluated on CT-ORG and BraTS 2020 datasets, achieving\nstate-of-the-art segmentation performance. The performance improvement due to\nthe proposed LK attention module was also statistically validated.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nan_Y/0/1/0/all/0/1\">Yang Nan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ser_J/0/1/0/all/0/1\">Javier Del Ser</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_G/0/1/0/all/0/1\">Guang Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FewGAN: Generating from the Joint Distribution of a Few Images. (arXiv:2207.11226v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11226","description":"<p>We introduce FewGAN, a generative model for generating novel, high-quality\nand diverse images whose patch distribution lies in the joint patch\ndistribution of a small number of N&gt;1 training samples. The method is, in\nessence, a hierarchical patch-GAN that applies quantization at the first coarse\nscale, in a similar fashion to VQ-GAN, followed by a pyramid of residual fully\nconvolutional GANs at finer scales. Our key idea is to first use quantization\nto learn a fixed set of patch embeddings for training images. We then use a\nseparate set of side images to model the structure of generated images using an\nautoregressive model trained on the learned patch embeddings of training\nimages. Using quantization at the coarsest scale allows the model to generate\nboth conditional and unconditional novel images. Subsequently, a patch-GAN\nrenders the fine details, resulting in high-quality images. In an extensive set\nof experiments, it is shown that FewGAN outperforms baselines both\nquantitatively and qualitatively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ben_Moshe_L/0/1/0/all/0/1\">Lior Ben-Moshe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benaim_S/0/1/0/all/0/1\">Sagie Benaim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_L/0/1/0/all/0/1\">Lior Wolf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Face editing with GAN -- A Review. (arXiv:2207.11227v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11227","description":"<p>In recent years, Generative Adversarial Networks (GANs) have become a hot\ntopic among researchers and engineers that work with deep learning. It has been\na ground-breaking technique which can generate new pieces of content of data in\na consistent way. The topic of GANs has exploded in popularity due to its\napplicability in fields like image generation and synthesis, and music\nproduction and composition. GANs have two competing neural networks: a\ngenerator and a discriminator. The generator is used to produce new samples or\npieces of content, while the discriminator is used to recognize whether the\npiece of content is real or generated. What makes it different from other\ngenerative models is its ability to learn unlabeled samples. In this review\npaper, we will discuss the evolution of GANs, several improvements proposed by\nthe authors and a brief comparison between the different models. Index Terms\ngenerative adversarial networks, unsupervised learning, deep learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mehta_P/0/1/0/all/0/1\">Parthak Mehta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Sarthak Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chouhan_N/0/1/0/all/0/1\">Nikhil Chouhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pethani_N/0/1/0/all/0/1\">Neel Pethani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saha_I/0/1/0/all/0/1\">Ishani Saha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Classifying Crop Types using Gaussian Bayesian Models and Neural Networks on GHISACONUS USGS data from NASA Hyperspectral Satellite Imagery. (arXiv:2207.11228v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11228","description":"<p>Hyperspectral Imagining is a type of digital imaging in which each pixel\ncontains typically hundreds of wavelengths of light providing spectroscopic\ninformation about the materials present in the pixel. In this paper we provide\nclassification methods for determining crop type in the USGS GHISACONUS data,\nwhich contains around 7,000 pixel spectra from the five major U.S. agricultural\ncrops (winter wheat, rice, corn, soybeans, and cotton) collected by the NASA\nHyperion satellite, and includes the spectrum, geolocation, crop type, and\nstage of growth for each pixel. We apply standard LDA and QDA as well as\nBayesian custom versions that compute the joint probability of crop type and\nstage, and then the marginal probability for crop type, outperforming the\nnon-Bayesian methods. We also test a single layer neural network with dropout\non the data, which performs comparable to LDA and QDA but not as well as the\nBayesian methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Basener_B/0/1/0/all/0/1\">Bill Basener</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"You Actually Look Twice At it (YALTAi): using an object detection approach instead of region segmentation within the Kraken engine. (arXiv:2207.11230v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11230","description":"<p>Layout Analysis (the identification of zones and their classification) is the\nfirst step along line segmentation in Optical Character Recognition and similar\ntasks. The ability of identifying main body of text from marginal text or\nrunning titles makes the difference between extracting the work full text of a\ndigitized book and noisy outputs. We show that most segmenters focus on pixel\nclassification and that polygonization of this output has not been used as a\ntarget for the latest competition on historical document (ICDAR 2017 and\nonwards), despite being the focus in the early 2010s. We propose to shift, for\nefficiency, the task from a pixel classification-based polygonization to an\nobject detection using isothetic rectangles. We compare the output of Kraken\nand YOLOv5 in terms of segmentation and show that the later severely\noutperforms the first on small datasets (1110 samples and below). We release\ntwo datasets for training and evaluation on historical documents as well as a\nnew package, YALTAi, which injects YOLOv5 in the segmentation pipeline of\nKraken 4.1.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Clerice_T/0/1/0/all/0/1\">Thibault Cl&#xe9;rice</a> (ENC, CJM, HiSoMA, UJML)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Seeing 3D Objects in a Single Image via Self-Supervised Static-Dynamic Disentanglement. (arXiv:2207.11232v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11232","description":"<p>Human perception reliably identifies movable and immovable parts of 3D\nscenes, and completes the 3D structure of objects and background from\nincomplete observations. We learn this skill not via labeled examples, but\nsimply by observing objects move. In this work, we propose an approach that\nobserves unlabeled multi-view videos at training time and learns to map a\nsingle image observation of a complex scene, such as a street with cars, to a\n3D neural scene representation that is disentangled into movable and immovable\nparts while plausibly completing its 3D structure. We separately parameterize\nmovable and immovable scene parts via 2D neural ground plans. These ground\nplans are 2D grids of features aligned with the ground plane that can be\nlocally decoded into 3D neural radiance fields. Our model is trained\nself-supervised via neural rendering. We demonstrate that the structure\ninherent to our disentangled 3D representation enables a variety of downstream\ntasks in street-scale 3D scenes using simple heuristics, such as extraction of\nobject-centric 3D representations, novel view synthesis, instance segmentation,\nand 3D bounding box prediction, highlighting its value as a backbone for\ndata-efficient 3D scene understanding models. This disentanglement further\nenables scene editing via object manipulation such as deletion, insertion, and\nrigid-body motion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_P/0/1/0/all/0/1\">Prafull Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tewari_A/0/1/0/all/0/1\">Ayush Tewari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yilun Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zakharov_S/0/1/0/all/0/1\">Sergey Zakharov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ambrus_R/0/1/0/all/0/1\">Rares Ambrus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaidon_A/0/1/0/all/0/1\">Adrien Gaidon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freeman_W/0/1/0/all/0/1\">William T. Freeman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durand_F/0/1/0/all/0/1\">Fredo Durand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1\">Joshua B. Tenenbaum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sitzmann_V/0/1/0/all/0/1\">Vincent Sitzmann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A System-driven Automatic Ground Truth Generation Method for DL Inner-City Driving Corridor Detectors. (arXiv:2207.11234v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11234","description":"<p>Data-driven perception approaches are well-established in automated driving\nsystems. In many fields even super-human performance is reached. Unlike\nprediction and planning approaches, mainly supervised learning algorithms are\nused for the perception domain. Therefore, a major remaining challenge is the\nefficient generation of ground truth data. As perception modules are positioned\nclose to the sensor, they typically run on raw sensor data of high bandwidth.\nDue to that, the generation of ground truth labels typically causes a\nsignificant manual effort, which leads to high costs for the labelling itself\nand the necessary quality control. In this contribution, we propose an\nautomatic labeling approach for semantic segmentation of the drivable ego\ncorridor that reduces the manual effort by a factor of 150 and more. The\nproposed holistic approach could be used in an automated data loop, allowing a\ncontinuous improvement of the depending perception modules.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ruthardt_J/0/1/0/all/0/1\">Jona Ruthardt</a> (Robert Bosch GmbH), <a href=\"http://arxiv.org/find/cs/1/au:+Michalke_T/0/1/0/all/0/1\">Thomas Michalke</a> (Robert Bosch GmbH)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improved lightweight identification of agricultural diseases based on MobileNetV3. (arXiv:2207.11238v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11238","description":"<p>At present, the identification of agricultural pests and diseases has the\nproblem that the model is not lightweight enough and difficult to apply. Based\non MobileNetV3, this paper introduces the Coordinate Attention block. The\nparameters of MobileNetV3-large are reduced by 22%, the model size is reduced\nby 19.7%, and the accuracy is improved by 0.92%. The parameters of\nMobileNetV3-small are reduced by 23.4%, the model size is reduced by 18.3%, and\nthe accuracy is increased by 0.40%. In addition, the improved MobileNetV3-small\nwas migrated to Jetson Nano for testing. The accuracy increased by 2.48% to\n98.31%, and the inference speed increased by 7.5%. It provides a reference for\ndeploying the agricultural pest identification model to embedded devices.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yuhang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_W/0/1/0/all/0/1\">Wenping Tong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multiface: A Dataset for Neural Face Rendering. (arXiv:2207.11243v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11243","description":"<p>Photorealistic avatars of human faces have come a long way in recent years,\nyet research along this area is limited by a lack of publicly available,\nhigh-quality datasets covering both, dense multi-view camera captures, and rich\nfacial expressions of the captured subjects. In this work, we present\nMultiface, a new multi-view, high-resolution human face dataset collected from\n13 identities at Reality Labs Research for neural face rendering. We introduce\nMugsy, a large scale multi-camera apparatus to capture high-resolution\nsynchronized videos of a facial performance. The goal of Multiface is to close\nthe gap in accessibility to high quality data in the academic community and to\nenable research in VR telepresence. Along with the release of the dataset, we\nconduct ablation studies on the influence of different model architectures\ntoward the model's interpolation capacity of novel viewpoint and expressions.\nWith a conditional VAE model serving as our baseline, we found that adding\nspatial bias, texture warp field, and residual connections improves performance\non novel view synthesis. Our code and data is available at:\nhttps://github.com/facebookresearch/multiface\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wuu_C/0/1/0/all/0/1\">Cheng-hsin Wuu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1\">Ningyuan Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ardisson_S/0/1/0/all/0/1\">Scott Ardisson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bali_R/0/1/0/all/0/1\">Rohan Bali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belko_D/0/1/0/all/0/1\">Danielle Belko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brockmeyer_E/0/1/0/all/0/1\">Eric Brockmeyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Evans_L/0/1/0/all/0/1\">Lucas Evans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Godisart_T/0/1/0/all/0/1\">Timothy Godisart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ha_H/0/1/0/all/0/1\">Hyowon Ha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hypes_A/0/1/0/all/0/1\">Alexander Hypes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koska_T/0/1/0/all/0/1\">Taylor Koska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krenn_S/0/1/0/all/0/1\">Steven Krenn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lombardi_S/0/1/0/all/0/1\">Stephen Lombardi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xiaomin Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McPhail_K/0/1/0/all/0/1\">Kevyn McPhail</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Millerschoen_L/0/1/0/all/0/1\">Laura Millerschoen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perdoch_M/0/1/0/all/0/1\">Michal Perdoch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pitts_M/0/1/0/all/0/1\">Mark Pitts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Richard_A/0/1/0/all/0/1\">Alexander Richard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saragih_J/0/1/0/all/0/1\">Jason Saragih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saragih_J/0/1/0/all/0/1\">Junko Saragih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shiratori_T/0/1/0/all/0/1\">Takaaki Shiratori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simon_T/0/1/0/all/0/1\">Tomas Simon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stewart_M/0/1/0/all/0/1\">Matt Stewart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trimble_A/0/1/0/all/0/1\">Autumn Trimble</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weng_X/0/1/0/all/0/1\">Xinshuo Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Whitewolf_D/0/1/0/all/0/1\">David Whitewolf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chenglei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Shoou-I Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheikh_Y/0/1/0/all/0/1\">Yaser Sheikh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning Hyperparameter Optimization for Breast Mass Detection in Mammograms. (arXiv:2207.11244v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11244","description":"<p>Accurate breast cancer diagnosis through mammography has the potential to\nsave millions of lives around the world. Deep learning (DL) methods have shown\nto be very effective for mass detection in mammograms. Additional improvements\nof current DL models will further improve the effectiveness of these methods. A\ncritical issue in this context is how to pick the right hyperparameters for DL\nmodels. In this paper, we present GA-E2E, a new approach for tuning the\nhyperparameters of DL models for brest cancer detection using Genetic\nAlgorithms (GAs). Our findings reveal that differences in parameter values can\nconsiderably alter the area under the curve (AUC), which is used to determine a\nclassifier's performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sehgal_A/0/1/0/all/0/1\">Adarsh Sehgal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sehgal_M/0/1/0/all/0/1\">Muskan Sehgal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+La_H/0/1/0/all/0/1\">Hung Manh La</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bebis_G/0/1/0/all/0/1\">George Bebis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Panoptic Scene Graph Generation. (arXiv:2207.11247v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11247","description":"<p>Existing research addresses scene graph generation (SGG) -- a critical\ntechnology for scene understanding in images -- from a detection perspective,\ni.e., objects are detected using bounding boxes followed by prediction of their\npairwise relationships. We argue that such a paradigm causes several problems\nthat impede the progress of the field. For instance, bounding box-based labels\nin current datasets usually contain redundant classes like hairs, and leave out\nbackground information that is crucial to the understanding of context. In this\nwork, we introduce panoptic scene graph generation (PSG), a new problem task\nthat requires the model to generate a more comprehensive scene graph\nrepresentation based on panoptic segmentations rather than rigid bounding\nboxes. A high-quality PSG dataset, which contains 49k well-annotated\noverlapping images from COCO and Visual Genome, is created for the community to\nkeep track of its progress. For benchmarking, we build four two-stage\nbaselines, which are modified from classic methods in SGG, and two one-stage\nbaselines called PSGTR and PSGFormer, which are based on the efficient\nTransformer-based detector, i.e., DETR. While PSGTR uses a set of queries to\ndirectly learn triplets, PSGFormer separately models the objects and relations\nin the form of queries from two Transformer decoders, followed by a\nprompting-like relation-object matching mechanism. In the end, we share\ninsights on open challenges and future directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jingkang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ang_Y/0/1/0/all/0/1\">Yi Zhe Ang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zujin Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kaiyang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wayne Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NASA: Neural Articulated Shape Approximation. (arXiv:1912.03207v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1912.03207","description":"<p>Efficient representation of articulated objects such as human bodies is an\nimportant problem in computer vision and graphics. To efficiently simulate\ndeformation, existing approaches represent 3D objects using polygonal meshes\nand deform them using skinning techniques. This paper introduces neural\narticulated shape approximation (NASA), an alternative framework that enables\nefficient representation of articulated deformable objects using neural\nindicator functions that are conditioned on pose. Occupancy testing using NASA\nis straightforward, circumventing the complexity of meshes and the issue of\nwater-tightness. We demonstrate the effectiveness of NASA for 3D tracking\napplications, and discuss other potential extensions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_B/0/1/0/all/0/1\">Boyang Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_J/0/1/0/all/0/1\">JP Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeruzalski_T/0/1/0/all/0/1\">Timothy Jeruzalski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pons_Moll_G/0/1/0/all/0/1\">Gerard Pons-Moll</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hinton_G/0/1/0/all/0/1\">Geoffrey Hinton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Norouzi_M/0/1/0/all/0/1\">Mohammad Norouzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tagliasacchi_A/0/1/0/all/0/1\">Andrea Tagliasacchi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Trustworthy are Performance Evaluations for Basic Vision Tasks?. (arXiv:2008.03533v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2008.03533","description":"<p>This paper examines performance evaluation criteria for basic vision tasks\ninvolving sets of objects namely, object detection, instance-level segmentation\nand multi-object tracking. The rankings of algorithms by an existing criterion\ncan fluctuate with different choices of parameters, e.g. Intersection over\nUnion (IoU) threshold, making their evaluations unreliable. More importantly,\nthere is no means to verify whether we can trust the evaluations of a\ncriterion. This work suggests a notion of trustworthiness for performance\ncriteria, which requires (i) robustness to parameters for reliability, (ii)\ncontextual meaningfulness in sanity tests, and (iii) consistency with\nmathematical requirements such as the metric properties. We observe that these\nrequirements were overlooked by many widely-used criteria, and explore\nalternative criteria using metrics for sets of shapes. We also assess all these\ncriteria based on the suggested requirements for trustworthiness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tran Thien Dat Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rezatofighi_H/0/1/0/all/0/1\">Hamid Rezatofighi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vo_B/0/1/0/all/0/1\">Ba-Ngu Vo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vo_B/0/1/0/all/0/1\">Ba-Tuong Vo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1\">Silvio Savarese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reid_I/0/1/0/all/0/1\">Ian Reid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Energy-Based Models With Adversarial Training. (arXiv:2012.06568v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2012.06568","description":"<p>We study a new approach to learning energy-based models (EBMs) based on\nadversarial training (AT). We show that (binary) AT learns a special kind of\nenergy function that models the support of the data distribution, and the\nlearning process is closely related to MCMC-based maximum likelihood learning\nof EBMs. We further propose improved techniques for generative modeling with\nAT, and demonstrate that this new approach is capable of generating diverse and\nrealistic images. Aside from having competitive image generation performance to\nexplicit EBMs, the studied approach is stable to train, is well-suited for\nimage translation tasks, and exhibits strong out-of-distribution adversarial\nrobustness. Our results demonstrate the viability of the AT approach to\ngenerative modeling, suggesting that AT is a competitive alternative approach\nto learning EBMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1\">Xuwang Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shiying Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohde_G/0/1/0/all/0/1\">Gustavo K. Rohde</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FoV-NeRF: Foveated Neural Radiance Fields for Virtual Reality. (arXiv:2103.16365v2 [cs.GR] UPDATED)","link":"http://arxiv.org/abs/2103.16365","description":"<p>Virtual Reality (VR) is becoming ubiquitous with the rise of consumer\ndisplays and commercial VR platforms. Such displays require low latency and\nhigh quality rendering of synthetic imagery with reduced compute overheads.\nRecent advances in neural rendering showed promise of unlocking new\npossibilities in 3D computer graphics via image-based representations of\nvirtual or physical environments. Specifically, the neural radiance fields\n(NeRF) demonstrated that photo-realistic quality and continuous view changes of\n3D scenes can be achieved without loss of view-dependent effects. While NeRF\ncan significantly benefit rendering for VR applications, it faces unique\nchallenges posed by high field-of-view, high resolution, and\nstereoscopic/egocentric viewing, typically causing low quality and high latency\nof the rendered images. In VR, this not only harms the interaction experience\nbut may also cause sickness. To tackle these problems toward\nsix-degrees-of-freedom, egocentric, and stereo NeRF in VR, we present the first\ngaze-contingent 3D neural representation and view synthesis method. We\nincorporate the human psychophysics of visual- and stereo-acuity into an\negocentric neural representation of 3D scenery. We then jointly optimize the\nlatency/performance and visual quality while mutually bridging human perception\nand neural scene synthesis to achieve perceptually high-quality immersive\ninteraction. We conducted both objective analysis and subjective studies to\nevaluate the effectiveness of our approach. We find that our method\nsignificantly reduces latency (up to 99% time reduction compared with NeRF)\nwithout loss of high-fidelity rendering (perceptually identical to\nfull-resolution ground truth). The presented approach may serve as the first\nstep toward future VR/AR systems that capture, teleport, and visualize remote\nenvironments in real-time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_N/0/1/0/all/0/1\">Nianchen Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhenyi He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jiannan Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duinkharjav_B/0/1/0/all/0/1\">Budmonde Duinkharjav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakravarthula_P/0/1/0/all/0/1\">Praneeth Chakravarthula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xubo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1\">Qi Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The effectiveness of feature attribution methods and its correlation with automatic evaluation scores. (arXiv:2105.14944v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.14944","description":"<p>Explaining the decisions of an Artificial Intelligence (AI) model is\nincreasingly critical in many real-world, high-stake applications. Hundreds of\npapers have either proposed new feature attribution methods, discussed or\nharnessed these tools in their work. However, despite humans being the target\nend-users, most attribution methods were only evaluated on proxy\nautomatic-evaluation metrics (Zhang et al. 2018; Zhou et al. 2016; Petsiuk et\nal. 2018). In this paper, we conduct the first user study to measure\nattribution map effectiveness in assisting humans in ImageNet classification\nand Stanford Dogs fine-grained classification, and when an image is natural or\nadversarial (i.e., contains adversarial perturbations). Overall, feature\nattribution is surprisingly not more effective than showing humans nearest\ntraining-set examples. On a harder task of fine-grained dog categorization,\npresenting attribution maps to humans does not help, but instead hurts the\nperformance of human-AI teams compared to AI alone. Importantly, we found\nautomatic attribution-map evaluation measures to correlate poorly with the\nactual human-AI team performance. Our findings encourage the community to\nrigorously test their methods on the downstream human-in-the-loop applications\nand to rethink the existing evaluation metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_G/0/1/0/all/0/1\">Giang Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Daeyoung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1\">Anh Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bi-level Feature Alignment for Versatile Image Translation and Manipulation. (arXiv:2107.03021v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.03021","description":"<p>Generative adversarial networks (GANs) have achieved great success in image\ntranslation and manipulation. However, high-fidelity image generation with\nfaithful style control remains a grand challenge in computer vision. This paper\npresents a versatile image translation and manipulation framework that achieves\naccurate semantic and style guidance in image generation by explicitly building\na correspondence. To handle the quadratic complexity incurred by building the\ndense correspondences, we introduce a bi-level feature alignment strategy that\nadopts a top-$k$ operation to rank block-wise features followed by dense\nattention between block features which reduces memory cost substantially. As\nthe top-$k$ operation involves index swapping which precludes the gradient\npropagation, we approximate the non-differentiable top-$k$ operation with a\nregularized earth mover's problem so that its gradient can be effectively\nback-propagated. In addition, we design a novel semantic position encoding\nmechanism that builds up coordinate for each individual semantic region to\npreserve texture structures while building correspondences. Further, we design\na novel confidence feature injection module which mitigates mismatch problem by\nfusing features adaptively according to the reliability of built\ncorrespondences. Extensive experiments show that our method achieves superior\nperformance qualitatively and quantitatively as compared with the\nstate-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhan_F/0/1/0/all/0/1\">Fangneng Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yingchen Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1\">Rongliang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiahui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_K/0/1/0/all/0/1\">Kaiwen Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_A/0/1/0/all/0/1\">Aoran Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shijian Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_C/0/1/0/all/0/1\">Chunyan Miao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Predict Diverse Human Motions from a Single Image via Mixture Density Networks. (arXiv:2109.05776v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.05776","description":"<p>Human motion prediction, which plays a key role in computer vision, generally\nrequires a past motion sequence as input. However, in real applications, a\ncomplete and correct past motion sequence can be too expensive to achieve. In\nthis paper, we propose a novel approach to predicting future human motions from\na much weaker condition, i.e., a single image, with mixture density networks\n(MDN) modeling. Contrary to most existing deep human motion prediction\napproaches, the multimodal nature of MDN enables the generation of diverse\nfuture motion hypotheses, which well compensates for the strong stochastic\nambiguity aggregated by the single input and human motion uncertainty. In\ndesigning the loss function, we further introduce the energy-based formulation\nto flexibly impose prior losses over the learnable parameters of MDN to\nmaintain motion coherence as well as improve the prediction accuracy by\ncustomizing the energy functions. Our trained model directly takes an image as\ninput and generates multiple plausible motions that satisfy the given\ncondition. Extensive experiments on two standard benchmark datasets demonstrate\nthe effectiveness of our method in terms of prediction diversity and accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_C/0/1/0/all/0/1\">Chunzhi Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scale-aware direct monocular odometry. (arXiv:2109.10077v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2109.10077","description":"<p>We present a generic framework for scale-aware direct monocular odometry\nbased on depth prediction from a deep neural network. In contrast with previous\nmethods where depth information is only partially exploited, we formulate a\nnovel depth prediction residual which allows us to incorporate multi-view depth\ninformation. In addition, we propose to use a truncated robust cost function\nwhich prevents considering inconsistent depth estimations. The photometric and\ndepth-prediction measurements are integrated into a tightly-coupled\noptimization leading to a scale-aware monocular system which does not\naccumulate scale drift. Our proposal does not particularize for a concrete\nneural network, being able to work along with the vast majority of the existing\ndepth prediction solutions. We demonstrate the validity and generality of our\nproposal evaluating it on the KITTI odometry dataset, using two publicly\navailable neural networks and comparing it with similar approaches and the\nstate-of-the-art for monocular and stereo SLAM. Experiments show that our\nproposal largely outperforms classic monocular SLAM, being 5 to 9 times more\nprecise, beating similar approaches and having an accuracy which is closer to\nthat of stereo systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Campos_C/0/1/0/all/0/1\">Carlos Campos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tardos_J/0/1/0/all/0/1\">Juan D. Tard&#xf3;s</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bounding-box deep calibration for high performance face detection. (arXiv:2110.03892v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.03892","description":"<p>Modern convolutional neural networks (CNNs)-based face detectors have\nachieved tremendous strides due to large annotated datasets. However,\nmisaligned results with high detection confidence but low localization accuracy\nrestrict the further improvement of detection performance. In this paper, the\nauthors first predict high confidence detection results on the training set\nitself. Surprisingly, a considerable part of them exist in the same\nmisalignment problem. Then, the authors carefully examine these cases and point\nout that annotation misalignment is the main reason. Later, a comprehensive\ndiscussion is given for the replacement rationality between predicted and\nannotated bounding-boxes. Finally, the authors propose a novel Bounding-Box\nDeep Calibration (BDC) method to reasonably replace misaligned annotations with\nmodel predicted bounding-boxes and offer calibrated annotations for the\ntraining set. Extensive experiments on multiple detectors and two popular\nbenchmark datasets show the effectiveness of BDC on improving models' precision\nand recall rate, without adding extra inference time and memory consumption.\nOur simple and effective method provides a general strategy for improving face\ndetection, especially for light-weight detectors in real-time situations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1\">Shi Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiongfei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaoli Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MFNet: Multi-class Few-shot Segmentation Network with Pixel-wise Metric Learning. (arXiv:2111.00232v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.00232","description":"<p>In visual recognition tasks, few-shot learning requires the ability to learn\nobject categories with few support examples. Its re-popularity in light of the\ndeep learning development is mainly in image classification. This work focuses\non few-shot semantic segmentation, which is still a largely unexplored field. A\nfew recent advances are often restricted to single-class few-shot segmentation.\nIn this paper, we first present a novel multi-way (class) encoding and decoding\narchitecture which effectively fuses multi-scale query information and\nmulti-class support information into one query-support embedding. Multi-class\nsegmentation is directly decoded upon this embedding. For better feature\nfusion, a multi-level attention mechanism is proposed within the architecture,\nwhich includes the attention for support feature modulation and attention for\nmulti-scale combination. Last, to enhance the embedding space learning, an\nadditional pixel-wise metric learning module is introduced with triplet loss\nformulated on the pixel-level embedding of the input image. Extensive\nexperiments on standard benchmarks PASCAL-5i and COCO-20i show clear benefits\nof our method over the state of the art in few-shot segmentation\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Miao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_M/0/1/0/all/0/1\">Miaojing Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Li Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding the Dynamics of DNNs Using Graph Modularity. (arXiv:2111.12485v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.12485","description":"<p>There are good arguments to support the claim that deep neural networks\n(DNNs) capture better feature representations than the previous hand-crafted\nfeature engineering, which leads to a significant performance improvement. In\nthis paper, we move a tiny step towards understanding the dynamics of feature\nrepresentations over layers. Specifically, we model the process of class\nseparation of intermediate representations in pre-trained DNNs as the evolution\nof communities in dynamic graphs. Then, we introduce modularity, a generic\nmetric in graph theory, to quantify the evolution of communities. In the\npreliminary experiment, we find that modularity roughly tends to increase as\nthe layer goes deeper and the degradation and plateau arise when the model\ncomplexity is great relative to the dataset. Through an asymptotic analysis, we\nprove that modularity can be broadly used for different applications. For\nexample, modularity provides new insights to quantify the difference between\nfeature representations. More crucially, we demonstrate that the degradation\nand plateau in modularity curves represent redundant layers in DNNs and can be\npruned with minimal impact on performance, which provides theoretical guidance\nfor layer pruning. Our code is available at\nhttps://github.com/yaolu-zjut/Dynamic-Graphs-Construction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yunzhe Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zuohui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jinyin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xuan_Q/0/1/0/all/0/1\">Qi Xuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaoniu Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Shape Part Slot Machine: Contact-based Reasoning for Generating 3D Shapes from Parts. (arXiv:2112.00584v2 [cs.GR] UPDATED)","link":"http://arxiv.org/abs/2112.00584","description":"<p>We present the Shape Part Slot Machine, a new method for assembling novel 3D\nshapes from existing parts by performing contact-based reasoning. Our method\nrepresents each shape as a graph of ``slots,'' where each slot is a region of\ncontact between two shape parts. Based on this representation, we design a\ngraph-neural-network-based model for generating new slot graphs and retrieving\ncompatible parts, as well as a gradient-descent-based optimization scheme for\nassembling the retrieved parts into a complete shape that respects the\ngenerated slot graph. This approach does not require any semantic part labels;\ninterestingly, it also does not require complete part geometries -- reasoning\nabout the slots proves sufficient to generate novel, high-quality 3D shapes. We\ndemonstrate that our method generates shapes that outperform existing\nmodeling-by-assembly approaches regarding quality, diversity, and structural\ncomplexity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guerrero_P/0/1/0/all/0/1\">Paul Guerrero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_V/0/1/0/all/0/1\">Vladimir Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhuri_S/0/1/0/all/0/1\">Siddhartha Chaudhuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sung_M/0/1/0/all/0/1\">Minhyuk Sung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ritchie_D/0/1/0/all/0/1\">Daniel Ritchie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"D3Net: A Unified Speaker-Listener Architecture for 3D Dense Captioning and Visual Grounding. (arXiv:2112.01551v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.01551","description":"<p>Recent studies on dense captioning and visual grounding in 3D have achieved\nimpressive results. Despite developments in both areas, the limited amount of\navailable 3D vision-language data causes overfitting issues for 3D visual\ngrounding and 3D dense captioning methods. Also, how to discriminatively\ndescribe objects in complex 3D environments is not fully studied yet. To\naddress these challenges, we present D3Net, an end-to-end neural\nspeaker-listener architecture that can detect, describe and discriminate. Our\nD3Net unifies dense captioning and visual grounding in 3D in a self-critical\nmanner. This self-critical property of D3Net also introduces discriminability\nduring object caption generation and enables semi-supervised training on\nScanNet data with partially annotated descriptions. Our method outperforms SOTA\nmethods in both tasks on the ScanRefer dataset, surpassing the SOTA 3D dense\ncaptioning method by a significant margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dave Zhenyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qirui Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niessner_M/0/1/0/all/0/1\">Matthias Nie&#xdf;ner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_A/0/1/0/all/0/1\">Angel X. Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MoFaNeRF: Morphable Facial Neural Radiance Field. (arXiv:2112.02308v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.02308","description":"<p>We propose a parametric model that maps free-view images into a vector space\nof coded facial shape, expression and appearance with a neural radiance field,\nnamely Morphable Facial NeRF. Specifically, MoFaNeRF takes the coded facial\nshape, expression and appearance along with space coordinate and view direction\nas input to an MLP, and outputs the radiance of the space point for\nphoto-realistic image synthesis. Compared with conventional 3D morphable models\n(3DMM), MoFaNeRF shows superiority in directly synthesizing photo-realistic\nfacial details even for eyes, mouths, and beards. Also, continuous face\nmorphing can be easily achieved by interpolating the input shape, expression\nand appearance codes. By introducing identity-specific modulation and texture\nencoder, our model synthesizes accurate photometric details and shows strong\nrepresentation ability. Our model shows strong ability on multiple applications\nincluding image-based fitting, random generation, face rigging, face editing,\nand novel view synthesis. Experiments show that our method achieves higher\nrepresentation ability than previous parametric models, and achieves\ncompetitive performance in several applications. To the best of our knowledge,\nour work is the first facial parametric model built upon a neural radiance\nfield that can be used in fitting, generation and manipulation. The code and\ndata is available at https://github.com/zhuhao-nju/mofanerf.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1\">Yiyu Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xusen Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1\">Xun Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"4DContrast: Contrastive Learning with Dynamic Correspondences for 3D Scene Understanding. (arXiv:2112.02990v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.02990","description":"<p>We present a new approach to instill 4D dynamic object priors into learned 3D\nrepresentations by unsupervised pre-training. We observe that dynamic movement\nof an object through an environment provides important cues about its\nobjectness, and thus propose to imbue learned 3D representations with such\ndynamic understanding, that can then be effectively transferred to improved\nperformance in downstream 3D semantic scene understanding tasks. We propose a\nnew data augmentation scheme leveraging synthetic 3D shapes moving in static 3D\nenvironments, and employ contrastive learning under 3D-4D constraints that\nencode 4D invariances into the learned 3D representations. Experiments\ndemonstrate that our unsupervised representation learning results in\nimprovement in downstream 3D semantic segmentation, object detection, and\ninstance segmentation tasks, and moreover, notably improves performance in\ndata-scarce scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yujin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niessner_M/0/1/0/all/0/1\">Matthias Nie&#xdf;ner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1\">Angela Dai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NeuroHSMD: Neuromorphic Hybrid Spiking Motion Detector. (arXiv:2112.06102v3 [cs.NE] UPDATED)","link":"http://arxiv.org/abs/2112.06102","description":"<p>Vertebrate retinas are highly-efficient in processing trivial visual tasks\nsuch as detecting moving objects, yet a complex challenges for modern\ncomputers. In vertebrates, the detection of object motion is performed by\nspecialised retinal cells named Object Motion Sensitive Ganglion Cells\n(OMS-GC). OMS-GC process continuous visual signals and generate spike patterns\nthat are post-processed by the Visual Cortex. Our previous Hybrid Sensitive\nMotion Detector (HSMD) algorithm was the first hybrid algorithm to enhance\nBackground subtraction (BS) algorithms with a customised 3-layer Spiking Neural\nNetwork (SNN) that generates OMS-GC spiking-like responses. In this work, we\npresent a Neuromorphic Hybrid Sensitive Motion Detector (NeuroHSMD) algorithm\nthat accelerates our HSMD algorithm using Field-Programmable Gate Arrays\n(FPGAs). The NeuroHSMD was compared against the HSMD algorithm, using the same\n2012 Change Detection (CDnet2012) and 2014 Change Detection (CDnet2014)\nbenchmark datasets. When tested against the CDnet2012 and CDnet2014 datasets,\nNeuroHSMD performs object motion detection at 720x480 at 28.06 Frames Per\nSecond (fps) and 720x480 at 28.71 fps, respectively, with no degradation of\nquality. Moreover, the NeuroHSMD proposed in this paper was completely\nimplemented in Open Computer Language (OpenCL) and therefore is easily\nreplicated in other devices such as Graphical Processing Units (GPUs) and\nclusters of Central Processing Units (CPUs).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Machado_P/0/1/0/all/0/1\">Pedro Machado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferreira_J/0/1/0/all/0/1\">Joao Filipe Ferreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oikonomou_A/0/1/0/all/0/1\">Andreas Oikonomou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McGinnity_T/0/1/0/all/0/1\">T.M. McGinnity</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Formulating Event-based Image Reconstruction as a Linear Inverse Problem using Optical Flow. (arXiv:2112.06242v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.06242","description":"<p>Event cameras are novel bio-inspired sensors that measure per-pixel\nbrightness differences asynchronously. Recovering brightness from events is\nappealing since the reconstructed images inherit the high dynamic range (HDR)\nand high-speed properties of events; hence they can be used in many robotic\nvision applications and to generate slow-motion HDR videos. However,\nstate-of-the-art methods tackle this problem by training an event-to-image\nrecurrent neural network (RNN), which lacks explainability and is difficult to\ntune. In this work we show, for the first time, how tackling the joint problem\nof motion and brightness estimation leads us to formulate event-based image\nreconstruction as a linear inverse problem that can be solved without training\nan image reconstruction RNN. Instead, classical and learning-based image priors\ncan be used to solve the problem and remove artifacts from the reconstructed\nimages. The experiments show that the proposed approach generates images with\nvisual quality on par with state-of-the-art methods despite only using data\nfrom a short time interval. The proposed linear formulation and solvers have a\nunifying character because they can be applied also to reconstruct brightness\nfrom the second derivative. Additionally, the linear formulation is attractive\nbecause it can be naturally combined with super-resolution, motion-segmentation\nand color demosaicing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zelin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yezzi_A/0/1/0/all/0/1\">Anthony Yezzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gallego_G/0/1/0/all/0/1\">Guillermo Gallego</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SAGA: Stochastic Whole-Body Grasping with Contact. (arXiv:2112.10103v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.10103","description":"<p>The synthesis of human grasping has numerous applications including AR/VR,\nvideo games and robotics. While methods have been proposed to generate\nrealistic hand-object interaction for object grasping and manipulation, these\ntypically only consider interacting hand alone. Our goal is to synthesize\nwhole-body grasping motions. Starting from an arbitrary initial pose, we aim to\ngenerate diverse and natural whole-body human motions to approach and grasp a\ntarget object in 3D space. This task is challenging as it requires modeling\nboth whole-body dynamics and dexterous finger movements. To this end, we\npropose SAGA (StochAstic whole-body Grasping with contAct), a framework which\nconsists of two key components: (a) Static whole-body grasping pose generation.\nSpecifically, we propose a multi-task generative model, to jointly learn static\nwhole-body grasping poses and human-object contacts. (b) Grasping motion\ninfilling. Given an initial pose and the generated whole-body grasping pose as\nthe start and end of the motion respectively, we design a novel contact-aware\ngenerative motion infilling module to generate a diverse set of grasp-oriented\nmotions. We demonstrate the effectiveness of our method, which is a novel\ngenerative framework to synthesize realistic and expressive whole-body motions\nthat approach and grasp randomly placed unseen objects. Code and models are\navailable at https://jiahaoplus.github.io/SAGA/saga.html.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiahao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Siwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hilliges_O/0/1/0/all/0/1\">Otmar Hilliges</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fisher Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Siyu Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RadioTransformer: A Cascaded Global-Focal Transformer for Visual Attention-guided Disease Classification. (arXiv:2202.11781v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.11781","description":"<p>In this work, we present RadioTransformer, a novel visual attention-driven\ntransformer framework, that leverages radiologists' gaze patterns and models\ntheir visuo-cognitive behavior for disease diagnosis on chest radiographs.\nDomain experts, such as radiologists, rely on visual information for medical\nimage interpretation. On the other hand, deep neural networks have demonstrated\nsignificant promise in similar tasks even where visual interpretation is\nchallenging. Eye-gaze tracking has been used to capture the viewing behavior of\ndomain experts, lending insights into the complexity of visual search. However,\ndeep learning frameworks, even those that rely on attention mechanisms, do not\nleverage this rich domain information. RadioTransformer fills this critical gap\nby learning from radiologists' visual search patterns, encoded as 'human visual\nattention regions' in a cascaded global-focal transformer framework. The\noverall 'global' image characteristics and the more detailed 'local' features\nare captured by the proposed global and focal modules, respectively. We\nexperimentally validate the efficacy of our student-teacher approach for 8\ndatasets involving different disease classification tasks where eye-gaze data\nis not available during the inference phase. Code:\nhttps://github.com/bmi-imaginelab/radiotransformer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_M/0/1/0/all/0/1\">Moinak Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Shubham Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prasanna_P/0/1/0/all/0/1\">Prateek Prasanna</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fusing Local Similarities for Retrieval-based 3D Orientation Estimation of Unseen Objects. (arXiv:2203.08472v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.08472","description":"<p>In this paper, we tackle the task of estimating the 3D orientation of\npreviously-unseen objects from monocular images. This task contrasts with the\none considered by most existing deep learning methods which typically assume\nthat the testing objects have been observed during training. To handle the\nunseen objects, we follow a retrieval-based strategy and prevent the network\nfrom learning object-specific features by computing multi-scale local\nsimilarities between the query image and synthetically-generated reference\nimages. We then introduce an adaptive fusion module that robustly aggregates\nthe local similarities into a global similarity score of pairwise images.\nFurthermore, we speed up the retrieval process by developing a fast retrieval\nstrategy. Our experiments on the LineMOD, LineMOD-Occluded, and T-LESS datasets\nshow that our method yields a significantly better generalization to unseen\nobjects than previous works. Our code and pre-trained models are available at\nhttps://sailor-z.github.io/projects/Unseen_Object_Pose.html.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yinlin Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salzmann_M/0/1/0/all/0/1\">Mathieu Salzmann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A workflow for segmenting soil and plant X-ray CT images with deep learning in Googles Colaboratory. (arXiv:2203.09674v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2203.09674","description":"<p>X-ray micro-computed tomography (X-ray microCT) has enabled the\ncharacterization of the properties and processes that take place in plants and\nsoils at the micron scale. Despite the widespread use of this advanced\ntechnique, major limitations in both hardware and software limit the speed and\naccuracy of image processing and data analysis. Recent advances in machine\nlearning, specifically the application of convolutional neural networks to\nimage analysis, have enabled rapid and accurate segmentation of image data.\nYet, challenges remain in applying convolutional neural networks to the\nanalysis of environmentally and agriculturally relevant images. Specifically,\nthere is a disconnect between the computer scientists and engineers, who build\nthese AI/ML tools, and the potential end users in agricultural research, who\nmay be unsure of how to apply these tools in their work. Additionally, the\ncomputing resources required for training and applying deep learning models are\nunique, more common to computer gaming systems or graphics design work, than to\ntraditional computational systems. To navigate these challenges, we developed a\nmodular workflow for applying convolutional neural networks to X-ray microCT\nimages, using low-cost resources in Googles Colaboratory web application. Here\nwe present the results of the workflow, illustrating how parameters can be\noptimized to achieve best results using example scans from walnut leaves,\nalmond flower buds, and a soil aggregate. We expect that this framework will\naccelerate the adoption and use of emerging deep learning techniques within the\nplant and soil sciences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Rippner_D/0/1/0/all/0/1\">Devin A. Rippner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Raja_P/0/1/0/all/0/1\">Pranav Raja</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Earles_J/0/1/0/all/0/1\">J. Mason Earles</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Buchko_A/0/1/0/all/0/1\">Alexander Buchko</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Momayyezi_M/0/1/0/all/0/1\">Mina Momayyezi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Duong_F/0/1/0/all/0/1\">Fiona Duong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Parkinson_D/0/1/0/all/0/1\">Dilworth Parkinson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Forrestel_E/0/1/0/all/0/1\">Elizabeth Forrestel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shackel_K/0/1/0/all/0/1\">Ken Shackel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Neyhart_J/0/1/0/all/0/1\">Jeffrey Neyhart</a>, <a href=\"http://arxiv.org/find/eess/1/au:+McElrone_A/0/1/0/all/0/1\">Andrew J. McElrone</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain Generalization by Mutual-Information Regularization with Pre-trained Models. (arXiv:2203.10789v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2203.10789","description":"<p>Domain generalization (DG) aims to learn a generalized model to an unseen\ntarget domain using only limited source domains. Previous attempts to DG fail\nto learn domain-invariant representations only from the source domains due to\nthe significant domain shifts between training and test domains. Instead, we\nre-formulate the DG objective using mutual information with the oracle model, a\nmodel generalized to any possible domain. We derive a tractable variational\nlower bound via approximating the oracle model by a pre-trained model, called\nMutual Information Regularization with Oracle (MIRO). Our extensive experiments\nshow that MIRO significantly improves the out-of-distribution performance.\nFurthermore, our scaling experiments show that the larger the scale of the\npre-trained model, the greater the performance improvement of MIRO. Source code\nis available at https://github.com/kakaobrain/miro.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cha_J/0/1/0/all/0/1\">Junbum Cha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kyungjae Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Sungrae Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chun_S/0/1/0/all/0/1\">Sanghyuk Chun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Portrait Delighting. (arXiv:2203.12088v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.12088","description":"<p>We present a deep neural network for removing undesirable shading features\nfrom an unconstrained portrait image, recovering the underlying texture. Our\ntraining scheme incorporates three regularization strategies: masked loss, to\nemphasize high-frequency shading features; soft-shadow loss, which improves\nsensitivity to subtle changes in lighting; and shading-offset estimation, to\nsupervise separation of shading and texture. Our method demonstrates improved\ndelighting quality and generalization when compared with the state-of-the-art.\nWe further demonstrate how our delighting method can enhance the performance of\nlight-sensitive computer vision tasks such as face relighting and semantic\nparsing, allowing them to handle extreme lighting conditions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Weir_J/0/1/0/all/0/1\">Joshua Weir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Junhong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chalmers_A/0/1/0/all/0/1\">Andrew Chalmers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rhee_T/0/1/0/all/0/1\">Taehyun Rhee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What to Hide from Your Students: Attention-Guided Masked Image Modeling. (arXiv:2203.12719v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.12719","description":"<p>Transformers and masked language modeling are quickly being adopted and\nexplored in computer vision as vision transformers and masked image modeling\n(MIM). In this work, we argue that image token masking differs from token\nmasking in text, due to the amount and correlation of tokens in an image. In\nparticular, to generate a challenging pretext task for MIM, we advocate a shift\nfrom random masking to informed masking. We develop and exhibit this idea in\nthe context of distillation-based MIM, where a teacher transformer encoder\ngenerates an attention map, which we use to guide masking for the student. We\nthus introduce a novel masking strategy, called attention-guided masking\n(AttMask), and we demonstrate its effectiveness over random masking for dense\ndistillation-based MIM as well as plain distillation-based self-supervised\nlearning on classification tokens. We confirm that AttMask accelerates the\nlearning process and improves the performance on a variety of downstream tasks.\nWe provide the implementation code at https://github.com/gkakogeorgiou/attmask.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kakogeorgiou_I/0/1/0/all/0/1\">Ioannis Kakogeorgiou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gidaris_S/0/1/0/all/0/1\">Spyros Gidaris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Psomas_B/0/1/0/all/0/1\">Bill Psomas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avrithis_Y/0/1/0/all/0/1\">Yannis Avrithis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bursuc_A/0/1/0/all/0/1\">Andrei Bursuc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karantzalos_K/0/1/0/all/0/1\">Konstantinos Karantzalos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Komodakis_N/0/1/0/all/0/1\">Nikos Komodakis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised Video-centralised Transformer for Video Face Clustering. (arXiv:2203.13166v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.13166","description":"<p>This paper presents a novel method for face clustering in videos using a\nvideo-centralised transformer. Previous works often employed contrastive\nlearning to learn frame-level representation and used average pooling to\naggregate the features along the temporal dimension. This approach may not\nfully capture the complicated video dynamics. In addition, despite the recent\nprogress in video-based contrastive learning, few have attempted to learn a\nself-supervised clustering-friendly face representation that benefits the video\nface clustering task. To overcome these limitations, our method employs a\ntransformer to directly learn video-level representations that can better\nreflect the temporally-varying property of faces in videos, while we also\npropose a video-centralised self-supervised framework to train the transformer\nmodel. We also investigate face clustering in egocentric videos, a\nfast-emerging field that has not been studied yet in works related to face\nclustering. To this end, we present and release the first large-scale\negocentric video face clustering dataset named EasyCom-Clustering. We evaluate\nour proposed method on both the widely used Big Bang Theory (BBT) dataset and\nthe new EasyCom-Clustering dataset. Results show the performance of our\nvideo-centralised transformer has surpassed all previous state-of-the-art\nmethods on both benchmarks, exhibiting a self-attentive understanding of face\nvideos.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yujiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_M/0/1/0/all/0/1\">Mingzhi Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jie Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yiming Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yiming Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_P/0/1/0/all/0/1\">Pingchuan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petridis_S/0/1/0/all/0/1\">Stavros Petridis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pantic_M/0/1/0/all/0/1\">Maja Pantic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"mc-BEiT: Multi-choice Discretization for Image BERT Pre-training. (arXiv:2203.15371v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.15371","description":"<p>Image BERT pre-training with masked image modeling (MIM) becomes a popular\npractice to cope with self-supervised representation learning. A seminal work,\nBEiT, casts MIM as a classification task with a visual vocabulary, tokenizing\nthe continuous visual signals into discrete vision tokens using a pre-learned\ndVAE. Despite a feasible solution, the improper discretization hinders further\nimprovements of image pre-training. Since image discretization has no\nground-truth answers, we believe that the masked patch should not be assigned\nwith a unique token id even if a better ``tokenizer'' can be obtained. In this\nwork, we introduce an improved BERT-style image pre-training method, namely\nmc-BEiT, which performs MIM proxy tasks towards eased and refined multi-choice\ntraining objectives. Specifically, the multi-choice supervision for the masked\nimage patches is formed by the soft probability vectors of the discrete token\nids, which are predicted by the off-the-shelf image ``tokenizer'' and further\nrefined by high-level inter-patch perceptions resorting to the observation that\nsimilar patches should share their choices. Extensive experiments on\nclassification, segmentation, and detection tasks demonstrate the superiority\nof our method, e.g., the pre-trained ViT-B achieves 84.1% top-1 fine-tuning\naccuracy on ImageNet-1K classification, 49.2% AP^b and 44.0% AP^m of object\ndetection and instance segmentation on COCO, 50.8% mIOU on ADE20K semantic\nsegmentation, outperforming the competitive counterparts. The code will be\navailable at https://github.com/lixiaotong97/mc-BEiT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaotong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yixiao Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_K/0/1/0/all/0/1\">Kun Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zixuan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1\">Ying Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_L/0/1/0/all/0/1\">Ling-Yu Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An application of Pixel Interval Down-sampling (PID) for dense tiny microorganism counting on environmental microorganism images. (arXiv:2204.01341v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.01341","description":"<p>This paper proposes a novel pixel interval down-sampling network (PID-Net)\nfor dense tiny object (yeast cells) counting tasks with higher accuracy. The\nPID-Net is an end-to-end convolutional neural network (CNN) model with an\nencoder--decoder architecture. The pixel interval down-sampling operations are\nconcatenated with max-pooling operations to combine the sparse and dense\nfeatures. This addresses the limitation of contour conglutination of dense\nobjects while counting. The evaluation was conducted using classical\nsegmentation metrics (the Dice, Jaccard and Hausdorff distance) as well as\ncounting metrics. The experimental results show that the proposed PID-Net had\nthe best performance and potential for dense tiny object counting tasks, which\nachieved 96.97\\% counting accuracy on the dataset with 2448 yeast cell images.\nBy comparing with the state-of-the-art approaches, such as Attention U-Net,\nSwin U-Net and Trans U-Net, the proposed PID-Net can segment dense tiny objects\nwith clearer boundaries and fewer incorrect debris, which shows the great\npotential of PID-Net in the task of accurate counting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiawei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1\">Tao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahaman_M/0/1/0/all/0/1\">Md Mamunur Rahaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yudong Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yu-Hao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jinghua Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_A/0/1/0/all/0/1\">Ao Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grzegorzek_M/0/1/0/all/0/1\">Marcin Grzegorzek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chen Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CHORE: Contact, Human and Object REconstruction from a single RGB image. (arXiv:2204.02445v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.02445","description":"<p>Most prior works in perceiving 3D humans from images reason human in\nisolation without their surroundings. However, humans are constantly\ninteracting with the surrounding objects, thus calling for models that can\nreason about not only the human but also the object and their interaction. The\nproblem is extremely challenging due to heavy occlusions between humans and\nobjects, diverse interaction types and depth ambiguity. In this paper, we\nintroduce CHORE, a novel method that learns to jointly reconstruct the human\nand the object from a single RGB image. CHORE takes inspiration from recent\nadvances in implicit surface learning and classical model-based fitting. We\ncompute a neural reconstruction of human and object represented implicitly with\ntwo unsigned distance fields, a correspondence field to a parametric body and\nan object pose field. This allows us to robustly fit a parametric body model\nand a 3D object template, while reasoning about interactions. Furthermore,\nprior pixel-aligned implicit learning methods use synthetic data and make\nassumptions that are not met in the real data. We propose a elegant depth-aware\nscaling that allows more efficient shape learning on real data. Experiments\nshow that our joint reconstruction learned with the proposed strategy\nsignificantly outperforms the SOTA. Our code and models are available at\nhttps://virtualhumans.mpi-inf.mpg.de/chore\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xianghui Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatnagar_B/0/1/0/all/0/1\">Bharat Lal Bhatnagar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pons_Moll_G/0/1/0/all/0/1\">Gerard Pons-Moll</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stripformer: Strip Transformer for Fast Image Deblurring. (arXiv:2204.04627v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.04627","description":"<p>Images taken in dynamic scenes may contain unwanted motion blur, which\nsignificantly degrades visual quality. Such blur causes short- and long-range\nregion-specific smoothing artifacts that are often directional and non-uniform,\nwhich is difficult to be removed. Inspired by the current success of\ntransformers on computer vision and image processing tasks, we develop,\nStripformer, a transformer-based architecture that constructs intra- and\ninter-strip tokens to reweight image features in the horizontal and vertical\ndirections to catch blurred patterns with different orientations. It stacks\ninterlaced intra-strip and inter-strip attention layers to reveal blur\nmagnitudes. In addition to detecting region-specific blurred patterns of\nvarious orientations and magnitudes, Stripformer is also a token-efficient and\nparameter-efficient transformer model, demanding much less memory usage and\ncomputation cost than the vanilla transformer but works better without relying\non tremendous training data. Experimental results show that Stripformer\nperforms favorably against state-of-the-art models in dynamic scene deblurring.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tsai_F/0/1/0/all/0/1\">Fu-Jen Tsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yan-Tsung Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yen-Yu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsai_C/0/1/0/all/0/1\">Chung-Chi Tsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chia-Wen Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TEMOS: Generating diverse human motions from textual descriptions. (arXiv:2204.14109v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.14109","description":"<p>We address the problem of generating diverse 3D human motions from textual\ndescriptions. This challenging task requires joint modeling of both modalities:\nunderstanding and extracting useful human-centric information from the text,\nand then generating plausible and realistic sequences of human poses. In\ncontrast to most previous work which focuses on generating a single,\ndeterministic, motion from a textual description, we design a variational\napproach that can produce multiple diverse human motions. We propose TEMOS, a\ntext-conditioned generative model leveraging variational autoencoder (VAE)\ntraining with human motion data, in combination with a text encoder that\nproduces distribution parameters compatible with the VAE latent space. We show\nthe TEMOS framework can produce both skeleton-based animations as in prior\nwork, as well more expressive SMPL body motions. We evaluate our approach on\nthe KIT Motion-Language benchmark and, despite being relatively\nstraightforward, demonstrate significant improvements over the state of the\nart. Code and models are available on our webpage.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Petrovich_M/0/1/0/all/0/1\">Mathis Petrovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Black_M/0/1/0/all/0/1\">Michael J. Black</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varol_G/0/1/0/all/0/1\">G&#xfc;l Varol</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Detection of Unknown Objects on Roads for Autonomous Driving. (arXiv:2205.01414v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.01414","description":"<p>Tremendous progress in deep learning over the last years has led towards a\nfuture with autonomous vehicles on our roads. Nevertheless, the performance of\ntheir perception systems is strongly dependent on the quality of the utilized\ntraining data. As these usually only cover a fraction of all object classes an\nautonomous driving system will face, such systems struggle with handling the\nunexpected. In order to safely operate on public roads, the identification of\nobjects from unknown classes remains a crucial task. In this paper, we propose\na novel pipeline to detect unknown objects. Instead of focusing on a single\nsensor modality, we make use of lidar and camera data by combining state-of-the\nart detection models in a sequential manner. We evaluate our approach on the\nWaymo Open Perception Dataset and point out current research gaps in anomaly\ndetection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bogdoll_D/0/1/0/all/0/1\">Daniel Bogdoll</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eisen_E/0/1/0/all/0/1\">Enrico Eisen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nitsche_M/0/1/0/all/0/1\">Maximilian Nitsche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scheib_C/0/1/0/all/0/1\">Christin Scheib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zollner_J/0/1/0/all/0/1\">J. Marius Z&#xf6;llner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EdgeViTs: Competing Light-weight CNNs on Mobile Devices with Vision Transformers. (arXiv:2205.03436v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.03436","description":"<p>Self-attention based models such as vision transformers (ViTs) have emerged\nas a very competitive architecture alternative to convolutional neural networks\n(CNNs) in computer vision. Despite increasingly stronger variants with\never-higher recognition accuracies, due to the quadratic complexity of\nself-attention, existing ViTs are typically demanding in computation and model\nsize. Although several successful design choices (e.g., the convolutions and\nhierarchical multi-stage structure) of prior CNNs have been reintroduced into\nrecent ViTs, they are still not sufficient to meet the limited resource\nrequirements of mobile devices. This motivates a very recent attempt to develop\nlight ViTs based on the state-of-the-art MobileNet-v2, but still leaves a\nperformance gap behind. In this work, pushing further along this under-studied\ndirection we introduce EdgeViTs, a new family of light-weight ViTs that, for\nthe first time, enable attention-based vision models to compete with the best\nlight-weight CNNs in the tradeoff between accuracy and on-device efficiency.\nThis is realized by introducing a highly cost-effective local-global-local\n(LGL) information exchange bottleneck based on optimal integration of\nself-attention and convolutions. For device-dedicated evaluation, rather than\nrelying on inaccurate proxies like the number of FLOPs or parameters, we adopt\na practical approach of focusing directly on on-device latency and, for the\nfirst time, energy efficiency. Specifically, we show that our models are\nPareto-optimal when both accuracy-latency and accuracy-energy trade-offs are\nconsidered, achieving strict dominance over other ViTs in almost all cases and\ncompeting with the most efficient CNNs. Code is available at\nhttps://github.com/saic-fi/edgevit.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Junting Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bulat_A/0/1/0/all/0/1\">Adrian Bulat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_F/0/1/0/all/0/1\">Fuwen Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiatian Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dudziak_L/0/1/0/all/0/1\">Lukasz Dudziak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tzimiropoulos_G/0/1/0/all/0/1\">Georgios Tzimiropoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martinez_B/0/1/0/all/0/1\">Brais Martinez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-shot Class-incremental Learning for 3D Point Cloud Objects. (arXiv:2205.15225v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.15225","description":"<p>Few-shot class-incremental learning (FSCIL) aims to incrementally fine-tune a\nmodel (trained on base classes) for a novel set of classes using a few examples\nwithout forgetting the previous training. Recent efforts address this problem\nprimarily on 2D images. However, due to the advancement of camera technology,\n3D point cloud data has become more available than ever, which warrants\nconsidering FSCIL on 3D data. This paper addresses FSCIL in the 3D domain. In\naddition to well-known issues of catastrophic forgetting of past knowledge and\noverfitting of few-shot data, 3D FSCIL can bring newer challenges. For example,\nbase classes may contain many synthetic instances in a realistic scenario. In\ncontrast, only a few real-scanned samples (from RGBD sensors) of novel classes\nare available in incremental steps. Due to the data variation from synthetic to\nreal, FSCIL endures additional challenges, degrading performance in later\nincremental steps. We attempt to solve this problem using Microshapes\n(orthogonal basis vectors) by describing any 3D objects using a pre-defined set\nof rules. It supports incremental training with few-shot examples minimizing\nsynthetic to real data variation. We propose new test protocols for 3D FSCIL\nusing popular synthetic datasets (ModelNet and ShapeNet) and 3D real-scanned\ndatasets (ScanObjectNN and CO3D). By comparing state-of-the-art methods, we\nestablish the effectiveness of our approach in the 3D domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_T/0/1/0/all/0/1\">Townim Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheraghian_A/0/1/0/all/0/1\">Ali Cheraghian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramasinghe_S/0/1/0/all/0/1\">Sameera Ramasinghe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmadi_S/0/1/0/all/0/1\">Sahar Ahmadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saberi_M/0/1/0/all/0/1\">Morteza Saberi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahman_S/0/1/0/all/0/1\">Shafin Rahman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EfficientFormer: Vision Transformers at MobileNet Speed. (arXiv:2206.01191v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.01191","description":"<p>Vision Transformers (ViT) have shown rapid progress in computer vision tasks,\nachieving promising results on various benchmarks. However, due to the massive\nnumber of parameters and model design, e.g., attention mechanism, ViT-based\nmodels are generally times slower than lightweight convolutional networks.\nTherefore, the deployment of ViT for real-time applications is particularly\nchallenging, especially on resource-constrained hardware such as mobile\ndevices. Recent efforts try to reduce the computation complexity of ViT through\nnetwork architecture search or hybrid design with MobileNet block, yet the\ninference speed is still unsatisfactory. This leads to an important question:\ncan transformers run as fast as MobileNet while obtaining high performance? To\nanswer this, we first revisit the network architecture and operators used in\nViT-based models and identify inefficient designs. Then we introduce a\ndimension-consistent pure transformer (without MobileNet blocks) as a design\nparadigm. Finally, we perform latency-driven slimming to get a series of final\nmodels dubbed EfficientFormer. Extensive experiments show the superiority of\nEfficientFormer in performance and speed on mobile devices. Our fastest model,\nEfficientFormer-L1, achieves $79.2\\%$ top-1 accuracy on ImageNet-1K with only\n$1.6$ ms inference latency on iPhone 12 (compiled with CoreML), which { runs as\nfast as MobileNetV2$\\times 1.4$ ($1.6$ ms, $74.7\\%$ top-1),} and our largest\nmodel, EfficientFormer-L7, obtains $83.3\\%$ accuracy with only $7.0$ ms\nlatency. Our work proves that properly designed transformers can reach\nextremely low latency on mobile devices while maintaining high performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_G/0/1/0/all/0/1\">Geng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yang Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_E/0/1/0/all/0/1\">Eric Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Evangelidis_G/0/1/0/all/0/1\">Georgios Evangelidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tulyakov_S/0/1/0/all/0/1\">Sergey Tulyakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Jian Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OmniXAI: A Library for Explainable AI. (arXiv:2206.01612v5 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2206.01612","description":"<p>We introduce OmniXAI (short for Omni eXplainable AI), an open-source Python\nlibrary of eXplainable AI (XAI), which offers omni-way explainable AI\ncapabilities and various interpretable machine learning techniques to address\nthe pain points of understanding and interpreting the decisions made by machine\nlearning (ML) in practice. OmniXAI aims to be a one-stop comprehensive library\nthat makes explainable AI easy for data scientists, ML researchers and\npractitioners who need explanation for various types of data, models and\nexplanation methods at different stages of ML process (data exploration,\nfeature engineering, model development, evaluation, and decision-making, etc).\nIn particular, our library includes a rich family of explanation methods\nintegrated in a unified interface, which supports multiple data types (tabular\ndata, images, texts, time-series), multiple types of ML models (traditional ML\nin Scikit-learn and deep learning models in PyTorch/TensorFlow), and a range of\ndiverse explanation methods including \"model-specific\" and \"model-agnostic\"\nones (such as feature-attribution explanation, counterfactual explanation,\ngradient-based explanation, etc). For practitioners, the library provides an\neasy-to-use unified interface to generate the explanations for their\napplications by only writing a few lines of codes, and also a GUI dashboard for\nvisualization of different explanations for more insights about decisions. In\nthis technical report, we present OmniXAI's design principles, system\narchitectures, and major functionalities, and also demonstrate several example\nuse cases across different types of data, tasks, and models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wenzhuo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1\">Hung Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1\">Silvio Savarese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoi_S/0/1/0/all/0/1\">Steven C.H. Hoi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Feature Re-calibration based Multiple Instance Learning for Whole Slide Image Classification. (arXiv:2206.10878v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.10878","description":"<p>Whole slide image (WSI) classification is a fundamental task for the\ndiagnosis and treatment of diseases; but, curation of accurate labels is\ntime-consuming and limits the application of fully-supervised methods. To\naddress this, multiple instance learning (MIL) is a popular method that poses\nclassification as a weakly supervised learning task with slide-level labels\nonly. While current MIL methods apply variants of the attention mechanism to\nre-weight instance features with stronger models, scant attention is paid to\nthe properties of the data distribution. In this work, we propose to\nre-calibrate the distribution of a WSI bag (instances) by using the statistics\nof the max-instance (critical) feature. We assume that in binary MIL, positive\nbags have larger feature magnitudes than negatives, thus we can enforce the\nmodel to maximize the discrepancy between bags with a metric feature loss that\nmodels positive bags as out-of-distribution. To achieve this, unlike existing\nMIL methods that use single-batch training modes, we propose balanced-batch\nsampling to effectively use the feature loss i.e., (+/-) bags simultaneously.\nFurther, we employ a position encoding module (PEM) to model\nspatial/morphological information, and perform pooling by multi-head\nself-attention (PSMA) with a Transformer encoder. Experimental results on\nexisting benchmark datasets show our approach is effective and improves over\nstate-of-the-art MIL methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chikontwe_P/0/1/0/all/0/1\">Philip Chikontwe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nam_S/0/1/0/all/0/1\">Soo Jeong Nam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Go_H/0/1/0/all/0/1\">Heounjeong Go</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Meejeong Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sung_H/0/1/0/all/0/1\">Hyun Jung Sung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Sang Hyun Park</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning approach for Classifying Trusses and Runners of Strawberries. (arXiv:2207.02721v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.02721","description":"<p>The use of artificial intelligence in the agricultural sector has been\ngrowing at a rapid rate to automate farming activities. Emergent farming\ntechnologies focus on mapping and classification of plants, fruits, diseases,\nand soil types. Although, assisted harvesting and pruning applications using\ndeep learning algorithms are in the early development stages, there is a demand\nfor solutions to automate such processes. This paper proposes the use of Deep\nLearning for the classification of trusses and runners of strawberry plants\nusing semantic segmentation and dataset augmentation. The proposed approach is\nbased on the use of noises (i.e. Gaussian, Speckle, Poisson and\nSalt-and-Pepper) to artificially augment the dataset and compensate the low\nnumber of data samples and increase the overall classification performance. The\nresults are evaluated using mean average of precision, recall and F1 score. The\nproposed approach achieved 91%, 95% and 92% on precision, recall and F1 score,\nrespectively, for truss detection using the ResNet101 with dataset augmentation\nutilising Salt-and-Pepper noise; and 83%, 53% and 65% on precision, recall and\nF1 score, respectively, for truss detection using the ResNet50 with dataset\naugmentation utilising Poisson noise.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pomykala_J/0/1/0/all/0/1\">Jakub Pomykala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lemos_F/0/1/0/all/0/1\">Francisco de Lemos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ihianle_I/0/1/0/all/0/1\">Isibor Kennedy Ihianle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adama_D/0/1/0/all/0/1\">David Ada Adama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Machado_P/0/1/0/all/0/1\">Pedro Machado</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learned Video Compression via Heterogeneous Deformable Compensation Network. (arXiv:2207.04589v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2207.04589","description":"<p>Learned video compression has recently emerged as an essential research topic\nin developing advanced video compression technologies, where motion\ncompensation is considered one of the most challenging issues. In this paper,\nwe propose a learned video compression framework via heterogeneous deformable\ncompensation strategy (HDCVC) to tackle the problems of unstable compression\nperformance caused by single-size deformable kernels in downsampled feature\ndomain. More specifically, instead of utilizing optical flow warping or\nsingle-size-kernel deformable alignment, the proposed algorithm extracts\nfeatures from the two adjacent frames to estimate content-adaptive\nheterogeneous deformable (HetDeform) kernel offsets. Then we transform the\nreference features with the HetDeform convolution to accomplish motion\ncompensation. Moreover, we design a Spatial-Neighborhood-Conditioned Divisive\nNormalization (SNCDN) to achieve more effective data Gaussianization combined\nwith the Generalized Divisive Normalization. Furthermore, we propose a\nmulti-frame enhanced reconstruction module for exploiting context and temporal\ninformation for final quality enhancement. Experimental results indicate that\nHDCVC achieves superior performance than the recent state-of-the-art learned\nvideo compression approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1\">Huairui Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Z/0/1/0/all/0/1\">Zhenzhong Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_C/0/1/0/all/0/1\">Chang Wen Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical Average Precision Training for Pertinent Image Retrieval. (arXiv:2207.04873v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.04873","description":"<p>Image Retrieval is commonly evaluated with Average Precision (AP) or\nRecall@k. Yet, those metrics, are limited to binary labels and do not take into\naccount errors' severity. This paper introduces a new hierarchical AP training\nmethod for pertinent image retrieval (HAP-PIER). HAPPIER is based on a new H-AP\nmetric, which leverages a concept hierarchy to refine AP by integrating errors'\nimportance and better evaluate rankings. To train deep models with H-AP, we\ncarefully study the problem's structure and design a smooth lower bound\nsurrogate combined with a clustering loss that ensures consistent ordering.\nExtensive experiments on 6 datasets show that HAPPIER significantly outperforms\nstate-of-the-art methods for hierarchical retrieval, while being on par with\nthe latest approaches when evaluating fine-grained ranking performances.\nFinally, we show that HAPPIER leads to better organization of the embedding\nspace, and prevents most severe failure cases of non-hierarchical methods. Our\ncode is publicly available at: https://github.com/elias-ramzi/HAPPIER.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ramzi_E/0/1/0/all/0/1\">Elias Ramzi</a> (CNAM, CEDRIC - VERTIGO), <a href=\"http://arxiv.org/find/cs/1/au:+Audebert_N/0/1/0/all/0/1\">Nicolas Audebert</a> (CNAM, CEDRIC - VERTIGO), <a href=\"http://arxiv.org/find/cs/1/au:+Thome_N/0/1/0/all/0/1\">Nicolas Thome</a> (CNAM, ISIR, CEDRIC - VERTIGO), <a href=\"http://arxiv.org/find/cs/1/au:+Rambour_C/0/1/0/all/0/1\">Cl&#xe9;ment Rambour</a> (CNAM, CEDRIC - VERTIGO), <a href=\"http://arxiv.org/find/cs/1/au:+Bitot_X/0/1/0/all/0/1\">Xavier Bitot</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pyramid Transformer for Traffic Sign Detection. (arXiv:2207.06067v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.06067","description":"<p>Traffic sign detection is a vital task in the visual system of self-driving\ncars and the automated driving system. Recently, novel Transformer-based models\nhave achieved encouraging results for various computer vision tasks. We still\nobserved that vanilla ViT could not yield satisfactory results in traffic sign\ndetection because the overall size of the datasets is very small and the class\ndistribution of traffic signs is extremely unbalanced. To overcome this\nproblem, a novel Pyramid Transformer with locality mechanisms is proposed in\nthis paper. Specifically, Pyramid Transformer has several spatial pyramid\nreduction layers to shrink and embed the input image into tokens with rich\nmulti-scale context by using atrous convolutions. Moreover, it inherits an\nintrinsic scale invariance inductive bias and is able to learn local feature\nrepresentation for objects at various scales, thereby enhancing the network\nrobustness against the size discrepancy of traffic signs. The experiments are\nconducted on the German Traffic Sign Detection Benchmark (GTSDB). The results\ndemonstrate the superiority of the proposed model in the traffic sign detection\ntasks. More specifically, Pyramid Transformer achieves 77.8% mAP on GTSDB when\napplied to the Cascade RCNN as the backbone, which surpasses most well-known\nand widely-used state-of-the-art models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Manzari_O/0/1/0/all/0/1\">Omid Nejati Manzari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boudesh_A/0/1/0/all/0/1\">Amin Boudesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shokouhi_S/0/1/0/all/0/1\">Shahriar B. Shokouhi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarially-Aware Robust Object Detector. (arXiv:2207.06202v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.06202","description":"<p>Object detection, as a fundamental computer vision task, has achieved a\nremarkable progress with the emergence of deep neural networks. Nevertheless,\nfew works explore the adversarial robustness of object detectors to resist\nadversarial attacks for practical applications in various real-world scenarios.\nDetectors have been greatly challenged by unnoticeable perturbation, with sharp\nperformance drop on clean images and extremely poor performance on adversarial\nimages. In this work, we empirically explore the model training for adversarial\nrobustness in object detection, which greatly attributes to the conflict\nbetween learning clean images and adversarial images. To mitigate this issue,\nwe propose a Robust Detector (RobustDet) based on adversarially-aware\nconvolution to disentangle gradients for model learning on clean and\nadversarial images. RobustDet also employs the Adversarial Image Discriminator\n(AID) and Consistent Features with Reconstruction (CFR) to ensure a reliable\nrobustness. Extensive experiments on PASCAL VOC and MS-COCO demonstrate that\nour model effectively disentangles gradients and significantly enhances the\ndetection robustness with maintaining the detection ability on clean images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1\">Ziyi Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_P/0/1/0/all/0/1\">Pengxu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Liang Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sample-dependent Adaptive Temperature Scaling for Improved Calibration. (arXiv:2207.06211v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.06211","description":"<p>It is now well known that neural networks can be wrong with high confidence\nin their predictions, leading to poor calibration. The most common post-hoc\napproach to compensate for this is to perform temperature scaling, which\nadjusts the confidences of the predictions on any input by scaling the logits\nby a fixed value. Whilst this approach typically improves the average\ncalibration across the whole test dataset, this improvement typically reduces\nthe individual confidences of the predictions irrespective of whether the\nclassification of a given input is correct or incorrect. With this insight, we\nbase our method on the observation that different samples contribute to the\ncalibration error by varying amounts, with some needing to increase their\nconfidence and others needing to decrease it. Therefore, for each input, we\npropose to predict a different temperature value, allowing us to adjust the\nmismatch between confidence and accuracy at a finer granularity. Furthermore,\nwe observe improved results on OOD detection and can also extract a notion of\nhardness for the data-points. Our method is applied post-hoc, consequently\nusing very little computation time and with a negligible memory footprint and\nis applied to off-the-shelf pre-trained classifiers. We test our method on the\nResNet50 and WideResNet28-10 architectures using the CIFAR10/100 and\nTiny-ImageNet datasets, showing that producing per-data-point temperatures is\nbeneficial also for the expected calibration error across the whole test set.\nCode is available at: https://github.com/thwjoy/adats.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Joy_T/0/1/0/all/0/1\">Tom Joy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pinto_F/0/1/0/all/0/1\">Francesco Pinto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1\">Ser-Nam Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip H. S. Torr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dokania_P/0/1/0/all/0/1\">Puneet K. Dokania</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SHREC 2022 Track on Online Detection of Heterogeneous Gestures. (arXiv:2207.06706v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.06706","description":"<p>This paper presents the outcomes of a contest organized to evaluate methods\nfor the online recognition of heterogeneous gestures from sequences of 3D hand\nposes. The task is the detection of gestures belonging to a dictionary of 16\nclasses characterized by different pose and motion features. The dataset\nfeatures continuous sequences of hand tracking data where the gestures are\ninterleaved with non-significant motions. The data have been captured using the\nHololens 2 finger tracking system in a realistic use-case of mixed reality\ninteraction. The evaluation is based not only on the detection performances but\nalso on the latency and the false positives, making it possible to understand\nthe feasibility of practical interaction tools based on the algorithms\nproposed. The outcomes of the contest's evaluation demonstrate the necessity of\nfurther research to reduce recognition errors, while the computational cost of\nthe algorithms proposed is sufficiently low.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Caputo_A/0/1/0/all/0/1\">Ariel Caputo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Emporio_M/0/1/0/all/0/1\">Marco Emporio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giachetti_A/0/1/0/all/0/1\">Andrea Giachetti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cristani_M/0/1/0/all/0/1\">Marco Cristani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borghi_G/0/1/0/all/0/1\">Guido Borghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DEusanio_A/0/1/0/all/0/1\">Andrea D&#x27;Eusanio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_M/0/1/0/all/0/1\">Minh-Quan Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Hai-Dang Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_M/0/1/0/all/0/1\">Minh-Triet Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ambellan_F/0/1/0/all/0/1\">F. Ambellan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanik_M/0/1/0/all/0/1\">M. Hanik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nava_Yazdani_E/0/1/0/all/0/1\">E. Nava-Yazdani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tycowicz_C/0/1/0/all/0/1\">C. von Tycowicz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MDM:Visual Explanations for Neural Networks via Multiple Dynamic Mask. (arXiv:2207.08046v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.08046","description":"<p>The active region lookup of a neural network tells us which regions the\nneural network focuses on when making a decision, which gives us a basis for\ninterpretability when the neural network makes a classification decision. We\npropose an algorithm Multiple Dynamic Mask(MDM), which is a general saliency\ngraph query method with interpretability of the inference process. Its proposal\nis based on an assumption: when a picture is input to a neural network that has\nbeen trained, the activation features related to classification will affect the\nclassification results of the neural network, and the features unrelated to\nclassification will hardly affect the classification results of the network.\nMDM: A learning-based end-to-end algorithm for finding regions of interest for\nneural network classification. It has the following advantages: 1. It has the\ninterpretability of the reasoning process. 2. It is universal, it can be used\nfor any neural network and does not depend on the internal structure of the\nneural network. 3. The search performance is better. Because the algorithm is\nbased on learning to generate masks and has the ability to adapt to different\ndata and networks, the performance is better than the method proposed in the\nprevious paper. For the MDM saliency map search algorithm, we experimentally\ncompared the performance indicators of various saliency map search methods and\nthe MDM with ResNet and DenseNet as the trained neural networks. The search\neffect performance of the MDM reached the state of the art. We applied the MDM\nto the interpretable neural network ProtoPNet and XProtoNet, which improved the\ninterpretability of the model and the prototype search performance. We\nvisualize the performance of convolutional neural architecture and Transformer\narchitecture on saliency map search.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yitao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Longzhen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yihang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Lianghua He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DID-M3D: Decoupling Instance Depth for Monocular 3D Object Detection. (arXiv:2207.08531v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.08531","description":"<p>Monocular 3D detection has drawn much attention from the community due to its\nlow cost and setup simplicity. It takes an RGB image as input and predicts 3D\nboxes in the 3D space. The most challenging sub-task lies in the instance depth\nestimation. Previous works usually use a direct estimation method. However, in\nthis paper we point out that the instance depth on the RGB image is\nnon-intuitive. It is coupled by visual depth clues and instance attribute\nclues, making it hard to be directly learned in the network. Therefore, we\npropose to reformulate the instance depth to the combination of the instance\nvisual surface depth (visual depth) and the instance attribute depth (attribute\ndepth). The visual depth is related to objects' appearances and positions on\nthe image. By contrast, the attribute depth relies on objects' inherent\nattributes, which are invariant to the object affine transformation on the\nimage. Correspondingly, we decouple the 3D location uncertainty into visual\ndepth uncertainty and attribute depth uncertainty. By combining different types\nof depths and associated uncertainties, we can obtain the final instance depth.\nFurthermore, data augmentation in monocular 3D detection is usually limited due\nto the physical nature, hindering the boost of performance. Based on the\nproposed instance depth disentanglement strategy, we can alleviate this\nproblem. Evaluated on KITTI, our method achieves new state-of-the-art results,\nand extensive ablation studies validate the effectiveness of each component in\nour method. The codes are released at https://github.com/SPengLiang/DID-M3D.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_L/0/1/0/all/0/1\">Liang Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiaopei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zheng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Haifeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1\">Deng Cai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Latency-Aware Collaborative Perception. (arXiv:2207.08560v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.08560","description":"<p>Collaborative perception has recently shown great potential to improve\nperception capabilities over single-agent perception. Existing collaborative\nperception methods usually consider an ideal communication environment.\nHowever, in practice, the communication system inevitably suffers from latency\nissues, causing potential performance degradation and high risks in\nsafety-critical applications, such as autonomous driving. To mitigate the\neffect caused by the inevitable latency, from a machine learning perspective,\nwe present the first latency-aware collaborative perception system, which\nactively adapts asynchronous perceptual features from multiple agents to the\nsame time stamp, promoting the robustness and effectiveness of collaboration.\nTo achieve such a feature-level synchronization, we propose a novel latency\ncompensation module, called SyncNet, which leverages feature-attention\nsymbiotic estimation and time modulation techniques. Experiments results show\nthat the proposed latency aware collaborative perception system with SyncNet\ncan outperforms the state-of-the-art collaborative perception method by 15.6%\nin the communication latency scenario and keep collaborative perception being\nsuperior to single agent perception under severe latency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lei_Z/0/1/0/all/0/1\">Zixing Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1\">Shunli Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yue Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenjun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Siheng Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AiATrack: Attention in Attention for Transformer Visual Tracking. (arXiv:2207.09603v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.09603","description":"<p>Transformer trackers have achieved impressive advancements recently, where\nthe attention mechanism plays an important role. However, the independent\ncorrelation computation in the attention mechanism could result in noisy and\nambiguous attention weights, which inhibits further performance improvement. To\naddress this issue, we propose an attention in attention (AiA) module, which\nenhances appropriate correlations and suppresses erroneous ones by seeking\nconsensus among all correlation vectors. Our AiA module can be readily applied\nto both self-attention blocks and cross-attention blocks to facilitate feature\naggregation and information propagation for visual tracking. Moreover, we\npropose a streamlined Transformer tracking framework, dubbed AiATrack, by\nintroducing efficient feature reuse and target-background embeddings to make\nfull use of temporal references. Experiments show that our tracker achieves\nstate-of-the-art performance on six tracking benchmarks while running at a\nreal-time speed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Shenyuan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chunluan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Chao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinggang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Junsong Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ERA: Expert Retrieval and Assembly for Early Action Prediction. (arXiv:2207.09675v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.09675","description":"<p>Early action prediction aims to successfully predict the class label of an\naction before it is completely performed. This is a challenging task because\nthe beginning stages of different actions can be very similar, with only minor\nsubtle differences for discrimination. In this paper, we propose a novel Expert\nRetrieval and Assembly (ERA) module that retrieves and assembles a set of\nexperts most specialized at using discriminative subtle differences, to\ndistinguish an input sample from other highly similar samples. To encourage our\nmodel to effectively use subtle differences for early action prediction, we\npush experts to discriminate exclusively between samples that are highly\nsimilar, forcing these experts to learn to use subtle differences that exist\nbetween those samples. Additionally, we design an effective Expert Learning\nRate Optimization method that balances the experts' optimization and leads to\nbetter performance. We evaluate our ERA module on four public action datasets\nand achieve state-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Foo_L/0/1/0/all/0/1\">Lin Geng Foo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tianjiao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahmani_H/0/1/0/all/0/1\">Hossein Rahmani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ke_Q/0/1/0/all/0/1\">Qiuhong Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jun Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AU-Supervised Convolutional Vision Transformers for Synthetic Facial Expression Recognition. (arXiv:2207.09777v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.09777","description":"<p>The paper describes our proposed methodology for the six basic expression\nclassification track of Affective Behavior Analysis in-the-wild (ABAW)\nCompetition 2022. In Learing from Synthetic Data(LSD) task, facial expression\nrecognition (FER) methods aim to learn the representation of expression from\nthe artificially generated data and generalise to real data. Because of the\nambiguous of the synthetic data and the objectivity of the facial Action Unit\n(AU), we resort to the AU information for performance boosting, and make\ncontributions as follows. First, to adapt the model to synthetic scenarios, we\nuse the knowledge from pre-trained large-scale face recognition data. Second,\nwe propose a conceptually-new framework, termed as AU-Supervised Convolutional\nVision Transformers (AU-CVT), which clearly improves the performance of FER by\njointly training auxiliary datasets with AU or pseudo AU labels. Our AU-CVT\nachieved F1 score as $0.6863$, accuracy as $0.7433$ on the validation set. The\nsource code of our work is publicly available online:\nhttps://github.com/msy1412/ABAW4\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mao_S/0/1/0/all/0/1\">Shuyi Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xinpeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Junyao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1\">Xiaojiang Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Landmark-based Stent Tracking in X-ray Fluoroscopy. (arXiv:2207.09933v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.09933","description":"<p>In clinical procedures of angioplasty (i.e., open clogged coronary arteries),\ndevices such as balloons and stents need to be placed and expanded in arteries\nunder the guidance of X-ray fluoroscopy. Due to the limitation of X-ray dose,\nthe resulting images are often noisy. To check the correct placement of these\ndevices, typically multiple motion-compensated frames are averaged to enhance\nthe view. Therefore, device tracking is a necessary procedure for this purpose.\nEven though angioplasty devices are designed to have radiopaque markers for the\nease of tracking, current methods struggle to deliver satisfactory results due\nto the small marker size and complex scenes in angioplasty. In this paper, we\npropose an end-to-end deep learning framework for single stent tracking, which\nconsists of three hierarchical modules: U-Net based landmark detection, ResNet\nbased stent proposal and feature extraction, and graph convolutional neural\nnetwork (GCN) based stent tracking that temporally aggregates both spatial\ninformation and appearance features. The experiments show that our method\nperforms significantly better in detection compared with the state-of-the-art\npoint-based tracking models. In addition, its fast inference speed satisfies\nclinical requirements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Luojie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yikang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Li Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1\">Eric Z. Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1\">Shanhui Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"World Robot Challenge 2020 -- Partner Robot: A Data-Driven Approach for Room Tidying with Mobile Manipulator. (arXiv:2207.10106v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2207.10106","description":"<p>Tidying up a household environment using a mobile manipulator poses various\nchallenges in robotics, such as adaptation to large real-world environmental\nvariations, and safe and robust deployment in the presence of humans.The\nPartner Robot Challenge in World Robot Challenge (WRC) 2020, a global\ncompetition held in September 2021, benchmarked tidying tasks in the real home\nenvironments, and importantly, tested for full system performances.For this\nchallenge, we developed an entire household service robot system, which\nleverages a data-driven approach to adapt to numerous edge cases that occur\nduring the execution, instead of classical manual pre-programmed solutions. In\nthis paper, we describe the core ingredients of the proposed robot system,\nincluding visual recognition, object manipulation, and motion planning. Our\nrobot system won the second prize, verifying the effectiveness and potential of\ndata-driven robot systems for mobile manipulation in home environments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Matsushima_T/0/1/0/all/0/1\">Tatsuya Matsushima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noguchi_Y/0/1/0/all/0/1\">Yuki Noguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arima_J/0/1/0/all/0/1\">Jumpei Arima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aoki_T/0/1/0/all/0/1\">Toshiki Aoki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okita_Y/0/1/0/all/0/1\">Yuki Okita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ikeda_Y/0/1/0/all/0/1\">Yuya Ikeda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ishimoto_K/0/1/0/all/0/1\">Koki Ishimoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taniguchi_S/0/1/0/all/0/1\">Shohei Taniguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamashita_Y/0/1/0/all/0/1\">Yuki Yamashita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seto_S/0/1/0/all/0/1\">Shoichi Seto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1\">Shixiang Shane Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iwasawa_Y/0/1/0/all/0/1\">Yusuke Iwasawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsuo_Y/0/1/0/all/0/1\">Yutaka Matsuo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BRACE: The Breakdancing Competition Dataset for Dance Motion Synthesis. (arXiv:2207.10120v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.10120","description":"<p>Generative models for audio-conditioned dance motion synthesis map music\nfeatures to dance movements. Models are trained to associate motion patterns to\naudio patterns, usually without an explicit knowledge of the human body. This\napproach relies on a few assumptions: strong music-dance correlation,\ncontrolled motion data and relatively simple poses and movements. These\ncharacteristics are found in all existing datasets for dance motion synthesis,\nand indeed recent methods can achieve good results.We introduce a new dataset\naiming to challenge these common assumptions, compiling a set of dynamic dance\nsequences displaying complex human poses. We focus on breakdancing which\nfeatures acrobatic moves and tangled postures. We source our data from the Red\nBull BC One competition videos. Estimating human keypoints from these videos is\ndifficult due to the complexity of the dance, as well as the multiple moving\ncameras recording setup. We adopt a hybrid labelling pipeline leveraging deep\nestimation models as well as manual annotations to obtain good quality keypoint\nsequences at a reduced cost. Our efforts produced the BRACE dataset, which\ncontains over 3 hours and 30 minutes of densely annotated poses. We test\nstate-of-the-art methods on BRACE, showing their limitations when evaluated on\ncomplex sequences. Our dataset can readily foster advance in dance motion\nsynthesis. With intricate poses and swift movements, models are forced to go\nbeyond learning a mapping between modalities and reason more effectively about\nbody structure and movements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moltisanti_D/0/1/0/all/0/1\">Davide Moltisanti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jinyi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1\">Bo Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1\">Chen Change Loy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Knowledge Tracing. (arXiv:2207.10157v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.10157","description":"<p>Each year, thousands of people learn new visual categorization tasks --\nradiologists learn to recognize tumors, birdwatchers learn to distinguish\nsimilar species, and crowd workers learn how to annotate valuable data for\napplications like autonomous driving. As humans learn, their brain updates the\nvisual features it extracts and attend to, which ultimately informs their final\nclassification decisions. In this work, we propose a novel task of tracing the\nevolving classification behavior of human learners as they engage in\nchallenging visual classification tasks. We propose models that jointly extract\nthe visual features used by learners as well as predicting the classification\nfunctions they utilize. We collect three challenging new datasets from real\nhuman learners in order to evaluate the performance of different visual\nknowledge tracing methods. Our results show that our recurrent models are able\nto predict the classification behavior of human learners on three challenging\nmedical image and species identification tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kondapaneni_N/0/1/0/all/0/1\">Neehar Kondapaneni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perona_P/0/1/0/all/0/1\">Pietro Perona</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aodha_O/0/1/0/all/0/1\">Oisin Mac Aodha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video Anomaly Detection by Solving Decoupled Spatio-Temporal Jigsaw Puzzles. (arXiv:2207.10172v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.10172","description":"<p>Video Anomaly Detection (VAD) is an important topic in computer vision.\nMotivated by the recent advances in self-supervised learning, this paper\naddresses VAD by solving an intuitive yet challenging pretext task, i.e.,\nspatio-temporal jigsaw puzzles, which is cast as a multi-label fine-grained\nclassification problem. Our method exhibits several advantages over existing\nworks: 1) the spatio-temporal jigsaw puzzles are decoupled in terms of spatial\nand temporal dimensions, responsible for capturing highly discriminative\nappearance and motion features, respectively; 2) full permutations are used to\nprovide abundant jigsaw puzzles covering various difficulty levels, allowing\nthe network to distinguish subtle spatio-temporal differences between normal\nand abnormal events; and 3) the pretext task is tackled in an end-to-end manner\nwithout relying on any pre-trained models. Our method outperforms\nstate-of-the-art counterparts on three public benchmarks. Especially on\nShanghaiTech Campus, the result is superior to reconstruction and\nprediction-based methods by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guodong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunhong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1\">Jie Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_X/0/1/0/all/0/1\">Xiuguo Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">Di Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic-Aware Fine-Grained Correspondence. (arXiv:2207.10456v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.10456","description":"<p>Establishing visual correspondence across images is a challenging and\nessential task. Recently, an influx of self-supervised methods have been\nproposed to better learn representations for visual correspondence. However, we\nfind that these methods often fail to leverage semantic information and\nover-rely on the matching of low-level features. In contrast, human vision is\ncapable of distinguishing between distinct objects as a pretext to tracking.\nInspired by this paradigm, we propose to learn semantic-aware fine-grained\ncorrespondence. Firstly, we demonstrate that semantic correspondence is\nimplicitly available through a rich set of image-level self-supervised methods.\nWe further design a pixel-level self-supervised learning objective which\nspecifically targets fine-grained correspondence. For downstream tasks, we fuse\nthese two kinds of complementary correspondence representations together,\ndemonstrating that they boost performance synergistically. Our method surpasses\nprevious state-of-the-art self-supervised methods using convolutional networks\non a variety of visual correspondence tasks, including video object\nsegmentation, human pose tracking, and human part tracking.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yingdong Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Renhao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kaifeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yang Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-07-24T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/","dc":"http://purl.org/dc/elements/1.1/"}}]}]}