{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-04-26T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"ChapterBreak: A Challenge Dataset for Long-Range Language Models. (arXiv:2204.10878v1 [cs.CL])","link":"http://arxiv.org/abs/2204.10878","description":"<p>While numerous architectures for long-range language models (LRLMs) have\nrecently been proposed, a meaningful evaluation of their discourse-level\nlanguage understanding capabilities has not yet followed. To this end, we\nintroduce ChapterBreak, a challenge dataset that provides an LRLM with a long\nsegment from a narrative that ends at a chapter boundary and asks it to\ndistinguish the beginning of the ground-truth next chapter from a set of\nnegative segments sampled from the same narrative. A fine-grained human\nannotation reveals that our dataset contains many complex types of chapter\ntransitions (e.g., parallel narratives, cliffhanger endings) that require\nprocessing global context to comprehend. Experiments on ChapterBreak show that\nexisting LRLMs fail to effectively leverage long-range context, substantially\nunderperforming a segment-level model trained directly for this task. We\npublicly release our ChapterBreak dataset to spur more principled future\nresearch into LRLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1\">Simeng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thai_K/0/1/0/all/0/1\">Katherine Thai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyyer_M/0/1/0/all/0/1\">Mohit Iyyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Locally Aggregated Feature Attribution on Natural Language Model Understanding. (arXiv:2204.10893v1 [cs.CL])","link":"http://arxiv.org/abs/2204.10893","description":"<p>With the growing popularity of deep-learning models, model understanding\nbecomes more important. Much effort has been devoted to demystify deep neural\nnetworks for better interpretability. Some feature attribution methods have\nshown promising results in computer vision, especially the gradient-based\nmethods where effectively smoothing the gradients with reference data is key to\na robust and faithful result. However, direct application of these\ngradient-based methods to NLP tasks is not trivial due to the fact that the\ninput consists of discrete tokens and the \"reference\" tokens are not explicitly\ndefined. In this work, we propose Locally Aggregated Feature Attribution\n(LAFA), a novel gradient-based feature attribution method for NLP models.\nInstead of relying on obscure reference tokens, it smooths gradients by\naggregating similar reference texts derived from language model embeddings. For\nevaluation purpose, we also design experiments on different NLP tasks including\nEntity Recognition and Sentiment Analysis on public datasets as well as key\nfeature detection on a constructed Amazon catalogue dataset. The superior\nperformance of the proposed method is demonstrated through experiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Sheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Haitao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_R/0/1/0/all/0/1\">Rui Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MCSE: Multimodal Contrastive Learning of Sentence Embeddings. (arXiv:2204.10931v1 [cs.CL])","link":"http://arxiv.org/abs/2204.10931","description":"<p>Learning semantically meaningful sentence embeddings is an open problem in\nnatural language processing. In this work, we propose a sentence embedding\nlearning approach that exploits both visual and textual information via a\nmultimodal contrastive objective. Through experiments on a variety of semantic\ntextual similarity tasks, we demonstrate that our approach consistently\nimproves the performance across various datasets and pre-trained encoders. In\nparticular, combining a small amount of multimodal data with a large text-only\ncorpus, we improve the state-of-the-art average Spearman's correlation by 1.7%.\nBy analyzing the properties of the textual embedding space, we show that our\nmodel excels in aligning semantically similar sentences, providing an\nexplanation for its improved performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Miaoran Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mosbach_M/0/1/0/all/0/1\">Marius Mosbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adelani_D/0/1/0/all/0/1\">David Ifeoluwa Adelani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hedderich_M/0/1/0/all/0/1\">Michael A. Hedderich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klakow_D/0/1/0/all/0/1\">Dietrich Klakow</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Multi-level Alignment Training Scheme for Video-and-Language Grounding. (arXiv:2204.10938v1 [cs.CV])","link":"http://arxiv.org/abs/2204.10938","description":"<p>To solve video-and-language grounding tasks, the key is for the network to\nunderstand the connection between the two modalities. For a pair of video and\nlanguage description, their semantic relation is reflected by their encodings'\nsimilarity. A good multi-modality encoder should be able to well capture both\ninputs' semantics and encode them in the shared feature space where embedding\ndistance gets properly translated into their semantic similarity. In this work,\nwe focused on this semantic connection between video and language, and\ndeveloped a multi-level alignment training scheme to directly shape the\nencoding process. Global and segment levels of video-language alignment pairs\nwere designed, based on the information similarity ranging from high-level\ncontext to fine-grained semantics. The contrastive loss was used to contrast\nthe encodings' similarities between the positive and negative alignment pairs,\nand to ensure the network is trained in such a way that similar information is\nencoded closely in the shared feature space while information of different\nsemantics is kept apart. Our multi-level alignment training can be applied to\nvarious video-and-language grounding tasks. Together with the task-specific\ntraining loss, our framework achieved comparable performance to previous\nstate-of-the-arts on multiple video QA and retrieval datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yubo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_F/0/1/0/all/0/1\">Feiyang Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ping_Q/0/1/0/all/0/1\">Qing Ping</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thattai_G/0/1/0/all/0/1\">Govind Thattai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unified Pretraining Framework for Document Understanding. (arXiv:2204.10939v1 [cs.CL])","link":"http://arxiv.org/abs/2204.10939","description":"<p>Document intelligence automates the extraction of information from documents\nand supports many business applications. Recent self-supervised learning\nmethods on large-scale unlabeled document datasets have opened up promising\ndirections towards reducing annotation efforts by training models with\nself-supervised objectives. However, most of the existing document pretraining\nmethods are still language-dominated. We present UDoc, a new unified\npretraining framework for document understanding. UDoc is designed to support\nmost document understanding tasks, extending the Transformer to take multimodal\nembeddings as input. Each input element is composed of words and visual\nfeatures from a semantic region of the input document image. An important\nfeature of UDoc is that it learns a generic representation by making use of\nthree self-supervised losses, encouraging the representation to model\nsentences, learn similarities, and align modalities. Extensive empirical\nanalysis demonstrates that the pretraining procedure learns better joint\nrepresentations and leads to improvements in downstream tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jiuxiang Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuen_J/0/1/0/all/0/1\">Jason Kuen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morariu_V/0/1/0/all/0/1\">Vlad I. Morariu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Handong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barmpalios_N/0/1/0/all/0/1\">Nikolaos Barmpalios</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_R/0/1/0/all/0/1\">Rajiv Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nenkova_A/0/1/0/all/0/1\">Ani Nenkova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1\">Tong Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dialogue Meaning Representation for Task-Oriented Dialogue Systems. (arXiv:2204.10989v1 [cs.CL])","link":"http://arxiv.org/abs/2204.10989","description":"<p>Dialogue meaning representation formulates natural language utterance\nsemantics in their conversational context in an explicit and machine-readable\nform. Previous work typically follows the intent-slot framework, which is easy\nfor annotation yet limited on scalability for complex linguistic expressions. A\nline of works alleviates the representation issue by introducing hierarchical\nstructures but challenging to express complex compositional semantics, such as\nnegation and coreference. We propose Dialogue Meaning Representation (DMR), a\nflexible and easily extendable representation for task-oriented dialogue. Our\nrepresentation contains a set of nodes and edges with inheritance hierarchy to\nrepresent rich semantics for compositional semantics and task-specific\nconcepts. We annotated DMR-FastFood, a multi-turn dialogue dataset with more\nthan 70k utterances, with DMR. We propose two evaluation tasks to evaluate\ndifferent machine learning based dialogue models, and further propose a novel\ncoreference resolution model GNNCoref for the graph-based coreference\nresolution task. Experiments show that DMR can be parsed well with pretrained\nSeq2Seq model, and GNNCoref outperforms the baseline models by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiangkun Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1\">Junqi Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1\">Hang Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1\">Qipeng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MuCGEC: a Multi-Reference Multi-Source Evaluation Dataset for Chinese Grammatical Error Correction. (arXiv:2204.10994v1 [cs.CL])","link":"http://arxiv.org/abs/2204.10994","description":"<p>This paper presents MuCGEC, a multi-reference multi-source evaluation dataset\nfor Chinese Grammatical Error Correction (CGEC), consisting of 7,063 sentences\ncollected from three different Chinese-as-a-Second-Language (CSL) learner\nsources. Each sentence has been corrected by three annotators, and their\ncorrections are meticulously reviewed by an expert, resulting in 2.3 references\nper sentence. We conduct experiments with two mainstream CGEC models, i.e., the\nsequence-to-sequence (Seq2Seq) model and the sequence-to-edit (Seq2Edit) model,\nboth enhanced with large pretrained language models (PLMs), achieving\ncompetitive benchmark performance on previous and our datasets. We also discuss\nCGEC evaluation methodologies, including the effect of multiple references and\nusing a char-based metric. Our annotation guidelines, data, and code are\navailable at \\url{https://github.com/HillZhang1999/MuCGEC}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenghua Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_Z/0/1/0/all/0/1\">Zuyi Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiacheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LitMind Dictionary: An Open-Source Online Dictionary. (arXiv:2204.11087v1 [cs.CL])","link":"http://arxiv.org/abs/2204.11087","description":"<p>Dictionaries can help language learners to learn vocabulary by providing\ndefinitions of words. Since traditional dictionaries present word senses as\ndiscrete items in predefined inventories, they fall short of flexibility, which\nis required in providing specific meanings of words in particular contexts. In\nthis paper, we introduce the LitMind Dictionary\n(https://dictionary.litmind.ink), an open-source online generative dictionary\nthat takes a word and context containing the word as input and automatically\ngenerates a definition as output. Incorporating state-of-the-art definition\ngeneration models, it supports not only Chinese and English, but also\nChinese-English cross-lingual queries. Moreover, it has a user-friendly\nfront-end design that can help users understand the query words quickly and\neasily. All the code and data are available at\nhttps://github.com/blcuicall/litmind-dictionary.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kong_C/0/1/0/all/0/1\">Cunliang Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_X/0/1/0/all/0/1\">Xuezhi Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Liner Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_E/0/1/0/all/0/1\">Erhong Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WikiMulti: a Corpus for Cross-Lingual Summarization. (arXiv:2204.11104v1 [cs.CL])","link":"http://arxiv.org/abs/2204.11104","description":"<p>Cross-lingual summarization (CLS) is the task to produce a summary in one\nparticular language for a source document in a different language. We introduce\nWikiMulti - a new dataset for cross-lingual summarization based on Wikipedia\narticles in 15 languages. As a set of baselines for further studies, we\nevaluate the performance of existing cross-lingual abstractive summarization\nmethods on our dataset. We make our dataset publicly available here:\nhttps://github.com/tikhonovpavel/wikimulti\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tikhonov_P/0/1/0/all/0/1\">Pavel Tikhonov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malykh_V/0/1/0/all/0/1\">Valentin Malykh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring the Role of Task Transferability in Large-Scale Multi-Task Learning. (arXiv:2204.11117v1 [cs.CL])","link":"http://arxiv.org/abs/2204.11117","description":"<p>Recent work has found that multi-task training with a large number of diverse\ntasks can uniformly improve downstream performance on unseen target tasks. In\ncontrast, literature on task transferability has established that the choice of\nintermediate tasks can heavily affect downstream task performance. In this\nwork, we aim to disentangle the effect of scale and relatedness of tasks in\nmulti-task representation learning. We find that, on average, increasing the\nscale of multi-task learning, in terms of the number of tasks, indeed results\nin better learned representations than smaller multi-task setups. However, if\nthe target tasks are known ahead of time, then training on a smaller set of\nrelated tasks is competitive to the large-scale multi-task training at a\nreduced computational cost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Padmakumar_V/0/1/0/all/0/1\">Vishakh Padmakumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lausen_L/0/1/0/all/0/1\">Leonard Lausen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ballesteros_M/0/1/0/all/0/1\">Miguel Ballesteros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_S/0/1/0/all/0/1\">Sheng Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">He He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karypis_G/0/1/0/all/0/1\">George Karypis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Embedding Knowledge for Document Summarization: A Survey. (arXiv:2204.11190v1 [cs.CL])","link":"http://arxiv.org/abs/2204.11190","description":"<p>Knowledge-aware methods have boosted a range of Natural Language Processing\napplications over the last decades. With the gathered momentum, knowledge\nrecently has been pumped into enormous attention in document summarization\nresearch. Previous works proved that knowledge-embedded document summarizers\nexcel at generating superior digests, especially in terms of informativeness,\ncoherence, and fact consistency. This paper pursues to present the first\nsystematic survey for the state-of-the-art methodologies that embed knowledge\ninto document summarizers. Particularly, we propose novel taxonomies to\nrecapitulate knowledge and knowledge embeddings under the document\nsummarization view. We further explore how embeddings are generated in learning\narchitectures of document summarization models, especially in deep learning\nmodels. At last, we discuss the challenges of this topic and future directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qu_Y/0/1/0/all/0/1\">Yutong Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Emma Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lingfei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jia Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xindong Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EPiDA: An Easy Plug-in Data Augmentation Framework for High Performance Text Classification. (arXiv:2204.11205v1 [cs.CL])","link":"http://arxiv.org/abs/2204.11205","description":"<p>Recent works have empirically shown the effectiveness of data augmentation\n(DA) in NLP tasks, especially for those suffering from data scarcity.\nIntuitively, given the size of generated data, their diversity and quality are\ncrucial to the performance of targeted tasks. However, to the best of our\nknowledge, most existing methods consider only either the diversity or the\nquality of augmented data, thus cannot fully mine the potential of DA for NLP.\nIn this paper, we present an easy and plug-in data augmentation framework EPiDA\nto support effective text classification. EPiDA employs two mechanisms:\nrelative entropy maximization (REM) and conditional entropy minimization (CEM)\nto control data generation, where REM is designed to enhance the diversity of\naugmented data while CEM is exploited to ensure their semantic consistency.\nEPiDA can support efficient and continuous data generation for effective\nclassifier training. Extensive experiments show that EPiDA outperforms existing\nSOTA methods in most cases, though not using any agent networks or pre-trained\ngeneration networks, and it works well with various DA algorithms and\nclassification models. Code is available at\nhttps://github.com/zhaominyiz/EPiDA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1\">Minyi Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_J/0/1/0/all/0/1\">Jiandong Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_J/0/1/0/all/0/1\">Jihong Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shuigeng Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Win Lottery Tickets in BERT Transfer via Task-agnostic Mask Training. (arXiv:2204.11218v1 [cs.CL])","link":"http://arxiv.org/abs/2204.11218","description":"<p>Recent studies on the lottery ticket hypothesis (LTH) show that pre-trained\nlanguage models (PLMs) like BERT contain matching subnetworks that have similar\ntransfer learning performance as the original PLM. These subnetworks are found\nusing magnitude-based pruning. In this paper, we find that the BERT subnetworks\nhave even more potential than these studies have shown. Firstly, we discover\nthat the success of magnitude pruning can be attributed to the preserved\npre-training performance, which correlates with the downstream transferability.\nInspired by this, we propose to directly optimize the subnetwork structure\ntowards the pre-training objectives, which can better preserve the pre-training\nperformance. Specifically, we train binary masks over model weights on the\npre-training tasks, with the aim of preserving the universal transferability of\nthe subnetwork, which is agnostic to any specific downstream tasks. We then\nfine-tune the subnetworks on the GLUE benchmark and the SQuAD dataset. The\nresults show that, compared with magnitude pruning, mask training can\neffectively find BERT subnetworks with improved overall performance on\ndownstream tasks. Moreover, our method is also more efficient in searching\nsubnetworks and more advantageous when fine-tuning within a certain range of\ndata scarcity. Our code is available at https://github.com/llyx97/TAMT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuanxin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_P/0/1/0/all/0/1\">Peng Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yanan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving the Naturalness of Simulated Conversations for End-to-End Neural Diarization. (arXiv:2204.11232v1 [eess.AS])","link":"http://arxiv.org/abs/2204.11232","description":"<p>This paper investigates a method for simulating natural conversation in the\nmodel training of end-to-end neural diarization (EEND). Due to the lack of any\nannotated real conversational dataset, EEND is usually pretrained on a\nlarge-scale simulated conversational dataset first and then adapted to the\ntarget real dataset. Simulated datasets play an essential role in the training\nof EEND, but as yet there has been insufficient investigation into an optimal\nsimulation method. We thus propose a method to simulate natural conversational\nspeech. In contrast to conventional methods, which simply combine the speech of\nmultiple speakers, our method takes turn-taking into account. We define four\ntypes of speaker transition and sequentially arrange them to simulate natural\nconversations. The dataset simulated using our method was found to be\nstatistically similar to the real dataset in terms of the silence and overlap\nratios. The experimental results on two-speaker diarization using the CALLHOME\nand CSJ datasets showed that the simulated dataset contributes to improving the\nperformance of EEND.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yamashita_N/0/1/0/all/0/1\">Natsuo Yamashita</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Horiguchi_S/0/1/0/all/0/1\">Shota Horiguchi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Homma_T/0/1/0/all/0/1\">Takeshi Homma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Open-domain Dialogue Generation Grounded with Dynamic Multi-form Knowledge Fusion. (arXiv:2204.11239v1 [cs.CL])","link":"http://arxiv.org/abs/2204.11239","description":"<p>Open-domain multi-turn conversations normally face the challenges of how to\nenrich and expand the content of the conversation. Recently, many approaches\nbased on external knowledge are proposed to generate rich semantic and\ninformation conversation. Two types of knowledge have been studied for\nknowledge-aware open-domain dialogue generation: structured triples from\nknowledge graphs and unstructured texts from documents. To take both advantages\nof abundant unstructured latent knowledge in the documents and the information\nexpansion capabilities of the structured knowledge graph, this paper presents a\nnew dialogue generation model, Dynamic Multi-form Knowledge Fusion based\nOpen-domain Chatt-ing Machine (DMKCM).In particular, DMKCM applies an indexed\ntext (a virtual Knowledge Base) to locate relevant documents as 1st hop and\nthen expands the content of the dialogue and its 1st hop using a commonsense\nknowledge graph to get apposite triples as 2nd hop. To merge these two forms of\nknowledge into the dialogue effectively, we design a dynamic virtual knowledge\nselector and a controller that help to enrich and expand knowledge space.\nMoreover, DMKCM adopts a novel dynamic knowledge memory module that effectively\nuses historical reasoning knowledge to generate better responses. Experimental\nresults indicate the effectiveness of our method in terms of dialogue coherence\nand informativeness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1\">Feifei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shanlin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinpeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yunpu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenkai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhisong Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improved far-field speech recognition using Joint Variational Autoencoder. (arXiv:2204.11286v1 [eess.AS])","link":"http://arxiv.org/abs/2204.11286","description":"<p>Automatic Speech Recognition (ASR) systems suffer considerably when source\nspeech is corrupted with noise or room impulse responses (RIR). Typically,\nspeech enhancement is applied in both mismatched and matched scenario training\nand testing. In matched setting, acoustic model (AM) is trained on\ndereverberated far-field features while in mismatched setting, AM is fixed. In\nrecent past, mapping speech features from far-field to close-talk using\ndenoising autoencoder (DA) has been explored. In this paper, we focus on\nmatched scenario training and show that the proposed joint VAE based mapping\nachieves a significant improvement over DA. Specifically, we observe an\nabsolute improvement of 2.5% in word error rate (WER) compared to DA based\nenhancement and 3.96% compared to AM trained directly on far-field filterbank\nfeatures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kumar_S/0/1/0/all/0/1\">Shashi Kumar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rath_S/0/1/0/all/0/1\">Shakti P. Rath</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pandey_A/0/1/0/all/0/1\">Abhishek Pandey</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Complexity and Avoidance. (arXiv:2204.11289v1 [math.LO])","link":"http://arxiv.org/abs/2204.11289","description":"<p>In this dissertation we examine the relationships between the several\nhierarchies, including the complexity, $\\mathrm{LUA}$ (Linearly Universal\nAvoidance), and shift complexity hierarchies, with an eye towards quantitative\nbounds on growth rates therein. We show that for suitable $f$ and $p$, there\nare $q$ and $g$ such that $\\mathrm{LUA}(q) \\leq_\\mathrm{s} \\mathrm{COMPLEX}(f)$\nand $\\mathrm{COMPLEX}(g) \\leq_\\mathrm{s} \\mathrm{LUA}(p)$, as well as quantify\nthe growth rates of $q$ and $g$. In the opposite direction, we show that for\ncertain sub-identical $f$ satisfying $\\lim_{n \\to \\infty}{f(n)/n}=1$ there is a\n$q$ such that $\\mathrm{COMPLEX}(f) \\leq_\\mathrm{w} \\mathrm{LUA}(q)$, and for\ncertain fast-growing $p$ there is a $g$ such that $\\mathrm{LUA}(p)\n\\leq_\\mathrm{s} \\mathrm{COMPLEX}(g)$, as well as quantify the growth rates of\n$q$ and $g$.\n</p>\n<p>Concerning shift complexity, explicit bounds are given on how slow-growing\n$q$ must be for any member of $\\rm{LUA}(q)$ to compute $\\delta$-shift complex\nsequences. Motivated by the complexity hierarchy, we generalize the notion of\nshift complexity to consider sequences $X$ satisfying $\\operatorname{KP}(\\tau)\n\\geq f(|\\tau|) - O(1)$ for all substrings $\\tau$ of $X$ where $f$ is any order\nfunction. We show that for sufficiently slow-growing $f$, $f$-shift complex\nsequences can be uniformly computed by $g$-complex sequences, where $g$ grows\nslightly faster than $f$.\n</p>\n<p>The structure of the $\\mathrm{LUA}$ hierarchy is examined using bushy tree\nforcing, with the main result being that for any order function $p$, there is a\nslow-growing order function $q$ such that $\\mathrm{LUA}(p)$ and\n$\\mathrm{LUA}(q)$ are weakly incomparable. Using this, we prove new results\nabout the filter of the weak degrees of deep nonempty $\\Pi^0_1$ classes and the\nconnection between the shift complexity and $\\mathrm{LUA}$ hierarchies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/math/1/au:+Jananthan_H/0/1/0/all/0/1\">Hayden Jananthan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emotion-Aware Transformer Encoder for Empathetic Dialogue Generation. (arXiv:2204.11320v1 [cs.SD])","link":"http://arxiv.org/abs/2204.11320","description":"<p>Modern day conversational agents are trained to emulate the manner in which\nhumans communicate. To emotionally bond with the user, these virtual agents\nneed to be aware of the affective state of the user. Transformers are the\nrecent state of the art in sequence-to-sequence learning that involves training\nan encoder-decoder model with word embeddings from utterance-response pairs. We\npropose an emotion-aware transformer encoder for capturing the emotional\nquotient in the user utterance in order to generate human-like empathetic\nresponses. The contributions of our paper are as follows: 1) An emotion\ndetector module trained on the input utterances determines the affective state\nof the user in the initial phase 2) A novel transformer encoder is proposed\nthat adds and normalizes the word embedding with emotion embedding thereby\nintegrating the semantic and affective aspects of the input utterance 3) The\nencoder and decoder stacks belong to the Transformer-XL architecture which is\nthe recent state of the art in language modeling. Experimentation on the\nbenchmark Facebook AI empathetic dialogue dataset confirms the efficacy of our\nmodel from the higher BLEU-4 scores achieved for the generated responses as\ncompared to existing methods. Emotionally intelligent virtual agents are now a\nreality and inclusion of affect as a modality in all human-machine interfaces\nis foreseen in the immediate future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Goel_R/0/1/0/all/0/1\">Raman Goel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Susan_S/0/1/0/all/0/1\">Seba Susan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vashisht_S/0/1/0/all/0/1\">Sachin Vashisht</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhanda_A/0/1/0/all/0/1\">Armaan Dhanda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Item Response Theory Framework for Persuasion. (arXiv:2204.11337v1 [cs.CL])","link":"http://arxiv.org/abs/2204.11337","description":"<p>In this paper, we apply Item Response Theory, popular in education and\npolitical science research, to the analysis of argument persuasiveness in\nlanguage. We empirically evaluate the model's performance on three datasets,\nincluding a novel dataset in the area of political advocacy. We show the\nadvantages of separating these components under several style and content\nrepresentations, including evaluating the ability of the speaker embeddings\ngenerated by the model to parallel real-world observations about\npersuadability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kornilova_A/0/1/0/all/0/1\">Anastassia Kornilova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Argyle_D/0/1/0/all/0/1\">Daniel Argyle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eidelman_V/0/1/0/all/0/1\">Vladimir Eidelman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hate Me Not: Detecting Hate Inducing Memes in Code Switched Languages. (arXiv:2204.11356v1 [cs.LG])","link":"http://arxiv.org/abs/2204.11356","description":"<p>The rise in the number of social media users has led to an increase in the\nhateful content posted online. In countries like India, where multiple\nlanguages are spoken, these abhorrent posts are from an unusual blend of\ncode-switched languages. This hate speech is depicted with the help of images\nto form \"Memes\" which create a long-lasting impact on the human mind. In this\npaper, we take up the task of hate and offense detection from multimodal data,\ni.e. images (Memes) that contain text in code-switched languages. We firstly\npresent a novel triply annotated Indian political Memes (IPM) dataset, which\ncomprises memes from various Indian political events that have taken place\npost-independence and are classified into three distinct categories. We also\npropose a binary-channelled CNN cum LSTM based model to process the images\nusing the CNN model and text using the LSTM model to get state-of-the-art\nresults for this task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rajput_K/0/1/0/all/0/1\">Kshitij Rajput</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kapoor_R/0/1/0/all/0/1\">Raghav Kapoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rai_K/0/1/0/all/0/1\">Kaushal Rai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaur_P/0/1/0/all/0/1\">Preeti Kaur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Entity-Conditioned Question Generation for Robust Attention Distribution in Neural Information Retrieval. (arXiv:2204.11373v1 [cs.CL])","link":"http://arxiv.org/abs/2204.11373","description":"<p>We show that supervised neural information retrieval (IR) models are prone to\nlearning sparse attention patterns over passage tokens, which can result in key\nphrases including named entities receiving low attention weights, eventually\nleading to model under-performance. Using a novel targeted synthetic data\ngeneration method that identifies poorly attended entities and conditions the\ngeneration episodes on those, we teach neural IR to attend more uniformly and\nrobustly to all entities in a given passage. On two public IR benchmarks, we\nempirically show that the proposed method helps improve both the model's\nattention patterns and retrieval performance, including in zero-shot settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Reddy_R/0/1/0/all/0/1\">Revanth Gangi Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sultan_M/0/1/0/all/0/1\">Md Arafat Sultan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Franz_M/0/1/0/all/0/1\">Martin Franz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sil_A/0/1/0/all/0/1\">Avirup Sil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Financial data analysis application via multi-strategy text processing. (arXiv:2204.11394v1 [cs.CL])","link":"http://arxiv.org/abs/2204.11394","description":"<p>Maintaining financial system stability is critical to economic development,\nand early identification of risks and opportunities is essential. The financial\nindustry contains a wide variety of data, such as financial statements,\ncustomer information, stock trading data, news, etc. Massive heterogeneous data\ncalls for intelligent algorithms for machines to process and understand. This\npaper mainly focuses on the stock trading data and news about China A-share\ncompanies. We present a financial data analysis application, Financial Quotient\nPorter, designed to combine textual and numerical data by using a\nmulti-strategy data mining approach. Additionally, we present our efforts and\nplans in deep learning financial text processing application scenarios using\nnatural language processing (NLP) and knowledge graph (KG) technologies. Based\non KG technology, risks and opportunities can be identified from heterogeneous\ndata. NLP technology can be used to extract entities, relations, and events\nfrom unstructured text, and analyze market sentiment. Experimental results show\nmarket sentiments towards a company and an industry, as well as news-level\nassociations between companies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hongyin Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Augmentation for Named Entity Recognition with Meta Reweighting. (arXiv:2204.11406v1 [cs.CL])","link":"http://arxiv.org/abs/2204.11406","description":"<p>Self-augmentation has been received increasing research interest recently to\nimprove named entity recognition (NER) performance in low-resource scenarios.\nToken substitution and mixup are two feasible heterogeneous self-augmentation\ntechniques for NER that can achieve effective performance with certain\nspecialized efforts. Noticeably, self-augmentation may introduce potentially\nnoisy augmented data. Prior research has mainly resorted to heuristic rule\nbased constraints to reduce the noise for specific self-augmentation\nindividually. In this paper, we revisit the two self-augmentation methods for\nNER, and propose a unified meta-reweighting strategy for these heterogeneous\nmethods to achieve a natural integration. Our method is easily extensible,\nimposing little effort on a specific self-augmentation method. Experiments on\ndifferent Chinese and English NER benchmarks demonstrate that our token\nsubstitution and mixup method, as well as their integration, can obtain\neffective performance improvement. Based on the meta-reweighting mechanism, we\ncan enhance the advantages of the self-augmentation techniques without extra\nefforts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Linzhi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1\">Pengjun Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Meishan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Chunping Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guangwei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Headline Diagnosis: Manipulation of Content Farm Headlines. (arXiv:2204.11408v1 [cs.CL])","link":"http://arxiv.org/abs/2204.11408","description":"<p>As technology grows faster, the news spreads through social media. In order\nto attract more readers and acquire additional profit, some news agencies\nreproduce massive news in a more appealing manner. Therefore, it is essential\nto accurately predict whether a news article is from official news agencies.\nThis work develops a headline classification based on Convoluted Neural Network\nto determine credibility of a news article. The model primarily focuses on\ninvestigating key factors from headlines. These factors include word\nsegmentation, part-of-speech tags, and sentiment features. With integrating\nthese features into the proposed classification model, the demonstrated\nevaluation achieves 93.99% for accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yu-Chieh Chen</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1\">Pei-Yu Huang</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chun Lin</a> (3), <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yi-Ting Huang</a> (3), <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Meng Chang Chen</a> (3) ((1) Hal&#x131;c&#x131;o&#x11f;lu Data Science Institute, University of California San Diego, La Jolla, United States, (2) Management and Digital Innovation, University of London, Singapore, (3) Institute of Information Science, Academia Sinica, Taipei, Taiwan)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"It Takes Two Flints to Make a Fire: Multitask Learning of Neural Relation and Explanation Classifiers. (arXiv:2204.11424v1 [cs.CL])","link":"http://arxiv.org/abs/2204.11424","description":"<p>We propose an explainable approach for relation extraction that mitigates the\ntension between generalization and explainability by jointly training for the\ntwo goals. Our approach uses a multi-task learning architecture, which jointly\ntrains a classifier for relation extraction, and a sequence model that labels\nwords in the context of the relation that explain the decisions of the relation\nclassifier. We also convert the model outputs to rules to bring global\nexplanations to this approach. This sequence model is trained using a hybrid\nstrategy: supervised, when supervision from pre-existing patterns is available,\nand semi-supervised otherwise. In the latter situation, we treat the sequence\nmodel's labels as latent variables, and learn the best assignment that\nmaximizes the performance of the relation classifier. We evaluate the proposed\napproach on the two datasets and show that the sequence model provides labels\nthat serve as accurate explanations for the relation classifier's decisions,\nand, importantly, that the joint training generally improves the performance of\nthe relation classifier. We also evaluate the performance of the generated\nrules and show that the new rules are great add-on to the manual rules and\nbring the rule-based system much closer to the neural models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1\">Zheng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Surdeanu_M/0/1/0/all/0/1\">Mihai Surdeanu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Star-Transformer. (arXiv:1902.09113v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/1902.09113","description":"<p>Although Transformer has achieved great successes on many NLP tasks, its\nheavy structure with fully-connected attention connections leads to\ndependencies on large training data. In this paper, we present\nStar-Transformer, a lightweight alternative by careful sparsification. To\nreduce model complexity, we replace the fully-connected structure with a\nstar-shaped topology, in which every two non-adjacent nodes are connected\nthrough a shared relay node. Thus, complexity is reduced from quadratic to\nlinear, while preserving capacity to capture both local composition and\nlong-range dependency. The experiments on four tasks (22 datasets) show that\nStar-Transformer achieved significant improvements against the standard\nTransformer for the modestly sized datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1\">Qipeng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Pengfei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1\">Yunfan Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_X/0/1/0/all/0/1\">Xiangyang Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Drink Bleach or Do What Now? Covid-HeRA: A Study of Risk-Informed Health Decision Making in the Presence of COVID-19 Misinformation. (arXiv:2010.08743v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.08743","description":"<p>Given the widespread dissemination of inaccurate medical advice related to\nthe 2019 coronavirus pandemic (COVID-19), such as fake remedies, treatments and\nprevention suggestions, misinformation detection has emerged as an open problem\nof high importance and interest for the research community. Several works study\nhealth misinformation detection, yet little attention has been given to the\nperceived severity of misinformation posts. In this work, we frame health\nmisinformation as a risk assessment task. More specifically, we study the\nseverity of each misinformation story and how readers perceive this severity,\ni.e., how harmful a message believed by the audience can be and what type of\nsignals can be used to recognize potentially malicious fake news and detect\nrefuted claims. To address our research questions, we introduce a new benchmark\ndataset, accompanied by detailed data analysis. We evaluate several traditional\nand state-of-the-art models and show there is a significant gap in performance\nwhen applying traditional misinformation classification models to this task. We\nconclude with open challenges and future directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dharawat_A/0/1/0/all/0/1\">Arkin Dharawat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lourentzou_I/0/1/0/all/0/1\">Ismini Lourentzou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morales_A/0/1/0/all/0/1\">Alex Morales</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_C/0/1/0/all/0/1\">ChengXiang Zhai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Potential Idiomatic Expression (PIE)-English: Corpus for Classes of Idioms. (arXiv:2105.03280v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.03280","description":"<p>We present a fairly large, Potential Idiomatic Expression (PIE) dataset for\nNatural Language Processing (NLP) in English. The challenges with NLP systems\nwith regards to tasks such as Machine Translation (MT), word sense\ndisambiguation (WSD) and information retrieval make it imperative to have a\nlabelled idioms dataset with classes such as it is in this work. To the best of\nthe authors' knowledge, this is the first idioms corpus with classes of idioms\nbeyond the literal and the general idioms classification. In particular, the\nfollowing classes are labelled in the dataset: metaphor, simile, euphemism,\nparallelism, personification, oxymoron, paradox, hyperbole, irony and literal.\nWe obtain an overall inter-annotator agreement (IAA) score, between two\nindependent annotators, of 88.89%. Many past efforts have been limited in the\ncorpus size and classes of samples but this dataset contains over 20,100\nsamples with almost 1,200 cases of idioms (with their meanings) from 10 classes\n(or senses). The corpus may also be extended by researchers to meet specific\nneeds. The corpus has part of speech (PoS) tagging from the NLTK library.\nClassification experiments performed on the corpus to obtain a baseline and\ncomparison among three common models, including the BERT model, give good\nresults. We also make publicly available the corpus and the relevant codes for\nworking with it for NLP tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Adewumi_T/0/1/0/all/0/1\">Tosin P. Adewumi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vadoodi_R/0/1/0/all/0/1\">Roshanak Vadoodi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tripathy_A/0/1/0/all/0/1\">Aparajita Tripathy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikolaidou_K/0/1/0/all/0/1\">Konstantina Nikolaidou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liwicki_F/0/1/0/all/0/1\">Foteini Liwicki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liwicki_M/0/1/0/all/0/1\">Marcus Liwicki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Retrieval-Free Knowledge-Grounded Dialogue Response Generation with Adapters. (arXiv:2105.06232v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.06232","description":"<p>To diversify and enrich generated dialogue responses, knowledge-grounded\ndialogue has been investigated in recent years. The existing methods tackle the\nknowledge grounding challenge by retrieving the relevant sentences over a large\ncorpus and augmenting the dialogues with explicit extra information. Despite\ntheir success, however, the existing works have drawbacks in inference\nefficiency. This paper proposes KnowExpert, a framework to bypass the explicit\nretrieval process and inject knowledge into the pre-trained language models\nwith lightweight adapters and adapt to the knowledge-grounded dialogue task. To\nthe best of our knowledge, this is the first attempt to tackle this challenge\nwithout retrieval in this task under an open-domain chit-chat scenario. The\nexperimental results show that Knowexpert performs comparably with some\nretrieval-based baselines while being time-efficient in inference,\ndemonstrating the effectiveness of our proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ishii_E/0/1/0/all/0/1\">Etsuko Ishii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cahyawijaya_S/0/1/0/all/0/1\">Samuel Cahyawijaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zihan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winata_G/0/1/0/all/0/1\">Genta Indra Winata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madotto_A/0/1/0/all/0/1\">Andrea Madotto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_D/0/1/0/all/0/1\">Dan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1\">Pascale Fung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distantly Supervised Relation Extraction via Recursive Hierarchy-Interactive Attention and Entity-Order Perception. (arXiv:2105.08213v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.08213","description":"<p>Wrong-labeling problem and long-tail relations severely affect the\nperformance of distantly supervised relation extraction task. Many studies\nmitigate the effect of wrong-labeling through selective attention mechanism and\nhandle long-tail relations by introducing relation hierarchies to share\nknowledge. However, almost all existing studies ignore the fact that, in a\nsentence, the appearance order of two entities contributes to the understanding\nof its semantics. Furthermore, they only utilize each relation level of\nrelation hierarchies separately, but do not exploit the heuristic effect\nbetween relation levels, i.e., higher-level relations can give useful\ninformation to the lower ones. Based on the above, in this paper, we design a\nnovel Recursive Hierarchy-Interactive Attention network (RHIA) to further\nhandle long-tail relations, which models the heuristic effect between relation\nlevels. From the top down, it passes relation-related information layer by\nlayer, which is the most significant difference from existing models, and\ngenerates relation-augmented sentence representations for each relation level\nin a recursive structure. Besides, we introduce a newfangled training\nobjective, called Entity-Order Perception (EOP), to make the sentence encoder\nretain more entity appearance information. Substantial experiments on the\npopular (NYT) dataset are conducted. Compared to prior baselines, our RHIA-EOP\nachieves state-of-the-art performance in terms of precision-recall (P-R)\ncurves, AUC, Top-N precision and other evaluation metrics. Insightful analysis\nalso demonstrates the necessity and effectiveness of each component of\nRHIA-EOP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_R/0/1/0/all/0/1\">Ridong Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_T/0/1/0/all/0/1\">Tao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiayu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_H/0/1/0/all/0/1\">Hai Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lu Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Time-Aware Language Models as Temporal Knowledge Bases. (arXiv:2106.15110v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.15110","description":"<p>Many facts come with an expiration date, from the name of the President to\nthe basketball team Lebron James plays for. But language models (LMs) are\ntrained on snapshots of data collected at a specific moment in time, and this\ncan limit their utility, especially in the closed-book setting where the\npretraining corpus must contain the facts the model should memorize. We\nintroduce a diagnostic dataset aimed at probing LMs for factual knowledge that\nchanges over time and highlight problems with LMs at either end of the spectrum\n-- those trained on specific slices of temporal data, as well as those trained\non a wide range of temporal data. To mitigate these problems, we propose a\nsimple technique for jointly modeling text with its timestamp. This improves\nmemorization of seen facts from the training time period, as well as\ncalibration on predictions about unseen facts from future time periods. We also\nshow that models trained with temporal context can be efficiently \"refreshed\"\nas new data arrives, without the need for retraining from scratch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dhingra_B/0/1/0/all/0/1\">Bhuwan Dhingra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cole_J/0/1/0/all/0/1\">Jeremy R. Cole</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eisenschlos_J/0/1/0/all/0/1\">Julian Martin Eisenschlos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gillick_D/0/1/0/all/0/1\">Daniel Gillick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eisenstein_J/0/1/0/all/0/1\">Jacob Eisenstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_W/0/1/0/all/0/1\">William W. Cohen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation. (arXiv:2108.12409v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.12409","description":"<p>Since the introduction of the transformer model by Vaswani et al. (2017), a\nfundamental question has yet to be answered: how does a model achieve\nextrapolation at inference time for sequences that are longer than it saw\nduring training? We first show that extrapolation can be enabled by simply\nchanging the position representation method, though we find that current\nmethods do not allow for efficient extrapolation. We therefore introduce a\nsimpler and more efficient position method, Attention with Linear Biases\n(ALiBi). ALiBi does not add positional embeddings to word embeddings; instead,\nit biases query-key attention scores with a penalty that is proportional to\ntheir distance. We show that this method trains a 1.3 billion parameter model\non input sequences of length 1024 that extrapolates to input sequences of\nlength 2048, achieving the same perplexity as a sinusoidal position embedding\nmodel trained on inputs of length 2048 but training 11% faster and using 11%\nless memory. ALiBi's inductive bias towards recency also leads it to outperform\nmultiple strong position methods on the WikiText-103 benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Press_O/0/1/0/all/0/1\">Ofir Press</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1\">Mike Lewis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FedKD: Communication Efficient Federated Learning via Knowledge Distillation. (arXiv:2108.13323v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2108.13323","description":"<p>Federated learning is widely used to learn intelligent models from\ndecentralized data. In federated learning, clients need to communicate their\nlocal model updates in each iteration of model learning. However, model updates\nare large in size if the model contains numerous parameters, and there usually\nneeds many rounds of communication until model converges. Thus, the\ncommunication cost in federated learning can be quite heavy. In this paper, we\npropose a communication efficient federated learning method based on knowledge\ndistillation. Instead of directly communicating the large models between\nclients and server, we propose an adaptive mutual distillation framework to\nreciprocally learn a student and a teacher model on each client, where only the\nstudent model is shared by different clients and updated collaboratively to\nreduce the communication cost. Both the teacher and student on each client are\nlearned on its local data and the knowledge distilled from each other, where\ntheir distillation intensities are controlled by their prediction quality. To\nfurther reduce the communication cost, we propose a dynamic gradient\napproximation method based on singular value decomposition to approximate the\nexchanged gradients with dynamic precision. Extensive experiments on benchmark\ndatasets in different tasks show that our approach can effectively reduce the\ncommunication cost and achieve competitive results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chuhan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fangzhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_L/0/1/0/all/0/1\">Lingjuan Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yongfeng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xing Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Task-Oriented Dialogue System as Natural Language Generation. (arXiv:2108.13679v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.13679","description":"<p>In this paper, we propose to formulate the task-oriented dialogue system as\nthe purely natural language generation task, so as to fully leverage the\nlarge-scale pre-trained models like GPT-2 and simplify complicated\ndelexicalization prepossessing. However, directly applying this method heavily\nsuffers from the dialogue entity inconsistency caused by the removal of\ndelexicalized tokens, as well as the catastrophic forgetting problem of the\npre-trained model during fine-tuning, leading to unsatisfactory performance. To\nalleviate these problems, we design a novel GPT-Adapter-CopyNet network, which\nincorporates the lightweight adapter and CopyNet modules into GPT-2 to achieve\nbetter performance on transfer learning and dialogue entity generation.\nExperimental results conducted on the DSTC8 Track 1 benchmark and MultiWOZ\ndataset demonstrate that our proposed approach significantly outperforms\nbaseline models with a remarkable performance on automatic and human\nevaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weizhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhirui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Junliang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1\">Yinpei Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Boxing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1\">Weihua Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Hierarchical Structures with Differentiable Nondeterministic Stacks. (arXiv:2109.01982v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.01982","description":"<p>Learning hierarchical structures in sequential data -- from simple\nalgorithmic patterns to natural language -- in a reliable, generalizable way\nremains a challenging problem for neural language models. Past work has shown\nthat recurrent neural networks (RNNs) struggle to generalize on held-out\nalgorithmic or syntactic patterns without supervision or some inductive bias.\nTo remedy this, many papers have explored augmenting RNNs with various\ndifferentiable stacks, by analogy with finite automata and pushdown automata\n(PDAs). In this paper, we improve the performance of our recently proposed\nNondeterministic Stack RNN (NS-RNN), which uses a differentiable data structure\nthat simulates a nondeterministic PDA, with two important changes. First, the\nmodel now assigns unnormalized positive weights instead of probabilities to\nstack actions, and we provide an analysis of why this improves training.\nSecond, the model can directly observe the state of the underlying PDA. Our\nmodel achieves lower cross-entropy than all previous stack RNNs on five\ncontext-free language modeling tasks (within 0.05 nats of the\ninformation-theoretic lower bound), including a task on which the NS-RNN\npreviously failed to outperform a deterministic stack RNN baseline. Finally, we\npropose a restricted version of the NS-RNN that incrementally processes\ninfinitely long sequences, and we present language modeling results on the Penn\nTreebank.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+DuSell_B/0/1/0/all/0/1\">Brian DuSell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiang_D/0/1/0/all/0/1\">David Chiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"XLM-K: Improving Cross-Lingual Language Model Pre-training with Multilingual Knowledge. (arXiv:2109.12573v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.12573","description":"<p>Cross-lingual pre-training has achieved great successes using monolingual and\nbilingual plain text corpora. However, most pre-trained models neglect\nmultilingual knowledge, which is language agnostic but comprises abundant\ncross-lingual structure alignment. In this paper, we propose XLM-K, a\ncross-lingual language model incorporating multilingual knowledge in\npre-training. XLM-K augments existing multilingual pre-training with two\nknowledge tasks, namely Masked Entity Prediction Task and Object Entailment\nTask. We evaluate XLM-K on MLQA, NER and XNLI. Experimental results clearly\ndemonstrate significant improvements over existing multilingual language\nmodels. The results on MLQA and NER exhibit the superiority of XLM-K in\nknowledge related tasks. The success in XNLI shows a better cross-lingual\ntransferability obtained in XLM-K. What is more, we provide a detailed probing\nanalysis to confirm the desired knowledge captured in our pre-training regimen.\nThe code is available at\nhttps://github.com/microsoft/Unicoder/tree/master/pretraining/xlmk.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xiaoze Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yaobo Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Enhanced Span-based Decomposition Method for Few-Shot Sequence Labeling. (arXiv:2109.13023v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.13023","description":"<p>Few-Shot Sequence Labeling (FSSL) is a canonical paradigm for the tagging\nmodels, e.g., named entity recognition and slot filling, to generalize on an\nemerging, resource-scarce domain. Recently, the metric-based meta-learning\nframework has been recognized as a promising approach for FSSL. However, most\nprior works assign a label to each token based on the token-level similarities,\nwhich ignores the integrality of named entities or slots. To this end, in this\npaper, we propose ESD, an Enhanced Span-based Decomposition method for FSSL.\nESD formulates FSSL as a span-level matching problem between test query and\nsupporting instances. Specifically, ESD decomposes the span matching problem\ninto a series of span-level procedures, mainly including enhanced span\nrepresentation, class prototype aggregation and span conflicts resolution.\nExtensive experiments show that ESD achieves the new state-of-the-art results\non two popular FSSL benchmarks, FewNERD and SNIPS, and is proven to be more\nrobust in the nested and noisy tagging scenarios. Our code is available at\nhttps://github.com/Wangpeiyi9979/ESD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peiyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Runxin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qingyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yunbo Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_B/0/1/0/all/0/1\">Baobao Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sui_Z/0/1/0/all/0/1\">Zhifang Sui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Dense Retrieval for Dialogue Response Selection. (arXiv:2110.06612v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.06612","description":"<p>Recent progress in deep learning has continuously improved the accuracy of\ndialogue response selection. In particular, sophisticated neural network\narchitectures are leveraged to capture the rich interactions between dialogue\ncontext and response candidates. While remarkably effective, these models also\nbring in a steep increase in computational cost. Consequently, such models can\nonly be used as a re-rank module in practice. In this study, we present a\nsolution to directly select proper responses from a large corpus or even a\nnonparallel corpus that only consists of unpaired sentences, using a dense\nretrieval model. To push the limits of dense retrieval, we design an\ninteraction layer upon the dense retrieval models and apply a set of\ntailor-designed learning strategies. Our model shows superiority over strong\nbaselines on the conventional re-rank evaluation setting, which is remarkable\ngiven its efficiency. To verify the effectiveness of our approach in realistic\nscenarios, we also conduct full-rank evaluation, where the target is to select\nproper responses from a full candidate pool that may contain millions of\ncandidates and evaluate them fairly through human annotations. Our proposed\nmodel notably outperforms pipeline baselines that integrate fast recall and\nexpressive re-rank modules. Human evaluation results show that enlarging the\ncandidate pool with nonparallel corpora improves response quality further.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lan_T/0/1/0/all/0/1\">Tian Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1\">Deng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yixuan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Heyan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_X/0/1/0/all/0/1\">Xian-Ling Mao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unifying Cross-lingual Summarization and Machine Translation with Compression Rate. (arXiv:2110.07936v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.07936","description":"<p>Cross-Lingual Summarization (CLS) is a task that extracts important\ninformation from a source document and summarizes it into a summary in another\nlanguage. It is a challenging task that requires a system to understand,\nsummarize, and translate at the same time, making it highly related to\nMonolingual Summarization (MS) and Machine Translation (MT). In practice, the\ntraining resources for Machine Translation are far more than that for\ncross-lingual and monolingual summarization. Thus incorporating the Machine\nTranslation corpus into CLS would be beneficial for its performance. However,\nthe present work only leverages a simple multi-task framework to bring Machine\nTranslation in, lacking deeper exploration. In this paper, we propose a novel\ntask, Cross-lingual Summarization with Compression rate (CSC), to benefit\nCross-Lingual Summarization by large-scale Machine Translation corpus. Through\nintroducing compression rate, the information ratio between the source and the\ntarget text, we regard the MT task as a special CLS task with a compression\nrate of 100%. Hence they can be trained as a unified task, sharing knowledge\nmore effectively. However, a huge gap exists between the MT task and the CLS\ntask, where samples with compression rates between 30% and 90% are extremely\nrare. Hence, to bridge these two tasks smoothly, we propose an effective data\naugmentation method to produce document-summary pairs with different\ncompression rates. The proposed method not only improves the performance of the\nCLS task, but also provides controllability to generate summaries in desired\nlengths. Experiments demonstrate that our method outperforms various strong\nbaselines in three cross-lingual summarization datasets. We released our code\nand data at https://github.com/ybai-nlp/CLS_CR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yu Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Heyan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_K/0/1/0/all/0/1\">Kai Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yiming Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_J/0/1/0/all/0/1\">Jiaao Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chi_Z/0/1/0/all/0/1\">Zewen Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Boxing Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"milIE: Modular & Iterative Multilingual Open Information Extraction. (arXiv:2110.08144v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.08144","description":"<p>Open Information Extraction (OpenIE) is the task of extracting (subject,\npredicate, object) triples from natural language sentences. Current OpenIE\nsystems extract all triple slots independently. In contrast, we explore the\nhypothesis that it may be beneficial to extract triple slots iteratively: first\nextract easy slots, followed by the difficult ones by conditioning on the easy\nslots, and therefore achieve a better overall extraction. Based on this\nhypothesis, we propose a neural OpenIE system, milIE, that operates in an\niterative fashion. Due to the iterative nature, the system is also modular --\nit is possible to seamlessly integrate rule based extraction systems with a\nneural end-to-end system, thereby allowing rule based systems to supply\nextraction slots which milIE can leverage for extracting the remaining slots.\nWe confirm our hypothesis empirically: milIE outperforms SOTA systems on\nmultiple languages ranging from Chinese to Arabic. Additionally, we are the\nfirst to provide an OpenIE test dataset for Arabic and Galician.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kotnis_B/0/1/0/all/0/1\">Bhushan Kotnis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gashteovski_K/0/1/0/all/0/1\">Kiril Gashteovski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rubio_D/0/1/0/all/0/1\">Daniel O&#xf1;oro Rubio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_Tembras_V/0/1/0/all/0/1\">Vanesa Rodriguez-Tembras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaker_A/0/1/0/all/0/1\">Ammar Shaker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takamoto_M/0/1/0/all/0/1\">Makoto Takamoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niepert_M/0/1/0/all/0/1\">Mathias Niepert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lawrence_C/0/1/0/all/0/1\">Carolin Lawrence</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DYLE: Dynamic Latent Extraction for Abstractive Long-Input Summarization. (arXiv:2110.08168v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.08168","description":"<p>Transformer-based models have achieved state-of-the-art performance on\nshort-input summarization. However, they still struggle with summarizing longer\ntext. In this paper, we present DYLE, a novel dynamic latent extraction\napproach for abstractive long-input summarization. DYLE jointly trains an\nextractor and a generator and treats the extracted text snippets as the latent\nvariable, allowing dynamic snippet-level attention weights during decoding. To\nprovide adequate supervision, we propose simple yet effective heuristics for\noracle extraction as well as a consistency loss term, which encourages the\nextractor to approximate the averaged dynamic weights predicted by the\ngenerator. We evaluate our method on different long-document and long-dialogue\nsummarization tasks: GovReport, QMSum, and arXiv. Experiment results show that\nDYLE outperforms all existing methods on GovReport and QMSum, with gains up to\n6.1 ROUGE, while yielding strong results on arXiv. Further analysis shows that\nthe proposed dynamic weights provide interpretability of our generation\nprocess.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mao_Z/0/1/0/all/0/1\">Ziming Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chen Henry Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_A/0/1/0/all/0/1\">Ansong Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yusen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deb_B/0/1/0/all/0/1\">Budhaditya Deb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1\">Ahmed H. Awadallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1\">Dragomir Radev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distributionally Robust Recurrent Decoders with Random Network Distillation. (arXiv:2110.13229v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.13229","description":"<p>Neural machine learning models can successfully model language that is\nsimilar to their training distribution, but they are highly susceptible to\ndegradation under distribution shift, which occurs in many practical\napplications when processing out-of-domain (OOD) text. This has been attributed\nto \"shortcut learning\": relying on weak correlations over arbitrary large\ncontexts.\n</p>\n<p>We propose a method based on OOD detection with Random Network Distillation\nto allow an autoregressive language model to automatically disregard OOD\ncontext during inference, smoothly transitioning towards a less expressive but\nmore robust model as the data becomes more OOD while retaining its full context\ncapability when operating in-distribution. We apply our method to a GRU\narchitecture, demonstrating improvements on multiple language modeling (LM)\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Miceli_Barone_A/0/1/0/all/0/1\">Antonio Valerio Miceli-Barone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Birch_A/0/1/0/all/0/1\">Alexandra Birch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sennrich_R/0/1/0/all/0/1\">Rico Sennrich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comparative Explanations of Recommendations. (arXiv:2111.00670v3 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2111.00670","description":"<p>As recommendation is essentially a comparative (or ranking) process, a good\nexplanation should illustrate to users why an item is believed to be better\nthan another, i.e., comparative explanations about the recommended items.\nIdeally, after reading the explanations, a user should reach the same ranking\nof items as the system's. Unfortunately, little research attention has yet been\npaid on such comparative explanations.\n</p>\n<p>In this work, we develop an extract-and-refine architecture to explain the\nrelative comparisons among a set of ranked items from a recommender system. For\neach recommended item, we first extract one sentence from its associated\nreviews that best suits the desired comparison against a set of reference\nitems. Then this extracted sentence is further articulated with respect to the\ntarget user through a generative model to better explain why the item is\nrecommended. We design a new explanation quality metric based on BLEU to guide\nthe end-to-end training of the extraction and refinement components, which\navoids generation of generic content. Extensive offline evaluations on two\nlarge recommendation benchmark datasets and serious user studies against an\narray of state-of-the-art explainable recommendation algorithms demonstrate the\nnecessity of comparative explanations and the effectiveness of our solution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1\">Aobo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1\">Nan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_R/0/1/0/all/0/1\">Renqin Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_H/0/1/0/all/0/1\">Hongbo Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongning Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Enactivist account of Mind Reading in Natural Language Understanding. (arXiv:2111.06179v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.06179","description":"<p>In this paper we apply our understanding of the radical enactivist agenda to\nthe classic AI-hard problem of Natural Language Understanding. When Turing\ndevised his famous test the assumption was that a computer could use language\nand the challenge would be to mimic human intelligence. It turned out playing\nchess and formal logic were easy compared to understanding what people say. The\ntechniques of good old-fashioned AI (GOFAI) assume symbolic representation is\nthe core of reasoning and by that paradigm human communication consists of\ntransferring representations from one mind to another. However, one finds that\nrepresentations appear in another's mind, without appearing in the intermediary\nlanguage. People communicate by mind reading it seems. Systems with speech\ninterfaces such as Alexa and Siri are of course common, but they are limited.\nRather than adding mind reading skills, we introduced a \"cheat\" that enabled\nour systems to fake it. The cheat is simple and only slightly interesting to\ncomputer scientists and not at all interesting to philosophers. However,\nreading about the enactivist idea that we \"directly perceive\" the intentions of\nothers, our cheat took on a new light and in this paper look again at how\nnatural language understanding might actually work between humans.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wallis_P/0/1/0/all/0/1\">Peter Wallis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting Contextual Toxicity Detection in Conversations. (arXiv:2111.12447v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.12447","description":"<p>Understanding toxicity in user conversations is undoubtedly an important\nproblem. Addressing \"covert\" or implicit cases of toxicity is particularly hard\nand requires context. Very few previous studies have analysed the influence of\nconversational context in human perception or in automated detection models. We\ndive deeper into both these directions. We start by analysing existing\ncontextual datasets and come to the conclusion that toxicity labelling by\nhumans is in general influenced by the conversational structure, polarity and\ntopic of the context. We then propose to bring these findings into\ncomputational detection models by introducing and evaluating (a) neural\narchitectures for contextual toxicity detection that are aware of the\nconversational structure, and (b) data augmentation strategies that can help\nmodel contextual toxicity detection. Our results have shown the encouraging\npotential of neural architectures that are aware of the conversation structure.\nWe have also demonstrated that such models can benefit from synthetic data,\nespecially in the social media domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Anuchitanukul_A/0/1/0/all/0/1\">Atijit Anuchitanukul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ive_J/0/1/0/all/0/1\">Julia Ive</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Specia_L/0/1/0/all/0/1\">Lucia Specia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly Supervised Text-to-SQL Parsing through Question Decomposition. (arXiv:2112.06311v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.06311","description":"<p>Text-to-SQL parsers are crucial in enabling non-experts to effortlessly query\nrelational data. Training such parsers, by contrast, generally requires\nexpertise in annotating natural language (NL) utterances with corresponding SQL\nqueries. In this work, we propose a weak supervision approach for training\ntext-to-SQL parsers. We take advantage of the recently proposed question\nmeaning representation called QDMR, an intermediate between NL and formal query\nlanguages. Given questions, their QDMR structures (annotated by non-experts or\nautomatically predicted), and the answers, we are able to automatically\nsynthesize SQL queries that are used to train text-to-SQL models. We test our\napproach by experimenting on five benchmark datasets. Our results show that the\nweakly supervised models perform competitively with those trained on annotated\nNL-SQL data. Overall, we effectively train text-to-SQL parsers, while using\nzero SQL annotations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wolfson_T/0/1/0/all/0/1\">Tomer Wolfson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deutch_D/0/1/0/all/0/1\">Daniel Deutch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berant_J/0/1/0/all/0/1\">Jonathan Berant</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GPL: Generative Pseudo Labeling for Unsupervised Domain Adaptation of Dense Retrieval. (arXiv:2112.07577v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.07577","description":"<p>Dense retrieval approaches can overcome the lexical gap and lead to\nsignificantly improved search results. However, they require large amounts of\ntraining data which is not available for most domains. As shown in previous\nwork (Thakur et al., 2021b), the performance of dense retrievers severely\ndegrades under a domain shift. This limits the usage of dense retrieval\napproaches to only a few domains with large training datasets.\n</p>\n<p>In this paper, we propose the novel unsupervised domain adaptation method\nGenerative Pseudo Labeling (GPL), which combines a query generator with pseudo\nlabeling from a cross-encoder. On six representative domain-specialized\ndatasets, we find the proposed GPL can outperform an out-of-the-box\nstate-of-the-art dense retrieval approach by up to 9.3 points nDCG@10. GPL\nrequires less (unlabeled) data from the target domain and is more robust in its\ntraining than previous methods.\n</p>\n<p>We further investigate the role of six recent pre-training methods in the\nscenario of domain adaptation for retrieval tasks, where only three could yield\nimproved results. The best approach, TSDAE (Wang et al., 2021) can be combined\nwith GPL, yielding another average improvement of 1.4 points nDCG@10 across the\nsix tasks. The code and the models are available at\nhttps://github.com/UKPLab/gpl.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kexin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thakur_N/0/1/0/all/0/1\">Nandan Thakur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reimers_N/0/1/0/all/0/1\">Nils Reimers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Knowledge Graph Embeddings based Approach for Author Name Disambiguation using Literals. (arXiv:2201.09555v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2201.09555","description":"<p>Scholarly data is growing continuously containing information about the\narticles from plethora of venues including conferences, journals, etc. Many\ninitiatives have been taken to make scholarly data available in the for of\nKnowledge Graphs (KGs). These efforts to standardize these data and make them\naccessible have also lead to many challenges such as exploration of scholarly\narticles, ambiguous authors, etc. This study more specifically targets the\nproblem of Author Name Disambiguation (AND) on Scholarly KGs and presents a\nnovel framework, Literally Author Name Disambiguation (LAND), which utilizes\nKnowledge Graph Embeddings (KGEs) using multimodal literal information\ngenerated from these KGs. This framework is based on three components: 1)\nMultimodal KGEs, 2) A blocking procedure, and finally, 3) Hierarchical\nAgglomerative Clustering. Extensive experiments have been conducted on two\nnewly created KGs: (i) KG containing information from Scientometrics Journal\nfrom 1978 onwards (OC-782K), and (ii) a KG extracted from a well-known\nbenchmark for AND provided by AMiner (AMiner-534K). The results show that our\nproposed architecture outperforms our baselines of 8-14% in terms of F$_1$\nscore and shows competitive performances on a challenging benchmark such as\nAMiner. The code and the datasets are publicly available through Github and\nZenodo respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Santini_C/0/1/0/all/0/1\">Cristian Santini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gesese_G/0/1/0/all/0/1\">Genet Asefa Gesese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peroni_S/0/1/0/all/0/1\">Silvio Peroni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gangemi_A/0/1/0/all/0/1\">Aldo Gangemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sack_H/0/1/0/all/0/1\">Harald Sack</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alam_M/0/1/0/all/0/1\">Mehwish Alam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"XAlign: Cross-lingual Fact-to-Text Alignment and Generation for Low-Resource Languages. (arXiv:2202.00291v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.00291","description":"<p>Multiple critical scenarios (like Wikipedia text generation given English\nInfoboxes) need automated generation of descriptive text in low resource (LR)\nlanguages from English fact triples. Previous work has focused on English\nfact-to-text (F2T) generation. To the best of our knowledge, there has been no\nprevious attempt on cross-lingual alignment or generation for LR languages.\nBuilding an effective cross-lingual F2T (XF2T) system requires alignment\nbetween English structured facts and LR sentences. We propose two unsupervised\nmethods for cross-lingual alignment. We contribute XALIGN, an XF2T dataset with\n0.45M pairs across 8 languages, of which 5402 pairs have been manually\nannotated. We also train strong baseline XF2T generation models on the XAlign\ndataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abhishek_T/0/1/0/all/0/1\">Tushar Abhishek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sagare_S/0/1/0/all/0/1\">Shivprasad Sagare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_B/0/1/0/all/0/1\">Bhavyajeet Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Anubhav Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_M/0/1/0/all/0/1\">Manish Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varma_V/0/1/0/all/0/1\">Vasudeva Varma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Tracking Dialogue State by Inheriting Slot Values in Mentioned Slot Pools. (arXiv:2202.07156v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.07156","description":"<p>Dialogue state tracking (DST) is a component of the task-oriented dialogue\nsystem. It is responsible for extracting and managing slot values according to\ndialogue utterances, where each slot represents an essential part of the\ninformation to accomplish a task, and slot value is updated recurrently in each\ndialogue turn. However, many DST models cannot update slot values\nappropriately. These models may repeatedly inherit wrong slot values extracted\nin previous turns, resulting in the fail of the entire DST task. They cannot\nupdate indirectly mentioned slots well, either. This study designed a model\nwith a mentioned slot pool (MSP) to tackle the update problem. The MSP is a\nslot-specific memory that records all mentioned slot values that may be\ninherited, and our model updates slot values according to the MSP and the\ndialogue context. Our model rejects inheriting the previous slot value when it\npredicates the value is wrong. Then, it re-extracts the slot value from the\ncurrent dialogue context. As the contextual information accumulates with the\ndialogue progress, the new value is more likely to be correct. It also can\ntrack the indirectly mentioned slot by picking a value from the MSP.\nExperimental results showed our model reached state-of-the-art DST performance\non MultiWOZ 2.1 and 2.2 datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zhoujian Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhengxing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_N/0/1/0/all/0/1\">Nai Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SemEval 2022 Task 12: Symlink- Linking Mathematical Symbols to their Descriptions. (arXiv:2202.09695v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.09695","description":"<p>Given the increasing number of livestreaming videos, automatic speech\nrecognition and post-processing for livestreaming video transcripts are crucial\nfor efficient data management as well as knowledge mining. A key step in this\nprocess is punctuation restoration which restores fundamental text structures\nsuch as phrase and sentence boundaries from the video transcripts. This work\npresents a new human-annotated corpus, called BehancePR, for punctuation\nrestoration in livestreaming video transcripts. Our experiments on BehancePR\ndemonstrate the challenges of punctuation restoration for this domain.\nFurthermore, we show that popular natural language processing toolkits are\nincapable of detecting sentence boundary on non-punctuated transcripts of\nlivestreaming videos, calling for more research effort to develop robust models\nfor this area.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lai_V/0/1/0/all/0/1\">Viet Dac Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Veyseh_A/0/1/0/all/0/1\">Amir Pouran Ben Veyseh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dernoncourt_F/0/1/0/all/0/1\">Franck Dernoncourt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Thien Huu Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-CPR: A Multi Domain Chinese Dataset for Passage Retrieval. (arXiv:2203.03367v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2203.03367","description":"<p>Passage retrieval is a fundamental task in information retrieval (IR)\nresearch, which has drawn much attention recently. In the English field, the\navailability of large-scale annotated dataset (e.g, MS MARCO) and the emergence\nof deep pre-trained language models (e.g, BERT) has resulted in a substantial\nimprovement of existing passage retrieval systems. However, in the Chinese\nfield, especially for specific domains, passage retrieval systems are still\nimmature due to quality-annotated dataset being limited by scale. Therefore, in\nthis paper, we present a novel multi-domain Chinese dataset for passage\nretrieval (Multi-CPR). The dataset is collected from three different domains,\nincluding E-commerce, Entertainment video and Medical. Each dataset contains\nmillions of passages and a certain amount of human annotated query-passage\nrelated pairs. We implement various representative passage retrieval methods as\nbaselines. We find that the performance of retrieval models trained on dataset\nfrom general domain will inevitably decrease on specific domain. Nevertheless,\na passage retrieval system built on in-domain annotated dataset can achieve\nsignificant improvement, which indeed demonstrates the necessity of domain\nlabeled data for further optimization. We hope the release of the Multi-CPR\ndataset could benchmark Chinese passage retrieval task in specific domain and\nalso make advances for future studies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Long_D/0/1/0/all/0/1\">Dingkun Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Q/0/1/0/all/0/1\">Qiong Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_K/0/1/0/all/0/1\">Kuan Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guangwei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1\">Pengjun Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_R/0/1/0/all/0/1\">Ruijie Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jian Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_G/0/1/0/all/0/1\">Guanjun Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_L/0/1/0/all/0/1\">Luxi Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_P/0/1/0/all/0/1\">Ping Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Where Does the Performance Improvement Come From? -- A Reproducibility Concern about Image-Text Retrieval. (arXiv:2203.03853v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2203.03853","description":"<p>This article aims to provide the information retrieval community with some\nreflections on recent advances in retrieval learning by analyzing the\nreproducibility of image-text retrieval models. Due to the increase of\nmultimodal data over the last decade, image-text retrieval has steadily become\na major research direction in the field of information retrieval. Numerous\nresearchers train and evaluate image-text retrieval algorithms using benchmark\ndatasets such as MS-COCO and Flickr30k. Research in the past has mostly focused\non performance, with multiple state-of-the-art methodologies being suggested in\na variety of ways. According to their assertions, these techniques provide\nimproved modality interactions and hence more precise multimodal\nrepresentations. In contrast to previous works, we focus on the reproducibility\nof the approaches and the examination of the elements that lead to improved\nperformance by pretrained and nonpretrained models in retrieving images and\ntext. To be more specific, we first examine the related reproducibility\nconcerns and explain why our focus is on image-text retrieval tasks. Second, we\nsystematically summarize the current paradigm of image-text retrieval models\nand the stated contributions of those approaches. Third, we analyze various\naspects of the reproduction of pretrained and nonpretrained retrieval models.\nTo complete this, we conducted ablation experiments and obtained some\ninfluencing factors that affect retrieval recall more than the improvement\nclaimed in the original paper. Finally, we present some reflections and\nchallenges that the retrieval community should consider in the future. Our\nsource code is publicly available at\nhttps://github.com/WangFei-2019/Image-text-Retrieval.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rao_J/0/1/0/all/0/1\">Jun Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1\">Liang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_S/0/1/0/all/0/1\">Shuhan Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_Y/0/1/0/all/0/1\">Yibing Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weifeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Integrating Dependency Tree Into Self-attention for Sentence Representation. (arXiv:2203.05918v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.05918","description":"<p>Recent progress on parse tree encoder for sentence representation learning is\nnotable. However, these works mainly encode tree structures recursively, which\nis not conducive to parallelization. On the other hand, these works rarely take\ninto account the labels of arcs in dependency trees. To address both issues, we\npropose Dependency-Transformer, which applies a relation-attention mechanism\nthat works in concert with the self-attention mechanism. This mechanism aims to\nencode the dependency and the spatial positional relations between nodes in the\ndependency tree of sentences. By a score-based method, we successfully inject\nthe syntax information without affecting Transformer's parallelizability. Our\nmodel outperforms or is comparable to the state-of-the-art methods on four\ntasks for sentence representation and has obvious advantages in computational\nefficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Junhua Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiajun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuxuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shangbo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xue Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BIOS: An Algorithmically Generated Biomedical Knowledge Graph. (arXiv:2203.09975v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.09975","description":"<p>Biomedical knowledge graphs (BioMedKGs) are essential infrastructures for\nbiomedical and healthcare big data and artificial intelligence (AI),\nfacilitating natural language processing, model development, and data exchange.\nFor decades, these knowledge graphs have been developed via expert curation;\nhowever, this method can no longer keep up with today's AI development, and a\ntransition to algorithmically generated BioMedKGs is necessary. In this work,\nwe introduce the Biomedical Informatics Ontology System (BIOS), the first\nlarge-scale publicly available BioMedKG generated completely by machine\nlearning algorithms. BIOS currently contains 4.1 million concepts, 7.4 million\nterms in two languages, and 7.3 million relation triplets. We present the\nmethodology for developing BIOS, including the curation of raw biomedical\nterms, computational identification of synonymous terms and aggregation of\nthese terms to create concept nodes, semantic type classification of the\nconcepts, relation identification, and biomedical machine translation. We\nprovide statistics on the current BIOS content and perform preliminary\nassessments of term quality, synonym grouping, and relation extraction. The\nresults suggest that machine learning-based BioMedKG development is a viable\nalternative to traditional expert curation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Sheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zheng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_J/0/1/0/all/0/1\">Jun Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1\">Shengxuan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ying_H/0/1/0/all/0/1\">Huaiyuan Ying</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_S/0/1/0/all/0/1\">Sihang Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Jingyi Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1\">Hongyi Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhengyun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yucong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1\">Keming Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yutao Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shum_H/0/1/0/all/0/1\">Heung-Yeung Shum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Reason Deductively: Math Word Problem Solving as Complex Relation Extraction. (arXiv:2203.10316v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.10316","description":"<p>Solving math word problems requires deductive reasoning over the quantities\nin the text. Various recent research efforts mostly relied on\nsequence-to-sequence or sequence-to-tree models to generate mathematical\nexpressions without explicitly performing relational reasoning between\nquantities in the given context. While empirically effective, such approaches\ntypically do not provide explanations for the generated expressions. In this\nwork, we view the task as a complex relation extraction problem, proposing a\nnovel approach that presents explainable deductive reasoning steps to\niteratively construct target expressions, where each step involves a primitive\noperation over two quantities defining their relation. Through extensive\nexperiments on four benchmark datasets, we show that the proposed model\nsignificantly outperforms existing strong baselines. We further demonstrate\nthat the deductive procedure not only presents more explainable steps but also\nenables us to make more accurate predictions on questions that require more\ncomplex reasoning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jie_Z/0/1/0/all/0/1\">Zhanming Jie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jierui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Wei Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Empirical Study of Language Model Integration for Transducer based Speech Recognition. (arXiv:2203.16776v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2203.16776","description":"<p>Utilizing text-only data with an external language model (LM) in end-to-end\nRNN-Transducer (RNN-T) for speech recognition is challenging. Recently, a class\nof methods such as density ratio (DR) and ILM estimation (ILME) have been\ndeveloped, outperforming the classic shallow fusion (SF) method. The basic idea\nbehind these methods is that RNN-T posterior should first subtract the\nimplicitly learned ILM prior, in order to integrate the external LM. While\nrecent studies suggest that RNN-T only learns some low-order language model\ninformation, the DR method uses a well-trained ILM. We hypothesize that this\nsetting is appropriate and may deteriorate the performance of the DR method,\nand propose a low-order density ratio method (LODR) by training a low-order\nweak ILM for DR. Extensive empirical experiments are conducted on both\nin-domain and cross-domain scenarios on English LibriSpeech &amp; Tedlium-2 and\nChinese WenetSpeech &amp; AISHELL-1 datasets. It is shown that LODR consistently\noutperforms SF in all tasks, while performing generally close to ILME and\nbetter than DR in most tests.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zheng_H/0/1/0/all/0/1\">Huahuan Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+An_K/0/1/0/all/0/1\">Keyu An</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ou_Z/0/1/0/all/0/1\">Zhijian Ou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_C/0/1/0/all/0/1\">Chen Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ding_K/0/1/0/all/0/1\">Ke Ding</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wan_G/0/1/0/all/0/1\">Guanglu Wan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Winoground: Probing Vision and Language Models for Visio-Linguistic Compositionality. (arXiv:2204.03162v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.03162","description":"<p>We present a novel task and dataset for evaluating the ability of vision and\nlanguage models to conduct visio-linguistic compositional reasoning, which we\ncall Winoground. Given two images and two captions, the goal is to match them\ncorrectly - but crucially, both captions contain a completely identical set of\nwords, only in a different order. The dataset was carefully hand-curated by\nexpert annotators and is labeled with a rich set of fine-grained tags to assist\nin analyzing model performance. We probe a diverse range of state-of-the-art\nvision and language models and find that, surprisingly, none of them do much\nbetter than chance. Evidently, these models are not as skilled at\nvisio-linguistic compositional reasoning as we might have hoped. We perform an\nextensive analysis to obtain insights into how future work might try to\nmitigate these models' shortcomings. We aim for Winoground to serve as a useful\nevaluation set for advancing the state of the art and driving further progress\nin the field. The dataset is available at\nhttps://huggingface.co/datasets/facebook/winoground.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thrush_T/0/1/0/all/0/1\">Tristan Thrush</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_R/0/1/0/all/0/1\">Ryan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bartolo_M/0/1/0/all/0/1\">Max Bartolo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Amanpreet Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_A/0/1/0/all/0/1\">Adina Williams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiela_D/0/1/0/all/0/1\">Douwe Kiela</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ross_C/0/1/0/all/0/1\">Candace Ross</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated speech tools for helping communities process restricted-access corpora for language revival efforts. (arXiv:2204.07272v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.07272","description":"<p>Many archival recordings of speech from endangered languages remain\nunannotated and inaccessible to community members and language learning\nprograms. One bottleneck is the time-intensive nature of annotation. An even\nnarrower bottleneck occurs for recordings with access constraints, such as\nlanguage that must be vetted or filtered by authorised community members before\nannotation can begin. We propose a privacy-preserving workflow to widen both\nbottlenecks for recordings where speech in the endangered language is\nintermixed with a more widely-used language such as English for meta-linguistic\ncommentary and questions (e.g. What is the word for 'tree'?). We integrate\nvoice activity detection (VAD), spoken language identification (SLI), and\nautomatic speech recognition (ASR) to transcribe the metalinguistic content,\nwhich an authorised person can quickly scan to triage recordings that can be\nannotated by people with lower levels of access. We report work-in-progress\nprocessing 136 hours archival audio containing a mix of English and Muruwari.\nOur collaborative work with the Muruwari custodian of the archival materials\nshow that this workflow reduces metalanguage transcription time by 20% even\ngiven only minimal amounts of annotated training data: 10 utterances per\nlanguage for SLI and for ASR at most 39 minutes, and possibly as little as 39\nseconds.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+San_N/0/1/0/all/0/1\">Nay San</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bartelds_M/0/1/0/all/0/1\">Martijn Bartelds</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ogunremi_T/0/1/0/all/0/1\">Tol&#xfa;lop&#xe9; &#xd2;g&#xfa;nr&#xe8;m&#xed;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mount_A/0/1/0/all/0/1\">Alison Mount</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thompson_R/0/1/0/all/0/1\">Ruben Thompson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Higgins_M/0/1/0/all/0/1\">Michael Higgins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barker_R/0/1/0/all/0/1\">Roy Barker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simpson_J/0/1/0/all/0/1\">Jane Simpson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurafsky_D/0/1/0/all/0/1\">Dan Jurafsky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision-and-Language Pretrained Models: A Survey. (arXiv:2204.07356v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.07356","description":"<p>Pretrained models have produced great success in both Computer Vision (CV)\nand Natural Language Processing (NLP). This progress leads to learning joint\nrepresentations of vision and language pretraining by feeding visual and\nlinguistic contents into a multi-layer transformer, Visual-Language Pretrained\nModels (VLPMs). In this paper, we present an overview of the major advances\nachieved in VLPMs for producing joint representations of vision and language.\nAs the preliminaries, we briefly describe the general task definition and\ngenetic architecture of VLPMs. We first discuss the language and vision data\nencoding methods and then present the mainstream VLPM structure as the core\ncontent. We further summarise several essential pretraining and fine-tuning\nstrategies. Finally, we highlight three future directions for both CV and NLP\nresearchers to provide insightful guidance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Long_S/0/1/0/all/0/1\">Siqu Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_F/0/1/0/all/0/1\">Feiqi Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Soyeon Caren Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haiqing Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Summarization with Graphical Elements. (arXiv:2204.07551v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.07551","description":"<p>Automatic text summarization has experienced substantial progress in recent\nyears. With this progress, the question has arisen whether the types of\nsummaries that are typically generated by automatic summarization models align\nwith users' needs. Ter Hoeve et al (2020) answer this question negatively.\nAmongst others, they recommend focusing on generating summaries with more\ngraphical elements. This is in line with what we know from the\npsycholinguistics literature about how humans process text. Motivated from\nthese two angles, we propose a new task: summarization with graphical elements,\nand we verify that these summaries are helpful for a critical mass of people.\nWe collect a high quality human labeled dataset to support research into the\ntask. We present a number of baseline methods that show that the task is\ninteresting and challenging. Hence, with this work we hope to inspire a new\nline of research within the automatic summarization community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hoeve_M/0/1/0/all/0/1\">Maartje ter Hoeve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiseleva_J/0/1/0/all/0/1\">Julia Kiseleva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rijke_M/0/1/0/all/0/1\">Maarten de Rijke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Learning with Hard Negative Entities for Entity Set Expansion. (arXiv:2204.07789v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.07789","description":"<p>Entity Set Expansion (ESE) is a promising task which aims to expand entities\nof the target semantic class described by a small seed entity set. Various NLP\nand IR applications will benefit from ESE due to its ability to discover\nknowledge. Although previous ESE methods have achieved great progress, most of\nthem still lack the ability to handle hard negative entities (i.e., entities\nthat are difficult to distinguish from the target entities), since two entities\nmay or may not belong to the same semantic class based on different granularity\nlevels we analyze on. To address this challenge, we devise an entity-level\nmasked language model with contrastive learning to refine the representation of\nentities. In addition, we propose the ProbExpan, a novel probabilistic ESE\nframework utilizing the entity representation obtained by the aforementioned\nlanguage model to expand entities. Extensive experiments and detailed analyses\non three datasets show that our method outperforms previous state-of-the-art\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yinghui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yangning Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yuxin He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tianyu Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Ying Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Hai-Tao Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Logical Inference for Counting on Semi-structured Tables. (arXiv:2204.07803v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.07803","description":"<p>Recently, the Natural Language Inference (NLI) task has been studied for\nsemi-structured tables that do not have a strict format. Although neural\napproaches have achieved high performance in various types of NLI, including\nNLI between semi-structured tables and texts, they still have difficulty in\nperforming a numerical type of inference, such as counting. To handle a\nnumerical type of inference, we propose a logical inference system for\nreasoning between semi-structured tables and texts. We use logical\nrepresentations as meaning representations for tables and texts and use model\nchecking to handle a numerical type of inference between texts and tables. To\nevaluate the extent to which our system can perform inference with numerical\ncomparatives, we make an evaluation protocol that focuses on numerical\nunderstanding between semi-structured tables and texts in English. We show that\nour system can more robustly perform inference between tables and texts that\nrequires numerical understanding compared with current neural approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kurosawa_T/0/1/0/all/0/1\">Tomoya Kurosawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yanaka_H/0/1/0/all/0/1\">Hitomi Yanaka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BLISS: Robust Sequence-to-Sequence Learning via Self-Supervised Input Representation. (arXiv:2204.07837v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.07837","description":"<p>Data augmentations (DA) are the cores to achieving robust\nsequence-to-sequence learning on various natural language processing (NLP)\ntasks. However, most of the DA approaches force the decoder to make predictions\nconditioned on the perturbed input representation, underutilizing supervised\ninformation provided by perturbed input. In this work, we propose a\nframework-level robust sequence-to-sequence learning approach, named BLISS, via\nself-supervised input representation, which has the great potential to\ncomplement the data-level augmentation approaches. The key idea is to supervise\nthe sequence-to-sequence framework with both the \\textit{supervised}\n(\"input$\\rightarrow$output\") and \\textit{self-supervised} (\"perturbed\ninput$\\rightarrow$input\") information. We conduct comprehensive experiments to\nvalidate the effectiveness of BLISS on various tasks, including machine\ntranslation, grammatical error correction, and text summarization. The results\nshow that BLISS outperforms significantly the vanilla Transformer and\nconsistently works well across tasks than the other five contrastive baselines.\nExtensive analyses reveal that BLISS learns robust representations and rich\nlinguistic knowledge, confirming our claim. Source code will be released upon\npublication.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1\">Liang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_D/0/1/0/all/0/1\">Dazhao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xuebo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sim-2-Sim Transfer for Vision-and-Language Navigation in Continuous Environments. (arXiv:2204.09667v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.09667","description":"<p>Recent work in Vision-and-Language Navigation (VLN) has presented two\nenvironmental paradigms with differing realism -- the standard VLN setting\nbuilt on topological environments where navigation is abstracted away, and the\nVLN-CE setting where agents must navigate continuous 3D environments using\nlow-level actions. Despite sharing the high-level task and even the underlying\ninstruction-path data, performance on VLN-CE lags behind VLN significantly. In\nthis work, we explore this gap by transferring an agent from the abstract\nenvironment of VLN to the continuous environment of VLN-CE. We find that this\nsim-2-sim transfer is highly effective, improving over the prior state of the\nart in VLN-CE by +12% success rate. While this demonstrates the potential for\nthis direction, the transfer does not fully retain the original performance of\nthe agent in the abstract setting. We present a sequence of experiments to\nidentify what differences result in performance degradation, providing clear\ndirections for further improvement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Krantz_J/0/1/0/all/0/1\">Jacob Krantz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Stefan Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Summary of the ALQAC 2021 Competition. (arXiv:2204.10717v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.10717","description":"<p>We summarize the evaluation of the first Automated Legal Question Answering\nCompetition (ALQAC 2021). The competition this year contains three tasks, which\naims at processing the statute law document, which are Legal Text Information\nRetrieval (Task 1), Legal Text Entailment Prediction (Task 2), and Legal Text\nQuestion Answering (Task 3). The final goal of these tasks is to build a system\nthat can automatically determine whether a particular statement is lawful.\nThere is no limit to the approaches of the participating teams. This year,\nthere are 5 teams participating in Task 1, 6 teams participating in Task 2, and\n5 teams participating in Task 3. There are in total 36 runs submitted to the\norganizer. In this paper, we summarize each team's approaches, official\nresults, and some discussion about the competition. Only results of the teams\nwho successfully submit their approach description paper are reported in this\npaper.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thanh_N/0/1/0/all/0/1\">Nguyen Ha Thanh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quan_B/0/1/0/all/0/1\">Bui Minh Quan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1\">Chau Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1\">Tung Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phuong_N/0/1/0/all/0/1\">Nguyen Minh Phuong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Binh_D/0/1/0/all/0/1\">Dang Tran Binh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yen_V/0/1/0/all/0/1\">Vuong Thi Hai Yen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Racharak_T/0/1/0/all/0/1\">Teeradaj Racharak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minh_N/0/1/0/all/0/1\">Nguyen Le Minh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vu_T/0/1/0/all/0/1\">Tran Duc Vu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anh_P/0/1/0/all/0/1\">Phan Viet Anh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Son_N/0/1/0/all/0/1\">Nguyen Truong Son</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Huy Tien Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Butr_indr_B/0/1/0/all/0/1\">Bhumindr Butr-indr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vateekul_P/0/1/0/all/0/1\">Peerapon Vateekul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boonkwan_P/0/1/0/all/0/1\">Prachya Boonkwan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TOUR: Dynamic Topic and Sentiment Analysis of User Reviews for Assisting App Release. (arXiv:2103.15774v2 [cs.SE] CROSS LISTED)","link":"http://arxiv.org/abs/2103.15774","description":"<p>App reviews deliver user opinions and emerging issues (e.g., new bugs) about\nthe app releases. Due to the dynamic nature of app reviews, topics and\nsentiment of the reviews would change along with app release versions. Although\nseveral studies have focused on summarizing user opinions by analyzing user\nsentiment towards app features, no practical tool is released. The large\nquantity of reviews and noise words also necessitates an automated tool for\nmonitoring user reviews. In this paper, we introduce TOUR for dynamic TOpic and\nsentiment analysis of User Reviews. TOUR is able to (i) detect and summarize\nemerging app issues over app versions, (ii) identify user sentiment towards app\nfeatures, and (iii) prioritize important user reviews for facilitating\ndevelopers' examination. The core techniques of TOUR include the online topic\nmodeling approach and sentiment prediction strategy. TOUR provides entries for\ndevelopers to customize the hyper-parameters and the results are presented in\nan interactive way. We evaluate TOUR by conducting a developer survey that\ninvolves 15 developers, and all of them confirm the practical usefulness of the\nrecommended feature changes by TOUR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Tianyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Cuiyun Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zang_J/0/1/0/all/0/1\">Jingya Zang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_D/0/1/0/all/0/1\">David Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_M/0/1/0/all/0/1\">Michael R. Lyu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-04-25T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Identity Preserving Loss for Learned Image Compression. (arXiv:2204.10869v1 [cs.CV])","link":"http://arxiv.org/abs/2204.10869","description":"<p>Deep learning model inference on embedded devices is challenging due to the\nlimited availability of computation resources. A popular alternative is to\nperform model inference on the cloud, which requires transmitting images from\nthe embedded device to the cloud. Image compression techniques are commonly\nemployed in such cloud-based architectures to reduce transmission latency over\nlow bandwidth networks. This work proposes an end-to-end image compression\nframework that learns domain-specific features to achieve higher compression\nratios than standard HEVC/JPEG compression techniques while maintaining\naccuracy on downstream tasks (e.g., recognition). Our framework does not\nrequire fine-tuning of the downstream task, which allows us to drop-in any\noff-the-shelf downstream task model without retraining. We choose faces as an\napplication domain due to the ready availability of datasets and off-the-shelf\nrecognition models as representative downstream tasks. We present a novel\nIdentity Preserving Reconstruction (IPR) loss function which achieves\nBits-Per-Pixel (BPP) values that are ~38% and ~42% of CRF-23 HEVC compression\nfor LFW (low-resolution) and CelebA-HQ (high-resolution) datasets,\nrespectively, while maintaining parity in recognition accuracy. The superior\ncompression ratio is achieved as the model learns to retain the domain-specific\nfeatures (e.g., facial features) while sacrificing details in the background.\nFurthermore, images reconstructed by our proposed compression model are robust\nto changes in downstream model architectures. We show at-par recognition\nperformance on the LFW dataset with an unseen recognition model while retaining\na lower BPP value of ~38% of CRF-23 HEVC compression.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jiuhong Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aggarwal_L/0/1/0/all/0/1\">Lavisha Aggarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_P/0/1/0/all/0/1\">Prithviraj Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aggarwal_M/0/1/0/all/0/1\">Manoj Aggarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Medioni_G/0/1/0/all/0/1\">Gerard Medioni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative sampling in tractography using autoencoders (GESTA). (arXiv:2204.10891v1 [cs.CV])","link":"http://arxiv.org/abs/2204.10891","description":"<p>Current tractography methods use the local orientation information to\npropagate streamlines from seed locations. Many such seeds provide streamlines\nthat stop prematurely or fail to map the true pathways because some white\nmatter bundles are \"harder-to-track\" than others. This results in tractography\nreconstructions with poor white and gray matter spatial coverage. In this work,\nwe propose a generative, autoencoder-based method, named GESTA (Generative\nSampling in Tractography using Autoencoders), that produces streamlines with\nbetter spatial coverage. Compared to other deep learning methods, our\nautoencoder-based framework is not constrained by any prior or a fixed set of\nbundles. GESTA produces new and complete streamlines for any white matter\nbundle. GESTA is shown to be effective on both synthetic and human brain in\nvivo data. Our streamline evaluation framework ensures that the streamlines\nproduced by GESTA are anatomically plausible and fit well to the local\ndiffusion signal. The streamline evaluation criteria assess anatomy (white\nmatter coverage), local orientation alignment (direction), geometry features of\nstreamlines, and optionally, gray matter connectivity. The GESTA framework\noffers considerable gains in bundle coverage using a reduced set of seeding\nstreamlines with a 1.5x improvement for the \"Fiber Cup\", and 6x for the ISMRM\n2015 Tractography Challenge datasets. Similarly, it provides a 4x white matter\nvolume increase on the BIL&amp;GIN callosal homotopic dataset. It also successfully\ngenerates new streamlines in poorly populated bundles, such as the fornix and\nother hard-to-track bundles, on in vivo data. GESTA is thus the first deep\ntractography generative method that can improve white matter reconstruction of\nhard-to-track bundles.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Legarreta_J/0/1/0/all/0/1\">Jon Haitz Legarreta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petit_L/0/1/0/all/0/1\">Laurent Petit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jodoin_P/0/1/0/all/0/1\">Pierre-Marc Jodoin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Descoteaux_M/0/1/0/all/0/1\">Maxime Descoteaux</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transfer Learning from Synthetic In-vitro Soybean Pods Dataset for In-situ Segmentation of On-branch Soybean Pod. (arXiv:2204.10902v1 [cs.CV])","link":"http://arxiv.org/abs/2204.10902","description":"<p>The mature soybean plants are of complex architecture with pods frequently\ntouching each other, posing a challenge for in-situ segmentation of on-branch\nsoybean pods. Deep learning-based methods can achieve accurate training and\nstrong generalization capabilities, but it demands massive labeled data, which\nis often a limitation, especially for agricultural applications. As lacking the\nlabeled data to train an in-situ segmentation model for on-branch soybean pods,\nwe propose a transfer learning from synthetic in-vitro soybean pods. First, we\npresent a novel automated image generation method to rapidly generate a\nsynthetic in-vitro soybean pods dataset with plenty of annotated samples. The\nin-vitro soybean pods samples are overlapped to simulate the frequently\nphysically touching of on-branch soybean pods. Then, we design a two-step\ntransfer learning. In the first step, we finetune an instance segmentation\nnetwork pretrained by a source domain (MS COCO dataset) with a synthetic target\ndomain (in-vitro soybean pods dataset). In the second step, transferring from\nsimulation to reality is performed by finetuning on a few real-world mature\nsoybean plant samples. The experimental results show the effectiveness of the\nproposed two-step transfer learning method, such that AP$_{50}$ was 0.80 for\nthe real-world mature soybean plant test dataset, which is higher than that of\ndirect adaptation and its AP$_{50}$ was 0.77. Furthermore, the visualizations\nof in-situ segmentation results of on-branch soybean pods show that our method\nperforms better than other methods, especially when soybean pods overlap\ndensely.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Si Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Lihua Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xieyuanli Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zabawa_L/0/1/0/all/0/1\">Laura Zabawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Man Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Minjuan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Label a Herd in Minutes: Individual Holstein-Friesian Cattle Identification. (arXiv:2204.10905v1 [cs.CV])","link":"http://arxiv.org/abs/2204.10905","description":"<p>We describe a practically evaluated approach for training visual cattle ID\nsystems for a whole farm requiring only ten minutes of labelling effort. In\nparticular, for the task of automatic identification of individual\nHolstein-Friesians in real-world farm CCTV, we show that self-supervision,\nmetric learning, cluster analysis, and active learning can complement each\nother to significantly reduce the annotation requirements usually needed to\ntrain cattle identification frameworks. Evaluating the approach on the test\nportion of the publicly available Cows2021 dataset, for training we use 23,350\nframes across 435 single individual tracklets generated by automated oriented\ncattle detection and tracking in operational farm footage. Self-supervised\nmetric learning is first employed to initialise a candidate identity space\nwhere each tracklet is considered a distinct entity. Grouping entities into\nequivalence classes representing cattle identities is then performed by\nautomated merging via cluster analysis and active learning. Critically, we\nidentify the inflection point at which automated choices cannot replicate\nimprovements based on human intervention to reduce annotation to a minimum.\nExperimental results show that cluster analysis and a few minutes of labelling\nafter automated self-supervision can improve the test identification accuracy\nof 153 identities to 92.44% (ARI=0.93) from the 74.9% (ARI=0.754) obtained by\nself-supervision only. These promising results indicate that a tailored\ncombination of human and machine reasoning in visual cattle ID pipelines can be\nhighly effective whilst requiring only minimal labelling effort. We provide all\nkey source code and network weights with this paper for easy result\nreproduction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jing Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burghardt_T/0/1/0/all/0/1\">Tilo Burghardt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Campbell_N/0/1/0/all/0/1\">Neill W. Campbell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revealing Occlusions with 4D Neural Fields. (arXiv:2204.10916v1 [cs.CV])","link":"http://arxiv.org/abs/2204.10916","description":"<p>For computer vision systems to operate in dynamic situations, they need to be\nable to represent and reason about object permanence. We introduce a framework\nfor learning to estimate 4D visual representations from monocular RGB-D, which\nis able to persist objects, even once they become obstructed by occlusions.\nUnlike traditional video representations, we encode point clouds into a\ncontinuous representation, which permits the model to attend across the\nspatiotemporal context to resolve occlusions. On two large video datasets that\nwe release along with this paper, our experiments show that the representation\nis able to successfully reveal occlusions for several tasks, without any\narchitectural changes. Visualizations show that the attention mechanism\nautomatically learns to follow occluded objects. Since our approach can be\ntrained end-to-end and is easily adaptable, we believe it will be useful for\nhandling occlusions in many video understanding tasks. Data, code, and models\nare available at https://occlusions.cs.columbia.edu/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hoorick_B/0/1/0/all/0/1\">Basile Van Hoorick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tendulkar_P/0/1/0/all/0/1\">Purva Tendulkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suris_D/0/1/0/all/0/1\">Didac Suris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_D/0/1/0/all/0/1\">Dennis Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stent_S/0/1/0/all/0/1\">Simon Stent</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vondrick_C/0/1/0/all/0/1\">Carl Vondrick</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SegDiscover: Visual Concept Discovery via Unsupervised Semantic Segmentation. (arXiv:2204.10926v1 [cs.CV])","link":"http://arxiv.org/abs/2204.10926","description":"<p>Visual concept discovery has long been deemed important to improve\ninterpretability of neural networks, because a bank of semantically meaningful\nconcepts would provide us with a starting point for building machine learning\nmodels that exhibit intelligible reasoning process. Previous methods have\ndisadvantages: either they rely on labelled support sets that incorporate human\nbiases for objects that are \"useful,\" or they fail to identify multiple\nconcepts that occur within a single image. We reframe the concept discovery\ntask as an unsupervised semantic segmentation problem, and present SegDiscover,\na novel framework that discovers semantically meaningful visual concepts from\nimagery datasets with complex scenes without supervision. Our method contains\nthree important pieces: generating concept primitives from raw images,\ndiscovering concepts by clustering in the latent space of a self-supervised\npretrained encoder, and concept refinement via neural network smoothing.\nExperimental results provide evidence that our method can discover multiple\nconcepts within a single image and outperforms state-of-the-art unsupervised\nmethods on complex datasets such as Cityscapes and COCO-Stuff. Our method can\nbe further used as a neural network explanation tool by comparing results\nobtained by different encoders.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Haiyang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudin_C/0/1/0/all/0/1\">Cynthia Rudin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Multi-level Alignment Training Scheme for Video-and-Language Grounding. (arXiv:2204.10938v1 [cs.CV])","link":"http://arxiv.org/abs/2204.10938","description":"<p>To solve video-and-language grounding tasks, the key is for the network to\nunderstand the connection between the two modalities. For a pair of video and\nlanguage description, their semantic relation is reflected by their encodings'\nsimilarity. A good multi-modality encoder should be able to well capture both\ninputs' semantics and encode them in the shared feature space where embedding\ndistance gets properly translated into their semantic similarity. In this work,\nwe focused on this semantic connection between video and language, and\ndeveloped a multi-level alignment training scheme to directly shape the\nencoding process. Global and segment levels of video-language alignment pairs\nwere designed, based on the information similarity ranging from high-level\ncontext to fine-grained semantics. The contrastive loss was used to contrast\nthe encodings' similarities between the positive and negative alignment pairs,\nand to ensure the network is trained in such a way that similar information is\nencoded closely in the shared feature space while information of different\nsemantics is kept apart. Our multi-level alignment training can be applied to\nvarious video-and-language grounding tasks. Together with the task-specific\ntraining loss, our framework achieved comparable performance to previous\nstate-of-the-arts on multiple video QA and retrieval datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yubo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_F/0/1/0/all/0/1\">Feiyang Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ping_Q/0/1/0/all/0/1\">Qing Ping</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thattai_G/0/1/0/all/0/1\">Govind Thattai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unified Pretraining Framework for Document Understanding. (arXiv:2204.10939v1 [cs.CL])","link":"http://arxiv.org/abs/2204.10939","description":"<p>Document intelligence automates the extraction of information from documents\nand supports many business applications. Recent self-supervised learning\nmethods on large-scale unlabeled document datasets have opened up promising\ndirections towards reducing annotation efforts by training models with\nself-supervised objectives. However, most of the existing document pretraining\nmethods are still language-dominated. We present UDoc, a new unified\npretraining framework for document understanding. UDoc is designed to support\nmost document understanding tasks, extending the Transformer to take multimodal\nembeddings as input. Each input element is composed of words and visual\nfeatures from a semantic region of the input document image. An important\nfeature of UDoc is that it learns a generic representation by making use of\nthree self-supervised losses, encouraging the representation to model\nsentences, learn similarities, and align modalities. Extensive empirical\nanalysis demonstrates that the pretraining procedure learns better joint\nrepresentations and leads to improvements in downstream tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jiuxiang Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuen_J/0/1/0/all/0/1\">Jason Kuen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morariu_V/0/1/0/all/0/1\">Vlad I. Morariu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Handong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barmpalios_N/0/1/0/all/0/1\">Nikolaos Barmpalios</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_R/0/1/0/all/0/1\">Rajiv Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nenkova_A/0/1/0/all/0/1\">Ani Nenkova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1\">Tong Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluation of Multi-Scale Multiple Instance Learning to Improve Thyroid Cancer Classification. (arXiv:2204.10942v1 [eess.IV])","link":"http://arxiv.org/abs/2204.10942","description":"<p>Thyroid cancer is currently the fifth most common malignancy diagnosed in\nwomen. Since differentiation of cancer sub-types is important for treatment and\ncurrent, manual methods are time consuming and subjective, automatic\ncomputer-aided differentiation of cancer types is crucial. Manual\ndifferentiation of thyroid cancer is based on tissue sections, analysed by\npathologists using histological features. Due to the enormous size of gigapixel\nwhole slide images, holistic classification using deep learning methods is not\nfeasible. Patch based multiple instance learning approaches, combined with\naggregations such as bag-of-words, is a common approach. This work's\ncontribution is to extend a patch based state-of-the-art method by generating\nand combining feature vectors of three different patch resolutions and\nanalysing three distinct ways of combining them. The results showed\nimprovements in one of the three multi-scale approaches, while the others led\nto decreased scores. This provides motivation for analysis and discussion of\nthe individual approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Tschuchnig_M/0/1/0/all/0/1\">Maximilian E. Tschuchnig</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Grubmuller_P/0/1/0/all/0/1\">Philipp Grubm&#xfc;ller</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Stangassinger_L/0/1/0/all/0/1\">Lea M. Stangassinger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kreutzer_C/0/1/0/all/0/1\">Christina Kreutzer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Couillard_Despres_S/0/1/0/all/0/1\">S&#xe9;bastien Couillard-Despr&#xe9;s</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Oostingh_G/0/1/0/all/0/1\">Gertie J. Oostingh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hittmair_A/0/1/0/all/0/1\">Anton Hittmair</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gadermayr_M/0/1/0/all/0/1\">Michael Gadermayr</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HRPlanes: High Resolution Airplane Dataset for Deep Learning. (arXiv:2204.10959v1 [cs.CV])","link":"http://arxiv.org/abs/2204.10959","description":"<p>Airplane detection from satellite imagery is a challenging task due to the\ncomplex backgrounds in the images and differences in data acquisition\nconditions caused by the sensor geometry and atmospheric effects. Deep learning\nmethods provide reliable and accurate solutions for automatic detection of\nairplanes; however, huge amount of training data is required to obtain\npromising results. In this study, we create a novel airplane detection dataset\ncalled High Resolution Planes (HRPlanes) by using images from Google Earth (GE)\nand labeling the bounding box of each plane on the images. HRPlanes include GE\nimages of several different airports across the world to represent a variety of\nlandscape, seasonal and satellite geometry conditions obtained from different\nsatellites. We evaluated our dataset with two widely used object detection\nmethods namely YOLOv4 and Faster R-CNN. Our preliminary results show that the\nproposed dataset can be a valuable data source and benchmark data set for\nfuture applications. Moreover, proposed architectures and results of this study\ncould be used for transfer learning of different datasets and models for\nairplane detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bakirman_T/0/1/0/all/0/1\">Tolga Bakirman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sertel_E/0/1/0/all/0/1\">Elif Sertel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Attention Emerges from Recurrent Sparse Reconstruction. (arXiv:2204.10962v1 [cs.CV])","link":"http://arxiv.org/abs/2204.10962","description":"<p>Visual attention helps achieve robust perception under noise, corruption, and\ndistribution shifts in human vision, which are areas where modern neural\nnetworks still fall short. We present VARS, Visual Attention from Recurrent\nSparse reconstruction, a new attention formulation built on two prominent\nfeatures of the human visual attention mechanism: recurrency and sparsity.\nRelated features are grouped together via recurrent connections between\nneurons, with salient objects emerging via sparse regularization. VARS adopts\nan attractor network with recurrent connections that converges toward a stable\npattern over time. Network layers are represented as ordinary differential\nequations (ODEs), formulating attention as a recurrent attractor network that\nequivalently optimizes the sparse reconstruction of input using a dictionary of\n\"templates\" encoding underlying patterns of data. We show that self-attention\nis a special case of VARS with a single-step optimization and no sparsity\nconstraint. VARS can be readily used as a replacement for self-attention in\npopular vision transformers, consistently improving their robustness across\nvarious benchmarks. Code is released on GitHub (https://github.com/bfshi/VARS).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1\">Baifeng Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yale Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_N/0/1/0/all/0/1\">Neel Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1\">Trevor Darrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLIP-Dissect: Automatic Description of Neuron Representations in Deep Vision Networks. (arXiv:2204.10965v1 [cs.CV])","link":"http://arxiv.org/abs/2204.10965","description":"<p>In this paper, we propose CLIP-Dissect, a new technique to automatically\ndescribe the function of individual hidden neurons inside vision networks.\nCLIP-Dissect leverages recent advances in multimodal vision/language models to\nlabel internal neurons with open-ended concepts without the need for any\nlabeled data or human examples, which are required for existing tools to\nsucceed. We show that CLIP-Dissect provides more accurate descriptions than\nexisting methods for neurons where the ground-truth is available as well as\nqualitatively good descriptions for hidden layer neurons. In addition, our\nmethod is very flexible: it is model agnostic, can easily handle new concepts\nand can be extended to take advantage of better multimodal models in the\nfuture. Finally CLIP-Dissect is computationally efficient and labels all\nneurons of a layer in a large vision model in tens of minutes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Oikarinen_T/0/1/0/all/0/1\">Tuomas Oikarinen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weng_T/0/1/0/all/0/1\">Tsui-Wei Weng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Restoration of Weather-affected Images using Deep Gaussian Process-based CycleGAN. (arXiv:2204.10970v1 [cs.CV])","link":"http://arxiv.org/abs/2204.10970","description":"<p>Existing approaches for restoring weather-degraded images follow a\nfully-supervised paradigm and they require paired data for training. However,\ncollecting paired data for weather degradations is extremely challenging, and\nexisting methods end up training on synthetic data. To overcome this issue, we\ndescribe an approach for supervising deep networks that are based on CycleGAN,\nthereby enabling the use of unlabeled real-world data for training.\nSpecifically, we introduce new losses for training CycleGAN that lead to more\neffective training, resulting in high-quality reconstructions. These new losses\nare obtained by jointly modeling the latent space embeddings of predicted clean\nimages and original clean images through Deep Gaussian Processes. This enables\nthe CycleGAN architecture to transfer the knowledge from one domain\n(weather-degraded) to another (clean) more effectively. We demonstrate that the\nproposed method can be effectively applied to different restoration tasks like\nde-raining, de-hazing and de-snowing and it outperforms other unsupervised\ntechniques (that leverage weather-based characteristics) by a considerable\nmargin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yasarla_R/0/1/0/all/0/1\">Rajeev Yasarla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sindagi_V/0/1/0/all/0/1\">Vishwanath A. Sindagi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1\">Vishal M. Patel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GRM: Gradient Rectification Module for Visual Place Retrieval. (arXiv:2204.10972v1 [cs.CV])","link":"http://arxiv.org/abs/2204.10972","description":"<p>Visual place retrieval aims to search images in the database that depict\nsimilar places as the query image. However, global descriptors encoded by the\nnetwork usually fall into a low dimensional principal space, which is harmful\nto the retrieval performance. We first analyze the cause of this phenomenon,\npointing out that it is due to degraded distribution of the gradients of\ndescriptors. Then, a new module called Gradient Rectification Module(GRM) is\nproposed to alleviate this issue. It can be appended after the final pooling\nlayer. This module can rectify the gradients to the complement space of the\nprincipal space. Therefore, the network is encouraged to generate descriptors\nmore uniformly in the whole space. At last, we conduct experiments on multiple\ndatasets and generalize our method to classification task under prototype\nlearning framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lei_B/0/1/0/all/0/1\">Boshu Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1\">Wenjie Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_L/0/1/0/all/0/1\">Limeng Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xi Qiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Recolored Image by Spatial Correlation. (arXiv:2204.10973v1 [cs.CV])","link":"http://arxiv.org/abs/2204.10973","description":"<p>Image forensics, aiming to ensure the authenticity of the image, has made\ngreat progress in dealing with common image manipulation such as copy-move,\nsplicing, and inpainting in the past decades. However, only a few researchers\npay attention to an emerging editing technique called image recoloring, which\ncan manipulate the color values of an image to give it a new style. To prevent\nit from being used maliciously, the previous approaches address the\nconventional recoloring from the perspective of inter-channel correlation and\nillumination consistency. In this paper, we try to explore a solution from the\nperspective of the spatial correlation, which exhibits the generic detection\ncapability for both conventional and deep learning-based recoloring. Through\ntheoretical and numerical analysis, we find that the recoloring operation will\ninevitably destroy the spatial correlation between pixels, implying a new prior\nof statistical discriminability. Based on such fact, we generate a set of\nspatial correlation features and learn the informative representation from the\nset via a convolutional neural network. To train our network, we use three\nrecoloring methods to generate a large-scale and high-quality data set.\nExtensive experimental results in two recoloring scenes demonstrate that the\nspatial correlation features are highly discriminative. Our method achieves the\nstate-of-the-art detection accuracy on multiple benchmark datasets and exhibits\nwell generalization for unknown types of recoloring methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yushu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_S/0/1/0/all/0/1\">Shuren Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_M/0/1/0/all/0/1\">Mingfu Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1\">Xiaochun Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Federated Contrastive Learning for Volumetric Medical Image Segmentation. (arXiv:2204.10983v1 [eess.IV])","link":"http://arxiv.org/abs/2204.10983","description":"<p>Supervised deep learning needs a large amount of labeled data to achieve high\nperformance. However, in medical imaging analysis, each site may only have a\nlimited amount of data and labels, which makes learning ineffective. Federated\nlearning (FL) can help in this regard by learning a shared model while keeping\ntraining data local for privacy. Traditional FL requires fully-labeled data for\ntraining, which is inconvenient or sometimes infeasible to obtain due to high\nlabeling cost and the requirement of expertise. Contrastive learning (CL), as a\nself-supervised learning approach, can effectively learn from unlabeled data to\npre-train a neural network encoder, followed by fine-tuning for downstream\ntasks with limited annotations. However, when adopting CL in FL, the limited\ndata diversity on each client makes federated contrastive learning (FCL)\nineffective. In this work, we propose an FCL framework for volumetric medical\nimage segmentation with limited annotations. More specifically, we exchange the\nfeatures in the FCL pre-training process such that diverse contrastive data are\nprovided to each site for effective local CL while keeping raw data private.\nBased on the exchanged features, global structural matching further leverages\nthe structural similarity to align local features to the remote ones such that\na unified feature space can be learned among different sites. Experiments on a\ncardiac MRI dataset show the proposed framework substantially improves the\nsegmentation performance compared with state-of-the-art techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wu_Y/0/1/0/all/0/1\">Yawen Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zeng_D/0/1/0/all/0/1\">Dewen Zeng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1\">Zhepeng Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shi_Y/0/1/0/all/0/1\">Yiyu Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_J/0/1/0/all/0/1\">Jingtong Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TerrainMesh: Metric-Semantic Terrain Reconstruction from Aerial Images Using Joint 2D-3D Learning. (arXiv:2204.10993v1 [cs.CV])","link":"http://arxiv.org/abs/2204.10993","description":"<p>This paper considers outdoor terrain mapping using RGB images obtained from\nan aerial vehicle. While feature-based localization and mapping techniques\ndeliver real-time vehicle odometry and sparse keypoint depth reconstruction, a\ndense model of the environment geometry and semantics (vegetation, buildings,\netc.) is usually recovered offline with significant computation and storage.\nThis paper develops a joint 2D-3D learning approach to reconstruct a local\nmetric-semantic mesh at each camera keyframe maintained by a visual odometry\nalgorithm. Given the estimated camera trajectory, the local meshes can be\nassembled into a global environment model to capture the terrain topology and\nsemantics during online operation. A local mesh is reconstructed using an\ninitialization and refinement stage. In the initialization stage, we estimate\nthe mesh vertex elevation by solving a least squares problem relating the\nvertex barycentric coordinates to the sparse keypoint depth measurements. In\nthe refinement stage, we associate 2D image and semantic features with the 3D\nmesh vertices using camera projection and apply graph convolution to refine the\nmesh vertex spatial coordinates and semantic features based on joint 2D and 3D\nsupervision. Quantitative and qualitative evaluation using real aerial images\nshow the potential of our method to support environmental monitoring and\nsurveillance applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_Q/0/1/0/all/0/1\">Qiaojun Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atanasov_N/0/1/0/all/0/1\">Nikolay Atanasov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cerebral Palsy Prediction with Frequency Attention Informed Graph Convolutional Networks. (arXiv:2204.10997v1 [cs.CV])","link":"http://arxiv.org/abs/2204.10997","description":"<p>Early diagnosis and intervention are clinically considered the paramount part\nof treating cerebral palsy (CP), so it is essential to design an efficient and\ninterpretable automatic prediction system for CP. We highlight a significant\ndifference between CP infants' frequency of human movement and that of the\nhealthy group, which improves prediction performance. However, the existing\ndeep learning-based methods did not use the frequency information of infants'\nmovement for CP prediction. This paper proposes a frequency attention informed\ngraph convolutional network and validates it on two consumer-grade RGB video\ndatasets, namely MINI-RGBD and RVI-38 datasets. Our proposed frequency\nattention module aids in improving both classification performance and system\ninterpretability. In addition, we design a frequency-binning method that\nretains the critical frequency of the human joint position data while filtering\nthe noise. Our prediction performance achieves state-of-the-art research on\nboth datasets. Our work demonstrates the effectiveness of frequency information\nin supporting the prediction of CP non-intrusively and provides a way for\nsupporting the early diagnosis of CP in the resource-limited regions where the\nclinical resources are not abundant.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haozheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shum_H/0/1/0/all/0/1\">Hubert P. H. Shum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_E/0/1/0/all/0/1\">Edmond S. L. Ho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Training and challenging models for text-guided fashion image retrieval. (arXiv:2204.11004v1 [cs.CV])","link":"http://arxiv.org/abs/2204.11004","description":"<p>Retrieving relevant images from a catalog based on a query image together\nwith a modifying caption is a challenging multimodal task that can particularly\nbenefit domains like apparel shopping, where fine details and subtle variations\nmay be best expressed through natural language. We introduce a new evaluation\ndataset, Challenging Fashion Queries (CFQ), as well as a modeling approach that\nachieves state-of-the-art performance on the existing Fashion IQ (FIQ) dataset.\nCFQ complements existing benchmarks by including relative captions with\npositive and negative labels of caption accuracy and conditional image\nsimilarity, where others provided only positive labels with a combined meaning.\nWe demonstrate the importance of multimodal pretraining for the task and show\nthat domain-specific weak supervision based on attribute labels can augment\ngeneric large-scale pretraining. While previous modality fusion mechanisms lose\nthe benefits of multimodal pretraining, we introduce a residual attention\nfusion mechanism that improves performance. We release CFQ and our code to the\nresearch community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dodds_E/0/1/0/all/0/1\">Eric Dodds</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Culpepper_J/0/1/0/all/0/1\">Jack Culpepper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_G/0/1/0/all/0/1\">Gaurav Srivastava</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Surface Reconstruction from Point Clouds by Learning Predictive Context Priors. (arXiv:2204.11015v1 [cs.CV])","link":"http://arxiv.org/abs/2204.11015","description":"<p>Surface reconstruction from point clouds is vital for 3D computer vision.\nState-of-the-art methods leverage large datasets to first learn local context\npriors that are represented as neural network-based signed distance functions\n(SDFs) with some parameters encoding the local contexts. To reconstruct a\nsurface at a specific query location at inference time, these methods then\nmatch the local reconstruction target by searching for the best match in the\nlocal prior space (by optimizing the parameters encoding the local context) at\nthe given query location. However, this requires the local context prior to\ngeneralize to a wide variety of unseen target regions, which is hard to\nachieve. To resolve this issue, we introduce Predictive Context Priors by\nlearning Predictive Queries for each specific point cloud at inference time.\nSpecifically, we first train a local context prior using a large point cloud\ndataset similar to previous techniques. For surface reconstruction at inference\ntime, however, we specialize the local context prior into our Predictive\nContext Prior by learning Predictive Queries, which predict adjusted spatial\nquery locations as displacements of the original locations. This leads to a\nglobal SDF that fits the specific point cloud the best. Intuitively, the query\nprediction enables us to flexibly search the learned local context prior over\nthe entire prior space, rather than being restricted to the fixed query\nlocations, and this improves the generalizability. Our method does not require\nground truth signed distances, normals, or any additional procedure of signed\ndistance fusion across overlapping regions. Our experimental results in surface\nreconstruction for single shapes or complex scenes show significant\nimprovements over the state-of-the-art under widely used benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_B/0/1/0/all/0/1\">Baorui Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yu-Shen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zwicker_M/0/1/0/all/0/1\">Matthias Zwicker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1\">Zhizhong Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Negatives in Contrastive Learning for Unpaired Image-to-Image Translation. (arXiv:2204.11018v1 [cs.CV])","link":"http://arxiv.org/abs/2204.11018","description":"<p>Unpaired image-to-image translation aims to find a mapping between the source\ndomain and the target domain. To alleviate the problem of the lack of\nsupervised labels for the source images, cycle-consistency based methods have\nbeen proposed for image structure preservation by assuming a reversible\nrelationship between unpaired images. However, this assumption only uses\nlimited correspondence between image pairs. Recently, contrastive learning (CL)\nhas been used to further investigate the image correspondence in unpaired image\ntranslation by using patch-based positive/negative learning. Patch-based\ncontrastive routines obtain the positives by self-similarity computation and\nrecognize the rest patches as negatives. This flexible learning paradigm\nobtains auxiliary contextualized information at a low cost. As the negatives\nown an impressive sample number, with curiosity, we make an investigation based\non a question: are all negatives necessary for feature contrastive learning?\nUnlike previous CL approaches that use negatives as much as possible, in this\npaper, we study the negatives from an information-theoretic perspective and\nintroduce a new negative Pruning technology for Unpaired image-to-image\nTranslation (PUT) by sparsifying and ranking the patches. The proposed\nalgorithm is efficient, flexible and enables the model to learn essential\ninformation between corresponding patches stably. By putting quality over\nquantity, only a few negative patches are required to achieve better results.\nLastly, we validate the superiority, stability, and versatility of our model\nthrough comparative experiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yupei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Sen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianshui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yongyi Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guangping Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yukai Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Indoor simultaneous localization and mapping based on fringe projection profilometry. (arXiv:2204.11020v1 [cs.RO])","link":"http://arxiv.org/abs/2204.11020","description":"<p>Simultaneous Localization and Mapping (SLAM) plays an important role in\noutdoor and indoor applications ranging from autonomous driving to indoor\nrobotics. Outdoor SLAM has been widely used with the assistance of LiDAR or\nGPS. For indoor applications, the LiDAR technique does not satisfy the accuracy\nrequirement and the GPS signals will be lost. An accurate and efficient scene\nsensing technique is required for indoor SLAM. As the most promising 3D sensing\ntechnique, the opportunities for indoor SLAM with fringe projection\nprofilometry (FPP) systems are obvious, but methods to date have not fully\nleveraged the accuracy and speed of sensing that such systems offer. In this\npaper, we propose a novel FPP-based indoor SLAM method based on the coordinate\ntransformation relationship of FPP, where the 2D-to-3D descriptor-assisted is\nused for mapping and localization. The correspondences generated by matching\ndescriptors are used for fast and accurate mapping, and the transform\nestimation between the 2D and 3D descriptors is used to localize the sensor.\nThe provided experimental results demonstrate that the proposed indoor SLAM can\nachieve the localization and mapping accuracy around one millimeter.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Haotian Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_D/0/1/0/all/0/1\">Dongliang Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jing Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Data-Free Model Stealing in a Hard Label Setting. (arXiv:2204.11022v1 [cs.CR])","link":"http://arxiv.org/abs/2204.11022","description":"<p>Machine learning models deployed as a service (MLaaS) are susceptible to\nmodel stealing attacks, where an adversary attempts to steal the model within a\nrestricted access framework. While existing attacks demonstrate near-perfect\nclone-model performance using softmax predictions of the classification\nnetwork, most of the APIs allow access to only the top-1 labels. In this work,\nwe show that it is indeed possible to steal Machine Learning models by\naccessing only top-1 predictions (Hard Label setting) as well, without access\nto model gradients (Black-Box setting) or even the training dataset (Data-Free\nsetting) within a low query budget. We propose a novel GAN-based framework that\ntrains the student and generator in tandem to steal the model effectively while\novercoming the challenge of the hard label setting by utilizing gradients of\nthe clone network as a proxy to the victim's gradients. We propose to overcome\nthe large query costs associated with a typical Data-Free setting by utilizing\npublicly available (potentially unrelated) datasets as a weak image prior. We\nadditionally show that even in the absence of such data, it is possible to\nachieve state-of-the-art results within a low query budget using synthetically\ncrafted samples. We are the first to demonstrate the scalability of Model\nStealing in a restricted access setting on a 100 class dataset as well.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sanyal_S/0/1/0/all/0/1\">Sunandini Sanyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Addepalli_S/0/1/0/all/0/1\">Sravanti Addepalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Babu_R/0/1/0/all/0/1\">R. Venkatesh Babu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VISTA: Vision Transformer enhanced by U-Net and Image Colorfulness Frame Filtration for Automatic Retail Checkout. (arXiv:2204.11024v1 [cs.CV])","link":"http://arxiv.org/abs/2204.11024","description":"<p>Multi-class product counting and recognition identifies product items from\nimages or videos for automated retail checkout. The task is challenging due to\nthe real-world scenario of occlusions where product items overlap, fast\nmovement in the conveyor belt, large similarity in overall appearance of the\nitems being scanned, novel products, and the negative impact of misidentifying\nitems. Further, there is a domain bias between training and test sets,\nspecifically, the provided training dataset consists of synthetic images and\nthe test set videos consist of foreign objects such as hands and tray. To\naddress these aforementioned issues, we propose to segment and classify\nindividual frames from a video sequence. The segmentation method consists of a\nunified single product item- and hand-segmentation followed by entropy masking\nto address the domain bias problem. The multi-class classification method is\nbased on Vision Transformers (ViT). To identify the frames with target objects,\nwe utilize several image processing methods and propose a custom metric to\ndiscard frames not having any product items. Combining all these mechanisms,\nour best system achieves 3rd place in the AI City Challenge 2022 Track 4 with\nan F1 score of 0.4545. Code will be available at\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shihab_M/0/1/0/all/0/1\">Md. Istiak Hossain Shihab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tasnim_N/0/1/0/all/0/1\">Nazia Tasnim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zunair_H/0/1/0/all/0/1\">Hasib Zunair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rupty_L/0/1/0/all/0/1\">Labiba Kanij Rupty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammed_N/0/1/0/all/0/1\">Nabeel Mohammed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning by Erasing: Conditional Entropy based Transferable Out-Of-Distribution Detection. (arXiv:2204.11041v1 [cs.CV])","link":"http://arxiv.org/abs/2204.11041","description":"<p>Out-of-distribution (OOD) detection is essential to handle the distribution\nshifts between training and test scenarios. For a new in-distribution (ID)\ndataset, existing methods require retraining to capture the dataset-specific\nfeature representation or data distribution. In this paper, we propose a deep\ngenerative models (DGM) based transferable OOD detection method, which is\nunnecessary to retrain on a new ID dataset. We design an image erasing strategy\nto equip exclusive conditional entropy distribution for each ID dataset, which\ndetermines the discrepancy of DGM's posteriori ucertainty distribution on\ndifferent ID datasets. Owing to the powerful representation capacity of\nconvolutional neural networks, the proposed model trained on complex dataset\ncan capture the above discrepancy between ID datasets without retraining and\nthus achieve transferable OOD detection. We validate the proposed method on\nfive datasets and verity that ours achieves comparable performance to the\nstate-of-the-art group based OOD detection methods that need to be retrained to\ndeploy on new ID datasets. Our code is available at\nhttps://github.com/oOHCIOo/CETOOD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xing_M/0/1/0/all/0/1\">Meng Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zhiyong Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yong Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_C/0/1/0/all/0/1\">Changjae Oh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating Neural Architectures by Synthetic Dataset Design. (arXiv:2204.11045v1 [cs.CV])","link":"http://arxiv.org/abs/2204.11045","description":"<p>Recent years have seen the emergence of many new neural network structures\n(architectures and layers). To solve a given task, a network requires a certain\nset of abilities reflected in its structure. The required abilities depend on\neach task. There is so far no systematic study of the real capacities of the\nproposed neural structures. The question of what each structure can and cannot\nachieve is only partially answered by its performance on common benchmarks.\nIndeed, natural data contain complex unknown statistical cues. It is therefore\nimpossible to know what cues a given neural structure is taking advantage of in\nsuch data. In this work, we sketch a methodology to measure the effect of each\nstructure on a network's ability, by designing ad hoc synthetic datasets. Each\ndataset is tailored to assess a given ability and is reduced to its simplest\nform: each input contains exactly the amount of information needed to solve the\ntask. We illustrate our methodology by building three datasets to evaluate each\nof the three following network properties: a) the ability to link local cues to\ndistant inferences, b) the translation covariance and c) the ability to group\npixels with the same characteristics and share information among them. Using a\nfirst simplified depth estimation dataset, we pinpoint a serious nonlocal\ndeficit of the U-Net. We then evaluate how to resolve this limitation by\nembedding its structure with nonlocal layers, which allow computing complex\nfeatures with long-range dependencies. Using a second dataset, we compare\ndifferent positional encoding methods and use the results to further improve\nthe U-Net on the depth estimation task. The third introduced dataset serves to\ndemonstrate the need for self-attention-like mechanisms for resolving more\nrealistic depth estimation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Courtois_A/0/1/0/all/0/1\">Adrien Courtois</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morel_J/0/1/0/all/0/1\">Jean-Michel Morel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arias_P/0/1/0/all/0/1\">Pablo Arias</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Class Balanced PixelNet for Neurological Image Segmentation. (arXiv:2204.11048v1 [eess.IV])","link":"http://arxiv.org/abs/2204.11048","description":"<p>In this paper, we propose an automatic brain tumor segmentation approach\n(e.g., PixelNet) using a pixel-level convolutional neural network (CNN). The\nmodel extracts feature from multiple convolutional layers and concatenate them\nto form a hyper-column where samples a modest number of pixels for\noptimization. Hyper-column ensures both local and global contextual information\nfor pixel-wise predictors. The model confirms the statistical efficiency by\nsampling a few pixels in the training phase where spatial redundancy limits the\ninformation learning among the neighboring pixels in conventional pixel-level\nsemantic segmentation approaches. Besides, label skewness in training data\nleads the convolutional model often converge to certain classes which is a\ncommon problem in the medical dataset. We deal with this problem by selecting\nan equal number of pixels for all the classes in sampling time. The proposed\nmodel has achieved promising results in brain tumor and ischemic stroke lesion\nsegmentation datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Islam_M/0/1/0/all/0/1\">Mobarakol Islam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ren_H/0/1/0/all/0/1\">Hongliang Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncertain Label Correction via Auxiliary Action Unit Graphs for Facial Expression Recognition. (arXiv:2204.11053v1 [cs.CV])","link":"http://arxiv.org/abs/2204.11053","description":"<p>High-quality annotated images are significant to deep facial expression\nrecognition (FER) methods. However, uncertain labels, mostly existing in\nlarge-scale public datasets, often mislead the training process. In this paper,\nwe achieve uncertain label correction of facial expressions using auxiliary\naction unit (AU) graphs, called ULC-AG. Specifically, a weighted regularization\nmodule is introduced to highlight valid samples and suppress category imbalance\nin every batch. Based on the latent dependency between emotions and AUs, an\nauxiliary branch using graph convolutional layers is added to extract the\nsemantic information from graph topologies. Finally, a re-labeling strategy\ncorrects the ambiguous annotations by comparing their feature similarities with\nsemantic templates. Experiments show that our ULC-AG achieves 89.31% and 61.57%\naccuracy on RAF-DB and AffectNet datasets, respectively, outperforming the\nbaseline and state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xingming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kauttonen_J/0/1/0/all/0/1\">Janne Kauttonen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1\">Guoying Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MLP-Hash: Protecting Face Templates via Hashing of Randomized Multi-Layer Perceptron. (arXiv:2204.11054v1 [cs.CV])","link":"http://arxiv.org/abs/2204.11054","description":"<p>Applications of face recognition systems for authentication purposes are\ngrowing rapidly. Although state-of-the-art (SOTA) face recognition systems have\nhigh recognition performance, the features which are extracted for each user\nand are stored in the system's database contain privacy-sensitive information.\nAccordingly, compromising this data would jeopardize users' privacy. In this\npaper, we propose a new cancelable template protection method, dubbed MLP-hash,\nwhich generates protected templates by passing the extracted features through a\nuser-specific randomly-weighted multi-layer perceptron (MLP) and binarizing the\nMLP output. We evaluated the unlinkability, irreversibility, and recognition\nperformance of our proposed biometric template protection method to fulfill the\nISO/IEC 30136 standard requirements. Our experiments with SOTA face recognition\nsystems on the MOBIO and LFW datasets show that our method has competitive\nperformance with the BioHashing and IoM Hashing (IoM-GRP and IoM-URP) template\nprotection algorithms. We provide an open-source implementation of all the\nexperiments presented in this paper so that other researchers can verify our\nfindings and build upon our work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shahreza_H/0/1/0/all/0/1\">Hatef Otroshi Shahreza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hahn_V/0/1/0/all/0/1\">Vedrana Krivoku&#x107;a Hahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marcel_S/0/1/0/all/0/1\">S&#xe9;bastien Marcel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformation Invariant Cancerous Tissue Classification Using Spatially Transformed DenseNet. (arXiv:2204.11066v1 [eess.IV])","link":"http://arxiv.org/abs/2204.11066","description":"<p>In this work, we introduce a spatially transformed DenseNet architecture for\ntransformation invariant classification of cancer tissue. Our architecture\nincreases the accuracy of the base DenseNet architecture while adding the\nability to operate in a transformation invariant way while simultaneously being\nsimpler than other models that try to provide some form of invariance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Mahdi_O/0/1/0/all/0/1\">Omar Mahdi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nassif_A/0/1/0/all/0/1\">Ali Bou Nassif</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Shape Priors by Pairwise Comparison for Robust Semantic Segmentation. (arXiv:2204.11090v1 [cs.CV])","link":"http://arxiv.org/abs/2204.11090","description":"<p>Semantic segmentation is important in medical image analysis. Inspired by the\nstrong ability of traditional image analysis techniques in capturing shape\npriors and inter-subject similarity, many deep learning (DL) models have been\nrecently proposed to exploit such prior information and achieved robust\nperformance. However, these two types of important prior information are\nusually studied separately in existing models. In this paper, we propose a\nnovel DL model to model both type of priors within a single framework.\nSpecifically, we introduce an extra encoder into the classic encoder-decoder\nstructure to form a Siamese structure for the encoders, where one of them takes\na target image as input (the image-encoder), and the other concatenates a\ntemplate image and its foreground regions as input (the template-encoder). The\ntemplate-encoder encodes the shape priors and appearance characteristics of\neach foreground class in the template image. A cosine similarity based\nattention module is proposed to fuse the information from both encoders, to\nutilize both types of prior information encoded by the template-encoder and\nmodel the inter-subject similarity for each foreground class. Extensive\nexperiments on two public datasets demonstrate that our proposed method can\nproduce superior performance to competing methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1\">Cong Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hualuo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_S/0/1/0/all/0/1\">Shilei Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_D/0/1/0/all/0/1\">Dong Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Kai Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liansheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yefeng Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can domain adaptation make object recognition work for everyone?. (arXiv:2204.11122v1 [cs.CV])","link":"http://arxiv.org/abs/2204.11122","description":"<p>Despite the rapid progress in deep visual recognition, modern computer vision\ndatasets significantly overrepresent the developed world and models trained on\nsuch datasets underperform on images from unseen geographies. We investigate\nthe effectiveness of unsupervised domain adaptation (UDA) of such models across\ngeographies at closing this performance gap. To do so, we first curate two\nshifts from existing datasets to study the Geographical DA problem, and\ndiscover new challenges beyond data distribution shift: context shift, wherein\nobject surroundings may change significantly across geographies, and\nsubpopulation shift, wherein the intra-category distributions may shift. We\ndemonstrate the inefficacy of standard DA methods at Geographical DA,\nhighlighting the need for specialized geographical adaptation solutions to\naddress the challenge of making object recognition work for everyone.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Prabhu_V/0/1/0/all/0/1\">Viraj Prabhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Selvaraju_R/0/1/0/all/0/1\">Ramprasaath R. Selvaraju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoffman_J/0/1/0/all/0/1\">Judy Hoffman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naik_N/0/1/0/all/0/1\">Nikhil Naik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Bundle Adjustment for Satellite Imaging via Quantum Machine Learning. (arXiv:2204.11133v1 [quant-ph])","link":"http://arxiv.org/abs/2204.11133","description":"<p>Given is a set of images, where all images show views of the same area at\ndifferent points in time and from different viewpoints. The task is the\nalignment of all images such that relevant information, e.g., poses, changes,\nand terrain, can be extracted from the fused image. In this work, we focus on\nquantum methods for keypoint extraction and feature matching, due to the\ndemanding computational complexity of these sub-tasks. To this end, k-medoids\nclustering, kernel density clustering, nearest neighbor search, and kernel\nmethods are investigated and it is explained how these methods can be\nre-formulated for quantum annealers and gate-based quantum computers.\nExperimental results obtained on digital quantum emulation hardware, quantum\nannealers, and quantum gate computers show that classical systems still deliver\nsuperior results. However, the proposed methods are ready for the current and\nupcoming generations of quantum computing devices which have the potential to\noutperform classical systems in the near future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/quant-ph/1/au:+Piatkowski_N/0/1/0/all/0/1\">Nico Piatkowski</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Gerlach_T/0/1/0/all/0/1\">Thore Gerlach</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Hugues_R/0/1/0/all/0/1\">Romain Hugues</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Sifa_R/0/1/0/all/0/1\">Rafet Sifa</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Bauckhage_C/0/1/0/all/0/1\">Christian Bauckhage</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Barbaresco_F/0/1/0/all/0/1\">Frederic Barbaresco</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Supplementing Missing Visions via Dialog for Scene Graph Generations. (arXiv:2204.11143v1 [cs.CV])","link":"http://arxiv.org/abs/2204.11143","description":"<p>Most current AI systems rely on the premise that the input visual data are\nsufficient to achieve competitive performance in various computer vision tasks.\nHowever, the classic task setup rarely considers the challenging, yet common\npractical situations where the complete visual data may be inaccessible due to\nvarious reasons (e.g., restricted view range and occlusions). To this end, we\ninvestigate a computer vision task setting with incomplete visual input data.\nSpecifically, we exploit the Scene Graph Generation (SGG) task with various\nlevels of visual data missingness as input. While insufficient visual input\nintuitively leads to performance drop, we propose to supplement the missing\nvisions via the natural language dialog interactions to better accomplish the\ntask objective. We design a model-agnostic Supplementary Interactive Dialog\n(SI-Dial) framework that can be jointly learned with most existing models,\nendowing the current AI systems with the ability of question-answer\ninteractions in natural language. We demonstrate the feasibility of such a task\nsetting with missing visual input and the effectiveness of our proposed dialog\nmodule as the supplementary information source through extensive experiments\nand analysis, by achieving promising performance improvement over multiple\nbaselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Ye Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaoguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_Y/0/1/0/all/0/1\">Yuzhang Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhenghao Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yan Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gabor is Enough: Interpretable Deep Denoising with a Gabor Synthesis Dictionary Prior. (arXiv:2204.11146v1 [eess.IV])","link":"http://arxiv.org/abs/2204.11146","description":"<p>Image processing neural networks, natural and artificial, have a long history\nwith orientation-selectivity, often described mathematically as Gabor filters.\nGabor-like filters have been observed in the early layers of CNN classifiers\nand even throughout low-level image processing networks. In this work, we take\nthis observation to the extreme and explicitly constrain the filters of a\nnatural-image denoising CNN to be learned 2D real Gabor filters. Surprisingly,\nwe find that the proposed network (GDLNet) can achieve near state-of-the-art\ndenoising performance amongst popular fully convolutional neural networks, with\nonly a fraction of the learned parameters. We further verify that this\nparameterization maintains the noise-level generalization (training vs.\ninference mismatch) characteristics of the base network, and investigate the\ncontribution of individual Gabor filter parameters to the performance of the\ndenoiser. We present positive findings for the interpretation of dictionary\nlearning networks as performing accelerated sparse-coding via the importance of\nuntied learned scale parameters between network layers. Our network's success\nsuggests that representations used by low-level image processing CNNs can be as\nsimple and interpretable as Gabor filterbanks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Janjusevic_N/0/1/0/all/0/1\">Nikola Janju&#x161;evi&#x107;</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khalilian_Gourtani_A/0/1/0/all/0/1\">Amirhossein Khalilian-Gourtani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yao Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Unsupervised Industrial Anomaly Detection Algorithms. (arXiv:2204.11161v1 [cs.CV])","link":"http://arxiv.org/abs/2204.11161","description":"<p>Anomaly defect detection has become an indispensable part of industrial\nproduction process. In previous study, a large part of the traditional anomaly\ndetection algorithms belong to the category of supervised learning, while the\nunsupervised situation is more common for most practical application scenarios.\nHence gradually unsupervised anomaly detection has been the subject of much\nresearch over the last few years. In this survey, we provide a comprehensive\nintroduction to newly proposed approaches for visual anomaly detection. We hope\nthat it can help the research community as well as the industry field to build\na broader and cross-domain perspective.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Yajie Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhaoxiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_S/0/1/0/all/0/1\">Shiguo Lian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RelViT: Concept-guided Vision Transformer for Visual Relational Reasoning. (arXiv:2204.11167v1 [cs.CV])","link":"http://arxiv.org/abs/2204.11167","description":"<p>Reasoning about visual relationships is central to how humans interpret the\nvisual world. This task remains challenging for current deep learning\nalgorithms since it requires addressing three key technical problems jointly:\n1) identifying object entities and their properties, 2) inferring semantic\nrelations between pairs of entities, and 3) generalizing to novel\nobject-relation combinations, i.e., systematic generalization. In this work, we\nuse vision transformers (ViTs) as our base model for visual reasoning and make\nbetter use of concepts defined as object entities and their relations to\nimprove the reasoning ability of ViTs. Specifically, we introduce a novel\nconcept-feature dictionary to allow flexible image feature retrieval at\ntraining time with concept keys. This dictionary enables two new concept-guided\nauxiliary tasks: 1) a global task for promoting relational reasoning, and 2) a\nlocal task for facilitating semantic object-centric correspondence learning. To\nexamine the systematic generalization of visual reasoning models, we introduce\nsystematic splits for the standard HICO and GQA benchmarks. We show the\nresulting model, Concept-guided Vision Transformer (or RelViT for short)\nsignificantly outperforms prior approaches on HICO and GQA by 16% and 13% in\nthe original split, and by 43% and 18% in the systematic split. Our ablation\nanalyses also reveal our model's compatibility with multiple ViT variants and\nrobustness to hyper-parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaojian Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_W/0/1/0/all/0/1\">Weili Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhiding Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Huaizu Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chaowei Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yuke Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Song-Chun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1\">Anima Anandkumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Realistic Evaluation of Transductive Few-Shot Learning. (arXiv:2204.11181v1 [cs.LG])","link":"http://arxiv.org/abs/2204.11181","description":"<p>Transductive inference is widely used in few-shot learning, as it leverages\nthe statistics of the unlabeled query set of a few-shot task, typically\nyielding substantially better performances than its inductive counterpart. The\ncurrent few-shot benchmarks use perfectly class-balanced tasks at inference. We\nargue that such an artificial regularity is unrealistic, as it assumes that the\nmarginal label probability of the testing samples is known and fixed to the\nuniform distribution. In fact, in realistic scenarios, the unlabeled query sets\ncome with arbitrary and unknown label marginals. We introduce and study the\neffect of arbitrary class distributions within the query sets of few-shot tasks\nat inference, removing the class-balance artefact. Specifically, we model the\nmarginal probabilities of the classes as Dirichlet-distributed random\nvariables, which yields a principled and realistic sampling within the simplex.\nThis leverages the current few-shot benchmarks, building testing tasks with\narbitrary class distributions. We evaluate experimentally state-of-the-art\ntransductive methods over 3 widely used data sets, and observe, surprisingly,\nsubstantial performance drops, even below inductive methods in some cases.\nFurthermore, we propose a generalization of the mutual-information loss, based\non $\\alpha$-divergences, which can handle effectively class-distribution\nvariations. Empirically, we show that our transductive $\\alpha$-divergence\noptimization outperforms state-of-the-art methods across several data sets,\nmodels and few-shot settings. Our code is publicly available at\nhttps://github.com/oveilleux/Realistic_Transductive_Few_Shot.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Veilleux_O/0/1/0/all/0/1\">Olivier Veilleux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boudiaf_M/0/1/0/all/0/1\">Malik Boudiaf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piantanida_P/0/1/0/all/0/1\">Pablo Piantanida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayed_I/0/1/0/all/0/1\">Ismail Ben Ayed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MVP-Human Dataset for 3D Human Avatar Reconstruction from Unconstrained Frames. (arXiv:2204.11184v1 [cs.CV])","link":"http://arxiv.org/abs/2204.11184","description":"<p>In this paper, we consider a novel problem of reconstructing a 3D human\navatar from multiple unconstrained frames, independent of assumptions on camera\ncalibration, capture space, and constrained actions. The problem should be\naddressed by a framework that takes multiple unconstrained images as inputs,\nand generates a shape-with-skinning avatar in the canonical space, finished in\none feed-forward pass. To this end, we present 3D Avatar Reconstruction in the\nwild (ARwild), which first reconstructs the implicit skinning fields in a\nmulti-level manner, by which the image features from multiple images are\naligned and integrated to estimate a pixel-aligned implicit function that\nrepresents the clothed shape. To enable the training and testing of the new\nframework, we contribute a large-scale dataset, MVP-Human (Multi-View and\nmulti-Pose 3D Human), which contains 400 subjects, each of which has 15 scans\nin different poses and 8-view images for each pose, providing 6,000 3D scans\nand 48,000 images in total. Overall, benefits from the specific network\narchitecture and the diverse data, the trained model enables 3D avatar\nreconstruction from unconstrained frames and achieves state-of-the-art\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiangyu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_T/0/1/0/all/0/1\">Tingting Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_J/0/1/0/all/0/1\">Jiangjing Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1\">Xiang Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunfeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_K/0/1/0/all/0/1\">Kan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Q/0/1/0/all/0/1\">Qiong Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Stan Z. Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_Z/0/1/0/all/0/1\">Zhen Lei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PUERT: Probabilistic Under-sampling and Explicable Reconstruction Network for CS-MRI. (arXiv:2204.11189v1 [eess.IV])","link":"http://arxiv.org/abs/2204.11189","description":"<p>Compressed Sensing MRI (CS-MRI) aims at reconstructing de-aliased images from\nsub-Nyquist sampling k-space data to accelerate MR Imaging, thus presenting two\nbasic issues, i.e., where to sample and how to reconstruct. To deal with both\nproblems simultaneously, we propose a novel end-to-end Probabilistic\nUnder-sampling and Explicable Reconstruction neTwork, dubbed PUERT, to jointly\noptimize the sampling pattern and the reconstruction network. Instead of\nlearning a deterministic mask, the proposed sampling subnet explores an optimal\nprobabilistic sub-sampling pattern, which describes independent Bernoulli\nrandom variables at each possible sampling point, thus retaining robustness and\nstochastics for a more reliable CS reconstruction. A dynamic gradient\nestimation strategy is further introduced to gradually approximate the\nbinarization function in backward propagation, which efficiently preserves the\ngradient information and further improves the reconstruction quality. Moreover,\nin our reconstruction subnet, we adopt a model-based network design scheme with\nhigh efficiency and interpretability, which is shown to assist in further\nexploitation for the sampling subnet. Extensive experiments on two widely used\nMRI datasets demonstrate that our proposed PUERT not only achieves\nstate-of-the-art results in terms of both quantitative metrics and visual\nquality but also yields a sub-sampling pattern and a reconstruction model that\nare both customized to training data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Xie_J/0/1/0/all/0/1\">Jingfen Xie</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1\">Jian Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongbing Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ji_X/0/1/0/all/0/1\">Xiangyang Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"2D LiDAR and Camera Fusion Using Motion Cues for Indoor Layout Estimation. (arXiv:2204.11202v1 [cs.CV])","link":"http://arxiv.org/abs/2204.11202","description":"<p>This paper presents a novel indoor layout estimation system based on the\nfusion of 2D LiDAR and intensity camera data. A ground robot explores an indoor\nspace with a single floor and vertical walls, and collects a sequence of\nintensity images and 2D LiDAR datasets. The LiDAR provides accurate depth\ninformation, while the camera captures high-resolution data for semantic\ninterpretation. The alignment of sensor outputs and image segmentation are\ncomputed jointly by aligning LiDAR points, as samples of the room contour, to\nground-wall boundaries in the images. The alignment problem is decoupled into a\ntop-down view projection and a 2D similarity transformation estimation, which\ncan be solved according to the vertical vanishing point and motion of two\nsensors. The recursive random sample consensus algorithm is implemented to\ngenerate, evaluate and optimize multiple hypotheses with the sequential\nmeasurements. The system allows jointly analyzing the geometric interpretation\nfrom different sensors without offline calibration. The ambiguity in images for\nground-wall boundary extraction is removed with the assistance of LiDAR\nobservations, which improves the accuracy of semantic segmentation. The\nlocalization and mapping is refined using the fused data, which enables the\nsystem to work reliably in scenes with low texture or low geometric features.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jieyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stevenson_R/0/1/0/all/0/1\">Robert Stevenson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Progressive Learning for Image Retrieval with Hybrid-Modality Queries. (arXiv:2204.11212v1 [cs.CV])","link":"http://arxiv.org/abs/2204.11212","description":"<p>Image retrieval with hybrid-modality queries, also known as composing text\nand image for image retrieval (CTI-IR), is a retrieval task where the search\nintention is expressed in a more complex query format, involving both vision\nand text modalities. For example, a target product image is searched using a\nreference product image along with text about changing certain attributes of\nthe reference image as the query. It is a more challenging image retrieval task\nthat requires both semantic space learning and cross-modal fusion. Previous\napproaches that attempt to deal with both aspects achieve unsatisfactory\nperformance. In this paper, we decompose the CTI-IR task into a three-stage\nlearning problem to progressively learn the complex knowledge for image\nretrieval with hybrid-modality queries. We first leverage the semantic\nembedding space for open-domain image-text retrieval, and then transfer the\nlearned knowledge to the fashion-domain with fashion-related pre-training\ntasks. Finally, we enhance the pre-trained model from single-query to\nhybrid-modality query for the CTI-IR task. Furthermore, as the contribution of\nindividual modality in the hybrid-modality query varies for different retrieval\nscenarios, we propose a self-supervised adaptive weighting strategy to\ndynamically determine the importance of image and text in the hybrid-modality\nquery for better retrieval. Extensive experiments show that our proposed model\nsignificantly outperforms state-of-the-art methods in the mean of Recall@K by\n24.9% and 9.5% on the Fashion-IQ and Shoes benchmark datasets respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yida Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yuqing Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1\">Qin Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RealNet: Combining Optimized Object Detection with Information Fusion Depth Estimation Co-Design Method on IoT. (arXiv:2204.11216v1 [cs.CV])","link":"http://arxiv.org/abs/2204.11216","description":"<p>Depth Estimation and Object Detection Recognition play an important role in\nautonomous driving technology under the guidance of deep learning artificial\nintelligence. We propose a hybrid structure called RealNet: a co-design method\ncombining the model-streamlined recognition algorithm, the depth estimation\nalgorithm with information fusion, and deploying them on the Jetson-Nano for\nunmanned vehicles with monocular vision sensors. We use ROS for experiment. The\nmethod proposed in this paper is suitable for mobile platforms with high\nreal-time request. Innovation of our method is using information fusion to\ncompensate the problem of insufficient frame rate of output image, and improve\nthe robustness of target detection and depth estimation under monocular\nvision.Object Detection is based on YOLO-v5. We have simplified the network\nstructure of its DarkNet53 and realized a prediction speed up to 0.01s. Depth\nEstimation is based on the VNL Depth Estimation, which considers multiple\ngeometric constraints in 3D global space. It calculates the loss function by\ncalculating the deviation of the virtual normal vector VN and the label, which\ncan obtain deeper depth information. We use PnP fusion algorithm to solve the\nproblem of insufficient frame rate of depth map output. It solves the motion\nestimation depth from three-dimensional target to two-dimensional point based\non corner feature matching, which is faster than VNL calculation. We\ninterpolate VNL output and PnP output to achieve information fusion.\nExperiments show that this can effectively eliminate the jitter of depth\ninformation and improve robustness. At the control end, this method combines\nthe results of target detection and depth estimation to calculate the target\nposition, and uses a pure tracking control algorithm to track it.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhuohao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gou_F/0/1/0/all/0/1\">Fandi Gou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+De_Q/0/1/0/all/0/1\">Qixin De</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1\">Leqi Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuanhang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1\">Yunze Cai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lesion Localization in OCT by Semi-Supervised Object Detection. (arXiv:2204.11227v1 [cs.CV])","link":"http://arxiv.org/abs/2204.11227","description":"<p>Over 300 million people worldwide are affected by various retinal diseases.\nBy noninvasive Optical Coherence Tomography (OCT) scans, a number of abnormal\nstructural changes in the retina, namely retinal lesions, can be identified.\nAutomated lesion localization in OCT is thus important for detecting retinal\ndiseases at their early stage. To conquer the lack of manual annotation for\ndeep supervised learning, this paper presents a first study on utilizing\nsemi-supervised object detection (SSOD) for lesion localization in OCT images.\nTo that end, we develop a taxonomy to provide a unified and structured\nviewpoint of the current SSOD methods, and consequently identify key modules in\nthese methods. To evaluate the influence of these modules in the new task, we\nbuild OCT-SS, a new dataset consisting of over 1k expert-labeled OCT B-scan\nimages and over 13k unlabeled B-scans. Extensive experiments on OCT-SS identify\nUnbiased Teacher (UnT) as the best current SSOD method for lesion localization.\nMoreover, we improve over this strong baseline, with mAP increased from 49.34\nto 50.86.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yue Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jianchun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jingyuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Weihong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Youxin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xirong Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Source-Free Domain Adaptation via Distribution Estimation. (arXiv:2204.11257v1 [cs.CV])","link":"http://arxiv.org/abs/2204.11257","description":"<p>Domain Adaptation aims to transfer the knowledge learned from a labeled\nsource domain to an unlabeled target domain whose data distributions are\ndifferent. However, the training data in source domain required by most of the\nexisting methods is usually unavailable in real-world applications due to\nprivacy preserving policies. Recently, Source-Free Domain Adaptation (SFDA) has\ndrawn much attention, which tries to tackle domain adaptation problem without\nusing source data. In this work, we propose a novel framework called SFDA-DE to\naddress SFDA task via source Distribution Estimation. Firstly, we produce\nrobust pseudo-labels for target data with spherical k-means clustering, whose\ninitial class centers are the weight vectors (anchors) learned by the\nclassifier of pretrained model. Furthermore, we propose to estimate the\nclass-conditioned feature distribution of source domain by exploiting target\ndata and corresponding anchors. Finally, we sample surrogate features from the\nestimated distribution, which are then utilized to align two domains by\nminimizing a contrastive adaptation loss function. Extensive experiments show\nthat the proposed method achieves state-of-the-art performance on multiple DA\nbenchmarks, and even outperforms traditional DA methods which require plenty of\nsource data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_N/0/1/0/all/0/1\">Ning Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yixing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yehui Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunhe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RMGN: A Regional Mask Guided Network for Parser-free Virtual Try-on. (arXiv:2204.11258v1 [cs.CV])","link":"http://arxiv.org/abs/2204.11258","description":"<p>Virtual try-on(VTON) aims at fitting target clothes to reference person\nimages, which is widely adopted in e-commerce.Existing VTON approaches can be\nnarrowly categorized into Parser-Based(PB) and Parser-Free(PF) by whether\nrelying on the parser information to mask the persons' clothes and synthesize\ntry-on images. Although abandoning parser information has improved the\napplicability of PF methods, the ability of detail synthesizing has also been\nsacrificed. As a result, the distraction from original cloth may persistin\nsynthesized images, especially in complicated postures and high resolution\napplications. To address the aforementioned issue, we propose a novel PF method\nnamed Regional Mask Guided Network(RMGN). More specifically, a regional mask is\nproposed to explicitly fuse the features of target clothes and reference\npersons so that the persisted distraction can be eliminated. A posture\nawareness loss and a multi-level feature extractor are further proposed to\nhandle the complicated postures and synthesize high resolution images.\nExtensive experiments demonstrate that our proposed RMGN outperforms both\nstate-of-the-art PB and PF methods.Ablation studies further verify the\neffectiveness ofmodules in RMGN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Sheng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shichang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jialun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_L/0/1/0/all/0/1\">Linhao Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiarun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Longtao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yuan He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards the Semantic Weak Generalization Problem in Generative Zero-Shot Learning: Ante-hoc and Post-hoc. (arXiv:2204.11280v1 [cs.CV])","link":"http://arxiv.org/abs/2204.11280","description":"<p>In this paper, we present a simple and effective strategy lowering the\npreviously unexplored factors that limit the performance ceiling of generative\nZero-Shot Learning (ZSL). We begin by formally defining semantic\ngeneralization, then look into approaches for reducing the semantic weak\ngeneralization problem and minimizing its negative influence on classifier\ntraining. In the ante-hoc phase, we augment the generator's semantic input, as\nwell as relax the fitting target of the generator. In the post-hoc phase (after\ngenerating simulated unseen samples), we derive from the gradient of the loss\nfunction to minimize the gradient increment on seen classifier weights carried\nby biased unseen distribution, which tends to cause misleading on intra-seen\nclass decision boundaries. Without complicated designs, our approach hit the\nessential problem and significantly outperform the state-of-the-art on four\nwidely used ZSL datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dubing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yuming Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haofeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip H.S. Torr</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Scale Time-Series Representation Learning via Simultaneous Low and High Frequency Feature Bootstrapping. (arXiv:2204.11291v1 [cs.CV])","link":"http://arxiv.org/abs/2204.11291","description":"<p>Learning representation from unlabeled time series data is a challenging\nproblem. Most existing self-supervised and unsupervised approaches in the\ntime-series domain do not capture low and high-frequency features at the same\ntime. Further, some of these methods employ large scale models like\ntransformers or rely on computationally expensive techniques such as\ncontrastive learning. To tackle these problems, we propose a non-contrastive\nself-supervised learning approach efficiently captures low and high-frequency\ntime-varying features in a cost-effective manner. Our method takes raw time\nseries data as input and creates two different augmented views for two branches\nof the model, by randomly sampling the augmentations from same family.\nFollowing the terminology of BYOL, the two branches are called online and\ntarget network which allows bootstrapping of the latent representation. In\ncontrast to BYOL, where a backbone encoder is followed by multilayer perceptron\n(MLP) heads, the proposed model contains additional temporal convolutional\nnetwork (TCN) heads. As the augmented views are passed through large kernel\nconvolution blocks of the encoder, the subsequent combination of MLP and TCN\nenables an effective representation of low as well as high-frequency\ntime-varying features due to the varying receptive fields. The two modules (MLP\nand TCN) act in a complementary manner. We train an online network where each\nmodule learns to predict the outcome of the respective module of target network\nbranch. To demonstrate the robustness of our model we performed extensive\nexperiments and ablation studies on five real-world time-series datasets. Our\nmethod achieved state-of-art performance on all five real-world datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gorade_V/0/1/0/all/0/1\">Vandan Gorade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Azad Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_D/0/1/0/all/0/1\">Deepak Mishra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Colorectal cancer survival prediction using deep distribution based multiple-instance learning. (arXiv:2204.11294v1 [eess.IV])","link":"http://arxiv.org/abs/2204.11294","description":"<p>Several deep learning algorithms have been developed to predict survival of\ncancer patients using whole slide images (WSIs).However, identification of\nimage phenotypes within the WSIs that are relevant to patient survival and\ndisease progression is difficult for both clinicians, and deep learning\nalgorithms. Most deep learning based Multiple Instance Learning (MIL)\nalgorithms for survival prediction use either top instances (e.g., maxpooling)\nor top/bottom instances (e.g., MesoNet) to identify image phenotypes. In this\nstudy, we hypothesize that wholistic information of the distribution of the\npatch scores within a WSI can predict the cancer survival better. We developed\na distribution based multiple-instance survival learning algorithm\n(DeepDisMISL) to validate this hypothesis. We designed and executed experiments\nusing two large international colorectal cancer WSIs datasets - MCO CRC and\nTCGA COAD-READ. Our results suggest that the more information about the\ndistribution of the patch scores for a WSI, the better is the prediction\nperformance. Including multiple neighborhood instances around each selected\ndistribution location (e.g., percentiles) could further improve the prediction.\nDeepDisMISL demonstrated superior predictive ability compared to other recently\npublished, state-of-the-art algorithms. Furthermore, our algorithm is\ninterpretable and could assist in understanding the relationship between cancer\nmorphological phenotypes and patients cancer survival risk.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1\">Xingyu Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jonnagaddala_J/0/1/0/all/0/1\">Jitendra Jonnagaddala</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cen_M/0/1/0/all/0/1\">Min Cen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1\">Hong Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_X/0/1/0/all/0/1\">Xu Steven Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dictionary Attacks on Speaker Verification. (arXiv:2204.11304v1 [cs.SD])","link":"http://arxiv.org/abs/2204.11304","description":"<p>In this paper, we propose dictionary attacks against speaker verification - a\nnovel attack vector that aims to match a large fraction of speaker population\nby chance. We introduce a generic formulation of the attack that can be used\nwith various speech representations and threat models. The attacker uses\nadversarial optimization to maximize raw similarity of speaker embeddings\nbetween a seed speech sample and a proxy population. The resulting master voice\nsuccessfully matches a non-trivial fraction of people in an unknown population.\nAdversarial waveforms obtained with our approach can match on average 69% of\nfemales and 38% of males enrolled in the target system at a strict decision\nthreshold calibrated to yield false alarm rate of 1%. By using the attack with\na black-box voice cloning system, we obtain master voices that are effective in\nthe most challenging conditions and transferable between speaker encoders. We\nalso show that, combined with multiple attempts, this attack opens even more to\nserious issues on the security of these systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Marras_M/0/1/0/all/0/1\">Mirko Marras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korus_P/0/1/0/all/0/1\">Pawel Korus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Anubhav Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Memon_N/0/1/0/all/0/1\">Nasir Memon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EMOCA: Emotion Driven Monocular Face Capture and Animation. (arXiv:2204.11312v1 [cs.CV])","link":"http://arxiv.org/abs/2204.11312","description":"<p>As 3D facial avatars become more widely used for communication, it is\ncritical that they faithfully convey emotion. Unfortunately, the best recent\nmethods that regress parametric 3D face models from monocular images are unable\nto capture the full spectrum of facial expression, such as subtle or extreme\nemotions. We find the standard reconstruction metrics used for training\n(landmark reprojection error, photometric error, and face recognition loss) are\ninsufficient to capture high-fidelity expressions. The result is facial\ngeometries that do not match the emotional content of the input image. We\naddress this with EMOCA (EMOtion Capture and Animation), by introducing a novel\ndeep perceptual emotion consistency loss during training, which helps ensure\nthat the reconstructed 3D expression matches the expression depicted in the\ninput image. While EMOCA achieves 3D reconstruction errors that are on par with\nthe current best methods, it significantly outperforms them in terms of the\nquality of the reconstructed expression and the perceived emotional content. We\nalso directly regress levels of valence and arousal and classify basic\nexpressions from the estimated 3D face parameters. On the task of in-the-wild\nemotion recognition, our purely geometric approach is on par with the best\nimage-based methods, highlighting the value of 3D geometry in analyzing human\nbehavior. The model and code are publicly available at\nhttps://emoca.is.tue.mpg.de.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Danecek_R/0/1/0/all/0/1\">Radek Danecek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Black_M/0/1/0/all/0/1\">Michael J. Black</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bolkart_T/0/1/0/all/0/1\">Timo Bolkart</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simulating Fluids in Real-World Still Images. (arXiv:2204.11335v1 [cs.CV])","link":"http://arxiv.org/abs/2204.11335","description":"<p>In this work, we tackle the problem of real-world fluid animation from a\nstill image. The key of our system is a surface-based layered representation\nderiving from video decomposition, where the scene is decoupled into a surface\nfluid layer and an impervious background layer with corresponding\ntransparencies to characterize the composition of the two layers. The animated\nvideo can be produced by warping only the surface fluid layer according to the\nestimation of fluid motions and recombining it with the background. In\naddition, we introduce surface-only fluid simulation, a $2.5D$ fluid\ncalculation version, as a replacement for motion estimation. Specifically, we\nleverage the triangular mesh based on a monocular depth estimator to represent\nthe fluid surface layer and simulate the motion in the physics-based framework\nwith the inspiration of the classic theory of the hybrid Lagrangian-Eulerian\nmethod, along with a learnable network so as to adapt to complex real-world\nimage textures. We demonstrate the effectiveness of the proposed system through\ncomparison with existing methods in both standard objective metrics and\nsubjective ranking scores. Extensive experiments not only indicate our method's\ncompetitive performance for common fluid scenes but also better robustness and\nreasonability under complex transparent fluid scenarios. Moreover, as the\nproposed surface-based layer representation and surface-only fluid simulation\nnaturally disentangle the scene, interactive editing such as adding objects to\nthe river and texture replacing could be easily achieved with realistic\nresults.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_S/0/1/0/all/0/1\">Siming Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piao_J/0/1/0/all/0/1\">Jingtan Piao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1\">Chen Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_K/0/1/0/all/0/1\">Kwan-Yee Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning for Medical Image Registration: A Comprehensive Review. (arXiv:2204.11341v1 [eess.IV])","link":"http://arxiv.org/abs/2204.11341","description":"<p>Image registration is a critical component in the applications of various\nmedical image analyses. In recent years, there has been a tremendous surge in\nthe development of deep learning (DL)-based medical image registration models.\nThis paper provides a comprehensive review of medical image registration.\nFirstly, a discussion is provided for supervised registration categories, for\nexample, fully supervised, dual supervised, and weakly supervised registration.\nNext, similarity-based as well as generative adversarial network (GAN)-based\nregistration are presented as part of unsupervised registration. Deep iterative\nregistration is then described with emphasis on deep similarity-based and\nreinforcement learning-based registration. Moreover, the application areas of\nmedical image registration are reviewed. This review focuses on monomodal and\nmultimodal registration and associated imaging, for instance, X-ray, CT scan,\nultrasound, and MRI. The existing challenges are highlighted in this review,\nwhere it is shown that a major challenge is the absence of a training dataset\nwith known transformations. Finally, a discussion is provided on the promising\nfuture research areas in the field of DL-based medical image registration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bharati_S/0/1/0/all/0/1\">Subrato Bharati</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mondal_M/0/1/0/all/0/1\">M. Rubaiyat Hossain Mondal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Podder_P/0/1/0/all/0/1\">Prajoy Podder</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Prasath_V/0/1/0/all/0/1\">V. B. Surya Prasath</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Reinforcement Learning Using a Low-Dimensional Observation Filter for Visual Complex Video Game Playing. (arXiv:2204.11370v1 [cs.CV])","link":"http://arxiv.org/abs/2204.11370","description":"<p>Deep Reinforcement Learning (DRL) has produced great achievements since it\nwas proposed, including the possibility of processing raw vision input data.\nHowever, training an agent to perform tasks based on image feedback remains a\nchallenge. It requires the processing of large amounts of data from\nhigh-dimensional observation spaces, frame by frame, and the agent's actions\nare computed according to deep neural network policies, end-to-end. Image\npre-processing is an effective way of reducing these high dimensional spaces,\neliminating unnecessary information present in the scene, supporting the\nextraction of features and their representations in the agent's neural network.\nModern video-games are examples of this type of challenge for DRL algorithms\nbecause of their visual complexity. In this paper, we propose a low-dimensional\nobservation filter that allows a deep Q-network agent to successfully play in a\nvisually complex and modern video-game, called Neon Drive.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kich_V/0/1/0/all/0/1\">Victor Augusto Kich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jesus_J/0/1/0/all/0/1\">Junior Costa de Jesus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grando_R/0/1/0/all/0/1\">Ricardo Bedin Grando</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolling_A/0/1/0/all/0/1\">Alisson Henrique Kolling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heisler_G/0/1/0/all/0/1\">Gabriel Vin&#xed;cius Heisler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guerra_R/0/1/0/all/0/1\">Rodrigo da Silva Guerra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DRT: A Lightweight Single Image Deraining Recursive Transformer. (arXiv:2204.11385v1 [cs.CV])","link":"http://arxiv.org/abs/2204.11385","description":"<p>Over parameterization is a common technique in deep learning to help models\nlearn and generalize sufficiently to the given task; nonetheless, this often\nleads to enormous network structures and consumes considerable computing\nresources during training. Recent powerful transformer-based deep learning\nmodels on vision tasks usually have heavy parameters and bear training\ndifficulty. However, many dense-prediction low-level computer vision tasks,\nsuch as rain streak removing, often need to be executed on devices with limited\ncomputing power and memory in practice. Hence, we introduce a recursive local\nwindow-based self-attention structure with residual connections and propose\nderaining a recursive transformer (DRT), which enjoys the superiority of the\ntransformer but requires a small amount of computing resources. In particular,\nthrough recursive architecture, our proposed model uses only 1.3% of the number\nof parameters of the current best performing model in deraining while exceeding\nthe state-of-the-art methods on the Rain100L benchmark by at least 0.33 dB.\nAblation studies also investigate the impact of recursions on derain outcomes.\nMoreover, since the model contains no deliberate design for deraining, it can\nalso be applied to other image restoration tasks. Our experiment shows that it\ncan achieve competitive results on desnowing. The source code and pretrained\nmodel can be found at https://github.com/YC-Liang/DRT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yuanchu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anwar_S/0/1/0/all/0/1\">Saeed Anwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video Frame Interpolation Based on Deformable Kernel Region. (arXiv:2204.11396v1 [cs.CV])","link":"http://arxiv.org/abs/2204.11396","description":"<p>Video frame interpolation task has recently become more and more prevalent in\nthe computer vision field. At present, a number of researches based on deep\nlearning have achieved great success. Most of them are either based on optical\nflow information, or interpolation kernel, or a combination of these two\nmethods. However, these methods have ignored that there are grid restrictions\non the position of kernel region during synthesizing each target pixel. These\nlimitations result in that they cannot well adapt to the irregularity of object\nshape and uncertainty of motion, which may lead to irrelevant reference pixels\nused for interpolation. In order to solve this problem, we revisit the\ndeformable convolution for video interpolation, which can break the fixed grid\nrestrictions on the kernel region, making the distribution of reference points\nmore suitable for the shape of the object, and thus warp a more accurate\ninterpolation frame. Experiments are conducted on four datasets to demonstrate\nthe superior performance of the proposed model in comparison to the\nstate-of-the-art alternatives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_H/0/1/0/all/0/1\">Haoyue Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Pan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1\">Xiaojiang Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tensorial tomographic differential phase-contrast microscopy. (arXiv:2204.11397v1 [physics.optics])","link":"http://arxiv.org/abs/2204.11397","description":"<p>We report Tensorial Tomographic Differential Phase-Contrast microscopy\n(T2DPC), a quantitative label-free tomographic imaging method for simultaneous\nmeasurement of phase and anisotropy. T2DPC extends differential phase-contrast\nmicroscopy, a quantitative phase imaging technique, to highlight the vectorial\nnature of light. The method solves for permittivity tensor of anisotropic\nsamples from intensity measurements acquired with a standard microscope\nequipped with an LED matrix, a circular polarizer, and a polarization-sensitive\ncamera. We demonstrate accurate volumetric reconstructions of refractive index,\nbirefringence, and orientation for various validation samples, and show that\nthe reconstructed polarization structures of a biological specimen are\npredictive of pathology.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Xu_S/0/1/0/all/0/1\">Shiqi Xu</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Dai_X/0/1/0/all/0/1\">Xiang Dai</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Yang_X/0/1/0/all/0/1\">Xi Yang</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Zhou_K/0/1/0/all/0/1\">Kevin C. Zhou</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Kim_K/0/1/0/all/0/1\">Kanghyun Kim</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Pathak_V/0/1/0/all/0/1\">Vinayak Pathak</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Glass_C/0/1/0/all/0/1\">Carolyn Glass</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Horstmeyer_R/0/1/0/all/0/1\">Roarke Horstmeyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PointInst3D: Segmenting 3D Instances by Points. (arXiv:2204.11402v1 [cs.CV])","link":"http://arxiv.org/abs/2204.11402","description":"<p>The current state-of-the-art methods in 3D instance segmentation typically\ninvolve a clustering step, despite the tendency towards heuristics, greedy\nalgorithms, and a lack of robustness to the changes in data statistics. In\ncontrast, we propose a fully-convolutional 3D point cloud instance segmentation\nmethod that works in a per-point prediction fashion. In doing so it avoids the\nchallenges that clustering-based methods face: introducing dependencies among\ndifferent tasks of the model. We find the key to its success is assigning a\nsuitable target to each sampled point. Instead of the commonly used static or\ndistance-based assignment strategies, we propose to use an Optimal Transport\napproach to optimally assign target masks to the sampled points according to\nthe dynamic matching costs. Our approach achieves promising results on both\nScanNet and S3DIS benchmarks. The proposed approach removes intertask\ndependencies and thus represents a simpler and more flexible 3D instance\nsegmentation framework than other competing methods, while achieving improved\nsegmentation accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1\">Tong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hengel_A/0/1/0/all/0/1\">Anton van den Hengel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Point Cloud Compression with Cross-Sectional Approach. (arXiv:2204.11409v1 [cs.CV])","link":"http://arxiv.org/abs/2204.11409","description":"<p>The recent development of dynamic point clouds has introduced the possibility\nof mimicking natural reality, and greatly assisting quality of life. However,\nto broadcast successfully, the dynamic point clouds require higher compression\ndue to their huge volume of data compared to the traditional video. Recently,\nMPEG finalized a Video-based Point Cloud Compression standard known as V-PCC.\nHowever, V-PCC requires huge computational time due to expensive normal\ncalculation and segmentation, sacrifices some points to limit the number of 2D\npatches, and cannot occupy all spaces in the 2D frame. The proposed method\naddresses these limitations by using a novel cross-sectional approach. This\napproach reduces expensive normal estimation and segmentation, retains more\npoints, and utilizes more spaces for 2D frame generation compared to the VPCC.\nThe experimental results using standard video sequences show that the proposed\ntechnique can achieve better compression in both geometric and texture data\ncompared to the V-PCC standard.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tohidi_F/0/1/0/all/0/1\">Faranak Tohidi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paul_M/0/1/0/all/0/1\">Manoranjan Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ulhaq_A/0/1/0/all/0/1\">Anwaar Ulhaq</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Single Object Tracking Research: A Survey. (arXiv:2204.11410v1 [cs.CV])","link":"http://arxiv.org/abs/2204.11410","description":"<p>Visual object tracking is an important task in computer vision, which has\nmany real-world applications, e.g., video surveillance, visual navigation.\nVisual object tracking also has many challenges, e.g., object occlusion and\ndeformation. To solve above problems and track the target accurately and\nefficiently, many tracking algorithms have emerged in recent years. This paper\npresents the rationale and representative works of two most popular tracking\nframeworks in past ten years, i.e., the corelation filter and Siamese network\nfor object tracking. Then we present some deep learning based tracking methods\ncategorized by different network structures. We also introduce some classical\nstrategies for handling the challenges in tracking problem. Further, this paper\ndetailedly present and compare the benchmarks and challenges for tracking, from\nwhich we summarize the development history and development trend of visual\ntracking. Focusing on the future development of object tracking, which we think\nwould be applied in real-world scenes before some problems to be addressed,\nsuch as the problems in long-term tracking, low-power high-speed tracking and\nattack-robust tracking. In the future, the integration of multimodal data,\ne.g., the depth image, thermal image with traditional color image, will provide\nmore solutions for visual tracking. Moreover, tracking task will go together\nwith some other tasks, e.g., video object detection and segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_R/0/1/0/all/0/1\">Ruize Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1\">Wei Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1\">Qing Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1\">Qinghua Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Audio-Visual Scene Classification Using A Transfer Learning Based Joint Optimization Strategy. (arXiv:2204.11420v1 [cs.CV])","link":"http://arxiv.org/abs/2204.11420","description":"<p>Recently, audio-visual scene classification (AVSC) has attracted increasing\nattention from multidisciplinary communities. Previous studies tended to adopt\na pipeline training strategy, which uses well-trained visual and acoustic\nencoders to extract high-level representations (embeddings) first, then\nutilizes them to train the audio-visual classifier. In this way, the extracted\nembeddings are well suited for uni-modal classifiers, but not necessarily\nsuited for multi-modal ones. In this paper, we propose a joint training\nframework, using the acoustic features and raw images directly as inputs for\nthe AVSC task. Specifically, we retrieve the bottom layers of pre-trained image\nmodels as visual encoder, and jointly optimize the scene classifier and 1D-CNN\nbased acoustic encoder during training. We evaluate the approach on the\ndevelopment dataset of TAU Urban Audio-Visual Scenes 2021. The experimental\nresults show that our proposed approach achieves significant improvement over\nthe conventional pipeline training strategy. Moreover, our best single system\noutperforms previous state-of-the-art methods, yielding a log loss of 0.1517\nand accuracy of 94.59% on the official test fold.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chengxin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Meng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pengyuan Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BCI: Breast Cancer Immunohistochemical Image Generation through Pyramid Pix2pix. (arXiv:2204.11425v1 [eess.IV])","link":"http://arxiv.org/abs/2204.11425","description":"<p>The evaluation of human epidermal growth factor receptor 2 (HER2) expression\nis essential to formulate a precise treatment for breast cancer. The routine\nevaluation of HER2 is conducted with immunohistochemical techniques (IHC),\nwhich is very expensive. Therefore, for the first time, we propose a breast\ncancer immunohistochemical (BCI) benchmark attempting to synthesize IHC data\ndirectly with the paired hematoxylin and eosin (HE) stained images. The dataset\ncontains 4870 registered image pairs, covering a variety of HER2 expression\nlevels. Based on BCI, as a minor contribution, we further build a pyramid\npix2pix image generation method, which achieves better HE to IHC translation\nresults than the other current popular algorithms. Extensive experiments\ndemonstrate that BCI poses new challenges to the existing image translation\nresearch. Besides, BCI also opens the door for future pathology studies in HER2\nexpression evaluation based on the synthesized IHC images. BCI dataset can be\ndownloaded from https://bupt-ai-cz.github.io/BCI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Liu_S/0/1/0/all/0/1\">Shengjie Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_C/0/1/0/all/0/1\">Chuang Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_F/0/1/0/all/0/1\">Feng Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jia_X/0/1/0/all/0/1\">Xinyu Jia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shi_Z/0/1/0/all/0/1\">Zhongyue Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jin_M/0/1/0/all/0/1\">Mulan Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Hierarchical Semantic Segmentation with Multiview Cosegmentation and Clustering Transformers. (arXiv:2204.11432v1 [cs.CV])","link":"http://arxiv.org/abs/2204.11432","description":"<p>Unsupervised semantic segmentation aims to discover groupings within and\nacross images that capture object and view-invariance of a category without\nexternal supervision. Grouping naturally has levels of granularity, creating\nambiguity in unsupervised segmentation. Existing methods avoid this ambiguity\nand treat it as a factor outside modeling, whereas we embrace it and desire\nhierarchical grouping consistency for unsupervised segmentation.\n</p>\n<p>We approach unsupervised segmentation as a pixel-wise feature learning\nproblem. Our idea is that a good representation shall reveal not just a\nparticular level of grouping, but any level of grouping in a consistent and\npredictable manner. We enforce spatial consistency of grouping and bootstrap\nfeature learning with co-segmentation among multiple views of the same image,\nand enforce semantic consistency across the grouping hierarchy with clustering\ntransformers between coarse- and fine-grained features.\n</p>\n<p>We deliver the first data-driven unsupervised hierarchical semantic\nsegmentation method called Hierarchical Segment Grouping (HSG). Capturing\nvisual similarity and statistical co-occurrences, HSG also outperforms existing\nunsupervised segmentation methods by a large margin on five major object- and\nscene-centric benchmarks. Our code is publicly available at\nhttps://github.com/twke18/HSG .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ke_T/0/1/0/all/0/1\">Tsung-Wei Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1\">Jyh-Jing Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yunhui Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xudong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Stella X. Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Surpassing the Human Accuracy: Detecting Gallbladder Cancer from USG Images with Curriculum Learning. (arXiv:2204.11433v1 [cs.CV])","link":"http://arxiv.org/abs/2204.11433","description":"<p>We explore the potential of CNN-based models for gallbladder cancer (GBC)\ndetection from ultrasound (USG) images as no prior study is known. USG is the\nmost common diagnostic modality for GB diseases due to its low cost and\naccessibility. However, USG images are challenging to analyze due to low image\nquality, noise, and varying viewpoints due to the handheld nature of the\nsensor. Our exhaustive study of state-of-the-art (SOTA) image classification\ntechniques for the problem reveals that they often fail to learn the salient GB\nregion due to the presence of shadows in the USG images. SOTA object detection\ntechniques also achieve low accuracy because of spurious textures due to noise\nor adjacent organs. We propose GBCNet to tackle the challenges in our problem.\nGBCNet first extracts the regions of interest (ROIs) by detecting the GB (and\nnot the cancer), and then uses a new multi-scale, second-order pooling\narchitecture specializing in classifying GBC. To effectively handle spurious\ntextures, we propose a curriculum inspired by human visual acuity, which\nreduces the texture biases in GBCNet. Experimental results demonstrate that\nGBCNet significantly outperforms SOTA CNN models, as well as the expert\nradiologists. Our technical innovations are generic to other USG image analysis\ntasks as well. Hence, as a validation, we also show the efficacy of GBCNet in\ndetecting breast cancer from USG images. Project page with source code, trained\nmodels, and data is available at https://gbc-iitd.github.io/gbcnet\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Basu_S/0/1/0/all/0/1\">Soumen Basu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_M/0/1/0/all/0/1\">Mayank Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rana_P/0/1/0/all/0/1\">Pratyaksha Rana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_P/0/1/0/all/0/1\">Pankaj Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arora_C/0/1/0/all/0/1\">Chetan Arora</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Joint Feature Distribution Alignment Learning for NIR-VIS and VIS-VIS Face Recognition. (arXiv:2204.11434v1 [cs.CV])","link":"http://arxiv.org/abs/2204.11434","description":"<p>Face recognition for visible light (VIS) images achieve high accuracy thanks\nto the recent development of deep learning. However, heterogeneous face\nrecognition (HFR), which is a face matching in different domains, is still a\ndifficult task due to the domain discrepancy and lack of large HFR dataset.\nSeveral methods have attempted to reduce the domain discrepancy by means of\nfine-tuning, which causes significant degradation of the performance in the VIS\ndomain because it loses the highly discriminative VIS representation. To\novercome this problem, we propose joint feature distribution alignment learning\n(JFDAL) which is a joint learning approach utilizing knowledge distillation. It\nenables us to achieve high HFR performance with retaining the original\nperformance for the VIS domain. Extensive experiments demonstrate that our\nproposed method delivers statistically significantly better performances\ncompared with the conventional fine-tuning approach on a public HFR dataset\nOulu-CASIA NIR&amp;VIS and popular verification datasets in VIS domain such as FLW,\nCFP, AgeDB. Furthermore, comparative experiments with existing state-of-the-art\nHFR methods show that our method achieves a comparable HFR performance on the\nOulu-CASIA NIR&amp;VIS dataset with less degradation of VIS performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Miyamoto_T/0/1/0/all/0/1\">Takaya Miyamoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashimoto_H/0/1/0/all/0/1\">Hiroshi Hashimoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayasaka_A/0/1/0/all/0/1\">Akihiro Hayasaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ebihara_A/0/1/0/all/0/1\">Akinori F. Ebihara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Imaoka_H/0/1/0/all/0/1\">Hitoshi Imaoka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Concurrence-Aware Long Short-Term Sub-Memories for Person-Person Action Recognition. (arXiv:1706.00931v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1706.00931","description":"<p>Recently, Long Short-Term Memory (LSTM) has become a popular choice to model\nindividual dynamics for single-person action recognition due to its ability of\nmodeling the temporal information in various ranges of dynamic contexts.\nHowever, existing RNN models only focus on capturing the temporal dynamics of\nthe person-person interactions by naively combining the activity dynamics of\nindividuals or modeling them as a whole. This neglects the inter-related\ndynamics of how person-person interactions change over time. To this end, we\npropose a novel Concurrence-Aware Long Short-Term Sub-Memories (Co-LSTSM) to\nmodel the long-term inter-related dynamics between two interacting people on\nthe bounding boxes covering people. Specifically, for each frame, two\nsub-memory units store individual motion information, while a concurrent LSTM\nunit selectively integrates and stores inter-related motion information between\ninteracting people from these two sub-memory units via a new co-memory cell.\nExperimental results on the BIT and UT datasets show the superiority of\nCo-LSTSM compared with the state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shu_X/0/1/0/all/0/1\">Xiangbo Shu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Deep Hashing Methods. (arXiv:2003.03369v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2003.03369","description":"<p>Nearest neighbor search aims to obtain the samples in the database with the\nsmallest distances from them to the queries, which is a basic task in a range\nof fields, including computer vision and data mining. Hashing is one of the\nmost widely used methods for its computational and storage efficiency. With the\ndevelopment of deep learning, deep hashing methods show more advantages than\ntraditional methods. In this survey, we detailedly investigate current deep\nhashing algorithms including deep supervised hashing and deep unsupervised\nhashing. Specifically, we categorize deep supervised hashing methods into\npairwise methods, ranking-based methods, pointwise methods as well as\nquantization according to how measuring the similarities of the learned hash\ncodes. Moreover, deep unsupervised hashing is categorized into similarity\nreconstruction-based methods, pseudo-label-based methods and prediction-free\nself-supervised learning-based methods based on their semantic learning\nmanners. We also introduce three related important topics including\nsemi-supervised deep hashing, domain adaption deep hashing and multi-modal deep\nhashing. Meanwhile, we present some commonly used public datasets and the\nscheme to measure the performance of deep hashing algorithms. Finally, we\ndiscuss some potential research directions in conclusion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xiao Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haixin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Daqing Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_M/0/1/0/all/0/1\">Minghua Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jianqiang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_X/0/1/0/all/0/1\">Xian-Sheng Hua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cyclic Differentiable Architecture Search. (arXiv:2006.10724v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2006.10724","description":"<p>Differentiable ARchiTecture Search, i.e., DARTS, has drawn great attention in\nneural architecture search. It tries to find the optimal architecture in a\nshallow search network and then measures its performance in a deep evaluation\nnetwork. The independent optimization of the search and evaluation networks,\nhowever, leaves room for potential improvement by allowing interaction between\nthe two networks. To address the problematic optimization issue, we propose new\njoint optimization objectives and a novel Cyclic Differentiable ARchiTecture\nSearch framework, dubbed CDARTS. Considering the structure difference, CDARTS\nbuilds a cyclic feedback mechanism between the search and evaluation networks\nwith introspective distillation. First, the search network generates an initial\narchitecture for evaluation, and the weights of the evaluation network are\noptimized. Second, the architecture weights in the search network are further\noptimized by the label supervision in classification, as well as the\nregularization from the evaluation network through feature distillation.\nRepeating the above cycle results in joint optimization of the search and\nevaluation networks and thus enables the evolution of the architecture to fit\nthe final evaluation network. The experiments and analysis on CIFAR, ImageNet\nand NAS-Bench-201 demonstrate the effectiveness of the proposed approach over\nthe state-of-the-art ones. Specifically, in the DARTS search space, we achieve\n97.52% top-1 accuracy on CIFAR10 and 76.3% top-1 accuracy on ImageNet. In the\nchain-structured search space, we achieve 78.2% top-1 accuracy on ImageNet,\nwhich is 1.1% higher than EfficientNet-B0. Our code and models are publicly\navailable at https://github.com/microsoft/Cream.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hongyuan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Houwen Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jianlong Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_H/0/1/0/all/0/1\">Hao Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_H/0/1/0/all/0/1\">Haibin Ling</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fashion Captioning: Towards Generating Accurate Descriptions with Semantic Rewards. (arXiv:2008.02693v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2008.02693","description":"<p>Generating accurate descriptions for online fashion items is important not\nonly for enhancing customers' shopping experiences, but also for the increase\nof online sales. Besides the need of correctly presenting the attributes of\nitems, the expressions in an enchanting style could better attract customer\ninterests. The goal of this work is to develop a novel learning framework for\naccurate and expressive fashion captioning. Different from popular work on\nimage captioning, it is hard to identify and describe the rich attributes of\nfashion items. We seed the description of an item by first identifying its\nattributes, and introduce attribute-level semantic (ALS) reward and\nsentence-level semantic (SLS) reward as metrics to improve the quality of text\ndescriptions. We further integrate the training of our model with maximum\nlikelihood estimation (MLE), attribute embedding, and Reinforcement Learning\n(RL). To facilitate the learning, we build a new FAshion CAptioning Dataset\n(FACAD), which contains 993K images and 130K corresponding enchanting and\ndiverse descriptions. Experiments on FACAD demonstrate the effectiveness of our\nmodel.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xuewen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Heming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_D/0/1/0/all/0/1\">Di Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yingru Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chi-Hao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1\">Jianchao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_D/0/1/0/all/0/1\">Dongliang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast Facial Landmark Detection and Applications: A Survey. (arXiv:2101.10808v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.10808","description":"<p>Dense facial landmark detection is one of the key elements of face processing\npipeline. It is used in virtual face reenactment, emotion recognition, driver\nstatus tracking, etc. Early approaches were suitable for facial landmark\ndetection in controlled environments only, which is clearly insufficient.\nNeural networks have shown an astonishing qualitative improvement for\nin-the-wild face landmark detection problem, and are now being studied by many\nresearchers in the field. Numerous bright ideas are proposed, often\ncomplimentary to each other. However, exploration of the whole volume of novel\napproaches is quite challenging. Therefore, we present this survey, where we\nsummarize state-of-the-art algorithms into categories, provide a comparison of\nrecently introduced in-the-wild datasets (e.g., 300W, AFLW, COFW, WFLW) that\ncontain images with large pose, face occlusion, taken in unconstrained\nconditions. In addition to quality, applications require fast inference, and\npreferably on mobile devices. Hence, we include information about algorithm\ninference speed both on desktop and mobile hardware, which is rarely studied.\nImportantly, we highlight problems of algorithms, their applications,\nvulnerabilities, and briefly touch on established methods. We hope that the\nreader will find many novel ideas, will see how the algorithms are used in\napplications, which will enable further research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khabarlak_K/0/1/0/all/0/1\">Kostiantyn Khabarlak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koriashkina_L/0/1/0/all/0/1\">Larysa Koriashkina</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Masked Face Recognition: Human vs. Machine. (arXiv:2103.01924v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.01924","description":"<p>The recent COVID-19 pandemic has increased the focus on hygienic and\ncontactless identity verification methods. However, the pandemic led to the\nwide use of face masks, essential to keep the pandemic under control. The\neffect of wearing a mask on face recognition (FR) in a collaborative\nenvironment is a currently sensitive yet understudied issue. Recent reports\nhave tackled this by evaluating the masked probe effect on the performance of\nautomatic FR solutions. However, such solutions can fail in certain processes,\nleading to performing the verification task by a human expert. This work\nprovides a joint evaluation and in-depth analyses of the face verification\nperformance of human experts in comparison to state-of-the-art automatic FR\nsolutions. This involves an extensive evaluation by human experts and 4\nautomatic recognition solutions. The study concludes with a set of take-home\nmessages on different aspects of the correlation between the verification\nbehavior of humans and machines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Damer_N/0/1/0/all/0/1\">Naser Damer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boutros_F/0/1/0/all/0/1\">Fadi Boutros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sussmilch_M/0/1/0/all/0/1\">Marius S&#xfc;&#xdf;milch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_M/0/1/0/all/0/1\">Meiling Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirchbuchner_F/0/1/0/all/0/1\">Florian Kirchbuchner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuijper_A/0/1/0/all/0/1\">Arjan Kuijper</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Anytime Dense Prediction with Confidence Adaptivity. (arXiv:2104.00749v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.00749","description":"<p>Anytime inference requires a model to make a progression of predictions which\nmight be halted at any time. Prior research on anytime visual recognition has\nmostly focused on image classification. We propose the first unified and\nend-to-end approach for anytime dense prediction. A cascade of \"exits\" is\nattached to the model to make multiple predictions. We redesign the exits to\naccount for the depth and spatial resolution of the features for each exit. To\nreduce total computation, and make full use of prior predictions, we develop a\nnovel spatially adaptive approach to avoid further computation on regions where\nearly predictions are already sufficiently confident. Our full method, named\nanytime dense prediction with confidence (ADP-C), achieves the same level of\nfinal accuracy as the base model, and meanwhile significantly reduces total\ncomputation. We evaluate our method on Cityscapes semantic segmentation and\nMPII human pose estimation: ADP-C enables anytime inference without sacrificing\naccuracy while also reducing the total FLOPs of its base models by 44.4% and\n59.1%. We compare with anytime inference by deep equilibrium networks and\nfeature-based stochastic sampling, showing that ADP-C dominates both across the\naccuracy-computation curve. Our code is available at\nhttps://github.com/liuzhuang13/anytime .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhuang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhiqiu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hung-Ju Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1\">Trevor Darrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shelhamer_E/0/1/0/all/0/1\">Evan Shelhamer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Fusion of EMG and Vision for Human Grasp Intent Inference in Prosthetic Hand Control. (arXiv:2104.03893v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2104.03893","description":"<p>For lower arm amputees, robotic prosthetic hands offer the promise to regain\nthe capability to perform fine object manipulation in activities of daily\nliving. Current control methods based on physiological signals such as EEG and\nEMG are prone to poor inference outcomes due to motion artifacts, variability\nof skin electrode junction impedance over time, muscle fatigue, and other\nfactors. Visual evidence is also susceptible to its own artifacts, most often\ndue to object occlusion, lighting changes, variable shapes of objects depending\non view-angle, among other factors. Multimodal evidence fusion using\nphysiological and vision sensor measurements is a natural approach due to the\ncomplementary strengths of these modalities.\n</p>\n<p>In this paper, we present a Bayesian evidence fusion framework for grasp\nintent inference using eye-view video, gaze, and EMG from the forearm processed\nby neural network models. We analyze individual and fused performance as a\nfunction of time as the hand approaches the object to grasp it. For this\npurpose, we have also developed novel data processing and augmentation\ntechniques to train neural network components. Our experimental data analyses\ndemonstrate that EMG and visual evidence show complementary strengths, and as a\nconsequence, fusion of multimodal evidence can outperform each individual\nevidence modality at any given time. Specifically, results indicate that, on\naverage, fusion improves the instantaneous upcoming grasp type classification\naccuracy while in the reaching phase by 13.66% and 14.8%, relative to EMG and\nvisual evidence individually, resulting in an overall fusion accuracy of 95.3%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zandigohar_M/0/1/0/all/0/1\">Mehrshad Zandigohar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_M/0/1/0/all/0/1\">Mo Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharif_M/0/1/0/all/0/1\">Mohammadreza Sharif</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunay_S/0/1/0/all/0/1\">Sezen Yagmur Gunay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Furmanek_M/0/1/0/all/0/1\">Mariusz P. Furmanek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yarossi_M/0/1/0/all/0/1\">Mathew Yarossi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bonato_P/0/1/0/all/0/1\">Paolo Bonato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Onal_C/0/1/0/all/0/1\">Cagdas Onal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padir_T/0/1/0/all/0/1\">Taskin Padir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erdogmus_D/0/1/0/all/0/1\">Deniz Erdogmus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schirner_G/0/1/0/all/0/1\">Gunar Schirner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Probabilistic Rainfall Estimation from Automotive Lidar. (arXiv:2104.11467v2 [eess.SP] UPDATED)","link":"http://arxiv.org/abs/2104.11467","description":"<p>Robust sensing and perception in adverse weather conditions remain one of the\nbiggest challenges for realizing reliable autonomous vehicle mobility services.\nPrior work has established that rainfall rate is a useful measure for the\nadversity of atmospheric weather conditions. This work presents a probabilistic\nhierarchical Bayesian model that infers rainfall rate from automotive lidar\npoint cloud sequences with high accuracy and reliability. The model is a\nhierarchical mixture of experts model, or a probabilistic decision tree, with\ngating and expert nodes consisting of variational logistic and linear\nregression models. Experimental data used to train and evaluate the model is\ncollected in a large-scale rainfall experiment facility from both stationary\nand moving vehicle platforms. The results show prediction accuracy comparable\nto the measurement resolution of a disdrometer, and the soundness and\nusefulness of the uncertainty estimation. The model achieves RMSE 2.42\\,mm/h\nafter filtering out uncertain predictions. The error is comparable to the mean\nrainfall rate change of 3.5\\,mm/h between measurements. Model parameter studies\nshow how predictive performance changes with tree depth, sampling duration, and\ncrop box dimension. A second experiment demonstrates the predictability of\nhigher rainfall above 300\\,mm/h using a different lidar sensor, demonstrating\nsensor independence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Karlsson_R/0/1/0/all/0/1\">Robin Karlsson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wong_D/0/1/0/all/0/1\">David Robert Wong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kawabata_K/0/1/0/all/0/1\">Kazunari Kawabata</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Thompson_S/0/1/0/all/0/1\">Simon Thompson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sakai_N/0/1/0/all/0/1\">Naoki Sakai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attention and Prediction Guided Motion Detection for Low-Contrast Small Moving Targets. (arXiv:2104.13018v7 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.13018","description":"<p>Small target motion detection within complex natural environments is an\nextremely challenging task for autonomous robots. Surprisingly, the visual\nsystems of insects have evolved to be highly efficient in detecting mates and\ntracking prey, even though targets occupy as small as a few degrees of their\nvisual fields. The excellent sensitivity to small target motion relies on a\nclass of specialized neurons called small target motion detectors (STMDs).\nHowever, existing STMD-based models are heavily dependent on visual contrast\nand perform poorly in complex natural environments where small targets\ngenerally exhibit extremely low contrast against neighbouring backgrounds. In\nthis paper, we develop an attention and prediction guided visual system to\novercome this limitation. The developed visual system comprises three main\nsubsystems, namely, an attention module, an STMD-based neural network, and a\nprediction module. The attention module searches for potential small targets in\nthe predicted areas of the input image and enhances their contrast against\ncomplex background. The STMD-based neural network receives the\ncontrast-enhanced image and discriminates small moving targets from background\nfalse positives. The prediction module foresees future positions of the\ndetected targets and generates a prediction map for the attention module. The\nthree subsystems are connected in a recurrent architecture allowing information\nto be processed sequentially to activate specific areas for small target\ndetection. Extensive experiments on synthetic and real-world datasets\ndemonstrate the effectiveness and superiority of the proposed visual system for\ndetecting small, low-contrast moving targets against complex natural\nenvironments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jiannan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huatian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1\">Cheng Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1\">Jigen Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_S/0/1/0/all/0/1\">Shigang Yue</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Representation Learning for Clustering via Building Consensus. (arXiv:2105.01289v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.01289","description":"<p>In this paper, we focus on unsupervised representation learning for\nclustering of images. Recent advances in deep clustering and unsupervised\nrepresentation learning are based on the idea that different views of an input\nimage (generated through data augmentation techniques) must be close in the\nrepresentation space (exemplar consistency), and/or similar images must have\nsimilar cluster assignments (population consistency). We define an additional\nnotion of consistency, consensus consistency, which ensures that\nrepresentations are learned to induce similar partitions for variations in the\nrepresentation space, different clustering algorithms or different\ninitializations of a single clustering algorithm. We define a clustering loss\nby executing variations in the representation space and seamlessly integrate\nall three consistencies (consensus, exemplar and population) into an end-to-end\nlearning framework. The proposed algorithm, consensus clustering using\nunsupervised representation learning (ConCURL), improves upon the clustering\nperformance of state-of-the-art methods on four out of five image datasets.\nFurthermore, we extend the evaluation procedure for clustering to reflect the\nchallenges encountered in real-world clustering tasks, such as maintaining\nclustering performance in cases with distribution shifts. We also perform a\ndetailed ablation study for a deeper understanding of the proposed algorithm.\nThe code and the trained models are available at\nhttps://github.com/JayanthRR/ConCURL_NCE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deshmukh_A/0/1/0/all/0/1\">Aniket Anand Deshmukh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Regatti_J/0/1/0/all/0/1\">Jayanth Reddy Regatti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manavoglu_E/0/1/0/all/0/1\">Eren Manavoglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dogan_U/0/1/0/all/0/1\">Urun Dogan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The DEVIL is in the Details: A Diagnostic Evaluation Benchmark for Video Inpainting. (arXiv:2105.05332v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.05332","description":"<p>Quantitative evaluation has increased dramatically among recent video\ninpainting work, but the video and mask content used to gauge performance has\nreceived relatively little attention. Although attributes such as camera and\nbackground scene motion inherently change the difficulty of the task and affect\nmethods differently, existing evaluation schemes fail to control for them,\nthereby providing minimal insight into inpainting failure modes. To address\nthis gap, we propose the Diagnostic Evaluation of Video Inpainting on\nLandscapes (DEVIL) benchmark, which consists of two contributions: (i) a novel\ndataset of videos and masks labeled according to several key inpainting failure\nmodes, and (ii) an evaluation scheme that samples slices of the dataset\ncharacterized by a fixed content attribute, and scores performance on each\nslice according to reconstruction, realism, and temporal consistency quality.\nBy revealing systematic changes in performance induced by particular\ncharacteristics of the input content, our challenging benchmark enables more\ninsightful analysis into video inpainting methods and serves as an invaluable\ndiagnostic tool for the field. Our code and data are available at\nhttps://github.com/MichiganCOG/devil .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Szeto_R/0/1/0/all/0/1\">Ryan Szeto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corso_J/0/1/0/all/0/1\">Jason J. Corso</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Alpha Matte Generation from Single Input for Portrait Matting. (arXiv:2106.03210v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.03210","description":"<p>In the portrait matting, the goal is to predict an alpha matte that\nidentifies the effect of each pixel on the foreground subject. Traditional\napproaches and most of the existing works utilized an additional input, e.g.,\ntrimap, background image, to predict alpha matte. However, (1) providing\nadditional input is not always practical, and (2) models are too sensitive to\nthese additional inputs. To address these points, in this paper, we introduce\nan additional input-free approach to perform portrait matting. We divide the\ntask into two subtasks, segmentation and alpha matte prediction. We first\ngenerate a coarse segmentation map from the input image and then predict the\nalpha matte by utilizing the image and segmentation map. Besides, we present a\nsegmentation encoding block to downsample the coarse segmentation map and\nprovide useful feature representation to the residual block, since using a\nsingle encoder causes the vanishing of the segmentation information. We tested\nour model on four different benchmark datasets. The proposed method\noutperformed the MODNet and MGMatting methods that also take a single input.\nBesides, we obtained comparable results with BGM-V2 and FBA methods that\nrequire additional input.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yaman_D/0/1/0/all/0/1\">Dogucan Yaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ekenel_H/0/1/0/all/0/1\">Haz&#x131;m Kemal Ekenel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waibel_A/0/1/0/all/0/1\">Alexander Waibel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image2Point: 3D Point-Cloud Understanding with 2D Image Pretrained Models. (arXiv:2106.04180v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.04180","description":"<p>3D point-clouds and 2D images are different visual representations of the\nphysical world. While human vision can understand both representations,\ncomputer vision models designed for 2D image and 3D point-cloud understanding\nare quite different. Our paper explores the potential of transferring 2D model\narchitectures and weights to understand 3D point-clouds, by empirically\ninvestigating the feasibility of the transfer, the benefits of the transfer,\nand shedding light on why the transfer works. We discover that we can indeed\nuse the same architecture and pretrained weights of a neural net model to\nunderstand both images and point-clouds. Specifically, we transfer the\nimage-pretrained model to a point-cloud model by copying or inflating the\nweights. We find that finetuning the transformed image-pretrained models (FIP)\nwith minimal efforts -- only on input, output, and normalization layers -- can\nachieve competitive performance on 3D point-cloud classification, beating a\nwide range of point-cloud models that adopt task-specific architectures and use\na variety of tricks. When finetuning the whole model, the performance improves\neven further. Meanwhile, FIP improves data efficiency, reaching up to 10.0\ntop-1 accuracy percent on few-shot classification. It also speeds up the\ntraining of point-cloud models by up to 11.1x for a target accuracy (e.g., 90 %\naccuracy). Lastly, we provide an explanation of the image to point-cloud\ntransfer from the aspect of neural collapse. The code is available at:\n\\url{https://github.com/chenfengxu714/image2point}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chenfeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shijia Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galanti_T/0/1/0/all/0/1\">Tomer Galanti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Bichen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_X/0/1/0/all/0/1\">Xiangyu Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_B/0/1/0/all/0/1\">Bohan Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_W/0/1/0/all/0/1\">Wei Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vajda_P/0/1/0/all/0/1\">Peter Vajda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1\">Kurt Keutzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tomizuka_M/0/1/0/all/0/1\">Masayoshi Tomizuka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Steerable Partial Differential Operators for Equivariant Neural Networks. (arXiv:2106.10163v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.10163","description":"<p>Recent work in equivariant deep learning bears strong similarities to\nphysics. Fields over a base space are fundamental entities in both subjects, as\nare equivariant maps between these fields. In deep learning, however, these\nmaps are usually defined by convolutions with a kernel, whereas they are\npartial differential operators (PDOs) in physics. Developing the theory of\nequivariant PDOs in the context of deep learning could bring these subjects\neven closer together and lead to a stronger flow of ideas. In this work, we\nderive a $G$-steerability constraint that completely characterizes when a PDO\nbetween feature vector fields is equivariant, for arbitrary symmetry groups\n$G$. We then fully solve this constraint for several important groups. We use\nour solutions as equivariant drop-in replacements for convolutional layers and\nbenchmark them in that role. Finally, we develop a framework for equivariant\nmaps based on Schwartz distributions that unifies classical convolutions and\ndifferential operators and gives insight about the relation between the two.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jenner_E/0/1/0/all/0/1\">Erik Jenner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weiler_M/0/1/0/all/0/1\">Maurice Weiler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Deep Neural Network based Photometry and Astrometry Framework for Wide Field Small Aperture Telescopes. (arXiv:2106.14349v2 [astro-ph.IM] UPDATED)","link":"http://arxiv.org/abs/2106.14349","description":"<p>Wide field small aperture telescopes (WFSATs) are preferable observation\ninstruments for time domain astronomy, because they could obtain images of\ncelestial objects with high cadence in a cost-effective way. An automatic data\nprocessing algorithm which could detect celestial objects and obtain their\npositions and magnitudes from observed images is important for further\nscientific research. In this paper, we extend the ability of a deep neural\nnetwork based astronomical target detection algorithm to make it suitable for\nphotometry and astrometry, by adding two new branches. Because the photometry\nand astrometry neural network are data-driven regression algorithms, limited\ntraining data with limited diversity would introduce the epistemic uncertainty\nto final regression results. Therefore, we further investigate the epistemic\nuncertainty of our algorithm and have found that differences of background\nnoises and differences of point spread functions between the training data and\nthe real would introduce uncertainties to final measurements. To reduce this\neffect, we propose to use transfer learning strategy to train the neural\nnetwork with real data. The algorithm proposed in this paper could obtain\ntypes, positions and magnitudes of celestial objects with high accuracy and\ncost around 0.125 second to process an image, regardless of its size. The\nalgorithm could be integrated into data processing pipelines of WFSATs to\nincrease their response speed and detection ability to time-domain astronomical\nevents.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/astro-ph/1/au:+Jia_P/0/1/0/all/0/1\">Peng Jia</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Sun_Y/0/1/0/all/0/1\">Yongyang Sun</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Yang_Z/0/1/0/all/0/1\">Zhimin Yang</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Sun_R/0/1/0/all/0/1\">Rui Sun</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Liu_Q/0/1/0/all/0/1\">Qiang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SinGAN-Seg: Synthetic training data generation for medical image segmentation. (arXiv:2107.00471v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2107.00471","description":"<p>Analyzing medical data to find abnormalities is a time-consuming and costly\ntask, particularly for rare abnormalities, requiring tremendous efforts from\nmedical experts. Artificial intelligence has become a popular tool for the\nautomatic processing of medical data, acting as a supportive tool for doctors.\nHowever, the machine learning models used to build these tools are highly\ndependent on the data used to train them. Large amounts of data can be\ndifficult to obtain in medicine due to privacy, expensive and time-consuming\nannotations, and a general lack of data samples for infrequent lesions. Here,\nwe present a novel synthetic data generation pipeline, called SinGAN-Seg, to\nproduce synthetic medical images with corresponding masks using a single\ntraining image. Our method is different from the traditional GANs because our\nmodel needs only a single image and the corresponding ground truth to train.\nOur method produces alternative artificial segmentation datasets with ground\ntruth masks when real datasets are not allowed to share. The pipeline is\nevaluated using qualitative and quantitative comparisons between real and\nsynthetic data to show that the style transfer technique used in our pipeline\nsignificantly improves the quality of the generated data and our method is\nbetter than other state-of-the-art GANs to prepare synthetic images when the\nsize of training datasets are limited. By training UNet++ using both real and\nthe synthetic data generated from the SinGAN-Seg pipeline, we show that models\ntrained with synthetic data have very close performances to those trained on\nreal data when the datasets have a considerable amount of data. In contrast,\nSynthetic data generated from the SinGAN-Seg pipeline can improve the\nperformance of segmentation models when training datasets do not have a\nconsiderable amount of data. The code is available on GitHub.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Thambawita_V/0/1/0/all/0/1\">Vajira Thambawita</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Salehi_P/0/1/0/all/0/1\">Pegah Salehi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sheshkal_S/0/1/0/all/0/1\">Sajad Amouei Sheshkal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hicks_S/0/1/0/all/0/1\">Steven A. Hicks</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hammer_H/0/1/0/all/0/1\">Hugo L.Hammer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Parasa_S/0/1/0/all/0/1\">Sravanthi Parasa</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lange_T/0/1/0/all/0/1\">Thomas de Lange</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Halvorsen_P/0/1/0/all/0/1\">P&#xe5;l Halvorsen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Riegler_M/0/1/0/all/0/1\">Michael A. Riegler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing MR Image Segmentation with Realistic Adversarial Data Augmentation. (arXiv:2108.03429v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2108.03429","description":"<p>The success of neural networks on medical image segmentation tasks typically\nrelies on large labeled datasets for model training. However, acquiring and\nmanually labeling a large medical image set is resource-intensive, expensive,\nand sometimes impractical due to data sharing and privacy issues. To address\nthis challenge, we propose AdvChain, a generic adversarial data augmentation\nframework, aiming at improving both the diversity and effectiveness of training\ndata for medical image segmentation tasks. AdvChain augments data with dynamic\ndata augmentation, generating randomly chained photo-metric and geometric\ntransformations to resemble realistic yet challenging imaging variations to\nexpand training data. By jointly optimizing the data augmentation model and a\nsegmentation network during training, challenging examples are generated to\nenhance network generalizability for the downstream task. The proposed\nadversarial data augmentation does not rely on generative networks and can be\nused as a plug-in module in general segmentation networks. It is\ncomputationally efficient and applicable for both low-shot supervised and\nsemi-supervised learning. We analyze and evaluate the method on two MR image\nsegmentation tasks: cardiac segmentation and prostate segmentation with limited\nlabeled data. Results show that the proposed approach can alleviate the need\nfor labeled data while improving model generalization ability, indicating its\npractical value in medical imaging applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qin_C/0/1/0/all/0/1\">Chen Qin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ouyang_C/0/1/0/all/0/1\">Cheng Ouyang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1\">Zeju Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1\">Shuo Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qiu_H/0/1/0/all/0/1\">Huaqi Qiu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_L/0/1/0/all/0/1\">Liang Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tarroni_G/0/1/0/all/0/1\">Giacomo Tarroni</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bai_W/0/1/0/all/0/1\">Wenjia Bai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rueckert_D/0/1/0/all/0/1\">Daniel Rueckert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LGD: Label-guided Self-distillation for Object Detection. (arXiv:2109.11496v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.11496","description":"<p>In this paper, we propose the first self-distillation framework for general\nobject detection, termed LGD (Label-Guided self-Distillation). Previous studies\nrely on a strong pretrained teacher to provide instructive knowledge that could\nbe unavailable in real-world scenarios. Instead, we generate an instructive\nknowledge based only on student representations and regular labels. Our\nframework includes sparse label-appearance encoder, inter-object relation\nadapter and intra-object knowledge mapper that jointly form an implicit teacher\nat training phase, dynamically dependent on labels and evolving student\nrepresentations. They are trained end-to-end with detector and discarded in\ninference. Experimentally, LGD obtains decent results on various detectors,\ndatasets, and extensive tasks like instance segmentation. For example in\nMS-COCO dataset, LGD improves RetinaNet with ResNet-50 under 2x single-scale\ntraining from 36.2% to 39.0% mAP (+ 2.8%). It boosts much stronger detectors\nlike FCOS with ResNeXt-101 DCN v2 under 2x multi-scale training from 46.1% to\n47.9% (+ 1.8%). Compared with a classical teacher-based method FGFI, LGD not\nonly performs better without requiring pretrained teacher but also reduces 51%\ntraining cost beyond inherent student learning. Codes are available at\nhttps://github.com/megvii-research/LGD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Peizhen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_Z/0/1/0/all/0/1\">Zijian Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Tong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1\">Nanning Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Study on Transfer Learning Capabilities for Pneumonia Classification in Chest-X-Rays Image. (arXiv:2110.02780v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2110.02780","description":"<p>Over the last year, the severe acute respiratory syndrome coronavirus-2\n(SARS-CoV-2) and its variants have highlighted the importance of screening\ntools with high diagnostic accuracy for new illnesses such as COVID-19. To that\nregard, deep learning approaches have proven as effective solutions for\npneumonia classification, especially when considering chest-x-rays images.\nHowever, this lung infection can also be caused by other viral, bacterial or\nfungi pathogens. Consequently, efforts are being poured toward distinguishing\nthe infection source to help clinicians to diagnose the correct disease origin.\nFollowing this tendency, this study further explores the effectiveness of\nestablished neural network architectures on the pneumonia classification task\nthrough the transfer learning paradigm. To present a comprehensive comparison,\n12 well-known ImageNet pre-trained models were fine-tuned and used to\ndiscriminate among chest-x-rays of healthy people, and those showing pneumonia\nsymptoms derived from either a viral (i.e., generic or SARS-CoV-2) or bacterial\nsource. Furthermore, since a common public collection distinguishing between\nsuch categories is currently not available, two distinct datasets of\nchest-x-rays images, describing the aforementioned sources, were combined and\nemployed to evaluate the various architectures. The experiments were performed\nusing a total of 6330 images split between train, validation and test sets. For\nall models, common classification metrics were computed (e.g., precision,\nf1-score) and most architectures obtained significant performances, reaching,\namong the others, up to 84.46% average f1-score when discriminating the 4\nidentified classes. Moreover, confusion matrices and activation maps computed\nvia the Grad-CAM algorithm were also reported to present an informed discussion\non the networks classifications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Avola_D/0/1/0/all/0/1\">Danilo Avola</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bacciu_A/0/1/0/all/0/1\">Andrea Bacciu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cinque_L/0/1/0/all/0/1\">Luigi Cinque</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fagioli_A/0/1/0/all/0/1\">Alessio Fagioli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Marini_M/0/1/0/all/0/1\">Marco Raoul Marini</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Taiello_R/0/1/0/all/0/1\">Riccardo Taiello</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Network Augmentation for Tiny Deep Learning. (arXiv:2110.08890v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.08890","description":"<p>We introduce Network Augmentation (NetAug), a new training method for\nimproving the performance of tiny neural networks. Existing regularization\ntechniques (e.g., data augmentation, dropout) have shown much success on large\nneural networks by adding noise to overcome over-fitting. However, we found\nthese techniques hurt the performance of tiny neural networks. We argue that\ntraining tiny models are different from large models: rather than augmenting\nthe data, we should augment the model, since tiny models tend to suffer from\nunder-fitting rather than over-fitting due to limited capacity. To alleviate\nthis issue, NetAug augments the network (reverse dropout) instead of inserting\nnoise into the dataset or the network. It puts the tiny model into larger\nmodels and encourages it to work as a sub-model of larger models to get extra\nsupervision, in addition to functioning as an independent model. At test time,\nonly the tiny model is used for inference, incurring zero inference overhead.\nWe demonstrate the effectiveness of NetAug on image classification and object\ndetection. NetAug consistently improves the performance of tiny models,\nachieving up to 2.2% accuracy improvement on ImageNet. On object detection,\nachieving the same level of performance, NetAug requires 41% fewer MACs on\nPascal VOC and 38% fewer MACs on COCO than the baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1\">Han Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1\">Chuang Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Ji Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Song Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Dimensional Collapse in Contrastive Self-supervised Learning. (arXiv:2110.09348v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.09348","description":"<p>Self-supervised visual representation learning aims to learn useful\nrepresentations without relying on human annotations. Joint embedding approach\nbases on maximizing the agreement between embedding vectors from different\nviews of the same image. Various methods have been proposed to solve the\ncollapsing problem where all embedding vectors collapse to a trivial constant\nsolution. Among these methods, contrastive learning prevents collapse via\nnegative sample pairs. It has been shown that non-contrastive methods suffer\nfrom a lesser collapse problem of a different nature: dimensional collapse,\nwhereby the embedding vectors end up spanning a lower-dimensional subspace\ninstead of the entire available embedding space. Here, we show that dimensional\ncollapse also happens in contrastive learning. In this paper, we shed light on\nthe dynamics at play in contrastive learning that leads to dimensional\ncollapse. Inspired by our theory, we propose a novel contrastive learning\nmethod, called DirectCLR, which directly optimizes the representation space\nwithout relying on an explicit trainable projector. Experiments show that\nDirectCLR outperforms SimCLR with a trainable linear projector on ImageNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jing_L/0/1/0/all/0/1\">Li Jing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vincent_P/0/1/0/all/0/1\">Pascal Vincent</a>, <a href=\"http://arxiv.org/find/cs/1/au:+LeCun_Y/0/1/0/all/0/1\">Yann LeCun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yuandong Tian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DBSegment: Fast and robust segmentation of deep brain structures -- Evaluation of transportability across acquisition domains. (arXiv:2110.09473v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2110.09473","description":"<p>Segmenting deep brain structures from magnetic resonance images is important\nfor patient diagnosis, surgical planning, and research. Most current\nstate-of-the-art solutions follow a segmentation-by-registration approach,\nwhere subject MRIs are mapped to a template with well-defined segmentations.\nHowever, registration-based pipelines are time-consuming, thus, limiting their\nclinical use. This paper uses deep learning to provide a robust and efficient\ndeep brain segmentation solution. The method consists of a pre-processing step\nto conform all MRI images to the same orientation, followed by a convolutional\nneural network using the nnU-Net framework. We use a total of 14 datasets from\nboth research and clinical collections. Of these, seven were used for training\nand validation and seven were retained for independent testing. We trained the\nnetwork to segment 30 deep brain structures, as well as a brain mask, using\nlabels generated from a registration-based approach. We evaluated the\ngeneralizability of the network by performing a leave-one-dataset-out\ncross-validation, and extensive testing on external datasets. Furthermore, we\nassessed cross-domain transportability by evaluating the results separately on\ndifferent domains. We achieved an average DSC of 0.89 $\\pm$ 0.04 on the\nindependent testing datasets when compared to the registration-based gold\nstandard. On our test system, the computation time decreased from 42 minutes\nfor a reference registration-based pipeline to 1 minute. Our proposed method is\nfast, robust, and generalizes with high reliability. It can be extended to the\nsegmentation of other brain structures. The method is publicly available on\nGitHub, as well as a pip package for convenient usage.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Baniasadi_M/0/1/0/all/0/1\">Mehri Baniasadi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Petersen_M/0/1/0/all/0/1\">Mikkel V. Petersen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Goncalves_J/0/1/0/all/0/1\">Jorge Goncalves</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Horn_A/0/1/0/all/0/1\">Andreas Horn</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vlasov_V/0/1/0/all/0/1\">Vanja Vlasov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hertel_F/0/1/0/all/0/1\">Frank Hertel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Husch_A/0/1/0/all/0/1\">Andreas Husch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Benchmarks for Corruption Invariant Person Re-identification. (arXiv:2111.00880v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.00880","description":"<p>When deploying person re-identification (ReID) model in safety-critical\napplications, it is pivotal to understanding the robustness of the model\nagainst a diverse array of image corruptions. However, current evaluations of\nperson ReID only consider the performance on clean datasets and ignore images\nin various corrupted scenarios. In this work, we comprehensively establish six\nReID benchmarks for learning corruption invariant representation. In the field\nof ReID, we are the first to conduct an exhaustive study on corruption\ninvariant learning in single- and cross-modality datasets, including\nMarket-1501, CUHK03, MSMT17, RegDB, SYSU-MM01. After reproducing and examining\nthe robustness performance of 21 recent ReID methods, we have some\nobservations: 1) transformer-based models are more robust towards corrupted\nimages, compared with CNN-based models, 2) increasing the probability of random\nerasing (a commonly used augmentation method) hurts model corruption\nrobustness, 3) cross-dataset generalization improves with corruption robustness\nincreases. By analyzing the above observations, we propose a strong baseline on\nboth single- and cross-modality ReID datasets which achieves improved\nrobustness against diverse corruptions. Our codes are available on\nhttps://github.com/MinghuiChen43/CIL-ReID.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Minghui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhiqiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_F/0/1/0/all/0/1\">Feng Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are we ready for a new paradigm shift? A Survey on Visual Deep MLP. (arXiv:2111.04060v6 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.04060","description":"<p>Recently, the proposed deep MLP models have stirred up a lot of interest in\nthe vision community. Historically, the availability of larger datasets\ncombined with increased computing capacity leads to paradigm shifts. This\nreview paper provides detailed discussions on whether MLP can be a new paradigm\nfor computer vision. We compare the intrinsic connections and differences\nbetween convolution, self-attention mechanism, and Token-mixing MLP in detail.\nAdvantages and limitations of Token-mixing MLP are provided, followed by\ncareful analysis of recent MLP-like variants, from module design to network\narchitecture, and their applications. In the GPU era, the locally and globally\nweighted summations are the current mainstreams, represented by the convolution\nand self-attention mechanism, as well as MLP. We suggest the further\ndevelopment of paradigm to be considered alongside the next-generation\ncomputing devices.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Ruiyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yinghui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_L/0/1/0/all/0/1\">Linmi Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1\">Dun Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Hai-Tao Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stochastic Variance Reduced Ensemble Adversarial Attack for Boosting the Adversarial Transferability. (arXiv:2111.10752v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2111.10752","description":"<p>The black-box adversarial attack has attracted impressive attention for its\npractical use in the field of deep learning security. Meanwhile, it is very\nchallenging as there is no access to the network architecture or internal\nweights of the target model. Based on the hypothesis that if an example remains\nadversarial for multiple models, then it is more likely to transfer the attack\ncapability to other models, the ensemble-based adversarial attack methods are\nefficient and widely used for black-box attacks. However, ways of ensemble\nattack are rather less investigated, and existing ensemble attacks simply fuse\nthe outputs of all the models evenly. In this work, we treat the iterative\nensemble attack as a stochastic gradient descent optimization process, in which\nthe variance of the gradients on different models may lead to poor local\noptima. To this end, we propose a novel attack method called the stochastic\nvariance reduced ensemble (SVRE) attack, which could reduce the gradient\nvariance of the ensemble models and take full advantage of the ensemble attack.\nEmpirical results on the standard ImageNet dataset demonstrate that the\nproposed method could boost the adversarial transferability and outperforms\nexisting ensemble attacks significantly. Code is available at\nhttps://github.com/JHL-HUST/SVRE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1\">Yifeng Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jiadong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hopcroft_J/0/1/0/all/0/1\">John E. Hopcroft</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1\">Kun He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Decentralized Unsupervised Learning of Visual Representations. (arXiv:2111.10763v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2111.10763","description":"<p>Collaborative learning enables distributed clients to learn a shared model\nfor prediction while keeping the training data local on each client. However,\nexisting collaborative learning methods require fully-labeled data for\ntraining, which is inconvenient or sometimes infeasible to obtain due to the\nhigh labeling cost and the requirement of expertise. The lack of labels makes\ncollaborative learning impractical in many realistic settings. Self-supervised\nlearning can address this challenge by learning from unlabeled data.\nContrastive learning (CL), a self-supervised learning approach, can effectively\nlearn visual representations from unlabeled image data. However, the\ndistributed data collected on clients are usually not independent and\nidentically distributed (non-IID) among clients, and each client may only have\nfew classes of data, which degrades the performance of CL and learned\nrepresentations. To tackle this problem, we propose a collaborative contrastive\nlearning framework consisting of two approaches: feature fusion and\nneighborhood matching, by which a unified feature space among clients is\nlearned for better data representations. Feature fusion provides remote\nfeatures as accurate contrastive information to each client for better local\nlearning. Neighborhood matching further aligns each client's local features to\nthe remote features such that well-clustered features among clients can be\nlearned. Extensive experiments show the effectiveness of the proposed\nframework. It outperforms other methods by 11% on IID data and matches the\nperformance of centralized learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yawen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhepeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_D/0/1/0/all/0/1\">Dewen Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Meng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yiyu Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jingtong Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"JoinABLe: Learning Bottom-up Assembly of Parametric CAD Joints. (arXiv:2111.12772v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2111.12772","description":"<p>Physical products are often complex assemblies combining a multitude of 3D\nparts modeled in computer-aided design (CAD) software. CAD designers build up\nthese assemblies by aligning individual parts to one another using constraints\ncalled joints. In this paper we introduce JoinABLe, a learning-based method\nthat assembles parts together to form joints. JoinABLe uses the weak\nsupervision available in standard parametric CAD files without the help of\nobject class labels or human guidance. Our results show that by making network\npredictions over a graph representation of solid models we can outperform\nmultiple baseline methods with an accuracy (79.53%) that approaches human\nperformance (80%). Finally, to support future research we release the Fusion\n360 Gallery assembly dataset, containing assemblies with rich information on\njoints, contact surfaces, holes, and the underlying assembly graph structure.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Willis_K/0/1/0/all/0/1\">Karl D.D. Willis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jayaraman_P/0/1/0/all/0/1\">Pradeep Kumar Jayaraman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_H/0/1/0/all/0/1\">Hang Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yunsheng Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yifei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grandi_D/0/1/0/all/0/1\">Daniele Grandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanghi_A/0/1/0/all/0/1\">Aditya Sanghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_L/0/1/0/all/0/1\">Linh Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lambourne_J/0/1/0/all/0/1\">Joseph G. Lambourne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Solar_Lezama_A/0/1/0/all/0/1\">Armando Solar-Lezama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matusik_W/0/1/0/all/0/1\">Wojciech Matusik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning from Temporal Gradient for Semi-supervised Action Recognition. (arXiv:2111.13241v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.13241","description":"<p>Semi-supervised video action recognition tends to enable deep neural networks\nto achieve remarkable performance even with very limited labeled data. However,\nexisting methods are mainly transferred from current image-based methods (e.g.,\nFixMatch). Without specifically utilizing the temporal dynamics and inherent\nmultimodal attributes, their results could be suboptimal. To better leverage\nthe encoded temporal information in videos, we introduce temporal gradient as\nan additional modality for more attentive feature extraction in this paper. To\nbe specific, our method explicitly distills the fine-grained motion\nrepresentations from temporal gradient (TG) and imposes consistency across\ndifferent modalities (i.e., RGB and TG). The performance of semi-supervised\naction recognition is significantly improved without additional computation or\nparameters during inference. Our method achieves the state-of-the-art\nperformance on three video action recognition benchmarks (i.e., Kinetics-400,\nUCF-101, and HMDB-51) under several typical semi-supervised settings (i.e.,\ndifferent ratios of labeled data).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Junfei Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jing_L/0/1/0/all/0/1\">Longlong Jing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Ju He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+She_Q/0/1/0/all/0/1\">Qi She</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zongwei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1\">Alan Yuille</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yingwei Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Batch Normalization Tells You Which Filter is Important. (arXiv:2112.01155v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.01155","description":"<p>The goal of filter pruning is to search for unimportant filters to remove in\norder to make convolutional neural networks (CNNs) efficient without\nsacrificing the performance in the process. The challenge lies in finding\ninformation that can help determine how important or relevant each filter is\nwith respect to the final output of neural networks. In this work, we share our\nobservation that the batch normalization (BN) parameters of pre-trained CNNs\ncan be used to estimate the feature distribution of activation outputs, without\nprocessing of training data. Upon observation, we propose a simple yet\neffective filter pruning method by evaluating the importance of each filter\nbased on the BN parameters of pre-trained CNNs. The experimental results on\nCIFAR-10 and ImageNet demonstrate that the proposed method can achieve\noutstanding performance with and without fine-tuning in terms of the trade-off\nbetween the accuracy drop and the reduction in computational complexity and\nnumber of parameters of pruned networks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1\">Junghun Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Heewon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baik_S/0/1/0/all/0/1\">Sungyong Baik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_C/0/1/0/all/0/1\">Cheeun Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kyoung Mu Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transferring Unconditional to Conditional GANs with Hyper-Modulation. (arXiv:2112.02219v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.02219","description":"<p>GANs have matured in recent years and are able to generate high-resolution,\nrealistic images. However, the computational resources and the data required\nfor the training of high-quality GANs are enormous, and the study of transfer\nlearning of these models is therefore an urgent topic. Many of the available\nhigh-quality pretrained GANs are unconditional (like StyleGAN). For many\napplications, however, conditional GANs are preferable, because they provide\nmore control over the generation process, despite often suffering more training\ndifficulties. Therefore, in this paper, we focus on transferring from\nhigh-quality pretrained unconditional GANs to conditional GANs. This requires\narchitectural adaptation of the pretrained GAN to perform the conditioning. To\nthis end, we propose hyper-modulated generative networks that allow for shared\nand complementary supervision. To prevent the additional weights of the\nhypernetwork to overfit, with subsequent mode collapse on small target domains,\nwe introduce a self-initialization procedure that does not require any real\ndata to initialize the hypernetwork parameters. To further improve the sample\nefficiency of the transfer, we apply contrastive learning in the discriminator,\nwhich effectively works on very limited batch sizes. In extensive experiments,\nwe validate the efficiency of the hypernetworks, self-initialization and\ncontrastive loss for knowledge transfer on standard benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Laria_H/0/1/0/all/0/1\">H&#xe9;ctor Laria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaxing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weijer_J/0/1/0/all/0/1\">Joost van de Weijer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raducanu_B/0/1/0/all/0/1\">Bogdan Raducanu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Constrained Adaptive Projection with Pretrained Features for Anomaly Detection. (arXiv:2112.02597v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.02597","description":"<p>Anomaly detection aims to separate anomalies from normal samples, and the\npretrained network is promising for anomaly detection. However, adapting the\npretrained features would be confronted with the risk of pattern collapse when\nfinetuning on one-class training data. In this paper, we propose an anomaly\ndetection framework called constrained adaptive projection with pretrained\nfeatures (CAP). Combined with pretrained features, a simple linear projection\nhead applied on a specific input and its k most similar pretrained normal\nrepresentations is designed for feature adaptation, and a reformed\nself-attention is leveraged to mine the inner-relationship among one-class\nsemantic features. A loss function is proposed to avoid potential pattern\ncollapse. Concretely, it considers the similarity between a specific data and\nits corresponding adaptive normal representation, and incorporates a constraint\nterm slightly aligning pretrained and adaptive spaces. Our method achieves\nstate-ofthe-art anomaly detection performance on semantic anomaly detection and\nsensory anomaly detection benchmarks including 96.5% AUROC on CIFAR- 100\ndataset, 97.0% AUROC on CIFAR-10 dataset and 89.9% AUROC on MvTec dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gui_X/0/1/0/all/0/1\">Xingtai Gui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Di Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yang Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_S/0/1/0/all/0/1\">Shicai Fan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Auxiliary Learning for Self-Supervised Video Representation via Similarity-based Knowledge Distillation. (arXiv:2112.04011v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.04011","description":"<p>Despite the outstanding success of self-supervised pretraining methods for\nvideo representation learning, they generalise poorly when the unlabeled\ndataset for pretraining is small or the domain difference between unlabelled\ndata in source task (pretraining) and labeled data in target task (finetuning)\nis significant. To mitigate these issues, we propose a novel approach to\ncomplement self-supervised pretraining via an auxiliary pretraining phase,\nbased on knowledge similarity distillation, auxSKD, for better generalisation\nwith a significantly smaller amount of video data, e.g. Kinetics-100 rather\nthan Kinetics-400. Our method deploys a teacher network that iteratively\ndistills its knowledge to the student model by capturing the similarity\ninformation between segments of unlabelled video data. The student model\nmeanwhile solves a pretext task by exploiting this prior knowledge. We also\nintroduce a novel pretext task, Video Segment Pace Prediction or VSPP, which\nrequires our model to predict the playback speed of a randomly selected segment\nof the input video to provide more reliable self-supervised representations.\nOur experimental results show superior results to the state of the art on both\nUCF101 and HMDB51 datasets when pretraining on K100 in apple-to-apple\ncomparisons. Additionally, we show that our auxiliary pretraining, auxSKD, when\nadded as an extra pretraining phase to recent state of the art self-supervised\nmethods (i.e. VCOP, VideoPace, and RSPNet), improves their results on UCF101\nand HMDB51. Our code is available at https://github.com/Plrbear/auxSKD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dadashzadeh_A/0/1/0/all/0/1\">Amirhossein Dadashzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Whone_A/0/1/0/all/0/1\">Alan Whone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirmehdi_M/0/1/0/all/0/1\">Majid Mirmehdi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extending the WILDS Benchmark for Unsupervised Adaptation. (arXiv:2112.05090v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2112.05090","description":"<p>Machine learning systems deployed in the wild are often trained on a source\ndistribution but deployed on a different target distribution. Unlabeled data\ncan be a powerful point of leverage for mitigating these distribution shifts,\nas it is frequently much more available than labeled data and can often be\nobtained from distributions beyond the source distribution as well. However,\nexisting distribution shift benchmarks with unlabeled data do not reflect the\nbreadth of scenarios that arise in real-world applications. In this work, we\npresent the WILDS 2.0 update, which extends 8 of the 10 datasets in the WILDS\nbenchmark of distribution shifts to include curated unlabeled data that would\nbe realistically obtainable in deployment. These datasets span a wide range of\napplications (from histology to wildlife conservation), tasks (classification,\nregression, and detection), and modalities (photos, satellite images,\nmicroscope slides, text, molecular graphs). The update maintains consistency\nwith the original WILDS benchmark by using identical labeled training,\nvalidation, and test sets, as well as the evaluation metrics. On these\ndatasets, we systematically benchmark state-of-the-art methods that leverage\nunlabeled data, including domain-invariant, self-training, and self-supervised\nmethods, and show that their success on WILDS is limited. To facilitate method\ndevelopment and evaluation, we provide an open-source package that automates\ndata loading and contains all of the model architectures and methods used in\nthis paper. Code and leaderboards are available at https://wilds.stanford.edu.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sagawa_S/0/1/0/all/0/1\">Shiori Sagawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koh_P/0/1/0/all/0/1\">Pang Wei Koh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_T/0/1/0/all/0/1\">Tony Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_I/0/1/0/all/0/1\">Irena Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1\">Sang Michael Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_K/0/1/0/all/0/1\">Kendrick Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Ananya Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Weihua Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yasunaga_M/0/1/0/all/0/1\">Michihiro Yasunaga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marklund_H/0/1/0/all/0/1\">Henrik Marklund</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beery_S/0/1/0/all/0/1\">Sara Beery</a>, <a href=\"http://arxiv.org/find/cs/1/au:+David_E/0/1/0/all/0/1\">Etienne David</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stavness_I/0/1/0/all/0/1\">Ian Stavness</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1\">Wei Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leskovec_J/0/1/0/all/0/1\">Jure Leskovec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1\">Kate Saenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashimoto_T/0/1/0/all/0/1\">Tatsunori Hashimoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1\">Sergey Levine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1\">Chelsea Finn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Percy Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EMDS-6: Environmental Microorganism Image Dataset Sixth Version for Image Denoising, Segmentation, Feature Extraction, Classification and Detection Methods Evaluation. (arXiv:2112.07111v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.07111","description":"<p>Environmental microorganisms (EMs) are ubiquitous around us and have an\nimportant impact on the survival and development of human society. However, the\nhigh standards and strict requirements for the preparation of environmental\nmicroorganism (EM) data have led to the insufficient of existing related\ndatabases, not to mention the databases with GT images. This problem seriously\naffects the progress of related experiments. Therefore, This study develops the\nEnvironmental Microorganism Dataset Sixth Version (EMDS-6), which contains 21\ntypes of EMs. Each type of EM contains 40 original and 40 GT images, in total\n1680 EM images. In this study, in order to test the effectiveness of EMDS-6. We\nchoose the classic algorithms of image processing methods such as image\ndenoising, image segmentation and target detection. The experimental result\nshows that EMDS-6 can be used to evaluate the performance of image denoising,\nimage segmentation, image feature extraction, image classification, and object\ndetection methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1\">Peng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahaman_M/0/1/0/all/0/1\">Md Mamunur Rahaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_P/0/1/0/all/0/1\">Pingli Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hechen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Hongzan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1\">Tao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_N/0/1/0/all/0/1\">Ning Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grzegorzek_M/0/1/0/all/0/1\">Marcin Grzegorzek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HarmoFL: Harmonizing Local and Global Drifts in Federated Learning on Heterogeneous Medical Images. (arXiv:2112.10775v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2112.10775","description":"<p>Multiple medical institutions collaboratively training a model using\nfederated learning (FL) has become a promising solution for maximizing the\npotential of data-driven models, yet the non-independent and identically\ndistributed (non-iid) data in medical images is still an outstanding challenge\nin real-world practice. The feature heterogeneity caused by diverse scanners or\nprotocols introduces a drift in the learning process, in both local (client)\nand global (server) optimizations, which harms the convergence as well as model\nperformance. Many previous works have attempted to address the non-iid issue by\ntackling the drift locally or globally, but how to jointly solve the two\nessentially coupled drifts is still unclear. In this work, we concentrate on\nhandling both local and global drifts and introduce a new harmonizing framework\ncalled HarmoFL. First, we propose to mitigate the local update drift by\nnormalizing amplitudes of images transformed into the frequency domain to mimic\na unified imaging setting, in order to generate a harmonized feature space\nacross local clients. Second, based on harmonized features, we design a client\nweight perturbation guiding each local model to reach a flat optimum, where a\nneighborhood area of the local optimal solution has a uniformly low loss.\nWithout any extra communication cost, the perturbation assists the global model\nto optimize towards a converged optimal solution by aggregating several local\nflat optima. We have theoretically analyzed the proposed method and empirically\nconducted extensive experiments on three medical image classification and\nsegmentation tasks, showing that HarmoFL outperforms a set of recent\nstate-of-the-art methods with promising convergence behavior. Code is available\nat https://github.com/med-air/HarmoFL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Jiang_M/0/1/0/all/0/1\">Meirui Jiang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1\">Zirui Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dou_Q/0/1/0/all/0/1\">Qi Dou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Expansion-Squeeze-Excitation Fusion Network for Elderly Activity Recognition. (arXiv:2112.10992v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.10992","description":"<p>This work focuses on the task of elderly activity recognition, which is a\nchallenging task due to the existence of individual actions and human-object\ninteractions in elderly activities. Thus, we attempt to effectively aggregate\nthe discriminative information of actions and interactions from both RGB videos\nand skeleton sequences by attentively fusing multi-modal features. Recently,\nsome nonlinear multi-modal fusion approaches are proposed by utilizing\nnonlinear attention mechanism that is extended from Squeeze-and-Excitation\nNetworks (SENet). Inspired by this, we propose a novel\nExpansion-Squeeze-Excitation Fusion Network (ESE-FN) to effectively address the\nproblem of elderly activity recognition, which learns modal and channel-wise\nExpansion-Squeeze-Excitation (ESE) attentions for attentively fusing the\nmulti-modal features in the modal and channel-wise ways. Furthermore, we design\na new Multi-modal Loss (ML) to keep the consistency between the single-modal\nfeatures and the fused multi-modal features by adding the penalty of difference\nbetween the minimum prediction losses on single modalities and the prediction\nloss on the fused modality. Finally, we conduct experiments on a largest-scale\nelderly activity dataset, i.e., ETRI-Activity3D (including 110,000+ videos, and\n50+ categories), to demonstrate that the proposed ESE-FN achieves the best\naccuracy compared with the state-of-the-art methods. In addition, more\nextensive experimental results show that the proposed ESE-FN is also comparable\nto the other methods in terms of normal action recognition task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shu_X/0/1/0/all/0/1\">Xiangbo Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jiawen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1\">Rui Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yan Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision Transformer Slimming: Multi-Dimension Searching in Continuous Optimization Space. (arXiv:2201.00814v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.00814","description":"<p>This paper explores the feasibility of finding an optimal sub-model from a\nvision transformer and introduces a pure vision transformer slimming (ViT-Slim)\nframework. It can search a sub-structure from the original model end-to-end\nacross multiple dimensions, including the input tokens, MHSA and MLP modules\nwith state-of-the-art performance. Our method is based on a learnable and\nunified $\\ell_1$ sparsity constraint with pre-defined factors to reflect the\nglobal importance in the continuous searching space of different dimensions.\nThe searching process is highly efficient through a single-shot training\nscheme. For instance, on DeiT-S, ViT-Slim only takes ~43 GPU hours for the\nsearching process, and the searched structure is flexible with diverse\ndimensionalities in different modules. Then, a budget threshold is employed\naccording to the requirements of accuracy-FLOPs trade-off on running devices,\nand a re-training process is performed to obtain the final model. The extensive\nexperiments show that our ViT-Slim can compress up to 40% of parameters and 40%\nFLOPs on various vision transformers while increasing the accuracy by ~0.6% on\nImageNet. We also demonstrate the advantage of our searched models on several\ndownstream datasets. Our code is available at\nhttps://github.com/Arnav0400/ViT-Slim.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chavan_A/0/1/0/all/0/1\">Arnav Chavan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zhiqiang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhuang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zechun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_K/0/1/0/all/0/1\">Kwang-Ting Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1\">Eric Xing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multiview Transformers for Video Recognition. (arXiv:2201.04288v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.04288","description":"<p>Video understanding requires reasoning at multiple spatiotemporal resolutions\n-- from short fine-grained motions to events taking place over longer\ndurations. Although transformer architectures have recently advanced the\nstate-of-the-art, they have not explicitly modelled different spatiotemporal\nresolutions. To this end, we present Multiview Transformers for Video\nRecognition (MTV). Our model consists of separate encoders to represent\ndifferent views of the input video with lateral connections to fuse information\nacross views. We present thorough ablation studies of our model and show that\nMTV consistently performs better than single-view counterparts in terms of\naccuracy and computational cost across a range of model sizes. Furthermore, we\nachieve state-of-the-art results on six standard datasets, and improve even\nfurther with large-scale pretraining. Code and checkpoints are available at:\n\\href{https://github.com/google-research/scenic/tree/main/scenic/projects/mtv}{https://github.com/google-research/scenic}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Shen Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_X/0/1/0/all/0/1\">Xuehan Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arnab_A/0/1/0/all/0/1\">Anurag Arnab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhichao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Chen Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1\">Cordelia Schmid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TransBTSV2: Towards Better and More Efficient Volumetric Segmentation of Medical Images. (arXiv:2201.12785v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2201.12785","description":"<p>Transformer, benefiting from global (long-range) information modeling using\nself-attention mechanism, has been successful in natural language processing\nand computer vision recently. Convolutional Neural Networks, capable of\ncapturing local features, are difficult to model explicit long-distance\ndependencies from global feature space. However, both local and global features\nare crucial for dense prediction tasks, especially for 3D medical image\nsegmentation. In this paper, we present the further attempt to exploit\nTransformer in 3D CNN for 3D medical image volumetric segmentation and propose\na novel network named TransBTSV2 based on the encoder-decoder structure.\nDifferent from TransBTS, the proposed TransBTSV2 is not limited to brain tumor\nsegmentation (BTS) but focuses on general medical image segmentation, providing\na stronger and more efficient 3D baseline for volumetric segmentation of\nmedical images. As a hybrid CNN-Transformer architecture, TransBTSV2 can\nachieve accurate segmentation of medical images without any pre-training,\npossessing the strong inductive bias as CNNs and powerful global context\nmodeling ability as Transformer. With the proposed insight to redesign the\ninternal structure of Transformer block and the introduced Deformable\nBottleneck Module to capture shape-aware local details, a highly efficient\narchitecture is achieved with superior performance. Extensive experimental\nresults on four medical image datasets (BraTS 2019, BraTS 2020, LiTS 2017 and\nKiTS 2019) demonstrate that TransBTSV2 achieves comparable or better results\ncompared to the state-of-the-art methods for the segmentation of brain tumor,\nliver tumor as well as kidney tumor. Code will be publicly available at\nhttps://github.com/Wenxuan-1119/TransBTS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Jiangyun Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_W/0/1/0/all/0/1\">Wenxuan Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_T/0/1/0/all/0/1\">Tianxiang Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zha_S/0/1/0/all/0/1\">Sen Zha</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jing Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_H/0/1/0/all/0/1\">Hong Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning the Pedestrian-Vehicle Interaction for Pedestrian Trajectory Prediction. (arXiv:2202.05334v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.05334","description":"<p>In this paper, we study the interaction between pedestrians and vehicles and\npropose a novel neural network structure called the Pedestrian-Vehicle\nInteraction (PVI) extractor for learning the pedestrian-vehicle interaction. We\nimplement the proposed PVI extractor on both sequential approaches (long\nshort-term memory (LSTM) models) and non-sequential approaches (convolutional\nmodels). We use the Waymo Open Dataset that contains real-world urban traffic\nscenes with both pedestrian and vehicle annotations. For the LSTM-based models,\nour proposed model is compared with Social-LSTM and Social-GAN, and using our\nproposed PVI extractor reduces the average displacement error (ADE) and the\nfinal displacement error (FDE) by 7.46% and 5.24%, respectively. For the\nconvolutional-based models, our proposed model is compared with Social-STGCNN\nand Social-IWSTCNN, and using our proposed PVI extractor reduces the ADE and\nFDE by 2.10% and 1.27%, respectively. The results show that the\npedestrian-vehicle interaction influences pedestrian behavior, and the models\nusing the proposed PVI extractor can capture the interaction between\npedestrians and vehicles, and thereby outperform the compared methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chi Zhang</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Berger_C/0/1/0/all/0/1\">Christian Berger</a> (1) ((1) Department of Computer Science and Engineering, University of Gothenburg, Gothenburg, Sweden)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A State-of-the-art Survey of U-Net in Microscopic Image Analysis: from Simple Usage to Structure Mortification. (arXiv:2202.06465v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2202.06465","description":"<p>Image analysis technology is used to solve the inadvertences of artificial\ntraditional methods in disease, wastewater treatment, environmental change\nmonitoring analysis and convolutional neural networks (CNN) play an important\nrole in microscopic image analysis. An important step in detection, tracking,\nmonitoring, feature extraction, modeling and analysis is image segmentation, in\nwhich U-Net has increasingly applied in microscopic image segmentation. This\npaper comprehensively reviews the development history of U-Net, and analyzes\nvarious research results of various segmentation methods since the emergence of\nU-Net and conducts a comprehensive review of related papers. First, this paper\nhas summarized the improved methods of U-Net and then listed the existing\nsignificance of image segmentation techniques and their improvements that has\nintroduced over the years. Finally, focusing on the different improvement\nstrategies of U-Net in different papers, the related work of each application\ntarget is reviewed according to detailed technical categories to facilitate\nfuture research. Researchers can clearly see the dynamics of transmission of\ntechnological development and keep up with future trends in this\ninterdisciplinary field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wu_J/0/1/0/all/0/1\">Jian Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_W/0/1/0/all/0/1\">Wanli Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_C/0/1/0/all/0/1\">Chen Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jiang_T/0/1/0/all/0/1\">Tao Jiang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shariful_I/0/1/0/all/0/1\">Islam Mohammad Shariful</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_H/0/1/0/all/0/1\">Hongzan Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1\">Xiaoqi Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1\">Xintong Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_X/0/1/0/all/0/1\">Xinyu Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Grzegorzek_M/0/1/0/all/0/1\">Marcin Grzegorzek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SemEval 2022 Task 12: Symlink- Linking Mathematical Symbols to their Descriptions. (arXiv:2202.09695v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.09695","description":"<p>Given the increasing number of livestreaming videos, automatic speech\nrecognition and post-processing for livestreaming video transcripts are crucial\nfor efficient data management as well as knowledge mining. A key step in this\nprocess is punctuation restoration which restores fundamental text structures\nsuch as phrase and sentence boundaries from the video transcripts. This work\npresents a new human-annotated corpus, called BehancePR, for punctuation\nrestoration in livestreaming video transcripts. Our experiments on BehancePR\ndemonstrate the challenges of punctuation restoration for this domain.\nFurthermore, we show that popular natural language processing toolkits are\nincapable of detecting sentence boundary on non-punctuated transcripts of\nlivestreaming videos, calling for more research effort to develop robust models\nfor this area.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lai_V/0/1/0/all/0/1\">Viet Dac Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Veyseh_A/0/1/0/all/0/1\">Amir Pouran Ben Veyseh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dernoncourt_F/0/1/0/all/0/1\">Franck Dernoncourt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Thien Huu Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FreeSOLO: Learning to Segment Objects without Annotations. (arXiv:2202.12181v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.12181","description":"<p>Instance segmentation is a fundamental vision task that aims to recognize and\nsegment each object in an image. However, it requires costly annotations such\nas bounding boxes and segmentation masks for learning. In this work, we propose\na fully unsupervised learning method that learns class-agnostic instance\nsegmentation without any annotations. We present FreeSOLO, a self-supervised\ninstance segmentation framework built on top of the simple instance\nsegmentation method SOLO. Our method also presents a novel localization-aware\npre-training framework, where objects can be discovered from complicated scenes\nin an unsupervised manner. FreeSOLO achieves 9.8% AP_{50} on the challenging\nCOCO dataset, which even outperforms several segmentation proposal methods that\nuse manual annotations. For the first time, we demonstrate unsupervised\nclass-agnostic instance segmentation successfully. FreeSOLO's box localization\nsignificantly outperforms state-of-the-art unsupervised object\ndetection/discovery methods, with about 100% relative improvements in COCO AP.\nFreeSOLO further demonstrates superiority as a strong pre-training method,\noutperforming state-of-the-art self-supervised pre-training methods by +9.8% AP\nwhen fine-tuning instance segmentation with only 5% COCO masks. Code is\navailable at: github.com/NVlabs/FreeSOLO\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinlong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhiding Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mello_S/0/1/0/all/0/1\">Shalini De Mello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kautz_J/0/1/0/all/0/1\">Jan Kautz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1\">Anima Anandkumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alvarez_J/0/1/0/all/0/1\">Jose M. Alvarez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"QOC: Quantum On-Chip Training with Parameter Shift and Gradient Pruning. (arXiv:2202.13239v2 [quant-ph] UPDATED)","link":"http://arxiv.org/abs/2202.13239","description":"<p>Parameterized Quantum Circuits (PQC) are drawing increasing research interest\nthanks to its potential to achieve quantum advantages on near-term Noisy\nIntermediate Scale Quantum (NISQ) hardware. In order to achieve scalable PQC\nlearning, the training process needs to be offloaded to real quantum machines\ninstead of using exponential-cost classical simulators. One common approach to\nobtain PQC gradients is parameter shift whose cost scales linearly with the\nnumber of qubits. We present QOC, the first experimental demonstration of\npractical on-chip PQC training with parameter shift. Nevertheless, we find that\ndue to the significant quantum errors (noises) on real machines, gradients\nobtained from naive parameter shift have low fidelity and thus degrading the\ntraining accuracy. To this end, we further propose probabilistic gradient\npruning to firstly identify gradients with potentially large errors and then\nremove them. Specifically, small gradients have larger relative errors than\nlarge ones, thus having a higher probability to be pruned. We perform extensive\nexperiments with the Quantum Neural Network (QNN) benchmarks on 5\nclassification tasks using 5 real quantum machines. The results demonstrate\nthat our on-chip training achieves over 90% and 60% accuracy for 2-class and\n4-class image classification tasks. The probabilistic gradient pruning brings\nup to 7% PQC accuracy improvements over no pruning. Overall, we successfully\nobtain similar on-chip training accuracy compared with noise-free simulation\nbut have much better training scalability. The QOC code is available in the\nTorchQuantum library.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/quant-ph/1/au:+Wang_H/0/1/0/all/0/1\">Hanrui Wang</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Li_Z/0/1/0/all/0/1\">Zirui Li</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Gu_J/0/1/0/all/0/1\">Jiaqi Gu</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Ding_Y/0/1/0/all/0/1\">Yongshan Ding</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Pan_D/0/1/0/all/0/1\">David Z. Pan</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Han_S/0/1/0/all/0/1\">Song Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MixSTE: Seq2seq Mixed Spatio-Temporal Encoder for 3D Human Pose Estimation in Video. (arXiv:2203.00859v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.00859","description":"<p>Recent transformer-based solutions have been introduced to estimate 3D human\npose from 2D keypoint sequence by considering body joints among all frames\nglobally to learn spatio-temporal correlation. We observe that the motions of\ndifferent joints differ significantly. However, the previous methods cannot\nefficiently model the solid inter-frame correspondence of each joint, leading\nto insufficient learning of spatial-temporal correlation. We propose MixSTE\n(Mixed Spatio-Temporal Encoder), which has a temporal transformer block to\nseparately model the temporal motion of each joint and a spatial transformer\nblock to learn inter-joint spatial correlation. These two blocks are utilized\nalternately to obtain better spatio-temporal feature encoding. In addition, the\nnetwork output is extended from the central frame to entire frames of the input\nvideo, thereby improving the coherence between the input and output sequences.\nExtensive experiments are conducted on three benchmarks (Human3.6M,\nMPI-INF-3DHP, and HumanEva). The results show that our model outperforms the\nstate-of-the-art approach by 10.9% P-MPJPE and 7.6% MPJPE. The code is\navailable at https://github.com/JinluZhang1126/MixSTE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jinlu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhigang Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yujin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Junsong Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Where Does the Performance Improvement Come From? -- A Reproducibility Concern about Image-Text Retrieval. (arXiv:2203.03853v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2203.03853","description":"<p>This article aims to provide the information retrieval community with some\nreflections on recent advances in retrieval learning by analyzing the\nreproducibility of image-text retrieval models. Due to the increase of\nmultimodal data over the last decade, image-text retrieval has steadily become\na major research direction in the field of information retrieval. Numerous\nresearchers train and evaluate image-text retrieval algorithms using benchmark\ndatasets such as MS-COCO and Flickr30k. Research in the past has mostly focused\non performance, with multiple state-of-the-art methodologies being suggested in\na variety of ways. According to their assertions, these techniques provide\nimproved modality interactions and hence more precise multimodal\nrepresentations. In contrast to previous works, we focus on the reproducibility\nof the approaches and the examination of the elements that lead to improved\nperformance by pretrained and nonpretrained models in retrieving images and\ntext. To be more specific, we first examine the related reproducibility\nconcerns and explain why our focus is on image-text retrieval tasks. Second, we\nsystematically summarize the current paradigm of image-text retrieval models\nand the stated contributions of those approaches. Third, we analyze various\naspects of the reproduction of pretrained and nonpretrained retrieval models.\nTo complete this, we conducted ablation experiments and obtained some\ninfluencing factors that affect retrieval recall more than the improvement\nclaimed in the original paper. Finally, we present some reflections and\nchallenges that the retrieval community should consider in the future. Our\nsource code is publicly available at\nhttps://github.com/WangFei-2019/Image-text-Retrieval.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rao_J/0/1/0/all/0/1\">Jun Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1\">Liang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_S/0/1/0/all/0/1\">Shuhan Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_Y/0/1/0/all/0/1\">Yibing Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weifeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Multi-Task Learning and Online Refinement for Spacecraft Pose Estimation across Domain Gap. (arXiv:2203.04275v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.04275","description":"<p>This work presents Spacecraft Pose Network v2 (SPNv2), a Convolutional Neural\nNetwork (CNN) for pose estimation of noncooperative spacecraft across domain\ngap. SPNv2 is a multi-scale, multi-task CNN which consists of a shared\nmulti-scale feature encoder and multiple prediction heads that perform\ndifferent tasks on a shared feature output. These tasks are all related to\ndetection and pose estimation of a target spacecraft from an image, such as\nprediction of pre-defined satellite keypoints, direct pose regression, and\nbinary segmentation of the satellite foreground. It is shown that by jointly\ntraining on different yet related tasks with extensive data augmentations on\nsynthetic images only, the shared encoder learns features that are common\nacross image domains that have fundamentally different visual characteristics\ncompared to synthetic images. This work also introduces Online Domain\nRefinement (ODR) which refines the parameters of the normalization layers of\nSPNv2 on the target domain images online at deployment. Specifically, ODR\nperforms self-supervised entropy minimization of the predicted satellite\nforeground, thereby improving the CNN's performance on the target domain images\nwithout their pose labels and with minimal computational efforts. The GitHub\nrepository for SPNv2 is available at \\url{https://github.com/tpark94/spnv2}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_T/0/1/0/all/0/1\">Tae Ha Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DAmico_S/0/1/0/all/0/1\">Simone D&#x27;Amico</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Intention-aware Feature Propagation Network for Interactive Segmentation. (arXiv:2203.05145v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.05145","description":"<p>We aim to tackle the problem of point-based interactive segmentation, in\nwhich two key challenges are to infer user's intention correctly and to\npropagate the user-provided annotations to unlabeled regions efficiently. To\naddress those challenges, we propose a novel intention-aware feature\npropagation strategy that performs explicit user intention estimation and\nlearns an efficient click-augmented feature representation for high-resolution\nforeground segmentation. Specifically, we develop a coarse-to-fine sparse\npropagation network for each interactive segmentation step, which consists of a\ncoarse-level network for more effective tracking of user's interest, and a\nfine-level network for zooming to the target object and performing fine-level\nsegmentation. Moreover, we design a new sparse graph network module for both\nlevels to enable efficient long-range propagation of click information.\nExtensive experiments show that our method surpasses the previous\nstate-of-the-art methods on all popular benchmarks, demonstrating its efficacy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chuyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1\">Chuanyang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yongfei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xuming He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Remote Photoplethysmography with Temporal Derivative Modules and Time-Shift Invariant Loss. (arXiv:2203.10882v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.10882","description":"<p>We present a lightweight neural model for remote heart rate estimation\nfocused on the efficient spatio-temporal learning of facial\nphotoplethysmography (PPG) based on i) modelling of PPG dynamics by\ncombinations of multiple convolutional derivatives, and ii) increased\nflexibility of the model to learn possible offsets between the facial video PPG\nand the ground truth. PPG dynamics are modelled by a Temporal Derivative Module\n(TDM) constructed by the incremental aggregation of multiple convolutional\nderivatives, emulating a Taylor series expansion up to the desired order.\nRobustness to ground truth offsets is handled by the introduction of TALOS\n(Temporal Adaptive LOcation Shift), a new temporal loss to train learning-based\nmodels. We verify the effectiveness of our model by reporting accuracy and\nefficiency metrics on the public PURE and UBFC-rPPG datasets. Compared to\nexisting models, our approach shows competitive heart rate estimation accuracy\nwith a much lower number of parameters and lower computational cost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Comas_J/0/1/0/all/0/1\">Joaquim Comas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruiz_A/0/1/0/all/0/1\">Adria Ruiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sukno_F/0/1/0/all/0/1\">Federico Sukno</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Underwater Light Field Retention : Neural Rendering for Underwater Imaging. (arXiv:2203.11006v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.11006","description":"<p>Underwater Image Rendering aims to generate a true-tolife underwater image\nfrom a given clean one, which could be applied to various practical\napplications such as underwater image enhancement, camera filter, and virtual\ngaming. We explore two less-touched but challenging problems in underwater\nimage rendering, namely, i) how to render diverse underwater scenes by a single\nneural network? ii) how to adaptively learn the underwater light fields from\nnatural exemplars, i,e., realistic underwater images? To this end, we propose a\nneural rendering method for underwater imaging, dubbed UWNR (Underwater Neural\nRendering). Specifically, UWNR is a data-driven neural network that implicitly\nlearns the natural degenerated model from authentic underwater images, avoiding\nintroducing erroneous biases by hand-craft imaging models. Compared with\nexisting underwater image generation methods, UWNR utilizes the natural light\nfield to simulate the main characteristics ofthe underwater scene. Thus, it is\nable to synthesize a wide variety ofunderwater images from one clean image with\nvarious realistic underwater images. Extensive experiments demonstrate that our\napproach achieves better visual effects and quantitative metrics over previous\nmethods. Moreover, we adopt UWNR to build an open Large Neural Rendering\nUnderwater Dataset containing various types of water quality, dubbed LNRUD. The\nsource code and LNRUD are available at https: //github.com/Ephemeral182/UWNR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_T/0/1/0/all/0/1\">Tian Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Sixiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1\">Yi Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1\">Erkang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuche Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ImFace: A Nonlinear 3D Morphable Face Model with Implicit Neural Representations. (arXiv:2203.14510v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.14510","description":"<p>Precise representations of 3D faces are beneficial to various computer vision\nand graphics applications. Due to the data discretization and model linearity,\nhowever, it remains challenging to capture accurate identity and expression\nclues in current studies. This paper presents a novel 3D morphable face model,\nnamely ImFace, to learn a nonlinear and continuous space with implicit neural\nrepresentations. It builds two explicitly disentangled deformation fields to\nmodel complex shapes associated with identities and expressions, respectively,\nand designs an improved learning strategy to extend embeddings of expressions\nto allow more diverse changes. We further introduce a Neural Blend-Field to\nlearn sophisticated details by adaptively blending a series of local fields. In\naddition to ImFace, an effective preprocessing pipeline is proposed to address\nthe issue of watertight input requirement in implicit representations, enabling\nthem to work with common facial surfaces for the first time. Extensive\nexperiments are performed to demonstrate the superiority of ImFace.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1\">Mingwu Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hongyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">Di Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liming Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain Invariant Siamese Attention Mask for Small Object Change Detection via Everyday Indoor Robot Navigation. (arXiv:2203.15362v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.15362","description":"<p>The problem of image change detection via everyday indoor robot navigation is\nexplored from a novel perspective of the self-attention technique. Detecting\nsemantically non-distinctive and visually small changes remains a key challenge\nin the robotics community. Intuitively, these small non-distinctive changes may\nbe better handled by the recent paradigm of the attention mechanism, which is\nthe basic idea of this work. However, existing self-attention models require\nsignificant retraining cost per domain, so it is not directly applicable to\nrobotics applications. We propose a new self-attention technique with an\nability of unsupervised on-the-fly domain adaptation, which introduces an\nattention mask into the intermediate layer of an image change detection model,\nwithout modifying the input and output layers of the model. Experiments, in\nwhich an indoor robot aims to detect visually small changes in everyday\nnavigation, demonstrate that our attention technique significantly boosts the\nstate-of-the-art image change detection model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Takeda_K/0/1/0/all/0/1\">Koji Takeda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanaka_K/0/1/0/all/0/1\">Kanji Tanaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakamura_Y/0/1/0/all/0/1\">Yoshimasa Nakamura</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BEVDet4D: Exploit Temporal Cues in Multi-camera 3D Object Detection. (arXiv:2203.17054v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.17054","description":"<p>Single frame data contains finite information which limits the performance of\nthe existing vision-based multi-camera 3D object detection paradigms. For\nfundamentally pushing the performance boundary in this area, a novel paradigm\ndubbed BEVDet4D is proposed to lift the scalable BEVDet paradigm from the\nspatial-only 3D space to the spatial-temporal 4D space. We upgrade the naive\nBEVDet framework with a few modifications just for fusing the feature from the\nprevious frame with the corresponding one in the current frame. In this way,\nwith negligible additional computing budget, we enable BEVDet4D to access the\ntemporal cues by querying and comparing the two candidate features. Beyond\nthis, we simplify the task of velocity prediction by removing the factors of\nego-motion and time in the learning target. As a result, BEVDet4D with robust\ngeneralization performance reduces the velocity error by up to -62.9%. This\nmakes the vision-based methods, for the first time, become comparable with\nthose relied on LiDAR or radar in this aspect. On challenge benchmark nuScenes,\nwe report a new record of 54.5% NDS with the high-performance configuration\ndubbed BEVDet4D-Base, which surpasses the previous leading method BEVDet-Base\nby +7.3% NDS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Junjie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Guan Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Time Lens++: Event-based Frame Interpolation with Parametric Non-linear Flow and Multi-scale Fusion. (arXiv:2203.17191v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.17191","description":"<p>Recently, video frame interpolation using a combination of frame- and\nevent-based cameras has surpassed traditional image-based methods both in terms\nof performance and memory efficiency. However, current methods still suffer\nfrom (i) brittle image-level fusion of complementary interpolation results,\nthat fails in the presence of artifacts in the fused image, (ii) potentially\ntemporally inconsistent and inefficient motion estimation procedures, that run\nfor every inserted frame and (iii) low contrast regions that do not trigger\nevents, and thus cause events-only motion estimation to generate artifacts.\nMoreover, previous methods were only tested on datasets consisting of planar\nand faraway scenes, which do not capture the full complexity of the real world.\nIn this work, we address the above problems by introducing multi-scale\nfeature-level fusion and computing one-shot non-linear inter-frame motion from\nevents and images, which can be efficiently sampled for image warping. We also\ncollect the first large-scale events and frames dataset consisting of more than\n100 challenging scenes with depth variations, captured with a new experimental\nsetup based on a beamsplitter. We show that our method improves the\nreconstruction quality by up to 0.2 dB in terms of PSNR and up to 15% in LPIPS\nscore.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tulyakov_S/0/1/0/all/0/1\">Stepan Tulyakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bochicchio_A/0/1/0/all/0/1\">Alfredo Bochicchio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gehrig_D/0/1/0/all/0/1\">Daniel Gehrig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Georgoulis_S/0/1/0/all/0/1\">Stamatios Georgoulis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuanyou Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scaramuzza_D/0/1/0/all/0/1\">Davide Scaramuzza</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Connect, Not Collapse: Explaining Contrastive Learning for Unsupervised Domain Adaptation. (arXiv:2204.00570v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2204.00570","description":"<p>We consider unsupervised domain adaptation (UDA), where labeled data from a\nsource domain (e.g., photographs) and unlabeled data from a target domain\n(e.g., sketches) are used to learn a classifier for the target domain.\nConventional UDA methods (e.g., domain adversarial training) learn\ndomain-invariant features to improve generalization to the target domain. In\nthis paper, we show that contrastive pre-training, which learns features on\nunlabeled source and target data and then fine-tunes on labeled source data, is\ncompetitive with strong UDA methods. However, we find that contrastive\npre-training does not learn domain-invariant features, diverging from\nconventional UDA intuitions. We show theoretically that contrastive\npre-training can learn features that vary subtantially across domains but still\ngeneralize to the target domain, by disentangling domain and class information.\nOur results suggest that domain invariance is not necessary for UDA. We\nempirically validate our theory on benchmark vision datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_K/0/1/0/all/0/1\">Kendrick Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jones_R/0/1/0/all/0/1\">Robbie Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Ananya Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1\">Sang Michael Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+HaoChen_J/0/1/0/all/0/1\">Jeff Z. HaoChen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1\">Tengyu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Percy Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Optimize Deep Learning Models for Prediction of Gene Mutations Using Unsupervised Clustering. (arXiv:2204.01593v2 [q-bio.QM] UPDATED)","link":"http://arxiv.org/abs/2204.01593","description":"<p>Deep learning has become the mainstream methodological choice for analyzing\nand interpreting whole-slide digital pathology images (WSIs). It is commonly\nassumed that tumor regions carry most predictive information. In this paper, we\nproposed an unsupervised clustering-based multiple-instance learning, and apply\nour method to develop deep-learning models for prediction of gene mutations\nusing WSIs from three cancer types in The Cancer Genome Atlas (TCGA) studies\n(CRC, LUAD, and HNSCC). We showed that unsupervised clustering of image patches\ncould help identify predictive patches, exclude patches lack of predictive\ninformation, and therefore improve prediction on gene mutations in all three\ndifferent cancer types, compared with the WSI based method without selection of\nimage patches and models based on only tumor regions. Additionally, our\nproposed algorithm outperformed two recently published baseline algorithms\nleveraging unsupervised clustering to assist model prediction. The\nunsupervised-clustering-based approach for mutation prediction allows\nidentification of the spatial regions related to mutation of a specific gene\nvia the resolved probability scores, highlighting the heterogeneity of a\npredicted genotype in the tumor microenvironment. Finally, our study also\ndemonstrated that selection of tumor regions of WSIs is not always the best way\nto identify patches for prediction of gene mutations, and other tissue types in\nthe tumor micro-environment may provide better prediction ability for gene\nmutations than tumor tissues.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Chen_Z/0/1/0/all/0/1\">Zihan Chen</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Li_X/0/1/0/all/0/1\">Xingyu Li</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Yang_M/0/1/0/all/0/1\">Miaomiao Yang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Zhang_H/0/1/0/all/0/1\">Hong Zhang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Xu_X/0/1/0/all/0/1\">Xu Steven Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Complex-Valued Autoencoders for Object Discovery. (arXiv:2204.02075v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2204.02075","description":"<p>Object-centric representations form the basis of human perception and enable\nus to reason about the world and to systematically generalize to new settings.\nCurrently, most machine learning work on unsupervised object discovery focuses\non slot-based approaches, which explicitly separate the latent representations\nof individual objects. While the result is easily interpretable, it usually\nrequires the design of involved architectures. In contrast to this, we propose\na distributed approach to object-centric representations: the Complex\nAutoEncoder. Following a coding scheme theorized to underlie object\nrepresentations in biological neurons, its complex-valued activations represent\ntwo messages: their magnitudes express the presence of a feature, while the\nrelative phase differences between neurons express which features should be\nbound together to create joint object representations. We show that this simple\nand efficient approach achieves better reconstruction performance than an\nequivalent real-valued autoencoder on simple multi-object datasets.\nAdditionally, we show that it achieves competitive unsupervised object\ndiscovery performance to a SlotAttention model on two datasets, and manages to\ndisentangle objects in a third dataset where SlotAttention fails - all while\nbeing 7-70 times faster to train.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lowe_S/0/1/0/all/0/1\">Sindy L&#xf6;we</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lippe_P/0/1/0/all/0/1\">Phillip Lippe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudolph_M/0/1/0/all/0/1\">Maja Rudolph</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Welling_M/0/1/0/all/0/1\">Max Welling</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ShowFace: Coordinated Face Inpainting with Memory-Disentangled Refinement Networks. (arXiv:2204.02824v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.02824","description":"<p>Face inpainting aims to complete the corrupted regions of the face images,\nwhich requires coordination between the completed areas and the non-corrupted\nareas. Recently, memory-oriented methods illustrate great prospects in the\ngeneration related tasks by introducing an external memory module to improve\nimage coordination. However, such methods still have limitations in restoring\nthe consistency and continuity for specificfacial semantic parts. In this\npaper, we propose the coarse-to-fine Memory-Disentangled Refinement Networks\n(MDRNets) for coordinated face inpainting, in which two collaborative modules\nare integrated, Disentangled Memory Module (DMM) and Mask-Region Enhanced\nModule (MREM). Specifically, the DMM establishes a group of disentangled memory\nblocks to store the semantic-decoupled face representations, which could\nprovide the most relevant information to refine the semantic-level\ncoordination. The MREM involves a masked correlation mining mechanism to\nenhance the feature relationships into the corrupted regions, which could also\nmake up for the correlation loss caused by memory disentanglement. Furthermore,\nto better improve the inter-coordination between the corrupted and\nnon-corrupted regions and enhance the intra-coordination in corrupted regions,\nwe design InCo2 Loss, a pair of similarity based losses to constrain the\nfeature consistency. Eventually, extensive experiments conducted on CelebA-HQ\nand FFHQ datasets demonstrate the superiority of our MDRNets compared with\nprevious State-Of-The-Art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhuojie Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1\">Xingqun Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zijian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wanting Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_K/0/1/0/all/0/1\">Kun Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Muyi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zhenan Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sampling-based Fast Gradient Rescaling Method for Highly Transferable Adversarial Attacks. (arXiv:2204.02887v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.02887","description":"<p>Deep neural networks have shown to be very vulnerable to adversarial examples\ncrafted by adding human-imperceptible perturbations to benign inputs. After\nachieving impressive attack success rates in the white-box setting, more focus\nis shifted to black-box attacks. In either case, the common gradient-based\napproaches generally use the $sign$ function to generate perturbations at the\nend of the process. However, only a few works pay attention to the limitation\nof the $sign$ function. Deviation between the original gradient and the\ngenerated noises may lead to inaccurate gradient update estimation and\nsuboptimal solutions for adversarial transferability, which is crucial for\nblack-box attacks. To address this issue, we propose a Sampling-based Fast\nGradient Rescaling Method (S-FGRM) to improve the transferability of the\ncrafted adversarial examples. Specifically, we use data rescaling to substitute\nthe inefficient $sign$ function in gradient-based attacks without extra\ncomputational cost. We also propose a Depth First Sampling method to eliminate\nthe fluctuation of rescaling and stabilize the gradient update. Our method can\nbe used in any gradient-based optimizations and is extensible to be integrated\nwith various input transformation or ensemble methods for further improving the\nadversarial transferability. Extensive experiments on the standard ImageNet\ndataset show that our S-FGRM could significantly boost the transferability of\ngradient-based attacks and outperform the state-of-the-art baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Anmin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1\">Yifeng Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1\">Yanbo Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1\">Kun He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Winoground: Probing Vision and Language Models for Visio-Linguistic Compositionality. (arXiv:2204.03162v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.03162","description":"<p>We present a novel task and dataset for evaluating the ability of vision and\nlanguage models to conduct visio-linguistic compositional reasoning, which we\ncall Winoground. Given two images and two captions, the goal is to match them\ncorrectly - but crucially, both captions contain a completely identical set of\nwords, only in a different order. The dataset was carefully hand-curated by\nexpert annotators and is labeled with a rich set of fine-grained tags to assist\nin analyzing model performance. We probe a diverse range of state-of-the-art\nvision and language models and find that, surprisingly, none of them do much\nbetter than chance. Evidently, these models are not as skilled at\nvisio-linguistic compositional reasoning as we might have hoped. We perform an\nextensive analysis to obtain insights into how future work might try to\nmitigate these models' shortcomings. We aim for Winoground to serve as a useful\nevaluation set for advancing the state of the art and driving further progress\nin the field. The dataset is available at\nhttps://huggingface.co/datasets/facebook/winoground.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thrush_T/0/1/0/all/0/1\">Tristan Thrush</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_R/0/1/0/all/0/1\">Ryan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bartolo_M/0/1/0/all/0/1\">Max Bartolo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Amanpreet Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_A/0/1/0/all/0/1\">Adina Williams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiela_D/0/1/0/all/0/1\">Douwe Kiela</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ross_C/0/1/0/all/0/1\">Candace Ross</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HunYuan_tvr for Text-Video Retrieval. (arXiv:2204.03382v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.03382","description":"<p>Text-Video Retrieval plays an important role in multi-modal understanding and\nhas attracted increasing attention in recent years. Most existing methods focus\non constructing contrastive pairs between whole videos and complete caption\nsentences, while ignoring fine-grained cross-modal relationships, e.g., short\nclips and phrases or single frame and word. In this paper, we propose a novel\nmethod, named HunYuan\\_tvr, to explore hierarchical cross-modal interactions by\nsimultaneously exploring video-sentence, clip-phrase, and frame-word\nrelationships. Considering intrinsic semantic relations between frames,\nHunYuan\\_tvr first performs self-attention to explore frame-wise correlations\nand adaptively clusters correlated frames into clip-level representations.\nThen, the clip-wise correlation is explored to aggregate clip representations\ninto a compact one to describe the video globally. In this way, we can\nconstruct hierarchical video representations for frame-clip-video\ngranularities, and also explore word-wise correlations to form\nword-phrase-sentence embeddings for the text modality. Finally, hierarchical\ncontrastive learning is designed to explore cross-modal\nrelationships,~\\emph{i.e.,} frame-word, clip-phrase, and video-sentence, which\nenables HunYuan\\_tvr to achieve a comprehensive multi-modal understanding.\nFurther boosted by adaptive label denoising and marginal sample enhancement,\nHunYuan\\_tvr obtains new state-of-the-art results on various benchmarks, e.g.,\nRank@1 of 55.0%, 57.8%, 29.7%, 52.1%, and 57.3% on MSR-VTT, MSVD, LSMDC,\nDiDemo, and ActivityNet respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Min_S/0/1/0/all/0/1\">Shaobo Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_W/0/1/0/all/0/1\">Weijie Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_R/0/1/0/all/0/1\">Rong-Cheng Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_D/0/1/0/all/0/1\">Dihong Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_C/0/1/0/all/0/1\">Chengfei Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wenzhe Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chenyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Sixiao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongfa Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhifeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Incremental Prototype Prompt-tuning with Pre-trained Representation for Class Incremental Learning. (arXiv:2204.03410v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.03410","description":"<p>Class incremental learning has attracted much attention, but most existing\nworks still continually fine-tune the entire representation model, inevitably\nresulting in much catastrophic forgetting. Instead of struggling to fight\nagainst such forgetting by replaying or distillation like most of the existing\nmethods, we take a novel pre-train-and-prompt-tuning paradigm to sequentially\nlearn new visual concepts based on a fixed semantic-rich pre-trained\nrepresentation model. In detail, we incrementally prompt-tune category\nprototypes for classification and example prototypes to compensate for semantic\ndrift, the problem caused by learning bias at different phases. Extensive\nexperiments conducted on the mainstream incremental learning benchmarks\ndemonstrate that our method outperforms other state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jieren Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jianhua Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haojian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunkuan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision-and-Language Pretrained Models: A Survey. (arXiv:2204.07356v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.07356","description":"<p>Pretrained models have produced great success in both Computer Vision (CV)\nand Natural Language Processing (NLP). This progress leads to learning joint\nrepresentations of vision and language pretraining by feeding visual and\nlinguistic contents into a multi-layer transformer, Visual-Language Pretrained\nModels (VLPMs). In this paper, we present an overview of the major advances\nachieved in VLPMs for producing joint representations of vision and language.\nAs the preliminaries, we briefly describe the general task definition and\ngenetic architecture of VLPMs. We first discuss the language and vision data\nencoding methods and then present the mainstream VLPM structure as the core\ncontent. We further summarise several essential pretraining and fine-tuning\nstrategies. Finally, we highlight three future directions for both CV and NLP\nresearchers to provide insightful guidance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Long_S/0/1/0/all/0/1\">Siqu Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_F/0/1/0/all/0/1\">Feiqi Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Soyeon Caren Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haiqing Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Event-aided Direct Sparse Odometry. (arXiv:2204.07640v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.07640","description":"<p>We introduce EDS, a direct monocular visual odometry using events and frames.\nOur algorithm leverages the event generation model to track the camera motion\nin the blind time between frames. The method formulates a direct probabilistic\napproach of observed brightness increments. Per-pixel brightness increments are\npredicted using a sparse number of selected 3D points and are compared to the\nevents via the brightness increment error to estimate camera motion. The method\nrecovers a semi-dense 3D map using photometric bundle adjustment. EDS is the\nfirst method to perform 6-DOF VO using events and frames with a direct\napproach. By design, it overcomes the problem of changing appearance in\nindirect methods. We also show that, for a target error performance, EDS can\nwork at lower frame rates than state-of-the-art frame-based VO solutions. This\nopens the door to low-power motion-tracking applications where frames are\nsparingly triggered \"on demand\" and our method tracks the motion in between. We\nrelease code and datasets to the public.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hidalgo_Carrio_J/0/1/0/all/0/1\">Javier Hidalgo-Carri&#xf3;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gallego_G/0/1/0/all/0/1\">Guillermo Gallego</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scaramuzza_D/0/1/0/all/0/1\">Davide Scaramuzza</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning with Signatures. (arXiv:2204.07953v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.07953","description":"<p>In this work we investigate the use of the Signature Transform in the context\nof Learning. Under this assumption, we advance a supervised framework that\nprovides state-of-the-art classification accuracy with the use of very few\nlabels without the need of credit assignment and with minimal or no\noverfitting. We leverage tools from harmonic analysis by the use of the\nsignature and log-signature, and use as a score function RMSE and MAE Signature\nand log-signature. We develop a closed-form equation to compute probably good\noptimal scale factors. Classification is performed at the CPU level orders of\nmagnitude faster than other methods. We report results on AFHQ, MNIST and\nCIFAR10 achieving 100% accuracy on all tasks assuming we can determine at test\ntime which probably good optimal scale factor to use for each category.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Curto_J/0/1/0/all/0/1\">J. de Curt&#xf2;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zarza_I/0/1/0/all/0/1\">I. de Zarz&#xe0;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1\">Hong Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust PCA Unrolling Network for Super-resolution Vessel Extraction in X-ray Coronary Angiography. (arXiv:2204.08466v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2204.08466","description":"<p>Although robust PCA has been increasingly adopted to extract vessels from\nX-ray coronary angiography (XCA) images, challenging problems such as\ninefficient vessel-sparsity modelling, noisy and dynamic background artefacts,\nand high computational cost still remain unsolved. Therefore, we propose a\nnovel robust PCA unrolling network with sparse feature selection for\nsuper-resolution XCA vessel imaging. Being embedded within a patch-wise\nspatiotemporal super-resolution framework that is built upon a pooling layer\nand a convolutional long short-term memory network, the proposed network can\nnot only gradually prune complex vessel-like artefacts and noisy backgrounds in\nXCA during network training but also iteratively learn and select the\nhigh-level spatiotemporal semantic information of moving contrast agents\nflowing in the XCA-imaged vessels. The experimental results show that the\nproposed method significantly outperforms state-of-the-art methods, especially\nin the imaging of the vessel network and its distal vessels, by restoring the\nintensity and geometry profiles of heterogeneous vessels against complex and\ndynamic backgrounds.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Qin_B/0/1/0/all/0/1\">Binjie Qin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mao_H/0/1/0/all/0/1\">Haohao Mao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yiming Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_J/0/1/0/all/0/1\">Jun Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lv_Y/0/1/0/all/0/1\">Yisong Lv</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_Y/0/1/0/all/0/1\">Yueqi Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ding_S/0/1/0/all/0/1\">Song Ding</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1\">Xu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Shape-Aware Monocular 3D Object Detection. (arXiv:2204.08717v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.08717","description":"<p>The detection of 3D objects through a single perspective camera is a\nchallenging issue. The anchor-free and keypoint-based models receive increasing\nattention recently due to their effectiveness and simplicity. However, most of\nthese methods are vulnerable to occluded and truncated objects. In this paper,\na single-stage monocular 3D object detection model is proposed. An\ninstance-segmentation head is integrated into the model training, which allows\nthe model to be aware of the visible shape of a target object. The detection\nlargely avoids interference from irrelevant regions surrounding the target\nobjects. In addition, we also reveal that the popular IoU-based evaluation\nmetrics, which were originally designed for evaluating stereo or LiDAR-based\ndetection methods, are insensitive to the improvement of monocular 3D object\ndetection algorithms. A novel evaluation metric, namely average depth\nsimilarity (ADS) is proposed for the monocular 3D object detection models. Our\nmethod outperforms the baseline on both the popular and the proposed evaluation\nmetrics while maintaining real-time efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jie Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wan-Lei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Song-Yuan Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NTIRE 2022 Challenge on Super-Resolution and Quality Enhancement of Compressed Video: Dataset, Methods and Results. (arXiv:2204.09314v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.09314","description":"<p>This paper reviews the NTIRE 2022 Challenge on Super-Resolution and Quality\nEnhancement of Compressed Video. In this challenge, we proposed the LDV 2.0\ndataset, which includes the LDV dataset (240 videos) and 95 additional videos.\nThis challenge includes three tracks. Track 1 aims at enhancing the videos\ncompressed by HEVC at a fixed QP. Track 2 and Track 3 target both the\nsuper-resolution and quality enhancement of HEVC compressed video. They require\nx2 and x4 super-resolution, respectively. The three tracks totally attract more\nthan 600 registrations. In the test phase, 8 teams, 8 teams and 12 teams\nsubmitted the final results to Tracks 1, 2 and 3, respectively. The proposed\nmethods and solutions gauge the state-of-the-art of super-resolution and\nquality enhancement of compressed video. The proposed LDV 2.0 dataset is\navailable at https://github.com/RenYang-home/LDV_dataset. The homepage of this\nchallenge (including open-sourced codes) is at\nhttps://github.com/RenYang-home/NTIRE22_VEnh_SR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1\">Ren Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Timofte_R/0/1/0/all/0/1\">Radu Timofte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1\">Meisong Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_Q/0/1/0/all/0/1\">Qunliang Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_M/0/1/0/all/0/1\">Minglang Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mai Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Lai Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Huaida Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Ying Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ben_Y/0/1/0/all/0/1\">Youcheng Ben</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xiao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1\">Chen Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_P/0/1/0/all/0/1\">Pei Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_G/0/1/0/all/0/1\">Gang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1\">Renlong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhilu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_W/0/1/0/all/0/1\">Wei Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_Z/0/1/0/all/0/1\">Zhengyao Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yunjin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Mingcai Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_D/0/1/0/all/0/1\">Dongwei Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_W/0/1/0/all/0/1\">Wangmeng Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ostyakov_P/0/1/0/all/0/1\">Pavel Ostyakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dmitry_V/0/1/0/all/0/1\">Vyal Dmitry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soltanayev_S/0/1/0/all/0/1\">Shakarim Soltanayev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sergey_C/0/1/0/all/0/1\">Chervontsev Sergey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Magauiya_Z/0/1/0/all/0/1\">Zhussip Magauiya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_X/0/1/0/all/0/1\">Xueyi Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Youliang Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michelini_P/0/1/0/all/0/1\">Pablo Navarrete Michelini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yunhua Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Diankai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shaoli Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Si Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Biao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Chengjian Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaofeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1\">Kaidi Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1\">Ning Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Canh_T/0/1/0/all/0/1\">Thuong Nguyen Canh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bach_T/0/1/0/all/0/1\">Thong Bach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiaopeng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1\">Haoyu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shijie Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junlin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1\">Liangbin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shuwei Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yujiu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xintao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jinjin Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1\">Chao Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xiaodi Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nian_C/0/1/0/all/0/1\">Chunmei Nian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Dong Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jucai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zhihuai Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_M/0/1/0/all/0/1\">Mao Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_D/0/1/0/all/0/1\">Dengyan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_L/0/1/0/all/0/1\">Liuhan Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shengjie Chen</a>, et al. (16 additional authors not shown)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sim-2-Sim Transfer for Vision-and-Language Navigation in Continuous Environments. (arXiv:2204.09667v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.09667","description":"<p>Recent work in Vision-and-Language Navigation (VLN) has presented two\nenvironmental paradigms with differing realism -- the standard VLN setting\nbuilt on topological environments where navigation is abstracted away, and the\nVLN-CE setting where agents must navigate continuous 3D environments using\nlow-level actions. Despite sharing the high-level task and even the underlying\ninstruction-path data, performance on VLN-CE lags behind VLN significantly. In\nthis work, we explore this gap by transferring an agent from the abstract\nenvironment of VLN to the continuous environment of VLN-CE. We find that this\nsim-2-sim transfer is highly effective, improving over the prior state of the\nart in VLN-CE by +12% success rate. While this demonstrates the potential for\nthis direction, the transfer does not fully retain the original performance of\nthe agent in the abstract setting. We present a sequence of experiments to\nidentify what differences result in performance degradation, providing clear\ndirections for further improvement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Krantz_J/0/1/0/all/0/1\">Jacob Krantz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Stefan Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Color Invariant Skin Segmentation. (arXiv:2204.09882v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.09882","description":"<p>This paper addresses the problem of automatically detecting human skin in\nimages without reliance on color information. A primary motivation of the work\nhas been to achieve results that are consistent across the full range of skin\ntones, even while using a training dataset that is significantly biased toward\nlighter skin tones. Previous skin-detection methods have used color cues almost\nexclusively, and we present a new approach that performs well in the absence of\nsuch information. A key aspect of the work is dataset repair through\naugmentation that is applied strategically during training, with the goal of\ncolor invariant feature learning to enhance generalization. We have\ndemonstrated the concept using two architectures, and experimental results show\nimprovements in both precision and recall for most Fitzpatrick skin tones in\nthe benchmark ECU dataset. We further tested the system with the RFW dataset to\nshow that the proposed method performs much more consistently across different\nethnicities, thereby reducing the chance of bias based on skin color. To\ndemonstrate the effectiveness of our work, extensive experiments were performed\non grayscale images as well as images obtained under unconstrained illumination\nand with artificial filters. Source code:\nhttps://github.com/HanXuMartin/Color-Invariant-Skin-Segmentation\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Han Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_A/0/1/0/all/0/1\">Abhijit Sarkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbott_A/0/1/0/all/0/1\">A. Lynn Abbott</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Working memory inspired hierarchical video decomposition with transformative representations. (arXiv:2204.10105v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.10105","description":"<p>Video decomposition is very important to extract moving foreground objects\nfrom complex backgrounds in computer vision, machine learning, and medical\nimaging, e.g., extracting moving contrast-filled vessels from the complex and\nnoisy backgrounds of X-ray coronary angiography (XCA). However, the challenges\ncaused by dynamic backgrounds, overlapping heterogeneous environments and\ncomplex noises still exist in video decomposition. To solve these problems,\nthis study is the first to introduce a flexible visual working memory model in\nvideo decomposition tasks to provide interpretable and high-performance\nhierarchical deep architecture, integrating the transformative representations\nbetween sensory and control layers from the perspective of visual and cognitive\nneuroscience. Specifically, robust PCA unrolling networks acting as a\nstructure-regularized sensor layer decompose XCA into sparse/low-rank\nstructured representations to separate moving contrast-filled vessels from\nnoisy and complex backgrounds. Then, patch recurrent convolutional LSTM\nnetworks with a backprojection module embody unstructured random\nrepresentations of the control layer in working memory, recurrently projecting\nspatiotemporally decomposed nonlocal patches into orthogonal subspaces for\nheterogeneous vessel retrieval and interference suppression. This video\ndecomposition deep architecture effectively restores the heterogeneous profiles\nof intensity and the geometries of moving objects against the complex\nbackground interferences. Experiments show that the proposed method\nsignificantly outperforms state-of-the-art methods in accurate moving\ncontrast-filled vessel extraction with excellent flexibility and computational\nefficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1\">Binjie Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_H/0/1/0/all/0/1\">Haohao Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruipeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yueqi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1\">Song Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EmbedTrack -- Simultaneous Cell Segmentation and Tracking Through Learning Offsets and Clustering Bandwidths. (arXiv:2204.10713v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.10713","description":"<p>A systematic analysis of the cell behavior requires automated approaches for\ncell segmentation and tracking. While deep learning has been successfully\napplied for the task of cell segmentation, there are few approaches for\nsimultaneous cell segmentation and tracking using deep learning. Here, we\npresent EmbedTrack, a single convolutional neural network for simultaneous cell\nsegmentation and tracking which predicts easy to interpret embeddings. As\nembeddings, offsets of cell pixels to their cell center and bandwidths are\nlearned. We benchmark our approach on nine 2D data sets from the Cell Tracking\nChallenge, where our approach performs on seven out of nine data sets within\nthe top 3 contestants including three top 1 performances. The source code is\npublicly available at https://git.scc.kit.edu/kit-loe-ge/embedtrack.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Loffler_K/0/1/0/all/0/1\">Katharina L&#xf6;ffler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mikut_R/0/1/0/all/0/1\">Ralf Mikut</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-04-25T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}