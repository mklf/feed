{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.1","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-08-31T01:30:00Z","channels":[{"title":"cs.AI updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.AI","description":"Computer Science -- Artificial Intelligence (cs.AI) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Why and How Governments Should Monitor AI Development. (arXiv:2108.12427v1 [cs.CY])","link":"http://arxiv.org/abs/2108.12427","description":"<p>In this paper we outline a proposal for improving the governance of\nartificial intelligence (AI) by investing in government capacity to\nsystematically measure and monitor the capabilities and impacts of AI systems.\nIf adopted, this would give governments greater information about the AI\necosystem, equipping them to more effectively direct AI development and\ndeployment in the most societally and economically beneficial directions. It\nwould also create infrastructure that could rapidly identify potential threats\nor harms that could occur as a consequence of changes in the AI ecosystem, such\nas the emergence of strategically transformative capabilities, or the\ndeployment of harmful systems.\n</p>\n<p>We begin by outlining the problem which motivates this proposal: in brief,\ntraditional governance approaches struggle to keep pace with the speed of\nprogress in AI. We then present our proposal for addressing this problem:\ngovernments must invest in measurement and monitoring infrastructure. We\ndiscuss this proposal in detail, outlining what specific things governments\ncould focus on measuring and monitoring, and the kinds of benefits this would\ngenerate for policymaking. Finally, we outline some potential pilot projects\nand some considerations for implementing this in practice.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Whittlestone_J/0/1/0/all/0/1\">Jess Whittlestone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_J/0/1/0/all/0/1\">Jack Clark</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Text Evaluation through the Lens of Wasserstein Barycenters. (arXiv:2108.12463v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12463","description":"<p>A new metric \\texttt{BaryScore} to evaluate text generation based on deep\ncontextualized embeddings (\\textit{e.g.}, BERT, Roberta, ELMo) is introduced.\nThis metric is motivated by a new framework relying on optimal transport tools,\n\\textit{i.e.}, Wasserstein distance and barycenter. By modelling the layer\noutput of deep contextualized embeddings as a probability distribution rather\nthan by a vector embedding; this framework provides a natural way to aggregate\nthe different outputs through the Wasserstein space topology. In addition, it\nprovides theoretical grounds to our metric and offers an alternative to\navailable solutions (\\textit{e.g.}, MoverScore and BertScore). Numerical\nevaluation is performed on four different tasks: machine translation,\nsummarization, data2text generation and image captioning. Our results show that\n\\texttt{BaryScore} outperforms other BERT based metrics and exhibits more\nconsistent behaviour in particular for text summarization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Colombo_P/0/1/0/all/0/1\">Pierre Colombo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Staerman_G/0/1/0/all/0/1\">Guillaume Staerman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clavel_C/0/1/0/all/0/1\">Chloe Clavel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piantanida_P/0/1/0/all/0/1\">Pablo Piantanida</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Code-switched inspired losses for generic spoken dialog representations. (arXiv:2108.12465v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12465","description":"<p>Spoken dialog systems need to be able to handle both multiple languages and\nmultilinguality inside a conversation (\\textit{e.g} in case of code-switching).\nIn this work, we introduce new pretraining losses tailored to learn\nmultilingual spoken dialog representations. The goal of these losses is to\nexpose the model to code-switched language. To scale up training, we\nautomatically build a pretraining corpus composed of multilingual conversations\nin five different languages (French, Italian, English, German and Spanish) from\n\\texttt{OpenSubtitles}, a huge multilingual corpus composed of 24.3G tokens. We\ntest the generic representations on \\texttt{MIAM}, a new benchmark composed of\nfive dialog act corpora on the same aforementioned languages as well as on two\nnovel multilingual downstream tasks (\\textit{i.e} multilingual mask utterance\nretrieval and multilingual inconsistency identification). Our experiments show\nthat our new code switched-inspired losses achieve a better performance in both\nmonolingual and multilingual settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chapuis_E/0/1/0/all/0/1\">Emile Chapuis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colombo_P/0/1/0/all/0/1\">Pierre Colombo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labeau_M/0/1/0/all/0/1\">Matthieu Labeau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clave_C/0/1/0/all/0/1\">Chloe Clave</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Inner-Group Relations on Point Clouds. (arXiv:2108.12468v1 [cs.CV])","link":"http://arxiv.org/abs/2108.12468","description":"<p>The prevalence of relation networks in computer vision is in stark contrast\nto underexplored point-based methods. In this paper, we explore the\npossibilities of local relation operators and survey their feasibility. We\npropose a scalable and efficient module, called group relation aggregator. The\nmodule computes a feature of a group based on the aggregation of the features\nof the inner-group points weighted by geometric relations and semantic\nrelations. We adopt this module to design our RPNet. We further verify the\nexpandability of RPNet, in terms of both depth and width, on the tasks of\nclassification and segmentation. Surprisingly, empirical results show that\nwider RPNet fits for classification, while deeper RPNet works better on\nsegmentation. RPNet achieves state-of-the-art for classification and\nsegmentation on challenging benchmarks. We also compare our local aggregator\nwith PointNet++, with around 30% parameters and 50% computation saving.\nFinally, we conduct experiments to reveal the robustness of RPNet with regard\nto rigid transformation and noises.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ran_H/0/1/0/all/0/1\">Haoxi Ran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuo_W/0/1/0/all/0/1\">Wei Zhuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1\">Li Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Disrupting Adversarial Transferability in Deep Neural Networks. (arXiv:2108.12492v1 [cs.LG])","link":"http://arxiv.org/abs/2108.12492","description":"<p>Adversarial attack transferability is a well-recognized phenomenon in deep\nlearning. Prior work has partially explained transferability by recognizing\ncommon adversarial subspaces and correlations between decision boundaries, but\nwe have found little explanation in the literature beyond this. In this paper,\nwe propose that transferability between seemingly different models is due to a\nhigh linear correlation between features that different deep neural networks\nextract. In other words, two models trained on the same task that are seemingly\ndistant in the parameter space likely extract features in the same fashion,\njust with trivial shifts and rotations between the latent spaces. Furthermore,\nwe show how applying a feature correlation loss, which decorrelates the\nextracted features in a latent space, can drastically reduce the\ntransferability of adversarial attacks between models, suggesting that the\nmodels complete tasks in semantically different ways. Finally, we propose a\nDual Neck Autoencoder (DNA), which leverages this feature correlation loss to\ncreate two meaningfully different encodings of input information with reduced\ntransferability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wiedeman_C/0/1/0/all/0/1\">Christopher Wiedeman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Ge Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StressNAS: Affect State and Stress Detection Using Neural Architecture Search. (arXiv:2108.12502v1 [cs.LG])","link":"http://arxiv.org/abs/2108.12502","description":"<p>Smartwatches have rapidly evolved towards capabilities to accurately capture\nphysiological signals. As an appealing application, stress detection attracts\nmany studies due to its potential benefits to human health. It is propitious to\ninvestigate the applicability of deep neural networks (DNN) to enhance human\ndecision-making through physiological signals. However, manually engineering\nDNN proves a tedious task especially in stress detection due to the complex\nnature of this phenomenon. To this end, we propose an optimized deep neural\nnetwork training scheme using neural architecture search merely using\nwrist-worn data from WESAD. Experiments show that our approach outperforms\ntraditional ML methods by 8.22% and 6.02% in the three-state and two-state\nclassifiers, respectively, using the combination of WESAD wrist signals.\nMoreover, the proposed method can minimize the need for human-design DNN while\nimproving performance by 4.39% (three-state) and 8.99% (binary).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huynh_L/0/1/0/all/0/1\">Lam Huynh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tri Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Thu Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pirttikangas_S/0/1/0/all/0/1\">Susanna Pirttikangas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siirtola_P/0/1/0/all/0/1\">Pekka Siirtola</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robustness Disparities in Commercial Face Detection. (arXiv:2108.12508v1 [cs.CY])","link":"http://arxiv.org/abs/2108.12508","description":"<p>Facial detection and analysis systems have been deployed by large companies\nand critiqued by scholars and activists for the past decade. Critiques that\nfocus on system performance analyze disparity of the system's output, i.e., how\nfrequently is a face detected for different Fitzpatrick skin types or perceived\ngenders. However, we focus on the robustness of these system outputs under\nnoisy natural perturbations. We present the first of its kind detailed\nbenchmark of the robustness of three such systems: Amazon Rekognition,\nMicrosoft Azure, and Google Cloud Platform. We use both standard and recently\nreleased academic facial datasets to quantitatively analyze trends in\nrobustness for each. Across all the datasets and systems, we generally find\nthat photos of individuals who are older, masculine presenting, of darker skin\ntype, or have dim lighting are more susceptible to errors than their\ncounterparts in other identities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dooley_S/0/1/0/all/0/1\">Samuel Dooley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1\">Tom Goldstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dickerson_J/0/1/0/all/0/1\">John P. Dickerson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Combining chest X-rays and EHR data using machine learning to diagnose acute respiratory failure. (arXiv:2108.12530v1 [cs.LG])","link":"http://arxiv.org/abs/2108.12530","description":"<p>When patients develop acute respiratory failure, accurately identifying the\nunderlying etiology is essential for determining the best treatment, but it can\nbe challenging to differentiate between common diagnoses in clinical practice.\nMachine learning models could improve medical diagnosis by augmenting clinical\ndecision making and play a role in the diagnostic evaluation of patients with\nacute respiratory failure. While machine learning models have been developed to\nidentify common findings on chest radiographs (e.g. pneumonia), augmenting\nthese approaches by also analyzing clinically relevant data from the electronic\nhealth record (EHR) could aid in the diagnosis of acute respiratory failure.\nMachine learning models were trained to predict the cause of acute respiratory\nfailure (pneumonia, heart failure, and/or COPD) using chest radiographs and EHR\ndata from patients within an internal cohort using diagnoses based on physician\nchart review. Models were also tested on patients in an external cohort using\ndischarge diagnosis codes. A model combining chest radiographs and EHR data\noutperformed models based on each modality alone for pneumonia and COPD. For\npneumonia, the combined model AUROC was 0.79 (0.78-0.79), image model AUROC was\n0.73 (0.72-0.75), and EHR model AUROC was 0.73 (0.70-0.76); for COPD, combined:\n0.89 (0.83-0.91), image: 0.85 (0.77-0.89), and EHR: 0.80 (0.76-0.84); for heart\nfailure, combined: 0.80 (0.77-0.84), image: 0.77 (0.71-0.81), and EHR: 0.80\n(0.75-0.82). In the external cohort, performance was consistent for heart\nfailure and COPD, but declined slightly for pneumonia. Overall, machine\nlearning models combing chest radiographs and EHR data can accurately\ndifferentiate between common causes of acute respiratory failure. Further work\nis needed to determine whether these models could aid clinicians in the\ndiagnosis of acute respiratory failure in clinical settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jabbour_S/0/1/0/all/0/1\">Sarah Jabbour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fouhey_D/0/1/0/all/0/1\">David Fouhey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kazerooni_E/0/1/0/all/0/1\">Ella Kazerooni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiens_J/0/1/0/all/0/1\">Jenna Wiens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sjoding_M/0/1/0/all/0/1\">Michael W Sjoding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Anytime Stochastic Task and Motion Policies. (arXiv:2108.12537v1 [cs.RO])","link":"http://arxiv.org/abs/2108.12537","description":"<p>In order to solve complex, long-horizon tasks, intelligent robots need to\ncarry out high-level, abstract planning and reasoning in conjunction with\nmotion planning. However, abstract models are typically lossy and plans or\npolicies computed using them can be inexecutable. These problems are\nexacerbated in stochastic situations where the robot needs to reason about and\nplan for multiple contingencies. We present a new approach for integrated task\nand motion planning in stochastic settings. In contrast to prior work in this\ndirection, we show that our approach can effectively compute integrated task\nand motion policies whose branching structures encode agent behaviors that\nhandle multiple execution-time contingencies. We prove that our algorithm is\nprobabilistically complete and can compute feasible solution policies in an\nanytime fashion so that the probability of encountering an unresolved\ncontingency decreases over time. Empirical results on a set of challenging\nproblems show the utility and scope of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shah_N/0/1/0/all/0/1\">Naman Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_S/0/1/0/all/0/1\">Siddharth Srivastava</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AMMASurv: Asymmetrical Multi-Modal Attention for Accurate Survival Analysis with Whole Slide Images and Gene Expression Data. (arXiv:2108.12565v1 [cs.CV])","link":"http://arxiv.org/abs/2108.12565","description":"<p>The use of multi-modal data such as the combination of whole slide images\n(WSIs) and gene expression data for survival analysis can lead to more accurate\nsurvival predictions. Previous multi-modal survival models are not able to\nefficiently excavate the intrinsic information within each modality. Moreover,\ndespite experimental results show that WSIs provide more effective information\nthan gene expression data, previous methods regard the information from\ndifferent modalities as similarly important so they cannot flexibly utilize the\npotential connection between the modalities. To address the above problems, we\npropose a new asymmetrical multi-modal method, termed as AMMASurv.\nSpecifically, we design an asymmetrical multi-modal attention mechanism (AMMA)\nin Transformer encoder for multi-modal data to enable a more flexible\nmulti-modal information fusion for survival prediction. Different from previous\nworks, AMMASurv can effectively utilize the intrinsic information within every\nmodality and flexibly adapts to the modalities of different importance.\nExtensive experiments are conducted to validate the effectiveness of the\nproposed model. Encouraging results demonstrate the superiority of our method\nover other state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ruoqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Ziwang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haitao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hejun Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distilling the Knowledge of Large-scale Generative Models into Retrieval Models for Efficient Open-domain Conversation. (arXiv:2108.12582v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12582","description":"<p>Despite the remarkable performance of large-scale generative models in\nopen-domain conversation, they are known to be less practical for building\nreal-time conversation systems due to high latency. On the other hand,\nretrieval models could return responses with much lower latency but show\ninferior performance to the large-scale generative models since the\nconversation quality is bounded by the pre-defined response set. To take\nadvantage of both approaches, we propose a new training method called G2R\n(Generative-to-Retrieval distillation) that preserves the efficiency of a\nretrieval model while leveraging the conversational ability of a large-scale\ngenerative model by infusing the knowledge of the generative model into the\nretrieval model. G2R consists of two distinct techniques of distillation: the\ndata-level G2R augments the dialogue dataset with additional responses\ngenerated by the large-scale generative model, and the model-level G2R\ntransfers the response quality score assessed by the generative model to the\nscore of the retrieval model by the knowledge distillation loss. Through\nextensive experiments including human evaluation, we demonstrate that our\nretrieval-based conversation system trained with G2R shows a substantially\nimproved performance compared to the baseline retrieval model while showing\nsignificantly lower inference latency than the large-scale generative models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1\">Beomsu Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_S/0/1/0/all/0/1\">Seokjun Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Seungju Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erdenee_E/0/1/0/all/0/1\">Enkhbayar Erdenee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_B/0/1/0/all/0/1\">Buru Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Layer-wise Model Pruning based on Mutual Information. (arXiv:2108.12594v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12594","description":"<p>The proposed pruning strategy offers merits over weight-based pruning\ntechniques: (1) it avoids irregular memory access since representations and\nmatrices can be squeezed into their smaller but dense counterparts, leading to\ngreater speedup; (2) in a manner of top-down pruning, the proposed method\noperates from a more global perspective based on training signals in the top\nlayer, and prunes each layer by propagating the effect of global signals\nthrough layers, leading to better performances at the same sparsity level.\nExtensive experiments show that at the same sparsity level, the proposed\nstrategy offers both greater speedup and higher performances than weight-based\npruning methods (e.g., magnitude pruning, movement pruning).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1\">Chun Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ao_X/0/1/0/all/0/1\">Xiang Ao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1\">Yuxian Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiaofei Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Smoothing Dialogue States for Open Conversational Machine Reading. (arXiv:2108.12599v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12599","description":"<p>Conversational machine reading (CMR) requires machines to communicate with\nhumans through multi-turn interactions between two salient dialogue states of\ndecision making and question generation processes. In open CMR settings, as the\nmore realistic scenario, the retrieved background knowledge would be noisy,\nwhich results in severe challenges in the information transmission. Existing\nstudies commonly train independent or pipeline systems for the two subtasks.\nHowever, those methods are trivial by using hard-label decisions to activate\nquestion generation, which eventually hinders the model performance. In this\nwork, we propose an effective gating strategy by smoothing the two dialogue\nstates in only one decoder and bridge decision making and question generation\nto provide a richer dialogue state reference. Experiments on the OR-ShARC\ndataset show the effectiveness of our method, which achieves new\nstate-of-the-art results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhuosheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_S/0/1/0/all/0/1\">Siru Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Utiyama_M/0/1/0/all/0/1\">Masao Utiyama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sumita_E/0/1/0/all/0/1\">Eiichiro Sumita</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DKM: Differentiable K-Means Clustering Layer for Neural Network Compression. (arXiv:2108.12659v1 [cs.LG])","link":"http://arxiv.org/abs/2108.12659","description":"<p>Deep neural network (DNN) model compression for efficient on-device inference\nis becoming increasingly important to reduce memory requirements and keep user\ndata on-device. To this end, we propose a novel differentiable k-means\nclustering layer (DKM) and its application to train-time weight\nclustering-based DNN model compression. DKM casts k-means clustering as an\nattention problem and enables joint optimization of the parameters and\nclustering centroids. Unlike prior works that rely on additional regularizers\nand parameters, DKM-based compression keeps the original loss function and\nmodel architecture fixed. We evaluated DKM-based compression on various DNN\nmodels for computer vision and natural language processing (NLP) tasks. Our\nresults demonstrate that DMK delivers superior compression and accuracy\ntrade-off on ImageNet1k and GLUE benchmarks. For example, DKM-based compression\ncan offer 74.5% top-1 ImageNet1k accuracy on ResNet50 DNN model with 3.3MB\nmodel size (29.4x model compression factor). For MobileNet-v1, which is a\nchallenging DNN to compress, DKM delivers 62.8% top-1 ImageNet1k accuracy with\n0.74 MB model size (22.4x model compression factor). This result is 6.8% higher\ntop-1 accuracy and 33% relatively smaller model size than the current\nstate-of-the-art DNN compression algorithms. Additionally, DKM enables\ncompression of DistilBERT model by 11.8x with minimal (1.1%) accuracy loss on\nGLUE NLP benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cho_M/0/1/0/all/0/1\">Minsik Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vahid_K/0/1/0/all/0/1\">Keivan A. Vahid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adya_S/0/1/0/all/0/1\">Saurabh Adya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rastegari_M/0/1/0/all/0/1\">Mohammad Rastegari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CHAINGE: A Blockchain Solution to Automate Payment Detail Updates to Subscription Services. (arXiv:2108.12705v1 [cs.CR])","link":"http://arxiv.org/abs/2108.12705","description":"<p>The rise of the subscription-based business model has led to a corresponding\nincrease in the number of subscriptions where a customer needs to manage their\npayments. This management of payments for multiple subscriptions has become a\nvery complicated and insecure task for customers, especially when it comes to\nrenewing payment details when the card is lost, stolen, or expires. In\naddition, this, mostly manual, process is vulnerable to human error, digital\nfrauds, and data breaches, according to security reports. Thus, in this paper,\nwe propose a novel approach to automate, manage and simplify the Financial\nSupply Chain involved in the process of updating and managing payments to user\nsubscriptions. This is done by utilising the Hyperledger Sawtooth blockchain\nframework, that allows a consumer to enter their payment card details in a\ncentral digital wallet and link their subscriptions to their cards. The card\nbeing updated triggers an event on the blockchain, which allow for the payment\ndetails to be updated on subscription systems automatically. The verification\ntests performed on the prototype of the proposed system shows that its current\nimplementation has been securely achieved.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Buckley_D/0/1/0/all/0/1\">David Buckley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bendiab_G/0/1/0/all/0/1\">Gueltoum Bendiab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shiaeles_S/0/1/0/all/0/1\">Stavros Shiaeles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savage_N/0/1/0/all/0/1\">Nick Savage</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolokotronis_N/0/1/0/all/0/1\">Nicholas Kolokotronis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Event Extraction as Natural Language Generation. (arXiv:2108.12724v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12724","description":"<p>Event extraction (EE), the task that identifies event triggers and their\narguments in text, is usually formulated as a classification or structured\nprediction problem. Such models usually reduce labels to numeric identifiers,\nmaking them unable to take advantage of label semantics (e.g. an event type\nnamed Arrest is related to words like arrest, detain, or apprehend). This\nprevents the generalization to new event types. In this work, we formulate EE\nas a natural language generation task and propose GenEE, a model that not only\ncaptures complex dependencies within an event but also generalizes well to\nunseen or rare event types. Given a passage and an event type, GenEE is trained\nto generate a natural sentence following a predefined template for that event\ntype. The generated output is then decoded into trigger and argument\npredictions. The autoregressive generation process naturally models the\ndependencies among the predictions -- each new word predicted depends on those\nalready generated in the output sentence. Using carefully designed input\nprompts during generation, GenEE is able to capture label semantics, which\nenables the generalization to new event types. Empirical results show that our\nmodel achieves strong performance on event extraction tasks under all\nzero-shot, few-shot, and high-resource scenarios. Especially, in the\nhigh-resource setting, GenEE outperforms the state-of-the-art model on argument\nextraction and gets competitive results with the current best on end-to-end EE\ntasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hsu_I/0/1/0/all/0/1\">I-Hung Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kuan-Hao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boschee_E/0/1/0/all/0/1\">Elizabeth Boschee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miller_S/0/1/0/all/0/1\">Scott Miller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Natarajan_P/0/1/0/all/0/1\">Prem Natarajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Risk-Aware Fine-Grained Access Control in Cyber-Physical Contexts. (arXiv:2108.12739v1 [cs.CR])","link":"http://arxiv.org/abs/2108.12739","description":"<p>Access to resources by users may need to be granted only upon certain\nconditions and contexts, perhaps particularly in cyber-physical settings.\nUnfortunately, creating and modifying context-sensitive access control\nsolutions in dynamic environments creates ongoing challenges to manage the\nauthorization contexts. This paper proposes RASA, a context-sensitive access\nauthorization approach and mechanism leveraging unsupervised machine learning\nto automatically infer risk-based authorization decision boundaries. We explore\nRASA in a healthcare usage environment, wherein cyber and physical conditions\ncreate context-specific risks for protecting private health information. The\nrisk levels are associated with access control decisions recommended by a\nsecurity policy. A coupling method is introduced to track coexistence of the\nobjects within context using frequency and duration of coexistence, and these\nare clustered to reveal sets of actions with common risk levels; these are used\nto create authorization decision boundaries. In addition, we propose a method\nfor assessing the risk level and labelling the clusters with respect to their\ncorresponding risk levels. We evaluate the promise of RASA-generated policies\nagainst a heuristic rule-based policy. By employing three different coupling\nfeatures (frequency-based, duration-based, and combined features), the\ndecisions of the unsupervised method and that of the policy are more than 99%\nconsistent.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jinxin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simsek_M/0/1/0/all/0/1\">Murat Simsek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kantarci_B/0/1/0/all/0/1\">Burak Kantarci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erol_Kantarci_M/0/1/0/all/0/1\">Melike Erol-Kantarci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malton_A/0/1/0/all/0/1\">Andrew Malton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walenstein_A/0/1/0/all/0/1\">Andrew Walenstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TCCT: Tightly-Coupled Convolutional Transformer on Time Series Forecasting. (arXiv:2108.12784v1 [cs.LG])","link":"http://arxiv.org/abs/2108.12784","description":"<p>Time series forecasting is essential for a wide range of real-world\napplications. Recent studies have shown the superiority of Transformer in\ndealing with such problems, especially long sequence time series input(LSTI)\nand long sequence time series forecasting(LSTF) problems. To improve the\nefficiency and enhance the locality of Transformer, these studies combine\nTransformer with CNN in varying degrees. However, their combinations are\nloosely-coupled and do not make full use of CNN. To address this issue, we\npropose the concept of tightly-coupled convolutional Transformer(TCCT) and\nthree TCCT architectures which apply transformed CNN architectures into\nTransformer: (1) CSPAttention: through fusing CSPNet with self-attention\nmechanism, the computation cost of self-attention mechanism is reduced by 30%\nand the memory usage is reduced by 50% while achieving equivalent or beyond\nprediction accuracy. (2) Dilated causal convolution: this method is to modify\nthe distilling operation proposed by Informer through replacing canonical\nconvolutional layers with dilated causal convolutional layers to gain\nexponentially receptive field growth. (3) Passthrough mechanism: the\napplication of passthrough mechanism to stack of self-attention blocks helps\nTransformer-like models get more fine-grained information with negligible extra\ncomputation costs. Our experiments on real-world datasets show that our TCCT\narchitectures could greatly improve the performance of existing state-of-art\nTransformer models on time series forecasting with much lower computation and\nmemory costs, including canonical Transformer, LogTrans and Informer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Li Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yangzhu Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Markov Switching Model for Driver Behavior Prediction: Use cases on Smartphones. (arXiv:2108.12801v1 [cs.LG])","link":"http://arxiv.org/abs/2108.12801","description":"<p>Several intelligent transportation systems focus on studying the various\ndriver behaviors for numerous objectives. This includes the ability to analyze\ndriver actions, sensitivity, distraction, and response time. As the data\ncollection is one of the major concerns for learning and validating different\ndriving situations, we present a driver behavior switching model validated by a\nlow-cost data collection solution using smartphones. The proposed model is\nvalidated using a real dataset to predict the driver behavior in short duration\nperiods. A literature survey on motion detection (specifically driving behavior\ndetection using smartphones) is presented. Multiple Markov Switching Variable\nAuto-Regression (MSVAR) models are implemented to achieve a sophisticated\nfitting with the collected driver behavior data. This yields more accurate\npredictions not only for driver behavior but also for the entire driving\nsituation. The performance of the presented models together with a suitable\nmodel selection criteria is also presented. The proposed driver behavior\nprediction framework can potentially be used in accident prediction and driver\nsafety systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zaky_A/0/1/0/all/0/1\">Ahmed B. Zaky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khamis_M/0/1/0/all/0/1\">Mohamed A. Khamis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomaa_W/0/1/0/all/0/1\">Walid Gomaa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interpretable Propaganda Detection in News Articles. (arXiv:2108.12802v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12802","description":"<p>Online users today are exposed to misleading and propagandistic news articles\nand media posts on a daily basis. To counter thus, a number of approaches have\nbeen designed aiming to achieve a healthier and safer online news and media\nconsumption. Automatic systems are able to support humans in detecting such\ncontent; yet, a major impediment to their broad adoption is that besides being\naccurate, the decisions of such systems need also to be interpretable in order\nto be trusted and widely adopted by users. Since misleading and propagandistic\ncontent influences readers through the use of a number of deception techniques,\nwe propose to detect and to show the use of such techniques as a way to offer\ninterpretability. In particular, we define qualitatively descriptive features\nand we analyze their suitability for detecting deception techniques. We further\nshow that our interpretable features can be easily combined with pre-trained\nlanguage models, yielding state-of-the-art results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Seunghak Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martino_G/0/1/0/all/0/1\">Giovanni Da San Martino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohtarami_M/0/1/0/all/0/1\">Mitra Mohtarami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glass_J/0/1/0/all/0/1\">James Glass</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DropAttack: A Masked Weight Adversarial Training Method to Improve Generalization of Neural Networks. (arXiv:2108.12805v1 [cs.LG])","link":"http://arxiv.org/abs/2108.12805","description":"<p>Adversarial training has been proven to be a powerful regularization method\nto improve the generalization of models. However, current adversarial training\nmethods only attack the original input sample or the embedding vectors, and\ntheir attacks lack coverage and diversity. To further enhance the breadth and\ndepth of attack, we propose a novel masked weight adversarial training method\ncalled DropAttack, which enhances generalization of model by adding\nintentionally worst-case adversarial perturbations to both the input and hidden\nlayers in different dimensions and minimize the adversarial risks generated by\neach layer. DropAttack is a general technique and can be adopt to a wide\nvariety of neural networks with different architectures. To validate the\neffectiveness of the proposed method, we used five public datasets in the\nfields of natural language processing (NLP) and computer vision (CV) for\nexperimental evaluating. We compare the proposed method with other adversarial\ntraining methods and regularization methods, and our method achieves\nstate-of-the-art on all datasets. In addition, Dropattack can achieve the same\nperformance when it use only a half training data compared to other standard\ntraining method. Theoretical analysis reveals that DropAttack can perform\ngradient regularization at random on some of the input and wight parameters of\nthe model. Further visualization experiments show that DropAttack can push the\nminimum risk of the model to a lower and flatter loss landscapes. Our source\ncode is publicly available on https://github.com/nishiwen1214/DropAttack.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ni_S/0/1/0/all/0/1\">Shiwen Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiawen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kao_H/0/1/0/all/0/1\">Hung-Yu Kao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Hybrid Rule-Based and Data-Driven Approach to Driver Modeling through Particle Filtering. (arXiv:2108.12820v1 [cs.RO])","link":"http://arxiv.org/abs/2108.12820","description":"<p>Autonomous vehicles need to model the behavior of surrounding human driven\nvehicles to be safe and efficient traffic participants. Existing approaches to\nmodeling human driving behavior have relied on both data-driven and rule-based\nmethods. While data-driven models are more expressive, rule-based models are\ninterpretable, which is an important requirement for safety-critical domains\nlike driving. However, rule-based models are not sufficiently representative of\ndata, and data-driven models are yet unable to generate realistic traffic\nsimulation due to unrealistic driving behavior such as collisions. In this\npaper, we propose a methodology that combines rule-based modeling with\ndata-driven learning. While the rules are governed by interpretable parameters\nof the driver model, these parameters are learned online from driving\ndemonstration data using particle filtering. We perform driver modeling\nexperiments on the task of highway driving and merging using data from three\nreal-world driving demonstration datasets. Our results show that driver models\nbased on our hybrid rule-based and data-driven approach can accurately capture\nreal-world driving behavior. Further, we assess the realism of the driving\nbehavior generated by our model by having humans perform a driving Turing test,\nwhere they are asked to distinguish between videos of real driving and those\ngenerated using our driver models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharyya_R/0/1/0/all/0/1\">Raunak Bhattacharyya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_S/0/1/0/all/0/1\">Soyeon Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kruse_L/0/1/0/all/0/1\">Liam Kruse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Senanayake_R/0/1/0/all/0/1\">Ransalu Senanayake</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kochenderfer_M/0/1/0/all/0/1\">Mykel Kochenderfer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Flow-Guided Video Inpainting with Scene Templates. (arXiv:2108.12845v1 [cs.CV])","link":"http://arxiv.org/abs/2108.12845","description":"<p>We consider the problem of filling in missing spatio-temporal regions of a\nvideo. We provide a novel flow-based solution by introducing a generative model\nof images in relation to the scene (without missing regions) and mappings from\nthe scene to images. We use the model to jointly infer the scene template, a 2D\nrepresentation of the scene, and the mappings. This ensures consistency of the\nframe-to-frame flows generated to the underlying scene, reducing geometric\ndistortions in flow based inpainting. The template is mapped to the missing\nregions in the video by a new L2-L1 interpolation scheme, creating crisp\ninpaintings and reducing common blur and distortion artifacts. We show on two\nbenchmark datasets that our approach out-performs state-of-the-art\nquantitatively and in user studies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lao_D/0/1/0/all/0/1\">Dong Lao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_P/0/1/0/all/0/1\">Peihao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wonka_P/0/1/0/all/0/1\">Peter Wonka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sundaramoorthi_G/0/1/0/all/0/1\">Ganesh Sundaramoorthi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Network Gaussian Processes by Increasing Depth. (arXiv:2108.12862v1 [cs.LG])","link":"http://arxiv.org/abs/2108.12862","description":"<p>Recent years have witnessed an increasing interest in the correspondence\nbetween infinitely wide networks and Gaussian processes. Despite the\neffectiveness and elegance of the current neural network Gaussian process\ntheory, to the best of our knowledge, all the neural network Gaussian processes\nare essentially induced by increasing width. However, in the era of deep\nlearning, what concerns us more regarding a neural network is its depth as well\nas how depth impacts the behaviors of a network. Inspired by a width-depth\nsymmetry consideration, we use a shortcut network to show that increasing the\ndepth of a neural network can also give rise to a Gaussian process, which is a\nvaluable addition to the existing theory and contributes to revealing the true\npicture of deep learning. Beyond the proposed Gaussian process by depth, we\ntheoretically characterize its uniform tightness property and the smallest\neigenvalue of its associated kernel. These characterizations can not only\nenhance our understanding of the proposed depth-induced Gaussian processes, but\nalso pave the way for future applications. Lastly, we examine the performance\nof the proposed Gaussian process by regression experiments on two real-world\ndata sets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shao-Qun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_F/0/1/0/all/0/1\">Feng-Lei Fan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating Answer Candidates for Quizzes and Answer-Aware Question Generators. (arXiv:2108.12898v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12898","description":"<p>In education, open-ended quiz questions have become an important tool for\nassessing the knowledge of students. Yet, manually preparing such questions is\na tedious task, and thus automatic question generation has been proposed as a\npossible alternative. So far, the vast majority of research has focused on\ngenerating the question text, relying on question answering datasets with\nreadily picked answers, and the problem of how to come up with answer\ncandidates in the first place has been largely ignored. Here, we aim to bridge\nthis gap. In particular, we propose a model that can generate a specified\nnumber of answer candidates for a given passage of text, which can then be used\nby instructors to write questions manually or can be passed as an input to\nautomatic answer-aware question generators. Our experiments show that our\nproposed answer candidate generation model outperforms several baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vachev_K/0/1/0/all/0/1\">Kristiyan Vachev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hardalov_M/0/1/0/all/0/1\">Momchil Hardalov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karadzhov_G/0/1/0/all/0/1\">Georgi Karadzhov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Georgiev_G/0/1/0/all/0/1\">Georgi Georgiev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koychev_I/0/1/0/all/0/1\">Ivan Koychev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lipschitz Continuity Guided Knowledge Distillation. (arXiv:2108.12905v1 [cs.LG])","link":"http://arxiv.org/abs/2108.12905","description":"<p>Knowledge distillation has become one of the most important model compression\ntechniques by distilling knowledge from larger teacher networks to smaller\nstudent ones. Although great success has been achieved by prior distillation\nmethods via delicately designing various types of knowledge, they overlook the\nfunctional properties of neural networks, which makes the process of applying\nthose techniques to new tasks unreliable and non-trivial. To alleviate such\nproblem, in this paper, we initially leverage Lipschitz continuity to better\nrepresent the functional characteristic of neural networks and guide the\nknowledge distillation process. In particular, we propose a novel Lipschitz\nContinuity Guided Knowledge Distillation framework to faithfully distill\nknowledge by minimizing the distance between two neural networks' Lipschitz\nconstants, which enables teacher networks to better regularize student networks\nand improve the corresponding performance. We derive an explainable\napproximation algorithm with an explicit theoretical derivation to address the\nNP-hard problem of calculating the Lipschitz constant. Experimental results\nhave shown that our method outperforms other benchmarks over several knowledge\ndistillation tasks (e.g., classification, segmentation and object detection) on\nCIFAR-100, ImageNet, and PASCAL VOC datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shang_Y/0/1/0/all/0/1\">Yuzhang Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_B/0/1/0/all/0/1\">Bin Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zong_Z/0/1/0/all/0/1\">Ziliang Zong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_L/0/1/0/all/0/1\">Liqiang Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yan Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KO codes: Inventing Nonlinear Encoding and Decoding for Reliable Wireless Communication via Deep-learning. (arXiv:2108.12920v1 [cs.IT])","link":"http://arxiv.org/abs/2108.12920","description":"<p>Landmark codes underpin reliable physical layer communication, e.g.,\nReed-Muller, BCH, Convolution, Turbo, LDPC and Polar codes: each is a linear\ncode and represents a mathematical breakthrough. The impact on humanity is\nhuge: each of these codes has been used in global wireless communication\nstandards (satellite, WiFi, cellular). Reliability of communication over the\nclassical additive white Gaussian noise (AWGN) channel enables benchmarking and\nranking of the different codes. In this paper, we construct KO codes, a\ncomputationaly efficient family of deep-learning driven (encoder, decoder)\npairs that outperform the state-of-the-art reliability performance on the\nstandardized AWGN channel. KO codes beat state-of-the-art Reed-Muller and Polar\ncodes, under the low-complexity successive cancellation decoding, in the\nchallenging short-to-medium block length regime on the AWGN channel. We show\nthat the gains of KO codes are primarily due to the nonlinear mapping of\ninformation bits directly to transmit real symbols (bypassing modulation) and\nyet possess an efficient, high performance decoder. The key technical\ninnovation that renders this possible is design of a novel family of neural\narchitectures inspired by the computation tree of the {\\bf K}ronecker {\\bf\nO}peration (KO) central to Reed-Muller and Polar codes. These architectures\npave way for the discovery of a much richer class of hitherto unexplored\nnonlinear algebraic structures. The code is available at\n\\href{https://github.com/deepcomm/KOcodes}{https://github.com/deepcomm/KOcodes}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Makkuva_A/0/1/0/all/0/1\">Ashok Vardhan Makkuva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jamali_M/0/1/0/all/0/1\">Mohammad Vahid Jamali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahdavifar_H/0/1/0/all/0/1\">Hessam Mahdavifar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1\">Sewoong Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Viswanath_P/0/1/0/all/0/1\">Pramod Viswanath</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distributed Swarm Collision Avoidance Based on Angular Calculations. (arXiv:2108.12934v1 [cs.RO])","link":"http://arxiv.org/abs/2108.12934","description":"<p>Collision avoidance is one of the most important topics in the robotics\nfield. The goal is to move the robots from initial locations to target\nlocations such that they follow shortest non-colliding paths in the shortest\ntime and with the least amount of energy. In this paper, a distributed and\nreal-time algorithm for dense and complex 2D and 3D environments is proposed.\nThis algorithm uses angular calculations to select the optimal direction for\nthe movement of each robot and it has been shown that these separate\ncalculations lead to a form of cooperative behavior among agents. We evaluated\nthe proposed approach on various simulation and experimental scenarios and\ncompared the results with FMP and ORCA, two important algorithms in this field.\nThe results show that the proposed approach is at least 25% faster than ORCA\nand at least 7% faster than FMP and also more reliable than both methods. The\nproposed method is shown to enable fully autonomous navigation of a swarm of\ncrazyflies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qazavi_S/0/1/0/all/0/1\">SeyedZahir Qazavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Semnani_S/0/1/0/all/0/1\">Samaneh Hosseini Semnani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RetroGAN: A Cyclic Post-Specialization System for Improving Out-of-Knowledge and Rare Word Representations. (arXiv:2108.12941v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12941","description":"<p>Retrofitting is a technique used to move word vectors closer together or\nfurther apart in their space to reflect their relationships in a Knowledge Base\n(KB). However, retrofitting only works on concepts that are present in that KB.\nRetroGAN uses a pair of Generative Adversarial Networks (GANs) to learn a\none-to-one mapping between concepts and their retrofitted counterparts. It\napplies that mapping (post-specializes) to handle concepts that do not appear\nin the original KB in a manner similar to how some natural language systems\nhandle out-of-vocabulary entries. We test our system on three word-similarity\nbenchmarks and a downstream sentence simplification task and achieve the state\nof the art (CARD-660). Altogether, our results demonstrate our system's\neffectiveness for out-of-knowledge and rare word generalization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Colon_Hernandez_P/0/1/0/all/0/1\">Pedro Colon-Hernandez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xin_Y/0/1/0/all/0/1\">Yida Xin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lieberman_H/0/1/0/all/0/1\">Henry Lieberman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Havasi_C/0/1/0/all/0/1\">Catherine Havasi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Breazeal_C/0/1/0/all/0/1\">Cynthia Breazeal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chin_P/0/1/0/all/0/1\">Peter Chin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Searching for Two-Stream Models in Multivariate Space for Video Recognition. (arXiv:2108.12957v1 [cs.CV])","link":"http://arxiv.org/abs/2108.12957","description":"<p>Conventional video models rely on a single stream to capture the complex\nspatial-temporal features. Recent work on two-stream video models, such as\nSlowFast network and AssembleNet, prescribe separate streams to learn\ncomplementary features, and achieve stronger performance. However, manually\ndesigning both streams as well as the in-between fusion blocks is a daunting\ntask, requiring to explore a tremendously large design space. Such manual\nexploration is time-consuming and often ends up with sub-optimal architectures\nwhen computational resources are limited and the exploration is insufficient.\nIn this work, we present a pragmatic neural architecture search approach, which\nis able to search for two-stream video models in giant spaces efficiently. We\ndesign a multivariate search space, including 6 search variables to capture a\nwide variety of choices in designing two-stream models. Furthermore, we propose\na progressive search procedure, by searching for the architecture of individual\nstreams, fusion blocks, and attention blocks one after the other. We\ndemonstrate two-stream models with significantly better performance can be\nautomatically discovered in our design space. Our searched two-stream models,\nnamely Auto-TSNet, consistently outperform other models on standard benchmarks.\nOn Kinetics, compared with the SlowFast model, our Auto-TSNet-L model reduces\nFLOPS by nearly 11 times while achieving the same accuracy 78.9%. On\nSomething-Something-V2, Auto-TSNet-M improves the accuracy by at least 2% over\nother methods which use less than 50 GFLOPS per video.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gong_X/0/1/0/all/0/1\">Xinyu Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Heng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shou_Z/0/1/0/all/0/1\">Zheng Shou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feiszli_M/0/1/0/all/0/1\">Matt Feiszli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zhicheng Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3DStyleNet: Creating 3D Shapes with Geometric and Texture Style Variations. (arXiv:2108.12958v1 [cs.CV])","link":"http://arxiv.org/abs/2108.12958","description":"<p>We propose a method to create plausible geometric and texture style\nvariations of 3D objects in the quest to democratize 3D content creation. Given\na pair of textured source and target objects, our method predicts a part-aware\naffine transformation field that naturally warps the source shape to imitate\nthe overall geometric style of the target. In addition, the texture style of\nthe target is transferred to the warped source object with the help of a\nmulti-view differentiable renderer. Our model, 3DStyleNet, is composed of two\nsub-networks trained in two stages. First, the geometric style network is\ntrained on a large set of untextured 3D shapes. Second, we jointly optimize our\ngeometric style network and a pre-trained image style transfer network with\nlosses defined over both the geometry and the rendering of the result. Given a\nsmall set of high-quality textured objects, our method can create many novel\nstylized shapes, resulting in effortless 3D content creation and style-ware\ndata augmentation. We showcase our approach qualitatively on 3D content\nstylization, and provide user studies to validate the quality of our results.\nIn addition, our method can serve as a valuable tool to create 3D data\naugmentations for computer vision tasks. Extensive quantitative analysis shows\nthat 3DStyleNet outperforms alternative data augmentation techniques for the\ndownstream task of single-image 3D reconstruction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yin_K/0/1/0/all/0/1\">Kangxue Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jun Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shugrina_M/0/1/0/all/0/1\">Maria Shugrina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khamis_S/0/1/0/all/0/1\">Sameh Khamis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fidler_S/0/1/0/all/0/1\">Sanja Fidler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"X2Teeth: 3D Teeth Reconstruction from a Single Panoramic Radiograph. (arXiv:2108.13004v1 [cs.CV])","link":"http://arxiv.org/abs/2108.13004","description":"<p>3D teeth reconstruction from X-ray is important for dental diagnosis and many\nclinical operations. However, no existing work has explored the reconstruction\nof teeth for a whole cavity from a single panoramic radiograph. Different from\nsingle object reconstruction from photos, this task has the unique challenge of\nconstructing multiple objects at high resolutions. To conquer this task, we\ndevelop a novel ConvNet X2Teeth that decomposes the task into teeth\nlocalization and single-shape estimation. We also introduce a patch-based\ntraining strategy, such that X2Teeth can be end-to-end trained for optimal\nperformance. Extensive experiments show that our method can successfully\nestimate the 3D structure of the cavity and reflect the details for each tooth.\nMoreover, X2Teeth achieves a reconstruction IoU of 0.681, which significantly\noutperforms the encoder-decoder method by $1.71X and the retrieval-based method\nby $1.52X. Our method can also be promising for other multi-anatomy 3D\nreconstruction tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yuan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_W/0/1/0/all/0/1\">Weinan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jiawei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_L/0/1/0/all/0/1\">Liang Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Lei He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Communication-Computation Efficient Device-Edge Co-Inference via AutoML. (arXiv:2108.13009v1 [cs.LG])","link":"http://arxiv.org/abs/2108.13009","description":"<p>Device-edge co-inference, which partitions a deep neural network between a\nresource-constrained mobile device and an edge server, recently emerges as a\npromising paradigm to support intelligent mobile applications. To accelerate\nthe inference process, on-device model sparsification and intermediate feature\ncompression are regarded as two prominent techniques. However, as the on-device\nmodel sparsity level and intermediate feature compression ratio have direct\nimpacts on computation workload and communication overhead respectively, and\nboth of them affect the inference accuracy, finding the optimal values of these\nhyper-parameters brings a major challenge due to the large search space. In\nthis paper, we endeavor to develop an efficient algorithm to determine these\nhyper-parameters. By selecting a suitable model split point and a pair of\nencoder/decoder for the intermediate feature vector, this problem is casted as\na sequential decision problem, for which, a novel automated machine learning\n(AutoML) framework is proposed based on deep reinforcement learning (DRL).\nExperiment results on an image classification task demonstrate the\neffectiveness of the proposed framework in achieving a better\ncommunication-computation trade-off and significant inference speedup against\nvarious baseline schemes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinjie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1\">Jiawei Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1\">Yuyi Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jun Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Temporal Knowledge Graph Completion Method Based on Balanced Timestamp Distribution. (arXiv:2108.13024v1 [cs.AI])","link":"http://arxiv.org/abs/2108.13024","description":"<p>Completion through the embedding representation of the knowledge graph (KGE)\nhas been a research hotspot in recent years. Realistic knowledge graphs are\nmostly related to time, while most of the existing KGE algorithms ignore the\ntime information. A few existing methods directly or indirectly encode the time\ninformation, ignoring the balance of timestamp distribution, which greatly\nlimits the performance of temporal knowledge graph completion (KGC). In this\npaper, a temporal KGC method is proposed based on the direct encoding time\ninformation framework, and a given time slice is treated as the finest\ngranularity for balanced timestamp distribution. A large number of experiments\non temporal knowledge graph datasets extracted from the real world demonstrate\nthe effectiveness of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kangzheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuhong Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transport-based Counterfactual Models. (arXiv:2108.13025v1 [cs.AI])","link":"http://arxiv.org/abs/2108.13025","description":"<p>Counterfactual frameworks have grown popular in explainable and fair machine\nlearning, as they offer a natural notion of causation. However,\nstate-of-the-art models to compute counterfactuals are either unrealistic or\nunfeasible. In particular, while Pearl's causal inference provides appealing\nrules to calculate counterfactuals, it relies on a model that is unknown and\nhard to discover in practice. We address the problem of designing realistic and\nfeasible counterfactuals in the absence of a causal model. We define\ntransport-based counterfactual models as collections of joint probability\ndistributions between observable distributions, and show their connection to\ncausal counterfactuals. More specifically, we argue that optimal transport\ntheory defines relevant transport-based counterfactual models, as they are\nnumerically feasible, statistically-faithful, and can even coincide with causal\ncounterfactual models. We illustrate the practicality of these models by\ndefining sharper fairness criteria than typical group fairness conditions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lara_L/0/1/0/all/0/1\">Lucas de Lara</a> (IMT), <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Sanz_A/0/1/0/all/0/1\">Alberto Gonz&#xe1;lez-Sanz</a> (IMT), <a href=\"http://arxiv.org/find/cs/1/au:+Asher_N/0/1/0/all/0/1\">Nicholas Asher</a> (IRIT-MELODI, CNRS), <a href=\"http://arxiv.org/find/cs/1/au:+Loubes_J/0/1/0/all/0/1\">Jean-Michel Loubes</a> (IMT)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SurRoL: An Open-source Reinforcement Learning Centered and dVRK Compatible Platform for Surgical Robot Learning. (arXiv:2108.13035v1 [cs.RO])","link":"http://arxiv.org/abs/2108.13035","description":"<p>Autonomous surgical execution relieves tedious routines and surgeon's\nfatigue. Recent learning-based methods, especially reinforcement learning (RL)\nbased methods, achieve promising performance for dexterous manipulation, which\nusually requires the simulation to collect data efficiently and reduce the\nhardware cost. The existing learning-based simulation platforms for medical\nrobots suffer from limited scenarios and simplified physical interactions,\nwhich degrades the real-world performance of learned policies. In this work, we\ndesigned SurRoL, an RL-centered simulation platform for surgical robot learning\ncompatible with the da Vinci Research Kit (dVRK). The designed SurRoL\nintegrates a user-friendly RL library for algorithm development and a real-time\nphysics engine, which is able to support more PSM/ECM scenarios and more\nrealistic physical interactions. Ten learning-based surgical tasks are built in\nthe platform, which are common in the real autonomous surgical execution. We\nevaluate SurRoL using RL algorithms in simulation, provide in-depth analysis,\ndeploy the trained policies on the real dVRK, and show that our SurRoL achieves\nbetter transferability in the real world.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jiaqi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_B/0/1/0/all/0/1\">Bo Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yun-Hui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_Q/0/1/0/all/0/1\">Qi Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heng_P/0/1/0/all/0/1\">Pheng-Ann Heng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Aleatoric Description Logic for Probailistic Reasoning (Long Version). (arXiv:2108.13036v1 [cs.AI])","link":"http://arxiv.org/abs/2108.13036","description":"<p>Description logics are a powerful tool for describing ontological knowledge\nbases. That is, they give a factual account of the world in terms of\nindividuals, concepts and relations. In the presence of uncertainty, such\nfactual accounts are not feasible, and a subjective or epistemic approach is\nrequired. Aleatoric description logic models uncertainty in the world as\naleatoric events, by the roll of the dice, where an agent has subjective\nbeliefs about the bias of these dice. This provides a subjective Bayesian\ndescription logic, where propositions and relations are assigned probabilities\naccording to what a rational agent would bet, given a configuration of possible\nindividuals and dice. Aleatoric description logic is shown to generalise the\ndescription logic ALC, and can be seen to describe a probability space of\ninterpretations of a restriction of ALC where all roles are functions. Several\ncomputational problems are considered and model-checking and consistency\nchecking algorithms are presented. Finally, aleatoric description logic is\nshown to be able to model learning, where agents are able to condition their\nbeliefs on the bias of dice according to observations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+French_T/0/1/0/all/0/1\">Tim French</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smoker_T/0/1/0/all/0/1\">Tom Smoker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Integrated Decision and Control at Multi-Lane Intersections with Mixed Traffic Flow. (arXiv:2108.13038v1 [cs.LG])","link":"http://arxiv.org/abs/2108.13038","description":"<p>Autonomous driving at intersections is one of the most complicated and\naccident-prone traffic scenarios, especially with mixed traffic participants\nsuch as vehicles, bicycles and pedestrians. The driving policy should make safe\ndecisions to handle the dynamic traffic conditions and meet the requirements of\non-board computation. However, most of the current researches focuses on\nsimplified intersections considering only the surrounding vehicles and\nidealized traffic lights. This paper improves the integrated decision and\ncontrol framework and develops a learning-based algorithm to deal with complex\nintersections with mixed traffic flows, which can not only take account of\nrealistic characteristics of traffic lights, but also learn a safe policy under\ndifferent safety constraints. We first consider different velocity models for\ngreen and red lights in the training process and use a finite state machine to\nhandle different modes of light transformation. Then we design different types\nof distance constraints for vehicles, traffic lights, pedestrians, bicycles\nrespectively and formulize the constrained optimal control problems (OCPs) to\nbe optimized. Finally, reinforcement learning (RL) with value and policy\nnetworks is adopted to solve the series of OCPs. In order to verify the safety\nand efficiency of the proposed method, we design a multi-lane intersection with\nthe existence of large-scale mixed traffic participants and set practical\ntraffic light phases. The simulation results indicate that the trained decision\nand control policy can well balance safety and tracking performance. Compared\nwith model predictive control (MPC), the computational time is three orders of\nmagnitude lower.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jianhua Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Yangang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_Y/0/1/0/all/0/1\">Yang Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shengbo Eben Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1\">Yuming Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1\">Xiaoping Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Auto-Split: A General Framework of Collaborative Edge-Cloud AI. (arXiv:2108.13041v1 [cs.LG])","link":"http://arxiv.org/abs/2108.13041","description":"<p>In many industry scale applications, large and resource consuming machine\nlearning models reside in powerful cloud servers. At the same time, large\namounts of input data are collected at the edge of cloud. The inference results\nare also communicated to users or passed to downstream tasks at the edge. The\nedge often consists of a large number of low-power devices. It is a big\nchallenge to design industry products to support sophisticated deep model\ndeployment and conduct model inference in an efficient manner so that the model\naccuracy remains high and the end-to-end latency is kept low. This paper\ndescribes the techniques and engineering practice behind Auto-Split, an\nedge-cloud collaborative prototype of Huawei Cloud. This patented technology is\nalready validated on selected applications, is on its way for broader\nsystematic edge-cloud application integration, and is being made available for\npublic use as an automated pipeline service for end-to-end cloud-edge\ncollaborative intelligence deployment. To the best of our knowledge, there is\nno existing industry product that provides the capability of Deep Neural\nNetwork (DNN) splitting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Banitalebi_Dehkordi_A/0/1/0/all/0/1\">Amin Banitalebi-Dehkordi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vedula_N/0/1/0/all/0/1\">Naveen Vedula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pei_J/0/1/0/all/0/1\">Jian Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1\">Fei Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lanjun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yong Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Demystifying Drug Repurposing Domain Comprehension with Knowledge Graph Embedding. (arXiv:2108.13051v1 [cs.LG])","link":"http://arxiv.org/abs/2108.13051","description":"<p>Drug repurposing is more relevant than ever due to drug development's rising\ncosts and the need to respond to emerging diseases quickly. Knowledge graph\nembedding enables drug repurposing using heterogeneous data sources combined\nwith state-of-the-art machine learning models to predict new drug-disease links\nin the knowledge graph. As in many machine learning applications, significant\nwork is still required to understand the predictive models' behavior. We\npropose a structured methodology to understand better machine learning models'\nresults for drug repurposing, suggesting key elements of the knowledge graph to\nimprove predictions while saving computational resources. We reduce the\ntraining set of 11.05% and the embedding space by 31.87%, with only a 2%\naccuracy reduction, and increase accuracy by 60% on the open ogbl-biokg graph\nadding only 1.53% new triples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ramalli_E/0/1/0/all/0/1\">Edoardo Ramalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parravicini_A/0/1/0/all/0/1\">Alberto Parravicini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Donato_G/0/1/0/all/0/1\">Guido Walter Di Donato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salaris_M/0/1/0/all/0/1\">Mirko Salaris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hudelot_C/0/1/0/all/0/1\">C&#xe9;line Hudelot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santambrogio_M/0/1/0/all/0/1\">Marco Domenico Santambrogio</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Satisfiability and Containment of Recursive SHACL. (arXiv:2108.13063v1 [cs.AI])","link":"http://arxiv.org/abs/2108.13063","description":"<p>The Shapes Constraint Language (SHACL) is the recent W3C recommendation\nlanguage for validating RDF data, by verifying certain shapes on graphs.\nPrevious work has largely focused on the validation problem and the standard\ndecision problems of satisfiability and containment, crucial for design and\noptimisation purposes, have only been investigated for simplified versions of\nSHACL. Moreover, the SHACL specification does not define the semantics of\nrecursively-defined constraints, which led to several alternative recursive\nsemantics being proposed in the literature. The interaction between these\ndifferent semantics and important decision problems has not been investigated\nyet. In this article we provide a comprehensive study of the different features\nof SHACL, by providing a translation to a new first-order language, called SCL,\nthat precisely captures the semantics of SHACL. We also present MSCL, a\nsecond-order extension of SCL, which allows us to define, in a single formal\nlogic framework, the main recursive semantics of SHACL. Within this language we\nalso provide an effective treatment of filter constraints which are often\nneglected in the related literature. Using this logic we provide a detailed map\nof (un)decidability and complexity results for the satisfiability and\ncontainment decision problems for different SHACL fragments. Notably, we prove\nthat both problems are undecidable for the full language, but we present\ndecidable combinations of interesting features, even in the face of recursion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pareti_P/0/1/0/all/0/1\">Paolo Pareti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konstantinidis_G/0/1/0/all/0/1\">George Konstantinidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mogavero_F/0/1/0/all/0/1\">Fabio Mogavero</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"To tune or not to tune? An Approach for Recommending Important Hyperparameters. (arXiv:2108.13066v1 [cs.LG])","link":"http://arxiv.org/abs/2108.13066","description":"<p>Novel technologies in automated machine learning ease the complexity of\nalgorithm selection and hyperparameter optimization. Hyperparameters are\nimportant for machine learning models as they significantly influence the\nperformance of machine learning models. Many optimization techniques have\nachieved notable success in hyperparameter tuning and surpassed the performance\nof human experts. However, depending on such techniques as blackbox algorithms\ncan leave machine learning practitioners without insight into the relative\nimportance of different hyperparameters. In this paper, we consider building\nthe relationship between the performance of the machine learning models and\ntheir hyperparameters to discover the trend and gain insights, with empirical\nresults based on six classifiers and 200 datasets. Our results enable users to\ndecide whether it is worth conducting a possibly time-consuming tuning\nstrategy, to focus on the most important hyperparameters, and to choose\nadequate hyperparameter spaces for tuning. The results of our experiments show\nthat gradient boosting and Adaboost outperform other classifiers across 200\nproblems. However, they need tuning to boost their performance. Overall, the\nresults obtained from this study provide a quantitative basis to focus efforts\ntoward guided automated hyperparameter optimization and contribute toward the\ndevelopment of better-automated machine learning frameworks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bahmani_M/0/1/0/all/0/1\">Mohamadjavad Bahmani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shawi_R/0/1/0/all/0/1\">Radwa El Shawi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potikyan_N/0/1/0/all/0/1\">Nshan Potikyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sakr_S/0/1/0/all/0/1\">Sherif Sakr</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Introduction to Variational Inference. (arXiv:2108.13083v1 [cs.LG])","link":"http://arxiv.org/abs/2108.13083","description":"<p>Approximating complex probability densities is a core problem in modern\nstatistics. In this paper, we introduce the concept of Variational Inference\n(VI), a popular method in machine learning that uses optimization techniques to\nestimate complex probability densities. This property allows VI to converge\nfaster than classical methods, such as, Markov Chain Monte Carlo sampling.\nConceptually, VI works by choosing a family of probability density functions\nand then finding the one closest to the actual probability density -- often\nusing the Kullback-Leibler (KL) divergence as the optimization metric. We\nintroduce the Evidence Lower Bound to tractably compute the approximated\nprobability density and we review the ideas behind mean-field variational\ninference. Finally, we discuss the applications of VI to variational\nauto-encoders (VAE) and VAE-Generative Adversarial Network (VAE-GAN). With this\npaper, we aim to explain the concept of VI and assist in future research with\nthis approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ganguly_A/0/1/0/all/0/1\">Ankush Ganguly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Earp_S/0/1/0/all/0/1\">Samuel W. F. Earp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating Vulnerabilities of Deep Neural Policies. (arXiv:2108.13093v1 [cs.LG])","link":"http://arxiv.org/abs/2108.13093","description":"<p>Reinforcement learning policies based on deep neural networks are vulnerable\nto imperceptible adversarial perturbations to their inputs, in much the same\nway as neural network image classifiers. Recent work has proposed several\nmethods to improve the robustness of deep reinforcement learning agents to\nadversarial perturbations based on training in the presence of these\nimperceptible perturbations (i.e. adversarial training). In this paper, we\nstudy the effects of adversarial training on the neural policy learned by the\nagent. In particular, we follow two distinct parallel approaches to investigate\nthe outcomes of adversarial training on deep neural policies based on\nworst-case distributional shift and feature sensitivity. For the first\napproach, we compare the Fourier spectrum of minimal perturbations computed for\nboth adversarially trained and vanilla trained neural policies. Via experiments\nin the OpenAI Atari environments we show that minimal perturbations computed\nfor adversarially trained policies are more focused on lower frequencies in the\nFourier domain, indicating a higher sensitivity of these policies to low\nfrequency perturbations. For the second approach, we propose a novel method to\nmeasure the feature sensitivities of deep neural policies and we compare these\nfeature sensitivity differences in state-of-the-art adversarially trained deep\nneural policies and vanilla trained deep neural policies. We believe our\nresults can be an initial step towards understanding the relationship between\nadversarial training and different notions of robustness for neural policies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Korkmaz_E/0/1/0/all/0/1\">Ezgi Korkmaz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Sentiment Analysis Dataset for Trustworthiness Evaluation. (arXiv:2108.13140v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13140","description":"<p>While deep learning models have greatly improved the performance of most\nartificial intelligence tasks, they are often criticized to be untrustworthy\ndue to the black-box problem. Consequently, many works have been proposed to\nstudy the trustworthiness of deep learning. However, as most open datasets are\ndesigned for evaluating the accuracy of model outputs, there is still a lack of\nappropriate datasets for evaluating the inner workings of neural networks. The\nlack of datasets obviously hinders the development of trustworthiness research.\nTherefore, in order to systematically evaluate the factors for building\ntrustworthy systems, we propose a novel and well-annotated sentiment analysis\ndataset to evaluate robustness and interpretability. To evaluate these factors,\nour dataset contains diverse annotations about the challenging distribution of\ninstances, manual adversarial instances and sentiment explanations. Several\nevaluation metrics are further proposed for interpretability and robustness.\nBased on the dataset and metrics, we conduct comprehensive comparisons for the\ntrustworthiness of three typical models, and also study the relations between\naccuracy, robustness and interpretability. We release this trustworthiness\nevaluation dataset at \\url{https://github/xyz} and hope our work can facilitate\nthe progress on building more trustworthy systems for real-world applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lijie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1\">Shuyuan Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hongxuan Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1\">Xinyan Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Ying Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enterprise Architecture Model Transformation Engine. (arXiv:2108.13169v1 [cs.DC])","link":"http://arxiv.org/abs/2108.13169","description":"<p>With increasing linkage within value chains, the IT systems of different\ncompanies are also being connected with each other. This enables the\nintegration of services within the movement of Industry 4.0 in order to improve\nthe quality and performance of the processes. Enterprise architecture models\nform the basis for this with a better buisness IT-alignment. However, the\nheterogeneity of the modeling frameworks and description languages makes a\nconcatenation considerably difficult, especially differences in syntax,\nsemantic and relations. Therefore, this paper presents a transformation engine\nto convert enterprise architecture models between several languages. We\ndeveloped the first generic translation approach that is free of specific\nmeta-modeling, which is flexible adaptable to arbitrary modeling languages. The\ntransformation process is defined by various pattern matching techniques using\na rule-based description language. It uses set theory and first-order logic for\nan intuitive description as a basis. The concept is practical evaluated using\nan example in the area of a large German IT-service provider. Anyhow, the\napproach is applicable between a wide range of enterprise architecture\nframeworks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Heiland_E/0/1/0/all/0/1\">Erik Heiland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hillmann_P/0/1/0/all/0/1\">Peter Hillmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karcher_A/0/1/0/all/0/1\">Andreas Karcher</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Personalized Recommender System for Children's Book Recommendation with A Realtime Interactive Robot. (arXiv:1710.00310v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/1710.00310","description":"<p>In this paper we study the personalized book recommender system in a\nchild-robot interactive environment. Firstly, we propose a novel text search\nalgorithm using an inverse filtering mechanism that improves the efficiency.\nSecondly, we propose a user interest prediction method based on the Bayesian\nnetwork and a novel feedback mechanism. According to children's fuzzy language\ninput, the proposed method gives the predicted interests. Thirdly, the domain\nspecific synonym association is proposed based on word vectorization, in order\nto improve the understanding of user intention. Experimental results show that\nthe proposed recommender system has an improved performance and it can operate\non embedded consumer devices with limited computational resources.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_T/0/1/0/all/0/1\">Tianmeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_B/0/1/0/all/0/1\">Baolin Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chengwei Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inventory Balancing with Online Learning. (arXiv:1810.05640v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/1810.05640","description":"<p>We study a general problem of allocating limited resources to heterogeneous\ncustomers over time under model uncertainty. Each type of customer can be\nserviced using different actions, each of which stochastically consumes some\ncombination of resources, and returns different rewards for the resources\nconsumed. We consider a general model where the resource consumption\ndistribution associated with each (customer type, action)-combination is not\nknown, but is consistent and can be learned over time. In addition, the\nsequence of customer types to arrive over time is arbitrary and completely\nunknown.\n</p>\n<p>We overcome both the challenges of model uncertainty and customer\nheterogeneity by judiciously synthesizing two algorithmic frameworks from the\nliterature: inventory balancing, which \"reserves\" a portion of each resource\nfor high-reward customer types which could later arrive, and online learning,\nwhich shows how to \"explore\" the resource consumption distributions of each\ncustomer type under different actions. We define an auxiliary problem, which\nallows for existing competitive ratio and regret bounds to be seamlessly\nintegrated. Furthermore, we show that the performance guarantee generated by\nour framework is tight, that is, we provide an information-theoretic lower\nbound which shows that both the loss from competitive ratio and the loss for\nregret are relevant in the combined problem.\n</p>\n<p>Finally, we demonstrate the efficacy of our algorithms on a publicly\navailable hotel data set. Our framework is highly practical in that it requires\nno historical data (no fitted customer choice models, nor forecasting of\ncustomer arrival patterns) and can be used to initialize allocation strategies\nin fast-changing environments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheung_W/0/1/0/all/0/1\">Wang Chi Cheung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1\">Will Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simchi_Levi_D/0/1/0/all/0/1\">David Simchi-Levi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinshang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deduction Theorem: The Problematic Nature of Common Practice in Game Theory. (arXiv:1908.00409v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/1908.00409","description":"<p>We consider the Deduction Theorem used in the literature of game theory to\nrun a purported proof by contradiction. In the context of game theory, it is\nstated that if we have a proof of $\\phi \\vdash \\varphi$, then we also have a\nproof of $\\phi \\Rightarrow \\varphi$. Hence, the proof of $\\phi \\Rightarrow\n\\varphi$ is deduced from a previously known statement. However, we argue that\none has to manage to establish that a proof exists for the clauses $\\phi$ and\n$\\varphi$, i.e., they are known true statements in order to show that $\\phi\n\\vdash \\varphi$ is provable, and that therefore $\\phi \\Rightarrow \\varphi$ is\nprovable as well. Thus, we are not allowed to assume that the clause $\\phi$ or\n$\\varphi$ is a true statement. This leads immediately to a wrong conclusion.\nApart from this, we stress to other facts why the Deduction Theorem is not\napplicable to run a proof by contradiction. Finally, we present an example from\nindustrial cooperation where the Deduction Theorem is not correctly applied\nwith the consequence that the obtained result contradicts the well-known\naggregation issue.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meinhardt_H/0/1/0/all/0/1\">Holger I. Meinhardt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"First return, then explore. (arXiv:2004.12919v4 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2004.12919","description":"<p>The promise of reinforcement learning is to solve complex sequential decision\nproblems autonomously by specifying a high-level reward function only. However,\nreinforcement learning algorithms struggle when, as is often the case, simple\nand intuitive rewards provide sparse and deceptive feedback. Avoiding these\npitfalls requires thoroughly exploring the environment, but creating algorithms\nthat can do so remains one of the central challenges of the field. We\nhypothesise that the main impediment to effective exploration originates from\nalgorithms forgetting how to reach previously visited states (\"detachment\") and\nfrom failing to first return to a state before exploring from it\n(\"derailment\"). We introduce Go-Explore, a family of algorithms that addresses\nthese two challenges directly through the simple principles of explicitly\nremembering promising states and first returning to such states before\nintentionally exploring. Go-Explore solves all heretofore unsolved Atari games\nand surpasses the state of the art on all hard-exploration games, with orders\nof magnitude improvements on the grand challenges Montezuma's Revenge and\nPitfall. We also demonstrate the practical potential of Go-Explore on a\nsparse-reward pick-and-place robotics task. Additionally, we show that adding a\ngoal-conditioned policy can further improve Go-Explore's exploration efficiency\nand enable it to handle stochasticity throughout training. The substantial\nperformance gains from Go-Explore suggest that the simple principles of\nremembering states, returning to them, and exploring from them are a powerful\nand general approach to exploration, an insight that may prove critical to the\ncreation of truly intelligent learning agents.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ecoffet_A/0/1/0/all/0/1\">Adrien Ecoffet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huizinga_J/0/1/0/all/0/1\">Joost Huizinga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lehman_J/0/1/0/all/0/1\">Joel Lehman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stanley_K/0/1/0/all/0/1\">Kenneth O. Stanley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clune_J/0/1/0/all/0/1\">Jeff Clune</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Implementing Agent-Based Systems via Computability Logic CL2. (arXiv:2010.08925v3 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2010.08925","description":"<p>Computability logic(CoL) is a powerful computational model. In this paper, we\nshow that CoL naturally supports multi-agent programming models where resources\n(coffee for example) are involved. To be specific, we discuss an implementation\nof the Starbucks based on CoL (CL2 to be exact).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kwon_K/0/1/0/all/0/1\">Keehang Kwon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Anonymizing Sensor Data on the Edge: A Representation Learning and Transformation Approach. (arXiv:2011.08315v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2011.08315","description":"<p>The abundance of data collected by sensors in Internet of Things (IoT)\ndevices, and the success of deep neural networks in uncovering hidden patterns\nin time series data have led to mounting privacy concerns. This is because\nprivate and sensitive information can be potentially learned from sensor data\nby applications that have access to this data. In this paper, we aim to examine\nthe tradeoff between utility and privacy loss by learning low-dimensional\nrepresentations that are useful for data obfuscation. We propose deterministic\nand probabilistic transformations in the latent space of a variational\nautoencoder to synthesize time series data such that intrusive inferences are\nprevented while desired inferences can still be made with sufficient accuracy.\nIn the deterministic case, we use a linear transformation to move the\nrepresentation of input data in the latent space such that the reconstructed\ndata is likely to have the same public attribute but a different private\nattribute than the original input data. In the probabilistic case, we apply the\nlinear transformation to the latent representation of input data with some\nprobability. We compare our technique with autoencoder-based anonymization\ntechniques and additionally show that it can anonymize data in real time on\nresource-constrained edge devices.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hajihassani_O/0/1/0/all/0/1\">Omid Hajihassani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ardakanian_O/0/1/0/all/0/1\">Omid Ardakanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khazaei_H/0/1/0/all/0/1\">Hamzeh Khazaei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FairOD: Fairness-aware Outlier Detection. (arXiv:2012.03063v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2012.03063","description":"<p>Fairness and Outlier Detection (OD) are closely related, as it is exactly the\ngoal of OD to spot rare, minority samples in a given population. However, when\nbeing a minority (as defined by protected variables, such as\nrace/ethnicity/sex/age) does not reflect positive-class membership (such as\ncriminal/fraud), OD produces unjust outcomes. Surprisingly, fairness-aware OD\nhas been almost untouched in prior work, as fair machine learning literature\nmainly focuses on supervised settings. Our work aims to bridge this gap.\nSpecifically, we develop desiderata capturing well-motivated fairness criteria\nfor OD, and systematically formalize the fair OD problem. Further, guided by\nour desiderata, we propose FairOD, a fairness-aware outlier detector that has\nthe following desirable properties: FairOD (1) exhibits treatment parity at\ntest time, (2) aims to flag equal proportions of samples from all groups (i.e.\nobtain group fairness, via statistical parity), and (3) strives to flag truly\nhigh-risk samples within each group. Extensive experiments on a diverse set of\nsynthetic and real world datasets show that FairOD produces outcomes that are\nfair with respect to protected variables, while performing comparable to (and\nin some cases, even better than) fairness-agnostic detectors in terms of\ndetection performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shekhar_S/0/1/0/all/0/1\">Shubhranshu Shekhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_N/0/1/0/all/0/1\">Neil Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akoglu_L/0/1/0/all/0/1\">Leman Akoglu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Layer Distillation with Semantic Calibration. (arXiv:2012.03236v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.03236","description":"<p>Knowledge distillation is a technique to enhance the generalization ability\nof a student model by exploiting outputs from a teacher model. Recently,\nfeature-map based variants explore knowledge transfer between manually assigned\nteacher-student pairs in intermediate layers for further improvement. However,\nlayer semantics may vary in different neural networks and semantic mismatch in\nmanual layer associations will lead to performance degeneration due to negative\nregularization. To address this issue, we propose Semantic Calibration for\ncross-layer Knowledge Distillation (SemCKD), which automatically assigns proper\ntarget layers of the teacher model for each student layer with an attention\nmechanism. With a learned attention distribution, each student layer distills\nknowledge contained in multiple teacher layers rather than a specific\nintermediate layer for appropriate cross-layer supervision. We further provide\ntheoretical analysis of the association weights and conduct extensive\nexperiments to demonstrate the effectiveness of our approach. Code is avaliable\nat \\url{https://github.com/DefangChen/SemCKD}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Defang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_J/0/1/0/all/0/1\">Jian-Ping Mei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Can Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yan Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Query-free Black-box Adversarial Attacks on Graphs. (arXiv:2012.06757v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2012.06757","description":"<p>Adversarial attacks on graphs have attracted considerable research interests.\nExisting works assume the attacker is either (partly) aware of the victim\nmodel, or able to send queries to it. These assumptions are, however,\nunrealistic. To bridge the gap between theoretical graph attacks and real-world\nscenarios, in this work, we propose a novel and more realistic setting: strict\nblack-box graph attack, in which the attacker has no knowledge about the victim\nmodel at all and is not allowed to send any queries. To design such an attack\nstrategy, we first propose a generic graph filter to unify different families\nof graph-based models. The strength of attacks can then be quantified by the\nchange in the graph filter before and after attack. By maximizing this change,\nwe are able to find an effective attack strategy, regardless of the underlying\nmodel. To solve this optimization problem, we also propose a relaxation\ntechnique and approximation theories to reduce the difficulty as well as the\ncomputational expense. Experiments demonstrate that, even with no exposure to\nthe model, the Macro-F1 drops 6.4% in node classification and 29.5% in graph\nclassification, which is a significant result compared with existent works.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jiarong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yizhou Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanhao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chunping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiangang Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Variance Reduction on General Adaptive Stochastic Mirror Descent. (arXiv:2012.13760v3 [stat.ML] UPDATED)","link":"http://arxiv.org/abs/2012.13760","description":"<p>In this work, we investigate the idea of variance reduction by studying its\nproperties with general adaptive mirror descent algorithms in nonsmooth\nnonconvex finite-sum optimization problems. We propose a simple yet generalized\nframework for variance reduced adaptive mirror descent algorithms named SVRAMD\nand provide its convergence analysis in both the nonsmooth nonconvex problem\nand the P-L conditioned problem. We prove that variance reduction reduces the\nSFO complexity of adaptive mirror descent algorithms and thus accelerates their\nconvergence. In particular, our general theory implies that variance reduction\ncan be applied to algorithms using time-varying step sizes and self-adaptive\nalgorithms such as AdaGrad and RMSProp. Moreover, the convergence rates of\nSVRAMD recover the best existing rates of non-adaptive variance reduced mirror\ndescent algorithms without complicated algorithmic components. Extensive\nexperiments in deep learning validate our theoretical findings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Li_W/0/1/0/all/0/1\">Wenjie Li</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wang_Z/0/1/0/all/0/1\">Zhanyu Wang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhang_Y/0/1/0/all/0/1\">Yichen Zhang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Cheng_G/0/1/0/all/0/1\">Guang Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reducing bias and increasing utility by federated generative modeling of medical images using a centralized adversary. (arXiv:2101.07235v2 [stat.ML] UPDATED)","link":"http://arxiv.org/abs/2101.07235","description":"<p>We introduce FELICIA (FEderated LearnIng with a CentralIzed Adversary) a\ngenerative mechanism enabling collaborative learning. In particular, we show\nhow a data owner with limited and biased data could benefit from other data\nowners while keeping data from all the sources private. This is a common\nscenario in medical image analysis where privacy legislation prevents data from\nbeing shared outside local premises. FELICIA works for a large family of\nGenerative Adversarial Networks (GAN) architectures including vanilla and\nconditional GANs as demonstrated in this work. We show that by using the\nFELICIA mechanism, a data owner with limited image samples can generate\nhigh-quality synthetic images with high utility while neither data owners has\nto provide access to its data. The sharing happens solely through a central\ndiscriminator that has access limited to synthetic data. Here, utility is\ndefined as classification performance on a real test set. We demonstrate these\nbenefits on several realistic healthcare scenarions using benchmark image\ndatasets (MNIST, CIFAR-10) as well as on medical images for the task of skin\nlesion classification. With multiple experiments, we show that even in the\nworst cases, combining FELICIA with real data gracefully achieves performance\non par with real data while most results significantly improves the utility.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Rajotte_J/0/1/0/all/0/1\">Jean-Francois Rajotte</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Mukherjee_S/0/1/0/all/0/1\">Sumit Mukherjee</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Robinson_C/0/1/0/all/0/1\">Caleb Robinson</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ortiz_A/0/1/0/all/0/1\">Anthony Ortiz</a>, <a href=\"http://arxiv.org/find/stat/1/au:+West_C/0/1/0/all/0/1\">Christopher West</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ferres_J/0/1/0/all/0/1\">Juan Lavista Ferres</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ng_R/0/1/0/all/0/1\">Raymond T Ng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast End-to-End Speech Recognition via Non-Autoregressive Models and Cross-Modal Knowledge Transferring from BERT. (arXiv:2102.07594v6 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2102.07594","description":"<p>Attention-based encoder-decoder (AED) models have achieved promising\nperformance in speech recognition. However, because the decoder predicts text\ntokens (such as characters or words) in an autoregressive manner, it is\ndifficult for an AED model to predict all tokens in parallel. This makes the\ninference speed relatively slow. We believe that because the encoder already\ncaptures the whole speech utterance, which has the token-level relationship\nimplicitly, we can predict a token without explicitly autoregressive language\nmodeling. When the prediction of a token does not rely on other tokens, the\nparallel prediction of all tokens in the sequence is realizable. Based on this\nidea, we propose a non-autoregressive speech recognition model called LASO\n(Listen Attentively, and Spell Once). The model consists of an encoder, a\ndecoder, and a position dependent summarizer (PDS). The three modules are based\non basic attention blocks. The encoder extracts high-level representations from\nthe speech. The PDS uses positional encodings corresponding to tokens to\nconvert the acoustic representations into token-level representations. The\ndecoder further captures token-level relationships with the self-attention\nmechanism. At last, the probability distribution on the vocabulary is computed\nfor each token position. Therefore, speech recognition is re-formulated as a\nposition-wise classification problem. Further, we propose a cross-modal\ntransfer learning method to refine semantics from a large-scale pre-trained\nlanguage model BERT for improving the performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Ye Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1\">Jiangyan Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_J/0/1/0/all/0/1\">Jianhua Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1\">Zhengkun Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Z/0/1/0/all/0/1\">Zhengqi Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shuai Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leading or Following? Dyadic Robot Imitative Interaction Using the Active Inference Framework. (arXiv:2103.02137v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2103.02137","description":"<p>This study investigated how social interaction among robotic agents changes\ndynamically depending on the individual belief of action intention. In a set of\nsimulation studies, we examine dyadic imitative interactions of robots using a\nvariational recurrent neural network model. The model is based on the free\nenergy principle such that a pair of interacting robots find themselves in a\nloop, attempting to predict and infer each other's actions using active\ninference. We examined how regulating the complexity term to minimize free\nenergy determines the dynamic characteristics of networks and interactions.\nWhen one robot trained with tighter regulation and another trained with looser\nregulation interact, the latter tends to lead the interaction by exerting\nstronger action intention, while the former tends to follow by adapting to its\nobservations. The study confirms that the dyadic imitative interaction becomes\nsuccessful by achieving a high synchronization rate when a leader and a\nfollower are determined by developing action intentions with strong belief and\nweak belief, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wirkuttis_N/0/1/0/all/0/1\">Nadine Wirkuttis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tani_J/0/1/0/all/0/1\">Jun Tani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Digital Peter: Dataset, Competition and Handwriting Recognition Methods. (arXiv:2103.09354v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.09354","description":"<p>This paper presents a new dataset of Peter the Great's manuscripts and\ndescribes a segmentation procedure that converts initial images of documents\ninto the lines. The new dataset may be useful for researchers to train\nhandwriting text recognition models as a benchmark for comparing different\nmodels. It consists of 9 694 images and text files corresponding to lines in\nhistorical documents. The open machine learning competition Digital Peter was\nheld based on the considered dataset. The baseline solution for this\ncompetition as well as more advanced methods on handwritten text recognition\nare described in the article. Full dataset and all code are publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Potanin_M/0/1/0/all/0/1\">Mark Potanin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dimitrov_D/0/1/0/all/0/1\">Denis Dimitrov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shonenkov_A/0/1/0/all/0/1\">Alex Shonenkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bataev_V/0/1/0/all/0/1\">Vladimir Bataev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karachev_D/0/1/0/all/0/1\">Denis Karachev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Novopoltsev_M/0/1/0/all/0/1\">Maxim Novopoltsev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"In-air Knotting of Rope using Dual-Arm Robot based on Deep Learning. (arXiv:2103.09402v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2103.09402","description":"<p>In this study, we report the successful execution of in-air knotting of rope\nusing a dual-arm two-finger robot based on deep learning. Owing to its\nflexibility, the state of the rope was in constant flux during the operation of\nthe robot. This required the robot control system to dynamically correspond to\nthe state of the object at all times. However, a manual description of\nappropriate robot motions corresponding to all object states is difficult to be\nprepared in advance. To resolve this issue, we constructed a model that\ninstructed the robot to perform bowknots and overhand knots based on two deep\nneural networks trained using the data gathered from its sensorimotor,\nincluding visual and proximity sensors. The resultant model was verified to be\ncapable of predicting the appropriate robot motions based on the sensory\ninformation available online. In addition, we designed certain task motions\nbased on the Ian knot method using the dual-arm two-fingers robot. The designed\nknotting motions do not require a dedicated workbench or robot hand, thereby\nenhancing the versatility of the proposed method. Finally, experiments were\nperformed to estimate the knotting performance of the real robot while\nexecuting overhand knots and bowknots on rope and its success rate. The\nexperimental results established the effectiveness and high performance of the\nproposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Suzuki_K/0/1/0/all/0/1\">Kanata Suzuki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanamura_M/0/1/0/all/0/1\">Momomi Kanamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suga_Y/0/1/0/all/0/1\">Yuki Suga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mori_H/0/1/0/all/0/1\">Hiroki Mori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ogata_T/0/1/0/all/0/1\">Tetsuya Ogata</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge-aware Prompt-tuning with Synergistic Optimization for Relation Extraction. (arXiv:2104.07650v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07650","description":"<p>Recently, prompt-tuning has achieved promising results on some few-shot\nclassification tasks. The core idea of prompt-tuning is to insert text pieces,\ni.e., templates, into the input and transform a classification task into a\nmasked language modeling problem. However, as for relation extraction,\ndetermining the appropriate prompt template requires domain expertise. Single\nlabel word handcrafted or auto-searched is cumbersome and time-consuming to\nverify their effectiveness in non-few-shot scenarios. Further, there exist\nabundant semantic knowledge among the entities and relation labels which cannot\nbe ignored. To this end, we focus on incorporating knowledge into prompt-tuning\nfor relation extraction and propose a Knowledge-aware prompt-tuning with\nsynergistic optimization (KNIGHT) approach. Specifically, we inject entity and\nrelation knowledge into prompt construction with learnable virtual template\nwords and answer words and jointly optimize their representation with knowledge\nconstraints. Extensive experimental results on five datasets with standard and\nlow-resource settings demonstrate the effectiveness of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yunzhi Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Node Selection Toward Faster Convergence for Federated Learning on Non-IID Data. (arXiv:2105.07066v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2105.07066","description":"<p>Federated Learning (FL) is a distributed learning paradigm that enables a\nlarge number of resource-limited nodes to collaboratively train a model without\ndata sharing. The non-independent-and-identically-distributed (non-i.i.d.) data\nsamples invoke discrepancy between global and local objectives, making the FL\nmodel slow to converge. In this paper, we proposed Optimal Aggregation\nalgorithm for better aggregation, which finds out the optimal subset of local\nupdates of participating nodes in each global round, by identifying and\nexcluding the adverse local updates via checking the relationship between the\nlocal gradient and the global gradient. Then, we proposed a Probabilistic Node\nSelection framework (FedPNS) to dynamically change the probability for each\nnode to be selected based on the output of Optimal Aggregation. FedPNS can\npreferentially select nodes that propel faster model convergence. The\nunbiasedness of the proposed FedPNS design is illustrated and the convergence\nrate improvement of FedPNS over the commonly adopted Federated Averaging\n(FedAvg) algorithm is analyzed theoretically. Experimental results demonstrate\nthe effectiveness of FedPNS in accelerating the FL convergence rate, as\ncompared to FedAvg with random node selection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hongda Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Ping Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Survey and Perspective on Social Emotions in Robotics. (arXiv:2105.09647v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2105.09647","description":"<p>This study reviews research on social emotions in robotics. In robotics, the\nstudy of emotions has been pursued for a long time, including the study of\ntheir recognition, expression, and computational modeling of the basic\nmechanisms which underlie them. Research has advanced according to well-known\npsychological findings, such as category and dimension theories. Many studies\nhave been based on these basic theories, addressing only basic emotions.\nHowever, social emotions, also referred to as higher-level emotions, have been\nstudied in psychology. We believe that these higher-level emotions are worth\npursuing in robotics for next-generation, socially aware robots. In this review\npaper, we summarize the findings on social emotions in psychology and\nneuroscience, along with a survey of the studies on social emotions in robotics\nthat have been conducted to date. Thereafter, research directions toward the\nimplementation of social emotions in robots are discussed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hieida_C/0/1/0/all/0/1\">Chie Hieida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagai_T/0/1/0/all/0/1\">Takayuki Nagai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CodeNet: A Large-Scale AI for Code Dataset for Learning a Diversity of Coding Tasks. (arXiv:2105.12655v2 [cs.SE] UPDATED)","link":"http://arxiv.org/abs/2105.12655","description":"<p>Over the last several decades, software has been woven into the fabric of\nevery aspect of our society. As software development surges and code\ninfrastructure of enterprise applications ages, it is now more critical than\never to increase software development productivity and modernize legacy\napplications. Advances in deep learning and machine learning algorithms have\nenabled numerous breakthroughs, motivating researchers to leverage AI\ntechniques to improve software development efficiency. Thus, the fast-emerging\nresearch area of AI for Code has garnered new interest and gathered momentum.\nIn this paper, we present a large-scale dataset CodeNet, consisting of over 14\nmillion code samples and about 500 million lines of code in 55 different\nprogramming languages, which is aimed at teaching AI to code. In addition to\nits large scale, CodeNet has a rich set of high-quality annotations to\nbenchmark and help accelerate research in AI techniques for a variety of\ncritical coding tasks, including code similarity and classification, code\ntranslation between a large variety of programming languages, and code\nperformance (runtime and memory) improvement techniques. Additionally, CodeNet\nprovides sample input and output test sets for 98.5% of the code samples, which\ncan be used as an oracle for determining code correctness and potentially guide\nreinforcement learning for code quality improvements. As a usability feature,\nwe provide several pre-processing tools in CodeNet to transform source code\ninto representations that can be readily used as inputs into machine learning\nmodels. Results of code classification and code similarity experiments using\nthe CodeNet dataset are provided as a reference. We hope that the scale,\ndiversity and rich, high-quality annotations of CodeNet will offer\nunprecedented research opportunities at the intersection of AI and Software\nEngineering.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Puri_R/0/1/0/all/0/1\">Ruchir Puri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kung_D/0/1/0/all/0/1\">David S. Kung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Janssen_G/0/1/0/all/0/1\">Geert Janssen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Domeniconi_G/0/1/0/all/0/1\">Giacomo Domeniconi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zolotov_V/0/1/0/all/0/1\">Vladimir Zolotov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dolby_J/0/1/0/all/0/1\">Julian Dolby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choudhury_M/0/1/0/all/0/1\">Mihir Choudhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Decker_L/0/1/0/all/0/1\">Lindsey Decker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thost_V/0/1/0/all/0/1\">Veronika Thost</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buratti_L/0/1/0/all/0/1\">Luca Buratti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pujar_S/0/1/0/all/0/1\">Saurabh Pujar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramji_S/0/1/0/all/0/1\">Shyam Ramji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finkler_U/0/1/0/all/0/1\">Ulrich Finkler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malaika_S/0/1/0/all/0/1\">Susan Malaika</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reiss_F/0/1/0/all/0/1\">Frederick Reiss</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Probabilistic Forecast-Driven Strategy for a Risk-Aware Participation in the Capacity Firming Market: extended version. (arXiv:2105.13801v3 [stat.AP] UPDATED)","link":"http://arxiv.org/abs/2105.13801","description":"<p>This paper addresses the energy management of a grid-connected renewable\ngeneration plant coupled with a battery energy storage device in the capacity\nfirming market, designed to promote renewable power generation facilities in\nsmall non-interconnected grids. The core contribution is to propose a\nprobabilistic forecast-driven strategy, modeled as a min-max-min robust\noptimization problem with recourse. It is solved using a Benders-dual cutting\nplane algorithm and a column and constraints generation algorithm in a\ntractable manner. A dynamic risk-averse parameters selection strategy based on\nthe quantile forecasts distribution is proposed to improve the results. A\nsecondary contribution is to use a recently developed deep learning model known\nas normalizing flows to generate quantile forecasts of renewable generation for\nthe robust optimization problem. This technique provides a general mechanism\nfor defining expressive probability distributions, only requiring the\nspecification of a base distribution and a series of bijective transformations.\nOverall, the robust approach improves the results over a deterministic approach\nwith nominal point forecasts by finding a trade-off between conservative and\nrisk-seeking policies. The case study uses the photovoltaic generation\nmonitored on-site at the University of Li\\`ege (ULi\\`ege), Belgium.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Dumas_J/0/1/0/all/0/1\">Jonathan Dumas</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Cointe_C/0/1/0/all/0/1\">Colin Cointe</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wehenkel_A/0/1/0/all/0/1\">Antoine Wehenkel</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sutera_A/0/1/0/all/0/1\">Antonio Sutera</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Fettweis_X/0/1/0/all/0/1\">Xavier Fettweis</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Cornelusse_B/0/1/0/all/0/1\">Bertrand Corn&#xe9;lusse</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multiplierless MP-Kernel Machine For Energy-efficient Edge Devices. (arXiv:2106.01958v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.01958","description":"<p>We present a novel framework for designing multiplierless kernel machines\nthat can be used on resource-constrained platforms like intelligent edge\ndevices. The framework uses a piecewise linear (PWL) approximation based on a\nmargin propagation (MP) technique and uses only addition/subtraction, shift,\ncomparison, and register underflow/overflow operations. We propose a\nhardware-friendly MP-based inference and online training algorithm that has\nbeen optimized for a Field Programmable Gate Array (FPGA) platform. Our FPGA\nimplementation eliminates the need for DSP units and reduces the number of\nLUTs. By reusing the same hardware for inference and training, we show that the\nplatform can overcome classification errors and local minima artifacts that\nresult from the MP approximation. Using the FPGA platform, we also show that\nthe proposed multiplierless MP-kernel machine demonstrates superior performance\nin terms of power, performance, and area compared to other comparable\nimplementations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nair_A/0/1/0/all/0/1\">Abhishek Ramdas Nair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nath_P/0/1/0/all/0/1\">Pallab Kumar Nath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakrabartty_S/0/1/0/all/0/1\">Shantanu Chakrabartty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thakur_C/0/1/0/all/0/1\">Chetan Singh Thakur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Node Classification Meets Link Prediction on Knowledge Graphs. (arXiv:2106.07297v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.07297","description":"<p>Node classification and link prediction are widely studied in graph\nrepresentation learning. While both transductive node classification and link\nprediction operate over a single input graph, they have so far been studied\nseparately. Node classification models take an input graph with node features\nand incomplete node labels, and implicitly assume that the graph is\nrelationally complete, i.e., no edges are missing. By contrast, link prediction\nmodels are solely motivated by relational incompleteness of the input graphs,\nand do not typically leverage node features or classes. We propose a unifying\nperspective and study the problems of (i) transductive node classification over\nincomplete graphs and (ii) link prediction over graphs with node features,\nintroduce a new dataset for this setting, WikiAlumni, and conduct an extensive\nbenchmarking study.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abboud_R/0/1/0/all/0/1\">Ralph Abboud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ceylan_I/0/1/0/all/0/1\">&#x130;smail &#x130;lkan Ceylan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hi-Phy: A Benchmark for Hierarchical Physical Reasoning. (arXiv:2106.09692v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2106.09692","description":"<p>Reasoning about the behaviour of physical objects is a key capability of\nagents operating in physical worlds. Humans are very experienced in physical\nreasoning while it remains a major challenge for AI. To facilitate research\naddressing this problem, several benchmarks have been proposed recently.\nHowever, these benchmarks do not enable us to measure an agent's granular\nphysical reasoning capabilities when solving a complex reasoning task. In this\npaper, we propose a new benchmark for physical reasoning that allows us to test\nindividual physical reasoning capabilities. Inspired by how humans acquire\nthese capabilities, we propose a general hierarchy of physical reasoning\ncapabilities with increasing complexity. Our benchmark tests capabilities\naccording to this hierarchy through generated physical reasoning tasks in the\nvideo game Angry Birds. This benchmark enables us to conduct a comprehensive\nagent evaluation by measuring the agent's granular physical reasoning\ncapabilities. We conduct an evaluation with human players, learning agents, and\nheuristic agents and determine their capabilities. Our evaluation shows that\nlearning agents, with good local generalization ability, still struggle to\nlearn the underlying physical reasoning capabilities and perform worse than\ncurrent state-of-the-art heuristic agents and humans. We believe that this\nbenchmark will encourage researchers to develop intelligent agents with\nadvanced, human-like physical reasoning capabilities. URL:\nhttps://github.com/Cheng-Xue/Hi-Phy\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xue_C/0/1/0/all/0/1\">Cheng Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pinto_V/0/1/0/all/0/1\">Vimukthini Pinto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gamage_C/0/1/0/all/0/1\">Chathura Gamage</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Peng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Renz_J/0/1/0/all/0/1\">Jochen Renz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RadGraph: Extracting Clinical Entities and Relations from Radiology Reports. (arXiv:2106.14463v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.14463","description":"<p>Extracting structured clinical information from free-text radiology reports\ncan enable the use of radiology report information for a variety of critical\nhealthcare applications. In our work, we present RadGraph, a dataset of\nentities and relations in full-text chest X-ray radiology reports based on a\nnovel information extraction schema we designed to structure radiology reports.\nWe release a development dataset, which contains board-certified radiologist\nannotations for 500 radiology reports from the MIMIC-CXR dataset (14,579\nentities and 10,889 relations), and a test dataset, which contains two\nindependent sets of board-certified radiologist annotations for 100 radiology\nreports split equally across the MIMIC-CXR and CheXpert datasets. Using these\ndatasets, we train and test a deep learning model, RadGraph Benchmark, that\nachieves a micro F1 of 0.82 and 0.73 on relation extraction on the MIMIC-CXR\nand CheXpert test sets respectively. Additionally, we release an inference\ndataset, which contains annotations automatically generated by RadGraph\nBenchmark across 220,763 MIMIC-CXR reports (around 6 million entities and 4\nmillion relations) and 500 CheXpert reports (13,783 entities and 9,908\nrelations) with mappings to associated chest radiographs. Our freely available\ndataset can facilitate a wide range of research in medical natural language\nprocessing, as well as computer vision and multi-modal learning when linked to\nchest radiographs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Saahil Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_A/0/1/0/all/0/1\">Ashwin Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saporta_A/0/1/0/all/0/1\">Adriel Saporta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Truong_S/0/1/0/all/0/1\">Steven QH Truong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duong_D/0/1/0/all/0/1\">Du Nguyen Duong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1\">Tan Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chambon_P/0/1/0/all/0/1\">Pierre Chambon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lungren_M/0/1/0/all/0/1\">Matthew P. Lungren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_A/0/1/0/all/0/1\">Andrew Y. Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Langlotz_C/0/1/0/all/0/1\">Curtis P. Langlotz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajpurkar_P/0/1/0/all/0/1\">Pranav Rajpurkar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Crowdsourcing Evaluation of Saliency-based XAI Methods. (arXiv:2107.00456v2 [cs.HC] UPDATED)","link":"http://arxiv.org/abs/2107.00456","description":"<p>Understanding the reasons behind the predictions made by deep neural networks\nis critical for gaining human trust in many important applications, which is\nreflected in the increasing demand for explainability in AI (XAI) in recent\nyears. Saliency-based feature attribution methods, which highlight important\nparts of images that contribute to decisions by classifiers, are often used as\nXAI methods, especially in the field of computer vision. In order to compare\nvarious saliency-based XAI methods quantitatively, several approaches for\nautomated evaluation schemes have been proposed; however, there is no guarantee\nthat such automated evaluation metrics correctly evaluate explainability, and a\nhigh rating by an automated evaluation scheme does not necessarily mean a high\nexplainability for humans. In this study, instead of the automated evaluation,\nwe propose a new human-based evaluation scheme using crowdsourcing to evaluate\nXAI methods. Our method is inspired by a human computation game, \"Peek-a-boom\",\nand can efficiently compare different XAI methods by exploiting the power of\ncrowds. We evaluate the saliency maps of various XAI methods on two datasets\nwith automated and crowd-based evaluation schemes. Our experiments show that\nthe result of our crowd-based evaluation scheme is different from those of\nautomated evaluation schemes. In addition, we regard the crowd-based evaluation\nresults as ground truths and provide a quantitative performance measure to\ncompare different automated evaluation schemes. We also discuss the impact of\ncrowd workers on the results and show that the varying ability of crowd workers\ndoes not significantly impact the results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiaotian Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tolmachev_A/0/1/0/all/0/1\">Arseny Tolmachev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamamoto_T/0/1/0/all/0/1\">Tatsuya Yamamoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takeuchi_K/0/1/0/all/0/1\">Koh Takeuchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okajima_S/0/1/0/all/0/1\">Seiji Okajima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takebayashi_T/0/1/0/all/0/1\">Tomoyoshi Takebayashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maruhashi_K/0/1/0/all/0/1\">Koji Maruhashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kashima_H/0/1/0/all/0/1\">Hisashi Kashima</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Approximate Search for Sets of Vectors. (arXiv:2107.06817v2 [cs.DS] UPDATED)","link":"http://arxiv.org/abs/2107.06817","description":"<p>We consider a similarity measure between two sets $A$ and $B$ of vectors,\nthat balances the average and maximum cosine distance between pairs of vectors,\none from set $A$ and one from set $B$. As a motivation for this measure, we\npresent lineage tracking in a database. To practically realize this measure, we\nneed an approximate search algorithm that given a set of vectors $A$ and sets\nof vectors $B_1,...,B_n$, the algorithm quickly locates the set $B_i$ that\nmaximizes the similarity measure. For the case where all sets are singleton\nsets, essentially each is a single vector, there are known efficient\napproximate search algorithms, e.g., approximated versions of tree search\nalgorithms, locality-sensitive hashing (LSH), vector quantization (VQ) and\nproximity graph algorithms. In this work, we present approximate search\nalgorithms for the general case. The underlying idea in these algorithms is\nencoding a set of vectors via a \"long\" single vector. The proposed approximate\napproach achieves significant performance gains over an optimized, exact search\non vector sets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Leybovich_M/0/1/0/all/0/1\">Michael Leybovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shmueli_O/0/1/0/all/0/1\">Oded Shmueli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bi-Bimodal Modality Fusion for Correlation-Controlled Multimodal Sentiment Analysis. (arXiv:2107.13669v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2107.13669","description":"<p>Multimodal sentiment analysis aims to extract and integrate semantic\ninformation collected from multiple modalities to recognize the expressed\nemotions and sentiment in multimodal data. This research area's major concern\nlies in developing an extraordinary fusion scheme that can extract and\nintegrate key information from various modalities. However, one issue that may\nrestrict previous work to achieve a higher level is the lack of proper modeling\nfor the dynamics of the competition between the independence and relevance\namong modalities, which could deteriorate fusion outcomes by causing the\ncollapse of modality-specific feature space or introducing extra noise. To\nmitigate this, we propose the Bi-Bimodal Fusion Network (BBFN), a novel\nend-to-end network that performs fusion (relevance increment) and separation\n(difference increment) on pairwise modality representations. The two parts are\ntrained simultaneously such that the combat between them is simulated. The\nmodel takes two bimodal pairs as input due to the known information imbalance\namong modalities. In addition, we leverage a gated control mechanism in the\nTransformer architecture to further improve the final output. Experimental\nresults on three datasets (CMU-MOSI, CMU-MOSEI, and UR-FUNNY) verifies that our\nmodel significantly outperforms the SOTA. The implementation of this work is\navailable at https://github.com/declare-lab/multimodal-deep-learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1\">Wei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gelbukh_A/0/1/0/all/0/1\">Alexander Gelbukh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zadeh_A/0/1/0/all/0/1\">Amir Zadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1\">Louis-philippe Morency</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poria_S/0/1/0/all/0/1\">Soujanya Poria</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ManiSkill: Generalizable Manipulation Skill Benchmark with Large-Scale Demonstrations. (arXiv:2107.14483v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2107.14483","description":"<p>Object manipulation from 3D visual inputs poses many challenges on building\ngeneralizable perception and policy models. However, 3D assets in existing\nbenchmarks mostly lack the diversity of 3D shapes that align with real-world\nintra-class complexity in topology and geometry. Here we propose SAPIEN\nManipulation Skill Benchmark (ManiSkill) to benchmark manipulation skills over\ndiverse objects in a full-physics simulator. 3D assets in ManiSkill include\nlarge intra-class topological and geometric variations. Tasks are carefully\nchosen to cover distinct types of manipulation challenges. Latest progress in\n3D vision also makes us believe that we should customize the benchmark so that\nthe challenge is inviting to researchers working on 3D deep learning. To this\nend, we simulate a moving panoramic camera that returns ego-centric point\nclouds or RGB-D images. In addition, we would like ManiSkill to serve a broad\nset of researchers interested in manipulation research. Besides supporting the\nlearning of policies from interactions, we also support\nlearning-from-demonstrations (LfD) methods, by providing a large number of\nhigh-quality demonstrations (~36,000 successful trajectories, ~1.5M point\ncloud/RGB-D frames in total). We provide baselines using 3D deep learning and\nLfD algorithms. All code of our benchmark (simulator, environment, SDK, and\nbaselines) is open-sourced, and a challenge facing interdisciplinary\nresearchers will be held based on the benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mu_T/0/1/0/all/0/1\">Tongzhou Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_Z/0/1/0/all/0/1\">Zhan Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_F/0/1/0/all/0/1\">Fanbo Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Derek Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuanlin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_S/0/1/0/all/0/1\">Stone Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhiao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_Z/0/1/0/all/0/1\">Zhiwei Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hao Su</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data Driven VRP: A Neural Network Model to Learn Hidden Preferences for VRP. (arXiv:2108.04578v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2108.04578","description":"<p>The traditional Capacitated Vehicle Routing Problem (CVRP) minimizes the\ntotal distance of the routes under the capacity constraints of the vehicles.\nBut more often, the objective involves multiple criteria including not only the\ntotal distance of the tour but also other factors such as travel costs, travel\ntime, and fuel consumption.Moreover, in reality, there are numerous implicit\npreferences ingrained in the minds of the route planners and the drivers.\nDrivers, for instance, have familiarity with certain neighborhoods and\nknowledge of the state of roads, and often consider the best places for rest\nand lunch breaks. This knowledge is difficult to formulate and balance when\noperational routing decisions have to be made. This motivates us to learn the\nimplicit preferences from past solutions and to incorporate these learned\npreferences in the optimization process. These preferences are in the form of\narc probabilities, i.e., the more preferred a route is, the higher is the joint\nprobability. The novelty of this work is the use of a neural network model to\nestimate the arc probabilities, which allows for additional features and\nautomatic parameter estimation. This first requires identifying suitable\nfeatures, neural architectures and loss functions, taking into account that\nthere is typically few data available. We investigate the difference with a\nprior weighted Markov counting approach, and study the applicability of neural\nnetworks in this setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mandi_J/0/1/0/all/0/1\">Jayanta Mandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Canoy_R/0/1/0/all/0/1\">Rocsildes Canoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bucarey_V/0/1/0/all/0/1\">V&#xed;ctor Bucarey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guns_T/0/1/0/all/0/1\">Tias Guns</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep adversarial attack on target detection systems. (arXiv:2108.05948v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2108.05948","description":"<p>Target detection systems identify targets by localizing their coordinates on\nthe input image of interest. This is ideally achieved by labeling each pixel in\nan image as a background or a potential target pixel. Deep Convolutional Neural\nNetwork (DCNN) classifiers have proven to be successful tools for computer\nvision applications. However,prior research confirms that even state of the art\nclassifier models are susceptible to adversarial attacks. In this paper, we\nshow how to generate adversarial infrared images by adding small perturbations\nto the targets region to deceive a DCNN-based target detector at remarkable\nlevels. We demonstrate significant progress in developing visually\nimperceptible adversarial infrared images where the targets are visually\nrecognizable by an expert but a DCNN-based target detector cannot detect the\ntargets in the image.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Osahor_U/0/1/0/all/0/1\">Uche M. Osahor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasrabadi_N/0/1/0/all/0/1\">Nasser M. Nasrabadi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reinforcement Learning for Robot Navigation with Adaptive ExecutionDuration (AED) in a Semi-Markov Model. (arXiv:2108.06161v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2108.06161","description":"<p>Deep reinforcement learning (DRL) algorithms have proven effective in robot\nnavigation, especially in unknown environments, through directly mapping\nperception inputs into robot control commands. Most existing methods adopt\nuniform execution duration with robots taking commands at fixed intervals. As\nsuch, the length of execution duration becomes a crucial parameter to the\nnavigation algorithm. In particular, if the duration is too short, then the\nnavigation policy would be executed at a high frequency, with increased\ntraining difficulty and high computational cost. Meanwhile, if the duration is\ntoo long, then the policy becomes unable to handle complex situations, like\nthose with crowded obstacles. It is thus tricky to find the \"sweet\" duration\nrange; some duration values may render a DRL model to fail to find a navigation\npath. In this paper, we propose to employ adaptive execution duration to\novercome this problem. Specifically, we formulate the navigation task as a\nSemi-Markov Decision Process (SMDP) problem to handle adaptive execution\nduration. We also improve the distributed proximal policy optimization (DPPO)\nalgorithm and provide its theoretical guarantee for the specified SMDP problem.\nWe evaluate our approach both in the simulator and on an actual robot. The\nresults show that our approach outperforms the other DRL-based method (with\nfixed execution duration) by 10.3% in terms of the navigation success rate.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yu&#x27;an Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_R/0/1/0/all/0/1\">Ruosong Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_Z/0/1/0/all/0/1\">Ziyang Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hongjian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guangda Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1\">Jie Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jun Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yanyong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_J/0/1/0/all/0/1\">Jianmin Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TFRD: A Benchmark Dataset for Research on Temperature Field Reconstruction of Heat-Source Systems. (arXiv:2108.08298v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2108.08298","description":"<p>Temperature field reconstruction of heat source systems (TFR-HSS) with\nlimited monitoring sensors occurred in thermal management plays an important\nrole in real time health detection system of electronic equipment in\nengineering. However, prior methods with common interpolations usually cannot\nprovide accurate reconstruction performance as needed. In addition, there\nexists no public dataset for widely research of reconstruction methods to\nfurther boost the reconstruction performance and engineering applications. To\novercome this problem, this work constructs a novel dataset, namely Temperature\nField Reconstruction Dataset (TFRD), for TFR-HSS task with commonly used\nmethods, including the interpolation methods and the machine learning based\nmethods, as baselines to advance the research over temperature field\nreconstruction. First, the TFR-HSS task is mathematically modelled from\nreal-world engineering problem and four types of numerically modellings have\nbeen constructed to transform the problem into discrete mapping forms. Besides,\nthis work selects three typical reconstruction problem over heat-source systems\nwith different heat-source information and boundary conditions, and generate\nthe training and testing samples for further research. Finally, a comprehensive\nreview of the prior methods for TFR-HSS task as well as recent widely used deep\nlearning methods is given and a performance analysis of typical methods is\nprovided on TFRD, which can be served as the baseline results on this\nbenchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaoqian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1\">Zhiqiang Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xiaoyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Weien Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1\">Wen Yao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Feature-weighted Stacking for Nonseasonal Time Series Forecasts: A Case Study of the COVID-19 Epidemic Curves. (arXiv:2108.08723v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2108.08723","description":"<p>We investigate ensembling techniques in forecasting and examine their\npotential for use in nonseasonal time-series similar to those in the early days\nof the COVID-19 pandemic. Developing improved forecast methods is essential as\nthey provide data-driven decisions to organisations and decision-makers during\ncritical phases. We propose using late data fusion, using a stacked ensemble of\ntwo forecasting models and two meta-features that prove their predictive power\nduring a preliminary forecasting stage. The final ensembles include a Prophet\nand long short term memory (LSTM) neural network as base models. The base\nmodels are combined by a multilayer perceptron (MLP), taking into account\nmeta-features that indicate the highest correlation with each base model's\nforecast accuracy. We further show that the inclusion of meta-features\ngenerally improves the ensemble's forecast accuracy across two forecast\nhorizons of seven and fourteen days. This research reinforces previous work and\ndemonstrates the value of combining traditional statistical models with deep\nlearning models to produce more accurate forecast models for time-series from\ndifferent domains and seasonality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cawood_P/0/1/0/all/0/1\">Pieter Cawood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zyl_T/0/1/0/all/0/1\">Terence L. van Zyl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Safe Transformative AI via a Windfall Clause. (arXiv:2108.09404v2 [cs.CY] UPDATED)","link":"http://arxiv.org/abs/2108.09404","description":"<p>Society could soon see transformative artificial intelligence (TAI). Models\nof competition for TAI show firms face strong competitive pressure to deploy\nTAI systems before they are safe. This paper explores a proposed solution to\nthis problem, a Windfall Clause, where developers commit to donating a\nsignificant portion of any eventual extremely large profits to good causes.\nHowever, a key challenge for a Windfall Clause is that firms must have reason\nto join one. Firms must also believe these commitments are credible. We extend\na model of TAI competition with a Windfall Clause to show how firms and\npolicymakers can design a Windfall Clause which overcomes these challenges.\nEncouragingly, firms benefit from joining a Windfall Clause under a wide range\nof scenarios. We also find that firms join the Windfall Clause more often when\nthe competition is more dangerous. Even when firms learn each other's\ncapabilities, firms rarely wish to withdraw their support for the Windfall\nClause. These three findings strengthen the case for using a Windfall Clause to\npromote the safe development of TAI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bova_P/0/1/0/all/0/1\">Paolo Bova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_J/0/1/0/all/0/1\">Jonas Emanuel M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harack_B/0/1/0/all/0/1\">Benjamin Harack</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MS-DARTS: Mean-Shift Based Differentiable Architecture Search. (arXiv:2108.09996v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2108.09996","description":"<p>Differentiable Architecture Search (DARTS) is an effective continuous\nrelaxation-based network architecture search (NAS) method with low search cost.\nIt has attracted significant attentions in Auto-ML research and becomes one of\nthe most useful paradigms in NAS. Although DARTS can produce superior\nefficiency over traditional NAS approaches with better control of complex\nparameters, oftentimes it suffers from stabilization issues in producing\ndeteriorating architectures when discretizing the continuous architecture. We\nobserved considerable loss of validity causing dramatic decline in performance\nat this final discretization step of DARTS. To address this issue, we propose a\nMean-Shift based DARTS (MS-DARTS) to improve stability based on sampling and\nperturbation. Our approach can improve bot the stability and accuracy of DARTS,\nby smoothing the loss landscape and sampling architecture parameters within a\nsuitable bandwidth. We investigate the convergence of our mean-shift approach,\ntogether with the effects of bandwidth selection that affects stability and\naccuracy. Evaluations performed on CIFAR-10, CIFAR-100, and ImageNet show that\nMS-DARTS archives higher performance over other state-of-the-art NAS methods\nwith reduced search cost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_J/0/1/0/all/0/1\">Jun-Wei Hsieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1\">Ming-Ching Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Ping-Yang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santra_S/0/1/0/all/0/1\">Santanu Santra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chou_C/0/1/0/all/0/1\">Cheng-Han Chou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chih-Sheng Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Social Norm Bias: Residual Harms of Fairness-Aware Algorithms. (arXiv:2108.11056v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2108.11056","description":"<p>Many modern learning algorithms mitigate bias by enforcing fairness across\ncoarsely-defined groups related to a sensitive attribute like gender or race.\nHowever, the same algorithms seldom account for the within-group biases that\narise due to the heterogeneity of group members. In this work, we characterize\nSocial Norm Bias (SNoB), a subtle but consequential type of discrimination that\nmay be exhibited by automated decision-making systems, even when these systems\nachieve group fairness objectives. We study this issue through the lens of\ngender bias in occupation classification from biographies. We quantify SNoB by\nmeasuring how an algorithm's predictions are associated with conformity to\ngender norms, which is measured using a machine learning approach. This\nframework reveals that for classification tasks related to male-dominated\noccupations, fairness-aware classifiers favor biographies written in ways that\nalign with masculine gender norms. We compare SNoB across fairness intervention\ntechniques and show that post-processing interventions do not mitigate this\ntype of bias at all.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1\">Myra Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+De_Arteaga_M/0/1/0/all/0/1\">Maria De-Arteaga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mackey_L/0/1/0/all/0/1\">Lester Mackey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalai_A/0/1/0/all/0/1\">Adam Tauman Kalai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MCML: A Novel Memory-based Contrastive Meta-Learning Method for Few Shot Slot Tagging. (arXiv:2108.11635v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2108.11635","description":"<p>Meta-learning is widely used for few-shot slot tagging in the task of\nfew-shot learning. The performance of existing methods is, however, seriously\naffected by catastrophic forgetting. This phenomenon is common in deep learning\nas the training and testing modules fail to take into account historical\ninformation, i.e. previously trained episodes in the metric-based\nmeta-learning. To overcome this predicament, we propose the Memory-based\nContrastive Meta-learning (MCML) method. Specifically, we propose a\nlearn-from-memory mechanism that use explicit memory to keep track of the label\nrepresentations of previously trained episodes and propose a contrastive\nlearning method to compare the current label embedded in the few shot episode\nwith the historic ones stored in the memory, and an adaption-from memory\nmechanism to determine the output label based on the contrast between the input\nlabels embedded in the test episode and the label clusters in the memory.\nExperimental results show that MCML is scalable and outperforms metric-based\nmeta-learning and optimization-based meta-learning on all 1shot, 5-shot,\n10-shot, and 20-shot scenarios of the SNIPS dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongru Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zezhong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_G/0/1/0/all/0/1\">Gabriel Pui Cheong Fung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_K/0/1/0/all/0/1\">Kam-Fai Wong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cleaning Inconsistent Data in Temporal DL-Lite Under Best Repair Semantics. (arXiv:2108.12149v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2108.12149","description":"<p>In this paper, we address the problem of handling inconsistent data in\nTemporal Description Logic (TDL) knowledge bases. Considering the data part of\nthe Knowledge Base as the source of inconsistency over time, we propose an ABox\nrepair approach. This is the first work handling the repair in TDL Knowledge\nbases. To do so, our goal is twofold: 1) detect temporal inconsistencies and 2)\npropose a data temporal reparation. For the inconsistency detection, we propose\na reduction approach from TDL to DL which allows to provide a tight NP-complete\nupper bound for TDL concept satisfiability and to use highly optimised DL\nreasoners that can bring precise explanation (the set of inconsistent data\nassertions). Thereafter, from the obtained explanation, we propose a method for\nautomatically computing the best repair in the temporal setting based on the\nallowed rigid predicates and the time order of assertions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ouziri_M/0/1/0/all/0/1\">Mourad Ouziri</a> (LIPADE - EA 2517), <a href=\"http://arxiv.org/find/cs/1/au:+Tahrat_S/0/1/0/all/0/1\">Sabiha Tahrat</a> (LIPADE - EA 2517), <a href=\"http://arxiv.org/find/cs/1/au:+Benbernou_S/0/1/0/all/0/1\">Salima Benbernou</a> (LIPADE - EA 2517), <a href=\"http://arxiv.org/find/cs/1/au:+Ouzirri_M/0/1/0/all/0/1\">Mourad Ouzirri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-08-30T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Artificial Intelligence"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}},{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"From Pivots to Graphs: Augmented CycleDensity as a Generalization to One Time InverseConsultation. (arXiv:2108.12459v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12459","description":"<p>This paper describes an approach used to generate new translations using raw\nbilingual dictionaries as part of the 4th Task Inference Across Dictionaries\n(TIAD 2021) shared task. We propose Augmented Cycle Density (ACD) as a\nframework that combines insights from two state of the art methods that require\nno sense information and parallel corpora: Cycle Density (CD) and One Time\nInverse Consultation (OTIC). The task results show that across 3 unseen\nlanguage pairs, ACD's predictions, has more than double (74%) the coverage of\nOTIC at almost the same precision (76%). ACD combines CD's scalability -\nleveraging rich multilingual graphs for better predictions, and OTIC's data\nefficiency - producing good results with the minimum possible resource of one\npivot language.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Goel_S/0/1/0/all/0/1\">Shashwat Goel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grover_K/0/1/0/all/0/1\">Kunwar Shaanjeet Singh Grover</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Text Evaluation through the Lens of Wasserstein Barycenters. (arXiv:2108.12463v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12463","description":"<p>A new metric \\texttt{BaryScore} to evaluate text generation based on deep\ncontextualized embeddings (\\textit{e.g.}, BERT, Roberta, ELMo) is introduced.\nThis metric is motivated by a new framework relying on optimal transport tools,\n\\textit{i.e.}, Wasserstein distance and barycenter. By modelling the layer\noutput of deep contextualized embeddings as a probability distribution rather\nthan by a vector embedding; this framework provides a natural way to aggregate\nthe different outputs through the Wasserstein space topology. In addition, it\nprovides theoretical grounds to our metric and offers an alternative to\navailable solutions (\\textit{e.g.}, MoverScore and BertScore). Numerical\nevaluation is performed on four different tasks: machine translation,\nsummarization, data2text generation and image captioning. Our results show that\n\\texttt{BaryScore} outperforms other BERT based metrics and exhibits more\nconsistent behaviour in particular for text summarization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Colombo_P/0/1/0/all/0/1\">Pierre Colombo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Staerman_G/0/1/0/all/0/1\">Guillaume Staerman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clavel_C/0/1/0/all/0/1\">Chloe Clavel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piantanida_P/0/1/0/all/0/1\">Pablo Piantanida</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Code-switched inspired losses for generic spoken dialog representations. (arXiv:2108.12465v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12465","description":"<p>Spoken dialog systems need to be able to handle both multiple languages and\nmultilinguality inside a conversation (\\textit{e.g} in case of code-switching).\nIn this work, we introduce new pretraining losses tailored to learn\nmultilingual spoken dialog representations. The goal of these losses is to\nexpose the model to code-switched language. To scale up training, we\nautomatically build a pretraining corpus composed of multilingual conversations\nin five different languages (French, Italian, English, German and Spanish) from\n\\texttt{OpenSubtitles}, a huge multilingual corpus composed of 24.3G tokens. We\ntest the generic representations on \\texttt{MIAM}, a new benchmark composed of\nfive dialog act corpora on the same aforementioned languages as well as on two\nnovel multilingual downstream tasks (\\textit{i.e} multilingual mask utterance\nretrieval and multilingual inconsistency identification). Our experiments show\nthat our new code switched-inspired losses achieve a better performance in both\nmonolingual and multilingual settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chapuis_E/0/1/0/all/0/1\">Emile Chapuis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colombo_P/0/1/0/all/0/1\">Pierre Colombo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labeau_M/0/1/0/all/0/1\">Matthieu Labeau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clave_C/0/1/0/all/0/1\">Chloe Clave</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ReGen: Reinforcement Learning for Text and Knowledge Base Generation using Pretrained Language Models. (arXiv:2108.12472v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12472","description":"<p>Automatic construction of relevant Knowledge Bases (KBs) from text, and\ngeneration of semantically meaningful text from KBs are both long-standing\ngoals in Machine Learning. In this paper, we present ReGen, a bidirectional\ngeneration of text and graph leveraging Reinforcement Learning (RL) to improve\nperformance. Graph linearization enables us to re-frame both tasks as a\nsequence to sequence generation problem regardless of the generative direction,\nwhich in turn allows the use of Reinforcement Learning for sequence training\nwhere the model itself is employed as its own critic leading to Self-Critical\nSequence Training (SCST). We present an extensive investigation demonstrating\nthat the use of RL via SCST benefits graph and text generation on WebNLG+ 2020\nand TekGen datasets. Our system provides state-of-the-art results on WebNLG+\n2020 by significantly improving upon published results from the WebNLG 2020+\nChallenge for both text-to-graph and graph-to-text generation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dognin_P/0/1/0/all/0/1\">Pierre L. Dognin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padhi_I/0/1/0/all/0/1\">Inkit Padhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melnyk_I/0/1/0/all/0/1\">Igor Melnyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_P/0/1/0/all/0/1\">Payel Das</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Opinions are Made to be Changed: Temporally Adaptive Stance Classification. (arXiv:2108.12476v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12476","description":"<p>Given the rapidly evolving nature of social media and people's views, word\nusage changes over time. Consequently, the performance of a classifier trained\non old textual data can drop dramatically when tested on newer data. While\nresearch in stance classification has advanced in recent years, no effort has\nbeen invested in making these classifiers have persistent performance over\ntime. To study this phenomenon we introduce two novel large-scale, longitudinal\nstance datasets. We then evaluate the performance persistence of stance\nclassifiers over time and demonstrate how it decays as the temporal gap between\ntraining and testing data increases. We propose a novel approach to mitigate\nthis performance drop, which is based on temporal adaptation of the word\nembeddings used for training the stance classifier. This enables us to make use\nof readily available unlabelled data from the current time period instead of\nexpensive annotation efforts. We propose and compare several approaches to\nembedding adaptation and find that the Incremental Temporal Alignment (ITA)\nmodel leads to the best results in reducing performance drop over time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alkhalifa_R/0/1/0/all/0/1\">Rabab Alkhalifa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kochkina_E/0/1/0/all/0/1\">Elena Kochkina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zubiaga_A/0/1/0/all/0/1\">Arkaitz Zubiaga</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-Shot Table-to-Text Generation with Prototype Memory. (arXiv:2108.12516v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12516","description":"<p>Neural table-to-text generation models have achieved remarkable progress on\nan array of tasks. However, due to the data-hungry nature of neural models,\ntheir performances strongly rely on large-scale training examples, limiting\ntheir applicability in real-world applications. To address this, we propose a\nnew framework: Prototype-to-Generate (P2G), for table-to-text generation under\nthe few-shot scenario. The proposed framework utilizes the retrieved\nprototypes, which are jointly selected by an IR system and a novel prototype\nselector to help the model bridging the structural gap between tables and\ntexts. Experimental results on three benchmark datasets with three\nstate-of-the-art models demonstrate that the proposed framework significantly\nimproves the model performance across various evaluation metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yixuan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1\">Zaiqiao Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baker_S/0/1/0/all/0/1\">Simon Baker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collier_N/0/1/0/all/0/1\">Nigel Collier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predicting the Factuality of Reporting of News Media Using Observations About User Attention in Their YouTube Channels. (arXiv:2108.12519v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12519","description":"<p>We propose a novel framework for predicting the factuality of reporting of\nnews media outlets by studying the user attention cycles in their YouTube\nchannels. In particular, we design a rich set of features derived from the\ntemporal evolution of the number of views, likes, dislikes, and comments for a\nvideo, which we then aggregate to the channel level. We develop and release a\ndataset for the task, containing observations of user attention on YouTube\nchannels for 489 news media. Our experiments demonstrate both complementarity\nand sizable improvements over state-of-the-art textual representations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bozhanova_K/0/1/0/all/0/1\">Krasimira Bozhanova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dinkov_Y/0/1/0/all/0/1\">Yoan Dinkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koychev_I/0/1/0/all/0/1\">Ivan Koychev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castaldo_M/0/1/0/all/0/1\">Maria Castaldo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venturini_T/0/1/0/all/0/1\">Tommaso Venturini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TweetBLM: A Hate Speech Dataset and Analysis of Black Lives Matter-related Microblogs on Twitter. (arXiv:2108.12521v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12521","description":"<p>In the past few years, there has been a significant rise in toxic and hateful\ncontent on various social media platforms. Recently Black Lives Matter movement\ncame into the picture, causing an avalanche of user generated responses on the\ninternet. In this paper, we have proposed a Black Lives Matter related tweet\nhate speech dataset TweetBLM. Our dataset comprises 9165 manually annotated\ntweets that target the Black Lives Matter movement. We annotated the tweets\ninto two classes, i.e., HATE and NONHATE based on their content related to\nracism erupted from the movement for the black community. In this work, we also\ngenerated useful statistical insights on our dataset and performed a systematic\nanalysis of various machine learning models such as Random Forest, CNN, LSTM,\nBiLSTM, Fasttext, BERTbase, and BERTlarge for the classification task on our\ndataset. Through our work, we aim at contributing to the substantial efforts of\nthe research community for the identification and mitigation of hate speech on\nthe internet. The dataset is publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Sumit Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pranesh_R/0/1/0/all/0/1\">Raj Ratn Pranesh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Energy-Based Approximate Inference Networks for Structured Applications in NLP. (arXiv:2108.12522v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12522","description":"<p>Structured prediction in natural language processing (NLP) has a long\nhistory. The complex models of structured application come at the difficulty of\nlearning and inference. These difficulties lead researchers to focus more on\nmodels with simple structure components (e.g., local classifier). Deep\nrepresentation learning has become increasingly popular in recent years. The\nstructure components of their method, on the other hand, are usually relatively\nsimple. We concentrate on complex structured models in this dissertation. We\nprovide a learning framework for complicated structured models as well as an\ninference method with a better speed/accuracy/search error trade-off. The\ndissertation begins with a general introduction to energy-based models. In NLP\nand other applications, an energy function is comparable to the concept of a\nscoring function. In this dissertation, we discuss the concept of the energy\nfunction and structured models with different energy functions. Then, we\npropose a method in which we train a neural network to do argmax inference\nunder a structured energy function, referring to the trained networks as\n\"inference networks\" or \"energy-based inference networks\". We then develop ways\nof jointly learning energy functions and inference networks using an\nadversarial learning framework. Despite the inference and learning difficulties\nof energy-based models, we present approaches in this thesis that enable\nenergy-based models more easily to be applied in structured NLP applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tu_L/0/1/0/all/0/1\">Lifu Tu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Speech Representations and Phoneme Classification for Preserving the Endangered Language of Ladin. (arXiv:2108.12531v1 [eess.AS])","link":"http://arxiv.org/abs/2108.12531","description":"<p>A vast majority of the world's 7,000 spoken languages are predicted to become\nextinct within this century, including the endangered language of Ladin from\nthe Italian Alps. Linguists who work to preserve a language's phonetic and\nphonological structure can spend hours transcribing each minute of speech from\nnative speakers. To address this problem in the context of Ladin, our paper\npresents the first analysis of speech representations and machine learning\nmodels for classifying 32 phonemes of Ladin. We experimented with a novel\ndataset of the Fascian dialect of Ladin, collected from native speakers in\nItaly. We created frame-level and segment-level speech feature extraction\napproaches and conducted extensive experiments with 8 different classifiers\ntrained on 9 different speech representations. Our speech representations\nranged from traditional features (MFCC, LPC) to features learned with deep\nneural network models (autoencoders, LSTM autoencoders, and WaveNet). Our\nhighest-performing classifier, trained on MFCC representations of speech\nsignals, achieved an 86% average accuracy across all Ladin phonemes. We also\nobtained average accuracies above 77% for all Ladin phoneme subgroups examined.\nOur findings contribute insights for learning discriminative Ladin phoneme\nrepresentations and demonstrate the potential for leveraging machine learning\nand speech signal processing to preserve Ladin and other endangered languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Durante_Z/0/1/0/all/0/1\">Zane Durante</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mathur_L/0/1/0/all/0/1\">Leena Mathur</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ye_E/0/1/0/all/0/1\">Eric Ye</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_S/0/1/0/all/0/1\">Sichong Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ramdas_T/0/1/0/all/0/1\">Tejas Ramdas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Iskarous_K/0/1/0/all/0/1\">Khalil Iskarous</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"QACE: Asking Questions to Evaluate an Image Caption. (arXiv:2108.12560v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12560","description":"<p>In this paper, we propose QACE, a new metric based on Question Answering for\nCaption Evaluation. QACE generates questions on the evaluated caption and\nchecks its content by asking the questions on either the reference caption or\nthe source image. We first develop QACE-Ref that compares the answers of the\nevaluated caption to its reference, and report competitive results with the\nstate-of-the-art metrics. To go further, we propose QACE-Img, which asks the\nquestions directly on the image, instead of reference. A Visual-QA system is\nnecessary for QACE-Img. Unfortunately, the standard VQA models are framed as a\nclassification among only a few thousand categories. Instead, we propose\nVisual-T5, an abstractive VQA system. The resulting metric, QACE-Img is\nmulti-modal, reference-less, and explainable. Our experiments show that\nQACE-Img compares favorably w.r.t. other reference-less metrics. We will\nrelease the pre-trained models to compute QACE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hwanhee Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scialom_T/0/1/0/all/0/1\">Thomas Scialom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Seunghyun Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dernoncourt_F/0/1/0/all/0/1\">Franck Dernoncourt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_K/0/1/0/all/0/1\">Kyomin Jung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Goal-driven text descriptions for images. (arXiv:2108.12575v1 [cs.CV])","link":"http://arxiv.org/abs/2108.12575","description":"<p>A big part of achieving Artificial General Intelligence(AGI) is to build a\nmachine that can see and listen like humans. Much work has focused on designing\nmodels for image classification, video classification, object detection, pose\nestimation, speech recognition, etc., and has achieved significant progress in\nrecent years thanks to deep learning. However, understanding the world is not\nenough. An AI agent also needs to know how to talk, especially how to\ncommunicate with a human. While perception (vision, for example) is more common\nacross animal species, the use of complicated language is unique to humans and\nis one of the most important aspects of intelligence.\n</p>\n<p>In this thesis, we focus on generating textual output given visual input. In\nChapter 3, we focus on generating the referring expression, a text description\nfor an object in the image so that a receiver can infer which object is being\ndescribed. We use a comprehension machine to directly guide the generated\nreferring expressions to be more discriminative. In Chapter 4, we introduce a\nmethod that encourages discriminability in image caption generation. We show\nthat more discriminative captioning models generate more descriptive captions.\nIn Chapter 5, we study how training objectives and sampling methods affect the\nmodels' ability to generate diverse captions. We find that a popular captioning\ntraining strategy will be detrimental to the diversity of generated captions.\nIn Chapter 6, we propose a model that can control the length of generated\ncaptions. By changing the desired length, one can influence the style and\ndescriptiveness of the captions. Finally, in Chapter 7, we rank/generate\ninformative image tags according to their information utility. The proposed\nmethod better matches what humans think are the most important tags for the\nimages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_R/0/1/0/all/0/1\">Ruotian Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distilling the Knowledge of Large-scale Generative Models into Retrieval Models for Efficient Open-domain Conversation. (arXiv:2108.12582v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12582","description":"<p>Despite the remarkable performance of large-scale generative models in\nopen-domain conversation, they are known to be less practical for building\nreal-time conversation systems due to high latency. On the other hand,\nretrieval models could return responses with much lower latency but show\ninferior performance to the large-scale generative models since the\nconversation quality is bounded by the pre-defined response set. To take\nadvantage of both approaches, we propose a new training method called G2R\n(Generative-to-Retrieval distillation) that preserves the efficiency of a\nretrieval model while leveraging the conversational ability of a large-scale\ngenerative model by infusing the knowledge of the generative model into the\nretrieval model. G2R consists of two distinct techniques of distillation: the\ndata-level G2R augments the dialogue dataset with additional responses\ngenerated by the large-scale generative model, and the model-level G2R\ntransfers the response quality score assessed by the generative model to the\nscore of the retrieval model by the knowledge distillation loss. Through\nextensive experiments including human evaluation, we demonstrate that our\nretrieval-based conversation system trained with G2R shows a substantially\nimproved performance compared to the baseline retrieval model while showing\nsignificantly lower inference latency than the large-scale generative models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1\">Beomsu Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_S/0/1/0/all/0/1\">Seokjun Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Seungju Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erdenee_E/0/1/0/all/0/1\">Enkhbayar Erdenee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_B/0/1/0/all/0/1\">Buru Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-training Improves Pre-training for Few-shot Learning in Task-oriented Dialog Systems. (arXiv:2108.12589v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12589","description":"<p>As the labeling cost for different modules in task-oriented dialog (ToD)\nsystems is expensive, a major challenge is to train different modules with the\nleast amount of labeled data. Recently, large-scale pre-trained language\nmodels, have shown promising results for few-shot learning in ToD. In this\npaper, we devise a self-training approach to utilize the abundant unlabeled\ndialog data to further improve state-of-the-art pre-trained models in few-shot\nlearning scenarios for ToD systems. Specifically, we propose a self-training\napproach that iteratively labels the most confident unlabeled data to train a\nstronger Student model. Moreover, a new text augmentation technique (GradAug)\nis proposed to better train the Student by replacing non-crucial tokens using a\nmasked language model. We conduct extensive experiments and present analyses on\nfour downstream tasks in ToD, including intent classification, dialog state\ntracking, dialog act prediction, and response selection. Empirical results\ndemonstrate that the proposed self-training approach consistently improves\nstate-of-the-art pre-trained models (BERT, ToD-BERT) when only a small number\nof labeled data are available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mi_F/0/1/0/all/0/1\">Fei Mi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wanhao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_F/0/1/0/all/0/1\">Fengyu Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1\">Lingjing Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faltings_B/0/1/0/all/0/1\">Boi Faltings</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Layer-wise Model Pruning based on Mutual Information. (arXiv:2108.12594v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12594","description":"<p>The proposed pruning strategy offers merits over weight-based pruning\ntechniques: (1) it avoids irregular memory access since representations and\nmatrices can be squeezed into their smaller but dense counterparts, leading to\ngreater speedup; (2) in a manner of top-down pruning, the proposed method\noperates from a more global perspective based on training signals in the top\nlayer, and prunes each layer by propagating the effect of global signals\nthrough layers, leading to better performances at the same sparsity level.\nExtensive experiments show that at the same sparsity level, the proposed\nstrategy offers both greater speedup and higher performances than weight-based\npruning methods (e.g., magnitude pruning, movement pruning).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1\">Chun Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ao_X/0/1/0/all/0/1\">Xiang Ao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1\">Yuxian Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiaofei Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Smoothing Dialogue States for Open Conversational Machine Reading. (arXiv:2108.12599v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12599","description":"<p>Conversational machine reading (CMR) requires machines to communicate with\nhumans through multi-turn interactions between two salient dialogue states of\ndecision making and question generation processes. In open CMR settings, as the\nmore realistic scenario, the retrieved background knowledge would be noisy,\nwhich results in severe challenges in the information transmission. Existing\nstudies commonly train independent or pipeline systems for the two subtasks.\nHowever, those methods are trivial by using hard-label decisions to activate\nquestion generation, which eventually hinders the model performance. In this\nwork, we propose an effective gating strategy by smoothing the two dialogue\nstates in only one decoder and bridge decision making and question generation\nto provide a richer dialogue state reference. Experiments on the OR-ShARC\ndataset show the effectiveness of our method, which achieves new\nstate-of-the-art results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhuosheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_S/0/1/0/all/0/1\">Siru Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Utiyama_M/0/1/0/all/0/1\">Masao Utiyama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sumita_E/0/1/0/all/0/1\">Eiichiro Sumita</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mitigation of Diachronic Bias in Fake News Detection Dataset. (arXiv:2108.12601v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12601","description":"<p>Fake news causes significant damage to society.To deal with these fake news,\nseveral studies on building detection models and arranging datasets have been\nconducted. Most of the fake news datasets depend on a specific time period.\nConsequently, the detection models trained on such a dataset have difficulty\ndetecting novel fake news generated by political changes and social changes;\nthey may possibly result in biased output from the input, including specific\nperson names and organizational names. We refer to this problem as\n\\textbf{Diachronic Bias} because it is caused by the creation date of news in\neach dataset. In this study, we confirm the bias, especially proper nouns\nincluding person names, from the deviation of phrase appearances in each\ndataset. Based on these findings, we propose masking methods using Wikidata to\nmitigate the influence of person names and validate whether they make fake news\ndetection models robust through experiments with in-domain and out-of-domain\ndata.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Murayama_T/0/1/0/all/0/1\">Taichi Murayama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wakamiya_S/0/1/0/all/0/1\">Shoko Wakamiya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aramaki_E/0/1/0/all/0/1\">Eiji Aramaki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WALNUT: A Benchmark on Weakly Supervised Learning for Natural Language Understanding. (arXiv:2108.12603v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12603","description":"<p>Building quality machine learning models for natural language understanding\n(NLU) tasks relies heavily on labeled data. Weak supervision has been shown to\nprovide valuable supervision when large amount of labeled data is unavailable\nor expensive to obtain. Existing works studying weak supervision for NLU either\nmostly focus on a specific task or simulate weak supervision signals from\nground-truth labels. To date a benchmark for NLU with real world weak\nsupervision signals for a collection of NLU tasks is still not available. In\nthis paper, we propose such a benchmark, named WALNUT, to advocate and\nfacilitate research on weak supervision for NLU. WALNUT consists of NLU tasks\nwith different types, including both document-level prediction tasks and\ntoken-level prediction tasks and for each task contains weak labels generated\nby multiple real-world weak sources. We conduct baseline evaluations on the\nbenchmark to systematically test the value of weak supervision for NLU tasks,\nwith various weak supervision methods and model architectures. We demonstrate\nthe benefits of weak supervision for low-resource NLU tasks and expect WALNUT\nto stimulate further research on methodologies to best leverage weak\nsupervision. The benchmark and code for baselines will be publicly available at\naka.ms/walnut_benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1\">Guoqing Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karamanolakis_G/0/1/0/all/0/1\">Giannis Karamanolakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_K/0/1/0/all/0/1\">Kai Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1\">Ahmed Hassan Awadallah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HeadlineCause: A Dataset of News Headlines for Detecting Casualties. (arXiv:2108.12626v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12626","description":"<p>Detecting implicit causal relations in texts is a task that requires both\ncommon sense and world knowledge. Existing datasets are focused either on\ncommonsense causal reasoning or explicit causal relations. In this work, we\npresent HeadlineCause, a dataset for detecting implicit causal relations\nbetween pairs of news headlines. The dataset includes over 5000 headline pairs\nfrom English news and over 9000 headline pairs from Russian news labeled\nthrough crowdsourcing. The pairs vary from totally unrelated or belonging to\nthe same general topic to the ones including causation and refutation\nrelations. We also present a set of models and experiments that demonstrates\nthe dataset validity, including a multilingual XLM-RoBERTa based model for\ncausality detection and a GPT-2 based model for possible effects prediction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gusev_I/0/1/0/all/0/1\">Ilya Gusev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tikhonov_A/0/1/0/all/0/1\">Alexey Tikhonov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Oh My Mistake!: Toward Realistic Dialogue State Tracking including Turnback Utterances. (arXiv:2108.12637v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12637","description":"<p>The primary purpose of dialogue state tracking (DST), a critical component of\nan end-to-end conversational system, is to build a model that responds well to\nreal-world situations. Although we often change our minds during ordinary\nconversations, current benchmark datasets do not adequately reflect such\noccurrences and instead consist of over-simplified conversations, in which no\none changes their mind during a conversation. As the main question inspiring\nthe present study,``Are current benchmark datasets sufficiently diverse to\nhandle casual conversations in which one changes their mind?'' We found that\nthe answer is ``No'' because simply injecting template-based turnback\nutterances significantly degrades the DST model performance. The test joint\ngoal accuracy on the MultiWOZ decreased by over 5\\%p when the simplest form of\nturnback utterance was injected. Moreover, the performance degeneration worsens\nwhen facing more complicated turnback situations. However, we also observed\nthat the performance rebounds when a turnback is appropriately included in the\ntraining dataset, implying that the problem is not with the DST models but\nrather with the construction of the benchmark dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Takyoung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yukyung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_H/0/1/0/all/0/1\">Hoonsang Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_P/0/1/0/all/0/1\">Pilsung Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Misuk Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Event Extraction as Natural Language Generation. (arXiv:2108.12724v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12724","description":"<p>Event extraction (EE), the task that identifies event triggers and their\narguments in text, is usually formulated as a classification or structured\nprediction problem. Such models usually reduce labels to numeric identifiers,\nmaking them unable to take advantage of label semantics (e.g. an event type\nnamed Arrest is related to words like arrest, detain, or apprehend). This\nprevents the generalization to new event types. In this work, we formulate EE\nas a natural language generation task and propose GenEE, a model that not only\ncaptures complex dependencies within an event but also generalizes well to\nunseen or rare event types. Given a passage and an event type, GenEE is trained\nto generate a natural sentence following a predefined template for that event\ntype. The generated output is then decoded into trigger and argument\npredictions. The autoregressive generation process naturally models the\ndependencies among the predictions -- each new word predicted depends on those\nalready generated in the output sentence. Using carefully designed input\nprompts during generation, GenEE is able to capture label semantics, which\nenables the generalization to new event types. Empirical results show that our\nmodel achieves strong performance on event extraction tasks under all\nzero-shot, few-shot, and high-resource scenarios. Especially, in the\nhigh-resource setting, GenEE outperforms the state-of-the-art model on argument\nextraction and gets competitive results with the current best on end-to-end EE\ntasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hsu_I/0/1/0/all/0/1\">I-Hung Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kuan-Hao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boschee_E/0/1/0/all/0/1\">Elizabeth Boschee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miller_S/0/1/0/all/0/1\">Scott Miller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Natarajan_P/0/1/0/all/0/1\">Prem Natarajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"$k$Folden: $k$-Fold Ensemble for Out-Of-Distribution Detection. (arXiv:2108.12731v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12731","description":"<p>Out-of-Distribution (OOD) detection is an important problem in natural\nlanguage processing (NLP). In this work, we propose a simple yet effective\nframework $k$Folden, which mimics the behaviors of OOD detection during\ntraining without the use of any external data. For a task with $k$ training\nlabels, $k$Folden induces $k$ sub-models, each of which is trained on a subset\nwith $k-1$ categories with the left category masked unknown to the sub-model.\nExposing an unknown label to the sub-model during training, the model is\nencouraged to learn to equally attribute the probability to the seen $k-1$\nlabels for the unknown label, enabling this framework to simultaneously resolve\nin- and out-distribution examples in a natural way via OOD simulations. Taking\ntext classification as an archetype, we develop benchmarks for OOD detection\nusing existing text classification datasets. By conducting comprehensive\ncomparisons and analyses on the developed benchmarks, we demonstrate the\nsuperiority of $k$Folden against current methods in terms of improving OOD\ndetection performances while maintaining improved in-domain classification\naccuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoya Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiaofei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1\">Chun Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1\">Yuxian Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jun Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SummerTime: Text Summarization Toolkit for Non-experts. (arXiv:2108.12738v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12738","description":"<p>Recent advances in summarization provide models that can generate summaries\nof higher quality. Such models now exist for a number of summarization tasks,\nincluding query-based summarization, dialogue summarization, and multi-document\nsummarization. While such models and tasks are rapidly growing in the research\nfield, it has also become challenging for non-experts to keep track of them. To\nmake summarization methods more accessible to a wider audience, we develop\nSummerTime by rethinking the summarization task from the perspective of an NLP\nnon-expert. SummerTime is a complete toolkit for text summarization, including\nvarious models, datasets and evaluation metrics, for a full spectrum of\nsummarization-related tasks. SummerTime integrates with libraries designed for\nNLP researchers, and enables users with easy-to-use APIs. With SummerTime,\nusers can locate pipeline solutions and search for the best model with their\nown data, and visualize the differences, all with a few lines of code. We also\nprovide explanations for models and evaluation metrics to help users understand\nthe model behaviors and select models that best suit their needs. Our library,\nalong with a notebook demo, is available at\nhttps://github.com/Yale-LILY/SummerTime.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ni_A/0/1/0/all/0/1\">Ansong Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azerbayev_Z/0/1/0/all/0/1\">Zhangir Azerbayev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mutuma_M/0/1/0/all/0/1\">Mutethia Mutuma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_T/0/1/0/all/0/1\">Troy Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yusen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1\">Ahmed Hassan Awadallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1\">Dragomir Radev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sentence Structure and Word Relationship Modeling for Emphasis Selection. (arXiv:2108.12750v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12750","description":"<p>Emphasis Selection is a newly proposed task which focuses on choosing words\nfor emphasis in short sentences. Traditional methods only consider the sequence\ninformation of a sentence while ignoring the rich sentence structure and word\nrelationship information. In this paper, we propose a new framework that\nconsiders sentence structure via a sentence structure graph and word\nrelationship via a word similarity graph. The sentence structure graph is\nderived from the parse tree of a sentence. The word similarity graph allows\nnodes to share information with their neighbors since we argue that in emphasis\nselection, similar words are more likely to be emphasized together. Graph\nneural networks are employed to learn the representation of each node of these\ntwo graphs. Experimental results demonstrate that our framework can achieve\nsuperior performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haoran Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_W/0/1/0/all/0/1\">Wai Lam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Searching for an Effective Defender: Benchmarking Defense against Adversarial Word Substitution. (arXiv:2108.12777v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12777","description":"<p>Recent studies have shown that deep neural networks are vulnerable to\nintentionally crafted adversarial examples, and various methods have been\nproposed to defend against adversarial word-substitution attacks for neural NLP\nmodels. However, there is a lack of systematic study on comparing different\ndefense approaches under the same attacking setting. In this paper, we seek to\nfill the gap of systematic studies through comprehensive researches on\nunderstanding the behavior of neural text classifiers trained by various\ndefense methods under representative adversarial attacks. In addition, we\npropose an effective method to further improve the robustness of neural text\nclassifiers against such attacks and achieved the highest accuracy on both\nclean and adversarial examples on AGNEWS and IMDB datasets by a significant\nmargin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zongyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jianhan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_J/0/1/0/all/0/1\">Jiehang Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1\">Xiaoqing Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1\">Cho-Jui Hsieh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interpretable Propaganda Detection in News Articles. (arXiv:2108.12802v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12802","description":"<p>Online users today are exposed to misleading and propagandistic news articles\nand media posts on a daily basis. To counter thus, a number of approaches have\nbeen designed aiming to achieve a healthier and safer online news and media\nconsumption. Automatic systems are able to support humans in detecting such\ncontent; yet, a major impediment to their broad adoption is that besides being\naccurate, the decisions of such systems need also to be interpretable in order\nto be trusted and widely adopted by users. Since misleading and propagandistic\ncontent influences readers through the use of a number of deception techniques,\nwe propose to detect and to show the use of such techniques as a way to offer\ninterpretability. In particular, we define qualitatively descriptive features\nand we analyze their suitability for detecting deception techniques. We further\nshow that our interpretable features can be easily combined with pre-trained\nlanguage models, yielding state-of-the-art results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Seunghak Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martino_G/0/1/0/all/0/1\">Giovanni Da San Martino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohtarami_M/0/1/0/all/0/1\">Mitra Mohtarami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glass_J/0/1/0/all/0/1\">James Glass</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DropAttack: A Masked Weight Adversarial Training Method to Improve Generalization of Neural Networks. (arXiv:2108.12805v1 [cs.LG])","link":"http://arxiv.org/abs/2108.12805","description":"<p>Adversarial training has been proven to be a powerful regularization method\nto improve the generalization of models. However, current adversarial training\nmethods only attack the original input sample or the embedding vectors, and\ntheir attacks lack coverage and diversity. To further enhance the breadth and\ndepth of attack, we propose a novel masked weight adversarial training method\ncalled DropAttack, which enhances generalization of model by adding\nintentionally worst-case adversarial perturbations to both the input and hidden\nlayers in different dimensions and minimize the adversarial risks generated by\neach layer. DropAttack is a general technique and can be adopt to a wide\nvariety of neural networks with different architectures. To validate the\neffectiveness of the proposed method, we used five public datasets in the\nfields of natural language processing (NLP) and computer vision (CV) for\nexperimental evaluating. We compare the proposed method with other adversarial\ntraining methods and regularization methods, and our method achieves\nstate-of-the-art on all datasets. In addition, Dropattack can achieve the same\nperformance when it use only a half training data compared to other standard\ntraining method. Theoretical analysis reveals that DropAttack can perform\ngradient regularization at random on some of the input and wight parameters of\nthe model. Further visualization experiments show that DropAttack can push the\nminimum risk of the model to a lower and flatter loss landscapes. Our source\ncode is publicly available on https://github.com/nishiwen1214/DropAttack.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ni_S/0/1/0/all/0/1\">Shiwen Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiawen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kao_H/0/1/0/all/0/1\">Hung-Yu Kao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analyzing and Mitigating Interference in Neural Architecture Search. (arXiv:2108.12821v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12821","description":"<p>Weight sharing has become the \\textit{de facto} approach to reduce the\ntraining cost of neural architecture search (NAS) by reusing the weights of\nshared operators from previously trained child models. However, the estimated\naccuracy of those child models has a low rank correlation with the ground truth\naccuracy due to the interference among different child models caused by weight\nsharing. In this paper, we investigate the interference issue by sampling\ndifferent child models and calculating the gradient similarity of shared\noperators, and observe that: 1) the interference on a shared operator between\ntwo child models is positively correlated to the number of different operators\nbetween them; 2) the interference is smaller when the inputs and outputs of the\nshared operator are more similar. Inspired by these two observations, we\npropose two approaches to mitigate the interference: 1) rather than randomly\nsampling child models for optimization, we propose a gradual modification\nscheme by modifying one operator between adjacent optimization steps to\nminimize the interference on the shared operators; 2) forcing the inputs and\noutputs of the operator across all child models to be similar to reduce the\ninterference. Experiments on a BERT search space verify that mitigating\ninterference via each of our proposed methods improves the rank correlation of\nsuper-pet and combining both methods can achieve better results. Our searched\narchitecture outperforms RoBERTa$_{\\rm base}$ by 1.1 and 0.6 scores and\nELECTRA$_{\\rm base}$ by 1.6 and 1.1 scores on the dev and test set of GLUE\nbenchmark. Extensive results on the BERT compression task, SQuAD datasets and\nother search spaces also demonstrate the effectiveness and generality of our\nproposed methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xu Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1\">Kaitao Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_R/0/1/0/all/0/1\">Renqian Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leng_Y/0/1/0/all/0/1\">Yichong Leng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jian Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extractive and Abstractive Sentence Labelling of Sentiment-bearing Topics. (arXiv:2108.12822v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12822","description":"<p>This paper tackles the problem of automatically labelling sentiment-bearing\ntopics with descriptive sentence labels. We propose two approaches to the\nproblem, one extractive and the other abstractive. Both approaches rely on a\nnovel mechanism to automatically learn the relevance of each sentence in a\ncorpus to sentiment-bearing topics extracted from that corpus. The extractive\napproach uses a sentence ranking algorithm for label selection which for the\nfirst time jointly optimises topic--sentence relevance as well as\naspect--sentiment co-coverage. The abstractive approach instead addresses\naspect--sentiment co-coverage by using sentence fusion to generate a sentential\nlabel that includes relevant content from multiple sentences. To our knowledge,\nwe are the first to study the problem of labelling sentiment-bearing topics.\nOur experimental results on three real-world datasets show that both the\nextractive and abstractive approaches outperform four strong baselines in terms\nof facilitating topic understanding and interpretation. In addition, when\ncomparing extractive and abstractive labels, our evaluation shows that our best\nperforming abstractive method is able to provide more topic information\ncoverage in fewer words, at the cost of generating less grammatical labels than\nthe extractive method. We conclude that abstractive methods can effectively\nsynthesise the rich information contained in sentiment-bearing topics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Barawi_M/0/1/0/all/0/1\">Mohamad Hardyman Barawi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chenghua Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siddharthan_A/0/1/0/all/0/1\">Advaith Siddharthan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yinbin Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Behind the Scenes: An Exploration of Trigger Biases Problem in Few-Shot Event Classification. (arXiv:2108.12844v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12844","description":"<p>Few-Shot Event Classification (FSEC) aims at developing a model for event\nprediction, which can generalize to new event types with a limited number of\nannotated data. Existing FSEC studies have achieved high accuracy on different\nbenchmarks. However, we find they suffer from trigger biases that signify the\nstatistical homogeneity between some trigger words and target event types,\nwhich we summarize as trigger overlapping and trigger separability. The biases\ncan result in context-bypassing problem, i.e., correct classifications can be\ngained by looking at only the trigger words while ignoring the entire context.\nTherefore, existing models can be weak in generalizing to unseen data in real\nscenarios. To further uncover the trigger biases and assess the generalization\nability of the models, we propose two new sampling methods, Trigger-Uniform\nSampling (TUS) and COnfusion Sampling (COS), for the meta tasks construction\nduring evaluation. Besides, to cope with the context-bypassing problem in FSEC\nmodels, we introduce adversarial training and trigger reconstruction\ntechniques. Experiments show these techniques help not only improve the\nperformance, but also enhance the generalization ability of models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peiyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Runxin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1\">Damai Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_B/0/1/0/all/0/1\">Baobao Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sui_Z/0/1/0/all/0/1\">Zhifang Sui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Span Fine-tuning for Pre-trained Language Models. (arXiv:2108.12848v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12848","description":"<p>Pre-trained language models (PrLM) have to carefully manage input units when\ntraining on a very large text with a vocabulary consisting of millions of\nwords. Previous works have shown that incorporating span-level information over\nconsecutive words in pre-training could further improve the performance of\nPrLMs. However, given that span-level clues are introduced and fixed in\npre-training, previous methods are time-consuming and lack of flexibility. To\nalleviate the inconvenience, this paper presents a novel span fine-tuning\nmethod for PrLMs, which facilitates the span setting to be adaptively\ndetermined by specific downstream tasks during the fine-tuning phase. In\ndetail, any sentences processed by the PrLM will be segmented into multiple\nspans according to a pre-sampled dictionary. Then the segmentation information\nwill be sent through a hierarchical CNN module together with the representation\noutputs of the PrLM and ultimately generate a span-enhanced representation.\nExperiments on GLUE benchmark show that the proposed span fine-tuning method\nsignificantly enhances the PrLM, and at the same time, offer more flexibility\nin an efficient way.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bao_R/0/1/0/all/0/1\">Rongzhou Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhuosheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multiplex Graph Neural Network for Extractive Text Summarization. (arXiv:2108.12870v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12870","description":"<p>Extractive text summarization aims at extracting the most representative\nsentences from a given document as its summary. To extract a good summary from\na long text document, sentence embedding plays an important role. Recent\nstudies have leveraged graph neural networks to capture the inter-sentential\nrelationship (e.g., the discourse graph) to learn contextual sentence\nembedding. However, those approaches neither consider multiple types of\ninter-sentential relationships (e.g., semantic similarity &amp; natural\nconnection), nor model intra-sentential relationships (e.g, semantic &amp;\nsyntactic relationship among words). To address these problems, we propose a\nnovel Multiplex Graph Convolutional Network (Multi-GCN) to jointly model\ndifferent types of relationships among sentences and words. Based on Multi-GCN,\nwe propose a Multiplex Graph Summarization (Multi-GraS) model for extractive\ntext summarization. Finally, we evaluate the proposed models on the\nCNN/DailyMail benchmark dataset to demonstrate the effectiveness and\nsuperiority of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jing_B/0/1/0/all/0/1\">Baoyu Jing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_Z/0/1/0/all/0/1\">Zeyu You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Tao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_W/0/1/0/all/0/1\">Wei Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_H/0/1/0/all/0/1\">Hanghang Tong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigations on Speech Recognition Systems for Low-Resource Dialectal Arabic-English Code-Switching Speech. (arXiv:2108.12881v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12881","description":"<p>Code-switching (CS), defined as the mixing of languages in conversations, has\nbecome a worldwide phenomenon. The prevalence of CS has been recently met with\na growing demand and interest to build CS ASR systems. In this paper, we\npresent our work on code-switched Egyptian Arabic-English automatic speech\nrecognition (ASR). We first contribute in filling the huge gap in resources by\ncollecting, analyzing and publishing our spontaneous CS Egyptian Arabic-English\nspeech corpus. We build our ASR systems using DNN-based hybrid and\nTransformer-based end-to-end models. In this paper, we present a thorough\ncomparison between both approaches under the setting of a low-resource,\northographically unstandardized, and morphologically rich language pair. We\nshow that while both systems give comparable overall recognition results, each\nsystem provides complementary sets of strength points. We show that recognition\ncan be improved by combining the outputs of both systems. We propose several\neffective system combination approaches, where hypotheses of both systems are\nmerged on sentence- and word-levels. Our approaches result in overall WER\nrelative improvement of 4.7%, over a baseline performance of 32.1% WER. In the\ncase of intra-sentential CS sentences, we achieve WER relative improvement of\n4.8%. Our best performing system achieves 30.6% WER on ArzEn test set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hamed_I/0/1/0/all/0/1\">Injy Hamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denisov_P/0/1/0/all/0/1\">Pavel Denisov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chia-Yu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elmahdy_M/0/1/0/all/0/1\">Mohamed Elmahdy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdennadher_S/0/1/0/all/0/1\">Slim Abdennadher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vu_N/0/1/0/all/0/1\">Ngoc Thang Vu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating Answer Candidates for Quizzes and Answer-Aware Question Generators. (arXiv:2108.12898v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12898","description":"<p>In education, open-ended quiz questions have become an important tool for\nassessing the knowledge of students. Yet, manually preparing such questions is\na tedious task, and thus automatic question generation has been proposed as a\npossible alternative. So far, the vast majority of research has focused on\ngenerating the question text, relying on question answering datasets with\nreadily picked answers, and the problem of how to come up with answer\ncandidates in the first place has been largely ignored. Here, we aim to bridge\nthis gap. In particular, we propose a model that can generate a specified\nnumber of answer candidates for a given passage of text, which can then be used\nby instructors to write questions manually or can be passed as an input to\nautomatic answer-aware question generators. Our experiments show that our\nproposed answer candidate generation model outperforms several baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vachev_K/0/1/0/all/0/1\">Kristiyan Vachev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hardalov_M/0/1/0/all/0/1\">Momchil Hardalov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karadzhov_G/0/1/0/all/0/1\">Georgi Karadzhov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Georgiev_G/0/1/0/all/0/1\">Georgi Georgiev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koychev_I/0/1/0/all/0/1\">Ivan Koychev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-Grained Chemical Entity Typing with Multimodal Knowledge Representation. (arXiv:2108.12899v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12899","description":"<p>Automated knowledge discovery from trending chemical literature is essential\nfor more efficient biomedical research. How to extract detailed knowledge about\nchemical reactions from the core chemistry literature is a new emerging\nchallenge that has not been well studied. In this paper, we study the new\nproblem of fine-grained chemical entity typing, which poses interesting new\nchallenges especially because of the complex name mentions frequently occurring\nin chemistry literature and graphic representation of entities. We introduce a\nnew benchmark data set (CHEMET) to facilitate the study of the new task and\npropose a novel multi-modal representation learning framework to solve the\nproblem of fine-grained chemical entity typing by leveraging external resources\nwith chemical structures and using cross-modal attention to learn effective\nrepresentation of text in the chemistry domain. Experiment results show that\nthe proposed framework outperforms multiple state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Chenkai Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Weijiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jinfeng Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parulian_N/0/1/0/all/0/1\">Nikolaus Nova Parulian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_C/0/1/0/all/0/1\">ChengXiang Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mischievous Nominal Constructions in Universal Dependencies. (arXiv:2108.12928v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12928","description":"<p>While the highly multilingual Universal Dependencies (UD) project provides\nextensive guidelines for clausal structure as well as structure within\ncanonical nominal phrases, a standard treatment is lacking for many\n\"mischievous\" nominal phenomena that break the mold. As a result, numerous\ninconsistencies within and across corpora can be found, even in languages with\nextensive UD treebanking work, such as English. This paper surveys the kinds of\nmischievous nominal expressions attested in English UD corpora and proposes\nsolutions primarily with English in mind, but which may offer paths to\nsolutions for a variety of UD languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schneider_N/0/1/0/all/0/1\">Nathan Schneider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeldes_A/0/1/0/all/0/1\">Amir Zeldes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RetroGAN: A Cyclic Post-Specialization System for Improving Out-of-Knowledge and Rare Word Representations. (arXiv:2108.12941v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12941","description":"<p>Retrofitting is a technique used to move word vectors closer together or\nfurther apart in their space to reflect their relationships in a Knowledge Base\n(KB). However, retrofitting only works on concepts that are present in that KB.\nRetroGAN uses a pair of Generative Adversarial Networks (GANs) to learn a\none-to-one mapping between concepts and their retrofitted counterparts. It\napplies that mapping (post-specializes) to handle concepts that do not appear\nin the original KB in a manner similar to how some natural language systems\nhandle out-of-vocabulary entries. We test our system on three word-similarity\nbenchmarks and a downstream sentence simplification task and achieve the state\nof the art (CARD-660). Altogether, our results demonstrate our system's\neffectiveness for out-of-knowledge and rare word generalization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Colon_Hernandez_P/0/1/0/all/0/1\">Pedro Colon-Hernandez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xin_Y/0/1/0/all/0/1\">Yida Xin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lieberman_H/0/1/0/all/0/1\">Henry Lieberman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Havasi_C/0/1/0/all/0/1\">Catherine Havasi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Breazeal_C/0/1/0/all/0/1\">Cynthia Breazeal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chin_P/0/1/0/all/0/1\">Peter Chin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Selective Differential Privacy for Language Modeling. (arXiv:2108.12944v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12944","description":"<p>With the increasing adoption of language models in applications involving\nsensitive data, it has become crucial to protect these models from leaking\nprivate information. Previous work has attempted to tackle this challenge by\ntraining RNN-based language models with differential privacy guarantees.\nHowever, applying classical differential privacy to language models leads to\npoor model performance as the underlying privacy notion is over-pessimistic and\nprovides undifferentiated protection for all tokens of the data. Given that the\nprivate information in natural language is sparse (for example, the bulk of an\nemail might not carry personally identifiable information), we propose a new\nprivacy notion, selective differential privacy, to provide rigorous privacy\nguarantees on the sensitive portion of the data to improve model utility. To\nrealize such a new notion, we develop a corresponding privacy mechanism,\nSelective-DPSGD, for RNN-based language models. Besides language modeling, we\nalso apply the method to a more concrete application -- dialog systems.\nExperiments on both language modeling and dialog system building show that the\nproposed privacy-preserving mechanism achieves better utilities while remaining\nsafe under various privacy attacks compared to the baselines. The data, code\nand models are available at https://github.com/wyshi/lm_privacy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1\">Weiyan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_A/0/1/0/all/0/1\">Aiqi Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_E/0/1/0/all/0/1\">Evan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1\">Ruoxi Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LOT: A Benchmark for Evaluating Chinese Long Text Understanding and Generation. (arXiv:2108.12960v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12960","description":"<p>Standard multi-task benchmarks are essential for driving the progress of\ngeneral pretraining models to generalize to various downstream tasks. However,\nexisting benchmarks such as GLUE and GLGE tend to focus on short text\nunderstanding and generation tasks, without considering long text modeling,\nwhich requires many distinct capabilities such as modeling long-range\ncommonsense and discourse relations, as well as the coherence and\ncontrollability of generation. The lack of standardized benchmarks makes it\ndifficult to fully evaluate these capabilities of a model and fairly compare\ndifferent models, especially Chinese pretraining models. Therefore, we propose\nLOT, a benchmark including two understanding and two generation tasks for\nChinese long text modeling evaluation. We construct the datasets for the tasks\nbased on various kinds of human-written Chinese stories. Besides, we release an\nencoder-decoder Chinese long text pretraining model named LongLM with up to 1\nbillion parameters. We pretrain LongLM on 120G Chinese novels with two\ngenerative tasks including text infilling and conditional continuation.\nExtensive experiments on LOT demonstrate that LongLM matches the performance of\nsimilar-sized pretraining models on the understanding tasks and outperforms\nstrong baselines substantially on the generation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guan_J/0/1/0/all/0/1\">Jian Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zhuoer Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yamei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1\">Ruilin He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_X/0/1/0/all/0/1\">Xiaoxi Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1\">Changjie Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scheduled Sampling Based on Decoding Steps for Neural Machine Translation. (arXiv:2108.12963v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12963","description":"<p>Scheduled sampling is widely used to mitigate the exposure bias problem for\nneural machine translation. Its core motivation is to simulate the inference\nscene during training by replacing ground-truth tokens with predicted tokens,\nthus bridging the gap between training and inference. However, vanilla\nscheduled sampling is merely based on training steps and equally treats all\ndecoding steps. Namely, it simulates an inference scene with uniform error\nrates, which disobeys the real inference scene, where larger decoding steps\nusually have higher error rates due to error accumulations. To alleviate the\nabove discrepancy, we propose scheduled sampling methods based on decoding\nsteps, increasing the selection chance of predicted tokens with the growth of\ndecoding steps. Consequently, we can more realistically simulate the inference\nscene during training, thus better bridging the gap between training and\ninference. Moreover, we investigate scheduled sampling based on both training\nsteps and decoding steps for further improvements. Experimentally, our\napproaches significantly outperform the Transformer baseline and vanilla\nscheduled sampling on three large-scale WMT tasks. Additionally, our approaches\nalso generalize well to the text summarization task on two popular benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yijin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yufeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jinan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HELMHOLTZ: A Verifier for Tezos Smart Contracts Based on Refinement Types. (arXiv:2108.12971v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12971","description":"<p>A smart contract is a program executed on a blockchain, based on which many\ncryptocurrencies are implemented, and is being used for automating\ntransactions. Due to the large amount of money that smart contracts deal with,\nthere is a surging demand for a method that can statically and formally verify\nthem.\n</p>\n<p>This article describes our type-based static verification tool HELMHOLTZ for\nMichelson, which is a statically typed stack-based language for writing smart\ncontracts that are executed on the blockchain platform Tezos. HELMHOLTZ is\ndesigned on top of our extension of Michelson's type system with refinement\ntypes. HELMHOLTZ takes a Michelson program annotated with a user-defined\nspecification written in the form of a refinement type as input; it then\ntypechecks the program against the specification based on the refinement type\nsystem, discharging the generated verification conditions with the SMT solver\nZ3. We briefly introduce our refinement type system for the core calculus\nMini-Michelson of Michelson, which incorporates the characteristic features\nsuch as compound datatypes (e.g., lists and pairs), higher-order functions, and\ninvocation of another contract. \\HELMHOLTZ{} successfully verifies several\npractical Michelson programs, including one that transfers money to an account\nand that checks a digital signature.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nishida_Y/0/1/0/all/0/1\">Yuki Nishida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saito_H/0/1/0/all/0/1\">Hiromasa Saito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1\">Ran Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawata_A/0/1/0/all/0/1\">Akira Kawata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Furuse_J/0/1/0/all/0/1\">Jun Furuse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suenaga_K/0/1/0/all/0/1\">Kohei Suenaga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Igarashi_A/0/1/0/all/0/1\">Atsushi Igarashi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Shatter: An Efficient Transformer Encoder with Single-Headed Self-Attention and Relative Sequence Partitioning. (arXiv:2108.13032v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13032","description":"<p>The highly popular Transformer architecture, based on self-attention, is the\nfoundation of large pretrained models such as BERT, that have become an\nenduring paradigm in NLP. While powerful, the computational resources and time\nrequired to pretrain such models can be prohibitive. In this work, we present\nan alternative self-attention architecture, Shatter, that more efficiently\nencodes sequence information by softly partitioning the space of relative\npositions and applying different value matrices to different parts of the\nsequence. This mechanism further allows us to simplify the multi-headed\nattention in Transformer to single-headed. We conduct extensive experiments\nshowing that Shatter achieves better performance than BERT, with pretraining\nbeing faster per step (15% on TPU), converging in fewer steps, and offering\nconsiderable memory savings (&gt;50%). Put together, Shatter can be pretrained on\n8 V100 GPUs in 7 days, and match the performance of BERT_Base -- making the\ncost of pretraining much more affordable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_R/0/1/0/all/0/1\">Ran Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maynez_J/0/1/0/all/0/1\">Joshua Maynez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parikh_A/0/1/0/all/0/1\">Ankur P. Parikh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ASR-GLUE: A New Multi-task Benchmark for ASR-Robust Natural Language Understanding. (arXiv:2108.13048v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13048","description":"<p>Language understanding in speech-based systems have attracted much attention\nin recent years with the growing demand for voice interface applications.\nHowever, the robustness of natural language understanding (NLU) systems to\nerrors introduced by automatic speech recognition (ASR) is under-examined. %To\nfacilitate the research on ASR-robust general language understanding, In this\npaper, we propose ASR-GLUE benchmark, a new collection of 6 different NLU tasks\nfor evaluating the performance of models under ASR error across 3 different\nlevels of background noise and 6 speakers with various voice characteristics.\nBased on the proposed benchmark, we systematically investigate the effect of\nASR error on NLU tasks in terms of noise intensity, error type and speaker\nvariants. We further purpose two ways, correction-based method and data\naugmentation-based method to improve robustness of the NLU systems. Extensive\nexperimental results and analysises show that the proposed methods are\neffective to some extent, but still far from human performance, demonstrating\nthat NLU under ASR error is still very challenging and requires further\nresearch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_L/0/1/0/all/0/1\">Lingyun Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jianwei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1\">Deng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Songxiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Haitao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Base Completion Meets Transfer Learning. (arXiv:2108.13073v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13073","description":"<p>The aim of knowledge base completion is to predict unseen facts from existing\nfacts in knowledge bases. In this work, we introduce the first approach for\ntransfer of knowledge from one collection of facts to another without the need\nfor entity or relation matching. The method works for both canonicalized\nknowledge bases and uncanonicalized or open knowledge bases, i.e., knowledge\nbases where more than one copy of a real-world entity or relation may exist.\nSuch knowledge bases are a natural output of automated information extraction\ntools that extract structured data from unstructured text. Our main\ncontribution is a method that can make use of a large-scale pre-training on\nfacts, collected from unstructured text, to improve predictions on structured\ndata from a specific domain. The introduced method is the most impactful on\nsmall datasets such as ReVerb20K, where we obtained 6% absolute increase of\nmean reciprocal rank and 65% relative decrease of mean rank over the previously\nbest method, despite not relying on large pre-trained models like BERT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kocijan_V/0/1/0/all/0/1\">Vid Kocijan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lukasiewicz_T/0/1/0/all/0/1\">Thomas Lukasiewicz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NEREL: A Russian Dataset with Nested Named Entities and Relations. (arXiv:2108.13112v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13112","description":"<p>In this paper, we present NEREL, a Russian dataset for named entity\nrecognition and relation extraction. NEREL is significantly larger than\nexisting Russian datasets: to date it contains 56K annotated named entities and\n39K annotated relations. Its important difference from previous datasets is\nannotation of nested named entities, as well as relations within nested\nentities and at the discourse level. NEREL can facilitate development of novel\nmodels that can extract relations between nested named entities, as well as\nrelations on both sentence and document levels. NEREL also contains the\nannotation of events involving named entities and their roles in the events.\nThe NEREL collection is available via https://github.com/nerel-ds/NEREL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Loukachevitch_N/0/1/0/all/0/1\">Natalia Loukachevitch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artemova_E/0/1/0/all/0/1\">Ekaterina Artemova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batura_T/0/1/0/all/0/1\">Tatiana Batura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Braslavski_P/0/1/0/all/0/1\">Pavel Braslavski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denisov_I/0/1/0/all/0/1\">Ilia Denisov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ivanov_V/0/1/0/all/0/1\">Vladimir Ivanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manandhar_S/0/1/0/all/0/1\">Suresh Manandhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pugachev_A/0/1/0/all/0/1\">Alexander Pugachev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tutubalina_E/0/1/0/all/0/1\">Elena Tutubalina</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Factual Consistency Evaluation for Text Summarization via Counterfactual Estimation. (arXiv:2108.13134v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13134","description":"<p>Despite significant progress has been achieved in text summarization, factual\ninconsistency in generated summaries still severely limits its practical\napplications. Among the key factors to ensure factual consistency, a reliable\nautomatic evaluation metric is the first and the most crucial one. However,\nexisting metrics either neglect the intrinsic cause of the factual\ninconsistency or rely on auxiliary tasks, leading to an unsatisfied correlation\nwith human judgments or increasing the inconvenience of usage in practice. In\nlight of these challenges, we propose a novel metric to evaluate the factual\nconsistency in text summarization via counterfactual estimation, which\nformulates the causal relationship among the source document, the generated\nsummary, and the language prior. We remove the effect of language prior, which\ncan cause factual inconsistency, from the total causal effect on the generated\nsummary, and provides a simple yet effective way to evaluate consistency\nwithout relying on other auxiliary tasks. We conduct a series of experiments on\nthree public abstractive text summarization datasets, and demonstrate the\nadvantages of the proposed metric in both improving the correlation with human\njudgments and the convenience of usage. The source code is available at\nhttps://github.com/xieyxclack/factual_coco.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yuexiang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1\">Fei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yang Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yaliang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_B/0/1/0/all/0/1\">Bolin Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neuron-level Interpretation of Deep NLP Models: A Survey. (arXiv:2108.13138v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13138","description":"<p>The proliferation of deep neural networks in various domains has seen an\nincreased need for interpretability of these methods. A plethora of research\nhas been carried out to analyze and understand components of the deep neural\nnetwork models. Preliminary work done along these lines and papers that\nsurveyed such, were focused on a more high-level representation analysis.\nHowever, a recent branch of work has concentrated on interpretability at a more\ngranular level, analyzing neurons and groups of neurons in these large models.\nIn this paper, we survey work done on fine-grained neuron analysis including:\ni) methods developed to discover and understand neurons in a network, ii) their\nlimitations and evaluation, iii) major findings including cross architectural\ncomparison that such analyses unravel and iv) direct applications of neuron\nanalysis such as model behavior control and domain adaptation along with\npotential directions for future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sajjad_H/0/1/0/all/0/1\">Hassan Sajjad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrani_N/0/1/0/all/0/1\">Nadir Durrani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dalvi_F/0/1/0/all/0/1\">Fahim Dalvi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CSDS: A Fine-grained Chinese Dataset for Customer Service Dialogue Summarization. (arXiv:2108.13139v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13139","description":"<p>Dialogue summarization has drawn much attention recently. Especially in the\ncustomer service domain, agents could use dialogue summaries to help boost\ntheir works by quickly knowing customers' issues and service progress. These\napplications require summaries to contain the perspective of a single speaker\nand have a clear topic flow structure. Neither are available in existing\ndatasets. Therefore, in this paper, we introduce a novel Chinese dataset for\nCustomer Service Dialogue Summarization (CSDS). CSDS improves the abstractive\nsummaries in two aspects: (1) In addition to the overall summary for the whole\ndialogue, role-oriented summaries are also provided to acquire different\nspeakers' viewpoints. (2) All the summaries sum up each topic separately, thus\ncontaining the topic-level structure of the dialogue. We define tasks in CSDS\nas generating the overall summary and different role-oriented summaries for a\ngiven dialogue. Next, we compare various summarization methods on CSDS, and\nexperiment results show that existing methods are prone to generate redundant\nand incoherent summaries. Besides, the performance becomes much worse when\nanalyzing the performance on role-oriented summaries and topic structures. We\nhope that this study could benchmark Chinese dialogue summarization and benefit\nfurther studies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Haitao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Liqun Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Junnan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_L/0/1/0/all/0/1\">Lu Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiajun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zong_C/0/1/0/all/0/1\">Chengqing Zong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Sentiment Analysis Dataset for Trustworthiness Evaluation. (arXiv:2108.13140v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13140","description":"<p>While deep learning models have greatly improved the performance of most\nartificial intelligence tasks, they are often criticized to be untrustworthy\ndue to the black-box problem. Consequently, many works have been proposed to\nstudy the trustworthiness of deep learning. However, as most open datasets are\ndesigned for evaluating the accuracy of model outputs, there is still a lack of\nappropriate datasets for evaluating the inner workings of neural networks. The\nlack of datasets obviously hinders the development of trustworthiness research.\nTherefore, in order to systematically evaluate the factors for building\ntrustworthy systems, we propose a novel and well-annotated sentiment analysis\ndataset to evaluate robustness and interpretability. To evaluate these factors,\nour dataset contains diverse annotations about the challenging distribution of\ninstances, manual adversarial instances and sentiment explanations. Several\nevaluation metrics are further proposed for interpretability and robustness.\nBased on the dataset and metrics, we conduct comprehensive comparisons for the\ntrustworthiness of three typical models, and also study the relations between\naccuracy, robustness and interpretability. We release this trustworthiness\nevaluation dataset at \\url{https://github/xyz} and hope our work can facilitate\nthe progress on building more trustworthy systems for real-world applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lijie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1\">Shuyuan Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hongxuan Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1\">Xinyan Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Ying Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Differentiable Prompt Makes Pre-trained Language Models Better Few-shot Learners. (arXiv:2108.13161v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13161","description":"<p>Large-scale pre-trained language models have contributed significantly to\nnatural language processing by demonstrating remarkable abilities as few-shot\nlearners. However, their effectiveness depends mainly on scaling the model\nparameters and prompt design, hindering their implementation in most real-world\napplications. This study proposes a novel pluggable, extensible, and efficient\napproach named DifferentiAble pRompT (DART), which can convert small language\nmodels into better few-shot learners without any prompt engineering. The main\nprinciple behind this approach involves reformulating potential natural\nlanguage processing tasks into the task of a pre-trained language model and\ndifferentially optimizing the prompt template as well as the target label with\nbackpropagation. Furthermore, the proposed approach can be: (i) Plugged to any\npre-trained language models; (ii) Extended to widespread classification tasks.\nA comprehensive evaluation of standard NLP tasks demonstrates that the proposed\napproach achieves a better few-shot performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Luoqiu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_Z/0/1/0/all/0/1\">Zhen Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exposure Bias versus Self-Recovery: Are Distortions Really Incremental for Autoregressive Text Generation?. (arXiv:1905.10617v9 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/1905.10617","description":"<p>Exposure bias has been regarded as a central problem for auto-regressive\nlanguage models (LM). It claims that teacher forcing would cause the test-time\ngeneration to be incrementally distorted due to the training-generation\ndiscrepancy. Although a lot of algorithms have been proposed to avoid teacher\nforcing and therefore alleviate exposure bias, there is little work showing how\nserious the exposure bias problem actually is. In this work, we focus on the\ntask of open-ended language generation, propose metrics to quantify the impact\nof exposure bias in the aspects of quality, diversity, and consistency. Our key\nintuition is that if we feed ground-truth data prefixes (instead of prefixes\ngenerated by the model itself) into the model and ask it to continue the\ngeneration, the performance should become much better because the\ntraining-generation discrepancy in the prefix is removed. Both automatic and\nhuman evaluations are conducted in our experiments. On the contrary to the\npopular belief in exposure bias, we find that the the distortion induced by the\nprefix discrepancy is limited, and does not seem to be incremental during the\ngeneration. Moreover, our analysis reveals an interesting self-recovery ability\nof the LM, which we hypothesize to be countering the harmful effects from\nexposure bias.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1\">Tianxing He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jingzhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhiming Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glass_J/0/1/0/all/0/1\">James Glass</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Keeping it simple: Implementation and performance of the proto-principle of adaptation and learning in the language sciences. (arXiv:2003.03813v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2003.03813","description":"<p>In this paper we present the Widrow-Hoff rule and its applications to\nlanguage data. After contextualizing the rule historically and placing it in\nthe chain of neurally inspired artificial learning models, we explain its\nrationale and implementational considerations. Using a number of case studies\nwe illustrate how the Widrow-Hoff rule offers unexpected opportunities for the\ncomputational simulation of a range of language phenomena that make it possible\nto approach old problems from a novel perspective.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Milin_P/0/1/0/all/0/1\">Petar Milin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madabushi_H/0/1/0/all/0/1\">Harish Tayyar Madabushi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Croucher_M/0/1/0/all/0/1\">Michael Croucher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Divjak_D/0/1/0/all/0/1\">Dagmar Divjak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-Shot Learning with Common Sense Knowledge Graphs. (arXiv:2006.10713v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2006.10713","description":"<p>Zero-shot learning relies on semantic class representations such as\nhand-engineered attributes or learned embeddings to predict classes without any\nlabeled examples. We propose to learn class representations by embedding nodes\nfrom common sense knowledge graphs in a vector space. Common sense knowledge\ngraphs are an untapped source of explicit high-level knowledge that requires\nlittle human effort to apply to a range of tasks. To capture the knowledge in\nthe graph, we introduce ZSL-KG, a general-purpose framework with a novel\ntransformer graph convolutional network (TrGCN) for generating class\nrepresentations. Our proposed TrGCN architecture computes non-linear\ncombinations of node neighbourhoods. Our results show that ZSL-KG improves over\nexisting WordNet-based methods on five out of six zero-shot benchmark datasets\nin language and vision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nayak_N/0/1/0/all/0/1\">Nihal V. Nayak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bach_S/0/1/0/all/0/1\">Stephen H. Bach</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Best-First Beam Search. (arXiv:2007.03909v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2007.03909","description":"<p>Decoding for many NLP tasks requires an effective heuristic algorithm for\napproximating exact search since the problem of searching the full output space\nis often intractable, or impractical in many settings. The default algorithm\nfor this job is beam search -- a pruned version of breadth-first search. Quite\nsurprisingly, beam search often returns better results than exact inference due\nto beneficial search bias for NLP tasks. In this work, we show that the\nstandard implementation of beam search can be made up to 10x faster in\npractice. Our method assumes that the scoring function is monotonic in the\nsequence length, which allows us to safely prune hypotheses that cannot be in\nthe final set of hypotheses early on. We devise effective monotonic\napproximations to popular nonmonontic scoring functions, including length\nnormalization and mutual information decoding. Lastly, we propose a\nmemory-reduced variant of Best-First Beam Search, which has a similar\nbeneficial search bias in terms of downstream performance, but runs in a\nfraction of the time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meister_C/0/1/0/all/0/1\">Clara Meister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vieira_T/0/1/0/all/0/1\">Tim Vieira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Audiovisual Speech Synthesis using Tacotron2. (arXiv:2008.00620v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2008.00620","description":"<p>Audiovisual speech synthesis is the problem of synthesizing a talking face\nwhile maximizing the coherency of the acoustic and visual speech. In this\npaper, we propose and compare two audiovisual speech synthesis systems for 3D\nface models. The first system is the AVTacotron2, which is an end-to-end\ntext-to-audiovisual speech synthesizer based on the Tacotron2 architecture.\nAVTacotron2 converts a sequence of phonemes representing the sentence to\nsynthesize into a sequence of acoustic features and the corresponding\ncontrollers of a face model. The output acoustic features are used to condition\na WaveRNN to reconstruct the speech waveform, and the output facial controllers\nare used to generate the corresponding video of the talking face. The second\naudiovisual speech synthesis system is modular, where acoustic speech is\nsynthesized from text using the traditional Tacotron2. The reconstructed\nacoustic speech signal is then used to drive the facial controls of the face\nmodel using an independently trained audio-to-facial-animation neural network.\nWe further condition both the end-to-end and modular approaches on emotion\nembeddings that encode the required prosody to generate emotional audiovisual\nspeech. We analyze the performance of the two systems and compare them to the\nground truth videos using subjective evaluation tests. The end-to-end and\nmodular systems are able to synthesize close to human-like audiovisual speech\nwith mean opinion scores (MOS) of 4.1 and 3.9, respectively, compared to a MOS\nof 4.1 for the ground truth generated from professionally recorded videos.\nWhile the end-to-end system gives a better overall quality, the modular\napproach is more flexible and the quality of acoustic speech and visual speech\nsynthesis is almost independent of each other.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Abdelaziz_A/0/1/0/all/0/1\">Ahmed Hussen Abdelaziz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kumar_A/0/1/0/all/0/1\">Anushree Prasanna Kumar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Seivwright_C/0/1/0/all/0/1\">Chloe Seivwright</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fanelli_G/0/1/0/all/0/1\">Gabriele Fanelli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Binder_J/0/1/0/all/0/1\">Justin Binder</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Stylianou_Y/0/1/0/all/0/1\">Yannis Stylianou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kajarekar_S/0/1/0/all/0/1\">Sachin Kajarekar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rank over Class: The Untapped Potential of Ranking in Natural Language Processing. (arXiv:2009.05160v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2009.05160","description":"<p>Text classification has long been a staple within Natural Language Processing\n(NLP) with applications spanning across diverse areas such as sentiment\nanalysis, recommender systems and spam detection. With such a powerful\nsolution, it is often tempting to use it as the go-to tool for all NLP problems\nsince when you are holding a hammer, everything looks like a nail. However, we\nargue here that many tasks which are currently addressed using classification\nare in fact being shoehorned into a classification mould and that if we instead\naddress them as a ranking problem, we not only improve the model, but we\nachieve better performance. We propose a novel end- to-end ranking approach\nconsisting of a Transformer network responsible for producing representations\nfor a pair of text sequences, which are in turn passed into a context\naggregating network outputting ranking scores used to determine an ordering to\nthe sequences based on some notion of relevance. We perform numerous\nexperiments on publicly-available datasets and investigate the applications of\nranking in problems often solved using classification. In an experiment on a\nheavily-skewed sentiment analysis dataset, converting ranking results to\nclassification labels yields an approximately 22% improvement over\nstate-of-the-art text classification, demonstrating the efficacy of text\nranking over text classification in certain scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Atapour_Abarghouei_A/0/1/0/all/0/1\">Amir Atapour-Abarghouei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bonner_S/0/1/0/all/0/1\">Stephen Bonner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McGough_A/0/1/0/all/0/1\">Andrew Stephen McGough</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weight Squeezing: Reparameterization for Knowledge Transfer and Model Compression. (arXiv:2010.06993v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2010.06993","description":"<p>In this work, we present a novel approach for simultaneous knowledge transfer\nand model compression called Weight Squeezing. With this method, we perform\nknowledge transfer from a teacher model by learning the mapping from its\nweights to smaller student model weights.\n</p>\n<p>We applied Weight Squeezing to a pre-trained text classification model based\non BERT-Medium model and compared our method to various other knowledge\ntransfer and model compression methods on GLUE multitask benchmark. We observed\nthat our approach produces better results while being significantly faster than\nother methods for training student models.\n</p>\n<p>We also proposed a variant of Weight Squeezing called Gated Weight Squeezing,\nfor which we combined fine-tuning of BERT-Medium model and learning mapping\nfrom BERT-Base weights. We showed that fine-tuning with Gated Weight Squeezing\noutperforms plain fine-tuning of BERT-Medium model as well as other concurrent\nSoTA approaches while much being easier to implement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chumachenko_A/0/1/0/all/0/1\">Artem Chumachenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gavrilov_D/0/1/0/all/0/1\">Daniil Gavrilov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balagansky_N/0/1/0/all/0/1\">Nikita Balagansky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalaidin_P/0/1/0/all/0/1\">Pavel Kalaidin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Topic-Guided Abstractive Text Summarization: a Joint Learning Approach. (arXiv:2010.10323v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.10323","description":"<p>We introduce a new approach for abstractive text summarization, Topic-Guided\nAbstractive Summarization, which calibrates long-range dependencies from\ntopic-level features with globally salient content. The idea is to incorporate\nneural topic modeling with a Transformer-based sequence-to-sequence (seq2seq)\nmodel in a joint learning framework. This design can learn and preserve the\nglobal semantics of the document, which can provide additional contextual\nguidance for capturing important ideas of the document, thereby enhancing the\ngeneration of summary. We conduct extensive experiments on two datasets and the\nresults show that our proposed model outperforms many extractive and\nabstractive systems in terms of both ROUGE measurements and human evaluation.\nOur code is available at: https://github.com/chz816/tas.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Chujie Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kunpeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Harry Jiannan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1\">Ling Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhe Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SlimIPL: Language-Model-Free Iterative Pseudo-Labeling. (arXiv:2010.11524v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.11524","description":"<p>Recent results in end-to-end automatic speech recognition have demonstrated\nthe efficacy of pseudo-labeling for semi-supervised models trained both with\nConnectionist Temporal Classification (CTC) and Sequence-to-Sequence (seq2seq)\nlosses. Iterative Pseudo-Labeling (IPL), which continuously trains a single\nmodel using pseudo-labels iteratively re-generated as the model learns, has\nbeen shown to further improve performance in ASR. We improve upon the IPL\nalgorithm: as the model learns, we propose to iteratively re-generate\ntranscriptions with hard labels (the most probable tokens), that is, without a\nlanguage model. We call this approach Language-Model-Free IPL (slimIPL) and\ngive a resultant training setup for low-resource settings with CTC-based\nmodels. slimIPL features a dynamic cache for pseudo-labels which reduces\nsensitivity to changes in relabeling hyperparameters and results in improves\ntraining stability. slimIPL is also highly-efficient and requires 3.5-4x fewer\ncomputational resources to converge than other state-of-the-art\nsemi/self-supervised approaches. With only 10 hours of labeled audio, slimIPL\nis competitive with self-supervised approaches, and is state-of-the-art with\n100 hours of labeled audio without the use of a language model both at test\ntime and during pseudo-label generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Likhomanenko_T/0/1/0/all/0/1\">Tatiana Likhomanenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qiantong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kahn_J/0/1/0/all/0/1\">Jacob Kahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Synnaeve_G/0/1/0/all/0/1\">Gabriel Synnaeve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collobert_R/0/1/0/all/0/1\">Ronan Collobert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BanglaBERT: Combating Embedding Barrier in Multilingual Models for Low-Resource Language Understanding. (arXiv:2101.00204v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2101.00204","description":"<p>In this paper, we introduce ``Embedding Barrier'', a phenomenon that limits\nthe monolingual performance of multilingual models on low-resource languages\nhaving unique typologies. We build `BanglaBERT', a Bangla language model\npretrained on 18.6 GB Internet-crawled data and benchmark on five standard NLU\ntasks. We discover a significant drop in the performance of the\nstate-of-the-art multilingual model (XLM-R) from BanglaBERT and attribute this\nto the Embedding Barrier through comprehensive experiments. We identify that a\nmultilingual model's performance on a low-resource language is hurt when its\nwriting script is not similar to any of the high-resource languages. To tackle\nthe barrier, we propose a straightforward solution by transcribing languages to\na common script, which can effectively improve the performance of a\nmultilingual model for the Bangla language. As a bi-product of the standard NLU\nbenchmarks, we introduce a new downstream dataset on natural language inference\n(NLI) and show that BanglaBERT outperforms previous state-of-the-art results on\nall tasks by up to 3.5%. We are making the BanglaBERT language model and the\nnew Bangla NLI dataset publicly available in the hope of advancing the\ncommunity. The resources can be found at\n\\url{https://github.com/csebuetnlp/banglabert}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharjee_A/0/1/0/all/0/1\">Abhik Bhattacharjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_T/0/1/0/all/0/1\">Tahmid Hasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samin_K/0/1/0/all/0/1\">Kazi Samin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1\">Md Saiful Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1\">M. Sohel Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iqbal_A/0/1/0/all/0/1\">Anindya Iqbal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahriyar_R/0/1/0/all/0/1\">Rifat Shahriyar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast End-to-End Speech Recognition via Non-Autoregressive Models and Cross-Modal Knowledge Transferring from BERT. (arXiv:2102.07594v6 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2102.07594","description":"<p>Attention-based encoder-decoder (AED) models have achieved promising\nperformance in speech recognition. However, because the decoder predicts text\ntokens (such as characters or words) in an autoregressive manner, it is\ndifficult for an AED model to predict all tokens in parallel. This makes the\ninference speed relatively slow. We believe that because the encoder already\ncaptures the whole speech utterance, which has the token-level relationship\nimplicitly, we can predict a token without explicitly autoregressive language\nmodeling. When the prediction of a token does not rely on other tokens, the\nparallel prediction of all tokens in the sequence is realizable. Based on this\nidea, we propose a non-autoregressive speech recognition model called LASO\n(Listen Attentively, and Spell Once). The model consists of an encoder, a\ndecoder, and a position dependent summarizer (PDS). The three modules are based\non basic attention blocks. The encoder extracts high-level representations from\nthe speech. The PDS uses positional encodings corresponding to tokens to\nconvert the acoustic representations into token-level representations. The\ndecoder further captures token-level relationships with the self-attention\nmechanism. At last, the probability distribution on the vocabulary is computed\nfor each token position. Therefore, speech recognition is re-formulated as a\nposition-wise classification problem. Further, we propose a cross-modal\ntransfer learning method to refine semantics from a large-scale pre-trained\nlanguage model BERT for improving the performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Ye Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1\">Jiangyan Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_J/0/1/0/all/0/1\">Jianhua Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1\">Zhengkun Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Z/0/1/0/all/0/1\">Zhengqi Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shuai Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A More Fine-Grained Aspect-Sentiment-Opinion Triplet Extraction Task. (arXiv:2103.15255v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.15255","description":"<p>Aspect Sentiment Triplet Extraction (ASTE) aims to extract aspect term,\nsentiment and opinion term triplets from sentences and tries to provide a\ncomplete solution for aspect-based sentiment analysis (ABSA). However, some\ntriplets extracted by ASTE are confusing, since the sentiment in a triplet\nextracted by ASTE is the sentiment that the sentence expresses toward the\naspect term rather than the sentiment of the aspect term and opinion term pair.\nIn this paper, we introduce a more fine-grained Aspect-Sentiment-Opinion\nTriplet Extraction (ASOTE) Task. ASOTE also extracts aspect term, sentiment and\nopinion term triplets. However, the sentiment in a triplet extracted by ASOTE\nis the sentiment of the aspect term and opinion term pair. We build four\ndatasets for ASOTE based on several popular ABSA benchmarks. We propose a\nPosition-aware BERT-based Framework (PBF) to address this task. PBF first\nextracts aspect terms from sentences. For each extracted aspect term, PBF first\ngenerates aspect term-specific sentence representations considering both the\nmeaning and the position of the aspect term, then extracts associated opinion\nterms and predicts the sentiments of the aspect term and opinion term pairs\nbased on the sentence representations. Experimental results on the four\ndatasets show the effectiveness of PBF.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuncong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenjun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_S/0/1/0/all/0/1\">Sheng-hua Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_C/0/1/0/all/0/1\">Cunxiang Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yancheng He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting of a Patient's Condition From Clinical Narratives Using Natural Language Representation. (arXiv:2104.03969v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.03969","description":"<p>The rapid progress in clinical data management systems and artificial\nintelligence approaches enable the era of personalized medicine. Intensive care\nunits (ICUs) are the ideal clinical research environment for such development\nbecause they collect many clinical data and are highly computerized\nenvironments. We designed a retrospective clinical study on a prospective ICU\ndatabase using clinical natural language to help in the early diagnosis of\nheart failure in critically ill children. The methodology consisted of\nempirical experiments of a learning algorithm to learn the hidden\ninterpretation and presentation of the French clinical note data. This study\nincluded 1386 patients' clinical notes with 5444 single lines of notes. There\nwere 1941 positive cases (36 % of total) and 3503 negative cases classified by\ntwo independent physicians using a standardized approach. The multilayer\nperceptron neural network outperforms other discriminative and generative\nclassifiers. Consequently, the proposed framework yields an overall\nclassification performance with 89 % accuracy, 88 % recall, and 89 % precision.\nThis study successfully applied learning representation and machine learning\nalgorithms to detect heart failure from clinical natural language in a single\nFrench institution. Further work is needed to use the same methodology in other\ninstitutions and other languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1\">Thanh-Dung Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noumeir_R/0/1/0/all/0/1\">Rita Noumeir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rambaud_J/0/1/0/all/0/1\">Jerome Rambaud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sans_G/0/1/0/all/0/1\">Guillaume Sans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jouvet_P/0/1/0/all/0/1\">Philippe Jouvet</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge-aware Prompt-tuning with Synergistic Optimization for Relation Extraction. (arXiv:2104.07650v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07650","description":"<p>Recently, prompt-tuning has achieved promising results on some few-shot\nclassification tasks. The core idea of prompt-tuning is to insert text pieces,\ni.e., templates, into the input and transform a classification task into a\nmasked language modeling problem. However, as for relation extraction,\ndetermining the appropriate prompt template requires domain expertise. Single\nlabel word handcrafted or auto-searched is cumbersome and time-consuming to\nverify their effectiveness in non-few-shot scenarios. Further, there exist\nabundant semantic knowledge among the entities and relation labels which cannot\nbe ignored. To this end, we focus on incorporating knowledge into prompt-tuning\nfor relation extraction and propose a Knowledge-aware prompt-tuning with\nsynergistic optimization (KNIGHT) approach. Specifically, we inject entity and\nrelation knowledge into prompt construction with learnable virtual template\nwords and answer words and jointly optimize their representation with knowledge\nconstraints. Extensive experimental results on five datasets with standard and\nlow-resource settings demonstrate the effectiveness of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yunzhi Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cost-effective End-to-end Information Extraction for Semi-structured Document Images. (arXiv:2104.08041v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08041","description":"<p>A real-world information extraction (IE) system for semi-structured document\nimages often involves a long pipeline of multiple modules, whose complexity\ndramatically increases its development and maintenance cost. One can instead\nconsider an end-to-end model that directly maps the input to the target output\nand simplify the entire process. However, such generation approach is known to\nlead to unstable performance if not designed carefully. Here we present our\nrecent effort on transitioning from our existing pipeline-based IE system to an\nend-to-end system focusing on practical challenges that are associated with\nreplacing and deploying the system in real, large-scale production. By\ncarefully formulating document IE as a sequence generation task, we show that a\nsingle end-to-end IE system can be built and still achieve competent\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hwang_W/0/1/0/all/0/1\">Wonseok Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hyunji Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yim_J/0/1/0/all/0/1\">Jinyeong Yim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1\">Geewook Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_M/0/1/0/all/0/1\">Minjoon Seo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Model Evaluation Beyond Perplexity. (arXiv:2106.00085v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.00085","description":"<p>We propose an alternate approach to quantifying how well language models\nlearn natural language: we ask how well they match the statistical tendencies\nof natural language. To answer this question, we analyze whether text generated\nfrom language models exhibits the statistical tendencies present in the\nhuman-generated text on which they were trained. We provide a framework--paired\nwith significance tests--for evaluating the fit of language models to these\ntrends. We find that neural language models appear to learn only a subset of\nthe tendencies considered, but align much more closely with empirical trends\nthan proposed theoretical distributions (when present). Further, the fit to\ndifferent distributions is highly-dependent on both model architecture and\ngeneration strategy. As concrete examples, text generated under the nucleus\nsampling scheme adheres more closely to the type--token relationship of natural\nlanguage than text produced using standard ancestral sampling; text from LSTMs\nreflects the natural language distributions over length, stopwords, and symbols\nsurprisingly well.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meister_C/0/1/0/all/0/1\">Clara Meister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CBLUE: A Chinese Biomedical Language Understanding Evaluation Benchmark. (arXiv:2106.08087v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.08087","description":"<p>Artificial Intelligence (AI), along with the recent progress in biomedical\nlanguage understanding, is gradually changing medical practice. With the\ndevelopment of biomedical language understanding benchmarks, AI applications\nare widely used in the medical field. However, most benchmarks are limited to\nEnglish, which makes it challenging to replicate many of the successes in\nEnglish for other languages. To facilitate research in this direction, we\ncollect real-world biomedical data and present the first Chinese Biomedical\nLanguage Understanding Evaluation (CBLUE) benchmark: a collection of natural\nlanguage understanding tasks including named entity recognition, information\nextraction, clinical diagnosis normalization, single-sentence/sentence-pair\nclassification, and an associated online platform for model evaluation,\ncomparison, and analysis. To establish evaluation on these tasks, we report\nempirical results with the current 11 pre-trained Chinese models, and\nexperimental results show that state-of-the-art neural models perform by far\nworse than the human ceiling. Our benchmark is released at\n\\url{https://tianchi.aliyun.com/dataset/dataDetail?dataId=95414&amp;lang=en-us}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mosha Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_Z/0/1/0/all/0/1\">Zhen Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaozhuan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_X/0/1/0/all/0/1\">Xin Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_K/0/1/0/all/0/1\">Kangping Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jian Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_Y/0/1/0/all/0/1\">Yuan Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_G/0/1/0/all/0/1\">Guotong Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sui_Z/0/1/0/all/0/1\">Zhifang Sui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_B/0/1/0/all/0/1\">Baobao Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zong_H/0/1/0/all/0/1\">Hui Zong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zheng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linfeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Jun Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zan_H/0/1/0/all/0/1\">Hongying Zan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kunli Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_B/0/1/0/all/0/1\">Buzhou Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qingcai Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RadGraph: Extracting Clinical Entities and Relations from Radiology Reports. (arXiv:2106.14463v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.14463","description":"<p>Extracting structured clinical information from free-text radiology reports\ncan enable the use of radiology report information for a variety of critical\nhealthcare applications. In our work, we present RadGraph, a dataset of\nentities and relations in full-text chest X-ray radiology reports based on a\nnovel information extraction schema we designed to structure radiology reports.\nWe release a development dataset, which contains board-certified radiologist\nannotations for 500 radiology reports from the MIMIC-CXR dataset (14,579\nentities and 10,889 relations), and a test dataset, which contains two\nindependent sets of board-certified radiologist annotations for 100 radiology\nreports split equally across the MIMIC-CXR and CheXpert datasets. Using these\ndatasets, we train and test a deep learning model, RadGraph Benchmark, that\nachieves a micro F1 of 0.82 and 0.73 on relation extraction on the MIMIC-CXR\nand CheXpert test sets respectively. Additionally, we release an inference\ndataset, which contains annotations automatically generated by RadGraph\nBenchmark across 220,763 MIMIC-CXR reports (around 6 million entities and 4\nmillion relations) and 500 CheXpert reports (13,783 entities and 9,908\nrelations) with mappings to associated chest radiographs. Our freely available\ndataset can facilitate a wide range of research in medical natural language\nprocessing, as well as computer vision and multi-modal learning when linked to\nchest radiographs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Saahil Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_A/0/1/0/all/0/1\">Ashwin Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saporta_A/0/1/0/all/0/1\">Adriel Saporta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Truong_S/0/1/0/all/0/1\">Steven QH Truong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duong_D/0/1/0/all/0/1\">Du Nguyen Duong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1\">Tan Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chambon_P/0/1/0/all/0/1\">Pierre Chambon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lungren_M/0/1/0/all/0/1\">Matthew P. Lungren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_A/0/1/0/all/0/1\">Andrew Y. Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Langlotz_C/0/1/0/all/0/1\">Curtis P. Langlotz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajpurkar_P/0/1/0/all/0/1\">Pranav Rajpurkar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"M2Lens: Visualizing and Explaining Multimodal Models for Sentiment Analysis. (arXiv:2107.08264v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2107.08264","description":"<p>Multimodal sentiment analysis aims to recognize people's attitudes from\nmultiple communication channels such as verbal content (i.e., text), voice, and\nfacial expressions. It has become a vibrant and important research topic in\nnatural language processing. Much research focuses on modeling the complex\nintra- and inter-modal interactions between different communication channels.\nHowever, current multimodal models with strong performance are often\ndeep-learning-based techniques and work like black boxes. It is not clear how\nmodels utilize multimodal information for sentiment predictions. Despite recent\nadvances in techniques for enhancing the explainability of machine learning\nmodels, they often target unimodal scenarios (e.g., images, sentences), and\nlittle research has been done on explaining multimodal models. In this paper,\nwe present an interactive visual analytics system, M2Lens, to visualize and\nexplain multimodal models for sentiment analysis. M2Lens provides explanations\non intra- and inter-modal interactions at the global, subset, and local levels.\nSpecifically, it summarizes the influence of three typical interaction types\n(i.e., dominance, complement, and conflict) on the model predictions. Moreover,\nM2Lens identifies frequent and influential multimodal features and supports the\nmulti-faceted exploration of model behaviors from language, acoustic, and\nvisual modalities. Through two case studies and expert interviews, we\ndemonstrate our system can help users gain deep insights into the multimodal\nmodels for sentiment analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xingbo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jianben He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhihua Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Muqiao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_H/0/1/0/all/0/1\">Huamin Qu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Detection of COVID-19 Vaccine Misinformation with Graph Link Prediction. (arXiv:2108.02314v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.02314","description":"<p>Enormous hope in the efficacy of vaccines became recently a successful\nreality in the fight against the COVID-19 pandemic. However, vaccine hesitancy,\nfueled by exposure to social media misinformation about COVID-19 vaccines\nbecame a major hurdle. Therefore, it is essential to automatically detect where\nmisinformation about COVID-19 vaccines on social media is spread and what kind\nof misinformation is discussed, such that inoculation interventions can be\ndelivered at the right time and in the right place, in addition to\ninterventions designed to address vaccine hesitancy. This paper is addressing\nthe first step in tackling hesitancy against COVID-19 vaccines, namely the\nautomatic detection of known misinformation about the vaccines on Twitter, the\nsocial media platform that has the highest volume of conversations about\nCOVID-19 and its vaccines. We present CoVaxLies, a new dataset of tweets judged\nrelevant to several misinformation targets about COVID-19 vaccines on which a\nnovel method of detecting misinformation was developed. Our method organizes\nCoVaxLies in a Misinformation Knowledge Graph as it casts misinformation\ndetection as a graph link prediction problem. The misinformation detection\nmethod detailed in this paper takes advantage of the link scoring functions\nprovided by several knowledge embedding methods. The experimental results\ndemonstrate the superiority of this method when compared with\nclassification-based methods, widely used currently.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Weinzierl_M/0/1/0/all/0/1\">Maxwell A. Weinzierl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harabagiu_S/0/1/0/all/0/1\">Sanda M. Harabagiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AMMUS : A Survey of Transformer-based Pretrained Models in Natural Language Processing. (arXiv:2108.05542v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.05542","description":"<p>Transformer-based pretrained language models (T-PTLMs) have achieved great\nsuccess in almost every NLP task. The evolution of these models started with\nGPT and BERT. These models are built on the top of transformers,\nself-supervised learning and transfer learning. Transformed-based PTLMs learn\nuniversal language representations from large volumes of text data using\nself-supervised learning and transfer this knowledge to downstream tasks. These\nmodels provide good background knowledge to downstream tasks which avoids\ntraining of downstream models from scratch. In this comprehensive survey paper,\nwe initially give a brief overview of self-supervised learning. Next, we\nexplain various core concepts like pretraining, pretraining methods,\npretraining tasks, embeddings and downstream adaptation methods. Next, we\npresent a new taxonomy of T-PTLMs and then give brief overview of various\nbenchmarks including both intrinsic and extrinsic. We present a summary of\nvarious useful libraries to work with T-PTLMs. Finally, we highlight some of\nthe future research directions which will further improve these models. We\nstrongly believe that this comprehensive survey paper will serve as a good\nreference to learn the core concepts as well as to stay updated with the recent\nhappenings in T-PTLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kalyan_K/0/1/0/all/0/1\">Katikapalli Subramanyam Kalyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajasekharan_A/0/1/0/all/0/1\">Ajit Rajasekharan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sangeetha_S/0/1/0/all/0/1\">Sivanesan Sangeetha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HiTab: A Hierarchical Table Dataset for Question Answering and Natural Language Generation. (arXiv:2108.06712v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.06712","description":"<p>Tables are often created with hierarchies, but existing works on table\nreasoning mainly focus on flat tables and neglect hierarchical tables.\nHierarchical tables challenge existing methods by hierarchical indexing, as\nwell as implicit relationships of calculation and semantics. This work presents\nHiTab, a free and open dataset to study question answering (QA) and natural\nlanguage generation (NLG) over hierarchical tables. HiTab is a cross-domain\ndataset constructed from a wealth of statistical reports (analyses) and\nWikipedia pages, and has unique characteristics: (1) nearly all tables are\nhierarchical, and (2) both target sentences for NLG and questions for QA are\nrevised from original, meaningful, and diverse descriptive sentences authored\nby analysts and professions of reports. (3) to reveal complex numerical\nreasoning in statistical analyses, we provide fine-grained annotations of\nentity and quantity alignment. HiTab provides 10,686 QA pairs and descriptive\nsentences with well-annotated quantity and entity alignment on 3,597 tables\nwith broad coverage of table hierarchies and numerical reasoning types.\n</p>\n<p>Targeting hierarchical structure, we devise a novel hierarchy-aware logical\nform for symbolic reasoning over tables, which shows high effectiveness.\nTargeting complex numerical reasoning, we propose partially supervised training\ngiven annotations of entity and quantity alignment, which helps models to\nlargely reduce spurious predictions in the QA task. In the NLG task, we find\nthat entity and quantity alignment also helps NLG models to generate better\nresults in a conditional generation setting. Experiment results of\nstate-of-the-art baselines suggest that this dataset presents a strong\nchallenge and a valuable benchmark for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zhoujun Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Haoyu Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhiruo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1\">Ran Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jiaqi Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Shi Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1\">Jian-Guang Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongmei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fastformer: Additive Attention Can Be All You Need. (arXiv:2108.09084v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.09084","description":"<p>Transformer is a powerful model for text understanding. However, it is\ninefficient due to its quadratic complexity to input sequence length. Although\nthere are many methods on Transformer acceleration, they are still either\ninefficient on long sequences or not effective enough. In this paper, we\npropose Fastformer, which is an efficient Transformer model based on additive\nattention. In Fastformer, instead of modeling the pair-wise interactions\nbetween tokens, we first use additive attention mechanism to model global\ncontexts, and then further transform each token representation based on its\ninteraction with global context representations. In this way, Fastformer can\nachieve effective context modeling with linear complexity. Extensive\nexperiments on five datasets show that Fastformer is much more efficient than\nmany existing Transformer models and can meanwhile achieve comparable or even\nbetter long text modeling performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chuhan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fangzhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_T/0/1/0/all/0/1\">Tao Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yongfeng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xing Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-tuning Pretrained Language Models with Label Attention for Explainable Biomedical Text Classification. (arXiv:2108.11809v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.11809","description":"<p>The massive growth of digital biomedical data is making biomedical text\nindexing and classification increasingly important. Accordingly, previous\nresearch has devised numerous deep learning techniques focused on using\nfeedforward, convolutional or recurrent neural architectures. More recently,\nfine-tuned transformers-based pretrained models (PTMs) have demonstrated\nsuperior performance compared to such models in many natural language\nprocessing tasks. However, the direct use of PTMs in the biomedical domain is\nonly limited to the target documents, ignoring the rich semantic information in\nthe label descriptions. In this paper, we develop an improved label\nattention-based architecture to inject semantic label description into the\nfine-tuning process of PTMs. Results on two public medical datasets show that\nthe proposed fine-tuning scheme outperforms the conventionally fine-tuned PTMs\nand prior state-of-the-art models. Furthermore, we show that fine-tuning with\nthe label attention mechanism is interpretable in the interpretability study.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_B/0/1/0/all/0/1\">Bruce Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shaoxiong Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Partition Filter Network for Joint Entity and Relation Extraction. (arXiv:2108.12202v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.12202","description":"<p>In joint entity and relation extraction, existing work either sequentially\nencode task-specific features, leading to an imbalance in inter-task feature\ninteraction where features extracted later have no direct contact with those\nthat come first. Or they encode entity features and relation features in a\nparallel manner, meaning that feature representation learning for each task is\nlargely independent of each other except for input sharing. We propose a\npartition filter network to model two-way interaction between tasks properly,\nwhere feature encoding is decomposed into two steps: partition and filter. In\nour encoder, we leverage two gates: entity and relation gate, to segment\nneurons into two task partitions and one shared partition. The shared partition\nrepresents inter-task information valuable to both tasks and is evenly shared\nacross two tasks to ensure proper two-way interaction. The task partitions\nrepresent intra-task information and are formed through concerted efforts of\nboth gates, making sure that encoding of task-specific features are dependent\nupon each other. Experiment results on five public datasets show that our model\nperforms significantly better than previous approaches. The source code can be\nfound in https://github.com/Coopercoppers/PFN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zhiheng Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jinlan Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhongyu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ProtoInfoMax: Prototypical Networks with Mutual Information Maximization for Out-of-Domain Detection. (arXiv:2108.12229v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.12229","description":"<p>The ability to detect Out-of-Domain (OOD) inputs has been a critical\nrequirement in many real-world NLP applications since the inclusion of\nunsupported OOD inputs may lead to catastrophic failure of systems. However, it\nremains an empirical question whether current algorithms can tackle such\nproblem reliably in a realistic scenario where zero OOD training data is\navailable. In this study, we propose ProtoInfoMax, a new architecture that\nextends Prototypical Networks to simultaneously process In-Domain (ID) and OOD\nsentences via Mutual Information Maximization (InfoMax) objective. Experimental\nresults show that our proposed method can substantially improve performance up\nto 20% for OOD detection in low resource settings of text classification. We\nalso show that ProtoInfoMax is less prone to typical over-confidence Error of\nNeural Networks, leading to more reliable ID and OOD prediction outcomes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nimah_I/0/1/0/all/0/1\">Iftitahu Ni&#x27;mah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_M/0/1/0/all/0/1\">Meng Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menkovski_V/0/1/0/all/0/1\">Vlado Menkovski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pechenizkiy_M/0/1/0/all/0/1\">Mykola Pechenizkiy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-08-30T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/","syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"High Fidelity Deep Learning-based MRI Reconstruction with Instance-wise Discriminative Feature Matching Loss. (arXiv:2108.12460v1 [eess.IV])","link":"http://arxiv.org/abs/2108.12460","description":"<p>Purpose: To improve reconstruction fidelity of fine structures and textures\nin deep learning (DL) based reconstructions.\n</p>\n<p>Methods: A novel patch-based Unsupervised Feature Loss (UFLoss) is proposed\nand incorporated into the training of DL-based reconstruction frameworks in\norder to preserve perceptual similarity and high-order statistics. The UFLoss\nprovides instance-level discrimination by mapping similar instances to similar\nlow-dimensional feature vectors and is trained without any human annotation. By\nadding an additional loss function on the low-dimensional feature space during\ntraining, the reconstruction frameworks from under-sampled or corrupted data\ncan reproduce more realistic images that are closer to the original with finer\ntextures, sharper edges, and improved overall image quality. The performance of\nthe proposed UFLoss is demonstrated on unrolled networks for accelerated 2D and\n3D knee MRI reconstruction with retrospective under-sampling. Quantitative\nmetrics including NRMSE, SSIM, and our proposed UFLoss were used to evaluate\nthe performance of the proposed method and compare it with others.\n</p>\n<p>Results: In-vivo experiments indicate that adding the UFLoss encourages\nsharper edges and more faithful contrasts compared to traditional and\nlearning-based methods with pure l2 loss. More detailed textures can be seen in\nboth 2D and 3D knee MR images. Quantitative results indicate that\nreconstruction with UFLoss can provide comparable NRMSE and a higher SSIM while\nachieving a much lower UFLoss value.\n</p>\n<p>Conclusion: We present UFLoss, a patch-based unsupervised learned feature\nloss, which allows the training of DL-based reconstruction to obtain more\ndetailed texture, finer features, and sharper edges with higher overall image\nquality under DL-based reconstruction frameworks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_K/0/1/0/all/0/1\">Ke Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tamir_J/0/1/0/all/0/1\">Jonathan I Tamir</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Goyeneche_A/0/1/0/all/0/1\">Alfredo De Goyeneche</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wollner_U/0/1/0/all/0/1\">Uri Wollner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Brada_R/0/1/0/all/0/1\">Rafi Brada</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_S/0/1/0/all/0/1\">Stella Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lustig_M/0/1/0/all/0/1\">Michael Lustig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Inner-Group Relations on Point Clouds. (arXiv:2108.12468v1 [cs.CV])","link":"http://arxiv.org/abs/2108.12468","description":"<p>The prevalence of relation networks in computer vision is in stark contrast\nto underexplored point-based methods. In this paper, we explore the\npossibilities of local relation operators and survey their feasibility. We\npropose a scalable and efficient module, called group relation aggregator. The\nmodule computes a feature of a group based on the aggregation of the features\nof the inner-group points weighted by geometric relations and semantic\nrelations. We adopt this module to design our RPNet. We further verify the\nexpandability of RPNet, in terms of both depth and width, on the tasks of\nclassification and segmentation. Surprisingly, empirical results show that\nwider RPNet fits for classification, while deeper RPNet works better on\nsegmentation. RPNet achieves state-of-the-art for classification and\nsegmentation on challenging benchmarks. We also compare our local aggregator\nwith PointNet++, with around 30% parameters and 50% computation saving.\nFinally, we conduct experiments to reveal the robustness of RPNet with regard\nto rigid transformation and noises.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ran_H/0/1/0/all/0/1\">Haoxi Ran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuo_W/0/1/0/all/0/1\">Wei Zhuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1\">Li Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VisGraphNet: a complex network interpretation of convolutional neural features. (arXiv:2108.12490v1 [cs.CV])","link":"http://arxiv.org/abs/2108.12490","description":"<p>Here we propose and investigate the use of visibility graphs to model the\nfeature map of a neural network. The model, initially devised for studies on\ncomplex networks, is employed here for the classification of texture images.\nThe work is motivated by an alternative viewpoint provided by these graphs over\nthe original data. The performance of the proposed method is verified in the\nclassification of four benchmark databases, namely, KTHTIPS-2b, FMD, UIUC, and\nUMD and in a practical problem, which is the identification of plant species\nusing scanned images of their leaves. Our method was competitive with other\nstate-of-the-art approaches, confirming the potential of techniques used for\ndata analysis in different contexts to give more meaningful interpretation to\nthe use of neural networks in texture classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Florindo_J/0/1/0/all/0/1\">Joao B. Florindo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Young-Sup Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jun_K/0/1/0/all/0/1\">Kyungkoo Jun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeon_G/0/1/0/all/0/1\">Gwanggil Jeon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Albertini_M/0/1/0/all/0/1\">Marcelo K. Albertini</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fractal measures of image local features: an application to texture recognition. (arXiv:2108.12491v1 [cs.CV])","link":"http://arxiv.org/abs/2108.12491","description":"<p>Here we propose a new method for the classification of texture images\ncombining fractal measures (fractal dimension, multifractal spectrum and\nlacunarity) with local binary patterns. More specifically we compute the box\ncounting dimension of the local binary codes thresholded at different levels to\ncompose the feature vector. The proposal is assessed in the classification of\nthree benchmark databases: KTHTIPS-2b, UMD and UIUC as well as in a real-world\nproblem, namely the identification of Brazilian plant species (database\n1200Tex) using scanned images of their leaves. The proposed method demonstrated\nto be competitive with other state-of-the-art solutions reported in the\nliterature. Such results confirmed the potential of combining a powerful local\ncoding description with the multiscale information captured by the fractal\ndimension for texture classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Silva_P/0/1/0/all/0/1\">Pedro M. Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Florindo_J/0/1/0/all/0/1\">Joao B. Florindo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the impact of using X-ray energy response imagery for object detection via Convolutional Neural Networks. (arXiv:2108.12505v1 [cs.CV])","link":"http://arxiv.org/abs/2108.12505","description":"<p>Automatic detection of prohibited items within complex and cluttered X-ray\nsecurity imagery is essential to maintaining transport security, where prior\nwork on automatic prohibited item detection focus primarily on pseudo-colour\n(rgb}) X-ray imagery. In this work we study the impact of variant X-ray\nimagery, i.e., X-ray energy response (high, low}) and effective-z compared to\nrgb, via the use of deep Convolutional Neural Networks (CNN) for the joint\nobject detection and segmentation task posed within X-ray baggage security\nscreening. We evaluate state-of-the-art CNN architectures (Mask R-CNN, YOLACT,\nCARAFE and Cascade Mask R-CNN) to explore the transferability of models trained\nwith such 'raw' variant imagery between the varying X-ray security scanners\nthat exhibits differing imaging geometries, image resolutions and material\ncolour profiles. Overall, we observe maximal detection performance using\nCARAFE, attributable to training using combination of rgb, high, low, and\neffective-z X-ray imagery, obtaining 0.7 mean Average Precision (mAP) for a six\nclass object detection problem. Our results also exhibit a remarkable degree of\ngeneralisation capability in terms of cross-scanner transferability (AP:\n0.835/0.611) for a one class object detection problem by combining rgb, high,\nlow, and effective-z imagery.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhowmik_N/0/1/0/all/0/1\">Neelanjan Bhowmik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaus_Y/0/1/0/all/0/1\">Yona Falinie A. Gaus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Breckon_T/0/1/0/all/0/1\">Toby P. Breckon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated Kidney Segmentation by Mask R-CNN in T2-weighted Magnetic Resonance Imaging. (arXiv:2108.12506v1 [eess.IV])","link":"http://arxiv.org/abs/2108.12506","description":"<p>Despite the recent advances of deep learning algorithms in medical imaging,\nthe automatic segmentation algorithms for kidneys in MRI exams are still\nscarce. Automated segmentation of kidneys in Magnetic Resonance Imaging (MRI)\nexams are important for enabling radiomics and machine learning analysis of\nrenal disease. In this work, we propose to use the popular Mask R-CNN for the\nautomatic segmentation of kidneys in coronal T2-weighted Fast Spin Eco slices\nof 100 MRI exams. We propose the morphological operations as post-processing to\nfurther improve the performance of Mask R-CNN for this task. With 5-fold\ncross-validation data, the proposed Mask R-CNN is trained and validated on 70\nand 10 MRI exams and then evaluated on the remaining 20 exams in each fold. Our\nproposed method achieved a dice score of 0.904 and IoU of 0.822.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Goyal_M/0/1/0/all/0/1\">Manu Goyal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guo_J/0/1/0/all/0/1\">Junyu Guo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hinojosa_L/0/1/0/all/0/1\">Lauren Hinojosa</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hulsey_K/0/1/0/all/0/1\">Keith Hulsey</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pedrosa_I/0/1/0/all/0/1\">Ivan Pedrosa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robustness Disparities in Commercial Face Detection. (arXiv:2108.12508v1 [cs.CY])","link":"http://arxiv.org/abs/2108.12508","description":"<p>Facial detection and analysis systems have been deployed by large companies\nand critiqued by scholars and activists for the past decade. Critiques that\nfocus on system performance analyze disparity of the system's output, i.e., how\nfrequently is a face detected for different Fitzpatrick skin types or perceived\ngenders. However, we focus on the robustness of these system outputs under\nnoisy natural perturbations. We present the first of its kind detailed\nbenchmark of the robustness of three such systems: Amazon Rekognition,\nMicrosoft Azure, and Google Cloud Platform. We use both standard and recently\nreleased academic facial datasets to quantitatively analyze trends in\nrobustness for each. Across all the datasets and systems, we generally find\nthat photos of individuals who are older, masculine presenting, of darker skin\ntype, or have dim lighting are more susceptible to errors than their\ncounterparts in other identities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dooley_S/0/1/0/all/0/1\">Samuel Dooley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1\">Tom Goldstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dickerson_J/0/1/0/all/0/1\">John P. Dickerson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SIGN: Spatial-information Incorporated Generative Network for Generalized Zero-shot Semantic Segmentation. (arXiv:2108.12517v1 [cs.CV])","link":"http://arxiv.org/abs/2108.12517","description":"<p>Unlike conventional zero-shot classification, zero-shot semantic segmentation\npredicts a class label at the pixel level instead of the image level. When\nsolving zero-shot semantic segmentation problems, the need for pixel-level\nprediction with surrounding context motivates us to incorporate spatial\ninformation using positional encoding. We improve standard positional encoding\nby introducing the concept of Relative Positional Encoding, which integrates\nspatial information at the feature level and can handle arbitrary image sizes.\nFurthermore, while self-training is widely used in zero-shot semantic\nsegmentation to generate pseudo-labels, we propose a new\nknowledge-distillation-inspired self-training strategy, namely Annealed\nSelf-Training, which can automatically assign different importance to\npseudo-labels to improve performance. We systematically study the proposed\nRelative Positional Encoding and Annealed Self-Training in a comprehensive\nexperimental evaluation, and our empirical results confirm the effectiveness of\nour method on three benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1\">Jiaxin Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nandi_S/0/1/0/all/0/1\">Soumyaroop Nandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Natarajan_P/0/1/0/all/0/1\">Prem Natarajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abd_Almageed_W/0/1/0/all/0/1\">Wael Abd-Almageed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Combining chest X-rays and EHR data using machine learning to diagnose acute respiratory failure. (arXiv:2108.12530v1 [cs.LG])","link":"http://arxiv.org/abs/2108.12530","description":"<p>When patients develop acute respiratory failure, accurately identifying the\nunderlying etiology is essential for determining the best treatment, but it can\nbe challenging to differentiate between common diagnoses in clinical practice.\nMachine learning models could improve medical diagnosis by augmenting clinical\ndecision making and play a role in the diagnostic evaluation of patients with\nacute respiratory failure. While machine learning models have been developed to\nidentify common findings on chest radiographs (e.g. pneumonia), augmenting\nthese approaches by also analyzing clinically relevant data from the electronic\nhealth record (EHR) could aid in the diagnosis of acute respiratory failure.\nMachine learning models were trained to predict the cause of acute respiratory\nfailure (pneumonia, heart failure, and/or COPD) using chest radiographs and EHR\ndata from patients within an internal cohort using diagnoses based on physician\nchart review. Models were also tested on patients in an external cohort using\ndischarge diagnosis codes. A model combining chest radiographs and EHR data\noutperformed models based on each modality alone for pneumonia and COPD. For\npneumonia, the combined model AUROC was 0.79 (0.78-0.79), image model AUROC was\n0.73 (0.72-0.75), and EHR model AUROC was 0.73 (0.70-0.76); for COPD, combined:\n0.89 (0.83-0.91), image: 0.85 (0.77-0.89), and EHR: 0.80 (0.76-0.84); for heart\nfailure, combined: 0.80 (0.77-0.84), image: 0.77 (0.71-0.81), and EHR: 0.80\n(0.75-0.82). In the external cohort, performance was consistent for heart\nfailure and COPD, but declined slightly for pneumonia. Overall, machine\nlearning models combing chest radiographs and EHR data can accurately\ndifferentiate between common causes of acute respiratory failure. Further work\nis needed to determine whether these models could aid clinicians in the\ndiagnosis of acute respiratory failure in clinical settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jabbour_S/0/1/0/all/0/1\">Sarah Jabbour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fouhey_D/0/1/0/all/0/1\">David Fouhey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kazerooni_E/0/1/0/all/0/1\">Ella Kazerooni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiens_J/0/1/0/all/0/1\">Jenna Wiens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sjoding_M/0/1/0/all/0/1\">Michael W Sjoding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image-to-Graph Convolutional Network for Deformable Shape Reconstruction from a Single Projection Image. (arXiv:2108.12533v1 [eess.IV])","link":"http://arxiv.org/abs/2108.12533","description":"<p>Shape reconstruction of deformable organs from two-dimensional X-ray images\nis a key technology for image-guided intervention. In this paper, we propose an\nimage-to-graph convolutional network (IGCN) for deformable shape reconstruction\nfrom a single-viewpoint projection image. The IGCN learns relationship between\nshape/deformation variability and the deep image features based on a\ndeformation mapping scheme. In experiments targeted to the respiratory motion\nof abdominal organs, we confirmed the proposed framework with a regularized\nloss function can reconstruct liver shapes from a single digitally\nreconstructed radiograph with a mean distance error of 3.6mm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Nakao_M/0/1/0/all/0/1\">M. Nakao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tong_F/0/1/0/all/0/1\">F. Tong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nakamura_M/0/1/0/all/0/1\">M. Nakamura</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Matsuda_T/0/1/0/all/0/1\">T. Matsuda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SeeTheSeams: Localized Detection of Seam Carving based Image Forgery in Satellite Imagery. (arXiv:2108.12534v1 [cs.CV])","link":"http://arxiv.org/abs/2108.12534","description":"<p>Seam carving is a popular technique for content aware image retargeting. It\ncan be used to deliberately manipulate images, for example, change the GPS\nlocations of a building or insert/remove roads in a satellite image. This paper\nproposes a novel approach for detecting and localizing seams in such images.\nWhile there are methods to detect seam carving based manipulations, this is the\nfirst time that robust localization and detection of seam carving forgery is\nmade possible. We also propose a seam localization score (SLS) metric to\nevaluate the effectiveness of localization. The proposed method is evaluated\nextensively on a large collection of images from different sources,\ndemonstrating a high level of detection and localization performance across\nthese datasets. The datasets curated during this work will be released to the\npublic.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gudavalli_C/0/1/0/all/0/1\">Chandrakanth Gudavalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosten_E/0/1/0/all/0/1\">Erik Rosten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nataraj_L/0/1/0/all/0/1\">Lakshmanan Nataraj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandrasekaran_S/0/1/0/all/0/1\">Shivkumar Chandrasekaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manjunath_B/0/1/0/all/0/1\">B. S. Manjunath</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"High performing ensemble of convolutional neural networks for insect pest image detection. (arXiv:2108.12539v1 [cs.CV])","link":"http://arxiv.org/abs/2108.12539","description":"<p>Pest infestation is a major cause of crop damage and lost revenues worldwide.\nAutomatic identification of invasive insects would greatly speedup the\nidentification of pests and expedite their removal. In this paper, we generate\nensembles of CNNs based on different topologies (ResNet50, GoogleNet,\nShuffleNet, MobileNetv2, and DenseNet201) altered by random selection from a\nsimple set of data augmentation methods or optimized with different Adam\nvariants for pest identification. Two new Adam algorithms for deep network\noptimization based on DGrad are proposed that introduce a scaling factor in the\nlearning rate. Sets of the five CNNs that vary in either data augmentation or\nthe type of Adam optimization were trained on both the Deng (SMALL) and the\nlarge IP102 pest data sets. Ensembles were compared and evaluated using three\nperformance indicators. The best performing ensemble, which combined the CNNs\nusing the different augmentation methods and the two new Adam variants proposed\nhere, achieved state of the art on both insect data sets: 95.52% on Deng and\n73.46% on IP102, a score on Deng that competed with human expert\nclassifications. Additional tests were performed on data sets for medical\nimagery classification that further validated the robustness and power of the\nproposed Adam optimization variants. All MATLAB source code is available at\nhttps://github.com/LorisNanni/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nanni_L/0/1/0/all/0/1\">Loris Nanni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manfe_A/0/1/0/all/0/1\">Alessandro Manfe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maguolo_G/0/1/0/all/0/1\">Gianluca Maguolo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lumini_A/0/1/0/all/0/1\">Alessandra Lumini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brahnam_S/0/1/0/all/0/1\">Sheryl Brahnam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Semi-Supervised and Domain-Adaptive Semantic Segmentation with Self-Supervised Depth Estimation. (arXiv:2108.12545v1 [cs.CV])","link":"http://arxiv.org/abs/2108.12545","description":"<p>Training deep networks for semantic segmentation requires large amounts of\nlabeled training data, which presents a major challenge in practice, as\nlabeling segmentation masks is a highly labor-intensive process. To address\nthis issue, we present a framework for semi-supervised and domain-adaptive\nsemantic segmentation, which is enhanced by self-supervised monocular depth\nestimation (SDE) trained only on unlabeled image sequences.\n</p>\n<p>In particular, we utilize SDE as an auxiliary task comprehensively across the\nentire learning framework: First, we automatically select the most useful\nsamples to be annotated for semantic segmentation based on the correlation of\nsample diversity and difficulty between SDE and semantic segmentation. Second,\nwe implement a strong data augmentation by mixing images and labels using the\ngeometry of the scene. Third, we transfer knowledge from features learned\nduring SDE to semantic segmentation by means of transfer and multi-task\nlearning. And fourth, we exploit additional labeled synthetic data with\nCross-Domain DepthMix and Matching Geometry Sampling to align synthetic and\nreal data.\n</p>\n<p>We validate the proposed model on the Cityscapes dataset, where all four\ncontributions demonstrate significant performance gains, and achieve\nstate-of-the-art results for semi-supervised semantic segmentation as well as\nfor semi-supervised domain adaptation. In particular, with only 1/30 of the\nCityscapes labels, our method achieves 92% of the fully-supervised baseline\nperformance and even 97% when exploiting additional data from GTA. The source\ncode is available at\nhttps://github.com/lhoyer/improving_segmentation_with_selfsupervised_depth.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hoyer_L/0/1/0/all/0/1\">Lukas Hoyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1\">Dengxin Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuhua Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"QACE: Asking Questions to Evaluate an Image Caption. (arXiv:2108.12560v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12560","description":"<p>In this paper, we propose QACE, a new metric based on Question Answering for\nCaption Evaluation. QACE generates questions on the evaluated caption and\nchecks its content by asking the questions on either the reference caption or\nthe source image. We first develop QACE-Ref that compares the answers of the\nevaluated caption to its reference, and report competitive results with the\nstate-of-the-art metrics. To go further, we propose QACE-Img, which asks the\nquestions directly on the image, instead of reference. A Visual-QA system is\nnecessary for QACE-Img. Unfortunately, the standard VQA models are framed as a\nclassification among only a few thousand categories. Instead, we propose\nVisual-T5, an abstractive VQA system. The resulting metric, QACE-Img is\nmulti-modal, reference-less, and explainable. Our experiments show that\nQACE-Img compares favorably w.r.t. other reference-less metrics. We will\nrelease the pre-trained models to compute QACE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hwanhee Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scialom_T/0/1/0/all/0/1\">Thomas Scialom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Seunghyun Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dernoncourt_F/0/1/0/all/0/1\">Franck Dernoncourt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_K/0/1/0/all/0/1\">Kyomin Jung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AMMASurv: Asymmetrical Multi-Modal Attention for Accurate Survival Analysis with Whole Slide Images and Gene Expression Data. (arXiv:2108.12565v1 [cs.CV])","link":"http://arxiv.org/abs/2108.12565","description":"<p>The use of multi-modal data such as the combination of whole slide images\n(WSIs) and gene expression data for survival analysis can lead to more accurate\nsurvival predictions. Previous multi-modal survival models are not able to\nefficiently excavate the intrinsic information within each modality. Moreover,\ndespite experimental results show that WSIs provide more effective information\nthan gene expression data, previous methods regard the information from\ndifferent modalities as similarly important so they cannot flexibly utilize the\npotential connection between the modalities. To address the above problems, we\npropose a new asymmetrical multi-modal method, termed as AMMASurv.\nSpecifically, we design an asymmetrical multi-modal attention mechanism (AMMA)\nin Transformer encoder for multi-modal data to enable a more flexible\nmulti-modal information fusion for survival prediction. Different from previous\nworks, AMMASurv can effectively utilize the intrinsic information within every\nmodality and flexibly adapts to the modalities of different importance.\nExtensive experiments are conducted to validate the effectiveness of the\nproposed model. Encouraging results demonstrate the superiority of our method\nover other state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ruoqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Ziwang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haitao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hejun Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An implementation of ROS Autonomous Navigation on Parallax Eddie platform. (arXiv:2108.12571v1 [cs.RO])","link":"http://arxiv.org/abs/2108.12571","description":"<p>This paper presents an implementation of autonomous navigation functionality\nbased on Robot Operating System (ROS) on a wheeled differential drive mobile\nplatform called Eddie robot. ROS is a framework that contains many reusable\nsoftware stacks as well as visualization and debugging tools that provides an\nideal environment for any robotic project development. The main contribution of\nthis paper is the description of the customized hardware and software system\nsetup of Eddie robot to work with an autonomous navigation system in ROS called\nNavigation Stack and to implement one application use case for autonomous\nnavigation. For this paper, photo taking is chosen to demonstrate a use case of\nthe mobile robot.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Anas_H/0/1/0/all/0/1\">Hafiq Anas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ong_W/0/1/0/all/0/1\">Wee Hong Ong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Goal-driven text descriptions for images. (arXiv:2108.12575v1 [cs.CV])","link":"http://arxiv.org/abs/2108.12575","description":"<p>A big part of achieving Artificial General Intelligence(AGI) is to build a\nmachine that can see and listen like humans. Much work has focused on designing\nmodels for image classification, video classification, object detection, pose\nestimation, speech recognition, etc., and has achieved significant progress in\nrecent years thanks to deep learning. However, understanding the world is not\nenough. An AI agent also needs to know how to talk, especially how to\ncommunicate with a human. While perception (vision, for example) is more common\nacross animal species, the use of complicated language is unique to humans and\nis one of the most important aspects of intelligence.\n</p>\n<p>In this thesis, we focus on generating textual output given visual input. In\nChapter 3, we focus on generating the referring expression, a text description\nfor an object in the image so that a receiver can infer which object is being\ndescribed. We use a comprehension machine to directly guide the generated\nreferring expressions to be more discriminative. In Chapter 4, we introduce a\nmethod that encourages discriminability in image caption generation. We show\nthat more discriminative captioning models generate more descriptive captions.\nIn Chapter 5, we study how training objectives and sampling methods affect the\nmodels' ability to generate diverse captions. We find that a popular captioning\ntraining strategy will be detrimental to the diversity of generated captions.\nIn Chapter 6, we propose a model that can control the length of generated\ncaptions. By changing the desired length, one can influence the style and\ndescriptiveness of the captions. Finally, in Chapter 7, we rank/generate\ninformative image tags according to their information utility. The proposed\nmethod better matches what humans think are the most important tags for the\nimages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_R/0/1/0/all/0/1\">Ruotian Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Significance of Question Encoder Sequence Model in the Out-of-Distribution Performance in Visual Question Answering. (arXiv:2108.12585v1 [cs.CV])","link":"http://arxiv.org/abs/2108.12585","description":"<p>Generalizing beyond the experiences has a significant role in developing\npractical AI systems. It has been shown that current Visual Question Answering\n(VQA) models are over-dependent on the language-priors (spurious correlations\nbetween question-types and their most frequent answers) from the train set and\npose poor performance on Out-of-Distribution (OOD) test sets. This conduct\nlimits their generalizability and restricts them from being utilized in\nreal-world situations. This paper shows that the sequence model architecture\nused in the question-encoder has a significant role in the generalizability of\nVQA models. To demonstrate this, we performed a detailed analysis of various\nexisting RNN-based and Transformer-based question-encoders, and along, we\nproposed a novel Graph attention network (GAT)-based question-encoder. Our\nstudy found that a better choice of sequence model in the question-encoder\nimproves the generalizability of VQA models even without using any additional\nrelatively complex bias-mitigation approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+KV_G/0/1/0/all/0/1\">Gouthaman KV</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mittal_A/0/1/0/all/0/1\">Anurag Mittal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Threshold: Pruning Tool for Densely Connected Convolutional Networks. (arXiv:2108.12604v1 [cs.CV])","link":"http://arxiv.org/abs/2108.12604","description":"<p>Deep neural networks have made significant progress in the field of computer\nvision. Recent studies have shown that depth, width and shortcut connections of\nneural network architectures play a crucial role in their performance. One of\nthe most advanced neural network architectures, DenseNet, has achieved\nexcellent convergence rates through dense connections. However, it still has\nobvious shortcomings in the usage of amount of memory. In this paper, we\nintroduce a new type of pruning tool, threshold, which refers to the principle\nof the threshold voltage in MOSFET. This work employs this method to connect\nblocks of different depths in different ways to reduce the usage of memory. It\nis denoted as ThresholdNet. We compare ThresholdNet with other different\nnetworks for FLOPs and memory usage, and the experiments show that ThresholdNet\nis 70% less memory than that of the original DenseNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ju_R/0/1/0/all/0/1\">Rui-Yang Ju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Ting-Yu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiang_J/0/1/0/all/0/1\">Jen-Shiun Chiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stagewise Unsupervised Domain Adaptation with Adversarial Self-Training for Road Segmentation of Remote Sensing Images. (arXiv:2108.12611v1 [cs.CV])","link":"http://arxiv.org/abs/2108.12611","description":"<p>Road segmentation from remote sensing images is a challenging task with wide\nranges of application potentials. Deep neural networks have advanced this field\nby leveraging the power of large-scale labeled data, which, however, are\nextremely expensive and time-consuming to acquire. One solution is to use cheap\navailable data to train a model and deploy it to directly process the data from\na specific application domain. Nevertheless, the well-known domain shift (DS)\nissue prevents the trained model from generalizing well on the target domain.\nIn this paper, we propose a novel stagewise domain adaptation model called\nRoadDA to address the DS issue in this field. In the first stage, RoadDA adapts\nthe target domain features to align with the source ones via generative\nadversarial networks (GAN) based inter-domain adaptation. Specifically, a\nfeature pyramid fusion module is devised to avoid information loss of long and\nthin roads and learn discriminative and robust features. Besides, to address\nthe intra-domain discrepancy in the target domain, in the second stage, we\npropose an adversarial self-training method. We generate the pseudo labels of\ntarget domain using the trained generator and divide it to labeled easy split\nand unlabeled hard split based on the road confidence scores. The features of\nhard split are adapted to align with the easy ones using adversarial learning\nand the intra-domain adaptation process is repeated to progressively improve\nthe segmentation performance. Experiment results on two benchmarks demonstrate\nthat RoadDA can efficiently reduce the domain gap and outperforms\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lefei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_M/0/1/0/all/0/1\">Meng Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncertainty-Aware Model Adaptation for Unsupervised Cross-Domain Object Detection. (arXiv:2108.12612v1 [cs.CV])","link":"http://arxiv.org/abs/2108.12612","description":"<p>This work tackles the unsupervised cross-domain object detection problem\nwhich aims to generalize a pre-trained object detector to a new target domain\nwithout labels. We propose an uncertainty-aware model adaptation method, which\nis based on two motivations: 1) the estimation and exploitation of model\nuncertainty in a new domain is critical for reliable domain adaptation; and 2)\nthe joint alignment of distributions for inputs (feature alignment) and outputs\n(self-training) is needed. To this end, we compose a Bayesian CNN-based\nframework for uncertainty estimation in object detection, and propose an\nalgorithm for generation of uncertainty-aware pseudo-labels. We also devise a\nscheme for joint feature alignment and self-training of the object detection\nmodel with uncertainty-aware pseudo-labels. Experiments on multiple\ncross-domain object detection benchmarks show that our proposed method achieves\nstate-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_M/0/1/0/all/0/1\">Minjie Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_M/0/1/0/all/0/1\">Minyi Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_X/0/1/0/all/0/1\">Xionghu Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AP-10K: A Benchmark for Animal Pose Estimation in the Wild. (arXiv:2108.12617v1 [cs.CV])","link":"http://arxiv.org/abs/2108.12617","description":"<p>Accurate animal pose estimation is an essential step towards understanding\nanimal behavior, and can potentially benefit many downstream applications, such\nas wildlife conservation. Previous works only focus on specific animals while\nignoring the diversity of animal species, limiting the generalization ability.\nIn this paper, we propose AP-10K, the first large-scale benchmark for general\nanimal pose estimation, to facilitate the research in animal pose estimation.\nAP-10K consists of 10,015 images collected and filtered from 23 animal families\nand 60 species following the taxonomic rank and high-quality keypoint\nannotations labeled and checked manually. Based on AP-10K, we benchmark\nrepresentative pose estimation models on the following three tracks: (1)\nsupervised learning for animal pose estimation, (2) cross-domain transfer\nlearning from human pose estimation to animal pose estimation, and (3) intra-\nand inter-family domain generalization for unseen animals. The experimental\nresults provide sound empirical evidence on the superiority of learning from\ndiverse animals species in terms of both accuracy and generalization ability.\nIt opens new directions for facilitating future research in animal pose\nestimation. AP-10k is publicly available at\nhttps://github.com/AlexTheBad/AP10K.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yufei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_Z/0/1/0/all/0/1\">Ziyu Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GroupFormer: Group Activity Recognition with Clustered Spatial-Temporal Transformer. (arXiv:2108.12630v1 [cs.CV])","link":"http://arxiv.org/abs/2108.12630","description":"<p>Group activity recognition is a crucial yet challenging problem, whose core\nlies in fully exploring spatial-temporal interactions among individuals and\ngenerating reasonable group representations. However, previous methods either\nmodel spatial and temporal information separately, or directly aggregate\nindividual features to form group features. To address these issues, we propose\na novel group activity recognition network termed GroupFormer. It captures\nspatial-temporal contextual information jointly to augment the individual and\ngroup representations effectively with a clustered spatial-temporal\ntransformer. Specifically, our GroupFormer has three appealing advantages: (1)\nA tailor-modified Transformer, Clustered Spatial-Temporal Transformer, is\nproposed to enhance the individual representation and group representation. (2)\nIt models the spatial and temporal dependencies integrally and utilizes\ndecoders to build the bridge between the spatial and temporal information. (3)\nA clustered attention mechanism is utilized to dynamically divide individuals\ninto multiple clusters for better learning activity-aware semantic\nrepresentations. Moreover, experimental results show that the proposed\nframework outperforms state-of-the-art methods on the Volleyball dataset and\nCollective Activity dataset. Code is available at\nhttps://github.com/xueyee/GroupFormer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuaicheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Q/0/1/0/all/0/1\">Qianggang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lingbo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kunlin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shinan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1\">Jun Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_S/0/1/0/all/0/1\">Shuai Yi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised Neural Networks for Spectral Snapshot Compressive Imaging. (arXiv:2108.12654v1 [eess.IV])","link":"http://arxiv.org/abs/2108.12654","description":"<p>We consider using {\\bf\\em untrained neural networks} to solve the\nreconstruction problem of snapshot compressive imaging (SCI), which uses a\ntwo-dimensional (2D) detector to capture a high-dimensional (usually 3D)\ndata-cube in a compressed manner. Various SCI systems have been built in recent\nyears to capture data such as high-speed videos, hyperspectral images, and the\nstate-of-the-art reconstruction is obtained by the deep neural networks.\nHowever, most of these networks are trained in an end-to-end manner by a large\namount of corpus with sometimes simulated ground truth, measurement pairs. In\nthis paper, inspired by the untrained neural networks such as deep image priors\n(DIP) and deep decoders, we develop a framework by integrating DIP into the\nplug-and-play regime, leading to a self-supervised network for spectral SCI\nreconstruction. Extensive synthetic and real data results show that the\nproposed algorithm without training is capable of achieving competitive results\nto the training based networks. Furthermore, by integrating the proposed method\nwith a pre-trained deep denoising prior, we have achieved state-of-the-art\nresults. {Our code is available at\n\\url{https://github.com/mengziyi64/CASSI-Self-Supervised}.}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Meng_Z/0/1/0/all/0/1\">Ziyi Meng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_Z/0/1/0/all/0/1\">Zhenming Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_K/0/1/0/all/0/1\">Kun Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yuan_X/0/1/0/all/0/1\">Xin Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DenseLiDAR: A Real-Time Pseudo Dense Depth Guided Depth Completion Network. (arXiv:2108.12655v1 [cs.CV])","link":"http://arxiv.org/abs/2108.12655","description":"<p>Depth Completion can produce a dense depth map from a sparse input and\nprovide a more complete 3D description of the environment. Despite great\nprogress made in depth completion, the sparsity of the input and low density of\nthe ground truth still make this problem challenging. In this work, we propose\nDenseLiDAR, a novel real-time pseudo-depth guided depth completion neural\nnetwork. We exploit dense pseudo-depth map obtained from simple morphological\noperations to guide the network in three aspects: (1) Constructing a residual\nstructure for the output; (2) Rectifying the sparse input data; (3) Providing\ndense structural loss for training the network. Thanks to these novel designs,\nhigher performance of the output could be achieved. In addition, two new\nmetrics for better evaluating the quality of the predicted depth map are also\npresented. Extensive experiments on KITTI depth completion benchmark suggest\nthat our model is able to achieve the state-of-the-art performance at the\nhighest frame rate of 50Hz. The predicted dense depth is further evaluated by\nseveral downstream robotic perception or positioning tasks. For the task of 3D\nobject detection, 3~5 percent performance gains on small objects categories are\nachieved on KITTI 3D object detection dataset. For RGB-D SLAM, higher accuracy\non vehicle's trajectory is also obtained in KITTI Odometry dataset. These\npromising results not only verify the high quality of our depth prediction, but\nalso demonstrate the potential of improving the related downstream tasks by\nusing depth completion results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jiaqi Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_Z/0/1/0/all/0/1\">Zhiyu Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1\">Yuwen Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lingxuan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DKM: Differentiable K-Means Clustering Layer for Neural Network Compression. (arXiv:2108.12659v1 [cs.LG])","link":"http://arxiv.org/abs/2108.12659","description":"<p>Deep neural network (DNN) model compression for efficient on-device inference\nis becoming increasingly important to reduce memory requirements and keep user\ndata on-device. To this end, we propose a novel differentiable k-means\nclustering layer (DKM) and its application to train-time weight\nclustering-based DNN model compression. DKM casts k-means clustering as an\nattention problem and enables joint optimization of the parameters and\nclustering centroids. Unlike prior works that rely on additional regularizers\nand parameters, DKM-based compression keeps the original loss function and\nmodel architecture fixed. We evaluated DKM-based compression on various DNN\nmodels for computer vision and natural language processing (NLP) tasks. Our\nresults demonstrate that DMK delivers superior compression and accuracy\ntrade-off on ImageNet1k and GLUE benchmarks. For example, DKM-based compression\ncan offer 74.5% top-1 ImageNet1k accuracy on ResNet50 DNN model with 3.3MB\nmodel size (29.4x model compression factor). For MobileNet-v1, which is a\nchallenging DNN to compress, DKM delivers 62.8% top-1 ImageNet1k accuracy with\n0.74 MB model size (22.4x model compression factor). This result is 6.8% higher\ntop-1 accuracy and 33% relatively smaller model size than the current\nstate-of-the-art DNN compression algorithms. Additionally, DKM enables\ncompression of DistilBERT model by 11.8x with minimal (1.1%) accuracy loss on\nGLUE NLP benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cho_M/0/1/0/all/0/1\">Minsik Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vahid_K/0/1/0/all/0/1\">Keivan A. Vahid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adya_S/0/1/0/all/0/1\">Saurabh Adya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rastegari_M/0/1/0/all/0/1\">Mohammad Rastegari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Track Objects from Unlabeled Videos. (arXiv:2108.12711v1 [cs.CV])","link":"http://arxiv.org/abs/2108.12711","description":"<p>In this paper, we propose to learn an Unsupervised Single Object Tracker\n(USOT) from scratch. We identify that three major challenges, i.e., moving\nobject discovery, rich temporal variation exploitation, and online update, are\nthe central causes of the performance bottleneck of existing unsupervised\ntrackers. To narrow the gap between unsupervised trackers and supervised\ncounterparts, we propose an effective unsupervised learning approach composed\nof three stages. First, we sample sequentially moving objects with unsupervised\noptical flow and dynamic programming, instead of random cropping. Second, we\ntrain a naive Siamese tracker from scratch using single-frame pairs. Third, we\ncontinue training the tracker with a novel cycle memory learning scheme, which\nis conducted in longer temporal spans and also enables our tracker to update\nonline. Extensive experiments show that the proposed USOT learned from\nunlabeled videos performs well over the state-of-the-art unsupervised trackers\nby large margins, and on par with recent supervised deep trackers. Code is\navailable at https://github.com/VISION-SJTU/USOT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Jilai Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Chao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Houwen Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaokang Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepFake Detection with Inconsistent Head Poses: Reproducibility and Analysis. (arXiv:2108.12715v1 [cs.CV])","link":"http://arxiv.org/abs/2108.12715","description":"<p>Applications of deep learning to synthetic media generation allow the\ncreation of convincing forgeries, called DeepFakes, with limited technical\nexpertise. DeepFake detection is an increasingly active research area. In this\npaper, we analyze an existing DeepFake detection technique based on head pose\nestimation, which can be applied when fake images are generated with an\nautoencoder-based face swap. Existing literature suggests that this method is\nan effective DeepFake detector, and its motivating principles are attractively\nsimple. With an eye towards using these principles to develop new DeepFake\ndetectors, we conduct a reproducibility study of the existing method. We\nconclude that its merits are dramatically overstated, despite its celebrated\nstatus. By investigating this discrepancy we uncover a number of important and\ngeneralizable insights related to facial landmark detection, identity-agnostic\nhead pose estimation, and algorithmic bias in DeepFake detectors. Our results\ncorrect the current literature's perception of state of the art performance for\nDeepFake detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lutz_K/0/1/0/all/0/1\">Kevin Lutz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bassett_R/0/1/0/all/0/1\">Robert Bassett</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Dual Adversarial Calibration Framework for Automatic Fetal Brain Biometry. (arXiv:2108.12719v1 [eess.IV])","link":"http://arxiv.org/abs/2108.12719","description":"<p>This paper presents a novel approach to automatic fetal brain biometry\nmotivated by needs in low- and medium- income countries. Specifically, we\nleverage high-end (HE) ultrasound images to build a biometry solution for\nlow-cost (LC) point-of-care ultrasound images. We propose a novel unsupervised\ndomain adaptation approach to train deep models to be invariant to significant\nimage distribution shift between the image types. Our proposed method, which\nemploys a Dual Adversarial Calibration (DAC) framework, consists of adversarial\npathways which enforce model invariance to; i) adversarial perturbations in the\nfeature space derived from LC images, and ii) appearance domain discrepancy.\nOur Dual Adversarial Calibration method estimates transcerebellar diameter and\nhead circumference on images from low-cost ultrasound devices with a mean\nabsolute error (MAE) of 2.43mm and 1.65mm, compared with 7.28 mm and 5.65 mm\nrespectively for SOTA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Gao_Y/0/1/0/all/0/1\">Yuan Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_L/0/1/0/all/0/1\">Lok Hin Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Droste_R/0/1/0/all/0/1\">Richard Droste</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Craik_R/0/1/0/all/0/1\">Rachel Craik</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Beriwal_S/0/1/0/all/0/1\">Sridevi Beriwal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Papageorghiou_A/0/1/0/all/0/1\">Aris Papageorghiou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Noble_A/0/1/0/all/0/1\">Alison Noble</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Variational voxelwise rs-fMRI representation learning: Evaluation of sex, age, and neuropsychiatric signatures. (arXiv:2108.12756v1 [cs.CV])","link":"http://arxiv.org/abs/2108.12756","description":"<p>We propose to apply non-linear representation learning to voxelwise rs-fMRI\ndata. Learning the non-linear representations is done using a variational\nautoencoder (VAE). The VAE is trained on voxelwise rs-fMRI data and performs\nnon-linear dimensionality reduction that retains meaningful information. The\nretention of information in the model's representations is evaluated using\ndownstream age regression and sex classification tasks. The results on these\ntasks are highly encouraging and a linear regressor trained with the\nrepresentations of our unsupervised model performs almost as well as a\nsupervised neural network, trained specifically for age regression on the same\ndataset. The model is also evaluated with a schizophrenia diagnosis prediction\ntask, to assess its feasibility as a dimensionality reduction method for\nneuropsychiatric datasets. These results highlight the potential for\npre-training on a larger set of individuals who do not have mental illness, to\nimprove the downstream neuropsychiatric task results. The pre-trained model is\nfine-tuned for a variable number of epochs on a schizophrenia dataset and we\nfind that fine-tuning for 1 epoch yields the best results. This work therefore\nnot only opens up non-linear dimensionality reduction for voxelwise rs-fMRI\ndata but also shows that pre-training a deep learning model on voxelwise\nrs-fMRI datasets greatly increases performance even on smaller datasets. It\nalso opens up the ability to look at the distribution of rs-fMRI time series in\nthe latent space of the VAE for heterogeneous neuropsychiatric disorders like\nschizophrenia in future work. This can be complemented with the generative\naspect of the model that allows us to reconstruct points from the model's\nlatent space back into brain space and obtain an improved understanding of the\nrelation that the VAE learns between subjects, timepoints, and a subject's\ncharacteristics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Geenjaar_E/0/1/0/all/0/1\">Eloy Geenjaar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+White_T/0/1/0/all/0/1\">Tonya White</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calhoun_V/0/1/0/all/0/1\">Vince Calhoun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Calibrating Class Activation Maps for Long-Tailed Visual Recognition. (arXiv:2108.12757v1 [cs.CV])","link":"http://arxiv.org/abs/2108.12757","description":"<p>Real-world visual recognition problems often exhibit long-tailed\ndistributions, where the amount of data for learning in different categories\nshows significant imbalance. Standard classification models learned on such\ndata distribution often make biased predictions towards the head classes while\ngeneralizing poorly to the tail classes. In this paper, we present two\neffective modifications of CNNs to improve network learning from long-tailed\ndistribution. First, we present a Class Activation Map Calibration (CAMC)\nmodule to improve the learning and prediction of network classifiers, by\nenforcing network prediction based on important image regions. The proposed\nCAMC module highlights the correlated image regions across data and reinforces\nthe representations in these areas to obtain a better global representation for\nclassification. Furthermore, we investigate the use of normalized classifiers\nfor representation learning in long-tailed problems. Our empirical study\ndemonstrates that by simply scaling the outputs of the classifier with an\nappropriate scalar, we can effectively improve the classification accuracy on\ntail classes without losing the accuracy of head classes. We conduct extensive\nexperiments to validate the effectiveness of our design and we set new\nstate-of-the-art performance on five benchmarks, including ImageNet-LT,\nPlaces-LT, iNaturalist 2018, CIFAR10-LT, and CIFAR100-LT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1\">Guosheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_L/0/1/0/all/0/1\">Lvlong Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1\">Henghui Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qingyao Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CrossedWires: A Dataset of Syntactically Equivalent but Semantically Disparate Deep Learning Models. (arXiv:2108.12768v1 [cs.LG])","link":"http://arxiv.org/abs/2108.12768","description":"<p>The training of neural networks using different deep learning frameworks may\nlead to drastically differing accuracy levels despite the use of the same\nneural network architecture and identical training hyperparameters such as\nlearning rate and choice of optimization algorithms. Currently, our ability to\nbuild standardized deep learning models is limited by the availability of a\nsuite of neural network and corresponding training hyperparameter benchmarks\nthat expose differences between existing deep learning frameworks. In this\npaper, we present a living dataset of models and hyperparameters, called\nCrossedWires, that exposes semantic differences between two popular deep\nlearning frameworks: PyTorch and Tensorflow. The CrossedWires dataset currently\nconsists of models trained on CIFAR10 images using three different computer\nvision architectures: VGG16, ResNet50 and DenseNet121 across a large\nhyperparameter space. Using hyperparameter optimization, each of the three\nmodels was trained on 400 sets of hyperparameters suggested by the HyperSpace\nsearch algorithm. The CrossedWires dataset includes PyTorch and Tensforflow\nmodels with test accuracies as different as 0.681 on syntactically equivalent\nmodels and identical hyperparameter choices. The 340 GB dataset and benchmarks\npresented here include the performance statistics, training curves, and model\nweights for all 1200 hyperparameter choices, resulting in 2400 total models.\nThe CrossedWires dataset provides an opportunity to study semantic differences\nbetween syntactically equivalent models across popular deep learning\nframeworks. Further, the insights obtained from this study can enable the\ndevelopment of algorithms and tools that improve reliability and\nreproducibility of deep learning frameworks. The dataset is freely available at\nhttps://github.com/maxzvyagin/crossedwires through a Python API and direct\ndownload link.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zvyagin_M/0/1/0/all/0/1\">Max Zvyagin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brettin_T/0/1/0/all/0/1\">Thomas Brettin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanathan_A/0/1/0/all/0/1\">Arvind Ramanathan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1\">Sumit Kumar Jha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attentive Rotation Invariant Convolution for Point Cloud-based Large Scale Place Recognition. (arXiv:2108.12790v1 [cs.CV])","link":"http://arxiv.org/abs/2108.12790","description":"<p>Autonomous Driving and Simultaneous Localization and Mapping(SLAM) are\nbecoming increasingly important in real world, where point cloud-based large\nscale place recognition is the spike of them. Previous place recognition\nmethods have achieved acceptable performances by regarding the task as a point\ncloud retrieval problem. However, all of them are suffered from a common\ndefect: they can't handle the situation when the point clouds are rotated,\nwhich is common, e.g, when viewpoints or motorcycle types are changed. To\ntackle this issue, we propose an Attentive Rotation Invariant Convolution\n(ARIConv) in this paper. The ARIConv adopts three kind of Rotation Invariant\nFeatures (RIFs): Spherical Signals (SS), Individual-Local Rotation Invariant\nFeatures (ILRIF) and Group-Local Rotation Invariant features (GLRIF) in its\nstructure to learn rotation invariant convolutional kernels, which are robust\nfor learning rotation invariant point cloud features. What's more, to highlight\npivotal RIFs, we inject an attentive module in ARIConv to give different RIFs\ndifferent importance when learning kernels. Finally, utilizing ARIConv, we\nbuild a DenseNet-like network architecture to learn rotation-insensitive global\ndescriptors used for retrieving. We experimentally demonstrate that our model\ncan achieve state-of-the-art performance on large scale place recognition task\nwhen the point cloud scans are rotated and can achieve comparable results with\nmost of existing methods on the original non-rotated datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1\">Zhaoxin Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1\">Zhenbo Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenping Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hongyan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jun He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1\">Xiaoyong Du</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Airplane Type Identification Based on Mask RCNN and Drone Images. (arXiv:2108.12811v1 [cs.CV])","link":"http://arxiv.org/abs/2108.12811","description":"<p>For dealing with traffic bottlenecks at airports, aircraft object detection\nis insufficient. Every airport generally has a variety of planes with various\nphysical and technological requirements as well as diverse service\nrequirements. Detecting the presence of new planes will not address all traffic\ncongestion issues. Identifying the type of airplane, on the other hand, will\nentirely fix the problem because it will offer important information about the\nplane's technical specifications (i.e., the time it needs to be served and its\nappropriate place in the airport). Several studies have provided various\ncontributions to address airport traffic jams; however, their ultimate goal was\nto determine the existence of airplane objects. This paper provides a practical\napproach to identify the type of airplane in airports depending on the results\nprovided by the airplane detection process using mask region convolution neural\nnetwork. The key feature employed to identify the type of airplane is the\nsurface area calculated based on the results of airplane detection. The surface\narea is used to assess the estimated cabin length which is considered as an\nadditional key feature for identifying the airplane type. The length of any\ndetected plane may be calculated by measuring the distance between the detected\nplane's two furthest points. The suggested approach's performance is assessed\nusing average accuracies and a confusion matrix. The findings show that this\nmethod is dependable. This method will greatly aid in the management of airport\ntraffic congestion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alshaibani_W/0/1/0/all/0/1\">W.T Alshaibani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Helvaci_M/0/1/0/all/0/1\">Mustafa Helvaci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shayea_I/0/1/0/all/0/1\">Ibraheem Shayea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saad_S/0/1/0/all/0/1\">Sawsan A. Saad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azizan_A/0/1/0/all/0/1\">Azizul Azizan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yakub_F/0/1/0/all/0/1\">Fitri Yakub</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Airplane Detection Based on Mask Region Convolution Neural Network. (arXiv:2108.12817v1 [cs.CV])","link":"http://arxiv.org/abs/2108.12817","description":"<p>Addressing airport traffic jams is one of the most crucial and challenging\ntasks in the remote sensing field, especially for the busiest airports. Several\nsolutions have been employed to address this problem depending on the airplane\ndetection process. The most effective solutions are through the use of\nsatellite images with deep learning techniques. Such solutions, however, are\nsignificantly costly and require satellites and modern complicated technology\nwhich may not be available in most countries worldwide. This paper provides a\nuniversal, low cost and fast solution for airplane detection in airports. This\npaper recommends the use of drones instead of satellites to feed the system\nwith drone images using a proposed deep learning model. Drone images are\nemployed as the dataset to train and evaluate a mask region convolution neural\nnetwork (RCNN) model. The Mask RCNN model applies faster RCNN as its base\nconfiguration with critical modifications on its head neural network\nconstructions. The model detects whether or not an airplane is present and\nincludes mask estimations to approximate surface area and length, which will\nhelp future works identify the airplane type. This solution can be easily\nimplemented globally as it is a low-cost and fast solution for airplane\ndetection at airports. The evaluation process reveals promising results\naccording to Microsoft Common Objects in Context (COCO) metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alshaibani_W/0/1/0/all/0/1\">W.T. Alshaibani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Helvaci_M/0/1/0/all/0/1\">Mustafa Helvaci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shayea_I/0/1/0/all/0/1\">Ibraheem Shayea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohamad_H/0/1/0/all/0/1\">Hafizal Mohamad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MEDIC: A Multi-Task Learning Dataset for Disaster Image Classification. (arXiv:2108.12828v1 [cs.CV])","link":"http://arxiv.org/abs/2108.12828","description":"<p>Recent research in disaster informatics demonstrates a practical and\nimportant use case of artificial intelligence to save human lives and\nsufferings during post-natural disasters based on social media contents (text\nand images). While notable progress has been made using texts, research on\nexploiting the images remains relatively under-explored. To advance the\nimage-based approach, we propose MEDIC (available at:\nhttps://crisisnlp.qcri.org/medic/index.html), which is the largest social media\nimage classification dataset for humanitarian response consisting of 71,198\nimages to address four different tasks in a multi-task learning setup. This is\nthe first dataset of its kind: social media image, disaster response, and\nmulti-task learning research. An important property of this dataset is its high\npotential to contribute research on multi-task learning, which recently\nreceives much interest from the machine learning community and has shown\nremarkable results in terms of memory, inference speed, performance, and\ngeneralization capability. Therefore, the proposed dataset is an important\nresource for advancing image-based disaster management and multi-task machine\nlearning research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alam_F/0/1/0/all/0/1\">Firoj Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alam_T/0/1/0/all/0/1\">Tanvirul Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1\">Md. Arid Hasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasnat_A/0/1/0/all/0/1\">Abul Hasnat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Imran_M/0/1/0/all/0/1\">Muhammad Imran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ofli_F/0/1/0/all/0/1\">Ferda Ofli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Deep Image Prior for Denoising. (arXiv:2108.12841v1 [eess.IV])","link":"http://arxiv.org/abs/2108.12841","description":"<p>Deep image prior (DIP) serves as a good inductive bias for diverse inverse\nproblems. Among them, denoising is known to be particularly challenging for the\nDIP due to noise fitting with the requirement of an early stopping. To address\nthe issue, we first analyze the DIP by the notion of effective degrees of\nfreedom (DF) to monitor the optimization progress and propose a principled\nstopping criterion before fitting to noise without access of a paired ground\ntruth image for Gaussian noise. We also propose the `stochastic temporal\nensemble (STE)' method for incorporating techniques to further improve DIP's\nperformance for denoising. We additionally extend our method to Poisson noise.\nOur empirical validations show that given a single noisy image, our method\ndenoises the image while preserving rich textual details. Further, our approach\noutperforms prior arts in LPIPS by large margins with comparable PSNR and SSIM\non seven different datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Jo_Y/0/1/0/all/0/1\">Yeonsik Jo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chun_S/0/1/0/all/0/1\">Se Young Chun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Choi_J/0/1/0/all/0/1\">Jonghyun Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Decentralized Autofocusing System with Hierarchical Agents. (arXiv:2108.12842v1 [cs.CV])","link":"http://arxiv.org/abs/2108.12842","description":"<p>State-of-the-art object detection models are frequently trained offline using\navailable datasets, such as ImageNet: large and overly diverse data that are\nunbalanced and hard to cluster semantically. This kind of training drops the\nobject detection performance should the change in illumination, in the\nenvironmental conditions (e.g., rain), or in the lens positioning (out-of-focus\nblur) occur. We propose a decentralized hierarchical multi-agent deep\nreinforcement learning approach for intelligently controlling the camera and\nthe lens focusing settings, leading to significant improvement to the capacity\nof the popular detection models (YOLO, Fast R-CNN, and Retina are considered).\nThe algorithm relies on the latent representation of the camera's stream and,\nthus, it is the first method to allow a completely no-reference tuning of the\ncamera, where the system trains itself to auto-focus itself.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Anikina_A/0/1/0/all/0/1\">Anna Anikina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rogov_O/0/1/0/all/0/1\">Oleg Y. Rogov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dylov_D/0/1/0/all/0/1\">Dmitry V. Dylov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Flow-Guided Video Inpainting with Scene Templates. (arXiv:2108.12845v1 [cs.CV])","link":"http://arxiv.org/abs/2108.12845","description":"<p>We consider the problem of filling in missing spatio-temporal regions of a\nvideo. We provide a novel flow-based solution by introducing a generative model\nof images in relation to the scene (without missing regions) and mappings from\nthe scene to images. We use the model to jointly infer the scene template, a 2D\nrepresentation of the scene, and the mappings. This ensures consistency of the\nframe-to-frame flows generated to the underlying scene, reducing geometric\ndistortions in flow based inpainting. The template is mapped to the missing\nregions in the video by a new L2-L1 interpolation scheme, creating crisp\ninpaintings and reducing common blur and distortion artifacts. We show on two\nbenchmark datasets that our approach out-performs state-of-the-art\nquantitatively and in user studies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lao_D/0/1/0/all/0/1\">Dong Lao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_P/0/1/0/all/0/1\">Peihao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wonka_P/0/1/0/all/0/1\">Peter Wonka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sundaramoorthi_G/0/1/0/all/0/1\">Ganesh Sundaramoorthi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Non-Parametric Neural Style Transfer. (arXiv:2108.12847v1 [cs.CV])","link":"http://arxiv.org/abs/2108.12847","description":"<p>It seems easy to imagine a photograph of the Eiffel Tower painted in the\nstyle of Vincent van Gogh's 'The Starry Night', but upon introspection it is\ndifficult to precisely define what this would entail. What visual elements must\nan image contain to represent the 'content' of the Eiffel Tower? What visual\nelements of 'The Starry Night' are caused by van Gogh's 'style' rather than his\ndecision to depict a village under the night sky? Precisely defining 'content'\nand 'style' is a central challenge of designing algorithms for artistic style\ntransfer, algorithms which can recreate photographs using an artwork's style.\nMy efforts defining these terms, and designing style transfer algorithms\nthemselves, are the focus of this thesis. I will begin by proposing novel\ndefinitions of style and content based on optimal transport and\nself-similarity, and demonstrating how a style transfer algorithm based on\nthese definitions generates outputs with improved visual quality. Then I will\ndescribe how the traditional texture-based definition of style can be expanded\nto include elements of geometry and proportion by jointly optimizing a\nkeypoint-guided deformation field alongside the stylized output's pixels.\nFinally I will describe a framework inspired by both modern neural style\ntransfer algorithms and traditional patch-based synthesis approaches which is\nfast, general, and offers state-of-the-art visual quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kolkin_N/0/1/0/all/0/1\">Nicholas Kolkin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Differentiable Convolution Search for Point Cloud Processing. (arXiv:2108.12856v1 [cs.CV])","link":"http://arxiv.org/abs/2108.12856","description":"<p>Exploiting convolutional neural networks for point cloud processing is quite\nchallenging, due to the inherent irregular distribution and discrete shape\nrepresentation of point clouds. To address these problems, many handcrafted\nconvolution variants have sprung up in recent years. Though with elaborate\ndesign, these variants could be far from optimal in sufficiently capturing\ndiverse shapes formed by discrete points. In this paper, we propose\nPointSeaConv, i.e., a novel differential convolution search paradigm on point\nclouds. It can work in a purely data-driven manner and thus is capable of\nauto-creating a group of suitable convolutions for geometric shape modeling. We\nalso propose a joint optimization framework for simultaneous search of internal\nconvolution and external architecture, and introduce epsilon-greedy algorithm\nto alleviate the effect of discretization error. As a result, PointSeaNet, a\ndeep network that is sufficient to capture geometric shapes at both convolution\nlevel and architecture level, can be searched out for point cloud processing.\nExtensive experiments strongly evidence that our proposed PointSeaNet surpasses\ncurrent handcrafted deep models on challenging benchmarks across multiple tasks\nwith remarkable margins.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nie_X/0/1/0/all/0/1\">Xing Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yongcheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shaohong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1\">Jianlong Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huo_C/0/1/0/all/0/1\">Chunlei Huo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_G/0/1/0/all/0/1\">Gaofeng Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Weiming Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_C/0/1/0/all/0/1\">Chunhong Pan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Edge-Cloud Collaborated Object Detection via Difficult-Case Discriminator. (arXiv:2108.12858v1 [cs.CV])","link":"http://arxiv.org/abs/2108.12858","description":"<p>As one of the basic tasks of computer vision, object detection has been\nwidely used in many intelligent applications. However, object detection\nalgorithms are usually heavyweight in computation, hindering their\nimplementations on resource-constrained edge devices. Current edge-cloud\ncollaboration methods, such as CNN partition over Edge-cloud devices, are not\nsuitable for object detection since the huge data size of the intermediate\nresults will introduce extravagant communication costs. To address this\nchallenge, we propose a small-big model framework that deploys a big model in\nthe cloud and a small model on the edge devices. Upon receiving data, the edge\ndevice operates a difficult-case discriminator to classify the images into easy\ncases and difficult cases according to the specific semantics of the images.\nThe easy cases will be processed locally at the edge, and the difficult cases\nwill be uploaded to the cloud. Experimental results on the VOC, COCO, HELMET\ndatasets using two different object detection algorithms demonstrate that the\nsmall-big model system can detect 94.01%-97.84% of objects with only about 50%\nimages uploaded to the cloud when using SSD. In addition, the small-big model\naveragely reaches 91.22%- 92.52% end-to-end mAP of the scheme that uploading\nall images to the cloud.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Zhiqiang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhijun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heng_P/0/1/0/all/0/1\">Pan Heng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yongrui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_D/0/1/0/all/0/1\">Daqi Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jie Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MBDF-Net: Multi-Branch Deep Fusion Network for 3D Object Detection. (arXiv:2108.12863v1 [cs.CV])","link":"http://arxiv.org/abs/2108.12863","description":"<p>Point clouds and images could provide complementary information when\nrepresenting 3D objects. Fusing the two kinds of data usually helps to improve\nthe detection results. However, it is challenging to fuse the two data\nmodalities, due to their different characteristics and the interference from\nthe non-interest areas. To solve this problem, we propose a Multi-Branch Deep\nFusion Network (MBDF-Net) for 3D object detection. The proposed detector has\ntwo stages. In the first stage, our multi-branch feature extraction network\nutilizes Adaptive Attention Fusion (AAF) modules to produce cross-modal fusion\nfeatures from single-modal semantic features. In the second stage, we use a\nregion of interest (RoI) -pooled fusion module to generate enhanced local\nfeatures for refinement. A novel attention-based hybrid sampling strategy is\nalso proposed for selecting key points in the downsampling process. We evaluate\nour approach on two widely used benchmark datasets including KITTI and\nSUN-RGBD. The experimental results demonstrate the advantages of our method\nover state-of-the-art approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xun Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xingyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guowei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_J/0/1/0/all/0/1\">Jishiyu Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_X/0/1/0/all/0/1\">Xuguang Lan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Partial Domain Adaptation without Domain Alignment. (arXiv:2108.12867v1 [cs.CV])","link":"http://arxiv.org/abs/2108.12867","description":"<p>Unsupervised domain adaptation (UDA) aims to transfer knowledge from a\nwell-labeled source domain to a different but related unlabeled target domain\nwith identical label space. Currently, the main workhorse for solving UDA is\ndomain alignment, which has proven successful. However, it is often difficult\nto find an appropriate source domain with identical label space. A more\npractical scenario is so-called partial domain adaptation (PDA) in which the\nsource label set or space subsumes the target one. Unfortunately, in PDA, due\nto the existence of the irrelevant categories in the source domain, it is quite\nhard to obtain a perfect alignment, thus resulting in mode collapse and\nnegative transfer. Although several efforts have been made by down-weighting\nthe irrelevant source categories, the strategies used tend to be burdensome and\nrisky since exactly which irrelevant categories are unknown. These challenges\nmotivate us to find a relatively simpler alternative to solve PDA. To achieve\nthis, we first provide a thorough theoretical analysis, which illustrates that\nthe target risk is bounded by both model smoothness and between-domain\ndiscrepancy. Considering the difficulty of perfect alignment in solving PDA, we\nturn to focus on the model smoothness while discard the riskier domain\nalignment to enhance the adaptability of the model. Specifically, we\ninstantiate the model smoothness as a quite simple intra-domain structure\npreserving (IDSP). To our best knowledge, this is the first naive attempt to\naddress the PDA without domain alignment. Finally, our empirical results on\nmultiple benchmark datasets demonstrate that IDSP is not only superior to the\nPDA SOTAs by a significant margin on some benchmarks (e.g., +10% on Cl-&gt;Rw and\n+8% on Ar-&gt;Rw ), but also complementary to domain alignment in the standard UDA\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Weikai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Songcan Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Multimodal Framework for Video Ads Understanding. (arXiv:2108.12868v1 [cs.CV])","link":"http://arxiv.org/abs/2108.12868","description":"<p>There is a growing trend in placing video advertisements on social platforms\nfor online marketing, which demands automatic approaches to understand the\ncontents of advertisements effectively. Taking the 2021 TAAC competition as an\nopportunity, we developed a multimodal system to improve the ability of\nstructured analysis of advertising video content. In our framework, we break\ndown the video structuring analysis problem into two tasks, i.e., scene\nsegmentation and multi-modal tagging. In scene segmentation, we build upon a\ntemporal convolution module for temporal modeling to predict whether adjacent\nframes belong to the same scene. In multi-modal tagging, we first compute\nclip-level visual features by aggregating frame-level features with\nNeXt-SoftDBoF. The visual features are further complemented with textual\nfeatures that are derived using a global-local attention mechanism to extract\nuseful information from OCR (Optical Character Recognition) and ASR (Audio\nSpeech Recognition) outputs. Our solution achieved a score of 0.2470 measured\nin consideration of localization and prediction accuracy, ranking fourth in the\n2021 TAAC final leaderboard.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Weng_Z/0/1/0/all/0/1\">Zejia Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_L/0/1/0/all/0/1\">Lingchen Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zuxuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yu-Gang Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Solving Viewing Graph Optimization for Simultaneous Position and Rotation Registration. (arXiv:2108.12876v1 [cs.CV])","link":"http://arxiv.org/abs/2108.12876","description":"<p>A viewing graph is a set of unknown camera poses, as the vertices, and the\nobserved relative motions, as the edges. Solving the viewing graph is an\nessential step in a Structure-from-Motion procedure, where a set of relative\nmotions is obtained from a collection of 2D images. Almost all methods in the\nliterature solve for the rotations separately, through rotation averaging\nprocess, and use them for solving the positions. Obtaining positions is the\nchallenging part because the translation observations only tell the direction\nof the motions. It becomes more challenging when the set of edges comprises\npairwise translation observations between either near and far cameras. In this\npaper an iterative method is proposed that overcomes these issues. Also a\nmethod is proposed which obtains the rotations and positions simultaneously.\nExperimental results show the-state-of-the-art performance of the proposed\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nasiri_S/0/1/0/all/0/1\">Seyed-Mahdi Nasiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hosseini_R/0/1/0/all/0/1\">Reshad Hosseini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moradi_H/0/1/0/all/0/1\">Hadi Moradi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Layout-to-Image Translation with Double Pooling Generative Adversarial Networks. (arXiv:2108.12900v1 [cs.CV])","link":"http://arxiv.org/abs/2108.12900","description":"<p>In this paper, we address the task of layout-to-image translation, which aims\nto translate an input semantic layout to a realistic image. One open challenge\nwidely observed in existing methods is the lack of effective semantic\nconstraints during the image translation process, leading to models that cannot\npreserve the semantic information and ignore the semantic dependencies within\nthe same object. To address this issue, we propose a novel Double Pooing GAN\n(DPGAN) for generating photo-realistic and semantically-consistent results from\nthe input layout. We also propose a novel Double Pooling Module (DPM), which\nconsists of the Square-shape Pooling Module (SPM) and the Rectangle-shape\nPooling Module (RPM). Specifically, SPM aims to capture short-range semantic\ndependencies of the input layout with different spatial scales, while RPM aims\nto capture long-range semantic dependencies from both horizontal and vertical\ndirections. We then effectively fuse both outputs of SPM and RPM to further\nenlarge the receptive field of our generator. Extensive experiments on five\npopular datasets show that the proposed DPGAN achieves better results than\nstate-of-the-art methods. Finally, both SPM and SPM are general and can be\nseamlessly integrated into any GAN-based architectures to strengthen the\nfeature representation. The code is available at\nhttps://github.com/Ha0Tang/DPGAN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1\">Nicu Sebe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lipschitz Continuity Guided Knowledge Distillation. (arXiv:2108.12905v1 [cs.LG])","link":"http://arxiv.org/abs/2108.12905","description":"<p>Knowledge distillation has become one of the most important model compression\ntechniques by distilling knowledge from larger teacher networks to smaller\nstudent ones. Although great success has been achieved by prior distillation\nmethods via delicately designing various types of knowledge, they overlook the\nfunctional properties of neural networks, which makes the process of applying\nthose techniques to new tasks unreliable and non-trivial. To alleviate such\nproblem, in this paper, we initially leverage Lipschitz continuity to better\nrepresent the functional characteristic of neural networks and guide the\nknowledge distillation process. In particular, we propose a novel Lipschitz\nContinuity Guided Knowledge Distillation framework to faithfully distill\nknowledge by minimizing the distance between two neural networks' Lipschitz\nconstants, which enables teacher networks to better regularize student networks\nand improve the corresponding performance. We derive an explainable\napproximation algorithm with an explicit theoretical derivation to address the\nNP-hard problem of calculating the Lipschitz constant. Experimental results\nhave shown that our method outperforms other benchmarks over several knowledge\ndistillation tasks (e.g., classification, segmentation and object detection) on\nCIFAR-100, ImageNet, and PASCAL VOC datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shang_Y/0/1/0/all/0/1\">Yuzhang Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_B/0/1/0/all/0/1\">Bin Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zong_Z/0/1/0/all/0/1\">Ziliang Zong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_L/0/1/0/all/0/1\">Liqiang Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yan Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NeuroCartography: Scalable Automatic Visual Summarization of Concepts in Deep Neural Networks. (arXiv:2108.12931v1 [cs.CV])","link":"http://arxiv.org/abs/2108.12931","description":"<p>Existing research on making sense of deep neural networks often focuses on\nneuron-level interpretation, which may not adequately capture the bigger\npicture of how concepts are collectively encoded by multiple neurons. We\npresent NeuroCartography, an interactive system that scalably summarizes and\nvisualizes concepts learned by neural networks. It automatically discovers and\ngroups neurons that detect the same concepts, and describes how such neuron\ngroups interact to form higher-level concepts and the subsequent predictions.\nNeuroCartography introduces two scalable summarization techniques: (1) neuron\nclustering groups neurons based on the semantic similarity of the concepts\ndetected by neurons (e.g., neurons detecting \"dog faces\" of different breeds\nare grouped); and (2) neuron embedding encodes the associations between related\nconcepts based on how often they co-occur (e.g., neurons detecting \"dog face\"\nand \"dog tail\" are placed closer in the embedding space). Key to our scalable\ntechniques is the ability to efficiently compute all neuron pairs'\nrelationships, in time linear to the number of neurons instead of quadratic\ntime. NeuroCartography scales to large data, such as the ImageNet dataset with\n1.2M images. The system's tightly coordinated views integrate the scalable\ntechniques to visualize the concepts and their relationships, projecting the\nconcept associations to a 2D space in Neuron Projection View, and summarizing\nneuron clusters and their relationships in Graph View. Through a large-scale\nhuman evaluation, we demonstrate that our technique discovers neuron groups\nthat represent coherent, human-meaningful concepts. And through usage\nscenarios, we describe how our approaches enable interesting and surprising\ndiscoveries, such as concept cascades of related and isolated concepts. The\nNeuroCartography visualization runs in modern browsers and is open-sourced.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_H/0/1/0/all/0/1\">Haekyu Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_N/0/1/0/all/0/1\">Nilaksh Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duggal_R/0/1/0/all/0/1\">Rahul Duggal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wright_A/0/1/0/all/0/1\">Austin P. Wright</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaikh_O/0/1/0/all/0/1\">Omar Shaikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hohman_F/0/1/0/all/0/1\">Fred Hohman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chau_D/0/1/0/all/0/1\">Duen Horng Chau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning JPEG Compression Artifacts for Image Manipulation Detection and Localization. (arXiv:2108.12947v1 [eess.IV])","link":"http://arxiv.org/abs/2108.12947","description":"<p>Detecting and localizing image manipulation are necessary to counter\nmalicious use of image editing techniques. Accordingly, it is essential to\ndistinguish between authentic and tampered regions by analyzing intrinsic\nstatistics in an image. We focus on JPEG compression artifacts left during\nimage acquisition and editing. We propose a convolutional neural network (CNN)\nthat uses discrete cosine transform (DCT) coefficients, where compression\nartifacts remain, to localize image manipulation. Standard CNNs cannot learn\nthe distribution of DCT coefficients because the convolution throws away the\nspatial coordinates, which are essential for DCT coefficients. We illustrate\nhow to design and train a neural network that can learn the distribution of DCT\ncoefficients. Furthermore, we introduce Compression Artifact Tracing Network\n(CAT-Net) that jointly uses image acquisition artifacts and compression\nartifacts. It significantly outperforms traditional and deep neural\nnetwork-based methods in detecting and localizing tampered regions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kwon_M/0/1/0/all/0/1\">Myung-Joon Kwon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nam_S/0/1/0/all/0/1\">Seung-Hun Nam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_I/0/1/0/all/0/1\">In-Jae Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_H/0/1/0/all/0/1\">Heung-Kyu Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_C/0/1/0/all/0/1\">Changick Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Discover Reflection Symmetry via Polar Matching Convolution. (arXiv:2108.12952v1 [cs.CV])","link":"http://arxiv.org/abs/2108.12952","description":"<p>The task of reflection symmetry detection remains challenging due to\nsignificant variations and ambiguities of symmetry patterns in the wild.\nFurthermore, since the local regions are required to match in reflection for\ndetecting a symmetry pattern, it is hard for standard convolutional networks,\nwhich are not equivariant to rotation and reflection, to learn the task. To\naddress the issue, we introduce a new convolutional technique, dubbed the polar\nmatching convolution, which leverages a polar feature pooling, a\nself-similarity encoding, and a systematic kernel design for axes of different\nangles. The proposed high-dimensional kernel convolution network effectively\nlearns to discover symmetry patterns from real-world images, overcoming the\nlimitations of standard convolution. In addition, we present a new dataset and\nintroduce a self-supervised learning strategy by augmenting the dataset with\nsynthesizing images. Experiments demonstrate that our method outperforms\nstate-of-the-art methods in terms of accuracy and robustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Seo_A/0/1/0/all/0/1\">Ahyun Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shim_W/0/1/0/all/0/1\">Woohyeon Shim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_M/0/1/0/all/0/1\">Minsu Cho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Searching for Two-Stream Models in Multivariate Space for Video Recognition. (arXiv:2108.12957v1 [cs.CV])","link":"http://arxiv.org/abs/2108.12957","description":"<p>Conventional video models rely on a single stream to capture the complex\nspatial-temporal features. Recent work on two-stream video models, such as\nSlowFast network and AssembleNet, prescribe separate streams to learn\ncomplementary features, and achieve stronger performance. However, manually\ndesigning both streams as well as the in-between fusion blocks is a daunting\ntask, requiring to explore a tremendously large design space. Such manual\nexploration is time-consuming and often ends up with sub-optimal architectures\nwhen computational resources are limited and the exploration is insufficient.\nIn this work, we present a pragmatic neural architecture search approach, which\nis able to search for two-stream video models in giant spaces efficiently. We\ndesign a multivariate search space, including 6 search variables to capture a\nwide variety of choices in designing two-stream models. Furthermore, we propose\na progressive search procedure, by searching for the architecture of individual\nstreams, fusion blocks, and attention blocks one after the other. We\ndemonstrate two-stream models with significantly better performance can be\nautomatically discovered in our design space. Our searched two-stream models,\nnamely Auto-TSNet, consistently outperform other models on standard benchmarks.\nOn Kinetics, compared with the SlowFast model, our Auto-TSNet-L model reduces\nFLOPS by nearly 11 times while achieving the same accuracy 78.9%. On\nSomething-Something-V2, Auto-TSNet-M improves the accuracy by at least 2% over\nother methods which use less than 50 GFLOPS per video.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gong_X/0/1/0/all/0/1\">Xinyu Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Heng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shou_Z/0/1/0/all/0/1\">Zheng Shou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feiszli_M/0/1/0/all/0/1\">Matt Feiszli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zhicheng Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3DStyleNet: Creating 3D Shapes with Geometric and Texture Style Variations. (arXiv:2108.12958v1 [cs.CV])","link":"http://arxiv.org/abs/2108.12958","description":"<p>We propose a method to create plausible geometric and texture style\nvariations of 3D objects in the quest to democratize 3D content creation. Given\na pair of textured source and target objects, our method predicts a part-aware\naffine transformation field that naturally warps the source shape to imitate\nthe overall geometric style of the target. In addition, the texture style of\nthe target is transferred to the warped source object with the help of a\nmulti-view differentiable renderer. Our model, 3DStyleNet, is composed of two\nsub-networks trained in two stages. First, the geometric style network is\ntrained on a large set of untextured 3D shapes. Second, we jointly optimize our\ngeometric style network and a pre-trained image style transfer network with\nlosses defined over both the geometry and the rendering of the result. Given a\nsmall set of high-quality textured objects, our method can create many novel\nstylized shapes, resulting in effortless 3D content creation and style-ware\ndata augmentation. We showcase our approach qualitatively on 3D content\nstylization, and provide user studies to validate the quality of our results.\nIn addition, our method can serve as a valuable tool to create 3D data\naugmentations for computer vision tasks. Extensive quantitative analysis shows\nthat 3DStyleNet outperforms alternative data augmentation techniques for the\ndownstream task of single-image 3D reconstruction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yin_K/0/1/0/all/0/1\">Kangxue Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jun Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shugrina_M/0/1/0/all/0/1\">Maria Shugrina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khamis_S/0/1/0/all/0/1\">Sameh Khamis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fidler_S/0/1/0/all/0/1\">Sanja Fidler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BioFors: A Large Biomedical Image Forensics Dataset. (arXiv:2108.12961v1 [cs.CV])","link":"http://arxiv.org/abs/2108.12961","description":"<p>Research in media forensics has gained traction to combat the spread of\nmisinformation. However, most of this research has been directed towards\ncontent generated on social media. Biomedical image forensics is a related\nproblem, where manipulation or misuse of images reported in biomedical research\ndocuments is of serious concern. The problem has failed to gain momentum beyond\nan academic discussion due to an absence of benchmark datasets and standardized\ntasks. In this paper we present BioFors -- the first dataset for benchmarking\ncommon biomedical image manipulations. BioFors comprises 47,805 images\nextracted from 1,031 open-source research papers. Images in BioFors are divided\ninto four categories -- Microscopy, Blot/Gel, FACS and Macroscopy. We also\npropose three tasks for forensic analysis -- external duplication detection,\ninternal duplication detection and cut/sharp-transition detection. We benchmark\nBioFors on all tasks with suitable state-of-the-art algorithms. Our results and\nanalysis show that existing algorithms developed on common computer vision\ndatasets are not robust when applied to biomedical images, validating that more\nresearch is required to address the unique challenges of biomedical image\nforensics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sabir_E/0/1/0/all/0/1\">Ekraam Sabir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nandi_S/0/1/0/all/0/1\">Soumyaroop Nandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+AbdAlmageed_W/0/1/0/all/0/1\">Wael AbdAlmageed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Natarajan_P/0/1/0/all/0/1\">Prem Natarajan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Font Completion and Manipulation by Cycling Between Multi-Modality Representations. (arXiv:2108.12965v1 [cs.CV])","link":"http://arxiv.org/abs/2108.12965","description":"<p>Generating font glyphs of consistent style from one or a few reference\nglyphs, i.e., font completion, is an important task in topographical design. As\nthe problem is more well-defined than general image style transfer tasks, thus\nit has received interest from both vision and machine learning communities.\nExisting approaches address this problem as a direct image-to-image translation\ntask. In this work, we innovate to explore the generation of font glyphs as 2D\ngraphic objects with the graph as an intermediate representation, so that more\nintrinsic graphic properties of font styles can be captured. Specifically, we\nformulate a cross-modality cycled image-to-image model structure with a graph\nconstructor between an image encoder and an image renderer. The novel graph\nconstructor maps a glyph's latent code to its graph representation that matches\nexpert knowledge, which is trained to help the translation task. Our model\ngenerates improved results than both image-to-image baseline and previous\nstate-of-the-art methods for glyph completion. Furthermore, the graph\nrepresentation output by our model also provides an intuitive interface for\nusers to do local editing and manipulation. Our proposed cross-modality cycled\nrepresentation learning has the potential to be applied to other domains with\nprior knowledge from different data modalities. Our code is available at\nhttps://github.com/VITA-Group/Font_Completion_Graph.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Ye Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wuyang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhaowen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fisher_M/0/1/0/all/0/1\">Matthew Fisher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhifei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1\">Hailin Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Digging into Uncertainty in Self-supervised Multi-view Stereo. (arXiv:2108.12966v1 [cs.CV])","link":"http://arxiv.org/abs/2108.12966","description":"<p>Self-supervised Multi-view stereo (MVS) with a pretext task of image\nreconstruction has achieved significant progress recently. However, previous\nmethods are built upon intuitions, lacking comprehensive explanations about the\neffectiveness of the pretext task in self-supervised MVS. To this end, we\npropose to estimate epistemic uncertainty in self-supervised MVS, accounting\nfor what the model ignores. Specially, the limitations can be categorized into\ntwo types: ambiguious supervision in foreground and invalid supervision in\nbackground. To address these issues, we propose a novel Uncertainty reduction\nMulti-view Stereo (UMVS) framework for self-supervised learning. To alleviate\nambiguous supervision in foreground, we involve extra correspondence prior with\na flow-depth consistency loss. The dense 2D correspondence of optical flows is\nused to regularize the 3D stereo correspondence in MVS. To handle the invalid\nsupervision in background, we use Monte-Carlo Dropout to acquire the\nuncertainty map and further filter the unreliable supervision signals on\ninvalid regions. Extensive experiments on DTU and Tank&amp;Temples benchmark show\nthat our U-MVS framework achieves the best performance among unsupervised MVS\nmethods, with competitive performance with its supervised opponents.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hongbin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhipeng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yali Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_W/0/1/0/all/0/1\">Wenxiong Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1\">Baigui Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SHIFT15M: Multiobjective Large-Scale Fashion Dataset with Distributional Shifts. (arXiv:2108.12992v1 [cs.LG])","link":"http://arxiv.org/abs/2108.12992","description":"<p>Many machine learning algorithms assume that the training data and the test\ndata follow the same distribution. However, such assumptions are often violated\nin real-world machine learning problems. In this paper, we propose SHIFT15M, a\ndataset that can be used to properly evaluate models in situations where the\ndistribution of data changes between training and testing. The SHIFT15M dataset\nhas several good properties: (i) Multiobjective. Each instance in the dataset\nhas several numerical values that can be used as target variables. (ii)\nLarge-scale. The SHIFT15M dataset consists of 15million fashion images. (iii)\nCoverage of types of dataset shifts. SHIFT15M contains multiple dataset shift\nproblem settings (e.g., covariate shift or target shift). SHIFT15M also enables\nthe performance evaluation of the model under various magnitudes of dataset\nshifts by switching the magnitude. In addition, we provide software to handle\nSHIFT15M in a very simple way: https://github.com/st-tech/zozo-shift15m.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kimura_M/0/1/0/all/0/1\">Masanari Kimura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakamura_T/0/1/0/all/0/1\">Takuma Nakamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saito_Y/0/1/0/all/0/1\">Yuki Saito</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pseudo-mask Matters inWeakly-supervised Semantic Segmentation. (arXiv:2108.12995v1 [cs.CV])","link":"http://arxiv.org/abs/2108.12995","description":"<p>Most weakly supervised semantic segmentation (WSSS) methods follow the\npipeline that generates pseudo-masks initially and trains the segmentation\nmodel with the pseudo-masks in fully supervised manner after. However, we find\nsome matters related to the pseudo-masks, including high quality pseudo-masks\ngeneration from class activation maps (CAMs), and training with noisy\npseudo-mask supervision. For these matters, we propose the following designs to\npush the performance to new state-of-art: (i) Coefficient of Variation\nSmoothing to smooth the CAMs adaptively; (ii) Proportional Pseudo-mask\nGeneration to project the expanded CAMs to pseudo-mask based on a new metric\nindicating the importance of each class on each location, instead of the scores\ntrained from binary classifiers. (iii) Pretended Under-Fitting strategy to\nsuppress the influence of noise in pseudo-mask; (iv) Cyclic Pseudo-mask to\nboost the pseudo-masks during training of fully supervised semantic\nsegmentation (FSSS). Experiments based on our methods achieve new state-of-art\nresults on two changeling weakly supervised semantic segmentation datasets,\npushing the mIoU to 70.0% and 40.2% on PAS-CAL VOC 2012 and MS COCO 2014\nrespectively. Codes including segmentation framework are released at\nhttps://github.com/Eli-YiLi/PMM\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuang_Z/0/1/0/all/0/1\">Zhanghui Kuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Liyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yimin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wayne Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Battle of Network Structures: An Empirical Study of CNN, Transformer, and MLP. (arXiv:2108.13002v1 [cs.CV])","link":"http://arxiv.org/abs/2108.13002","description":"<p>Convolutional neural networks (CNN) are the dominant deep neural network\n(DNN) architecture for computer vision. Recently, Transformer and multi-layer\nperceptron (MLP)-based models, such as Vision Transformer and MLP-Mixer,\nstarted to lead new trends as they showed promising results in the ImageNet\nclassification task. In this paper, we conduct empirical studies on these DNN\nstructures and try to understand their respective pros and cons. To ensure a\nfair comparison, we first develop a unified framework called SPACH which adopts\nseparate modules for spatial and channel processing. Our experiments under the\nSPACH framework reveal that all structures can achieve competitive performance\nat a moderate scale. However, they demonstrate distinctive behaviors when the\nnetwork size scales up. Based on our findings, we propose two hybrid models\nusing convolution and Transformer modules. The resulting Hybrid-MS-S+ model\nachieves 83.9% top-1 accuracy with 63M parameters and 12.3G FLOPS. It is\nalready on par with the SOTA models with sophisticated designs. The code and\nmodels will be made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yucheng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guangting Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1\">Chuanxin Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1\">Chong Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1\">Wenjun Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1\">Zheng-Jun Zha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Embedding Novel Views in a Single JPEG Image. (arXiv:2108.13003v1 [cs.CV])","link":"http://arxiv.org/abs/2108.13003","description":"<p>We propose a novel approach for embedding novel views in a single JPEG image\nwhile preserving the perceptual fidelity of the modified JPEG image and the\nrestored novel views. We adopt the popular novel view synthesis representation\nof multiplane images (MPIs). Our model first encodes 32 MPI layers (totally 128\nchannels) into a 3-channel JPEG image that can be decoded for MPIs to render\nnovel views, with an embedding capacity of 1024 bits per pixel. We conducted\nexperiments on public datasets with different novel view synthesis methods, and\nthe results show that the proposed method can restore high-fidelity novel views\nfrom a slightly modified JPEG image. Furthermore, our method is robust to JPEG\ncompression, color adjusting, and cropping. Our source code will be publicly\navailable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yue Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_G/0/1/0/all/0/1\">Guotao Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qifeng Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"X2Teeth: 3D Teeth Reconstruction from a Single Panoramic Radiograph. (arXiv:2108.13004v1 [cs.CV])","link":"http://arxiv.org/abs/2108.13004","description":"<p>3D teeth reconstruction from X-ray is important for dental diagnosis and many\nclinical operations. However, no existing work has explored the reconstruction\nof teeth for a whole cavity from a single panoramic radiograph. Different from\nsingle object reconstruction from photos, this task has the unique challenge of\nconstructing multiple objects at high resolutions. To conquer this task, we\ndevelop a novel ConvNet X2Teeth that decomposes the task into teeth\nlocalization and single-shape estimation. We also introduce a patch-based\ntraining strategy, such that X2Teeth can be end-to-end trained for optimal\nperformance. Extensive experiments show that our method can successfully\nestimate the 3D structure of the cavity and reflect the details for each tooth.\nMoreover, X2Teeth achieves a reconstruction IoU of 0.681, which significantly\noutperforms the encoder-decoder method by $1.71X and the retrieval-based method\nby $1.52X. Our method can also be promising for other multi-anatomy 3D\nreconstruction tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yuan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_W/0/1/0/all/0/1\">Weinan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jiawei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_L/0/1/0/all/0/1\">Liang Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Lei He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring and Improving Mobile Level Vision Transformers. (arXiv:2108.13015v1 [cs.CV])","link":"http://arxiv.org/abs/2108.13015","description":"<p>We study the vision transformer structure in the mobile level in this paper,\nand find a dramatic performance drop. We analyze the reason behind this\nphenomenon, and propose a novel irregular patch embedding module and adaptive\npatch fusion module to improve the performance. We conjecture that the vision\ntransformer blocks (which consist of multi-head attention and feed-forward\nnetwork) are more suitable to handle high-level information than low-level\nfeatures. The irregular patch embedding module extracts patches that contain\nrich high-level information with different receptive fields. The transformer\nblocks can obtain the most useful information from these irregular patches.\nThen the processed patches pass the adaptive patch merging module to get the\nfinal features for the classifier. With our proposed improvements, the\ntraditional uniform vision transformer structure can achieve state-of-the-art\nresults in mobile level. We improve the DeiT baseline by more than 9\\% under\nthe mobile-level settings and surpass other transformer architectures like Swin\nand CoaT by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Pengguang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yixin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Mingchang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\">Jiaya Jia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What You Can Learn by Staring at a Blank Wall. (arXiv:2108.13027v1 [cs.CV])","link":"http://arxiv.org/abs/2108.13027","description":"<p>We present a passive non-line-of-sight method that infers the number of\npeople or activity of a person from the observation of a blank wall in an\nunknown room. Our technique analyzes complex imperceptible changes in indirect\nillumination in a video of the wall to reveal a signal that is correlated with\nmotion in the hidden part of a scene. We use this signal to classify between\nzero, one, or two moving people, or the activity of a person in the hidden\nscene. We train two convolutional neural networks using data collected from 20\ndifferent scenes, and achieve an accuracy of $\\approx94\\%$ for both tasks in\nunseen test environments and real-time online settings. Unlike other passive\nnon-line-of-sight methods, the technique does not rely on known occluders or\ncontrollable light sources, and generalizes to unknown rooms with no\nre-calibration. We analyze the generalization and robustness of our method with\nboth real and synthetic data, and study the effect of the scene parameters on\nthe signal quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_P/0/1/0/all/0/1\">Prafull Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aittala_M/0/1/0/all/0/1\">Miika Aittala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schechner_Y/0/1/0/all/0/1\">Yoav Y. Schechner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1\">Antonio Torralba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wornell_G/0/1/0/all/0/1\">Gregory W. Wornell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freeman_W/0/1/0/all/0/1\">William T. Freeman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durand_F/0/1/0/all/0/1\">Fredo Durand</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Visual Recognition with Deep Neural Networks: A Survey on Recent Advances and New Directions. (arXiv:2108.13055v1 [cs.CV])","link":"http://arxiv.org/abs/2108.13055","description":"<p>Visual recognition is currently one of the most important and active research\nareas in computer vision, pattern recognition, and even the general field of\nartificial intelligence. It has great fundamental importance and strong\nindustrial needs. Deep neural networks (DNNs) have largely boosted their\nperformances on many concrete tasks, with the help of large amounts of training\ndata and new powerful computation resources. Though recognition accuracy is\nusually the first concern for new progresses, efficiency is actually rather\nimportant and sometimes critical for both academic research and industrial\napplications. Moreover, insightful views on the opportunities and challenges of\nefficiency are also highly required for the entire community. While general\nsurveys on the efficiency issue of DNNs have been done from various\nperspectives, as far as we are aware, scarcely any of them focused on visual\nrecognition systematically, and thus it is unclear which progresses are\napplicable to it and what else should be concerned. In this paper, we present\nthe review of the recent advances with our suggestions on the new possible\ndirections towards improving the efficiency of DNN-related visual recognition\napproaches. We investigate not only from the model but also the data point of\nview (which is not the case in existing surveys), and focus on three most\nstudied data types (images, videos and points). This paper attempts to provide\na systematic summary via a comprehensive survey which can serve as a valuable\nreference and inspire both researchers and practitioners who work on visual\nrecognition problems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dingheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiaotong Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guoqi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_W/0/1/0/all/0/1\">Weisheng Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jianbo Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Monocular Depth Perception: Focusing on Moving Objects. (arXiv:2108.13062v1 [cs.CV])","link":"http://arxiv.org/abs/2108.13062","description":"<p>As a flexible passive 3D sensing means, unsupervised learning of depth from\nmonocular videos is becoming an important research topic. It utilizes the\nphotometric errors between the target view and the synthesized views from its\nadjacent source views as the loss instead of the difference from the ground\ntruth. Occlusion and scene dynamics in real-world scenes still adversely affect\nthe learning, despite significant progress made recently. In this paper, we\nshow that deliberately manipulating photometric errors can efficiently deal\nwith these difficulties better. We first propose an outlier masking technique\nthat considers the occluded or dynamic pixels as statistical outliers in the\nphotometric error map. With the outlier masking, the network learns the depth\nof objects that move in the opposite direction to the camera more accurately.\nTo the best of our knowledge, such cases have not been seriously considered in\nthe previous works, even though they pose a high risk in applications like\nautonomous driving. We also propose an efficient weighted multi-scale scheme to\nreduce the artifacts in the predicted depth maps. Extensive experiments on the\nKITTI dataset and additional experiments on the Cityscapes dataset have\nverified the proposed approach's effectiveness on depth or ego-motion\nestimation. Furthermore, for the first time, we evaluate the predicted depth on\nthe regions of dynamic objects and static background separately for both\nsupervised and unsupervised methods. The evaluation further verifies the\neffectiveness of our proposed technical approach and provides some interesting\nobservations that might inspire future research in this direction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Hualie Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1\">Laiyan Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zhenglong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1\">Rui Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating Vulnerabilities of Deep Neural Policies. (arXiv:2108.13093v1 [cs.LG])","link":"http://arxiv.org/abs/2108.13093","description":"<p>Reinforcement learning policies based on deep neural networks are vulnerable\nto imperceptible adversarial perturbations to their inputs, in much the same\nway as neural network image classifiers. Recent work has proposed several\nmethods to improve the robustness of deep reinforcement learning agents to\nadversarial perturbations based on training in the presence of these\nimperceptible perturbations (i.e. adversarial training). In this paper, we\nstudy the effects of adversarial training on the neural policy learned by the\nagent. In particular, we follow two distinct parallel approaches to investigate\nthe outcomes of adversarial training on deep neural policies based on\nworst-case distributional shift and feature sensitivity. For the first\napproach, we compare the Fourier spectrum of minimal perturbations computed for\nboth adversarially trained and vanilla trained neural policies. Via experiments\nin the OpenAI Atari environments we show that minimal perturbations computed\nfor adversarially trained policies are more focused on lower frequencies in the\nFourier domain, indicating a higher sensitivity of these policies to low\nfrequency perturbations. For the second approach, we propose a novel method to\nmeasure the feature sensitivities of deep neural policies and we compare these\nfeature sensitivity differences in state-of-the-art adversarially trained deep\nneural policies and vanilla trained deep neural policies. We believe our\nresults can be an initial step towards understanding the relationship between\nadversarial training and different notions of robustness for neural policies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Korkmaz_E/0/1/0/all/0/1\">Ezgi Korkmaz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Object-aware Long-short-range Spatial Alignment for Few-Shot Fine-Grained Image Classification. (arXiv:2108.13098v1 [cs.CV])","link":"http://arxiv.org/abs/2108.13098","description":"<p>The goal of few-shot fine-grained image classification is to recognize rarely\nseen fine-grained objects in the query set, given only a few samples of this\nclass in the support set. Previous works focus on learning discriminative image\nfeatures from a limited number of training samples for distinguishing various\nfine-grained classes, but ignore one important fact that spatial alignment of\nthe discriminative semantic features between the query image with arbitrary\nchanges and the support image, is also critical for computing the semantic\nsimilarity between each support-query pair. In this work, we propose an\nobject-aware long-short-range spatial alignment approach, which is composed of\na foreground object feature enhancement (FOE) module, a long-range semantic\ncorrespondence (LSC) module and a short-range spatial manipulation (SSM)\nmodule. The FOE is developed to weaken background disturbance and encourage\nhigher foreground object response. To address the problem of long-range object\nfeature misalignment between support-query image pairs, the LSC is proposed to\nlearn the transferable long-range semantic correspondence by a designed feature\nsimilarity metric. Further, the SSM module is developed to refine the\ntransformed support feature after the long-range step to align short-range\nmisaligned features (or local details) with the query features. Extensive\nexperiments have been conducted on four benchmark datasets, and the results\nshow superior performance over most state-of-the-art methods under both 1-shot\nand 5-shot classification scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yike Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_G/0/1/0/all/0/1\">Gang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weixi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1\">Jiayuan Fan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Densely Semantic Enhancement for Domain Adaptive Region-free Detectors. (arXiv:2108.13101v1 [cs.CV])","link":"http://arxiv.org/abs/2108.13101","description":"<p>Unsupervised domain adaptive object detection aims to adapt a well-trained\ndetector from its original source domain with rich labeled data to a new target\ndomain with unlabeled data. Previous works focus on improving the domain\nadaptability of region-based detectors, e.g., Faster-RCNN, through matching\ncross-domain instance-level features that are explicitly extracted from a\nregion proposal network (RPN). However, this is unsuitable for region-free\ndetectors such as single shot detector (SSD), which perform a dense prediction\nfrom all possible locations in an image and do not have the RPN to encode such\ninstance-level features. As a result, they fail to align important image\nregions and crucial instance-level features between the domains of region-free\ndetectors. In this work, we propose an adversarial module to strengthen the\ncross-domain matching of instance-level features for region-free detectors.\nFirstly, to emphasize the important regions of image, the DSEM learns to\npredict a transferable foreground enhancement mask that can be utilized to\nsuppress the background disturbance in an image. Secondly, considering that\nregion-free detectors recognize objects of different scales using multi-scale\nfeature maps, the DSEM encodes both multi-level semantic representations and\nmulti-instance spatial-contextual relationships across different domains.\nFinally, the DSEM is pluggable into different region-free detectors, ultimately\nachieving the densely semantic feature matching via adversarial learning.\nExtensive experiments have been conducted on PASCAL VOC, Clipart, Comic,\nWatercolor, and FoggyCityscape benchmarks, and their results well demonstrate\nthat the proposed approach not only improves the domain adaptability of\nregion-free detectors but also outperforms existing domain adaptive\nregion-based detectors under various domain shift settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiaofeng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1\">Jiayuan Fan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Preprocessing and Ensemble Learning for Low Quality Cell Image Segmentation. (arXiv:2108.13118v1 [eess.IV])","link":"http://arxiv.org/abs/2108.13118","description":"<p>We propose an automatic preprocessing and ensemble learning for segmentation\nof cell images with low quality. It is difficult to capture cells with strong\nlight. Therefore, the microscopic images of cells tend to have low image\nquality but these images are not good for semantic segmentation. Here we\npropose a method to translate an input image to the images that are easy to\nrecognize by deep learning. The proposed method consists of two deep neural\nnetworks. The first network is the usual training for semantic segmentation,\nand penultimate feature maps of the first network are used as filters to\ntranslate an input image to the images that emphasize each class. This is the\nautomatic preprocessing and translated cell images are easily classified. The\ninput cell image with low quality is translated by the feature maps in the\nfirst network, and the translated images are fed into the second network for\nsemantic segmentation. Since the outputs of the second network are multiple\nsegmentation results, we conduct the weighted ensemble of those segmentation\nimages. Two networks are trained by end-to-end manner, and we do not need to\nprepare images with high quality for the translation. We confirmed that our\nproposed method can translate cell images with low quality to the images that\nare easy to segment, and segmentation accuracy has improved using the weighted\nensemble learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kato_S/0/1/0/all/0/1\">Sota Kato</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hotta_K/0/1/0/all/0/1\">Kazuhiro Hotta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tune It or Don't Use It: Benchmarking Data-Efficient Image Classification. (arXiv:2108.13122v1 [cs.CV])","link":"http://arxiv.org/abs/2108.13122","description":"<p>Data-efficient image classification using deep neural networks in settings,\nwhere only small amounts of labeled data are available, has been an active\nresearch area in the recent past. However, an objective comparison between\npublished methods is difficult, since existing works use different datasets for\nevaluation and often compare against untuned baselines with default\nhyper-parameters. We design a benchmark for data-efficient image classification\nconsisting of six diverse datasets spanning various domains (e.g., natural\nimages, medical imagery, satellite data) and data types (RGB, grayscale,\nmultispectral). Using this benchmark, we re-evaluate the standard cross-entropy\nbaseline and eight methods for data-efficient deep learning published between\n2017 and 2021 at renowned venues. For a fair and realistic comparison, we\ncarefully tune the hyper-parameters of all methods on each dataset.\nSurprisingly, we find that tuning learning rate, weight decay, and batch size\non a separate validation split results in a highly competitive baseline, which\noutperforms all but one specialized method and performs competitively to the\nremaining one.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Brigato_L/0/1/0/all/0/1\">Lorenzo Brigato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barz_B/0/1/0/all/0/1\">Bj&#xf6;rn Barz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iocchi_L/0/1/0/all/0/1\">Luca Iocchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denzler_J/0/1/0/all/0/1\">Joachim Denzler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From General to Specific: Informative Scene Graph Generation via Balance Adjustment. (arXiv:2108.13129v1 [cs.CV])","link":"http://arxiv.org/abs/2108.13129","description":"<p>The scene graph generation (SGG) task aims to detect visual relationship\ntriplets, i.e., subject, predicate, object, in an image, providing a structural\nvision layout for scene understanding. However, current models are stuck in\ncommon predicates, e.g., \"on\" and \"at\", rather than informative ones, e.g.,\n\"standing on\" and \"looking at\", resulting in the loss of precise information\nand overall performance. If a model only uses \"stone on road\" rather than\n\"blocking\" to describe an image, it is easy to misunderstand the scene. We\nargue that this phenomenon is caused by two key imbalances between informative\npredicates and common ones, i.e., semantic space level imbalance and training\nsample level imbalance. To tackle this problem, we propose BA-SGG, a simple yet\neffective SGG framework based on balance adjustment but not the conventional\ndistribution fitting. It integrates two components: Semantic Adjustment (SA)\nand Balanced Predicate Learning (BPL), respectively for adjusting these\nimbalances. Benefited from the model-agnostic process, our method is easily\napplied to the state-of-the-art SGG models and significantly improves the SGG\nperformance. Our method achieves 14.3%, 8.0%, and 6.1% higher Mean Recall (mR)\nthan that of the Transformer model at three scene graph generation sub-tasks on\nVisual Genome, respectively. Codes are publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yuyu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Lianli Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuanhan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yuxuan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xu Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Heng Tao Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jingkuan Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Privacy-Preserving Motion Detection and Object Tracking in Encrypted Streaming Video. (arXiv:2108.13141v1 [eess.IV])","link":"http://arxiv.org/abs/2108.13141","description":"<p>Video privacy leakage is becoming an increasingly severe public problem,\nespecially in cloud-based video surveillance systems. It leads to the new need\nfor secure cloud-based video applications, where the video is encrypted for\nprivacy protection. Despite some methods that have been proposed for encrypted\nvideo moving object detection and tracking, none has robust performance against\ncomplex and dynamic scenes. In this paper, we propose an efficient and robust\nprivacy-preserving motion detection and multiple object tracking scheme for\nencrypted surveillance video bitstreams. By analyzing the properties of the\nvideo codec and format-compliant encryption schemes, we propose a new\ncompressed-domain feature to capture motion information in complex surveillance\nscenarios. Based on this feature, we design an adaptive clustering algorithm\nfor moving object segmentation with an accuracy of 4x4 pixels. We then propose\na multiple object tracking scheme that uses Kalman filter estimation and\nadaptive measurement refinement. The proposed scheme does not require video\ndecryption or full decompression and has a very low computation load. The\nexperimental results demonstrate that our scheme achieves the best detection\nand tracking performance compared with existing works in the encrypted and\ncompressed domain. Our scheme can be effectively used in complex surveillance\nscenarios with different challenges, such as camera movement/jitter, dynamic\nbackground, and shadows.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Tian_X/0/1/0/all/0/1\">Xianhao Tian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_P/0/1/0/all/0/1\">Peijia Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_J/0/1/0/all/0/1\">Jiwu Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LIGAR: Lightweight General-purpose Action Recognition. (arXiv:2108.13153v1 [cs.CV])","link":"http://arxiv.org/abs/2108.13153","description":"<p>Growing amount of different practical tasks in a video understanding problem\nhas addressed the great challenge aiming to design an universal solution, which\nshould be available for broad masses and suitable for the demanding\nedge-oriented inference. In this paper we are focused on designing a network\narchitecture and a training pipeline to tackle the mentioned challenges. Our\narchitecture takes the best from the previous ones and brings the ability to be\nsuccessful not only in appearance-based action recognition tasks but in\nmotion-based problems too. Furthermore, the induced label noise problem is\nformulated and Adaptive Clip Selection (ACS) framework is proposed to deal with\nit. Together it makes the LIGAR framework the general-purpose action\nrecognition solution. We also have reported the extensive analysis on the\ngeneral and gesture datasets to show the excellent trade-off between the\nperformance and the accuracy in comparison to the state-of-the-art solutions.\nTraining code is available at:\nhttps://github.com/openvinotoolkit/training_extensions. For the efficient\nedge-oriented inference all trained models can be exported into the OpenVINO\nformat.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Izutov_E/0/1/0/all/0/1\">Evgeny Izutov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enterprise Architecture Model Transformation Engine. (arXiv:2108.13169v1 [cs.DC])","link":"http://arxiv.org/abs/2108.13169","description":"<p>With increasing linkage within value chains, the IT systems of different\ncompanies are also being connected with each other. This enables the\nintegration of services within the movement of Industry 4.0 in order to improve\nthe quality and performance of the processes. Enterprise architecture models\nform the basis for this with a better buisness IT-alignment. However, the\nheterogeneity of the modeling frameworks and description languages makes a\nconcatenation considerably difficult, especially differences in syntax,\nsemantic and relations. Therefore, this paper presents a transformation engine\nto convert enterprise architecture models between several languages. We\ndeveloped the first generic translation approach that is free of specific\nmeta-modeling, which is flexible adaptable to arbitrary modeling languages. The\ntransformation process is defined by various pattern matching techniques using\na rule-based description language. It uses set theory and first-order logic for\nan intuitive description as a basis. The concept is practical evaluated using\nan example in the area of a large German IT-service provider. Anyhow, the\napproach is applicable between a wide range of enterprise architecture\nframeworks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Heiland_E/0/1/0/all/0/1\">Erik Heiland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hillmann_P/0/1/0/all/0/1\">Peter Hillmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karcher_A/0/1/0/all/0/1\">Andreas Karcher</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Domain Randomization. (arXiv:1812.00491v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1812.00491","description":"<p>Domain Randomization (DR) is known to require a significant amount of\ntraining data for good performance. We argue that this is due to DR's strategy\nof random data generation using a uniform distribution over simulation\nparameters, as a result, DR often generates samples which are uninformative for\nthe learner. In this work, we theoretically analyze DR using ideas from\nmulti-source domain adaptation. Based on our findings, we propose Adversarial\nDomain Randomization (ADR) as an efficient variant of DR which generates\nadversarial samples with respect to the learner during training. We implement\nADR as a policy whose action space is the quantized simulation parameter space.\nAt each iteration, the policy's action generates labeled data and the reward is\nset as negative of learner's loss on this data. As a result, we observe ADR\nfrequently generates novel samples for the learner like truncated and occluded\nobjects for object detection and confusing classes for image classification. We\nperform evaluations on datasets like CLEVR, Syn2Real, and VIRAT for various\ntasks where we demonstrate that ADR outperforms DR by generating fewer data\nsamples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khirodkar_R/0/1/0/all/0/1\">Rawal Khirodkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitani_K/0/1/0/all/0/1\">Kris M. Kitani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inner-Imaging Networks: Put Lenses into Convolutional Structure. (arXiv:1904.12639v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1904.12639","description":"<p>Despite the tremendous success in computer vision, deep convolutional\nnetworks suffer from serious computation costs and redundancies. Although\nprevious works address this issue by enhancing diversities of filters, they\nhave not considered the complementarity and the completeness of the internal\nstructure of the convolutional network. To deal with these problems, a novel\nInner-Imaging architecture is proposed in this paper, which allows\nrelationships between channels to meet the above requirement. Specifically, we\norganize the channel signal points in groups using convolutional kernels to\nmodel both the intra-group and inter-group relationships simultaneously. The\nconvolutional filter is a powerful tool for modeling spatial relations and\norganizing grouped signals, so the proposed methods map the channel signals\nonto a pseudo-image, like putting a lens into convolution internal structure.\nConsequently, not only the diversity of channels is increased, but also the\ncomplementarity and completeness can be explicitly enhanced. The proposed\narchitecture is lightweight and easy to be implemented. It provides an\nefficient self-organization strategy for convolutional networks so as to\nimprove their efficiency and performance. Extensive experiments are conducted\non multiple benchmark image recognition data sets including CIFAR, SVHN and\nImageNet. Experimental results verify the effectiveness of the Inner-Imaging\nmechanism with the most popular convolutional networks as the backbones.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_G/0/1/0/all/0/1\">Guihua Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_M/0/1/0/all/0/1\">Mingnan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1\">Dan Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_W/0/1/0/all/0/1\">Wenming Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhiwen Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hall_W/0/1/0/all/0/1\">Wendy Hall</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Signal2Image Modules in Deep Neural Networks for EEG Classification. (arXiv:1904.13216v6 [eess.SP] UPDATED)","link":"http://arxiv.org/abs/1904.13216","description":"<p>Deep learning has revolutionized computer vision utilizing the increased\navailability of big data and the power of parallel computational units such as\ngraphical processing units. The vast majority of deep learning research is\nconducted using images as training data, however the biomedical domain is rich\nin physiological signals that are used for diagnosis and prediction problems.\nIt is still an open research question how to best utilize signals to train deep\nneural networks.\n</p>\n<p>In this paper we define the term Signal2Image (S2Is) as trainable or\nnon-trainable prefix modules that convert signals, such as\nElectroencephalography (EEG), to image-like representations making them\nsuitable for training image-based deep neural networks defined as `base\nmodels'. We compare the accuracy and time performance of four S2Is (`signal as\nimage', spectrogram, one and two layer Convolutional Neural Networks (CNNs))\ncombined with a set of `base models' (LeNet, AlexNet, VGGnet, ResNet, DenseNet)\nalong with the depth-wise and 1D variations of the latter. We also provide\nempirical evidence that the one layer CNN S2I performs better in eleven out of\nfifteen tested models than non-trainable S2Is for classifying EEG signals and\nwe present visual comparisons of the outputs of the S2Is.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bizopoulos_P/0/1/0/all/0/1\">Paschalis Bizopoulos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lambrou_G/0/1/0/all/0/1\">George I Lambrou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Koutsouris_D/0/1/0/all/0/1\">Dimitrios Koutsouris</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparsely Activated Networks. (arXiv:1907.06592v7 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/1907.06592","description":"<p>Previous literature on unsupervised learning focused on designing structural\npriors with the aim of learning meaningful features. However, this was done\nwithout considering the description length of the learned representations which\nis a direct and unbiased measure of the model complexity. In this paper, first\nwe introduce the $\\varphi$ metric that evaluates unsupervised models based on\ntheir reconstruction accuracy and the degree of compression of their internal\nrepresentations. We then present and define two activation functions (Identity,\nReLU) as base of reference and three sparse activation functions (top-k\nabsolutes, Extrema-Pool indices, Extrema) as candidate structures that minimize\nthe previously defined $\\varphi$. We lastly present Sparsely Activated Networks\n(SANs) that consist of kernels with shared weights that, during encoding, are\nconvolved with the input and then passed through a sparse activation function.\nDuring decoding, the same weights are convolved with the sparse activation map\nand subsequently the partial reconstructions from each weight are summed to\nreconstruct the input. We compare SANs using the five previously defined\nactivation functions on a variety of datasets (Physionet, UCI-epilepsy, MNIST,\nFMNIST) and show that models that are selected using $\\varphi$ have small\ndescription representation length and consist of interpretable kernels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bizopoulos_P/0/1/0/all/0/1\">Paschalis Bizopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koutsouris_D/0/1/0/all/0/1\">Dimitrios Koutsouris</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Explanation for Deep Metric Learning. (arXiv:1909.12977v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1909.12977","description":"<p>This work explores the visual explanation for deep metric learning and its\napplications. As an important problem for learning representation, metric\nlearning has attracted much attention recently, while the interpretation of\nsuch model is not as well studied as classification. To this end, we propose an\nintuitive idea to show where contributes the most to the overall similarity of\ntwo input images by decomposing the final activation. Instead of only providing\nthe overall activation map of each image, we propose to generate point-to-point\nactivation intensity between two images so that the relationship between\ndifferent regions is uncovered. We show that the proposed framework can be\ndirectly deployed to a large range of metric learning applications and provides\nvaluable information for understanding the model. Furthermore, our experiments\nshow its effectiveness on two potential applications, i.e. cross-view pattern\ndiscovery and interactive retrieval. The source code is available at\n\\url{https://github.com/Jeff-Zilence/Explain_Metric_Learning}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Sijie Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Taojiannan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Scale Open-Set Deep Logo Detection. (arXiv:1911.07440v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1911.07440","description":"<p>We present an open-set logo detection (OSLD) system, which can detect\n(localize and recognize) any number of unseen logo classes without re-training;\nit only requires a small set of canonical logo images for each logo class. We\nachieve this using a two-stage approach: (1) Generic logo detection to detect\ncandidate logo regions in an image. (2) Logo matching for matching the detected\nlogo regions to a set of canonical logo images to recognize them. We also\nintroduce a 'simple deep metric learning' (SDML) framework that outperformed\nmore complicated ensemble and attention models and boosted the logo matching\naccuracy. Furthermore, we constructed an open-set logo detection dataset with\n12.1k logo classes and released it for research purposes. We demonstrate the\neffectiveness of OSLD on our dataset and on the standard Flickr-32 logo\ndataset, outperforming the state-of-the-art open-set and closed-set logo\ndetection methods by a large margin. OSLD is scalable to millions of logo\nclasses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bastan_M/0/1/0/all/0/1\">Muhammet Bastan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hao-Yu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_T/0/1/0/all/0/1\">Tian Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kota_B/0/1/0/all/0/1\">Bhargava Kota</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tek_M/0/1/0/all/0/1\">Mehmet Tek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-Shot Learning with Common Sense Knowledge Graphs. (arXiv:2006.10713v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2006.10713","description":"<p>Zero-shot learning relies on semantic class representations such as\nhand-engineered attributes or learned embeddings to predict classes without any\nlabeled examples. We propose to learn class representations by embedding nodes\nfrom common sense knowledge graphs in a vector space. Common sense knowledge\ngraphs are an untapped source of explicit high-level knowledge that requires\nlittle human effort to apply to a range of tasks. To capture the knowledge in\nthe graph, we introduce ZSL-KG, a general-purpose framework with a novel\ntransformer graph convolutional network (TrGCN) for generating class\nrepresentations. Our proposed TrGCN architecture computes non-linear\ncombinations of node neighbourhoods. Our results show that ZSL-KG improves over\nexisting WordNet-based methods on five out of six zero-shot benchmark datasets\nin language and vision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nayak_N/0/1/0/all/0/1\">Nihal V. Nayak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bach_S/0/1/0/all/0/1\">Stephen H. Bach</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain Adaptation without Source Data. (arXiv:2007.01524v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2007.01524","description":"<p>Domain adaptation assumes that samples from source and target domains are\nfreely accessible during a training phase. However, such an assumption is\nrarely plausible in the real-world and possibly causes data-privacy issues,\nespecially when the label of the source domain can be a sensitive attribute as\nan identifier. To avoid accessing source data that may contain sensitive\ninformation, we introduce Source data-Free Domain Adaptation (SFDA). Our key\nidea is to leverage a pre-trained model from the source domain and\nprogressively update the target model in a self-learning manner. We observe\nthat target samples with lower self-entropy measured by the pre-trained source\nmodel are more likely to be classified correctly. From this, we select the\nreliable samples with the self-entropy criterion and define these as class\nprototypes. We then assign pseudo labels for every target sample based on the\nsimilarity score with class prototypes. Furthermore, to reduce the uncertainty\nfrom the pseudo labeling process, we propose set-to-set distance-based\nfiltering which does not require any tunable hyperparameters. Finally, we train\nthe target model with the filtered pseudo labels with regularization from the\npre-trained source model. Surprisingly, without direct usage of labeled source\nsamples, our PrDA outperforms conventional domain adaptation methods on\nbenchmark datasets. Our code is publicly available at\nhttps://github.com/youngryan1993/SFDA-SourceFreeDA\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Youngeun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_D/0/1/0/all/0/1\">Donghyeon Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1\">Kyeongtak Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panda_P/0/1/0/all/0/1\">Priyadarshini Panda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_S/0/1/0/all/0/1\">Sungeun Hong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Video Representations from Textual Web Supervision. (arXiv:2007.14937v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2007.14937","description":"<p>Videos on the Internet are paired with pieces of text, such as titles and\ndescriptions. This text typically describes the most important content in the\nvideo, such as the objects in the scene and the actions being performed. Based\non this observation, we propose to use text as a method for learning video\nrepresentations. To accomplish this, we propose a data collection process and\nuse it to collect 70M video clips shared publicly on the Internet, and we then\ntrain a model to pair each video with its associated text. We evaluate the\nmodel on several down-stream action recognition tasks, including Kinetics,\nHMDB-51, and UCF-101. We find that this approach is an effective method of\npre-training video representations. Specifically, it outperforms all existing\nmethods for self-supervised and cross-modal video representation learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stroud_J/0/1/0/all/0/1\">Jonathan C. Stroud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhichao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Chen Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jia Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sukthankar_R/0/1/0/all/0/1\">Rahul Sukthankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1\">Cordelia Schmid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ross_D/0/1/0/all/0/1\">David A. Ross</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From noisy point clouds to complete ear shapes: unsupervised pipeline. (arXiv:2008.09831v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2008.09831","description":"<p>Ears are a particularly difficult region of the human face to model, not only\ndue to the non-rigid deformations existing between shapes but also to the\nchallenges in processing the retrieved data. The first step towards obtaining a\ngood model is to have complete scans in correspondence, but these usually\npresent a higher amount of occlusions, noise and outliers when compared to most\nface regions, thus requiring a specific procedure. Therefore, we propose a\ncomplete pipeline taking as input unordered 3D point clouds with the\naforementioned problems, and producing as output a dataset in correspondence,\nwith completion of the missing data. We provide a comparison of several\nstate-of-the-art registration methods and propose a new approach for one of the\nsteps of the pipeline, with better performance for our data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Valdeira_F/0/1/0/all/0/1\">Filipa Valdeira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferreira_R/0/1/0/all/0/1\">Ricardo Ferreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Micheletti_A/0/1/0/all/0/1\">Alessandra Micheletti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soares_C/0/1/0/all/0/1\">Cl&#xe1;udia Soares</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DPN: Detail-Preserving Network with High Resolution Representation for Efficient Segmentation of Retinal Vessels. (arXiv:2009.12053v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2009.12053","description":"<p>Retinal vessels are important biomarkers for many ophthalmological and\ncardiovascular diseases. Hence, it is of great significance to develop\nautomatic models for computer-aided diagnosis. Existing methods, such as U-Net\nfollow the encoder-decoder pipeline, where detailed information is lost in the\nencoder in order to achieve a large field of view. Although spatial detailed\ninformation could be recovered partly in the decoder, while there is noise in\nthe high-resolution feature maps of the encoder. And, we argue this\nencoder-decoder architecture is inefficient for vessel segmentation. In this\npaper, we present the detail-preserving network (DPN), which avoids the\nencoder-decoder pipeline. To preserve detailed information and learn structural\ninformation simultaneously, we designed the detail-preserving block (DP-Block).\nFurther, we stacked eight DP-Blocks together to form the DPN. More importantly,\nthere are no down-sampling operations among these blocks. Therefore, the DPN\ncould maintain a high/full resolution during processing, avoiding the loss of\ndetailed information. To illustrate the effectiveness of DPN, we conducted\nexperiments over three public datasets. Experimental results show, compared to\nstate-of-the-art methods, DPN shows competitive/better performance in terms of\nsegmentation accuracy, segmentation speed, and model size. Specifically, 1) Our\nmethod achieves comparable segmentation performance on the DRIVE, CHASE_DB1,\nand HRF datasets. 2) The segmentation speed of DPN is over 20-160 times faster\nthan other methods on the DRIVE dataset. 3) The number of parameters of DPN is1\naround 120k, far less than all comparison methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Guo_S/0/1/0/all/0/1\">Song Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SeasonDepth: Cross-Season Monocular Depth Prediction Dataset and Benchmark under Multiple Environments. (arXiv:2011.04408v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.04408","description":"<p>Different environments pose a great challenge on the outdoor robust visual\nperception for long-term autonomous driving and the generalization of\nlearning-based algorithms on different environmental effects is still an open\nproblem. Although monocular depth prediction has been well studied recently,\nthere is few work focusing on the robust learning-based depth prediction across\ndifferent environments, e.g., changing illumination and seasons, owing to the\nlack of such a multi-environment real-world dataset and benchmark. To this end,\nthe first cross-season monocular depth prediction dataset and benchmark\nSeasonDepth (available on https://seasondepth.github.io) is built based on CMU\nVisual Localization dataset. To benchmark the depth estimation performance\nunder different environments, we investigate representative and recent\nstate-of-the-art open-source supervised, self-supervised and domain adaptation\ndepth prediction methods from KITTI benchmark using several newly-formulated\nmetrics. Through extensive experimental evaluation on the proposed dataset, the\ninfluence of multiple environments on performance and robustness is analyzed\nboth qualitatively and quantitatively, showing that the long-term monocular\ndepth prediction is still challenging even with fine-tuning. We further give\npromising avenues that self-supervised training and stereo geometry constraint\nhelp to enhance the robustness to changing environments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Hanjiang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Baoquan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Z/0/1/0/all/0/1\">Zhijian Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Ding Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hesheng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continuous Conditional Generative Adversarial Networks: Novel Empirical Losses and Label Input Mechanisms. (arXiv:2011.07466v6 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.07466","description":"<p>This work proposes the continuous conditional generative adversarial network\n(CcGAN), the first generative model for image generation conditional on\ncontinuous, scalar conditions (termed regression labels). Existing conditional\nGANs (cGANs) are mainly designed for categorical conditions (eg, class labels);\nconditioning on regression labels is mathematically distinct and raises two\nfundamental problems:(P1) Since there may be very few (even zero) real images\nfor some regression labels, minimizing existing empirical versions of cGAN\nlosses (aka empirical cGAN losses) often fails in practice;(P2) Since\nregression labels are scalar and infinitely many, conventional label input\nmethods are not applicable. The proposed CcGAN solves the above problems,\nrespectively, by (S1) reformulating existing empirical cGAN losses to be\nappropriate for the continuous scenario; and (S2) proposing a naive label input\n(NLI) method and an improved label input (ILI) method to incorporate regression\nlabels into the generator and the discriminator. The reformulation in (S1)\nleads to two novel empirical discriminator losses, termed the hard vicinal\ndiscriminator loss (HVDL) and the soft vicinal discriminator loss (SVDL)\nrespectively, and a novel empirical generator loss. The error bounds of a\ndiscriminator trained with HVDL and SVDL are derived under mild assumptions in\nthis work. Two new benchmark datasets (RC-49 and Cell-200) and a novel\nevaluation metric (Sliding Fr\\'echet Inception Distance) are also proposed for\nthis continuous scenario. Our experiments on the Circular 2-D Gaussians, RC-49,\nUTKFace, Cell-200, and Steering Angle datasets show that CcGAN is able to\ngenerate diverse, high-quality samples from the image distribution conditional\non a given regression label. Moreover, in these experiments, CcGAN\nsubstantially outperforms cGAN both visually and quantitatively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1\">Xin Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zuheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Welch_W/0/1/0/all/0/1\">William J. Welch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Z. Jane Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Siamese Basis Function Network for Data Efficient Defect Classification in Technical Domains. (arXiv:2012.01338v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.01338","description":"<p>Training Deep Learning Models in technical domains often brings the\nchallenges that although the task is clear, insufficient data for training is\navailable. In this work we propose a novel approach based on the combination of\nSiamese-Networks and Radial-Basis- Function-Networks to perform data-efficient\nclassification without pre-Training by measuring the distance between images in\nsemantic space in a data efficient manner. We develop the models using three\ntechnical datasets, the NEU dataset the BSD dataset as well as the TEX dataset.\nAdditional to the technical domain show the general applicability to classical\ndatasets (cifar10 and MNIST) as well. The approach is tested against state of\nthe art models (Resnet50 and Resnet101) by stepwise reducing the number of\nsamples available for training. The authors show that the proposed approach\noutperforms the state of the art models in the low data regime.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schlagenhauf_T/0/1/0/all/0/1\">Tobias Schlagenhauf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fleischer_J/0/1/0/all/0/1\">J&#xfc;rgen Fleischer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Layer Distillation with Semantic Calibration. (arXiv:2012.03236v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.03236","description":"<p>Knowledge distillation is a technique to enhance the generalization ability\nof a student model by exploiting outputs from a teacher model. Recently,\nfeature-map based variants explore knowledge transfer between manually assigned\nteacher-student pairs in intermediate layers for further improvement. However,\nlayer semantics may vary in different neural networks and semantic mismatch in\nmanual layer associations will lead to performance degeneration due to negative\nregularization. To address this issue, we propose Semantic Calibration for\ncross-layer Knowledge Distillation (SemCKD), which automatically assigns proper\ntarget layers of the teacher model for each student layer with an attention\nmechanism. With a learned attention distribution, each student layer distills\nknowledge contained in multiple teacher layers rather than a specific\nintermediate layer for appropriate cross-layer supervision. We further provide\ntheoretical analysis of the association weights and conduct extensive\nexperiments to demonstrate the effectiveness of our approach. Code is avaliable\nat \\url{https://github.com/DefangChen/SemCKD}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Defang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_J/0/1/0/all/0/1\">Jian-Ping Mei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Can Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yan Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Mining Generation of Lung Cancer Malignancy Models from Chest X-ray Images. (arXiv:2012.05447v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2012.05447","description":"<p>Lung cancer is the leading cause of cancer death and morbidity worldwide.\nMany studies have shown machine learning models to be effective at detecting\nlung nodules from chest X-ray images. However, these techniques have yet to be\nembraced by the medical community due to several practical, ethical, and\nregulatory constraints stemming from the black-box nature of deep learning\nmodels. Additionally, most lung nodules visible on chest X-ray are benign;\ntherefore, the narrow task of computer vision-based lung nodule detection\ncannot be equated to automated lung cancer detection. Addressing both concerns,\nthis study introduces a novel hybrid deep learning and decision tree-based\ncomputer vision model which presents lung cancer malignancy predictions as\ninterpretable decision trees. The deep learning component of this process is\ntrained using a large publicly available dataset on pathological biomarkers\nassociated with lung cancer. These models are then used to inference biomarker\nscores for chest X-ray images from two, independent data sets for which\nmalignancy metadata is available. We mine multi-variate predictive models by\nfitting shallow decision trees to the malignancy stratified datasets and\ninterrogate a range of metrics to determine the best model. Our best decision\ntree model achieves sensitivity and specificity of 86.7% and 80.0% respectively\nwith a positive predictive value of 92.9%. Decision trees mined using this\nmethod may be considered as a starting point for refinement into clinically\nuseful multi-variate lung cancer malignancy models for implementation as a\nworkflow augmentation tool to improve the efficiency of human radiologists.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Horry_M/0/1/0/all/0/1\">Michael J. Horry</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chakraborty_S/0/1/0/all/0/1\">Subrata Chakraborty</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pradhan_B/0/1/0/all/0/1\">Biswajeet Pradhan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Paul_M/0/1/0/all/0/1\">Manoranjan Paul</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gomes_D/0/1/0/all/0/1\">Douglas P. S. Gomes</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ul_Haq_A/0/1/0/all/0/1\">Anwaar Ul-Haq</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Effect of the regularization hyperparameter on deep learning-based segmentation in LGE-MRI. (arXiv:2012.05661v4 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2012.05661","description":"<p>The extent to which the arbitrarily selected L2 regularization hyperparameter\nvalue affects the outcome of semantic segmentation with deep learning is\ndemonstrated. Demonstrations rely on training U-net on small LGE-MRI datasets\nusing the arbitrarily selected L2 regularization values. The remaining\nhyperparameters are to be manually adjusted or tuned only when 10 % of all\nepochs are reached before the training validation accuracy reaches 90%.\nSemantic segmentation with deep learning outcomes are objectively and\nsubjectively evaluated against the manual ground truth segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Rukundo_O/0/1/0/all/0/1\">Olivier Rukundo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluation of deep learning-based myocardial infarction quantification using Segment CMR software. (arXiv:2012.09070v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2012.09070","description":"<p>This work evaluates deep learning-based myocardial infarction (MI)\nquantification using Segment cardiovascular magnetic resonance (CMR) software.\nSegment CMR software incorporates the expectation-maximization, weighted\nintensity, a priori information (EWA) algorithm used to generate the infarct\nscar volume, infarct scar percentage, and microvascular obstruction percentage.\nAlso, Segment CMR software segmentation algorithm is updated with accurate\nsemantic segmentation with U-net for fully automated or deep learning-based MI\nquantification. The direct observation of graphs and the number of infarcted\nand contoured myocardium are two options used to estimate the relationship\nbetween deep learning-based MI quantification and medical expert-based results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Rukundo_O/0/1/0/all/0/1\">Olivier Rukundo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Power Normalizations in Fine-grained Image, Few-shot Image and Graph Classification. (arXiv:2012.13975v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.13975","description":"<p>Power Normalizations (PN) are useful non-linear operators which tackle\nfeature imbalances in classification problems. We study PNs in the deep\nlearning setup via a novel PN layer pooling feature maps. Our layer combines\nthe feature vectors and their respective spatial locations in the feature maps\nproduced by the last convolutional layer of CNN into a positive definite matrix\nwith second-order statistics to which PN operators are applied, forming\nso-called Second-order Pooling (SOP). As the main goal of this paper is to\nstudy Power Normalizations, we investigate the role and meaning of MaxExp and\nGamma, two popular PN functions. To this end, we provide probabilistic\ninterpretations of such element-wise operators and discover surrogates with\nwell-behaved derivatives for end-to-end training. Furthermore, we look at the\nspectral applicability of MaxExp and Gamma by studying Spectral Power\nNormalizations (SPN). We show that SPN on the autocorrelation/covariance matrix\nand the Heat Diffusion Process (HDP) on a graph Laplacian matrix are closely\nrelated, thus sharing their properties. Such a finding leads us to the\nculmination of our work, a fast spectral MaxExp which is a variant of HDP for\ncovariances/autocorrelation matrices. We evaluate our ideas on fine-grained\nrecognition, scene recognition, and material classification, as well as in\nfew-shot learning and graph classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Koniusz_P/0/1/0/all/0/1\">Piotr Koniusz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongguang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tensor Representations for Action Recognition. (arXiv:2012.14371v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.14371","description":"<p>Human actions in video sequences are characterized by the complex interplay\nbetween spatial features and their temporal dynamics. In this paper, we propose\nnovel tensor representations for compactly capturing such higher-order\nrelationships between visual features for the task of action recognition. We\npropose two tensor-based feature representations, viz. (i) sequence\ncompatibility kernel (SCK) and (ii) dynamics compatibility kernel (DCK). SCK\nbuilds on the spatio-temporal correlations between features, whereas DCK\nexplicitly models the action dynamics of a sequence. We also explore\ngeneralization of SCK, coined SCK(+), that operates on subsequences to capture\nthe local-global interplay of correlations, which can incorporate multi-modal\ninputs e.g., skeleton 3D body-joints and per-frame classifier scores obtained\nfrom deep learning models trained on videos. We introduce linearization of\nthese kernels that lead to compact and fast descriptors. We provide experiments\non (i) 3D skeleton action sequences, (ii) fine-grained video sequences, and\n(iii) standard non-fine-grained videos. As our final representations are\ntensors that capture higher-order relationships of features, they relate to\nco-occurrences for robust fine-grained recognition. We use higher-order tensors\nand so-called Eigenvalue Power Normalization (EPN) which have been long\nspeculated to perform spectral detection of higher-order occurrences, thus\ndetecting fine-grained relationships of features rather than merely count\nfeatures in action sequences. We prove that a tensor of order r, built from Z*\ndimensional features, coupled with EPN indeed detects if at least one\nhigher-order occurrence is `projected' into one of its binom(Z*,r) subspaces of\ndim. r represented by the tensor, thus forming a Tensor Power Normalization\nmetric endowed with binom(Z*,r) such `detectors'.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Koniusz_P/0/1/0/all/0/1\">Piotr Koniusz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cherian_A/0/1/0/all/0/1\">Anoop Cherian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mining Data Impressions from Deep Models as Substitute for the Unavailable Training Data. (arXiv:2101.06069v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.06069","description":"<p>Pretrained deep models hold their learnt knowledge in the form of model\nparameters. These parameters act as \"memory\" for the trained models and help\nthem generalize well on unseen data. However, in absence of training data, the\nutility of a trained model is merely limited to either inference or better\ninitialization towards a target task. In this paper, we go further and extract\nsynthetic data by leveraging the learnt model parameters. We dub them \"Data\nImpressions\", which act as proxy to the training data and can be used to\nrealize a variety of tasks. These are useful in scenarios where only the\npretrained models are available and the training data is not shared (e.g., due\nto privacy or sensitivity concerns). We show the applicability of data\nimpressions in solving several computer vision tasks such as unsupervised\ndomain adaptation, continual learning as well as knowledge distillation. We\nalso study the adversarial robustness of lightweight models trained via\nknowledge distillation using these data impressions. Further, we demonstrate\nthe efficacy of data impressions in generating data-free Universal Adversarial\nPerturbations (UAPs) with better fooling rates. Extensive experiments performed\non benchmark datasets demonstrate competitive performance achieved using data\nimpressions in absence of original training data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nayak_G/0/1/0/all/0/1\">Gaurav Kumar Nayak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mopuri_K/0/1/0/all/0/1\">Konda Reddy Mopuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Saksham Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_A/0/1/0/all/0/1\">Anirban Chakraborty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Task-driven Self-supervised Bi-channel Networks for Diagnosis of Breast Cancers with Mammography. (arXiv:2101.06228v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2101.06228","description":"<p>Deep learning can promote the mammography-based computer-aided diagnosis\n(CAD) for breast cancers, but it generally suffers from the small sample size\nproblem. Self-supervised learning (SSL) has shown its effectiveness in medical\nimage analysis with limited training samples. However, the network model\nsometimes cannot be well pre-trained in the conventional SSL framework due to\nthe limitation of the pretext task and fine-tuning mechanism. In this work, a\nTask-driven Self-supervised Bi-channel Networks (TSBN) framework is proposed to\nimprove the performance of classification model the mammography-based CAD. In\nparticular, a new gray-scale image mapping (GSIM) is designed as the pretext\ntask, which embeds the class label information of mammograms into the image\nrestoration task to improve discriminative feature representation. The proposed\nTSBN then innovatively integrates different network architecture, including the\nimage restoration network and the classification network, into a unified SSL\nframework. It jointly trains the bi-channel network models and collaboratively\ntransfers the knowledge from the pretext task network to the downstream task\nnetwork with improved diagnostic accuracy. The proposed TSBN is evaluated on a\npublic INbreast mammogram dataset. The experimental results indicate that it\noutperforms the conventional SSL and multi-task learning algorithms for\ndiagnosis of breast cancers with limited samples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Gong_R/0/1/0/all/0/1\">Ronglin Gong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shi_J/0/1/0/all/0/1\">Jun Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating Attribution Maps with Disentangled Masked Backpropagation. (arXiv:2101.06773v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.06773","description":"<p>Attribution map visualization has arisen as one of the most effective\ntechniques to understand the underlying inference process of Convolutional\nNeural Networks. In this task, the goal is to compute an score for each image\npixel related with its contribution to the final network output. In this paper,\nwe introduce Disentangled Masked Backpropagation (DMBP), a novel gradient-based\nmethod that leverages on the piecewise linear nature of ReLU networks to\ndecompose the model function into different linear mappings. This decomposition\naims to disentangle the positive, negative and nuisance factors from the\nattribution maps by learning a set of variables masking the contribution of\neach filter during back-propagation. A thorough evaluation over standard\narchitectures (ResNet50 and VGG16) and benchmark datasets (PASCAL VOC and\nImageNet) demonstrates that DMBP generates more visually interpretable\nattribution maps than previous approaches. Additionally, we quantitatively show\nthat the maps produced by our method are more consistent with the true\ncontribution of each pixel to the final network output.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ruiz_A/0/1/0/all/0/1\">Adria Ruiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agudo_A/0/1/0/all/0/1\">Antonio Agudo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreno_F/0/1/0/all/0/1\">Francesc Moreno</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reducing bias and increasing utility by federated generative modeling of medical images using a centralized adversary. (arXiv:2101.07235v2 [stat.ML] UPDATED)","link":"http://arxiv.org/abs/2101.07235","description":"<p>We introduce FELICIA (FEderated LearnIng with a CentralIzed Adversary) a\ngenerative mechanism enabling collaborative learning. In particular, we show\nhow a data owner with limited and biased data could benefit from other data\nowners while keeping data from all the sources private. This is a common\nscenario in medical image analysis where privacy legislation prevents data from\nbeing shared outside local premises. FELICIA works for a large family of\nGenerative Adversarial Networks (GAN) architectures including vanilla and\nconditional GANs as demonstrated in this work. We show that by using the\nFELICIA mechanism, a data owner with limited image samples can generate\nhigh-quality synthetic images with high utility while neither data owners has\nto provide access to its data. The sharing happens solely through a central\ndiscriminator that has access limited to synthetic data. Here, utility is\ndefined as classification performance on a real test set. We demonstrate these\nbenefits on several realistic healthcare scenarions using benchmark image\ndatasets (MNIST, CIFAR-10) as well as on medical images for the task of skin\nlesion classification. With multiple experiments, we show that even in the\nworst cases, combining FELICIA with real data gracefully achieves performance\non par with real data while most results significantly improves the utility.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Rajotte_J/0/1/0/all/0/1\">Jean-Francois Rajotte</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Mukherjee_S/0/1/0/all/0/1\">Sumit Mukherjee</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Robinson_C/0/1/0/all/0/1\">Caleb Robinson</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ortiz_A/0/1/0/all/0/1\">Anthony Ortiz</a>, <a href=\"http://arxiv.org/find/stat/1/au:+West_C/0/1/0/all/0/1\">Christopher West</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ferres_J/0/1/0/all/0/1\">Juan Lavista Ferres</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ng_R/0/1/0/all/0/1\">Raymond T Ng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Progressive Co-Attention Network for Fine-grained Visual Classification. (arXiv:2101.08527v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.08527","description":"<p>Fine-grained visual classification aims to recognize images belonging to\nmultiple sub-categories within a same category. It is a challenging task due to\nthe inherently subtle variations among highly-confused categories. Most\nexisting methods only take an individual image as input, which may limit the\nability of models to recognize contrastive clues from different images. In this\npaper, we propose an effective method called progressive co-attention network\n(PCA-Net) to tackle this problem. Specifically, we calculate the channel-wise\nsimilarity by encouraging interaction between the feature channels within\nsame-category image pairs to capture the common discriminative features.\nConsidering that complementary information is also crucial for recognition, we\nerase the prominent areas enhanced by the channel interaction to force the\nnetwork to focus on other discriminative regions. The proposed model has\nachieved competitive results on three fine-grained visual classification\nbenchmark datasets: CUB-200-2011, Stanford Cars, and FGVC Aircraft.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_D/0/1/0/all/0/1\">Dongliang Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhanyu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jun Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Global to Local Double Embedding Method for Multi-person Pose Estimation. (arXiv:2102.07318v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.07318","description":"<p>Multi-person pose estimation is a fundamental and challenging problem to many\ncomputer vision tasks. Most existing methods can be broadly categorized into\ntwo classes: top-down and bottom-up methods. Both of the two types of methods\ninvolve two stages, namely, person detection and joints detection.\nConventionally, the two stages are implemented separately without considering\ntheir interactions between them, and this may inevitably cause some issue\nintrinsically. In this paper, we present a novel method to simplify the\npipeline by implementing person detection and joints detection simultaneously.\nWe propose a Double Embedding (DE) method to complete the multi-person pose\nestimation task in a global-to-local way. DE consists of Global Embedding (GE)\nand Local Embedding (LE). GE encodes different person instances and processes\ninformation covering the whole image and LE encodes the local limbs\ninformation. GE functions for the person detection in top-down strategy while\nLE connects the rest joints sequentially which functions for joint grouping and\ninformation processing in A bottom-up strategy. Based on LE, we design the\nMutual Refine Machine (MRM) to reduce the prediction difficulty in complex\nscenarios. MRM can effectively realize the information communicating between\nkeypoints and further improve the accuracy. We achieve the competitive results\non benchmarks MSCOCO, MPII and CrowdPose, demonstrating the effectiveness and\ngeneralization ability of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yiming Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiaxin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yan Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_H/0/1/0/all/0/1\">Hua-Liang Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepCervix: A Deep Learning-based Framework for the Classification of Cervical Cells Using Hybrid Deep Feature Fusion Techniques. (arXiv:2102.12191v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2102.12191","description":"<p>Cervical cancer, one of the most common fatal cancers among women, can be\nprevented by regular screening to detect any precancerous lesions at early\nstages and treat them. Pap smear test is a widely performed screening technique\nfor early detection of cervical cancer, whereas this manual screening method\nsuffers from high false-positive results because of human errors. To improve\nthe manual screening practice, machine learning (ML) and deep learning (DL)\nbased computer-aided diagnostic (CAD) systems have been investigated widely to\nclassify cervical pap cells. Most of the existing researches require\npre-segmented images to obtain good classification results, whereas accurate\ncervical cell segmentation is challenging because of cell clustering. Some\nstudies rely on handcrafted features, which cannot guarantee the classification\nstage's optimality. Moreover, DL provides poor performance for a multiclass\nclassification task when there is an uneven distribution of data, which is\nprevalent in the cervical cell dataset. This investigation has addressed those\nlimitations by proposing DeepCervix, a hybrid deep feature fusion (HDFF)\ntechnique based on DL to classify the cervical cells accurately. Our proposed\nmethod uses various DL models to capture more potential information to enhance\nclassification performance. Our proposed HDFF method is tested on the publicly\navailable SIPAKMED dataset and compared the performance with base DL models and\nthe LF method. For the SIPAKMED dataset, we have obtained the state-of-the-art\nclassification accuracy of 99.85%, 99.38%, and 99.14% for 2-class, 3-class, and\n5-class classification. Moreover, our method is tested on the Herlev dataset\nand achieves an accuracy of 98.32% for binary class and 90.32% for 7-class\nclassification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Rahaman_M/0/1/0/all/0/1\">Md Mamunur Rahaman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_C/0/1/0/all/0/1\">Chen Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yao_Y/0/1/0/all/0/1\">Yudong Yao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kulwa_F/0/1/0/all/0/1\">Frank Kulwa</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_X/0/1/0/all/0/1\">Xiangchen Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1\">Xiaoyan Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Q/0/1/0/all/0/1\">Qian Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Digital Peter: Dataset, Competition and Handwriting Recognition Methods. (arXiv:2103.09354v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.09354","description":"<p>This paper presents a new dataset of Peter the Great's manuscripts and\ndescribes a segmentation procedure that converts initial images of documents\ninto the lines. The new dataset may be useful for researchers to train\nhandwriting text recognition models as a benchmark for comparing different\nmodels. It consists of 9 694 images and text files corresponding to lines in\nhistorical documents. The open machine learning competition Digital Peter was\nheld based on the considered dataset. The baseline solution for this\ncompetition as well as more advanced methods on handwritten text recognition\nare described in the article. Full dataset and all code are publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Potanin_M/0/1/0/all/0/1\">Mark Potanin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dimitrov_D/0/1/0/all/0/1\">Denis Dimitrov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shonenkov_A/0/1/0/all/0/1\">Alex Shonenkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bataev_V/0/1/0/all/0/1\">Vladimir Bataev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karachev_D/0/1/0/all/0/1\">Denis Karachev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Novopoltsev_M/0/1/0/all/0/1\">Maxim Novopoltsev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating Annotated Training Data for 6D Object Pose Estimation in Operational Environments with Minimal User Interaction. (arXiv:2103.09696v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2103.09696","description":"<p>Recently developed deep neural networks achieved state-of-the-art results in\nthe subject of 6D object pose estimation for robot manipulation. However, those\nsupervised deep learning methods require expensive annotated training data.\nCurrent methods for reducing those costs frequently use synthetic data from\nsimulations, but rely on expert knowledge and suffer from the \"domain gap\" when\nshifting to the real world. Here, we present a proof of concept for a novel\napproach of autonomously generating annotated training data for 6D object pose\nestimation. This approach is designed for learning new objects in operational\nenvironments while requiring little interaction and no expertise on the part of\nthe user. We evaluate our autonomous data generation approach in two grasping\nexperiments, where we archive a similar grasping success rate as related work\non a non autonomously generated data set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Koch_P/0/1/0/all/0/1\">Paul Koch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schluter_M/0/1/0/all/0/1\">Marian Schl&#xfc;ter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thill_S/0/1/0/all/0/1\">Serge Thill</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distilling Virtual Examples for Long-tailed Recognition. (arXiv:2103.15042v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.15042","description":"<p>We tackle the long-tailed visual recognition problem from the knowledge\ndistillation perspective by proposing a Distill the Virtual Examples (DiVE)\nmethod. Specifically, by treating the predictions of a teacher model as virtual\nexamples, we prove that distilling from these virtual examples is equivalent to\nlabel distribution learning under certain constraints. We show that when the\nvirtual example distribution becomes flatter than the original input\ndistribution, the under-represented tail classes will receive significant\nimprovements, which is crucial in long-tailed recognition. The proposed DiVE\nmethod can explicitly tune the virtual example distribution to become flat.\nExtensive experiments on three benchmark datasets, including the large-scale\niNaturalist ones, justify that the proposed DiVE method can significantly\noutperform state-of-the-art methods. Furthermore, additional analyses and\nexperiments verify the virtual example interpretation, and demonstrate the\neffectiveness of tailored designs in DiVE for long-tailed problems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yin-Yin He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jianxin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xiu-Shen Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Busy-Quiet Video Disentangling for Video Classification. (arXiv:2103.15584v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.15584","description":"<p>In video data, busy motion details from moving regions are conveyed within a\nspecific frequency bandwidth in the frequency domain. Meanwhile, the rest of\nthe frequencies of video data are encoded with quiet information containing\nsubstantial redundancy, which causes low efficiency for video models that take\nas input raw RGB frames. In this paper, we consider that busy and quiet\nspatio-temporal regions require different computational resources. We design a\ntrainable Motion Band-Pass Module (MBPM) for separating busy information from\nquiet information in raw video data. By representing the quiet information with\nlower resolution, we can increase the efficiency of video data processing. By\nembedding the MBPM into a two-pathway CNN architecture, we define a Busy-Quiet\nNet (BQN). The efficiency of BQN is determined by avoiding redundancy in the\nfeature space processed by the two pathways: one operates on quiet features of\nlow-resolution, while the other operates on busy features. The proposed BQN\noutperforms many recent video processing models on Something-Something V1,\nKinetics400, UCF101 and HMDB51.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Guoxi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bors_A/0/1/0/all/0/1\">Adrian G. Bors</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Teacher-Student Adversarial Depth Hallucination to Improve Face Recognition. (arXiv:2104.02424v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.02424","description":"<p>We present the Teacher-Student Generative Adversarial Network (TS-GAN) to\ngenerate depth images from single RGB images in order to boost the performance\nof face recognition systems. For our method to generalize well across unseen\ndatasets, we design two components in the architecture, a teacher and a\nstudent. The teacher, which itself consists of a generator and a discriminator,\nlearns a latent mapping between input RGB and paired depth images in a\nsupervised fashion. The student, which consists of two generators (one shared\nwith the teacher) and a discriminator, learns from new RGB data with no\navailable paired depth information, for improved generalization. The fully\ntrained shared generator can then be used in runtime to hallucinate depth from\nRGB for downstream applications such as face recognition. We perform rigorous\nexperiments to show the superiority of TS-GAN over other methods in generating\nsynthetic depth images. Moreover, face recognition experiments demonstrate that\nour hallucinated depth along with the input RGB images boosts performance\nacross various architectures when compared to a single RGB modality by average\nvalues of +1.2%, +2.6%, and +2.6% for IIIT-D, EURECOM, and LFW datasets\nrespectively. We make our implementation public at:\nhttps://github.com/hardik-uppal/teacher-student-gan.git.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Uppal_H/0/1/0/all/0/1\">Hardik Uppal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sepas_Moghaddam_A/0/1/0/all/0/1\">Alireza Sepas-Moghaddam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Greenspan_M/0/1/0/all/0/1\">Michael Greenspan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Etemad_A/0/1/0/all/0/1\">Ali Etemad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D Shape Generation and Completion through Point-Voxel Diffusion. (arXiv:2104.03670v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.03670","description":"<p>We propose a novel approach for probabilistic generative modeling of 3D\nshapes. Unlike most existing models that learn to deterministically translate a\nlatent vector to a shape, our model, Point-Voxel Diffusion (PVD), is a unified,\nprobabilistic formulation for unconditional shape generation and conditional,\nmulti-modal shape completion. PVD marries denoising diffusion models with the\nhybrid, point-voxel representation of 3D shapes. It can be viewed as a series\nof denoising steps, reversing the diffusion process from observed point cloud\ndata to Gaussian noise, and is trained by optimizing a variational lower bound\nto the (conditional) likelihood function. Experiments demonstrate that PVD is\ncapable of synthesizing high-fidelity shapes, completing partial point clouds,\nand generating multiple completion results from single-view depth scans of real\nobjects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Linqi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yilun Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiajun Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Geometry-Free View Synthesis: Transformers and no 3D Priors. (arXiv:2104.07652v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.07652","description":"<p>Is a geometric model required to synthesize novel views from a single image?\nBeing bound to local convolutions, CNNs need explicit 3D biases to model\ngeometric transformations. In contrast, we demonstrate that a transformer-based\nmodel can synthesize entirely novel views without any hand-engineered 3D\nbiases. This is achieved by (i) a global attention mechanism for implicitly\nlearning long-range 3D correspondences between source and target views, and\n(ii) a probabilistic formulation necessary to capture the ambiguity inherent in\npredicting novel views from a single image, thereby overcoming the limitations\nof previous approaches that are restricted to relatively small viewpoint\nchanges. We evaluate various ways to integrate 3D priors into a transformer\narchitecture. However, our experiments show that no such geometric priors are\nrequired and that the transformer is capable of implicitly learning 3D\nrelationships between images. Furthermore, this approach outperforms the state\nof the art in terms of visual quality while covering the full distribution of\npossible realizations. Code is available at https://git.io/JOnwn\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rombach_R/0/1/0/all/0/1\">Robin Rombach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Esser_P/0/1/0/all/0/1\">Patrick Esser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ommer_B/0/1/0/all/0/1\">Bj&#xf6;rn Ommer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"High-Resolution Optical Flow from 1D Attention and Correlation. (arXiv:2104.13918v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.13918","description":"<p>Optical flow is inherently a 2D search problem, and thus the computational\ncomplexity grows quadratically with respect to the search window, making large\ndisplacements matching infeasible for high-resolution images. In this paper, we\ntake inspiration from Transformers and propose a new method for high-resolution\noptical flow estimation with significantly less computation. Specifically, a 1D\nattention operation is first applied in the vertical direction of the target\nimage, and then a simple 1D correlation in the horizontal direction of the\nattended image is able to achieve 2D correspondence modeling effect. The\ndirections of attention and correlation can also be exchanged, resulting in two\n3D cost volumes that are concatenated for optical flow estimation. The novel 1D\nformulation empowers our method to scale to very high-resolution input images\nwhile maintaining competitive performance. Extensive experiments on Sintel,\nKITTI and real-world 4K ($2160 \\times 3840$) resolution images demonstrated the\neffectiveness and superiority of our proposed method. Code and models are\navailable at \\url{https://github.com/haofeixu/flow1d}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Haofei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jiaolong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jianfei Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Juyong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_X/0/1/0/all/0/1\">Xin Tong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepMultiCap: Performance Capture of Multiple Characters Using Sparse Multiview Cameras. (arXiv:2105.00261v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.00261","description":"<p>We propose DeepMultiCap, a novel method for multi-person performance capture\nusing sparse multi-view cameras. Our method can capture time varying surface\ndetails without the need of using pre-scanned template models. To tackle with\nthe serious occlusion challenge for close interacting scenes, we combine a\nrecently proposed pixel-aligned implicit function with parametric model for\nrobust reconstruction of the invisible surface areas. An effective\nattention-aware module is designed to obtain the fine-grained geometry details\nfrom multi-view images, where high-fidelity results can be generated. In\naddition to the spatial attention method, for video inputs, we further propose\na novel temporal fusion method to alleviate the noise and temporal\ninconsistencies for moving character reconstruction. For quantitative\nevaluation, we contribute a high quality multi-person dataset, MultiHuman,\nwhich consists of 150 static scenes with different levels of occlusions and\nground truth 3D human models. Experimental results demonstrate the\nstate-of-the-art performance of our method and the well generalization to real\nmultiview video data, which outperforms the prior works by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yang Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_R/0/1/0/all/0/1\">Ruizhi Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuxiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zerong Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Q/0/1/0/all/0/1\">Qionghai Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yebin Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Surveilling Surveillance: Estimating the Prevalence of Surveillance Cameras with Street View Data. (arXiv:2105.01764v2 [cs.CY] UPDATED)","link":"http://arxiv.org/abs/2105.01764","description":"<p>The use of video surveillance in public spaces -- both by government agencies\nand by private citizens -- has attracted considerable attention in recent\nyears, particularly in light of rapid advances in face-recognition technology.\nBut it has been difficult to systematically measure the prevalence and\nplacement of cameras, hampering efforts to assess the implications of\nsurveillance on privacy and public safety. Here we present a novel approach for\nestimating the spatial distribution of surveillance cameras: applying computer\nvision algorithms to large-scale street view image data. Specifically, we build\na camera detection model and apply it to 1.6 million street view images sampled\nfrom 10 large U.S. cities and 6 other major cities around the world, with\npositive model detections verified by human experts. After adjusting for the\nestimated recall of our model, and accounting for the spatial coverage of our\nsampled images, we are able to estimate the density of surveillance cameras\nvisible from the road. Across the 16 cities we consider, the estimated number\nof surveillance cameras per linear kilometer ranges from 0.1 (in Seattle) to\n0.9 (in Seoul). In a detailed analysis of the 10 U.S. cities, we find that\ncameras are concentrated in commercial, industrial, and mixed zones, and in\nneighborhoods with higher shares of non-white residents -- a pattern that\npersists even after adjusting for land use. These results help inform ongoing\ndiscussions on the use of surveillance technology, including its potential\ndisparate impacts on communities of color.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sheng_H/0/1/0/all/0/1\">Hao Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_K/0/1/0/all/0/1\">Keniel Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goel_S/0/1/0/all/0/1\">Sharad Goel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DARNet: Dual-Attention Residual Network for Automatic Diagnosis of COVID-19 via CT Images. (arXiv:2105.06779v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2105.06779","description":"<p>The ongoing global pandemic of Coronavirus Disease 2019 (COVID-19) poses a\nserious threat to public health and the economy. Rapid and accurate diagnosis\nof COVID-19 is crucial to prevent the further spread of the disease and reduce\nits mortality. Chest Computed tomography (CT) is an effective tool for the\nearly diagnosis of lung diseases including pneumonia. However, detecting\nCOVID-19 from CT is demanding and prone to human errors as some early-stage\npatients may have negative findings on images. Recently, many deep learning\nmethods have achieved impressive performance in this regard. Despite their\neffectiveness, most of these methods underestimate the rich spatial information\npreserved in the 3D structure or suffer from the propagation of errors. To\naddress this problem, we propose a Dual-Attention Residual Network (DARNet) to\nautomatically identify COVID-19 from other common pneumonia (CP) and healthy\npeople using 3D chest CT images. Specifically, we design a dual-attention\nmodule consisting of channel-wise attention and depth-wise attention\nmechanisms. The former is utilized to enhance channel independence, while the\nlatter is developed to recalibrate the depth-level features. Then, we integrate\nthem in a unified manner to extract and refine the features at different levels\nto further improve the diagnostic performance. We evaluate DARNet on a large\npublic CT dataset and obtain superior performance. Besides, the ablation study\nand visualization analysis prove the effectiveness and interpretability of the\nproposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Shi_J/0/1/0/all/0/1\">Jun Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yi_H/0/1/0/all/0/1\">Huite Yi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ruan_S/0/1/0/all/0/1\">Shulan Ruan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1\">Zhaohui Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hao_X/0/1/0/all/0/1\">Xiaoyu Hao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+An_H/0/1/0/all/0/1\">Hong An</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wei_W/0/1/0/all/0/1\">Wei Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Multi-Modality Registration Network based on Spatially Encoded Gradient Information. (arXiv:2105.07392v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2105.07392","description":"<p>Multi-modality medical images can provide relevant or complementary\ninformation for a target (organ, tumor or tissue). Registering multi-modality\nimages to a common space can fuse these comprehensive information, and bring\nconvenience for clinical application. Recently, neural networks have been\nwidely investigated to boost registration methods. However, it is still\nchallenging to develop a multi-modality registration network due to the lack of\nrobust criteria for network training. In this work, we propose a multi-modality\nregistration network (MMRegNet), which can perform registration between\nmulti-modality images. Meanwhile, we present spatially encoded gradient\ninformation to train MMRegNet in an unsupervised manner. The proposed network\nwas evaluated on MM-WHS 2017. Results show that MMRegNet can achieve promising\nperformance for left ventricle cardiac registration tasks. Meanwhile, to\ndemonstrate the versatility of MMRegNet, we further evaluate the method with a\nliver dataset from CHAOS 2019. Source code will be released\npublicly\\footnote{https://github.com/NanYoMy/mmregnet} once the manuscript is\naccepted.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ding_W/0/1/0/all/0/1\">Wangbin Ding</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhuang_X/0/1/0/all/0/1\">Xiahai Zhuang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_L/0/1/0/all/0/1\">Liqin Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cherry-Picking Gradients: Learning Low-Rank Embeddings of Visual Data via Differentiable Cross-Approximation. (arXiv:2105.14250v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.14250","description":"<p>We propose an end-to-end trainable framework that processes large-scale\nvisual data tensors by looking at a fraction of their entries only. Our method\ncombines a neural network encoder with a tensor train decomposition to learn a\nlow-rank latent encoding, coupled with cross-approximation (CA) to learn the\nrepresentation through a subset of the original samples. CA is an adaptive\nsampling algorithm that is native to tensor decompositions and avoids working\nwith the full high-resolution data explicitly. Instead, it actively selects\nlocal representative samples that we fetch out-of-core and on-demand. The\nrequired number of samples grows only logarithmically with the size of the\ninput. Our implicit representation of the tensor in the network enables\nprocessing large grids that could not be otherwise tractable in their\nuncompressed form. The proposed approach is particularly useful for large-scale\nmultidimensional grid data (e.g., 3D tomography), and for tasks that require\ncontext over a large receptive field (e.g., predicting the medical condition of\nentire organs). The code is available at https://github.com/aelphy/c-pic.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Usvyatsov_M/0/1/0/all/0/1\">Mikhail Usvyatsov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makarova_A/0/1/0/all/0/1\">Anastasia Makarova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ballester_Ripoll_R/0/1/0/all/0/1\">Rafael Ballester-Ripoll</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rakhuba_M/0/1/0/all/0/1\">Maxim Rakhuba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krause_A/0/1/0/all/0/1\">Andreas Krause</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schindler_K/0/1/0/all/0/1\">Konrad Schindler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding and Evaluating Racial Biases in Image Captioning. (arXiv:2106.08503v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.08503","description":"<p>Image captioning is an important task for benchmarking visual reasoning and\nfor enabling accessibility for people with vision impairments. However, as in\nmany machine learning settings, social biases can influence image captioning in\nundesirable ways. In this work, we study bias propagation pathways within image\ncaptioning, focusing specifically on the COCO dataset. Prior work has analyzed\ngender bias in captions using automatically-derived gender labels; here we\nexamine racial and intersectional biases using manual annotations. Our first\ncontribution is in annotating the perceived gender and skin color of 28,315 of\nthe depicted people after obtaining IRB approval. Using these annotations, we\ncompare racial biases present in both manual and automatically-generated image\ncaptions. We demonstrate differences in caption performance, sentiment, and\nword choice between images of lighter versus darker-skinned people. Further, we\nfind the magnitude of these differences to be greater in modern captioning\nsystems compared to older ones, thus leading to concerns that without proper\nconsideration and mitigation these differences will only become increasingly\nprevalent. Code and data is available at\nhttps://princetonvisualai.github.io/imagecaptioning-bias .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Dora Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1\">Angelina Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Russakovsky_O/0/1/0/all/0/1\">Olga Russakovsky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SIFT Matching by Context Exposed. (arXiv:2106.09584v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.09584","description":"<p>This paper investigates how to step up local image descriptor matching by\nexploiting matching context information. Two main contexts are identified,\noriginated respectively from the descriptor space and from the keypoint space.\nThe former is generally used to design the actual matching strategy while the\nlatter to filter matches according to the local spatial consistency. On this\nbasis, a new matching strategy and a novel local spatial filter, named\nrespectively blob matching and Delaunay Triangulation Matching (DTM) are\ndevised. Blob matching provides a general matching framework by merging\ntogether several strategies, including rank-based pre-filtering as well as\nmany-to-many and symmetric matching, enabling to achieve a global improvement\nupon each individual strategy. DTM alternates between Delaunay triangulation\ncontractions and expansions to figure out and adjust keypoint neighborhood\nconsistency. Experimental evaluation shows that DTM is comparable or better\nthan the state-of-the-art in terms of matching accuracy and robustness.\nEvaluation is carried out according to a new benchmark devised for analyzing\nthe matching pipeline in terms of correct correspondences on both planar and\nnon-planar scenes, including several state-of-the-art methods as well as the\ncommon SIFT matching approach for reference. This evaluation can be of\nassistance for future research in this field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bellavia_F/0/1/0/all/0/1\">Fabio Bellavia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Transferability of Adversarial Patches on Face Recognition with Generative Models. (arXiv:2106.15058v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.15058","description":"<p>Face recognition is greatly improved by deep convolutional neural networks\n(CNNs). Recently, these face recognition models have been used for identity\nauthentication in security sensitive applications. However, deep CNNs are\nvulnerable to adversarial patches, which are physically realizable and\nstealthy, raising new security concerns on the real-world applications of these\nmodels. In this paper, we evaluate the robustness of face recognition models\nusing adversarial patches based on transferability, where the attacker has\nlimited accessibility to the target models. First, we extend the existing\ntransfer-based attack techniques to generate transferable adversarial patches.\nHowever, we observe that the transferability is sensitive to initialization and\ndegrades when the perturbation magnitude is large, indicating the overfitting\nto the substitute models. Second, we propose to regularize the adversarial\npatches on the low dimensional data manifold. The manifold is represented by\ngenerative models pre-trained on legitimate human face images. Using face-like\nfeatures as adversarial perturbations through optimization on the manifold, we\nshow that the gaps between the responses of substitute models and the target\nmodels dramatically decrease, exhibiting a better transferability. Extensive\ndigital world experiments are conducted to demonstrate the superiority of the\nproposed method in the black-box setting. We apply the proposed method in the\nphysical world as well.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1\">Zihao Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1\">Chilin Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yinpeng Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1\">Wei Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaolu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jun Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Orthonormal Product Quantization Network for Scalable Face Image Retrieval. (arXiv:2107.00327v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.00327","description":"<p>Current deep quantization methods that produce binary code representations\nfor efficient image retrieval mostly learn codewords from data. They rarely\ninvestigate the effect of their inherent distribution on the quantization and\nthe learning metrics presently used are insufficient for face image retrieval\ntask. To address this, this paper integrates product quantization into an\nend-to-end deep learning framework to retrieve face images. We propose a novel\nscheme that uses predefined orthonormal vectors as codewords, to enhance the\nquantization informativeness and reduce redundancy in the codewords. A tailored\nloss function maximizes discriminability among identities in each quantization\nsubspace for both the quantized and original features. An entropy-based\nregularization term is imposed to reduce the quantization error. Experiments\nwere conducted on three commonly-used datasets for both single- and\ncross-domain retrieval. The proposed method outperformed all the deep\nhashing/quantization methods it was compared with under both settings. We\nobserve that the proposed orthonormal codewords consistently improved both\nmodels' standard retrieval performance and generalization ability. Therefore,\nthe proposed method is more suitable for scalable face image retrieval than\ndeep hashing methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Ming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhe_X/0/1/0/all/0/1\">Xuefei Zhe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1\">Hong Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Crowdsourcing Evaluation of Saliency-based XAI Methods. (arXiv:2107.00456v2 [cs.HC] UPDATED)","link":"http://arxiv.org/abs/2107.00456","description":"<p>Understanding the reasons behind the predictions made by deep neural networks\nis critical for gaining human trust in many important applications, which is\nreflected in the increasing demand for explainability in AI (XAI) in recent\nyears. Saliency-based feature attribution methods, which highlight important\nparts of images that contribute to decisions by classifiers, are often used as\nXAI methods, especially in the field of computer vision. In order to compare\nvarious saliency-based XAI methods quantitatively, several approaches for\nautomated evaluation schemes have been proposed; however, there is no guarantee\nthat such automated evaluation metrics correctly evaluate explainability, and a\nhigh rating by an automated evaluation scheme does not necessarily mean a high\nexplainability for humans. In this study, instead of the automated evaluation,\nwe propose a new human-based evaluation scheme using crowdsourcing to evaluate\nXAI methods. Our method is inspired by a human computation game, \"Peek-a-boom\",\nand can efficiently compare different XAI methods by exploiting the power of\ncrowds. We evaluate the saliency maps of various XAI methods on two datasets\nwith automated and crowd-based evaluation schemes. Our experiments show that\nthe result of our crowd-based evaluation scheme is different from those of\nautomated evaluation schemes. In addition, we regard the crowd-based evaluation\nresults as ground truths and provide a quantitative performance measure to\ncompare different automated evaluation schemes. We also discuss the impact of\ncrowd workers on the results and show that the varying ability of crowd workers\ndoes not significantly impact the results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiaotian Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tolmachev_A/0/1/0/all/0/1\">Arseny Tolmachev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamamoto_T/0/1/0/all/0/1\">Tatsuya Yamamoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takeuchi_K/0/1/0/all/0/1\">Koh Takeuchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okajima_S/0/1/0/all/0/1\">Seiji Okajima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takebayashi_T/0/1/0/all/0/1\">Tomoyoshi Takebayashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maruhashi_K/0/1/0/all/0/1\">Koji Maruhashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kashima_H/0/1/0/all/0/1\">Hisashi Kashima</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Contrastive Learning with Hard Negative Sampling for Self-supervised Point Cloud Learning. (arXiv:2107.01886v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.01886","description":"<p>Point clouds have attracted increasing attention. Significant progress has\nbeen made in methods for point cloud analysis, which often requires costly\nhuman annotation as supervision. To address this issue, we propose a novel\nself-contrastive learning for self-supervised point cloud representation\nlearning, aiming to capture both local geometric patterns and nonlocal semantic\nprimitives based on the nonlocal self-similarity of point clouds. The\ncontributions are two-fold: on the one hand, instead of contrasting among\ndifferent point clouds as commonly employed in contrastive learning, we exploit\nself-similar point cloud patches within a single point cloud as positive\nsamples and otherwise negative ones to facilitate the task of contrastive\nlearning. On the other hand, we actively learn hard negative samples that are\nclose to positive samples for discriminative feature learning. Experimental\nresults show that the proposed method achieves state-of-the-art performance on\nwidely used benchmark datasets for self-supervised point cloud segmentation and\ntransfer learning for classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1\">Bi&#x27;an Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xiang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Wei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Sampling Strategies for Unsupervised Person Re-identification. (arXiv:2107.03024v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.03024","description":"<p>Unsupervised person re-identification (re-ID) remains a challenging task.\nWhile extensive research has focused on the framework design or loss function,\nwe show in this paper that sampling strategy plays an equally important role.\nWe analyze the reasons for differences in performance between various sampling\nstrategies under the same framework and loss function. We suggest that\ndeteriorated over-fitting is an important factor causing poor performance, and\nenhancing statistical stability can rectify this issue. Inspired by that, a\nsimple yet effective approach is proposed, known as group sampling, which\ngathers groups of samples from the same class into a mini-batch. The model is\nthereby trained using normalized group samples, which helps to alleviate the\neffects associated with a single sample. Group sampling updates the pipeline of\npseudo label generation by guaranteeing that samples are more efficiently\ndivided into the correct classes. Group sampling regulates the representation\nlearning process, which enhances statistical stability for feature\nrepresentation in a progressive fashion. Qualitative and quantitative\nexperiments on Market-1501, DukeMTMC-reID, and MSMT17 show that group sampling\nimproves upon state-of-the-art methods by between 3.3%~6.1%. Code has been\navailable at https://github.com/ucas-vg/GroupSampling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xumeng Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xuehui Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guorong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jian Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_G/0/1/0/all/0/1\">Gang Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1\">Qixiang Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_J/0/1/0/all/0/1\">Jianbin Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1\">Zhenjun Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A stepped sampling method for video detection using LSTM. (arXiv:2107.08471v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2107.08471","description":"<p>Artificial neural networks that simulate human achieves great successes. From\nthe perspective of simulating human memory method, we propose a stepped sampler\nbased on the \"repeated input\". We repeatedly inputted data to the LSTM model\nstepwise in a batch. The stepped sampler is used to strengthen the ability of\nfusing the temporal information in LSTM. We tested the stepped sampler on the\nLSTM built-in in PyTorch. Compared with the traditional sampler of PyTorch,\nsuch as sequential sampler, batch sampler, the training loss of the proposed\nstepped sampler converges faster in the training of the model, and the training\nloss after convergence is more stable. Meanwhile, it can maintain a higher test\naccuracy. We quantified the algorithm of the stepped sampler. We assume that,\nthe artificial neural networks have human-like characteristics, and human\nlearning method could be used for machine learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dengshan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rujing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1\">Chengjun Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Non-binary deep transfer learning for image classification. (arXiv:2107.08585v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.08585","description":"<p>The current standard for a variety of computer vision tasks using smaller\nnumbers of labelled training examples is to fine-tune from weights pre-trained\non a large image classification dataset such as ImageNet. The application of\ntransfer learning and transfer learning methods tends to be rigidly binary. A\nmodel is either pre-trained or not pre-trained. Pre-training a model either\nincreases performance or decreases it, the latter being defined as negative\ntransfer. Application of L2-SP regularisation that decays the weights towards\ntheir pre-trained values is either applied or all weights are decayed towards\n0. This paper re-examines these assumptions. Our recommendations are based on\nextensive empirical evaluation that demonstrate the application of a non-binary\napproach to achieve optimal results. (1) Achieving best performance on each\nindividual dataset requires careful adjustment of various transfer learning\nhyperparameters not usually considered, including number of layers to transfer,\ndifferent learning rates for different layers and different combinations of\nL2SP and L2 regularization. (2) Best practice can be achieved using a number of\nmeasures of how well the pre-trained weights fit the target dataset to guide\noptimal hyperparameters. We present methods for non-binary transfer learning\nincluding combining L2SP and L2 regularization and performing non-traditional\nfine-tuning hyperparameter searches. Finally we suggest heuristics for\ndetermining the optimal transfer learning hyperparameters. The benefits of\nusing a non-binary approach are supported by final results that come close to\nor exceed state of the art performance on a variety of tasks that have\ntraditionally been more difficult for transfer learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Plested_J/0/1/0/all/0/1\">Jo Plested</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xuyang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gedeon_T/0/1/0/all/0/1\">Tom Gedeon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rank & Sort Loss for Object Detection and Instance Segmentation. (arXiv:2107.11669v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.11669","description":"<p>We propose Rank &amp; Sort (RS) Loss, a ranking-based loss function to train deep\nobject detection and instance segmentation methods (i.e. visual detectors). RS\nLoss supervises the classifier, a sub-network of these methods, to rank each\npositive above all negatives as well as to sort positives among themselves with\nrespect to (wrt.) their localisation qualities (e.g. Intersection-over-Union -\nIoU). To tackle the non-differentiable nature of ranking and sorting, we\nreformulate the incorporation of error-driven update with backpropagation as\nIdentity Update, which enables us to model our novel sorting error among\npositives. With RS Loss, we significantly simplify training: (i) Thanks to our\nsorting objective, the positives are prioritized by the classifier without an\nadditional auxiliary head (e.g. for centerness, IoU, mask-IoU), (ii) due to its\nranking-based nature, RS Loss is robust to class imbalance, and thus, no\nsampling heuristic is required, and (iii) we address the multi-task nature of\nvisual detectors using tuning-free task-balancing coefficients. Using RS Loss,\nwe train seven diverse visual detectors only by tuning the learning rate, and\nshow that it consistently outperforms baselines: e.g. our RS Loss improves (i)\nFaster R-CNN by ~ 3 box AP and aLRP Loss (ranking-based baseline) by ~ 2 box AP\non COCO dataset, (ii) Mask R-CNN with repeat factor sampling (RFS) by 3.5 mask\nAP (~ 7 AP for rare classes) on LVIS dataset; and also outperforms all\ncounterparts. Code is available at: https://github.com/kemaloksuz/RankSortLoss\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Oksuz_K/0/1/0/all/0/1\">Kemal Oksuz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cam_B/0/1/0/all/0/1\">Baris Can Cam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akbas_E/0/1/0/all/0/1\">Emre Akbas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalkan_S/0/1/0/all/0/1\">Sinan Kalkan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ManiSkill: Generalizable Manipulation Skill Benchmark with Large-Scale Demonstrations. (arXiv:2107.14483v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2107.14483","description":"<p>Object manipulation from 3D visual inputs poses many challenges on building\ngeneralizable perception and policy models. However, 3D assets in existing\nbenchmarks mostly lack the diversity of 3D shapes that align with real-world\nintra-class complexity in topology and geometry. Here we propose SAPIEN\nManipulation Skill Benchmark (ManiSkill) to benchmark manipulation skills over\ndiverse objects in a full-physics simulator. 3D assets in ManiSkill include\nlarge intra-class topological and geometric variations. Tasks are carefully\nchosen to cover distinct types of manipulation challenges. Latest progress in\n3D vision also makes us believe that we should customize the benchmark so that\nthe challenge is inviting to researchers working on 3D deep learning. To this\nend, we simulate a moving panoramic camera that returns ego-centric point\nclouds or RGB-D images. In addition, we would like ManiSkill to serve a broad\nset of researchers interested in manipulation research. Besides supporting the\nlearning of policies from interactions, we also support\nlearning-from-demonstrations (LfD) methods, by providing a large number of\nhigh-quality demonstrations (~36,000 successful trajectories, ~1.5M point\ncloud/RGB-D frames in total). We provide baselines using 3D deep learning and\nLfD algorithms. All code of our benchmark (simulator, environment, SDK, and\nbaselines) is open-sourced, and a challenge facing interdisciplinary\nresearchers will be held based on the benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mu_T/0/1/0/all/0/1\">Tongzhou Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_Z/0/1/0/all/0/1\">Zhan Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_F/0/1/0/all/0/1\">Fanbo Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Derek Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuanlin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_S/0/1/0/all/0/1\">Stone Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhiao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_Z/0/1/0/all/0/1\">Zhiwei Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hao Su</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PSE-Match: A Viewpoint-free Place Recognition Method with Parallel Semantic Embedding. (arXiv:2108.00552v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.00552","description":"<p>Accurate localization on autonomous driving cars is essential for autonomy\nand driving safety, especially for complex urban streets and search-and-rescue\nsubterranean environments where high-accurate GPS is not available. However\ncurrent odometry estimation may introduce the drifting problems in long-term\nnavigation without robust global localization. The main challenges involve\nscene divergence under the interference of dynamic environments and effective\nperception of observation and object layout variance from different viewpoints.\nTo tackle these challenges, we present PSE-Match, a viewpoint-free place\nrecognition method based on parallel semantic analysis of isolated semantic\nattributes from 3D point-cloud models. Compared with the original point cloud,\nthe observed variance of semantic attributes is smaller. PSE-Match incorporates\na divergence place learning network to capture different semantic attributes\nparallelly through the spherical harmonics domain. Using both existing\nbenchmark datasets and two in-field collected datasets, our experiments show\nthat the proposed method achieves above 70% average recall with top one\nretrieval and above 95% average recall with top ten retrieval cases. And\nPSE-Match has also demonstrated an obvious generalization ability with a\nlimited training dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yin_P/0/1/0/all/0/1\">Peng Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lingyun Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Ziyue Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Egorov_A/0/1/0/all/0/1\">Anton Egorov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bing Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GraphFPN: Graph Feature Pyramid Network for Object Detection. (arXiv:2108.00580v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.00580","description":"<p>Feature pyramids have been proven powerful in image understanding tasks that\nrequire multi-scale features. State-of-the-art methods for multi-scale feature\nlearning focus on performing feature interactions across space and scales using\nneural networks with a fixed topology. In this paper, we propose graph feature\npyramid networks that are capable of adapting their topological structures to\nvarying intrinsic image structures and supporting simultaneous feature\ninteractions across all scales. We first define an image-specific superpixel\nhierarchy for each input image to represent its intrinsic image structures. The\ngraph feature pyramid network inherits its structure from this superpixel\nhierarchy. Contextual and hierarchical layers are designed to achieve feature\ninteractions within the same scale and across different scales. To make these\nlayers more powerful, we introduce two types of local channel attention for\ngraph neural networks by generalizing global channel attention for\nconvolutional neural networks. The proposed graph feature pyramid network can\nenhance the multiscale features from a convolutional feature pyramid network.\nWe evaluate our graph feature pyramid network in the object detection task by\nintegrating it into the Faster R-CNN algorithm. The modified algorithm\noutperforms not only previous state-of-the-art feature pyramid-based methods\nwith a clear margin but also other popular detection methods on both MS-COCO\n2017 validation and test datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1\">Gangming Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_W/0/1/0/all/0/1\">Weifeng Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yizhou Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Character Recognition using Visual Explanations Derived from the Human Visual System and Deep Networks. (arXiv:2108.04558v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.04558","description":"<p>Human observers engage in selective information uptake when classifying\nvisual patterns. The same is true of deep neural networks, which currently\nconstitute the best performing artificial vision systems. Our goal is to\nexamine the congruence, or lack thereof, in the information-gathering\nstrategies of the two systems. We have operationalized our investigation as a\ncharacter recognition task. We have used eye-tracking to assay the spatial\ndistribution of information hotspots for humans via fixation maps and an\nactivation mapping technique for obtaining analogous distributions for deep\nnetworks through visualization maps. Qualitative comparison between\nvisualization maps and fixation maps reveals an interesting correlate of\ncongruence. The deep learning model considered similar regions in character,\nwhich humans have fixated in the case of correctly classified characters. On\nthe other hand, when the focused regions are different for humans and deep\nnets, the characters are typically misclassified by the latter. Hence, we\npropose to use the visual fixation maps obtained from the eye-tracking\nexperiment as a supervisory input to align the model's focus on relevant\ncharacter regions. We find that such supervision improves the model's\nperformance significantly and does not require any additional parameters. This\napproach has the potential to find applications in diverse domains such as\nmedical analysis and surveillance in which explainability helps to determine\nsystem fidelity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ralekar_C/0/1/0/all/0/1\">Chetan Ralekar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choudhary_S/0/1/0/all/0/1\">Shubham Choudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gandhi_T/0/1/0/all/0/1\">Tapan Kumar Gandhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhury_S/0/1/0/all/0/1\">Santanu Chaudhury</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Instance-weighted Central Similarity for Multi-label Image Retrieval. (arXiv:2108.05274v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.05274","description":"<p>Deep hashing has been widely applied to large-scale image retrieval by\nencoding high-dimensional data points into binary codes for efficient\nretrieval. Compared with pairwise/triplet similarity based hash learning,\ncentral similarity based hashing can more efficiently capture the global data\ndistribution. For multi-label image retrieval, however, previous methods only\nuse multiple hash centers with equal weights to generate one centroid as the\nlearning target, which ignores the relationship between the weights of hash\ncenters and the proportion of instance regions in the image. To address the\nabove issue, we propose a two-step alternative optimization approach,\nInstance-weighted Central Similarity (ICS), to automatically learn the center\nweight corresponding to a hash code. Firstly, we apply the maximum entropy\nregularizer to prevent one hash center from dominating the loss function, and\ncompute the center weights via projection gradient descent. Secondly, we update\nneural network parameters by standard back-propagation with fixed center\nweights. More importantly, the learned center weights can well reflect the\nproportion of foreground instances in the image. Our method achieves the\nstate-of-the-art performance on the image retrieval benchmarks, and especially\nimproves the mAP by 1.6%-6.4% on the MS COCO dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhiwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Hanyu Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Gaze Analysis: A Survey of Deep Learning based Approaches. (arXiv:2108.05479v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.05479","description":"<p>Eye gaze analysis is an important research problem in the field of Computer\nVision and Human-Computer Interaction. Even with notable progress in the last\n10 years, automatic gaze analysis still remains challenging due to the\nuniqueness of eye appearance, eye-head interplay, occlusion, image quality, and\nillumination conditions. There are several open questions including what are\nthe important cues to interpret gaze direction in an unconstrained environment\nwithout prior knowledge and how to encode them in real-time. We review the\nprogress across a range of gaze analysis tasks and applications to elucidate\nthese fundamental questions; identify effective methods in gaze analysis and\nprovide possible future directions. We analyze recent gaze estimation and\nsegmentation methods, especially in the unsupervised and weakly supervised\ndomain, based on their advantages and reported evaluation metrics. Our analysis\nshows that the development of a robust and generic gaze analysis method still\nneeds to address real-world challenges such as unconstrained setup and learning\nwith less supervision. We conclude by discussing future research directions for\ndesigning a real-world gaze analysis system that can propagate to other domains\nincluding Computer Vision, Augmented Reality (AR), Virtual Reality (VR), and\nHuman Computer Interaction (HCI). Project Page:\nhttps://github.com/i-am-shreya/EyeGazeSurvey}{https://github.com/i-am-shreya/EyeGazeSurvey\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Shreya Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhall_A/0/1/0/all/0/1\">Abhinav Dhall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayat_M/0/1/0/all/0/1\">Munawar Hayat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knibbe_J/0/1/0/all/0/1\">Jarrod Knibbe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Q/0/1/0/all/0/1\">Qiang Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Modal MRI Reconstruction Assisted with Spatial Alignment Network. (arXiv:2108.05603v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2108.05603","description":"<p>In clinical practice, magnetic resonance imaging (MRI) with multiple\ncontrasts is usually acquired in a single study to assess different properties\nof the same region of interest in human body. The whole acquisition process can\nbe accelerated by having one or more modalities under-sampled in the $k$-space.\nRecent researches demonstrate that, considering the redundancy between\ndifferent contrasts or modalities, a target MRI modality under-sampled in the\n$k$-space can be more efficiently reconstructed with a fully-sampled MRI\ncontrast as the reference modality. However, we find that the performance of\nthe above multi-modal reconstruction can be negatively affected by subtle\nspatial misalignment between different contrasts, which is actually common in\nclinical practice. In this paper, to compensate for such spatial misalignment,\nwe integrate the spatial alignment network with multi-modal reconstruction\ntowards better reconstruction quality of the target modality. First, the\nspatial alignment network estimates the spatial misalignment between the\nfully-sampled reference and the under-sampled target images, and warps the\nreference image accordingly. Then, the aligned fully-sampled reference image\njoins the multi-modal reconstruction of the under-sampled target image. Also,\nconsidering the contrast difference between the target and the reference\nimages, we particularly design the cross-modality-synthesis-based registration\nloss, in combination with the reconstruction loss, to jointly train the spatial\nalignment network and the reconstruction network. Experiments on both clinical\nMRI and multi-coil $k$-space raw data demonstrate the superiority and\nrobustness of multi-modal MRI reconstruction empowered with our spatial\nalignment network. Our code is publicly available at\n\\url{https://github.com/woxuankai/SpatialAlignmentNetwork}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Xuan_K/0/1/0/all/0/1\">Kai Xuan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiang_L/0/1/0/all/0/1\">Lei Xiang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_X/0/1/0/all/0/1\">Xiaoqian Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1\">Lichi Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liao_S/0/1/0/all/0/1\">Shu Liao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shen_D/0/1/0/all/0/1\">Dinggang Shen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Q/0/1/0/all/0/1\">Qian Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bi-Temporal Semantic Reasoning for the Semantic Change Detection in HR Remote Sensing Images. (arXiv:2108.06103v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.06103","description":"<p>Semantic change detection (SCD) extends the multi-class change detection\n(MCD) task to provide not only the change locations but also the detailed\nland-cover/land-use (LCLU) categories before and after the observation\nintervals. This fine-grained semantic change information is very useful in many\napplications. Recent studies indicate that the SCD can be modeled through a\ntriple-branch Convolutional Neural Network (CNN), which contains two temporal\nbranches and a change branch. However, in this architecture, the communications\nbetween the temporal branches and the change branch are insufficient. To\novercome the limitations in existing methods, we propose a novel CNN\narchitecture for the SCD, where the semantic temporal features are merged in a\ndeep CD unit. Furthermore, we elaborate on this architecture to reason the\nbi-temporal semantic correlations. The resulting Bi-temporal Semantic Reasoning\nNetwork (Bi-SRNet) contains two types of semantic reasoning blocks to reason\nboth single-temporal and cross-temporal semantic correlations, as well as a\nnovel loss function to improve the semantic consistency of change detection\nresults. Experimental results on a benchmark dataset show that the proposed\narchitecture obtains significant accuracy improvements over the existing\napproaches, while the added designs in the Bi-SRNet further improves the\nsegmentation of both semantic categories and the changed areas. The codes in\nthis paper are accessible at: github.com/ggsDing/Bi-SRNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1\">Lei Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Haitao Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sicong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mou_L/0/1/0/all/0/1\">Lichao Mou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruzzone_L/0/1/0/all/0/1\">Lorenzo Bruzzone</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pruning vs XNOR-Net: A Comprehensive Study of Deep Learning for Audio Classification on Edge-devices. (arXiv:2108.06128v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2108.06128","description":"<p>Deep Learning has celebrated resounding successes in many application areas\nof relevance to the Internet-of-Things, for example, computer vision and\nmachine listening. To fully harness the power of deep leaning for the IoT,\nthese technologies must ultimately be brought directly to the edge. The obvious\nchallenge is that deep learning techniques can only be implemented on strictly\nresource-constrained edge devices if the models are radically downsized. This\ntask relies on different model compression techniques, such as network pruning,\nquantization, and the recent advancement of XNOR-Net. This paper examines the\nsuitability of these techniques for audio classification on microcontrollers.\nWe present an XNOR-Net for end-to-end raw audio classification and a\ncomprehensive empirical study comparing this approach with\npruning-and-quantization methods. We show that raw audio classification with\nXNOR yields comparable performance to regular full precision networks for small\nnumbers of classes while reducing memory requirements 32-fold and computation\nrequirements 58-fold. However, as the number of classes increases\nsignificantly, performance degrades, and pruning-and-quantization based\ncompression techniques take over as the preferred technique being able to\nsatisfy the same space constraints but requiring about 8x more computation. We\nshow that these insights are consistent between raw audio classification and\nimage classification using standard benchmark sets. To the best of our\nknowledge, this is the first study applying XNOR to end-to-end audio\nclassification and evaluating it in the context of alternative techniques. All\ncode is publicly available on GitHub.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohaimenuzzaman_M/0/1/0/all/0/1\">Md Mohaimenuzzaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bergmeir_C/0/1/0/all/0/1\">Christoph Bergmeir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meyer_B/0/1/0/all/0/1\">Bernd Meyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NPBDREG: A Non-parametric Bayesian Deep-Learning Based Approach for Diffeomorphic Brain MRI Registration. (arXiv:2108.06771v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.06771","description":"<p>Quantification of uncertainty in deep-neural-networks (DNN) based image\nregistration algorithms plays an important role in the safe deployment of\nreal-world medical applications and research-oriented processing pipelines, and\nin improving generalization capabilities. Currently available approaches for\nuncertainty estimation, including the variational encoder-decoder architecture\nand the inference-time dropout approach, require specific network architectures\nand assume parametric distribution of the latent space which may result in\nsub-optimal characterization of the posterior distribution for the predicted\ndeformation-fields. We introduce the NPBDREG, a fully non-parametric Bayesian\nframework for unsupervised DNN-based deformable image registration by combining\nan \\texttt{Adam} optimizer with stochastic gradient Langevin dynamics (SGLD) to\ncharacterize the true posterior distribution through posterior sampling. The\nNPBDREG provides a principled non-parametric way to characterize the true\nposterior distribution, thus providing improved uncertainty estimates and\nconfidence measures in a theoretically well-founded and computationally\nefficient way. We demonstrated the added-value of NPBDREG, compared to the\nbaseline probabilistic \\texttt{VoxelMorph} unsupervised model (PrVXM), on brain\nMRI images registration using $390$ image pairs from four publicly available\ndatabases: MGH10, CMUC12, ISBR18 and LPBA40. The NPBDREG shows a slight\nimprovement in the registration accuracy compared to PrVXM (Dice score of\n$0.73$ vs. $0.68$, $p \\ll 0.01$), a better generalization capability for data\ncorrupted by a mixed structure noise (e.g Dice score of $0.729$ vs. $0.686$ for\n$\\alpha=0.2$) and last but foremost, a significantly better correlation of the\npredicted uncertainty with out-of-distribution data ($r&gt;0.95$ vs. $r&lt;0.5$).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khawaled_S/0/1/0/all/0/1\">Samah Khawaled</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freiman_M/0/1/0/all/0/1\">Moti Freiman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TOOD: Task-aligned One-stage Object Detection. (arXiv:2108.07755v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.07755","description":"<p>One-stage object detection is commonly implemented by optimizing two\nsub-tasks: object classification and localization, using heads with two\nparallel branches, which might lead to a certain level of spatial misalignment\nin predictions between the two tasks. In this work, we propose a Task-aligned\nOne-stage Object Detection (TOOD) that explicitly aligns the two tasks in a\nlearning-based manner. First, we design a novel Task-aligned Head (T-Head)\nwhich offers a better balance between learning task-interactive and\ntask-specific features, as well as a greater flexibility to learn the alignment\nvia a task-aligned predictor. Second, we propose Task Alignment Learning (TAL)\nto explicitly pull closer (or even unify) the optimal anchors for the two tasks\nduring training via a designed sample assignment scheme and a task-aligned\nloss. Extensive experiments are conducted on MS-COCO, where TOOD achieves a\n51.1 AP at single-model single-scale testing. This surpasses the recent\none-stage detectors by a large margin, such as ATSS (47.7 AP), GFL (48.2 AP),\nand PAA (49.0 AP), with fewer parameters and FLOPs. Qualitative results also\ndemonstrate the effectiveness of TOOD for better aligning the tasks of object\nclassification and localization. Code is available at\nhttps://github.com/fcjian/TOOD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_C/0/1/0/all/0/1\">Chengjian Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1\">Yujie Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scott_M/0/1/0/all/0/1\">Matthew R. Scott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Weilin Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Boosting Salient Object Detection with Transformer-based Asymmetric Bilateral U-Net. (arXiv:2108.07851v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.07851","description":"<p>Existing salient object detection (SOD) methods mainly rely on CNN-based\nU-shaped structures with skip connections to combine the global contexts and\nlocal spatial details that are crucial for locating salient objects and\nrefining object details, respectively. Despite great successes, the ability of\nCNN in learning global contexts is limited. Recently, the vision transformer\nhas achieved revolutionary progress in computer vision owing to its powerful\nmodeling of global dependencies. However, directly applying the transformer to\nSOD is suboptimal because the transformer lacks the ability to learn local\nspatial representations. To this end, this paper explores the combination of\ntransformer and CNN to learn both global and local representations for SOD. We\npropose a transformer-based Asymmetric Bilateral U-Net (ABiU-Net). The\nasymmetric bilateral encoder has a transformer path and a lightweight CNN path,\nwhere the two paths communicate at each encoder stage to learn complementary\nglobal contexts and local spatial details, respectively. The asymmetric\nbilateral decoder also consists of two paths to process features from the\ntransformer and CNN encoder paths, with communication at each decoder stage for\ndecoding coarse salient object locations and find-grained object details,\nrespectively. Such communication between the two encoder/decoder paths enables\nAbiU-Net to learn complementary global and local representations, taking\nadvantage of the natural properties of transformer and CNN, respectively.\nHence, ABiU-Net provides a new perspective for transformer-based SOD. Extensive\nexperiments demonstrate that ABiU-Net performs favorably against previous\nstate-of-the-art SOD methods. The code will be released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1\">Yu Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Le Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jing Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Effect of Parameter Optimization on Classical and Learning-based Image Matching Methods. (arXiv:2108.08179v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.08179","description":"<p>Deep learning-based image matching methods are improved significantly during\nthe recent years. Although these methods are reported to outperform the\nclassical techniques, the performance of the classical methods is not examined\nin detail. In this study, we compare classical and learning-based methods by\nemploying mutual nearest neighbor search with ratio test and optimizing the\nratio test threshold to achieve the best performance on two different\nperformance metrics. After a fair comparison, the experimental results on\nHPatches dataset reveal that the performance gap between classical and\nlearning-based methods is not that significant. Throughout the experiments, we\ndemonstrated that SuperGlue is the state-of-the-art technique for the image\nmatching problem on HPatches dataset. However, if a single parameter, namely\nratio test threshold, is carefully optimized, a well-known traditional method\nSIFT performs quite close to SuperGlue and even outperforms in terms of mean\nmatching accuracy (MMA) under 1 and 2 pixel thresholds. Moreover, a recent\napproach, DFM, which only uses pre-trained VGG features as descriptors and\nratio test, is shown to outperform most of the well-trained learning-based\nmethods. Therefore, we conclude that the parameters of any classical method\nshould be analyzed carefully before comparing against a learning-based\ntechnique.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Efe_U/0/1/0/all/0/1\">Ufuk Efe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ince_K/0/1/0/all/0/1\">Kutalmis Gokalp Ince</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alatan_A/0/1/0/all/0/1\">A. Aydin Alatan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Box-Adapt: Domain-Adaptive Medical Image Segmentation using Bounding BoxSupervision. (arXiv:2108.08432v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.08432","description":"<p>Deep learning has achieved remarkable success in medicalimage segmentation,\nbut it usually requires a large numberof images labeled with fine-grained\nsegmentation masks, andthe annotation of these masks can be very expensive\nandtime-consuming. Therefore, recent methods try to use un-supervised domain\nadaptation (UDA) methods to borrow in-formation from labeled data from other\ndatasets (source do-mains) to a new dataset (target domain). However, due tothe\nabsence of labels in the target domain, the performance ofUDA methods is much\nworse than that of the fully supervisedmethod. In this paper, we propose a\nweakly supervised do-main adaptation setting, in which we can partially label\nnewdatasets with bounding boxes, which are easier and cheaperto obtain than\nsegmentation masks. Accordingly, we proposea new weakly-supervised domain\nadaptation method calledBox-Adapt, which fully explores the fine-grained\nsegmenta-tion mask in the source domain and the weak bounding boxin the target\ndomain. Our Box-Adapt is a two-stage methodthat first performs joint training\non the source and target do-mains, and then conducts self-training with the\npseudo-labelsof the target domain. We demonstrate the effectiveness of\nourmethod in the liver segmentation task. Weakly supervised do-main adaptation\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yanwu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1\">Mingming Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1\">Shaoan Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batmanghelich_K/0/1/0/all/0/1\">Kayhan Batmanghelich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image coding for machines: an end-to-end learned approach. (arXiv:2108.09993v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.09993","description":"<p>Over recent years, deep learning-based computer vision systems have been\napplied to images at an ever-increasing pace, oftentimes representing the only\ntype of consumption for those images. Given the dramatic explosion in the\nnumber of images generated per day, a question arises: how much better would an\nimage codec targeting machine-consumption perform against state-of-the-art\ncodecs targeting human-consumption? In this paper, we propose an image codec\nfor machines which is neural network (NN) based and end-to-end learned. In\nparticular, we propose a set of training strategies that address the delicate\nproblem of balancing competing loss functions, such as computer vision task\nlosses, image distortion losses, and rate loss. Our experimental results show\nthat our NN-based codec outperforms the state-of-the-art Versa-tile Video\nCoding (VVC) standard on the object detection and instance segmentation tasks,\nachieving -37.87% and -32.90% of BD-rate gain, respectively, while being fast\nthanks to its compact size. To the best of our knowledge, this is the first\nend-to-end learned machine-targeted image codec.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Le_N/0/1/0/all/0/1\">Nam Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Honglei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cricri_F/0/1/0/all/0/1\">Francesco Cricri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghaznavi_Youvalari_R/0/1/0/all/0/1\">Ramin Ghaznavi-Youvalari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahtu_E/0/1/0/all/0/1\">Esa Rahtu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dense Optical Flow from Event Cameras. (arXiv:2108.10552v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.10552","description":"<p>We propose to incorporate feature correlation and sequential processing into\ndense optical flow estimation from event cameras. Modern frame-based optical\nflow methods heavily rely on matching costs computed from feature correlation.\nIn contrast, there exists no optical flow method for event cameras that\nexplicitly computes matching costs. Instead, learning-based approaches using\nevents usually resort to the U-Net architecture to estimate optical flow\nsparsely. Our key finding is that the introduction of correlation features\nsignificantly improves results compared to previous methods that solely rely on\nconvolution layers. Compared to the state-of-the-art, our proposed approach\ncomputes dense optical flow and reduces the end-point error by 23% on MVSEC.\nFurthermore, we show that all existing optical flow methods developed so far\nfor event cameras have been evaluated on datasets with very small displacement\nfields with a maximum flow magnitude of 10 pixels. Based on this observation,\nwe introduce a new real-world dataset that exhibits displacement fields with\nmagnitudes up to 210 pixels and 3 times higher camera resolution. Our proposed\napproach reduces the end-point error on this dataset by 66%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gehrig_M/0/1/0/all/0/1\">Mathias Gehrig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Millhausler_M/0/1/0/all/0/1\">Mario Millh&#xe4;usler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gehrig_D/0/1/0/all/0/1\">Daniel Gehrig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scaramuzza_D/0/1/0/all/0/1\">Davide Scaramuzza</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"YOLOP: You Only Look Once for Panoptic Driving Perception. (arXiv:2108.11250v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.11250","description":"<p>A panoptic driving perception system is an essential part of autonomous\ndriving. A high-precision and real-time perception system can assist the\nvehicle in making the reasonable decision while driving. We present a panoptic\ndriving perception network (YOLOP) to perform traffic object detection,\ndrivable area segmentation and lane detection simultaneously. It is composed of\none encoder for feature extraction and three decoders to handle the specific\ntasks. Our model performs extremely well on the challenging BDD100K dataset,\nachieving state-of-the-art on all three tasks in terms of accuracy and speed.\nBesides, we verify the effectiveness of our multi-task learning model for joint\ntraining via ablative studies. To our best knowledge, this is the first work\nthat can process these three visual perception tasks simultaneously in\nreal-time on an embedded device Jetson TX2(23 FPS) and maintain excellent\naccuracy. To facilitate further research, the source codes and pre-trained\nmodels will be released at https://github.com/hustvl/YOLOP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Dong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_M/0/1/0/all/0/1\">Manwen Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weitian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinggang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Blind Image Decomposition. (arXiv:2108.11364v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.11364","description":"<p>We present and study a novel task named Blind Image Decomposition (BID),\nwhich requires separating a superimposed image into constituent underlying\nimages in a blind setting, that is, both the source components involved in\nmixing as well as the mixing mechanism are unknown. For example, rain may\nconsist of multiple components, such as rain streaks, raindrops, snow, and\nhaze. Rainy images can be treated as an arbitrary combination of these\ncomponents, some of them or all of them. How to decompose superimposed images,\nlike rainy images, into distinct source components is a crucial step towards\nreal-world vision systems. To facilitate research on this new task, we\nconstruct three benchmark datasets, including mixed image decomposition across\nmultiple domains, real-scenario deraining, and joint\nshadow/reflection/watermark removal. Moreover, we propose a simple yet general\nBlind Image Decomposition Network (BIDeN) to serve as a strong baseline for\nfuture work. Experimental results demonstrate the tenability of our benchmarks\nand the effectiveness of BIDeN. Code and project page are available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Junlin Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Weihao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_P/0/1/0/all/0/1\">Pengfei Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Chunyi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1\">Jie Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Armin_M/0/1/0/all/0/1\">Mohammad Ali Armin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petersson_L/0/1/0/all/0/1\">Lars Petersson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongdong Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Modulation Network for Audio-Visual Event Localization. (arXiv:2108.11773v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.11773","description":"<p>We study the problem of localizing audio-visual events that are both audible\nand visible in a video. Existing works focus on encoding and aligning audio and\nvisual features at the segment level while neglecting informative correlation\nbetween segments of the two modalities and between multi-scale event proposals.\nWe propose a novel MultiModulation Network (M2N) to learn the above correlation\nand leverage it as semantic guidance to modulate the related auditory, visual,\nand fused features. In particular, during feature encoding, we propose\ncross-modal normalization and intra-modal normalization. The former modulates\nthe features of two modalities by establishing and exploiting the cross-modal\nrelationship. The latter modulates the features of a single modality with the\nevent-relevant semantic guidance of the same modality. In the fusion stage,we\npropose a multi-scale proposal modulating module and a multi-alignment segment\nmodulating module to introduce multi-scale event proposals and enable dense\nmatching between cross-modal segments. With the auditory, visual, and fused\nfeatures modulated by the correlation information regarding audio-visual\nevents, M2N performs accurate event localization. Extensive experiments\nconducted on the AVE dataset demonstrate that our proposed method outperforms\nthe state of the art in both supervised event localization and cross-modality\nlocalization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1\">Zheng-Jun Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xuejin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jiebo Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"User-Centric Semi-Automated Infographics Authoring and Recommendation. (arXiv:2108.11914v2 [cs.HC] UPDATED)","link":"http://arxiv.org/abs/2108.11914","description":"<p>Designing infographics can be a tedious process for non-experts and\ntime-consuming even for professional designers. Based on the literature and a\nformative study, we propose a flexible framework for automated and\nsemi-automated infographics design. This framework captures the main design\ncomponents in infographics and streamlines the generation workflow into three\nsteps, allowing users to control and optimize each aspect independently. Based\non the framework, we also propose an interactive tool, \\name{}, for assisting\nnovice designers with creating high-quality infographics from an input in a\nmarkdown format by offering recommendations of different design components of\ninfographics. Simultaneously, more experienced designers can provide custom\ndesigns and layout ideas to the tool using a canvas to control the automated\ngeneration process partially. As part of our work, we also contribute an\nindividual visual group (VG) and connection designs dataset (in SVG), along\nwith a 1k complete infographic image dataset with segmented VGs. This dataset\nplays a crucial role in diversifying the infographic designs created by our\nframework. We evaluate our approach with a comparison against similar tools, a\nuser study with novice and expert designers, and a case study. Results confirm\nthat our framework and \\name{} excel in creating customized infographics and\nexploring a large variety of designs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tyagi_A/0/1/0/all/0/1\">Anjul Tyagi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jian Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_P/0/1/0/all/0/1\">Pushkar Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khurana_S/0/1/0/all/0/1\">Swasti Khurana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mueller_K/0/1/0/all/0/1\">Klaus Mueller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Tutorial on Learning Disentangled Representations in the Imaging Domain. (arXiv:2108.12043v1 [cs.CV] CROSS LISTED)","link":"http://arxiv.org/abs/2108.12043","description":"<p>Disentangled representation learning has been proposed as an approach to\nlearning general representations. This can be done in the absence of, or with\nlimited, annotations. A good general representation can be readily fine-tuned\nfor new target tasks using modest amounts of data, or even be used directly in\nunseen domains achieving remarkable performance in the corresponding task. This\nalleviation of the data and annotation requirements offers tantalising\nprospects for tractable and affordable applications in computer vision and\nhealthcare. Finally, disentangled representations can offer model\nexplainability and can help us understand the underlying causal relations of\nthe factors of variation, increasing their suitability for real-world\ndeployment. In this tutorial paper, we will offer an overview of the\ndisentangled representation learning, its building blocks and criteria, and\ndiscuss applications in computer vision and medical imaging. We conclude our\ntutorial by presenting the identified opportunities for the integration of\nrecent machine learning advances into disentanglement, as well as the remaining\nchallenges.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_P/0/1/0/all/0/1\">Pedro Sanchez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thermos_S/0/1/0/all/0/1\">Spyridon Thermos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+ONeil_A/0/1/0/all/0/1\">Alison Q. O&#x27;Neil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsaftaris_S/0/1/0/all/0/1\">Sotirios A.Tsaftaris</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-08-30T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/"}}]}]}